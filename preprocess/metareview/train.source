Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes a new self supervised learning framework for learning latent representations of graphs. They provide a theoretical analysis of the method which provides some connections to other approaches. Strengths:* Simple architecture, with good results (Table 1)* Theory is provided to justify their method and approach. Weaknesses:* It appears that the BGRL accuracies shown in Table 2 were reported in the original work and were not reproduced by the authors. * In table 2, LaGraph is only better than BGRL on one dataset (PPI). It would be interesting to see how this method provides additional benefits beyond performance since the methods give similar performance. * The ablation studies are lacking as well. The theory lacks any discussion of properties of the graph or its connectivity and thus is limited in its impact. Thus the novelty of the approach is limited in this respect.<|endoftext|>Strengths * The paper studied self supervised learning for GNNs that is less explored than other domains. * The authors provide theoretical analyses of the proposed method. The relation between the proposed method and previous work is discussed with theoretical analysis. * The memory consumption is much less than a strong baseline GraphCL and works well (e.g., relatively small degradation) with a relatively small mini batch size. * It is not clear whether the performance gain is statistically significant. * Is the substitution of node features with random noise for randomly sampled nodes more effective than dropNode, which are used in previous work such as GRAND [1]? This paper studies a self supervised learning framework to perform representation learning for graph neural networks.<|endoftext|>This paper has good overall presentation of the technical content and sound theoretical analysis regarding the proposed method as well as related work.   In this paper, the authors propose a novel SSL method (i.e., LaGraph) for graph representation learning based on latent graph prediction. There is also a theoretical comparison between LaGraph and various related work with different categories. The settings of other baselines should also be briefly introduced. It seems there is no definition of  masked graph  before Corollary 1 and Corollary 2. 2.Some details regarding the experiments are missing. The authors can give some discussions regarding this direction. Hence, it is recommended to briefly introduce the statistic details and evaluation protocols even in the appendix. It is better to give a clear definition of this task.<|endoftext|>A method called LaGraph is proposed for semi supervised graph representation learning. It is impressive that the proposed method requires no negative samples and works with a small batch size. However, the model assumption, that the conditional distribution of the observed graph is centered at the latent graph, is too strong, which is not natural at all. This should be declared in the paper and the scenario that the proposed method is not suitable can also be discussed. The notation in this paper is not professional, especially the use of subscript.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper proposed a novel method for the multilingual non autoregressive machine translation (NAT) model. The paper is well written and contributes a novel approach for multilingual translation in NAT. 3) Two cross lingual experiments and the visualization of representation prove that the proposed method has better cross lingual capability compared to other multilingual models. Should it be $L_{bt}$? Since CLSR can learn the shared pathway and language specific capability together, why the authors did not choose to compare it in Section 5.1?<|endoftext|>This paper describes experiments in training a non autoregressive Transformer (glancing Transformer, or GLAT) in a multilingual MT setting. Strengths:  Novel application of NAT to a multilingual setting, with an interesting new strategy for code switched back translation. Potential to be a significant breakthrough in increasing both speed and accuracy of multilingual models. The NAT baseline used in the paper is not competitive. No information is supplied about how carefully they were tuned, or how they would generalize to new settings. Results are generally impressive, but the experiments are missing crucial model scaling data points, and they miss the opportunity to evaluate code switched back translation independent from the GLAT architecture the authors favour.<|endoftext|>This paper extends non autoregressive NMT models, particularly the GLAT model, from bilingual translation to multilingual translation. To enable such extension, the authors proposed token level language tags for the decoder, code switch decoder paired with back translation, and also scheduled training between the traditional MLE loss and the BT loss. The paper could be largely improved with more analysis and comparisons. But the necessity of such design is not tested. Neither are explored in the current experiments. Would switch GLAT benefit these languages in a multilingual setup?
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; This approach is shown experimentally to lead to superior model performance for the same model capacity (measured as relative layer width). Additionally, joint training on pairs of examples allows the use of auxiliary unlabelled examples. The empirical results look convincing, and the Authors  interpretation of the method as regularising training sounds convincing. If the model architecture does not ensure the factorisation of the h() function by design, the model has to competing objectives to fulfill: get better at classifying samples correctly and get better at classifying them independently. Some parts of the paper are not clear:  In the Introduction, the Authors say "By training with a pair instead of one sample, the model explicitly learns sample to sample relationships". How were the network architectures modified to support increased input and output dimensionality. The output layer must be larger (c^2 instead c logits), but what about the hidden layers? How is the model capacity compared between standard and joint models in Figure 2? Do both models have the same number of weights? Or the joint one has 2x as many? The presented method is interesting and should be accepted for publication. However, the paper could be improved and there are some open questions which should be investigated.<|endoftext|>Feeding in a pair of data and learn the join conditional probabilities for predictions. This is a huge number. Simple idea that works very well2. While results are good, there is a lack of explanation on why the results are good. My sense is that the combinations of pairs of data span a much larger input space than to feed data one by one into the network. In a sense the amount of ‘independent’ data grows exponentially. The data is not strictly independent since the authors feed different combinations of pairs. Icing on the cake would be having some theoretical proofs of properties of this network. Side by side or concat channel wise? Fig1 seems to suggest side by side.<|endoftext|>This paper introduces a pairwise loss function for supervised learning, named self joint learning. The paper shows empirical studies on dealing with overfitting, adversarial robustness, and detecting OOD data. For example, it is not clear where the robustness against adversarial attacks of the proposed method comes from. I would expect some theoretical or analytical study on this point. Specifically, for each specific applicaltion, the paper mainly compares with the fundamental methods. For example, in adversarial machine leanring, there are many advanced methods that improve adversarial robustness without adversarial training, e.g.in [1] where an ensemble model is used. As the proposed might need to sample multiple pairs of data to make prediction of one data sample, it is important to show how much addtional complexity would be added in the inference phase. The paper studies an interesting idea and shows that the proposed idea improves some baselines in several applications. But I feel that it needs additional study on why the idea works and more comparison with more advanced methods.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; This paper considers zero sum Markov games in a general regime where the model is parameterized by general function classes. The goal of this research is to investigate reinforcement learning algorithms that learn a Nash policy in a trial and error fashion. The main result three algorithms with provable regret upper bounds involving covering numbers and the minimax Eluder dimension. Approximate dynamic programming for two player zero sum Markov games. This is a big plus of the paper. In the operator approach to stochastic games, it is used to define an extension of dynamic programming to competitive settings.<|endoftext|>The paper discussed Markov games with `general’ function approximation. They consider two settings: the decoupled one where the player does not observe the opponent’s policy and the coordinated one where `optimistic planning is performed for both. The work seems to be a `collage’ of different ideas, namely the Bellman Eluder dimension and witnessed rank.<|endoftext|>This paper presents sample efficient algorithms for learning in two player zero sum markov games. The algorithms are for decoupled and coordinated settings, the latter of which is based on  alternate optimism . The authors also extend the Eluder dimension of MDPs to to zero sum markov games using the minimax Bellman operator. The discussion feels lacking. Is there a unifying framework of the three algorithms that you could expand on? Or comment on how the regret bounds compare with each other? When to use one over the other. The contributions seem to be significant.<|endoftext|>This paper studies efficient function approxiamtion in two player zero sum Markov games with general function classes. 3.One minor question is that whether the sample complexity of the model based algorithm can be compared with the model free one? Model free algorithms for both settings and model based algorithm for the leter are provided, all with proved sample complexities. Both decoupled and coordinated settings for learning agents are considered.
Accept (Oral); rating score: 8; rating score: 8; rating score: 10; Another concern is that should the ImageNet pretraining be used as a valid method for the study of robustness to distribution shifts? Besides, the authors also provided a number of practical tips and useful conclusions based on massive experiments under diverse settings and datasets. Strengths:In my opinion, the efforts to unify all the studies of distribution shifts into one general framework is the greatest contribution made by this paper.  Summary: The robustness to distribution shifts is one of the biggest concerns in deploying machine learning systems.<|endoftext|>## Very useful study on calibrating OOD models and benchmarksThis paper provides a fine grained analysis on nature of distribution shifts in image datasets through extensive experiments on various benchmark methods for robustness. They work with an assumption that the underlying factors of variation, including the label, are encoded through discrete attributes. The latent factorization models needs to be better motivated, and the generality of the framework needs to be clarified. Still, this is a very useful work to the community, and if the authors could clarify the questions raised, I would be happy to update the score. I could not pinpoint a single take home message from the paper. The domain adaptation works generally work with a *covariate shift* assumption [2].<|endoftext|>The authors perform an extensive study of different types of distribution shifts to judge which methods perform best on which dataset and/or method. For this, they carefully define which distribution shifts they are interested in and perform a large scale study for each of the shifts. The tips in 4.2. are not really surprising (or novel). “find that a model trained on one set of hospitals may not generalise to the imaging conditions of another.”  > Training on “one set of hospitals” is a weird formulation and it’s not clear what is meant; set of MRIs from multiple hospitals or similar? The authors should comment on this. I think the proposed benchmark / framework is a great addition to the research community as it will standardize model training and evaluation. For example, the code for the framework can be put at the very end such that it does not break the flow when looking at the additional results.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 10; In this paper, the authors present a method for justifying an agent’s policy in a sequential decision making task when the human proposes specific counterfactuals. The development is systematic, and the kind of explanations proposed appear sensible to me as an end user. In its current draft, I am somewhat inclined for the paper to be accepted.<|endoftext|>The paper presents a search procedure with concept classifiers to generate symbolic explanations (in terms of preconditions and costs) to justify an agent s action w.r.t.a foil agent. The authors conducted user study to show the usefulness of their method. Overall, I think this paper contributes nice ideas to the field of XAI in RL. I have some concerns on the real world applicability of the methodology. However, the paper is well written, and the information presented is a nice contribution to the discussion of XAI. Would be good to see some discussion in these areas.<|endoftext|>The authors misunderstand the preference for concise explanations. This concept vocabulary needs to be initially solicited from the user   either through user surveys or designed by researchers. The authors do a good job motivating the paper in the opening paragraph, and Figure 1, but unfortunately do not sustain this momentum through the remainder of the paper. I am not convinced that this paper is ready for publication in ICRL.<|endoftext|>In my opinion, this version is far more clear and addresses the most salient concerns of the reviewers, including me. *What about the action effects? In summary, I recommend the acceptance of the paper. The use of planning allows to produce small explanations in the most critical parts. In some cases, the reason for an action not being possible might be further away. Providing concepts is a relatively cheap label for sequential decision making.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper introduces the idea of estimating the influence an agent has on other agents  actions, in order to achieve better coordination among agents. I think some discussion should at least be provided to explain why the proposed method can perform better than the baselines in some tasks. The proposed method was tested in a wide range of multi agent tasks. The proposed method is tested in a large number of multi agent tasks. I vote for rejecting this paper as many technical details of the proposed method are not very clear to me and many experimental results in the paper are not analysed. It is unclear to me how significant the experimental contributions are. How do you ensure that the network is **slightly** overfitted?<|endoftext|>The paper proposes new forms of intrinsic rewards to improve multi agent exploration and a policy regularization term for an agent to have more influence over others. The proposed method was compared with selected MARL baselines and ablations. There are well known multi agent methods for exploration and influence [1,2,3], some even cited by this paper, that are not compared in the experiments. More comments:1. page 1, introduction, "dissimilarity between other agents  behaviors and their targets": It is unclear at this point of the paper what the authors mean by "targets". I can t find it.<|endoftext|>However, the reviewer has major concerns on the core component of this paper, i.e., the motivation of influence functions. This topic is of interest to the community because efficient exploration in MARL is a long standing problem. In addition, the reviewer thinks that the proposed method has some implicit assumptions and the discussion and comparison of related work need to be improved. Thus, it is not an additional component of the original cooperative MARL algorithms. The clearer motivation of this influence function is critical for this paper.<|endoftext|>My primary critique of this paper is that the application of baselines is inconsistent across benchmarks in a way that makes it hard to say that a thorough head to head comparison has been done. More minor, but I find (what at least I think is) the use of symbols for both policy networks and agent indexing confusing. I also have clarity concerns, but these are more minor and can easily be rectified with an edit. My secondary critique is on the clarity of the work.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; >+ As the authors consider DTN as a component that is able to better learn local context, except comparing different normalization designs, more other alternatives (e.g.[a], [c], [d], [e]) that help with local context learning should be considered in comparison. While some of these works are very new, but the authors should at least include some necessary evaluations on this aspect. The motivation is not sufficiently clear and solid. Motivation is not clear and strong sufficiently:> + Why is the token magnitude so critical for model performance? I pretty much vote a rejection rate. ******* Post rebuttal Update *********The authors have well resolved all of my concerns with additional interpretation and experiments. Therefore, I recommend acceptance. 3.Experiments>+ What positional embedding is used for baseline such as ViTs, PVTs, and Swin T? Given that DTN uses the relative positional embedding (RPE) at each block, for a fair comparison, RPE should be applied to baseline as well. This also helps to separate the effect of RPE from the proposed DTN in analysis.<|endoftext|>The paper first analyzes the limitation of LN in Transformers and then proposes DTN to capture both long range dependencies and local positional context. DTN is a unified version that balances LN and IN. Extensive experiments show the effectiveness of the proposed DTN with some small/middle scale Transformers on the ImageNet. +1) Both the motivation and idea are clear. It may be more robust if DTN can improve performance on some larger models (e.g.ViT L, Swin B, Swin L, PVT L, etc) or down stream tasks (detection, segmentation, recognition, etc.). Q4: The motivation is to balance LN and IN in token normalization. In other words, ViTs can introduce a better positional embedding but does not change LN to do the same thing. Therefore, I rate to  marginally above the acceptance threshold  now.<|endoftext|>This paper studies properties in layer normalization and instance normalization, and states that the two normalization operations have their own drawbacks in vision transformers: LN suffers from lacking inductive bias while IN may be affected by the different semantics within the tokens. Accordingly, this paper proposes a new normalization method called DTN and considers both inter  and intra  token normalization into vision transformers. As the proposed DTN is also integrated into Swin and PVT, can the authors provide the performance comparison between the origin Swin/PVT and the DTN integrated versions on downstream tasks, such as detection/segmentation? 3.All experiments are conducted upon models with performance less than 82.5 top 1 accuracy on ImageNet, which makes the utilization of DTN on more powerful models unclear. The authors should consider more experimental results of DTN on other models like Swin B or PVTv2 B3. However, it is not clear in the paper whether this finding is still valid for larger models, or whether it can handle inputs with different sizes. I am willing to raise my score if the authors can address my concerns in their response.<|endoftext|>Strengths:* The motivation of this work is clear and solid. This work analyses some properties of token normalization. * The proposed method is technically sound. The authors should discuss some related works [1]. One key motivation of this work is to easily induce inductive bias such as local context, which is more important in some downstream vision tasks such as detection and segmentation. Thus I stand in " marginally below the acceptance threshold" now and waiting for the author s response.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; The paper propose a new self supervised method. New loss is designed to explicitly avoid collapsed solution. And the design of minimizing standard deviation for each dimension is insightful. So I thought the main difference is the variance term. If the variance term is the key, it will be better to show the std of BarlowTwins features, and give more analysis of why the combination of variance invariance covariance is advantageous. According to my understanding, SimCLR, Barlow Twins can also work with two different architectures. I thought authors should also compare with these method in the setting of non shared architectures. 3.About the ESC 50 experiments. And I can not find details in the paper that whether BarlowTwins also use the multi modal data. I believe the effect of variance term on BarlowTwins is a key experiment to compare.<|endoftext|>This paper combines three objective functions for the self supervised visual pre training on ImageNet. (1) The alignment between the two different views of an identical image, which is very common for existing methods;(2) The covariance term to bring the off diagonal coefficients of the features  covariance matrix to zero, which is modified from the Barlow Twins;(3) The variance term that defines a hinge function on the standard deviation of embeddings along the batch dimension for every specific dimension of the feature projections . To the best of the reviewer s knowledge, such objective function is firstly applied for the visual pre training in this paper, although the same measure has been used to analyze the model collapse problem (e.g., in the paper of SimSiam), but not be designed as a specific pre trained loss function. The authors provide a clear and detailed discussion to compare this work with the previous methods. In fact, the three loss functions are not very novel. As the reviewer mentioned in the summary, the covariance term is just directly modified from the Barlow Twins. The same measure of the variance term has been used in some previous works (e.g., SimSiam) to analyze the model collapse problem, while it is not designed as a pre trained loss function. Besides, some of the previous methods do not use LARS optimizer and warmup strategy that are applied in this work. 3.While the proposed method is simple, however, the computation time of the covariance matrix is quadratic in terms of the feature dimension, which slow the pre training significantly. Overall, the reviewer tend to vote for accept for this work since the proposed method is simple and it has conducted thoughtful experiments to demonstrate the effectiveness.<|endoftext|>The authors propose a Variance Invariance Covariance regularization technique for self supervised learning. The loss function used in the paper consists of three terms: the invariance term encouraging samples with different view to have similar embedding; the variance term, which is a hinge loss on the variance of the embedded variables (this is the main contribution of the paper, and the authors claim that it helps to avoid variance collapse); and a covariance term which borrows from the previous work Barlow Twin. + A great number of experiments compared with prior methods with detailed set up have been conducted. + A study on multi modal signal representation learning is presented, demonstrating the importance of not requiring architecture or weight sharing in two branches. In fact, compared to Barlow Twins, which does not have the variance term, the proposed method in many cases actually underperforms. Not requiring shared weight between different branches is a feature of Barlow Twin as well. The paper is easy to understand, and has its contribution and novelty.<|endoftext|>The objective function consists of three terms, the invariance, the variance, and the covariance terms. # Strengths  The overall exposition of the paper is clear and easy to follow. The proposed method requires a moderately sized batch of 2048. # Weaknesses  It is unclear that the collapse of representations, the main problem tackled by the paper, is the major bottleneck in self supervised learning. A more natural setting, such as representation learning for multi modal data as in VSE [1], should be investigated. The difference from Barlow Twins needs to be elaborated in detail. I found that the definition of the covariance term is meaningfully different from that of Barlow Twins, but it is not emphasized.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The paper proposes a new scheduling technique for constructing the replay buffer during training. As different to existing baselines which use a task equal selection, they suggest a dynamic selection of the past tasks  replay exemplars, based on the observation that the time to revisit past tasks affects the averaged performance of the continual learner. A naive comparison with ETS is not attractive to validate the effectiveness of the proposed method and it requires in depth comparisons with recent adaptive sample selection based works. 3.The requirement of the validation dataset is also the weakness of the proposed method. The authors store and use 15% of training instances as a validation during the continual learning process. This is essential for computing scheduling rewards. To this end, the proposed method indeed needs more room to memorize previous samples compared to baselines. To this end, I m quite negative about fairness in comparison. Motivation is reasonable and the proposed technique is simple yet seems to be effective compared to a task equivalent selection schedule. However, there are several concerns.<|endoftext|>This paper proposes a new continual learning method that learns to select samples for the replaying process. The replay memory is filled with samples from previous tasks according to a specific proportion corresponding to the action performed at the current task. The paper proposes to find the optimal sequence of actions by using Monte Carlo Tree Search (MCTS) with the reward as the average accuracy over all tasks after seeing the final task. Learning to schedule replay for continual learning is an interesting and novel idea. Clear writing helps readers appreciate the importance of this topic and understand the proposed solution. The behaviour of the method is well illustrated with the analysis in Fig.4.Unfortunately, the experiments fall short of showing the benefit of the solution in a more realistic setting. Here, the paper focuses more on memory efficiency and ignore computing efficiency. In Fig.3, some tasks only need one iteration (i.e.the found policy is random) to achieve significantly better results. 2021.Overall, I like the topic and proposed method. I may consider raising my score if the authors can: (1) clarify the practicality of the trade off mentioned above, (2) combine MCTS with recent replay techniques and show clear improvement, and (3)  prove that MCTS is better than simple search/optimization methods.<|endoftext|>I overall found this work interesting and refreshing, since it proposes a novel approach to the improvement of rehearsal based methods by inquiring what is the best order for revising past tasks. I believe that this issue is very relevant and that there is plenty of room for improvements in this specific area. However, I find that this work suffers from the following weaknesses:+ The biggest concern I have with this work is that randomly sampling what information to replay might not be too relevant as a baseline. On the one hand, it is certainly true that this is how the majority of rehearsal approaches work; on the other, it is fairly atypical for these methods to make the assumptions that the authors make here, i.e.that data from past tasks are fully accessible and that the choice of what to replay is up to the model. + Along the lines of the previous point, I found the initial experiment to be rather unsurprising. + I am unsure of the efficiency implications of RS MCTS in a setting such as continual learning, which is generally quite focused on having a reduced model footprint and execution time. The following point were minor and did not affect my evaluation+ I am not sure I correctly understood how robust the proposed policy is to changes in the order of classes: can the findings of Figure 4 generalize to other orders? + While the paper makes it very clear that this approach is effective when the memory buffer is small, what does it happen when we assume that the memory buffer is very large? This could be an interesting point to further explore.<|endoftext|>The key motivation of this work is that the bottleneck of replay in continual learning is the processing time in each training cycle and not storage space for the historical dataset. ### **Questions**  It is not clear if the memory selection process through approaches like k means chooses datapoints or is it a replacement to MCTS for choosing a sequence of tasks? I have assumed the former while reading the paper. Are the K (number of branches) models stored in memory, so it is easier to increase the number of models from T to T+1 without training from scratch? Table 1 is quite interesting, as it shows a significant boost due to an optimal selection of tasks for replay memory strongly supporting the claim that replay schedule can be quite important for continual learning  For extreme scenarios where the replay memory does not have some of the classes that were previously seen, the approach of selecting the correct schedule is able to match performance for continual learning baselines that were designed specifically for memory efficient training. To prevent that K would need to be increased, which is often possible using a lot of parallel compute, but that is also limited to the processing constraints posed on the problem in the introduction. In the bubble plot in Figure 4, the explanation is that tasks 4 and 6 need lesser replay in the early stages probably because they are less correlated with the other tasks. Hence my initial rating is borderline
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper shows that an auxiliary self supervised task that enforces temporal consistency of latents improves sample efficiency in continuous control environemnts, and identifies important implementation details that make it work. Deep reinforcement learning at the edge of the statistical precipice. This seems to be a major issue. The empirical contribution here and the detailed analyses itself would have been valuable on its own.<|endoftext|>Is the paper about using k step latents plus an auxiliary loss tying these to observations? There is a very large body of work in model based reinforcement learning that uses k step model based predictions and corresponding losses to improve data efficiency and performance, and reduce model rollout errors. Why have both \Psi_o and P? That said, the main issue with the clarity and writing of the paper is that the contribution is not made clear both in the paper nor in the experimental analysis.<|endoftext|>Strenghts:* Simple approach* Clearly written* Representation learning for better generalization or sample efficiency is an important topic in RL* Positive experimental resultsWeaknesses: * My main worry with this application of BYOL to RL is that the introduction of the transition model T changes the  support  of z_m away from the support of z_o. However, the necessity to include action conditioned transition models in RL raises additional complications compared to BYOL which have not yet been addressed (or discussed) and I believe these should be included in the paper before publication.<|endoftext|>Summary:The authors introduced k Step Latent (KSL), a representation learning method for visual based continuous control tasks. The difference argument could be made convincing given that the authors could provide further clarifications of the differences between KSL and SPR, from an algorithmic perspective, and provide further empirical comparisons between the two agents (e.g., the authors state that "KSL’s architecture is general enough to be applied in both discrete  and continuous action domains", hence it would be interesting to see an adaptation of KSL to discrete control and test it on atari benchmarks, and compare with the SPR results). KSL indeed show state of the art performance on the presented tasks and I like the way the authors assessed the quality of representation learning. However, KSL seems like a combination of existing methods, but lacks comprehensive empirical evaluation in that sense.<|endoftext|>The paper tackles the problem of sample inefficiency in continuous control, by noting that standard RL methods deal with both policy optimisation and representation learning jointly with a single supervisory signal, namely the reward. I would have like to see discussion on how this differ/relates to successor features, when learned jointly or separately. It also seems to contradict another desired property: generalisation from one task to another. On the experiment:The experiments are extensive and the method is compared against sensible baselines. But overall an interesting contribution!
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper proposes a new approach to learn sample weights aiming to solve imbalance classification problem. The paper is well written. The authors argues that there exists a constraint and allow both parameters to be learned together thus this paper proposed a new algorithm combined with previous method to learn the sample weights. The solution is novel from theoretical perspective, and it is evident in the experiments as well.<|endoftext|>For this purpose, they first propose a mechanism to precisely learn the relationship between the weights and the trained model in the model objective. The authors start by explaining the process of learning from a class imbalance dataset. The authors first propose a method to learn theta and w with a constraint, which can accurately optimize both parameters. As such, they combine their approach with the one proposed by Hu et al., to get the best results. The paper is relatively smooth to read. Overall the approach seems justified and the results are promising.<|endoftext|>They propose a novel mechanism of learning with a constraint, which can accurately train the weights and model. Then, they propose a combined method of our learning mechanism and the work by Hu et al., which can promote each other to perform better. This is not comprehensive to show the advantages of the proposed method in the imbalanced classification. For example, the model parameters θ obtained by this method is biased.<|endoftext|>The paper presents a new method for learning example weights together with the parameters of a deep neural network. Therefore, I believe the contribution of this paper is just enough to warrant acceptance. The novelty and significance are limited, since the proposed method is similar to the one proposed by Hu et al and the experimental results in most cases show no statistically significant difference in the results (although for one case the difference is very large).
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposed to use neural processes for supervised multi modal learning, which have the abilities to estimate the uncertainty of prediction and to handle missing modalities. Specifically, a directed acyclic graph is learned for each modality in the neural process, which are then used to construct a mixture of graphs (MoG) to sidestep the modalities missing problem. The problem investigated in this paper is. But I think the contribution of this paper is not clear and the writing of this paper should be improved. I recommend the authors to take a thorough review of the field of neural process and think about how effectively adapt the it to address the modalities missing problems. I can say that the underlying ideas of the two paper are almost the same, including the introduction of directed acyclic graph in the model. It is better to pay more attention to how to adapt FNP on multimodal learning. However, there are no corresponding experiments to evaluate this hypothesis.<|endoftext|>To integrate information from multiple modalities, mixture of graphs (MoG) is introduced. Generally, this paper is easy to read. I have the following concerns about the paper:   1. 2.Some statements in the article are biased. For example, the introduction states that “existing generative models for multi modal learning focus on latent representation, but do not fully incorporate the label information.”  Generative models are only one of the branches of multimodal learning, and many multimodal supervised models have been developed, e.g., DeepIMV in the paper. The proposed MoG is not convincing and novel, which just averages the graphs from different modalities.<|endoftext|>This paper tries to use Bayesian relational generative model for scalable multi modal learning. They propose a class of stochastic processes that learns a graph of dependencies between samples across multi modal data types through adopting priors over the relational structure of the given data modalities. The so called mRNP method can address the limitations in joint posterior approximation. The method is interesting, and the introduction is attractive to read the following. Moreover, it seems the authors are not clear about variational inference. In fact, it should be maximized.<|endoftext|>Qualitative results corroborate the quality of the learnedrepresentations, which can be visualized as relational graphs, whereas thebaselines require additional dimensionality reduction techniques forvisualization. The paperis transparent about the limitations of the proposed approach, for instance,that it is fully supervised and that the results can dependent on thechoice of a good reference set. However, the paper makes strong claims in itscomparison to previous work, some of which are not sufficiently well supported. Further, for a fair comparison, the experiments would benefit from moresupervised and semi supervised baselines. Therefore, I tend towards rejectingthe paper in its current form. **Questions and comments:**  Why do you refer to the proposed approach as a generative model? If not, the naming makes the comparison to multimodal generative  models (e.g., the mVAE) confusing. The statement is not backed up by an  explanation or reference.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Motivated by the limitations of the expressive power of MPNNs this paper addresses one of the expressive shortcomings   counting substructures. While previous works allowed counting by suggesting higher order GNNs, they come at a high computational cost. The theoretical analysis is quite thorough and places the work in the context of other works. **Clarity and Readability**   I find the paper packed and hard to follow, although it seems that the authors put a lot of effort into defining every bit of notation. or whether several covering sequences should be used together? 3.**Scalability**   Although providing results on improved complexity, the method is only demonstrated on toy datasets, what are the gaps for scalability in real world datasets? 4.**Larger substructures**   results are shown only on substructures of size 3, I would have wanted to see results on larger ones.<|endoftext|>A difference with previous work is that, during recursion, this aggregation is always performed on recursive *subsets* of neighborhoods. This is not significant and sits in a family of similar structured GNNs. From my view, the paper is borderline on the negative side. However, the notations system is a bit cumbersome and hard to interpret. As an important hyper parameter, the proposed method relies on a vector of radius (r_1, r_2, ... ) that are recursively used in the neighborhood nodes when building the presentation of any node. In the experiments, $r_1$ is set to a very small value (1 or 2). This is in general an expensive method and only suitable for small degree graphs, and related application (like counting triangles). The statistics of the dataset are also missing, especially, it  is important to say more details about the EXP dataset, and what are "certain propositional formulas" and their sizes. As Deep LRP (rather than the authors  method) has the best performance,it is important to have another experiments to compare their efficiency.<|endoftext|>Please see my final comment to the authors for more details. Is it only the computational complexity (that perhaps makes it harder to do more extensive experimentation)? I am not negative about the paper, but I believe that this part is crucial to recommend acceptance. Suggestions: (1) counting various substructures and contrasting their r parameters with those predicted by the theory, (2) real world datasets where substructure counting is important (a single one would be OK – it might be useful here to measure the runtime of your method and contrast it with the sparsity of the graphs)  and optionally (3) predicting graph properties. I would be willing to discuss my score after that. To the best of my knowledge, this is the first time that the algorithmic technique of recursion is employed to define a Graph Neural Network. This is the heart of the proposed method, but it is a bit hard to follow. **Other comments and questions**: (I don’t expect all these to be addressed in the rebuttal, but it might be useful to clarify in the next revision of the paper) 	How important are the identifiers used to augment the node embeddings (Eq.(3))?Can RNP retain its expressive power without them or it might collapse to that of an MPNN? Although the authors argue that this might be hard to improve, there are some cases that this is not true. I believe at least the proof idea for Theorem 1 should be given in the main paper. Is that general enough? Several constructions and proof techniques are of independent interest which is also positive. Can the authors clarify their position w.r.t.that?For example, I would expect the triangle counting task to be relatively easy for this network, but RNP does not seem to be better than other more expressive GNNs. How do the authors comment on that?<|endoftext|>The paper provides a theoretical analysis of the proposed architecture, as well as 2 experiments. * The proposed architecture is backed with wide theoretical analysis, proving that it can count substructures and that it can approximate local graph functions. Main weaknesses:* The presentation of the method, described in section 4, was hard to follow, even with the example in Figure 1. I.e., in large, not so sparse graphs the method will not be feasible, yet for small graphs other GNN architectures can be just as good. In practice, I m not convinced that the new architecture has any real advantages over existing, simpler architectures. I (weakly) support the acceptance of the paper.<|endoftext|>The authors prove two expressiveness results: (1) RPN GNN succeeds in the task of counting subgraphs (with an appropriate parameter choice); (2) RPN GNN has the universal approximation property w.r.t local graph functions. Also, the computational complexity of this architecture is calculated. Finally, several experiments are done on the EXP dataset, and on counting substructures from random graphs. In this aspect, this paper makes a step forward in our understanding of the expressive power of GNNs. I have two concerns about the paper which I would be happy to see the author’s response:1) The comparison of computational complexity to other models in Section 5.3 is unclear, and it is also unclear whether RNP is really more efficient asymptotically than k WL. by Table 1, the time complexity of k WL is n^k, and for RNP it is O(n). This means, according to Theorem 3, that the time complexity (asymptotically) is exponential in k. Hence, it is not clear that RNP is more efficient than k WL. Also, if k O(1), then the time complexity of k WL is n^{O(1)}, which seems to be the same as RNP. 2) The experiments are done only on artificial datasets such as counting subgraphs from random graphs, and the EXP dataset from Abboud et al.which is specifically designed to test 3 WL. Given a certain choice of (r_1,...,r_\tau), where \tau k 1, is RNP and k WL equivalent in approximation power, or is there still a gap? I think this should be clarified in the paper.
Reject; rating score: 5; rating score: 6; rating score: 6; This paper proposes a regularization strategy for learning sparse deep CNN models. The Feature Flow Regularization (FFR), penalizes first and second order changes in the intermediate features between consecutive layers of the network. The intention of this regularization is to  smooth the evolution of features and make sparse weights. The feature flow regularization method (FFR) is simple and effective, and create smooth  consecutive features. 2.Authors provides detailed analysis in Sec.4 to show the method is valid. At the bottom level of CNN, the features are basically similar,  such as the edges. However, with constraints on contingous features and two order features, the representation power of intermediate features will be reduced significantly. The feature maps of CNNs are used to explore the high level distinctiveness of inputs. So, more explantations about the FFR will make this paper clear. 3.The results of experiments in Table1,2 have no advantage in contrast to other methods. Expecially in Table.2, the baseline of resnet 50 on Imagenet is 71.56\%, which is significantly lower than 76.1\% officially. 4.Table.1 and Table.2 is not consistency. Table.1 reports the error rate, but Table.2 presents the accuracy. The paper proposes a regularization loss that penalizes L1 distances between consecutive feature maps that are projected to the same dimension.<|endoftext|>The paper proposes a new method, called feature flow regularization, for structured sparsity and pruning in deep neural networks. The method seems to be original for structured sparsity and pruning and is rather simple. The experimental comparison is with SOTA methods, and the results show that its performance is reasonable. The paper provides some analysis for the connection of the proposed regularization and sparsity but unfortunately the analysis is not really rigorous. Moreover, there is no discussion and analysis for the curvature term. Although the presentation of the method and Eqs. The experiments are a bit limited, as they are only on two datasets and two architectures. While the results on CIFAR 10 are somewhat encouraging, overall, the results do not seem strongly in favor of the proposed method, especially on ImageNet. Also, one disadvantage of the method is that a user cannot select a desired pruning or sparsity ratio, and the sparsity is indirectly controlled by hyperparameters which are not adequately investigated in the paper.<|endoftext|>This paper deals with network pruning from a fresh perspective. Personally I think this idea is intellectually appealing, and may have uses beyond the network pruning setting. I m not super familiar with the latest developments in the crowded pruning literature and will have to rely on other reviewers on that. I want to underline a connection to some seemingly unrelated topics. The present idea seems to extend an earlier, frequently reinvented idea that a network s output should change only a little when the input changes a little. This was probably first coined by Drucker and LeCun as "double backpropagation" [1], and has been heavily used in adversarial learning (input regularization [2]) and GAN training (R1 regularization [3]) in recent years. This was furthermore explicitly relied on in StyleGAN2 [4] for trying to guarantee a simple, well behaved generator mapping between a latent space and output images (path length regularization). That said, I think the authors need to contrast the method with "trajectory regularization" [5], which was put forward in yet another context, but would seem to do a closely related thing by encouraging simpler representations by regularizing transitions inside a network. It will be important to draw the connection to assess how far beyond pruning the novelty extends. In my opinion Fig 1 should come sooner than page 4. It is useful mainly as a high level intuition builder. Figures 3 & 4 are good. Typo: "Strucuted sparsity" (Fig 4).
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; A key observation from this work is that: there is a connection between weight norm bouncing, weight decay and optimizer convergence. They also indicate that for those experiments without weight norm bouncing, like experiments in NLP and RL, the learning rate schedules could be simplified with no loss in performance. Basically, ABEL will drop the learning rate when the weight norm starts to bouncing. To compare ABEL with other scheduling methods, the authors not only provide the final performance of model but also study the roustness of ABEL. Cons:* Some parts of the paper are not well described or I could not find the related context. The current one is readable but could be more detailed. Also, in section 4.1 equation (1) pops up from nowhere. The proposed idea is concise and interesting.<|endoftext|>Among these, I would remark on: better treatment of the background material, clearer identification on when the weight norm behaviour happens beside $L^2$ norm (possibly looking also for counter examples!), rethinking section 6, and a more convincing set of experiments (for showing convincing evidence about e.g.5.2).Regarding this last point, I want to clarify that in my review I mentioned [1] not for the grid search, but rather for the time controlled experiments. If you go with random search for selecting the hyperparameters of the learning rate adaptation methods. I personally think that a recipe to make the comparison fair enough is to choose a prior distribution (e.g.uniform/log uniform) that covers reasonable values (e.g.as used for different datasets) with mean equal/close to the known well performing ("optimal") value.<|endoftext|>This paper presents a novel method for automatically decaying the learning rate when training deep neural networks. The authors conducted extensive experiments in different settings, highlighting the effectiveness of the proposed LR decay schedule, as well as the special cases where weight bouncing doesn t happen and simple decay schedules work competitively. The proposed technique is also simple enough to be practically useful. 2.I also appreciate the careful investigation on the generality of this finding, including experiments on different benchmarks as well as the importance of l2 weight decay. 2.As a paper about SGD, I m a bit disappointed that training loss curves are not shown in the main text, along side test errors. 4.Other than the weight norm, I m also curious how the gradient norm evolves during training and if it s useful in a similar way for guiding the LR decay. Overall, I decide to stand with my original ratings. And I think this observation can potentially help us better understand the training dynamics of deep models.<|endoftext|>This work deals with the optimization of learning rate schedules for deep learning. An equation relating the change in weight norm to the gradient norm, learning rate, weight decay, and the weight norm  for weights that go into batch norm. The scope of the experiments is also reasonable in my mind, although further comparison with other adaptive algorithms could be useful. This equation provides intuition for the behaviour of this weight norm. Other than that they compare with non adaptive methods. What about algorithms such as AdaGrad or other adaptive schedulers on the market? True the authors show that a plateau is not a robust feature. More minor comments are  1. In addition in their algorithm, I didn t see a precise treatment of a sticky point   what happens if one initializes the weights to a smaller value thereby reaching the maximum before the minimum? 4.In the motivation section, the authors say "From the two setups in figure 1 it seems that optimal schedules tend to decay the learning rate after bouncing when the weight norm growth slows down." which can be taken to think that these two schedules really match.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper presents a genetic algorithm framework for molecular optimization. It produces valid molecules through the crossover and mutation operations along with appropriate fitness functions. The evaluation was done in one of the property optimization tasks. 2.The proposed model outperforms other baselines in the PlogP optimization benchmark. The novelty is limited. They apply an existing genetic algorithm framework and the fitness functions are straightforward. The operations are the most novel point in this paper, however, its novelty is very limited because its modification is not significant compared to the existing one. Experiments only show one property optimization, but they claim it as a multi objective problem. There are many molecular properties that need to be optimized at the same time, which should have been considered in this paper in order to prove its superiority. 2.Many other experiment results should be provided in order to assess various aspects of the proposed model.<|endoftext|>This paper proposes a genetic algorithm for constrained molecular optimization. To generate molecules with better properties while staying similar to the starting molecule (lead), this paper proposes a two phase procedure. The first stage (constraint satisfaction) searches for feasible molecules that are similar to the lead compound. The second stage optimizes the objective function (property). The method is evaluated on logP optimization task, with minor improvement over previous work GA DNN (Table 1). * Lack of novelty. I vote for rejection. The method itself is not novel because it is a standard application of genetic algorithm. The results show that the proposed method did not outperform existing baselines. The main claim of the paper (benefit of two stage procedure) is not supported by ablation study.<|endoftext|>The authors proposed a genetic algorithm for molecule generation. The authors conduct experiments to optimize the LogP of molecules. 1.Compared to (Nigam et al., 2019), the objective function is almost the same, i.e., Eqn. (3) in this paper and Eqn. 2.The authors only conduct experiments on improving LogP, which cannot show that this is a general method for molecule generation. 3.The improvement over GA DNN is limited. The method lacks novelty and experiments are not convincing.<|endoftext|>This paper proposes to use a two stage genetic algorithm for similarity constrained molecular optimization. The authors define chemically relevant strategies for crossover/mutation, and show good results on constrained optimization of penalized logp. On a high level, molecular optimization under structural constraints is an important research direction, and it has relevance to real world tasks. 2.Results on the core optimization task look promising. The experiments in the paper are limited and not very relevant to real world drug discovery. Moreover, as the algorithmic novelty is rather mild, I d expect a more thorough experimental evaluation. (a) The only explored objective is penalized logp optimization from the JT VAE work. (a) When the algorithm cannot find molecules that satisfy the constraint to form the initial population, the authors say the initial SMILES are "randomly arranged". I believe the paper needs more work to be made ready for publication.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper proposes a new model, called SketchODE, for learning representations of sketches using neural ODEs. The decoder is modelled with a second order neural ODE acting on an augmented state (which allows for self overlapping trajectories which is necessary for handwritten data) that includes the stroke itself as well as an underlying hidden state. The encoder is parameterized by a neural CDE which takes as input some ground truth trajectory and returns a new state which is mapped to the latent vector. As far as I can tell, the only discussion of limitations is one sentence in the conclusion mentioning the computational complexity of the method. The model is then trained end to end using a reconstruction loss between the ground truth and reconstructed trajectories. The authors then discuss and demonstrate various compelling properties of their model. The main contributions of the paper in my eyes are then:  Introducing an interesting continuous neural model of handwritten data  Demonstrating compelling properties of this continuous model over traditional discrete models  Introducing the VectorMNIST dataset, which I believe could be useful for other researchers  Interesting experiments on various handwritten datasetsOverall, I think this is a strong paper which introduces an interesting model and demonstrates various compelling properties of this model through a series of experiments on several handwritten datasets. This paper introduces a new continuous time model for learning representations of handwritten data. The latent interpolation experiments are also very nice. Handwriting and drawing are inherently continuous, so it makes sense to model this data continuously. However, as discussed in the weaknesses section, it would be nice to have more samples/examples from the model (particularly on the more complicated datasets), as well as having error bars. It would be nice to have e.g.more examples of latent interpolations and conditional sampling on Quick Draw and DiDi. Sentences like “there has been little applied work using neural ODEs” and “parameterized ODE dynamics models have largely been treated as a replacement for ResNets where the intermediate states are of little importance” are misleading. As far as I can tell, the authors do not cite a single neural ODE + physics paper which both use the intermediate states and are applications of neural ODEs.<|endoftext|>By introducing the techniques from neural ODEs on chirographic data, the authors were able to train a first continuous time generative model. New observations that are only available on continuous time generative models are made. Although the chirographic data itself might have limited application, it served as a great benchmark for testing such a continuous time generative model. The paper is clearly written and well organized overall. ### Weaknesses  The tradeoff between computational complexity and representation power induced by using Neural ODEs is not directly handled. To mitigate this, the authors might want to simply share some information about the train/prediction cost and compare them with existing methods. ### Questions  In the context of the multi stroke dataset in Appendix C, what exactly are linear transformations u, v? The possibility of different applications might interest a wide range of ICLR audiences, and hence I recommend accepting the paper.<|endoftext|>This paper presents a novel approach to represent chirographic drawing data with Neural ODE and demonstrates its effectiveness by comparing with the standard baseline Seq2Seq model. Also I greatly appreciate the authors providing the details for working with the Multi stroke format data, not just with Full sequence format. The paper is very well written, but it was difficult to assess the level of novelty. The authors claim that this is a new continuous time sequential model, but they only tested their model with drawing datasets, that seems to me that there is a little bit of overstatement. This paper could be improved if the authors provide experimental results in other sequential domains, such as speech. Also, adding to that, it would be most appreciative if the authors could add a list of main contributions when this is compared against Neural ODE, as many ideas are dependent on this previous paper and it made me difficult to easily evaluate the level of contributions. Overall, the paper is very well written and shows fascinating results with continuous time seq2seq model. However there are still some missing components in the paper, and more clarification needed to distinguish this method from the Neural ODE work.<|endoftext|>The paper describes a way of representing online handwriting data, typically seen as a discrete sequence, in a continuous space, using neural ODEs. The learned representation allows sampling from and interpolation in the latent space. One of the central claims made by the authors is that models that use discrete representation need to spend their learning capacity for learning both the global structure and the temporal continuity (basically, the sampling method), while the models using continuous representation can use their full capacity to represent the global structure only, thus becoming more data efficient. Authors also capture the "velocity" of the pen tip in their model. Authors provide extensive details on the training procedure. Authors evaluate their approach on QuickDraw dataset of handwritten sketches, DiDi dataset of handwritten diagrams, and the collected small scale dataset of vectorized MNIST digits. **Concern** Authors show  the interpolation in latent space between two novel objects, and compare the quality to RNN RNN. Authors show an interesting emergent property of their work, namely, by controlling the capacity of the model decoder, they can control the amount of "complexity" or "high frequency" in their reconstructions. Authors propose a way of encoding online handwriting data in a continuous space representation by using Neural ODEs, and show interesting properties of the learned model. This is a novel idea interesting to wider audience. Their main weakness of the paper, however, is the experimental section, and based on its  current content I can not recommend the paper for acceptance. Combined with the complexity of the training requiring ODE solvers, it is not clear whether the proposed approach provides any advantages.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The authors proposed a new deep unrolling version of the proximal alternating minimization (PALM) algorithm for addressing the semi supervised blind source separation problem. The authors aim at overcoming some weaknesses of previous approaches such as the need of tuning parameters, convergence  to spurious local minima, etc. However, there is no empirical evidence showing that this statement is true. Typo:    page 3, 1st bullet on Contributions: tailored instead of taylored The paper provides an unrolling version of the well known PALM algorithm with application to blind source separation for unmixing of hyperspectral astronomical data. The authors provide empirical results that show the advantages of the proposed approach over other relevant state of the art unrolling algorithms. However, the result are not always convincing (see Weaknesses) and could be significantly improved. The paper could be also better organized since I feel the authors use a lot of subsections (especially in the experimental part), which disrupt the flow of the paper.<|endoftext|>Problem: in the context of blind source separation (BSS), the PALM algorithm is highly sensitive to regularization parameters and initialization. Solution: use algorithm unrolling to learn the sensitive (hyper )parameters. Results: LPALM outperforms PALM, and is compared to some competitors*. I am having a hard time understanding how your competitor methods are trained on "a part of the pixels of the image". 1) The reason I gave a 2/4 in "Correctness" is that, while you do solve the sensitivity problem w.r.t.PALM s hyperparmeters, you introduce a slew of new hyperparameters associated with training. 3)  To be clear, unrolling is fully understood to give improvements over a non learned counterpart (although it is good to rehash it in specificity in Sec 3.1,3.2, they are not sufficient for publication in ML).<|endoftext|>This paper studies the linear mixing inverse problem, also known as blind source separation, in the context of multispectral imaging. The authors consider unrolling an alternating minimization based on ISTA, and learning hyperparameters and weight matrices of the unrolled optimization, which they call LPALM. By explicitly representing A in the unrolls, the structure of A is directly used and can adapt at test time. There are also works that extend this to deep learning based regularization. Therefore, it is unclear what value/tradeoff exists between assuming a specific A (which is estimated from the data and known to be inaccurate), vs. a true semi blind loss function. It is also not clear how the proposed method would compare to a simpler version of learned PALM, such as "LPALM LLT", which would be similar to the ISTA LLT while still updating A through alternating minimization. Therefore, it is not clear which component is the driving force for improved results. There is also no theoretical justification for the choice of algorithm, and no analysis of convergence.<|endoftext|>The components are given by an unknown mixing matrix and source (or coefficient) matrix. For LPALM, the training data is synthetic – how does this affect results? The goal is to estimate both the mixing and source matrices from data. To solve this, the authors introduce an unrolled version of the proximal alternating linearized minimization (PALM) algorithm, where various matrices and parameters in the algorithm are learned through backpropagation in a differentiable programming framework. Some numerical results on synthetic and experimental data are provided, illustrating the potential of the approach. As such, I do not recommend that this paper be accepted for publication. What do these issues look like and what is causing them? The authors go on to introduce “semi blind” source separation. For their numerical experiments, the authors first concentrate on the source matrix update step. They test various algorithms for this and conclude that one of them (LISTA CP) performs better than the others and is chosen for inclusion in the proposed LPALM algorithm. The authors also make statements like “LISTA trainable parameters … tend to be optimized by marginalizing over the varying mixing matrices A* in the training set, which could have led to deteriorated results”.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper argues that language models (LMs) require mechanisms to retain information about context words for better next word prediction. Several transformer models and LSTM based language models of different sizes are compared in terms of reduction in surprisal. Also more training data and greater model depth lead to better performance in case of transformer models. **Strengths**  The paper investigates an interesting task. It is understandable, well structured and well written. [she took a break blah blah.] A limited number of words (~ 230 unique words) and lists (~ 230 lists of length < 10 words) are used in experiments. The number of parameters significantly differs between transformers and LSTM networks used in experiments, which naturally affects their performance and makes the results not directly comparable. It s not clear why transformer models are compared against vanilla LSTMs but not bidirectional LSTMs with attention. This submission investigates the interesting question of memory in language models, but the task formulation, data and experimental setup can be improved and therefore I don t recommend the paper for publication at this time.<|endoftext|>At the same time, it seems to have certain weaknesses in the experimental setup and design which diminishes the conclusiveness of some of their results. The paper attempts to investigate the ability of Transformer and LSTM based LMs to retrieve information about the prior context. The premise of the work is that making use of prior context is an important functionality of language models and while several prior works have demonstrated that LMs do so to capture certain types of dependencies, their work aims to understand the extent to which Transformers and LSTM based LMs can retrieve words from the prior context. The extent to which models such as GPT 2 can retrieve prior information looks very interesting and the difference in performance between Transformers and LSTMs is also worth noting. Since some models like GPT 2 did quite well with noun lists of length 10, I was curious to what extent they could maintain such a list. It is mentioned that the depth of a Transformer seems to be a key attribute.<|endoftext|>The paper proposes a task for pretrained language models: extract in verbatim a list of nouns occurred in previous context. The main conclusion seems to be that transformers can perform robust verbatim recall while LSTM tends to store a semantic gist. The other interesting results include that transformer performance on this task improves with corpus size and transformer depth, and the performance of transformers is linked to the attention patterns. If attention is the key ingredient that makes transformers perform better at this task, it will be interesting to see if integrating attention in RNNs can help with its performance.<|endoftext|>This paper studies how LSTMs and Transformers represent prior context. In particular, this work adapts benchmark tasks for human working memory to neural language models. For example, the experiments convinced me that Transformers are better than LSTMs at recalling verbatim context (which makes sense, given the way self attention works), while LSTMs only maintain a gist of the context that they ve processed. However (thinking about practical applications, which admittedly may be beyond the scope of this work), it s not clear to me when "verbatim recall" is necessary vs. it s better to have a semantic gist of context. Perhaps some more discussion about this would be useful. The work raises several interesting conclusions about LSTMs and Transformers, and they are well supported by the experiments. Overall, I thought the paper was interesting and well executed, if a bit narrow, and would be an interesting paper to the ICLR audience.<|endoftext|>They train several models (an LSTM and several transformer variants, including GPT 2) with the language modeling objective, and then measure the median LM loss of each model across all the words on list2. Further analysis studies the factors that influence this memorization, including the length of the list, the length of the infix, and the depth of the model. This paper studies a very important question: to what extent can NLP models retrieve past input? Overall these results will be of interest to the ICLR community. They then use the negative result to say (or at least hint) something general about LSTMs. There are other confounding factors that make it hard to compare against the two model, e.g., the tokenization (the LSTM uses words, while the transformer uses BPEs). 2.How deep is the transform that was used in the main experiment? References:[1] https://arxiv.org/abs/1805.11653Pros: A very important research question, a nice experimental setup, interesting results regarding GPT 2. Cons:Potentially inaccurate conclusions regarding the connection between LSTMs and transformers due to various confounding factors.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; In this paper, the authors propose a LoRA a lightweight fine tuning method for language models (LMs). LoRA achieves similar or better performance than full model fine tuning while adding only 0.01 0.05% of the original parameters and freezing the original weight of the LM, it is efficient in training and it does not add further computational cost during inference, as required by other methods. The central idea is to only learn a parallel set of low ranked parameters (Figure 1) rather than fine tuning the entire model. Moreover, LoRA does not add any inference overhead, as shown in Table 1, compared to other methods (e.g, Adapter), which required further computation. The paper is clearly motivated and covers a large amount of previous work. This method is efficient both in training and inference, and achieve comparable or better performance than full model fine tuning.<|endoftext|>This paper proposes a low rank adaptation method LoRA for application of pretrained models for downstream tasks. The proposed method is simple and effective in adapting pretrained models to downstream tasks, which could become one of the major solutions among the finetuning, adapters and promp tuning. This paper is well motivated and easy to follow. Moreover, with regard to training efficiency, it is better to compare with adapters and prompt/prefix tuning with the same amount of trainable parameters. This work proposes to add parameters of low rank matrices into pretrained model, which are tuned for specific downstream task. My overall recommendation to this paper is accept.<|endoftext|>The paper proposes a low rank adaptation that compresses the pre trained model weights by applying matrix decomposition (e.g., SVD) into large pre trained models. The factorized pre trained weights are frozen, and the small adaptation parameters are trainable during the finetuning stage. This method is very useful in the practical setting. Strengths:  Simple and efficient methods to utilize pre trained models such as GPT 3 for model adaptation  Good performance and comparison with fine tuning baselines. Weaknesses:  The idea is incremental. The approach is incremental, very similar to Adapters; adding small trainable parameters to the model.<|endoftext|>This paper presents LoRA, a method aiming to improve the efficiency of fine tuning large language models. Specifically, it fixes the underlying pretrained model, and learn low rank (by construction) parameter matrices to update the model. In this way, the amount of parameters to finetune is significantly reduced if the rank is small. One missing baseline is a full rank finetuning model, which only updates W_q and W_v. The memory/parameter efficiency of LoRA is exaggerated.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper formalizes the notion of an in context bias the pre trained neural language models are better at modeling dependencies that appear within one contiguous pre training text chunk, rather than those that appear in different pre training text chunks. The authors perform a theoretical analysis of this phenomenon and link the inability to model dependencies between different examples with the models  low pretraining learning rate. It wasn t intuitive for me that it d be useful for NQ. Strengths:  Thorough theoretical analysis that reveals the connection between (practically necessary) small learning rates and inability to use dependencies across text chunks  Useful framing and discussion of the "in context bias", where models are more likely to learn dependencies within text chunks seen during pre training.<|endoftext|>The paper presents a theoretical analysis of the strengths of including similar input examples S1 and S2 into the same actual transformer input. The theoretical analysis is supplemented by empirical results. The paper then proves empirically that including related examples in the same input (via a method the article calls KNN pretraining) allows transformers to learn much faster in cases when cross document dependency is relevant. Doing both is not.<|endoftext|>The empirical section sounded disparate from the theoretical part of the paper, and the results are weak. It does not follow from the theoretical results that adding similar sentences will be a good thing. As a result, having these in the same paper looked incoherent to me. It will be good to rewrite the section clarifying the contributions and novel conclusions from the analysis.<|endoftext|>Specifically, the text is continuous during pretraining while could be non neighboring in downstream tasks. 3.A theoretical analysis of the in context bias. The introduction of the motivation (the concept of in context bias) is not easy to understand at the very beginning. The paper addresses the inductive bias of in context learning. The studied problem is meaningful and could be a potential breakthrough for pretrained language model.<|endoftext|>The main results of this paper are of broad interest. The claim that the within  and between example is crucial is novel and thought provoking. Another problem is that a language model trained on this kind of interleaved text would presumably also generate less coherent texts. If so, then it seems that models tested in this paper are just too small or ineffective for the task. Is there an easier task where this method leads to strong performance but ordinary pretraining does not? These kinds of arguments are valuable.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a prime dual network to extract discriminative relationships between support and query examples of the multi classes with few examples. While the prime network predicts labels of a query out of the support set, the dual network reverses the process and infer the category labels of the support set using query examples. For training, a weight self supervised loss contains the sum of dual and prime networks and is evaluated on inner domain and cross domain cases. Strengths: The paper is well written and the classification improvements seem to be significant. Weekness:   While the training stage is a stochastic process and dual network is the reverse of the prime of in mapping support to the query, I was wondering how both of the networks wouldn’t be the copy of each other at the end of the training? How we can be sure that the accuracy gain of the model is not obtained by an extra learning module. I was wondering why 224x224 images are used while miniImageNet dataset contains 84x84 images? The idea of the paper looks interesting, but I have some concerns about the motivation of the method and some of the evaluations of the work are missing.<|endoftext|>The paper proposed a self supervised learning framework and a dual network to improve the performance of few shot learning. The dual network is based on GNN architectures and learn the relationship between the features from the support set and query set. Experiment shows improvement over the baselines on several few shot learning benchmarks. The idea of self supervision is interesting. For example, the prediction L in equation (6)   (7) are not consistent equation (11)   (14). And i would suggest use other notation because of the confusion of loss function. For figure 2, what is the difference between two types of samples with labels? what is the difference between two types of samples without labels? Although I can understand through the text but the figure can be improved a bit. The ablation study, the authors divide it into two parts, i.e., self supervised optimization and self supervised learning. I do not understand why these two can be divided? Are there any details for how to do that? In this way, is there any feature representation visualization for the learned model please? I am interested in how the representation would change by using the self supervised learning. The idea is interesting and seems to perform well.<|endoftext|>The paper proposes a prime dual network for few shot image classification, where the support set and query set are used in a flip manner so that the labeled samples in support set can be compared with themselves to determine a loss for network update. STREHGTHThe proposed method is easy to follow and understand, and the experiment results are promising when directly compared with state of that art methods. This method reminds me of the cycle consistency loss in the well known CycleGAN approach, but since CycleGAN is not referenced and discussed in the paper, I was wondering if I misunderstood that or is there a reason for that? If that s the case, it would be desired to discuss the connection to prior work for better justification. 3.Following 2, if SSO is more essential for the performance gain, is it fair comparison with other SOTA results? In other words, did other methods go through an optimization stage on unseen testing classes, or do some of them in fact not rely on updates during testing? The overall approach makes sense to me, but I m not entirely sure how does the proposed approach compare with existing work, in terms of methodology as well as experiment settings, so I ll need the questions above to be addressed before making a most informed decision.<|endoftext|>The authors design a prime dual architecture for few shot learning to characterize the inherent relationship between the support set and the query set and introduce a self supervision constraint for performance improvement. In particular, it proposes to correct query sample labels between the prime network and dual network and design an optimized prediction for the query samples with a selection mechanism. Extensive experiments are conducted on datasets with the ablation analysis, and the method achieves favorable results over existing algorithms. The paper tackles one of the important issues of meta/few shot learning: ground truth is not available for test samples. The issue is important and very practical in my opinion. 2.The proposed method utilizes the trained dual network to predict the class labels of support samples, which is kind of the concept of the autoencoder technique, simple and promising. The idea of constructing and learning the relationship between support set and query set is reasonable and interesting. 3.In this paper, the authors provide comprehensive experiments, including the comparison of the state of the art methods in public benchmarks, the ablation analysis, and how the parameters affect the performance such as the searched position number, etc. For example, Table 6 shows the number of the proposed SSO that can help mis correct the query labels. The analysis of ablation and SSO modules provide evidence and make it convincing. This may not utilize the property from a bipartite graph. 2.As the paper mentioned, the prime and dual networks share the same network design, but I d like to know whether the authors have used different network designs for prime and dual. What would be the pros and cons with the same/different designs? For example, why the proposed method has less gain in Cars compared to 1 shot and 5 shot, and why does only CUB increase the performance gain when it s 5 way 5 shot? Overall the paper is clear to understand, and it s a simple idea but effective approach. In this paper, most of the experiments provide the discussion and are convincing, except for a more detailed discussion in the cross domain FSL result.<|endoftext|>[2] The proposed method achieves considerable performance gains compared with other state of the art methods. The author conducted experiments on intra domain classification and inter domain classification tasks. ** The novelty of this paper is to design a dual network to make bi directional predictions. However, the motivation is not clearly stated in the paper. In the testing stage, the authors only use the support samples to make label predictions in the query set, so why the consistency loss can improve the performance in the few shot learning? I suggest the author visualize the feature distributions of samples in the support set and the query set to explain how the consistency loss influences the feature distributions. [2] **About the model design. When testing, how does the author use the dual networks? The author should carefully compare their method with these papers. Does this paper share similar ideas? I would like to remind that when message passing in GNN, the information also transforms from the query set to the suppert set. (ii) This paper should add semi supervised learning settings in EGNN and DPGN to show their effectiveness. Distribution propagation graph network for few shot learning, CVPR20C. Mutual crf rnn for few shot learning, CVPR21[4] It seems that this work is more effective for cross domain few shot classification than intra domain few shot classification. The author should explain why their methods are effective as I fail to find any specific design for cross domain classification. I think it is a bit unfair to compare their methods with other methods in Table 1. Although the paper achieves promising results, I have some concerns about motivation, model design, and experimental comparisons with other GNN based methods, as listed in Main Review.
Reject; rating score: 5; rating score: 5; rating score: 8; This paper focuses on transferability measures both in a supervised and unsupervised context. In particular, the authors propose a shrinkage based estimation of H score in order to correct its instability and discuss the limitations of the other approaches on two different scenarios: source model selection or target task selection. **Strenghs**+ The approach proposed in the paper is interesting. In particular, the idea of studying deeply an existing measure rather than proposing yet another new one is a good idea. In particular, the paper builds on robust knowledge in statistics and shrinkage operators to estimate high dimensional covariance. + Experimental results are promising. In particular, section 3.1 is very dense and some choices could be better motivated, such as instance the choice of using a linear operation on the eigenvalues( equation 3), choice of alpha (equation 5) in terms of their impact on the transferability criteria. For instance on text transfer learning tasks?<|endoftext|>This paper is interested in task transferability measures (both in the supervised and the unsupervised case). The 4 main contributions of this paper are:  Highlighting instabilities in the computation of the H score which leads to poor estimate, and proposing to correct these instabilities with regularized covariance estimations, demonstrating an 80% improvement over the original H score  Fast implementation that is 3 55 times faster than another state of the art baseline  Identify problems with other existing transferability measures (NCE, LEEP and N LEEP) and propose to measure correlation against relative target accuracy (instead of vanilla accuracy). For instance, the hypothesis that the sub optimal performance of the H score for measuring transferability in many of the evaluation cases is due to a lack of robustness in estimating the H score (page 3) is not empirically demonstrated. How does shrinkage based H score compare to the original H score in terms of speed? Could it be integrated into another section instead?<|endoftext|>The authors propose a new shrinkage based estimation of H score and provides a fast implementation for even high dimensional feature embeddings. The paper has a clear transferability setup that makes intuitive senses. 2.The paper improves the state of the arts H score and some newer metrics by studying both supervised and unsupervised TMs. The paper only considers the image classification and test on this problem while largely ignoring other tasks. The paper is a good paper in my opinion, it tries to address an important problem question in transfer learning: a good transferability metric. It addresses some issues of existent metrics and proposes a fast implementation of the proposed metric.
Reject; rating score: 5; rating score: 6; rating score: 6; This paper presents a transfer learning strategy for improving compositional generalization of semantic parsers based on pre trained language models. Before fine tuning the model on data from the target domain, the authors propose a pre finetuning step, where models are trained on compositional splits of data from another source domain, with the goal to transfer the model s learned knowledge about language compositionality during this pre finetuning step to the final learning stage on the target domain, therefore improving compositional generalization. While a like this idea, there are several issues with the proposal approach:1. These works are not mentioned in Section 5. The authors could present more analysis in terms of the language and compositionality styles of those datasets in order to have a better understanding of the upper bound performance of transfer learning approaches for compositional generalization. The results on GEO_{TMCD} outperforms the currently best approach.<|endoftext|>This paper is focused on the problem of compositional generalization in semantic parsing, and introduces a method called "DUEL", which involves "pre finetuning" iteratively on compositional train test splits from other datasets, before transferring to fine tuning on the training data from the target dataset. In general I found the method interesting and the paper overall clear. So I would like to see the authors address this concern. After this "pre finetuning", the model is fine tuned on the training data from the target dataset. Finally, I m not totally sure how surprised/impressed we should be by improvements from this method.<|endoftext|>Update after the response: Thanks to the authors for their thorough response to my comments! The approach relies on pre finetuning: training a model on a different dataset than the target dataset that also the requires the same sort of compositional generalization as the target dataset, before then training on the training set of the target dataset and then evaluating zero shot on the compositional set of the target dataset, in the standard way. S4) I found the demonstration of the benefits of pre finetuning interesting and convincing. The COGS_cg lexical challenge seems to mostly be obviated by pre trained representations. W2) It s not totally clear to me why the method should enable compositional generalization in general, and I feel like it would help to strengthen the motivation and intuition for the method, or perhaps do some some analysis could be done to indicate why it s working (where it is). It seems like the meta learning approach (which directly trains for compositional generalization) that other work has employed is more directly suited to this, or even perhaps some adversarial training approach if the paper is aiming to learn representations f(x) that encode invariances across s and ~s. Why reinitialize the parameters of g in fine tuning? However, since the improvements are consistent (albeit small), I don t think this is a crucial weakness. How is this value chosen? But, it does seem to show consistent (if sometimes small) improvements on a couple models and several datasets, the methodology seems sound, and the paper is very clearly written.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors show that deep convolutional networks restructure the eigenspaces of the inducing kernels,which empowers them to learn a dramatically broader class of functions, covering a wide rangeof space frequency combinations. The novelty is in its use in neural network architecture analysis. The paper evaluates the performance of deep neural networks by performing extensive experiments that show the tradeoff between space versus frequency. However, eigenspace restructuring is a well known concept and the novelty of the theorem is limited. It is not clear if there are any novel insights.<|endoftext|>The paper shows how the topological of convolutional neural network restructures the eigenspace of the neural kernel. The current score is mainly due to the issues related to clarity. The theorem statements are simple and clear. However, it is hard to decipher what is happening with just the theorem statements alone. * Is there an example of $n(\mathbf{r})$ without a common ancestor $u$ such that $\phi_u$ is admissible? Same goes with the Figure 4 (Figure Zoo) in the appendix. It is not clear what is meant by space and the space frequency trade off?<|endoftext|>The two main quantities are the frequency index (FI) and the spatial index (SI) which are calculated from the DAGs. The authors claim that these two quantities capture what type of functions (in terms of frequency and space) can be learned by specific architectures: i.e., MLPs learn LRLF functions, s CNNs learn SRHF functions, and so on. Weaknesses: The paper is very hard to follow partially because it is non standard. Figures need a much better explanation. But also, my lack of familiarity with the tools used in the paper (i.e., graph computations) may have made it difficult to read for me. Therefore, I can not recommend acceptance.<|endoftext|>This paper provides new insights to understanding the mechanism behind neural networks, thanks to a space frequency through the so called eigenspace restructuring. This is also the case about missing analysis on the influence of the training dataset. There are some spelling and grammatical errors in the paper, such as “An demonstration”, “It describe a”, “our theorem justify”, “spherial harmonics”... RebuttalThe authors reply is off the mark, as they did not tackle our concerns, neither address comments from the other reviews. After reading all the reviews and the authors  replies, I have downgraded my score. The paper provides solid foundations to understand the mechanism behind (deep) neural networks.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper discusses a new framework named FITVID to handle the problem of video prediction. However, the reviewer does have some concerns on the paper. (1).A major concern of the paper is about the model s novelty. (3).Although the authors provide experimental results on 4 datasets. It will be more convincing if the authors can provide results on future and counterfactual predictions of the video dynamics on the CLEVRER[A] dataset and compare the proposed model  performance with recent stronger backbones like transformer [B], GNN[C] and Differentiable Physics Engine [D].<|endoftext|>The paper proposes an architecture that is claimed capable of using parameters more efficiently in order to overfit. It is a weird claim about video prediction as the unseen future itself is output. Strength:+ Scale is an important aspect of the system. Some of my concerns are resolved (point 3,5,6). + Though the generalization ability remains further exploration, a robot experiment based on MPC shows that their prediction model can be deployed in downstreaming tasks. I think it was there by the time I reviewed but not 100% sure. Why is theirs better? For point 1,2, I appreciate the extra ablations by the authors. But this work seems brute force generates pixels with skip layers. 1) can table 1 include results for BAIR? 2) can table 2 include other datasets? 3) can other metrics be included?<|endoftext|>The paper proposes a simple and scalable variational video prediction model FitVid, which attains a better fit to video prediction datasets even with a similar parameter count as prior models. The paper did not analyze the effectiveness for each part of the FitVid. The explicit explanation of this problem is important and needs to be explored.<|endoftext|>This paper presents a new network architecture called FitVid to perform the task of video prediction, i.e.the task of predicting future frames from previous frames. As a result, FitVid achieves state of the art on four challenging video datasets across a wide range of metrics. Why?More discussion should be devoted to this topic. It seems miraculous to me that the model is able to figure out the random objects in the background and their appearance after a few frames. The main concerns revolve around a lack of novelty. The authors rebut by providing a detailed study of the proposed components.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The loss is augmented with a term that encourages units to have a more diversity in its activations. Strengths:* Theoretically motivated and simple (in a good way) technique to encourage activation diversity. To provide a more direct link between the theory, loss term, and empirical results, the paper could be improved by visualizing the activation diversity across layers   given that the diversity term is only applied in one layer, what is its broader effect? It should be noted that the DeCov results are not cited from the original paper, but the authors own implementations (because DeCov was only evaluated on AlexNet at the time). * given that DeCov results are not from the original paper, how were its hyperparameters tuned? e.g.in Table 7 in the Appendix, I see tuning of the hyperparameters for the author s method, but not for DeCov. The proposed method is appealingly simple, but yields mixed results.<|endoftext|>This paper proposes to use a regularizer to encourage the diversity of activations within each layer of deep neural networks. Second, the generalization bound seems to be very loose upon inspecting the constants: for example, $C_5$ in Eq.(8) is on the order of $C_1C_3$, with $C_1$ being the max norm of the input and $C_3$ being the max norm of the weight of the hidden layer. 5.Computationally, the proposed regularizer seems expensive. Overall, this is well structured work with both theoretical analyses and empirical results. Therefore, I am not recommending the acceptance of this paper.<|endoftext|>Loosely inspired by the bound, the authors then propose an optimization algorithm with a particular regularizer that encourages within layer activation value diversity. The empirical results suggest that the proposed algorithm boosts the performance in most regimes studied (CIFAR 10, CIFAR100 trained on ResNets, Dense nets). Why not state a generalization bound instead? *The purpose of the bound*The authors derive an upper bound on Rademacher complexity.<|endoftext|>In this paper, the authors propose a regularization technique encourage the "activation diversity". They also showed that encouraging the within layer diversity can be used to control the Rademacher complexity of model. In general, what is the impact of the weights on the diversity? A Note regarding the references:In many places in the manuscript, the citations do not match with the content. VC dimension is not from those papers you cited, and the same for Rademcher complexity.<|endoftext|>The paper proposes an interesting angle on the generalization of neural networks. Through proving a new generalization bound that has an internal distance term, the authors argue the neural networks’ generalization performance is related to the internal diversity of the neurons within each layer. As such, some regularization terms are introduced to encourage the internal ``diversity” within each layer. My review on this paper is mixed. The main concern is on the theory side. This needs to be explicitly stated. It has some interesting thoughts and empirical results with come concerns on the theoretical justifications.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; While this paper makes some interesting contribution of a new stochastic algorithm for deep learning, it appears to be far from a complete work. I think the paper needs to be substantially improved in both theory and empirical study.<|endoftext|>This paper presents a model building approach that replaces the one step backtracking in stochastic line search methods. }$ The proposed method is not adaptive in terms of step size! The text needs to be more clear and readable. Quasi newton methods for deep learning: Forget the past, just sample.<|endoftext|>The paper is an incremental work and is mainly a stochastic variant of Öztoprak et al 2018. However, in the paper, it is not clarified why partitioning is useful. However, to set \eta their proposed algorithm needs backtracking. 2   Generally tuning both direction and step size is not a new approach.<|endoftext|>This would not fit into the analysis of Wang et al.(2017) as their theory requires $H_k$ to be independent of $\nabla f(x_k,\xi_k)$ conditioned on the past. The algorithm in Wang et al.(2017) constructs $H_k$  based on a truncated history of the past stochastic gradients and therefore is independent of the randomness in iteration $k$. This is particularly suitable for implementation in popular deep learning libraries like PyTorch where parameters are stored in groups.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; In this submission, the authors proposed a framework for few shot font generation where the key components are content and style glyph attention modules. The proposed framework is essentially a transformer variant. Although this work is probably the first to apply a Transformer like model on few shot font generation, there exist works that have attempted to apply in closely related tasks, like style transfer [1]. 4.Experimental details are not clear. However, its technical novelty is limited, evaluation is not sufficient, and the discussion is not thorough.<|endoftext|>This paper considers Chinese glyph font style transfer problem with few reference inputs. The main novelty comes with the glyph attention design with both local and global structure for style and content. Strengths:+ the paper is well written and easy to follow in most parts+ the design of glyph attention component fits with problem well+ experiment results are comprehensiveWeaknesses:  I am confused why there is a query and a separate content set. They all have the same content, and there seems to be no requirement on their styles. Then why not treat them in the same way? It is very likely there are some training fonts very similar to the test ones. I think this is basically a self attention model. I like the design of the glyph attention part of the model.<|endoftext|>This paper proposed a few shot font generation method, GANet. The multi task adversarial loss was also employed to further improve the synthesizing performance. Experiments conducted on a dataset constructed by the authors verified the effectiveness of the proposed method in handling the task of few shot Chinese font synthesis. 5  In Page 5: “font does not have texture and color, just consists of some white and black strokes.”  > ? The utilization of style glyph attention and content glyph attention seems reasonable for few shot font generation. Therefore, it is important to show the nearest neighbors of synthesized glyph images in the training dataset to demonstrate that the model is still capable of handling unseen and special font styles.<|endoftext|>The paper proposes a glyph attention network for few shot font generation. The motivation is clear and reasonable but there are still some major issues in model construction and the experiment. 2.More evaluation metrics for image generation should be used, such as FID、LPIPS. 5.The writing is not clear. Overall, the paper’s motivation is clear and reasonable. However, there are still some issues on modeling and experiments that need to be clarified.
Reject; rating score: 1; rating score: 1; rating score: 3; This paper proposes a model combining convolutional operation and recurrent operation along with temporal attention for EEG classification. To me, the challenges, motivation, solutions, or contributions are not clearly expressed. 2.Lack of novelty. In specific, the CNN, RNN, excitation and squeeze, and FUlly connected layers are very commonly used and intensively studied components in the last 5 years. I am also confused that what is exact the baseline mentioned in Table 1?<|endoftext|>The model is a convolutional RNN with temporal attention. It is the opinion of the reviewer that following are the strengths and weaknesses of the paper. Strengths:  The paper is generally well written and structured. Therefore, it is very unclear how the method generalizes to other datasets. The model details are not all give, and so the work is not reproducible.<|endoftext|>The authors introduce a deep learning approach for short time motor imagery classification using EEG data. Conventional CNN and RNN   GRU layers are used. Remarkably, a data augmentation strategy and a class activation mapping approach are presented. Besides, the authors claim that short time interval EEG classification is achieved; however, 0.8s windows size does not seem to be a "short interval" compared to other state of the art methods. A class activation mapping approach for EEG data is presented. In addition, the paper presentation needs to be enhanced.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This work proposes Fixed Neural Network Steganography (FNNS), an algorithm for steganography leveraging adversarial attacks to hide a secret message in an image. # Strong Points* FNNS constitute a novel approach in the field of steganography leveraging adversarial attack techniques to hide a secret message in an image. * The paper is well written and easy to follow* The presented application of face anonymization is interesting. # Weak Points* I am wondering about the computation time of the proposed approach? Did the authors experiment with other attack algorithms such as PGD, DeepFool, or C&W? * The source code is not provided? Will the code be publicly available? In my opinion, FNNS can provide a contribution to the steganography community. The authors also show an interesting use case of FNNS.<|endoftext|>This paper proposes a method for image steganography based on adversarial attacks. Unlike previous works such as SteganoGAN, the proposed method fixes the decoder and performs an inference time attack to produce a perturbation which maps to the correct message. Empirical results show the proposed method out performs existing encoder decoder based design, and is also more robust in out of domain applications. The paper is written clearly and easy to follow. The parts regarding JPEG compression can be expanded. For example, (a) what are the JPEG compression quality factors used for the experiment? (b) It would be interesting to combine the approach with JPEG resistant adversarial attacks. But it s understandable if this is outside the current scope of the paper. Overall I enjoyed reading this work.<|endoftext|>The paper proposes a novel neural image steganography method. The fundamental idea is that the sensitivity of neural networks to adversarial perturbations can be leveraged to encode an arbitrary hidden message into imperceptible perturbations. ## After author rebuttalThe authors have addressed my concerns satisfactorily, and I believe that the final version of the paper will be a strong contribution to the literature. The authors  method heavily outperforms current classical and neural image steganography methods. However, the article has some weaknesses that if the authors can address I will be happy to raise my score. The proposed application of hiding images is also very interesting. On the whole, the paper is easy to follow and is generally well written with only a few mistakes. What bit strings were encoded for the experimental results? ## Questions     As I understand, the two sources of decoding errors are clipping the difference (line 6 in Alg 1) and clipping the addition of the delta (line 7 in Alg 1).<|endoftext|>This submission deals with steganographic paradigm : hiding a secret message in a cover content while remaining undetectable. They use a neural network decoder and exploit adversarial perturbations to compute the stego version of the cover image. The authors use ref. Stricto sensu, these papers do hide information in a cover content and this secret message is typically not visible to the human eye. (2018).Deep residual network for steganalysis of digital images. As a baseline, I think the authors should at least compare themselves to UNIWARD [Holub et al.2014] although there are now more secure steganography schemes in the mainstream steganography literature. While section 6 is somewhat interesting, I think this use case is quite narrow. The security relies on 0% error + encryption but many other concurrent approaches can do that relying on the cover produced by the face GAN. So why the proposed approach should be used ? The usual experimental setup is to provide a curve showing the detection rate as a function of bpp payload. So this must be an average bpp. StegoGAN is not SOTA steganography.. it might be ML based SOTA steganography.
Reject; rating score: 1; rating score: 5; rating score: 8; The authors proposed a matrix factorization based method for deep neural network weight compression. The article is erroneous and the results are misleading. Please refrain from misleading the readers. I did not read the paper after Section 3.1.1. This work is relevant and similar to the present work and requires mentioning. The weight tensors are factorized as two low precision quantized matrices, out of which one is sparse. Is it the number of observations in the training set? In the present form, it is misleading, and the treatment and evolution of the problem throughout the draft are highly faulty. ** Proposition A.1 is not correct. The projection is not on a convex set; projection onto the set of sparse matrices (by using hard threshold operation) is a nonconvex projection. Although the authors applied the projected gradient descent technique to nonconvex optimization, the convergence proof techniques used in the convex case do not apply directly to nonconvex problems. Please note that the claim that the authors made about the stationary point is erroneous. For (local linear) convergence of gradient descent with nonconvex projection, please see [1,2,3]. Therefore, “SVD decomposition” does not make any sense.<|endoftext|>This paper introduces a novel method of weight compression. Weight tensors are stored as sparse, quantized matrix factors, and the underlying matrix factorization problem can be considered as a quantized sparse PCA problem and be solved through iterative projected gradient descent methods. The method seems to be a simple combination of tensor factorization and vector quantization methods. Therefore, the novelty in this submission seems to be quite limited. In addition, the authors should clarify that Z is sparse in what sense? 5.All source codes required for conducting experiments should be included in a code appendix. The terminology "quantized sparse PCA" should be fixed, and the authors should not also use "sparse quantized PCA" arbitrarily (e.g., in the second dot point of the contributions). In Appendix A, problem 14 should be problem (14). Although the method proposed by the authors seems to be empirically promising, it seems to be a direct combination of existing methods, and the novelty seems to be limited. The authors provide no theoretical guarantee/intuition for their method. In addition, some terminologies/notations are vague or confusing.<|endoftext|>An algorithm is presented, with two different thresholding options (per iteration, or one shot). In experiments, these ideas lead to an accuracy/compression tradeoff which is either competitive with or better than previous state of the art across all compression ratios investigated. The paper is clearly written and the proposed method and results are convincing. Overall, a nice paper that I think is suitable for publication in ICLR. The method is simple yet seems to work well. S3.Although I’m not familiar with the literature in this area, the authors seem to do a good job of discussing related work (in Sec.2) and later on also discussing how their proposal relates to other previous methods (in Sec.3.1.2).# WeaknessesW1. I mentioned the simplicity of the method as a strength. However, the simplicity of the method comes from the fact that it just combines several previous ideas (PCA, sparsity, quantization) into a new method, which could be seen as incremental/less exciting. You should mention this in the paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper distinguishes between representations for "small" and "large" molecules based on a metric that takes the spatial extension and number of atoms in the molecule into account. Structures of small molecules are represented by an interatomic distance map. Large molecules are represented by what the authors refer to as a "sinusoidal function based absolute position encoding method". In both cases, a transformer architecture is used on top of this initial representation. The authors further introduce a subsampling procedure to select a subset of points/ atoms and aggregate information. The authors demonstrate quantitative results on datasets relating to 1) small molecule property prediction (QM datasets), 2) protein ligand binding (PDBbind dataset), and 3) a dataset from material science on metalorganic compounds (CoRE MOF dataset) The question of how to efficiently represent and learn from 3D atomic structure has attracted a lot of interest in recent years due to a wide range of promising applications. The methodological novelty in terms of molecule representation is limited, the distance map representation for small molecules and the sinusoidal function based absolute position encoding method (Li et al., 2019) for larger molecules have previously been established. My recommendation is based on the 1) methods  representation with respect to prior work and 2) limited technical novelty.<|endoftext|>The authors develop a multi scale attention module, an adaptive position encoding module, and a farthest sampling scheme for the 3D Transformer. These are all built upon the design of PointNet++(Qi et al., 2017b) without significant modifications. Without these comparisons that clearly show the difference for each module, the proposed "3D Transformer" is very similar to an attention based PointNet++ that is specially designed for molecular data. I recommend the authors carefully polish the writing. Overall, there are several components proposed in this paper.<|endoftext|>The authors target the molecular property prediction problem and propose the 3D transformer to utilize 3D geometric information of molecules and take advantage of the advanced (graph) transformer architecture. Specifically, the authors propose three key components of the 3D transformer, namely, the adaptive position encoding, the multi scale self attention, and the attentive farthest point sampling. The paper studies an emerging problem of learning representations from 3D structure of molecules. Several interesting methods taking advantage of the molecular graph transformers and molecule 3D information are proposed. However, there are several concerns and questions regarding the methods and presentation. I suggest the authors examine the writing again and make the presentation more clear. 2.The motivation of absolute position embedding for large molecules is reasonable to me, i.e., it aims to distinguish chiral molecules. Besides, could the authors explain why chirality is not that important for small molecules? It would be better the authors also provide the learned/tuned threshold values for each dataset or other results to support the claim. The selection of the subset is based on the representation of each node, and the optimization of node representations requires gradient back propagated through the node. in the appendix for better reproducibility and a deeper understanding of the proposed methods.<|endoftext|>In the paper, the authors proposed a new Transformer dedicated to working with molecular representation. It is a modification of classical Transformer using the information of 3D coordinates of atoms. In the paper, the authors proposed 3D Transformer. Therefore, I think that authors should use classification tasks from [1,2] or from (cited in the article) “Self Supervised Graph Transformer on Large Scale Molecular Data,” where authors use BBBP, SIDER, ClinTox, BACE, Tox21, ToxCast datasets. The paper proposes an interesting modification of transformer architecture dedicated to the chemical compound. Furthermore, experiments are applied only on classification tasks.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The authors propose a "reversible instance normalization" as an input pre and post processing procedure to improve the forecasting of any given base model   targeted at addressing the distribution shift that is common in time series data   e.g., time series are typically non stationary. The normalization is done by subtracting the mean and dividing by the std. My main hesitance to accept the paper is the lack of novelty   as sliding window normalization is already commonly used, and the method is a very minor change. I.e., "sliding window normalization" is commonly used in practice for time series forecasting and has been used in the literature on time series forecasting as well. Overall, I feel the set of results are impressive and useful. I.e., in each of the experiments, if the learnable parameters are excluded what is the impact? I agree with the authors that it s useful to bring the approach to the attention of the community (as there are some similar approaches used in some communities but not well known or used in this community), and also the motivation and perspective comes from very different places compared to the similar approaches, and the idea of how it affects the data distribution and showing results along that direction is also new compared to the prior similar approaches.<|endoftext|>This work proposes to use a normalization method to address temporal distribution shift in time series forecasting. The proposed approach, *RevIN*, consists of two steps: instance normalization on input sequences and "de normalization" of output sequences by re using statistics (mean and variance) computed during the normalization step. Experiments are conducted on two time series datasets (ETT and ECL) with varying splits and prediction windows lengths. **Strengths:**  Addressing distribution shift is a relevant task to the ICLR community and of crucial importance to deploy deep learning based model time series forecasting;  Despite being simple, the proposed approach is technically sound, easy to implement, and empirically validated on various datasets, split and prediction windows length;  The experimental evaluation is thorough and allow one s to evaluate the importance of the type of normalization and the effect de normalization module. **Post rebuttal comments**The authors have addressed my concerns and engaged with other reviewers concerns. Even though the proposed approach is straightforward, the idea behind RevIN is technically sound and the thorough experimental evaluation is conclusive. While instance normalization is a simple and well known method in the distribution shift literature, its application for time series forecasting is sound and empirically validated in this paper.<|endoftext|>The paper proposes RevIN that performs (a) instance normalization on input time series and (b) reversible instance normalization on output time series using normalization statistics of the input. By doing this, the paper claims to handle distribution shift in time series and hence yield better forecasting results. Strengths:The idea behind RevIN is straightforward but seems to work well, outperforming min max normalization, batch normalization, layer normalization and instance normalization. Weaknesses:W1: The method seems ad hoc with no theoretical justification. The paper is easy to follow as the main idea is straightforward. A proof for a simple case would make the paper more convincing.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The approach needs to improve the baseline performance of the IsoMax and IsoMax+ losses when combined with them to be valuable. In other words, it proposes an "entropy maximization regularizer." The authors could be a bit more explicit about the fact the proposed approach requires increased training times because we need to add augmented data to the training process. Increasing entropy to improve OOD detection performance is not very novel. ############################################################################################################################################################################################################################################################################################################################################################################The proposed method is interesting, but the novelty is regular as it is similar to Mixup and the performance is not too impressive when we also consider ResNet50 results. As we said before, this is not our main concern regarding the paper. We partially agree with the authors in this case. This is not our main concern regarding the paper. We did not find results for the other methods for the new OOD data. **We complained about this in our review. However, the authors did not answer. One final reason to include IsoMax and IsoMax+ in the experiments is the fact that they are distance based losses, which the community is starting to agree is the best approach for OOD detection.<|endoftext|>The objective of the method is to produce better deterministic uncertainty estimates without compromising accuracy. 2.The method can be applied without modifying the network architecture, and it seems to perform well without being sensitive to HP tuning. In the context of classifier calibration, the comparison to Mixup seems to be limited given the similarity between the methods and the calibration results in the paper. Energy based out of distributiondetection, 2020 ]. The authors also do not provide a sufficient explanation to avoid comparison to Mukhoti, Jishnu, et al."Calibrating deep neural networks using focal loss 2020. Moreover, some important results are deferred to the appendix and presented in an inconvinient way and with limited discussion (e.g., missing a summary for ablation study results, appendix G. tables are missing baseline results etc.).<|endoftext|>This paper proposes a simple approach called mix maxent to improve the robustness of neural networks. Experiment shows the good empirical results. The proposed algorithm can be easily used to modify the training. 1.Seems in the compared baselines, mixup tends to be the strongest one, which is quite surprising as mixup is not an algorithm specially designed for robustness and uncertainty. Overall, the main advantage is the simpleness but my concern on the significance of the result making me feel this paper is slightly under the acceptance bar.<|endoftext|>The author proposed a new method for improving uncertainty estimation in deep neural network output. The proposed technique is simple and can be easily integrated into other NN architecture. The performance of the proposed model is competitive compared to the baselines. Weakness:I have a few comments, concerns, and questions regarding the paper:1) The authors start the paper with an analysis of some of the observations they found on CIFAR and SVHN data. How much of these observations are applicable to other datasets? However, most of the OOD samples should lie in the area that is not in between two clusters (at least based on the areas). However, the author did not properly compare the model with the DUQ (with code base as the reason). The paper proposes an interesting approach in uncertainty estimation for deep neural networks. The paper has some weaknesses that I hope the author can fix it soon.<|endoftext|>This paper presents a new penalty function that can be augmented to the common cross entropy loss, which was shown to improve the accuracy and calibration of deep net classifiers, applied to in distribution data. The proposed regularizer is tightly connected to mixup. In a series of experiments, the authors show the advantage of this method over baseline approaches. Also, a table that summarizes the statistics of interest (evaluated on the entire test data) would be informative. It will be great to explore other cases for which detecting OOD samples is more challenging. In addition, it will be illuminating to compare the proposed method to existing techniques, such as [2]. The paper offers an interesting approach to improve robustness to domain shift and better detect OOD data. The major concern I have is about the novelty of the proposal and its similarity to mixup, especially because the latter was already studied in this context (calibration, OOD, and adversarial robustness).
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The paper proposed to represent prototypes by hyperspheres with dynamic sizes. A so called big prototype is characterized by the center of the hypersphere and the radius of the sphere. However, there are many works trying to improve prototypical network to model the prototypes more appropriately in literature. Why using hyperspheres  is better than using a distribution? Especially, please note that the hypersphere is still regular, while a distribution can be complicated. The paper lacks  thorough literature review. After rebuttalThanks for the new empirical results.<|endoftext|>The paper proposes to use areas to model prototypes in FSL. Rather than point based prototypes, the new area based prototypes  in the embedding space can represent the class level information with more expressivity. Using two sets of learnable parameters to represent the class prototype can reduce the influence of biased data. Weakness:1) More analysis on the experimental details are recommended. I am interested in the feature distribution in the training process. It seems that only the points outside the hyperspheres affect the loss function. 2) The applicability of the method is somewhat limited. The paper proposes "big prototypes" for few shot learning, which represents the prototypes by hyper spheres with dynamic sizes. Extensive experiments in both NLP and CV demonstrate the effectiveness of the proposed method. More details or analysis are needed to improve the paper.<|endoftext|>This paper proposes to use hyperspheres to model prototypes in the feature space, instead of vector points, to enhance the expressivity of class level information. Extensive experiments in both NLP and CV are conducted to evaluate the effectiveness of the proposed model. Strengths:  The proposed big prototype model is a simple and easy to understand extension of the prototype model, which brings more adaptability to the latter. Weaknesses：  In the paper, the metric of two vectors is calculated by a L2 norm distance function. Is there anything that needs to be adjusted? The feature representation of all samples of a class often has a specific manifold distribution, not necessarily Gaussian distribution. What is the effect on the manifold distribution of feature representation before and after optimization with big prototype model? Could relevant visualization results or analysis be provided to show this impact? For me, the method proposed in the article is simple and has aroused my thinking. At present, I think the proposed model is effective and enlightening. Therefore, I tend to suggest acceptance.<|endoftext|>The authors propose to augment an existing prototype based few shot learning approach by augmenting each prototype with a radius. There is a potentially very good paper to come from this. In the end, a simple yet interesting idea. the radius of the sphere is not constant if I understand it properly  what is an atactic manifold? the authors are constantly the world "metric" to mean "distance in a metric space." **Fig1:**  the fact that the embeddings change as well makes the figures hard to understand. This is not my understanding from the rest of the paperRelated work  type of NMS   "find the true location of the prototypes of the entire dataset" I don t think there is such a thing as a true embedding for a prototype. For example, z^j_n is never defined. "spaticularly"[1] Gao et al.Hybrid attention based prototypical networks for noisy few shot relation classification. However, the lack of comparison with modern approaches and the use of a single backbone from 4 years ago makes the method not as convincing as it could be. Ultimately, this is not enough for ICLR.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper makes comparison with techniques used in active learning (AL), domain shift detection (DS), and multi domain sampling to combine data from multiple sources. The experiments are conducted on datasets from questions answering and sentiment analysis. However, the contribution of this paper is not clear.<|endoftext|>This paper surveys a broad range of techniques for active learning in the multi domain setting applied to text   specifically, given a small labeled dataset from a target domain and a large amount of unlabeled data from a collection of source domains, how should examples be picked from the source domains to for labeling to maximise performance on the target domain. The main findings of the work are that selecting examples based on H divergence measures perform much better than most active learning approaches. Overall, I like this work since it serves as a guide for practitioners faced with similar settings, and gives good guidelines on what works in these challenging domain shift settings. However, if these are addressed i’m happy to adjust my scores. The paper is well written and the experiments are useful for practitioners looking to apply active learning, however I think some of the analysis could be improved.<|endoftext|>The authors investigate the efficacy of several active learning related techniques in classification under domain shift with multiple source domains. 2) None of the methods used in the paper are actual domain adaptation models. This is not Domain Adaptation. They also have used red font a few times, which is not necessary. But I am still not clear about this. Because, currently the paper claims that the proposed problem is a general and common challenge.\Another issue that further adds to the confusion is the data split used in the experiments. It is well written and covers a wide range of experiments. My second concern is regarding some of the methods used in the experiments.<|endoftext|>This paper studies active learning with multiple domains to select examples for two natural language tasks, sentiment analysis and question answering. Unlike most of previous work on active learning (AL) focusing on a single domain, this paper targets the multi domain AL setting, a more realistic setting. Plus, part of the analysis on AL is also covered by previous work (e.g., Lowell et al., 2019). This is not discussed in the paper. He et al.https://arxiv.org/pdf/2106.13516v1.pdf (June 2021)+ Good summary and thorough analysis of experiments studying 18 AL methods (2 of which are proposed variants) on 2 NLP tasks+ Paper is well written  Limited technical novelty (the two variants are simple modifications from existing methods)  Similar analysis exists but not discussed
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 3; The paper presents a quite rigorous analysis of approximate implicit differentiation with warm starts applied to strongly convex upper level/strongly convex lower level and nonconvex upper level/strongly convex lower level bilevel optimization algorithms in a very general yet also very practical framework. They allow for stochastic errors in the algorithms solving the upper and lower level problems, making their work practical and applicable to real problems in machine learning (hyperparameter optimization), while analyzing in a way that is agnostic towards which algorithms are specifically used for the lower and upper level problems.<|endoftext|>This work focuses on bilevel optimization. The analysis nicely builds on singularly perturbed systems (SPS). Numerical tests are also provided on both synthetic and real problems, where the merit of warm start is demonstrated. This is an interesting paper that gives nice theoretical links between the proposed algorithm, SPS, and also the stochastic estimate sequence framework. Here are some questions for the authors. Q1.In table 1, is it possible to include $\kappa_l$ and $\kappa_g$ dependence in stochastic settings as well? Is it possible to have a more detailed comparison with [Ji and Liang 2021]? More discussions should be provided on this point. In general this paper is theoretically interesting.<|endoftext|>The paper studies the problem of stochastic bilevel optimisation, proving convergence rates for an amortised algorithm which makes use of unbiased estimates of the inner and outer function. StrengthsSections 1 to 3 of the paper is well written and provides a clear overview of the problem and algorithm. The authors also propose a unified analysis which encompasses many of the existing results in the literature. WeaknessesI find Section 4 difficult to read. In general, I feel that a  unified  analysis should be elegant and more instructive than existing ad hoc analysis, but this does not seem to be the case here.<|endoftext|>The paper studies the problem of bi level optimization. In particular, it considers algorithms based on inexact implicit differentiation, where the inner problem and the implicit differentiation are not solved exactly. Warm up is used when solving the inner problem and implicit differentiation. The convergence of the proposed method is analyzed by viewing the iterates of the optimization as a dynamical system. It is different front the previous literature in the way that it analyzes approximate inner optimization and implicit differentiation with warm up strategy and with faster convergence in the stochastic setting. In terms of weakness, I would hope to see how AmIGO compares to AID BiO proposed in [1] with warm start only for the inner level problem or for solving the linear system in AID. It also proposes an improved convergence analysis of the warm start strategy with inspiration from the singularly perturbed systems.<|endoftext|>This paper proposes amortized implicit differentiation for the bilevel optimization when the inner  objective is stronglyconvex. This paper is not well motivated, and the theoretical and experimental results are not convincing enough. 1.This paper focuses on a class of problems: the inner objective is strongly convex (the strongly convex constant can be negative). This paper follows [Bilevel Optimization: Convergence Analysis and Enhanced Design] in many places. The theoretical results shown in Table 1 mean that the proposed algorithm seems worse than existing algorithms in the strongly convex case. While for the non convex case, the authors indeed consider the weakly convex function rather than general non convex cases. 4.The number of the inner loop $N O(\kappa_g^3\sigma_{g_{yy}}^2)$, which seems very large. Due to previous work  [Bilevel Optimization: Convergence Analysis and Enhanced Design], I deem that the novelty of this paper is limited.
Reject; rating score: 3; rating score: 5; rating score: 6; The idea of using a DNN to approximate state observers and predictors for general nonlinear systems is re examined here. It is not technically correct to assume obtaining an observer, i.e., a dynamical system, from a static DNN mapping. Theoretically the paper empty, and the empirical validation is too simple.<|endoftext|>They also provide a connection to the subspace state space system identification method for linear dynamical systems. The paper is not technically deep and in terms of dynamical systems not coherent.<|endoftext|>This paper proposes a new system identification and model predictive control method based on neural networks for non linear dynamical systems. The case study using a simple non linear system is not convincing enough. This paper proposes a non linear extension of subspace identification and model predictive control.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; Because of that, I tend towards a weak reject, but I am willing to change my score if the authors can address these issues. From the existing experiments, it is hard to judge how the method performs in different settings. (3) The paper is generally well written, and the narrative is easy to follow.<|endoftext|>This paper presents Value Function Spaces (VFS), an abstract "skill centric" representation for reinforcement learning and long horizon planning. The zero shot generalization results are indeed promising, but it is hard to really evaluate how well the VFS agent performed compared to the HRL Target agent (which is trained directly on this problem). Is that correct? Even without them, I think this work represents a (mostly) well executed, interesting idea, and so I am generally in favor of publication.<|endoftext|>This paper proposes that given a set of skills or options, the vector of the value functions for the individual skills becomes the abstract state representation in the approach proposed by this paper. Post Response:I am quite happy with the updated version of the paper, and willing to upgrade my score.<|endoftext|>The results indicate that it is working well. This is not touched at all by the paper. The paper is not touching this at all and assumes access to pre defined (well selected) low level skills.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper focuses on solving minimax optimization when the maximization oracle has a limited amount of computation power and can not compute the max exactly. They provide SGD based and proximal algorithms and analyze them for weakly convex function. 5)The authors successfully provide simulation results for their algorithm.<|endoftext|>The paper considers a min max optimization of general nonconvex functions. The paper is good enough and it is supported by experimental results. Experimental results show the superiority of the proposed approach to previous work. The paper proposes methods to approximately find stationary points by "smoothing" the max part.<|endoftext|>The paper is trying to address the problem of non convex non concave min max optimization under the perspective of application of smoothed algorithms between the two opponents. More precisely,the paper examine a model where the max player applied a zero memory smooth (from differential perspective) algorithm and min player SGD/SNAG or proximal methods providing results similar with the state of art.<|endoftext|>Some experimental results on generative gdversarial networks and adversarial training demonstrate the efficiency of the proposed algorithms.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The paper attempts for a better understanding of the mechanism of multi head self attention (MHSA). It proposes to interpret MHSA weight matrices as factors of the Tucker Decomposition and adds an additional core weight tensor to MHSA to obtain Tucker head self attention (THSA). The main idea of the paper, to interpret the weight tensors of MHSA as factors of a decomposition, is new and THSA appears to be supported by experimental results (although the results are so small that I wonder whether they are significant relative to hyper parameter tuning and stochastic fluctuations of the training).<|endoftext|>The paper investigates the multi head self attention mechanism (MHSA) of transformer networks through the lens of tensor decompositions via tensor diagram notation. The authors propose an extension to MHSA inspired by the Tucker decomposition (termed THSA), analyze its expressive power, and demonstrate that it belongs to a class of more expressive functions than MHSA. Further, the authors show the positive effect of this drop in replacement on the downstream tasks. This corresponds to the rank factorization of the said product. is missing3.<|endoftext|>The paper proposes a THSA that generalizes the standard self attention. This is quite similar to  Proposition 1 in this paper   of course, they are not the same, but with some differences. Tensor network diagram seems unnecessary to the paper. The paper introduces a general Self attention, which is interesting and insightful.<|endoftext|>Focusing on the multi head self attention (MHSA) structure, this paper proposes an extension of the tensor diagram to denote self attention (SA) structures more intuitively. Therefore, it may not provide a guarantee to THSA performance. Although the paper claims "inspired by a Tucker tensor form", what is the advantage of the Tucker tensor form?
Reject; rating score: 3; rating score: 5; rating score: 6; This paper proposes a single model active learning method for classification and segmentation. The paper presents results for classification and segmentation in different datasets. Strengths:  The idea is simple and easy to implement. The idea behind the paper is interesting. As this is basically a multi model approach (using variations of a single model), would be interesting to see a comparison with ensembles. I do believe the results are taken from one of the references, but details there are needed. Same with the proposed method. For all the experiments, I would suggest adding Entropy as a baseline for comparison.<|endoftext|>Bayesian active learning for classification and preference learning. Some elaboration would be helpful. Unfortunately, the experimental evaluation is still a little weak which prevents me from increasing my score further. * **Simplicity**: The proposed method is simple to understand and implement. I ll provide some specific examples here, however, this is a general comment for the paper. The BALD acquisition function  (Houlsby et al.) is also missing and would likely improve the strength of the Dropout baseline. (see the experiments in the references below). 5.No runtime comparisons. In the text there are brief discussions of the runtime for the proposed method and some of the baselines, however there are no empirical results to go with the discussion.<|endoftext|>In other words, prediction deviation when adding a small noise to the parameters is used as a score for selecting data to be labeled in the active learning loop. The authors provide some theoretical analysis for the method to connect it to variance reduction. +:+ The method is simple and does not involve auxiliary models. :  The clarity of the presentation can be improved in several places. It is not clear if this is implemented. One or more baselines are missing from some of the results figures without any explanation. While there is room for improving the paper, I tend towards acceptance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper concerns with uniform sampling from deep generative networks such as GANs and VAEs. (b) It proves the proposed method for a mild assmption that DGN only comprises continuous piecewise affine (CPA) non linearities, such as ReLu, absolute value, max pooling. (c) the x axis values of the two plots in Figure 3 are different by order of 100, it does not seem correct. The paper provides a novel provable method to produce samples that are uniformly distributed on the learned manifold, regardless of the training set distribution. The method is also well proven empirically through numerous experiments.<|endoftext|>This paper proposes a sampling method called MaGNET for generative models which aims at sampling data from the latent distribution of the images uniformly. This paper proposes a sampling method that aims at providing uniform samples from the latent space for deep generative models. This paper does not provide enough experimental results (both qualitatively and quantitively) to support their claims. The visual quality of some generated images using the MaGNET method is poorer than using the original GAN alone (Figure 7 &9) In addition, in addition, the provided samples of the generated images (Figure 15   20) are not clear enough to provide a good justification for their visual quality.<|endoftext|>The authors propose a uniform sampling technique for deep generative networks (DGNs) inspired by the probabilistic change of variables formula. In essence, the algorithm works by drawing many samples N >> K from the DGN, then sampling from these $N$ samples with probability inversely related to their pushforward density (as computed by the change of variables formula). However, I take issue with the framing of the algorithm in the abstract and introduction. While the authors do emphasize the difference between the term in these two contexts much later in the paper, I feel that it is not appropriate for the abstract to mislead in this manner. WIth regard to the uniform sampling property of MaGNET, I have two concerns about the practicality of the method.
Reject; rating score: 5; rating score: 6; rating score: 6; The paper proposes a new neural architecture search framework over graph neural networks in hyper relational knowledge graphs (HKG). I feel the paper is not currently clear in the contribution and there are some disjoint between the method section and the experiment section. The paper proposes a new search space of these configurations and proposes a new HKG method that achieves state of the art on several benchmarks.<|endoftext|>They also proposed a search space of message functions. The search algorithm is a variation of the NAS tailored for this framework. Overall I think the paper is technically strong and authors have shown effectiveness of their method.<|endoftext|>In this paper, the authors propose to conduct neural architecture search (NAS) for hyper relational knowledge graphs (HKGs). Specifically, a novel search space is proposed inspired by recent message passing GNNs on HKGs such as G MPNN, CompGCN, StarE, etc., and a differentiable search algorithm is utilized. Experiments demonstrate the effectiveness of the proposed method.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper proposes approximating the joint posterior with a variational model and then adding the KLD to this variational posterior as an extra term on the ELBO, analogous with but an approximation to the KLD between the approximate and true posteriors that controls the gap in the ELBO. The experiments report classification performance on MNIST + SVHN + Text and bi modal CelebA. Perhaps it is because I am not an expert in multi modal VAEs, but I found this paper quite hard to read. Relatedly, I found the paper s claims about the proposed framework, such as "it is unlikely that the performance of inference and generation will be degraded by the difficulty of cross generation" (p 6) to be hard to agree with or verify as these concepts are not defined with any rigor. Thus, I think the paper needs a major revision to improve clarity.<|endoftext|>The authors propose a new approach to the class of scalable, multimodal VAEs. They propose to use a surrogate joint posterior which, using an additional sum of KL divergence terms, helps keeping the posterior approximation of subsets of modalities similar to the joint posterior approximation. Previous work on multimodal VAEs showed that there is a trade off between quality of generated samples and their coherence (see Sutter et al, 2021). There are multiple typos in this work (e.g.eq.10)### Questions  In Sutter et al, 2020, the authors propose a similar concept as yours. Shouldn’t the results in Table 1 for the latent representation and the results in Table 2 (last row, 2 rows at the bottom) be the same numbers? I could only find it for the VGON case, but not for the VAE case of the proposed method. I would appreciate to see some qualitative results for the CelebA experiment as well. This is an interesting approach to scalable, multimodal learning with promising results regarding the learned latent representations and coherence of generated samples.<|endoftext|>In this paper, the authors propose a model for learning joint representations for multimodal data that allows inference when a few of the modalities are missing. The proposed model claims to be useful in the following scenarios:1) When cross generation is difficult (and hence, MoPoE VAE fails). 2) When the number of modalities is large. The authors achieve this by learning a surrogate posterior conditioned on all the modalities while simultaneously optimizing the KL Divergence between the surrogate posterior and a subset conditioned posterior. The generated modalities also perform well on secondary tasks. Weaknesses   It is not clear why the ELBO and the KL divergence (between the all modality posterior and subset modality posterior) need to be optimized simultaneously. A more intuitive approach would be to learn the all modality posterior first by optimizing the ELBO and then optimize the KL divergence term. However, the motivation for their work could be improved.<|endoftext|>SMVAE is based on MoPoE VAE model, the main architectural change is replacing the MoE rule with minimising the KL divergence between the latent space distribution generated by the PoE combined experts and the joint encoder. Since the scalability of the computational cost of SMVAE with the increasing number of modalities is one of the main motivations for the suggested method, benchmarking it on more modalities seems important. The paper follows the experiments conducted in MoPoE VAE study, with the exception of PolyMNIST of 5 modalities. The cross generation improvement is one of the main points of the paper. It is unclear whether those differences are significant  Some previous multi modal VAE models are relevant but are not part of the literature review: PoE based SVAE model arXiv:2101.07240, which is a generalisation of MVAE and has a quadratic increase in parameters with increasing number of modalities, and VAEVAE arXiv:1912.05075 that has architectural resemblance also using the surrogate joint posterior (or a joint encoder, as introduced in JMVAE study). Concern 1 is for me the most important one and it is unclear to me why this experiment has not been conducted. Otherwise the authors should clarify the result differences in Concern 2. If these problems are addressed, I can see raising my score.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper proposes sample probing, a meta learning scheme for zero shot learning (ZSL), to measure the quality of the synthetic samples provided by certain generative models. Specifically, an existing closed form ZSL solver is plugged into an existing generative ZSL framework. Owing to the differentiability of the solver, the whole pipeline is end to end trainable. Experiments were conducted on four standard benchmarks, where we can observe the state of the art performance achieved by the proposed sample probing based approach. However, in the method section, there are no detailed discussions regarding how the proposed solution endows the synthetic samples with these three properties. 3) Does the overall framework highly depend on the solvers it adopts? Does this indicate that the hyper parameter tuning policy is not suitable for the GZSL task? The paper is well motivated and the solution seems to be effective.<|endoftext|>The paper outlines a method for improving generative zero shot learning (ZSL) approaches. Rather than simply training the generative model with the goal of reproducing some real data distribution, the work proposes to train it with an additional goal of synthesizing samples that directly improve the performance of the downstream classification model. The authors propose to do so through a novel sample probing loss in which generated samples are used to train a closed form ZSL model with a differentiable solution. The ZSL model is then evaluated directly on the classification task   and gradients are back propagated to the generative model s parameters. If so, can this split be released? ******************************Post rebuttal update:The authors conducted an extensive set of experiments and addressed my primary concerns (the method s generalizability to additional baselines).<|endoftext|>To this end, it leverages the zero shot learning model of ESZSL that can be efficiently fit using a closed form solution. post rebuttal:I thank the authors for the extensive experiments and clarification made in the rebuttal. The approach is tested on standard generalized zero shot classification setups, including CUB, SUN, AWA2, and FLO, and compared with state of the art results. Experimental evaluations demonstrate the effect by introducing the sample probing method and end to end training. 4) It would be interesting to see the ablation study regarding the different loss components.<|endoftext|>This paper proposes a generalized zero shot learning approach by providing feedback for generator with the evaluation of real samples of unseen classes. 3  It also uses feature fine tune to improve the performance further. The novelty of the proposed approach is limited.<|endoftext|>The paper addresses the task of GZSL – more specifically, they provide a way to improve the quality of the generative samples in generative GZSL. The proposed method seems to be an integration of generative models and the existing closed form solution. In this manner, the generator receives feedback directly based on the value of its samples for model training purposes. 2  The proposed method is evaluated on four relatively comprehensive benchmark datasets.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 10; They found that the main source of variance is from the numerical instability. Update after rebuttal  I have read the insightful discussion on the newly discovered work and I appreciate that the authors  responded quickly with new results and pointed out the novelty compared to the new paper. The improvement of the combined agent seems solid.<|endoftext|>The paper is well written and easy to read. "Deep reinforcement learning at the edge of the statistical precipice." Update after rebuttal  Based on the discussion with the authors, I feel that the paper can be accepted at the conference. al (2021b) in the medium tasks. More detailed discussion about outlier runs in RL and discussing this in related work.<|endoftext|>Firstly, it would be useful to have a version of fig 3 that is done on a population level, like figure 1. Could the authors add a third row to this figure showing a histogram of the final return achieved by each run? Can the authors comment on this work?<|endoftext|>They even addressed a last minute question about a highly related work that I didn t notice before. The main weakness is not thoroughly discussing the result in table 2. The authors demonstrated that the failure of a few runs is responsible for much of the variance. Moreover, their new results showed that the existing work was inferior to their proposed solution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors propose a network that combines propioception information with visual inputs to estimate displacement along a trajectory. Pro:  Apart from some confusion with the terminology in the introduction, the paper is clear to read and there is an abundance of analysis performed. is it one for the state and one for the cell of the LSTM? The authors used R^2 to determine whether the absolute position of the agent could be predicted from the artificial neurons. If this is applied directly on the embeddings then the authors should adjust it to account for the number of predictors in the model, which in this case  is high dimensional. It’s not clear from the text  whether this was done or not so it should be clarified. The authors claim to have obtained stable cognitive maps in a network trained to do path integration. However this claim is not supported by the results, as they show only a correlation between learnt embeddings and location in the environment. A cognitive map is a spatial knowledge about the environment, that could be used to guide behavior in a flexible manner.<|endoftext|>They compare two different architectures for path integration, based on a vanilla and modified LSTM (the resetting path integrator) that both receive proprioceptive and local visual input processed by two separate MLPs. The R2 values are also very high. The link to cognitive maps is also a bit unclear here and overstated. I discuss major and minor issues below. Figure 7 – how were these examples chosen? Major issues(0)	My central issue with this work is whether the RPI architecture that is the main focus of the paper is interesting because it reaches state of the art precision on a key benchmark (path integration), or whether it is interesting because it is tractable to analysis. I am not convinced that this is a state of the art network for an important task, or conversely that the results of the network analysis are interesting enough to warrant acceptance on their own. While there is ample discussion of the problem of path integration in nervous systems, there is very little context on approaches in artificial systems, which is the main focus of the present manuscript.<|endoftext|>This paper considers the problem of path integration for the purpose of determining an agent’s position in space. The paper claims this helps to reduce the accumulation of errors compared with other non resetting systems. They claim this architecture is a good candidate for a cognitive map. Additional claims regarding the significance of the [R^2](tex://R^2) comparisions were less clear. 2.The proposed architecture consistently shows better performance than baseline LSTM architectures on the same problem. The line of questioning here was not explicit or clear from the writing. For this reason and several others I expand on below, I believe this paper is not ready for publication. I also have a few issues involving the evaluation, baselines, and significance of some results which I describe below. Comparing to the first baseline establishes that the system can benefit from prediction.<|endoftext|>The paper focuses on fusing external signals and proprioceptive signals for navigation tasks. The architecture outperforms LSTM in performance and interpretability, when benchmarked on a 2D navigation/localization problem. #### Strengths  The design and motivation of the proposed method is well grounded in cognitive science, which is quite interesting. #### Weaknesses:  I am not exactly sure if ICLR is the right venue for this paper. Particularly, the problem that the authors study is a toy problem for navigation, it is 2D navigation with obstacles. It is not clear if this architecture will work beyond simple 2D navigation tasks. In the abstract the author mentioned “state of the art” LSTM for 2D navigation/localization problems, I am not sure where is this coming from. This work proposes a simple yet effective way to fuse external signals and proprioceptive signals. It is benchmarked on a 2D maze like environment and shows better performance than LSTM.<|endoftext|>This paper proposes a model for path integration that combines visual and proprioceptive information, i.e.a model that keeps a prediction of where an agent is located, given the past sequence of actions and observations. In a practical scenario, whether it is a biological or an artificial agent, it is hard to see how this would work. Interestingly, the learned state depends not just on the particular location, but also on how the agent ended up there. It is a bit unclear whether the proposed system is intended to model biological systems or to be used for artificial or robotic ones. However, the simulated system is only shortly evaluated with respect to the degree of noise in Figure 6, which is left without much comment. The paper does have its merits for each group.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper revisits Global Pruning (GP) and makes a case to consider it seriously as part of the pruning literature. The paper also includes some discussion on how output patterns of each layer change with different pruning schemes and a note about how to set the optimal MT values. I will list my concerns in the main review. Strengths:1) The fact that authors revisited GP to show its power is a very nice step. GP has been (according to me) the strongest baselines for pruning for a given parameter budget even now (has been acknowledged in multiple papers). 2) The idea of MT makes sense and is a valid fix for the overly pruned layers in general. GP is not forgotten or overlooked by any means. The reason why people work on layer wise pruning is slightly different and I will come to it later. 2) While the authors mentioned people of LTH community probably used GP as one of the potential pruning techniques, GP was the only one that resulted in strong tickets at higher sparsities, making IMP explicitly work on GP no matter what other factors are. MT makes sense for one shot pruning, but one shot pruning itself is problematic (will explain in a bit). 5) Coming to the setup, the biggest problem I have with the way GP or GPMT is being used is the application on top of a pre trained network. Eg.On imagenet the fine tuning of GP or GPMT takes 20 extra epochs, which isn t the case with all the baselines in the discussion. I don t know how we can make a case for these trade offs to make the comparison fair. 6) The big one, the paper completely disregards the compute cost during inference (FLOPs) for these pruned models. The biggest difference GP and layer wise pruning methods have is the inference costs involved. GP often has 2x more inference FLOPs than uniform sparsity with very little gain in accuracy. If uniform sparsity was adjusted for that compute costs, the accuracy will be much higher. However, I would like to say Table 5 is a real thing for one shot pruning, but will not happen for gradual GP. 11) As mentioned in the paper, MT doesn t seem to bring anything to the table for ImageNet experiments, just because of how the weight norms across layers change. But the same paper says GMP schedule is complex. I would say searching for MT is as good as coming up with a cubic pruning pattern which will even help GP when done in a gradual fashion. The paper is not ready for publication, but I am happy to chat with the authors to help them understand and (myself) gain perspective if I am missing something. The paper does not propose anything novel (which is fine) but the observations are also not new while the paper ignores most of the issues and tries to underplay other baselines. After an extremely long discussion with the authors and providing them with things I felt were necessary for fixing the experiments, claims etc., I think the paper is not ready to be published. The authors should revisit most aspects of the paper if they want to make it a benchmark paper for GP and claim so. I  hope the authors understand that it is better to publish a ready paper than a paper changing around a lot at this point. I vote for rejection.<|endoftext|>The paper revisits a traditional model compression method, global magnitude pruning (GP), and shows that GP can achieves state of the art. The paper further improves GP by introducing minimum threshold (MT). 1) My major concern is that the paper does not clearly explain differences between GP in this paper and traditional GP methods. Based on Equation (1), I think GP in this paper is the same as GP adopted in Han et al.(2015a). If GP in this paper can achieve SOTA, Han s method can also achieve SOTA. My suggestion is that the authors provide qualitative or quantitative comparison with previous GP methods. Second, I am not sure whether MT is sensitive to selected threshold. 3) Since CIFAR 10 is a tiny scale dataset, I think experiments on CIFAR 10 cannot sufficiently validate that for WideResNet 22 8, GP+MT can further improve accuracy compared to GP with the same sparsity ratio. The proposed MT is not a sufficient improvement.<|endoftext|>A very simple and effective pruning method is proposed. Instead of layer wise pruning, a global threshold is used to prune weights according to their magnitude. In addition, to avoid over pruning, a minimum number of parameters is preserved for each layer after pruning. * Layer wise sparsity ratios used to be tricky hyper parameters. A global threshold and a minimum number of preserved weights eliminate layer wise sparsity ratios. In this context, the proposed method is practical. * Although the empirical results seem promising despite of the simplicity. it is not straightforward to understand why we should use global threshold across layers and why MT is crucial to ensure superior performance for certain models. For example, how does over pruning impact the performance? * A related study [1] argues that layer wise sparsity is of importance. A comparison (both theoretical and empirical) should make the result stronger. [1] Lee, Jaeho, et al."Layer adaptive Sparsity for the Magnitude based Pruning." 2020.The simplicity and effectiveness of proposed method are important and interesting. The findings will inspire more studies on the nature of network pruning. However, it is not easy for readers to understand where the effectiveness of proposed method comes.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; It also applies two of the most advanced model structures, i.e., EfficientNet and Swin Transformer. The proposed multi branch framework has already been used in previous works; the information of frequency domain, spatial domain and optical flow used in video/image forgery detection have been explored in previous works; the loss functions, i.e., ArcFace and SCL, and the attention module are all borrowed from existing methods. The novelty of this paper seems limited. 3.The authors claimed three clues that are helpful for the video forgery detection, i.e., 1, high frequency information, 2, texture in the shallow layer of the model, 3, the optical flow of the real video has variations while the optical flow of the deepfake video has rarely variations. While the first has already been explored while the second one is not surprised as well, it would be interesting to see the evidence (examples) of the clues, especially for the third one.<|endoftext|>The author proposes a method for detecting deepfakes that makes use of EfficientNet B5 and Swin Transformer. Additionally, the authors propose a loss function and an attention mechanism for EfficientNet B5. ### Weakness:1) The weakness that author mentions are the main contribution of this paper are already identified by [a]. 2) The paper lacks novelty as the proposed method is using previously available methods and new additions are not significant. 3) There are many places in the paper, where the author makes some claims but they are not supported by any reference. One detector to rule them all: Towards a general deepfake attack detection framework.<|endoftext|>This paper faces the problem of video deepfake detection. The idea is to rely on high frequency, texture and optical flow cues to gain generalization. To this end it is proposed a dual branch detection approach: low level features are extracted by EfficientNet B5 that also includes an attention mechanism, while temporal inconsistencies are captured by Swin Transformer. In fact, many of the observations/findings made by the authors are well known in the forensics field and already exploited in published papers. Generalization is tested only considering two datasets, FaceForensics++ and Celeb DF. In my opinion a more extensive analysis should be carried out (see for example the Kaggle dataset proposed by Facebook) and more competitive methods should be included for comparison. In my opinion the technical novelty is not sufficient to warrant publication in ICLR and the performance improvement is marginal with respect to state of the art.<|endoftext|>This paper proposes a new video forgery detection method (ENST) by combining multiple forgery clues including high frequency, low level texture, and optical flow. ENST employs a two branch network, an EfficientNet B5 branch for high frequent and texture info and an Swin transformer branch for optical flow info. 3.The proposed new loss function and attention mechanism and the ablation study for them make the proposed method more convincing. 1.The compared methods in Table1 and Table2 are not the same, which is confusing. 2.The authors claim that proposed method has better generalization but the experiment conducted for Table2 is not convincing to me. I would recommend accepting this paper. The two branch network structure, the feature interaction between EfficientNet B5 and Swin transformer, as well as the use of attention and new design of loss function for EfficientNet B5 make the proposed method solid and convincing.
Reject; rating score: 3; rating score: 3; rating score: 6; In this paper, the authors formalize a notion of task level differential privacy using joint differential privacy, a concept known in the differentially private mechanism design literature. They provide an algorithm with convergence guarantees to do multitask learning in this framework and perform some experiments to test this algorithm in the application of federated learningStrengths:This paper proposes a new idea of using a relaxation of differential privacy in the multitask learning framework. The idea to use joint differential privacy instead of differential privacy in MTL is interesting. The privacy analysis and the convergence rates presented are fairly standard. Hence, I believe in it’s current form the paper is not ready for publication.<|endoftext|>The paper considers the problem of multi task learning  (MTL) with task level differential privacy (TLDP) constraints. The authors propose using joint differential privacy (JDP) for MTL and provide a JDP MTL algorithm, which is a DP variation of FedAvg. They then provide numerical experiments. My main problem is with the convergence results, which are difficult to interpret due to too many parameters and no informal bound is provided. Additionally, it is not clear that the bounds are even non trivial (see below for more discussion). Detailed Review: p.1: is task level DP essentially the same as user level DP/client level DP (which is considered in McMahan et al 2018)?<|endoftext|>Many problems in machine learning rely on multi task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. In this work, authors formalize notions of task level privacy for MTL via joint differential privacy (JDP). They propose an algorithm for mean regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. Then analyze objective and solver, providing certifiable guarantees on both privacy and utility.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper introduces a trust region policy optimization method for cooperative multiagent reinforcement learning with a improvement guarantee. To my knowledge, this is the first extension of PPO and TRPO to the MARL setting with an improvement guarantee. Can you add a discussion like that if that is correct? Experiments demonstrate the efficacy of the approach. I guess the emphasis should be on "other" here. The paper is strong theoretically. It s fine if this goes in the appendix, but it should be derived fully somewhere.<|endoftext|>This paper proposes two algorithms, HATRPO and HAPPO, which extend TRPO and PPO to cooperative multi agent settings, respectively. I maintain my score. Compared to existing methods (e.g., IPPO and MAPPO), HATRPO/HAPPO can maintain the monotonic policy improvement guarantee of trust region learning in multi agent reinforcement learning. Also, no state of the art multi agent value based methods are compared against. How was this update order scheduled in the experiments? Can the authors comment a bit more about this? **Post rebuttal:**I would like to thank the authors for their rebuttal. It s nice to see that the authors provided some results on MAPPO/IPPO without parameter sharing.<|endoftext|>The goal of this work is to extend to the multi agent framework the guarantee that the policy is improved monotonically after each update, as provided by TRPO for the single agent case. The algorithms are supported by a theoretical basis that explains the expansion in the multi agent framework and relax the agents  homogeneity assumption present in the related works. Pro:   The paper clearly explains the related works and the improvement with respect to them. To me, the authors extend successfully a well known state of the art algorithm for single agent RL problems in the multi agent framework, providing potentially a new competitive algorithm also in this scenario. Claim supported by the results of the comparison with MADDPG, which is a state of the art algorithm for the multi agent problem. It seems to me that this prevents from converging to deterministic policies, which can be considered a limitation of the approach.<|endoftext|>Two adaptive algorithms are introduced to enable sequential updating that benefit from a monotonic improvement guarantee. The problem is well formulated, but difficult to follow in some sections. I have the following concerns and questions regarding this paper. Is there any guarantee on the positive update to happen? In other words, is the  joint policy improvement guarantee based on a positivity assumption? However, the readability of the paper is an issue and requires some modifications.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The authors propose a parabolic line search for SGD to automatically adjust the learning rate during training. The authors further show (in their method) how to overcome increasing noise levels in the gradient estimation of SGD by considering a mini batch size of 10. However, the results and (lack of) comparison to previous related methods does not look correct. At the very least, SGD with a decaying learning rate should be compared to. Furthermore, the evaluation of other methods does not seem correct. At the very least, the initial learning rate for this optimizer should always be the optimal learning rate for SGD output during hyperparameter optimization. Other comments: "Gradient only line search (GOLSI)... its performance is rather weak. Representative plots of their 10,000 measured full batch losses along lines are presented in Figure 1.<|endoftext|>The authors include various tricks to make this approach work in practice. The method 1) approximates the full batch loss using many mini batch evaluations, 2) reuses the same step size for some consecutive steps, 3) increases the batch size with a piecewise constant schedule as training progress, and 4)uses a slightly larger step size value than the actual optimum given by the parabola by multiplying by a factor between 1 and 2. Section 4.3 (adaptation to varying gradient noise) doesn’t make much sense to me. So then the comparison with other methods doesn’t seem fair, because the batch size is no longer the same. Comments and questions:  The distinction between step size and learning rate is unclear. What exactly is the difference? In Section G.1, there are some parameters under LABPAL that I’m not sure were mentioned in the main paper: initial measuring step size, parabolic approximation sample step size, and approximation batch size. I would be excited to see a future version of the paper that has a wider range of experiments (maybe showing that the properties also hold for other deep learning tasks).<|endoftext|>In this paper, the authors propose an algorithm for the automatic selection of the step size for SGD and apply this algorithm for DNN training. They suggest that full batch loss along the negative direction of the normalized batch gradient could be localy approximated with a parabola with high precision. So optimal step size could be also approximated as distance to minimum of that parabola. In LABPAL full batch loss is approximated via multiple  batches compared to single batch approximation in PAL. The experiments show that the proposed method outperforms other baseline methods on various datasets and that LABPAL produce stable results for a wide range of hyperparameters.<|endoftext|>This work proposes a line search method LABPAL for first order methods to train neural networks with SGD. The idea is to leverage the two empirical observations on the loss landscape that the full batch loss along lines in the SGD update direction is shaped parabolically and the optimal step size changes rather slowly. Without it the practical value of the proposed idea is quite limited even if it may outperform other line search methods or SGD tuned optimally.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; Various experiments suggest that this is often the case. The paper is generally well written, easy to follow, and contains enough technical details to understand the underlying principles.<|endoftext|>The paper introduces the equilibrium GNN based model with a linear transition map. The transition map is made contracting to ensure that the fixed point exists and is unique.<|endoftext|>Thus the authors proposed a linear map that is used to mimic this procedure. (3) In graph value iteration, the authors only considered deterministic transition case, while in practice the transition are often with uncertainty. I did not check all the derivations and proofs of the paper, but I believe the correctness of Theorem 1 is quite obvious. The motivation and the theory of the paper is clear and correct.<|endoftext|>This is why most of the experiments are done with small graphs. However, the approach presented here is limited exactly for the same reason. The paper is theoretically interesting but the results are limited to small graphs. The results show that CGSs are competitive with the state of the art and sometimes generalize better.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Maybe something like Lipschitz constrained Unsupervised Skill Acquisition (LUSA)? The paper evaluates LSD on 2D skills in the widely used AI GYM benchmark. LSD is also demonstrated on robotic tasks and is able to learn to pick objects in using unsupervised learning.<|endoftext|>The paper proposes a new skill discovery objective based on lipschitz constrained dynamics skills. Further, interesting theoretical connections to existing work are provided and the paper is generally well written with high quality evaluation.<|endoftext|>The paper introduces Lipschitz constraint skill discovery (LSD), a method for unsupervised skill discovery. Strengths:The paper is well written and motivated.<|endoftext|>This paper proposes a new objective (LSD) for unsupervised skill discovery, building on prior skill discovery work using mutual information criteria. The notion of "dynamic skill", which is used quite a bit in the abstract and motivation, is not really clear.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; To overcome oversmoothing in GNNs, this paper proposes node noise with noise correction training, similar to denoising autoencoder, to make node representation distinguishable. It also improves GCNs on non spatial tasks. * Pros:1.The node noise method is simple and easy to use with significant improvement on 3D molecular prediction tasks and non spatail tasks, which may make it a standard trick in the future. I agree that it would be interesting to incorporate the methods into small data regime. (This question is a little bit derivating from the original motivation of this paper.)<|endoftext|>This paper proposes a noise injected training mechanism for graph neural network. The idea is to perturb the node attributes and add an auxiliary loss for the noise correction task. Strength:  The method is pretty simple, but the results are very positive on several benchmarks, which has achieved improvements over several baselines. I find many important details are missing. Two categories of baselines need to beadded: (1) There are several existing works on injecting adversarial lossesinto graph neural networks, which can be also combined with GNS for 3dmolecular prediction. How does this proposed method compare with thoseexisting methods? The proposed idea is intuitive and sound, and the results are positive.<|endoftext|>The paper proposes a new regularization method for tackling both over smoothing and overfitting in GNNs. The main idea is to add node level noise to input graphs and augment the loss function with a denoising term. Random features strengthen graph neural networks. arXiv preprint, 2020. The proposed method outperforms the existing approaches. Overall, I like the simplicity of the proposal and the fact the paper tackles relevant issues in GNNs. **Weaknesses**  It is not entirely clear where the gain is coming from (W1);  No comparison against other ways to achieve deep GNNs, i.e., ways to tackle over smoothing (W2);  Limited technical novelty with non concrete claims (W3);  Experimental setup: not clear why the authors focus on molecules (W4). (W3) The authors provide little support to their claims in Section 4, which are supposed to explain why the method works. **Additional observations*** Noise has also been used to increase the expressive power of GNNs [4]. It should probably be mentioned in the paper. [2] Ming Chen et al., Simple and Deep Graph Convolutional Networks.<|endoftext|>The NN method adds a small noise to the input node representation (and optionally edge representations) during training. The paper includes experiments with both graph level prediction tasks as well as node level prediction tasks and NN is shown to improve for both kinds of tasks. 2.It has generally been hard to train very deep GNNs for many problems because of oversmoothing problem. NN adds additional hyper parameters like noise standard deviation and weight of the auxiliary loss. From reading the paper, it is unclear how sensitive the final performance is to these hyper params. A few more things that could be added to the paper to improve it:1. 2.NN can be seen as both a regularization method (which is most helpful in the small data setting), and a way to reduce oversmoothing to enable training very deep models (which is most helpful in the large data setting). It would be good if the authors can shed some light on this. The proposed method is based on existing ideas used in other domains, but their application to GNNs is novel.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; The paper discusses learning bounds in terms of data fit, and in terms of sparsity pattern. The main insight is to show that one only needs to sparsify the first layer weight of the neural ODE. The main contribution is the sparsity of the ODE Jacobian, which results in learning of differential covariate causalities.<|endoftext|>The proposed method is a straightforward application of the neural ODE and group Lasso method. In addition, it is unclear how the proposed method can be used to handle heterogeneous dynamic systems, which are often the case of practical systems and might make the proposed method less robust. This paper provides a reasonable method for graphical modeling of dynamic systems, although the technical novelty is limited.<|endoftext|>Traditionally structure learning involves using sampled data to learn the structure of graphs. This paper, however, looks at the graph structure learning problem from a different viewpoint, using continuous time dynamics inspired from neural ODEs. Theoretical guarantees on parameter estimation are provided. Weaknesses:  Unfortunately, my enthusiasm for this paper is somewhat tempered by the theoretical results, which do have potential, but also limitations. Why would we be interested in the parameters?
Reject; rating score: 1; rating score: 1; rating score: 1; rating score: 1; The paper proposes a method to quantify uncertainty in medical imaging, which is an important task for clinical applications, with variational inference. I also highly recommend that the authors compare their methodology with the previous works (e.g.[1]).In its current form, the paper does not provide any comparison with the existing studies in the literature, or a comparison of different backbones even though the authors stated that "We tried different backbones which have previously enjoyed success and found original U Net gave the best results." Weaknesses:  The novelty of the paper is quite limited.<|endoftext|>The paper proposes a method for uncertainty quantification for biomedical image segmentation. It is not very clear to me how the proposed method works  Experimental evaluations are quite limited. There are some Dice score and IoU values, but there is no comparison with another method in the literature.<|endoftext|>The paper propose a deep learning based method for taking into account both aleatoric uncertainty and epistemic uncertainty in biomedical image segmentation tasks. The paper is a number of weaknesses:  No attempt is made to put the work in the context of related work.<|endoftext|>Originality: The novelty is quite limited, as VAE has been explored in the medical domain [1]. Plus, it is a common practice in medical image segmentation to estimate the uncertainty of segmentation result by performing multiple times of dropout at test time [2]. Clarity: The paper is clearly written and easy to follow, but has some typos. So I vote for rejection.
Reject; rating score: 5; rating score: 5; rating score: 6; Under this setting, the paper presents the eigendecomposition of the kernel function. The paper also investigates the generalization properties of kernel regression using the depth one CNTK and bounds the risk for regression. The paper uses Gegenbauer polynomial expansion of the kernel function to prove bounds on the sample complexity. I was wondering if one could prove the same bound by Taylor expansion instead. The benefit of using Taylor expansion is that the assumption that the inputs are on the hypercube would be unnecessary. In particular, a degree p monomial <x,y>^p for x,y \in R^d can be decomposed as <x^{\otimes p}, y^{\otimes p}> where x^{\otimes p} is self tensor products of x. I am wondering if one could prove the results of this paper by Taylor expanding the CNTK kernel.<|endoftext|>[postrebuttal] My opinion, after reading others reviews, is that some claims of the paper are not well motivated or supported because the results are too specific. I think there is a form of paradox in the aforementioned references of the paper, which implies depth and laziness(ie linearization in a neighbourhood of the initialization) are important. To be precise:  I’m a bit curious about a couple of statements of this paper, as for instance pooling/down sampling to be an important component of CNNs. Also, I unfortunately was only partially convincing by authors  answers.<|endoftext|>The paper studies the generalization properties of simple convolutional neural networks with a single convolutional layer, equipped with some non linearity, followed by averaged pooling, and then a fully connected linear layer. * Paper is well written, and places the results well relative to the prior works on the subject. Main weaknesses:* While the paper clearly contains novel results, they are more incremental in nature. The techniques used are similar to prior works on NTK, and the derived implications are close to previous results obtained under slightly different settings (as the authors clearly note where applicable). * Some of the specific implications themselves are not that surprising. For example, in Mhaskar et.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper studies the compression of the weights of a neural network via a modification of the standard paradigm of using low rank approximations via SVD on the weight matrices. Here the Fisher information for parameter w is approximated by the average squared l2 norm of the gradient with respect to w over the empirical data distribution. They then conduct several experiments with this approach in the problem domain of compressing language models. They evaluate their method on compressing task specific models as well as the problem of compressing an already compressed model, and demonstrate their approach is generally better. Solid experiments demonstrating the proposed method outperforms other existing approaches. 2.It might be good to address the existing literature on compressing trained neural networks which also goes beyond simply trying to minimize the Frobenius norm of the difference between the weights. The paper introduces a new approach to weighting the low rank compression of neural nets, and empirically demonstrates that it outperforms other methods. The approach isn t too surprising, but it does appear to be novel (though I would recommend doing a more thorough lit review and make the related work section larger, since I feel certain that there are many more related papers doing similar things (I mention one in the main review).<|endoftext|>The proposed paper focuses on a new technique to compress weight matrices in Machine Learning, e.g., weight matrices of layers in DNN. The authors suggest to solve an alternative minimization problem which takes empirical Fisher information under consideration. Experiments on a few datasets imply that the proposed method can achieve better results than simply using truncated SVD of the weight matrix. ) The idea is interesting and the authors explain the main intuition adequately clear. In other words, what is the overall computational cost to create a fine tuned model and apply FWSVD versus generic training and SVD? ) Unfortunately it is not clear to me whether the proposed method comes with any guarantees. One needs to account for the cost of pre processing costs too.  ) I understand that the goal is to achieve higher accuracy in the classification task and not the matrix reconstruction part, but this seems to complicate the optimization part of the model, e.g., how do the authors make sure they do not overfit or spend more time than necessary computing the low rank approximation? ) Without proper explanation in this step, the experiments can not be judged fairly.  ) It would be very helpful if the authors could provide additional experiments about other benefits of the method, if applicable, in the appendix (e.g., is the new method faster than previous approaches based on SVD etc.)<|endoftext|>SVD of matrices in a language model is used to reduce model size. Specifically, importance of each parameter (matrix element) is estimated based on the magnitude of gradients w.r.t.to that element. The paper is well written and easy to follow. Strengths:+ the approach does not require re training (doing pre training again)+ experimental results on several tasks show only a small drop in model quality+ the approach can further reduce the size of already reduced size models (e.g.TinyBERT) Weaknesses:  the reduction in the number of parameters is modest compared to methods that re train the network in a compact representation, like ALBERT  Results in Table 2 look incomplete: why is only one task reported for each of the models? It would be better to pick e.g.three tasks and use them for all four models. the work does not provide much insight into the problem that motivates the approach (e.g.why "group 10" would affect "important parameters" more than "group 9")Questions:  how common is the behavior in Fig 1? It would be interesting to see evidence of it on at least one more dataset/task. The paper provides a simple method to reduce (modestly) the size of an already trained language model, without the need for costly re training. The evidence in favor of the method is empirical, with little in terms of theoretical or experimental exploration of the motivating phenomenon: the overlap between "important parameters" and "parameters poorly captured by SVD".
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors study the POMDP problem from the causal perspective, and the propose to combine offline and online data to infer the transition model via deconfounding. My main concerns are on the experimental side. Also, in the current RL community most RL algorithms take image pixels as input. Comparing with several baselines and SOTA methods is an important way to demonstrate the superiority of the proposed approach. Unfortunately, such a comparison is missing in the paper. I suggest the authors should add some, which would make the paper more convincing.<|endoftext|>This paper considers the model based reinforcement learning (RL) problem by combining the offline and online data. Proceedings of the 24th international conference on Machine learning. My main concerns lie in their framework and assumptions, novelty compared with existing literature, and comparison studies. Since one major contribution claimed in this paper is to bridge the causal inference with reinforcement learning, I was expecting that the authors could use a more rigorous causal framework and necessary assumptions to ensure the validation of their method and theory. "Models, reasoning and inference." (a).First, a number of works have proposed to combine observational and experimental data (though not for RL), such as [2], [3], etc. "Causal discovery from a mixture of experimental and observational data."<|endoftext|>The authors propose an unbiased estimator for evaluating system dynamics from the experimental data. This paper studies the evaluation of interventional distributions in a canonical POMDP model with a finite horizon. The authors validate their results through comprehensive simulations. That is, it would be interesting to see how the authors combine the unbiased estimator from the interventional data with the bound derived from the observational data. Unfortunately, this detail is not elaborated in the main manuscript. Overall, the authors study an exciting topic causal identification in POMDP, which is a quite general, and challenging learning setting. My main concern with this paper is its novelty.<|endoftext|>The paper formulates model based reinforcement learning in this setting as a causal inference problem. The paper presents empirical results on a number of toy problems. StrengthsThe paper proposes to learn a latent causal transition model explaining both the interventional and observational data and infer the standard POMDP transition model via deconfounding using the recovered latent variable. The writing is very poor. The main contribution, section 4.3 is not clearly explained. The authors have made their contributions and assumptions more clear, and will add  results comparing with related work.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The authors argue that bi encoders under perform cross encoders, not for lack of capacity, but due to poor generalization, and back this claim with some theory and empirical results. The authors then propose distillation of the cross encoder model into a bi encoder, which is shown to improve bi encoder results. Strengths:The paper is well written and easy to follow. The experiments are sound and proofs are correct. On the other hand, it s interesting to see empirically that bi encoders have similar training accuracy with cross encoders. From a practical standpoint, the novel contribution of this work is pretty weak.<|endoftext|>3)	In this work, 6 layer BERTs are used as student model. This paper tries to reveal the underlying reasons of performance gap between CA and DE models, and a KD method is further proposed to improve the performance of dual encoder model, while the analysis on the gap cannot guide the design of new student matching architecture or new distillation method. Closing the gap between cross attention models and dual encoder models is an essential research problem for industrial applications. It is obvious that a model with relatively large capacity tends to easily overfit the training data, while to get better OOD performances we need to establish new learning methods or design better model architectures.<|endoftext|>So one ideal solution is to transfer the knowledge from cross attention models to dual encoder models. Thirdly, the paper proposes a new method that further bridges the gap between the two methods. I like this analysis as the starting point of the paper. However, I would like to see more deep insight. 3.The two modification methods on knowledge distillation are interesting. However, I have some questions about this part (refer to the Cons part). I would like more empirical results on data distillations.<|endoftext|>This paper investigate the gap problem between cross attention (CA) and dual encoder (DE) models for document reranking. Through the experiments, they demonstrated that their proposed distillation approaches can further reduce the gap between DE and CA models. They have similar performance as CA models on the training data while performing much worse on the testing data. The experimental results verify that the proposed approach can further reduce the gap. Do the authors have some any explanations about this? * The theoretical analysis can not guide the algorithm design to reduce the gap.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper presents a distributed algorithm to solve Multivariate time series classification (MTSC). The proposed solution LightWaveS is based on the ROCKET algorithm. The authors show these advancements allow LightWaveS to outspeed both ROCKET and MINIROCKET (a faster variant of MINIROCKET) in inference. The proposed method, LightWaveS, is a relatively minor extension of the ROCKET method. For these reasons I believe the manuscript does not meet ICLR s standards and should be rejected. It is clear that this reduces computational complexity but it is not described what are the modeling issues related with this arbitrary choice. **ii ** the connection with the scattering transform is not clear. The computational “trick” suggested by the authors (i.e.considering only the paths in bold in Figure 1) oversimplifies the proposed model. I believe it would be important to spend more words and perhaps perform an experimental evaluation (if no theory can be used) on this crucial aspect. The choice of using ANOVA seems rather arbitrary and does not help the reader in gaining any deeper intuition on the design choice. **Experimental results:**  Give some intuitions on why the L2 model is worse than the L1 one (in all the experiments).<|endoftext|>This paper presents a multivariate time series classification (MTSC) model, namely LightWaveS, that extends another MTSC framework ROCKET. The authors claim that they utilize a wavelet scattering transformation in the model as the main contribution. The wavelet is realized by using a dilation operation in convolutional kernels. The author should justify this important claim and provide theoretical explanations if any. Obviously, the computational cost of the LightWaveS should be significantly less than (MINI)ROCKET. However, the authors only conduct feature selection on their method, what if feature selection is also conducted on (MINI)ROCKET? 3.The authors should conduct ablation studies on the features generated with wavelet scattering and the feature selection, and show which part contributes to the better accuracy/efficiency tradeoff more? (0.05, 0.05 0.1)While the results are promising compared to existing ROCKET models, the proposed solution is not well explained and the experimental results are not fully justified.<|endoftext|>The paper proposes LightWaveS, a distributed methodology for multivariate time series classification based on wavelet scatting transformation. The method utilizes a small percentage of the initial set of features while achieving comparable accuracy performance but with significant speedup. The results are not impressive: there is a loss in accuracyDetails:1. Even though it s not statistically significant, we can observe a significant loss. It s not significant because here you evaluate such a small number of datasets so there is no enough evidence to show significant better or worse results either way. Even though you make this connection with the work of Bruna and Mallat, there is a growing literature these days that is not mentioned or compared against that is critical for understanding if your path is the best/more meaningful than these more "modern" approaches. The solution is not clear how it generalizes to other problems as claimed.<|endoftext|>The paper describes a model for feature extraction and classifying multivariate time series data. The work is built on the existing ROCKET and Mini ROCKET models and instead of the kernel function used in ROCKET, it proposes using Wavelet Scattering. The key contribution of the paper is modifying the ROCKET model and adding Wavelet Scattering and allowing a flexible number of kernels/features that can be extracted from each segment of the time series data. The idea of adding scattered wavelets to an existing model is interesting but it is not new as the authors have also cited related work in this domain. The proposed method allows controlling the trade off between complexity and accuracy when performing inference. Especially in some use cases, the trade off between recall and precision will be very important. The presentation of the results in terms of providing detailed Precision/Recall/F1/AUC would have been very helpful. Comparing the speed/complexity and/or performance of the other methods at the same level of accuracy would have provided a more balanced comparison. Overall, the idea of wavelet scattering as a feature extraction is interesting.<|endoftext|>The authors present a multivariate time series classification (MTSC) pipeline that is an improvement over the (MINI)ROCKET algorithm. The authors also report averaged performances, which is the right practice and generates robust results. Thus, all the results are greatly dependent on whichever values were chosen. Hence, it is not possible to assess the generality of the presented results. The authors present an optimized pipeline with good results, whose generality is not possible to assess.<|endoftext|>This paper tries to improve the inference time of deep learning models by proposing a distributed feature selection process based on wavelet scattering transformations of the time series. In particular, authors proposes a method called LightWaveS for a fast, distributed transformation of multivariate time series based on convolutional kernels, wavelet scattering and feature selection, for lightweight and accurate classification with linear classifiers. The end goal is to improve the inference time while maintaining the performance. The paper provides promising directions for improving the inference time which is critical for edge devices; however, there are a couple of points that needs to be addressed first. It is recommended to provide more details about the functionality of these subroutines. Hence, in order to show the improvement in inference speedup, as in Figure 6, authors need to repeat the inference test several times and report the mean and standard deviation (SD) of the inference speedup instead of just a single number. See the comments provided above.<|endoftext|>This paper works to improve upon the ROCKET algorithm for multivariate time series classification. In particular, the proposed algorithm, LightWaveS, added wavelet scattering, convolutional kernels, and feature selection to make the algorithm much faster and more scalable while preserving close to the same accuracy. The algorithm can also be distributed and the experiments demonstrate good execution time reduction with additional computational nodes. The paper proposes an algorithm that deals with an important problem. 2.The experiments demonstrate the scalability and execution time benefits and show that the accuracy losses are not high. Several aspects of the algorithm need further explanation:     a. Why was the same set of kernels used for both levels of the two level wavelet scattering? This needs explanation.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper presents the learning online with guidance offline (LOGO) algorithm that leverages demonstration data to constrain policy search for reinforcement learning with sparse reward such that the initial exploration phase is guided. The authors performed a theoretical analysis on the algorithm and conducted experiments in a range of different tasks, including a navigation task on a real robot.<|endoftext|>Claims for the merit of the proposed method would also be strengthened if the paper included a sensitivity analysis with varying learning schedules for annealing away the guidance policy data. [5] Goecks, Vinicius G., et al."Integrating behavior cloning and reinforcement learning for improved performance in dense and sparse reward environments." appear to be sound, and it is good to see experiments on a real world robot which substantiates the claim that this method is useful for real world RL. I would like to see one or more additional relevant comparison algorithms included in the simulated experiments, as well as a sensitivity analysis for the learning schedule, as described above. Implementation is often a stumbling point for RL algorithms, and this is a non trivial strength of the method.<|endoftext|>The authors propose a trust region policy optimization based algorithm with offline demonstration data for guidance. Overall, this paper is well written and the idea is novel and promising. The idea of using behavior policy to guide the reinforcement learning algorithm is promising given the fact that demonstrations are sparse to learn.<|endoftext|>While there are several works exploring this general idea, the particular way presented in this work is novel and achieves better results on a set of MuJoCo environment. The approach is also shown to work on a physical robot (Turtlebot). Degree of deviation from the behavior policy? Did the authors attempt LOGO in such an environment? Wouldn t it be more natural to first consider something that works well with sparse rewards? Was this included in the manuscript because the authors have observed that LOGO works well for situations like this? I find the paper s goals clear and interesting and the execution faithful and technically sound.<|endoftext|>This paper aims to improve the performance of RL in sparse reward settings via the guidance of sub optimal demonstrations. Keep doing what worked: Behavioralmodelling priors for offline reinforcement learning, 2020BRAC: Yifan Wu, George Tucker, and Ofir Nachum. The method is evaluated on openAI gym style tasks as well as a real robot navigation task. This paper provides a novel formulation of how to keep an RL policy close to a behavior policy. However, I do have several concerns. Comparison with this method is critical. 3.Empirical evaluation environments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper leverages a simple but effective regularization technique for few shot incremental class learning. Given a few examples of a new unseen class, the authors enforce that the weights of the final layer of the classifier are close to the space that is spanned by the current classifier s last layer weights. **Strong points**Very thorough experiments. I also liked the idea of adding classes in each session. Finally, the paper is easy to follow and well structured. The paper proposes a method for using regularization on the final layer of a classifier to improve the performance on novel unseen classes with very few examples.<|endoftext|>In order to prevent the classifier from overfitting new classes, they authors propose to regularize novel class weights to keep them in the subspace of base classes, thus making the classifier rely on known features rather than new features that are probably spurious. * The authors provide enough details to reproduce their results and they promise to release the code public. * There are some relevant works that were not cited such as [A,B,C,D,E]. * Some previous work such as [C, Cheraghian et al.] It would be good if you clarified what exactly in your work is proposed by you and what is borrowed from previous literature. [A] Simon, Christian, et al."Adaptive subspaces for few shot learning." Springer, Cham, 2020. [C] Tao, Xiaoyu, et al."Few shot class incremental learning." arXiv preprint arXiv:2105.10195 (2021). The proposed method is simple and effective and the paper is well written. Overall, I think that the simplicity and effectiveness outweigh the negative aspects and **I would be inclined towards accept if the authors resolve my concerns and those of the other reviewers.<|endoftext|>The paper proposes subspace regularization technique for incremental few shot learning. The high level idea is to find the basis vectors of the subspace spanned by the base classes, and then project the new classes into the subspace. The regularizer encourages the new class weights to be similar to the projected vectors with an L2 loss. However, my other concern about comparison to Tao 2020 and Chen & Lee 2021 was not fully addressed.<|endoftext|>This paper introduces a new method of few shot incremental learning with its regularization based method motived by a subspace view. Regularization terms to the linear layers of incremental classes are proposed to encourage the novel weight vector to be in the subspace spanned by those of the base classes, and semantic information from the text domain can further be leveraged to guide the interpolation within the base class subspace.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper studies a minimax problem arising from finding the mixed Nash equilibrium for mean field two player games. A quasi static Wasserstein gradient flow is proposed to solve such optimization problems. By discretizing this flow, a simple while practical Langevin gradient descent algorithm is proposed. The paper is well written and have clear objectives. It might be a good idea to add more experiments illustrating the power of the authors  method.<|endoftext|>In this paper, the authors study the minimax problem for finding the mixed Nash equilibrium for mean field two player zero sum games. In this paper, the authors consider the case where the sup problem is solved and apply the gradient descent flow for the result inf sup functional. They show its convergence to the mixed Nash equilibrium under some conditions. Did the author provide some geodesic convexity property of F in Wasserstein space? I suggest the authors carefully compare them with the results in this paper. The authors study a gradient flow of a particular type functional, which comes from an inf sup problem in zero sum games.<|endoftext|>The paper under review studies the QuasiStatic Wasserstein Gradient Flow and its application to solve two player zero sum games. 2.The Wasserstein gradient flow is indeed a powerful tool. Note that the convergence of the density to the equilibrium in [1] is proved in $L^1$ norm. Can the author improve the convergence from this perspective? Is there anything potentially applicable or limitation of the proposed algorithms to be used in high dimension problems. The proposed algorithms show its powerfulness in solving the min max problem.<|endoftext|>The authors first introduces the corresponding quasi static Wasserstein gradient flow dynamics for solving the problem, which is a limiting dynamics with $q$ seen as infinitely faster than $p$. Then the continuous dynamics is proved to be convergent under mild assumptions. Furthermore, by discretizing the above gradient flow, the authors proposes a practical algorithm for solving min max optimization problems with GAN as a special example. Experimental results shows the superiority over the vanilla LGDA algorithm especially with large $\beta$’s. In the Introductory part of this paper, when the authors argues that the main contribution of their paper includes a convergence result of quasi static Wasserstein gradient flow dynamics under mild assumptions, and a practical discretization of the dynamics, it is insufficient to illustrate the significance of this paper.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper provides a lower bound on the generalization error for binary classification problems. This bound can be achieved by any transfer learning algorithm (regardless of its computational complexity) as a function of the amount of source and target samples. The paper is generally well organized. The relation to prior work is explained in detail. Consider a general case of transfer learning problem for bounds, i.e., $n_S>>n_T>>0$. This means that a large size of source data cannot improve the bound, i.e., this lower bound is very loose for a transfer learning problem. Therefore, the claim in the abstract, "the lower bound does not depend on the source/target data distributions" does not hold. 4.Experiments are running the datasets with a size of about 100. This is too small to confirm a theoretical bound. Run the experiments on datasets with large sizes such as 100K to check the bounds. The initial recommendation of the paper. is "reject, not good enough."<|endoftext|>The whole experiment is unclear  1.  it is unclear how the lower bound is indeed justified in the experiments. This paper demonstrates the min max lower bound of transfer learning. I could not understand the whole rationale in this setting. I decided to maintain my current rating, due to the concerns on theoretical and empirical significance. The detailed derivations are not examined.] Reference: [1] A theory of multiple source adaptation with limited target labeled data. However, the proof just simply assumes the small transfer distance, which is unreasonable. The author is encouraged to check papers [1,2] for a better understanding.<|endoftext|>Currently, only the bounds as a function of target sample size are shown. This paper considers real datasets for action recognition and image classification tasks for demonstration, based on which they also evaluate the sharpness of the derived bounds. Based on binary classification problems, the authors derive a lower bound for the generalization error that can be possibly achieved by any transfer learning algorithm as a function of source/target sample size. As the full knowledge of the source and target data distributions are not required in the analysis and as only minimal assumptions need to be made, the lower bound and the analysis presented in the paper may be relatively easily applicable to various transfer learning problems in a practical setting. MAJOR CONCERNSHowever, it is important to note that there exist relevant studies on binary classification in the context of transfer learning, where fundamental limits and efficacy of transfer learning have been discussed in depth. Considering that there exists prior work that addresses (at least to some extent) questions such as "what is the best achievable accuracy via transfer learning for binary classification" and "how does this accuracy depend on the source/target domain sample size and the similarity between the domains", review of the relevant literature and presenting the proposed error bound and the analysis results in a proper context would be critically important. The authors may also want to show the error bounds as a function of source sample size   for different levels of relatedness/similarities between the source/target domains/tasks.<|endoftext|>This work studies lower bounds on transfer learning in distribution free classification, a technique for leveraging related source data S for some target classification problem T. This is a useful idea in practice since common ML algorithms such as deep learning require massive amounts of labeled data, which can often be prohibitively expensive. They then provide empirical evidence that their distance measure captures the difficulty of transfer learning in practice, and that it can be estimated using available source and target data. Transfer learning is a natural, well studied approach to dealing with scenarios with limited data. The authors make two main contributions in this work: a new distance measure (called “transfer distance”) between learning problems, and a clean generalization lower bound for when transfer learning can be helpful. On the other hand, the work is not without weaknesses. The main focus of the paper is on proving a lower bound, but there is no indication of whether the new transfer term in the bound is tight. While some empirical experiments are done using ERM transfer methods, the results do not seem particularly convincing in this aspect. This is actually false, at least in the worst case. Covariate shift is never defined.<|endoftext|>This paper derives a novel lower bound on the generalization error of transfer learning as a function of the amount of source and target samples. It does not depend on the source/target data distributions and requires minimal assumptions. It gives new bounds with minimal assumptions, but1. It is better to give more comparison between the new bound and the existing ones. 2.It is only for binary classes, but in real transfer learning, it is usually multi class learning problems. 3.In experiments, it only shows that the bound is affected by the number of source and target training data, the similarity between source and target tasks. Moreover, how to find if the bound is tight or not?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposes an algorithm for federated learning that leverages recent advances in the NTK framework. Further, the authors introduce a practical implementation that reduces communication, and compare this approach with FedAvg and FedNova. The paper proposes a practical implementation that addresses potential communication burdens. The experimental section provides interesting experimentation showing this framework outperforms FedNova and FedAvg on three datasets. The paper is well organized and clear. My main criticism is that throughout the paper it is mentioned that the NTK “inherently solves the non IID data problem”, when learning global model under statistical heterogeneity of workers. Yet, from the formulation of the NTK this is not evident and it is not clearly developed in the paper, for example by  explaining why the Jacobian enhances generalization while gradients don’t. I acknowledge that at the end of section 4.1 there is a paragraph on this, however it does not respond to my question as it affirms that the global kernel $H^{(k)}$ is more expressive than gradients without an explanation of why. For example, the random projections do not protect from an "honest but curious" server, since all workers share the same projection matrix. This step does not provide Differential privacy. This paper proposes a practical algorithm for FL based on recent advances on the NTK, and FL NTK. However, there are several aspects related to clarifying intuitions and differentiating this paper from previous work that still need to be addressed.<|endoftext|>This paper presents a new federated learning (FL) framework that employs the neural tangent kernel (NTK) method to replace the widely used gradient descent algorithm for optimization. To improve communication efficiency and privacy preserving features, data sampling and random projection techniques are used in the proposed FL NTK framework. Strengths:+ The idea of using NTK for model optimization without gradient descent in the FL paradigm is interesting and somewhat novel. + Inherent issues such as communication cost and data privacy of the proposed method are properly discussed. Method  In a way, the proposed method shifts much of the computation to the server side (e.g., weight update and selecting the best one). To address the risk of data leakage and the high communication cost of FL NTK, random data projection, and Jacobian shuffling are used. In the data heterogeneity experiments, a few popular baselines such as FedProx and SCAFFOLD are not compared. Can you provide an explanation for this? However, there are a few key limitations/weaknesses: (1) FL NTK has a higher risk of data leakage and communication cost compared to FedAvg.<|endoftext|>This paper proposes a federated learning (FL) paradigm empowered by the neural tangent kernel (NTK) framework. The NTK under the infinite width regime allows us to analyze a dynamic of the corresponding neural network without a gradient descent algorithm. The paper is well written and the methodology is very clear. However, this framework requires transmitting Jacobian matrices between workers and the aggregation server, which results in increasing computational overhead and exposing more private information. In addition, the second algorithm with communication efficiency and privacy protection also needs more rigorous analyses. They also address that their NTK based FL scheme has a faster convergence rate compared to that of FedAvg, for a two layer network under specific assumptions. Finally, empirical results support that the proposed FL method performs better than other FL algorithms and achieves similar test accuracy to the ideal centralized case. This paper uses the NTK method as a tool for predicting the output of neural networks. The authors utilize it for predicting the best parameters aggregated from multiple workers. To this end, they adopt dimensionality reduction via randomness sharing projection, zeroing out compression as well as shuffling. Is there any reason that workers send the Jacobian matrix? Is it Gaussian random projection?<|endoftext|>This paper introduces new algorithms for the federated learning (FL) farmwork using neural tangent kernel (NTK) paradigm. Two algorithms are referred to as NTK FL and CP NTK FL, where the latter is a variant of the former for improved communication efficiency and privacy preserving. The proposed algorithms are aimed to address statistical heterogeneity across the workers. An important point is that unlike typical training algorithms for the FL setting, here the workers upload the labels and sample Jacobian matrices (representing NTK) to the server. The server then uses tools from NTK to obtain the trained neural network (instead of using gradient descent). Strengths:  The paper provides a novel approach to FL using NTK paradaigm  The text is well written and clear to followWeaknesses:  The notations can improve to make both the algorithm and the analysis more clear. I augest to use clear references for all the results which are not proven in this paper, to make the paper self contained. While the novel framework is nicely contextualized through a literature review, the information that is uploaded by the workers in this framework violates the privacy preserving feature of FL. Although CP NTK FL is introduced to improve the privacy by compression and shuffling, without a clear discussion and characterization of privacy guarantee, the framework may not be effective. I think authors should include a clear discussion on this issue.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 8; In this paper, the authors develop a second order method based on the cubic regularization technique for solving nonconvex strongly concave minimax problems. I think the paper addresses an interesting problem, but its algorithm development is not very novel. 2.The paper refers to y^*(x) as the minimizer, but it is in fact the maximizer.<|endoftext|>However, the theoretical contributions of the paper are not strong and the empirical study is missing. Such result is unbearable for ill conditioned problems (which is very common in real applications). My second concern is about the way of solving the cubic regularization sub problem. My last concern is this paper does not provide any empirical results. The previous two concerns show that the proposed algorithm may converge very slowly. Minors:Do you have any ideas to generalize current results to nonconvex concave (not strongly concave) setting?<|endoftext|>But I believe that the style of the current version is not very suitable for the machine learning community and the ICLR conference as it does not focus on the large scale setting and does not care about practical performance. The authors provide the very first second order convergence rate results for nonconvex strongly concave minimax problems.<|endoftext|>The proposed Cubic GDA algorithm can be viewed as an inexact cubic regularization algorithm. After reading the other reviews, it seems that the present version indeed consists of a number of issues and is not yet ready for publication. But overall, I feel the paper contributes to analysis and algorithm design for minimax optimization.<|endoftext|>2.The cubic regularization approach was first taken by: Griewank, A.: The Modification of Newton’s Method for Unconstrained Optimization by Bounding Cubic Terms. 4.There is a typo in the declaration of Theorem 4. Overall: I recommend to accept the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The contribution of the paper is theoretical, and about model based methods. The work takes a model based perspective, where a general, intractable algorithm is proposed first. The bound is then specialized to several well studied setting of interests. Per my understanding the computational tractability of the method is not discussed, and the algorithm is therefore not efficient in that respect. Since the algorithm is model free but many proposals in this space are model based, this work is a first serious attempt at a offline model based RL with pessimism.<|endoftext|>This paper studied offline RL for policy optimization and proposed a model based algorithm with guarantees that can hold under partial coverage. Overall, I feel this paper contains lots valuable results and is the first of such kind of guarantee for model based algorithm+partial coverage. Pessimism has been studied in quite a few other papers and so is the partial coverage condition. Some specific comments: 1. One concern is about the algorithm. In particular, even for linear mixture MDPs, you need to integrate over the whole state space which loses the computational efficiency. It will be good to see the comparison between model based algorithm and model free algorithm with pessimism. 2.I appreciate the authors would like to include as many examples as possible but some of them are not quite well explained and the paper now is a bit dense.<|endoftext|>With a model based approach using concentration properties of MLE, this work contributes to the literature of pessimistic offline RL and shows various concrete examples the framework can deal with. Based on TV distance between the learned MLE model and the realizable ground truth, PAC guarantee of the learned policy is established for various examples, some of which have not been tackled with model free pessimistic methods. Comparison of state of the art works is detailed and essential, which might be of use for upcoming works in the field. In general, the paper is well written and main points are neatly addressed.<|endoftext|>This paper propose an algorithm named Constrained Pessimistic Policy Optimization (CPPO) for offline reinforcement learning, and PAC guarantees are provided for many specialized Markov Decision Processes under the partial coverage assumption of offline data. In addition, the linear mixture MDP doesn t include linear MDP as a special case, which is misleading. "computational issue": the authors should discuss the implementability of the proposed algorithm, which is missing here. Many "model free" results stated in this paper are commonly considered as model based methods, such as methods for tabular MDP and linear MDP and so on. "technique correctness": in Corollary 2, $c_3$ is at most $1/S$, which seems to be considered as a constant here. Given the above technique issues, I think this paper is not ready for publication.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper tries to solve traditional scheduling problems by using the deep reinforcement learning (RL) framework. The proposed method performs well in the traveling salesmen problem and the job shop scheduling problem. I suggest that the authors analyze and explain these issues in their submission. In terms of the RL community, the contribution of this paper is low. However, by reading this submission, I believe that its theoretical contribution may be small and the experimental performance is not significantly improved compared to other methods. (1) Verb mistakes.<|endoftext|>ScheduleNet is trained using reinforcement learning and two methods are proposed to improve training performance: (a) reward normalization and (b) adding clip parameter in REINFORCE algorithm training. The paper proposes a generic framework for solving multi agent combinatorial scheduling problems. The reward normalization and usage of clip parameter in training REINFORCE algorithm are shown to be effective. I appreciate the extensive amount of empirical results on both mTSP and JSP domains, which demonstrate that the ScheduleNet not only performs efficiently during training process as compared to state of the art methods, but also generalizes well to other real world datasets. Is it generalizable enough to tackle additional constraints, e.g., time window constraints? (d) I am interested to understand the problem setting for the results of Table 2. The RL algorithm is trained using synthetic dataset (results of Table 1) and then executed in these public datasets?<|endoftext|>​* What are the computational limits of the method? The paper considers solving two well known combinatorial optimization problems (multiple traveling salesmen problem and job shop scheduling problem) using neural networks trained with the help of Reinforcement Learning. The paper is an interesting application of RL coupled with some tricks to the well known optimization problems. This seems not to be the case for normalized rewards used in the paper. Is this issue a limitation for the method to use it for different optimization problems? In particular, it seems that this choice has to be related to the *inner updates* loop from Algorithm 2 (lines 9 12) and the choice of $K$. ​3) Experiments:* The description of training in Section 6.1 is slightly confusing.<|endoftext|>This paper proposes a multi agent reinforcement learning approach to solve scheduling problems (including mTSP and job shop scheduling (JSP)), with the objective of makespan minimization. In the mTSP, the agent represents salesman, and in JSP, the agent represents machine. Experiments on mTSP and JSP simulations validate the strength of the proposed approach ScheduleNet. Strengths: The proposed ScheduleNet can make a tradeoff between computation time and solution efficiency (i.e., makespan). This can also explain that the ScheduleNet performs worse in JSP (Table 4) than that in mTSP. A potential re organization is to first propose a more simple mechanism for the JSP with homogenous agents, and then to propose a complex mechanism (e.g., ScheduleNet) for the mTSP. Especially when there should be an edge between two nodes? However, the techniques are not clearly described and the experimental results are not fascinating.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; However, this objective is a lower bound of the "true" objectives of MMVAE and MoPoE VAE, as pointed out by the authors. (Of course, there is nothing wrong with this, as it is consistent throughout the paper.) This problem is intuitively convincing. These results are clearly novel and important for future research in multimodal VAEs.<|endoftext|>The paper discusses a surprising failure mode of multi modal VAEs, in which their generation quality lags behind that of unimodal VAE. Finally, the theory and experiments proposed in the paper have implications for a large body of research into multi modal VAEs. Intuition and theoretical justification of the limitations appear sound, and is supported by extensive experiments.<|endoftext|>The paper claims this gap leads to quality drop of multimodal VAEs. Overall, the paper is clearly written and easy to follow. The writing in most parts of the paper is clear. This should tell us which term dominates. Figures should be made larger for better readability. It is a very interesting paper.<|endoftext|>They demonstrate their findings for two multi modal datasets, i.e., PolyMNIST and Caltech Birds. To better show the limitations of multi modal VAEs when applied to more complex data, the authors made an augmented version of PolyMNIST, called Translated PolyMNIST including 5 modalities. The authors  contribution is to characterize the limitations of existing VAEs as a step towards improving these methods, which is quite valuable. The paper is very well written and well organized. The authors  contribution on formulating and quantifying the limitation of the existing VAEs for multi modal data analysis is interesting and valuable.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper discusses solving robust subspace recovery by solving parallel versions of $\min_b$ $\|X^Tb\|_1$ st $\| b\|_2 1$, for random initializations of $b$, using projected subgradient method. The argument is that using this  method, in the limit where the number of inliers greatly outnumber the outliers, that if the inlier dimension is c, then c random initializations will converge on identifying this dimension with probability 1. The idea is interesting, and seems to be more of a statistical result than of optimization. Additionally, I am curious as to how much it matters that the data itself has isometry principles. I would suspect if the inlier dimensions have a very uneven contribution, this method should not work that well; how is this captured? Overall, I think the paper s idea is nice and looks sound; however, I am not an expert in this area.<|endoftext|>This paper studies robust sparse recovery when the dimension of the subspace is not known. The main result of this paper is rather surprising  that random initialization can replace the need for explicit orthogonality constraint in DPCP. These proofs and theoretical results need more careful review, but if this is accurate, I believe it will have meaningful impact on many other problems in signal processing and machine learning. This paper could be improved by addressing the following points:  Although the phrase "implicit bias" is in the title and repeated throughout the paper, it is not very well defined. Are there any computational analysis of the algorithm? The take home message of the main theoretical result is very interesting, and could open a new way to think about many other optimization problems with explicit orthogonality contrainsts.<|endoftext|>The work proposed a simple framework that allows performing robust subspace recovery without requiring a priori knowledge of the subspace codimension. The proposed approach is based on Dual Principal Component Pursuit (DPCP), which is amenable to handling subspaces of high relative dimensions. Empirical results corroborate the developed theory and showcase the merits of the proposed optimization methods. The work proposed a new analysis framework for justifying the robust subspace recovery under the DPCP formulation without requiring apriori knowledge of the subspace co dimension. The paper is well organized, and well presented overall. The work tackles an important challenge to perform robust subspace recovery in a high relative subspace dimension regime without requiring a priori knowledge of the true subspace dimension. The overall result is novel, while the approaches seem to be a combination of previous methods.<|endoftext|>This paper is concerned with the Robust Subspace Recovery (RSR) problem. I feel that in the related work section on robust subspace recovery some important work is missing. 7.In the formulation of Theorem 1 it would be helpful for the reader if the authors would refer to (1). Whereas previous work tackles this question, it requires that the dimension of the subspace to be recovered is known a priori. The contributions of this paper are threefold. 2.Some theory for the new algorithm is developed. However, I feel that the paper has a lot of potential and I hope that the authors keep working on the issues which have been raised in this report. However, I think that in its current state the paper is not yet ready for acceptance. Most importantly, I feel that Theorem 7, which is the main result of this paper, needs to be formulated much more cleanly.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; This paper focuses on the issue of assessing the robustness of the "explanations" for NLP models (how the explanation, not the prediction, for NLP models, change with respect to small perturbations in the input). I find section 2 and 3 generally well written and some discussions and intuitions were given about the attack algorithm. The authors follow Ghorbani et al 2019 in looking at the issue on perturbation to explanations and extend the problem to text (discrete input). I believe the robustness of model s explanations is not a very well studied issue in NLP and the community can benefit from this paper. My first concern is that I am confused by section 4.3. Novel paper investigating attribution (explanation) robustness in NLP models.<|endoftext|>The authors introduce a text explanation attack algorithm called “Text Explanation Fooler”, that changes output of explanation methods while keeping the classification output the same. This is done by maximizing the distance between attribution maps of original and perturbed sequences, while forcing these inputs have the same prediction outcome. Evaluations on several datasets and methods are a plus. As not much related work is available in this domain claimed by the authors, I would expect somehow reporting complete set of results. While the technical methodology seems to be valid and confirmed with the experiments, there are some drawbacks such as not reporting all the results, limited discussion, and partly applying some methods from vision domain to text processing, also reported in the weaknesses field of the main review.<|endoftext|>This paper proposes an attacking method to attack the explanations of text classification in NLP. The proposed method is easy to follow. The authors conducted comprehensive experiments to evaluate the effectiveness of their methods. The proposed task is very interesting and important. The proposed attribution robustness sounds nice in Eq.4 and 6.Although they are almost the first work to attack explanations in NLP task (???), the solution is not too novel and straightforward. In this case, the solutions to attack the explanation of text classification would be trivial.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; The experimental results demonstrate the effectiveness of the proposed modules. This paper proposed a novel approach for multi view learning. The pros and cons are listed below:Pros:The general motivation of the proposed model is reasonable and logical.<|endoftext|>Algorithmically, the assumed constraints are converted in "soft constraints" (regularization) and an appropriate optimization is proposed. In general, I would have liked the paper to have a remark after both Assumption  1 & 2 about the reasonability of these assumptions. Empirical results indicate that the approximate algorithm behaves as dictated by the theory. Perhaps this paragraph could be moved elsewhere?<|endoftext|>It seems that $D,D_1,D_2$ are implicitly assumed to be known. Based on the presented theory, a new learning method is developed and implemented. (2021).Overall, I enjoyed reading this paper.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper provides an interesting and novel solution for the critical cold start problem. The paper is well motivated.<|endoftext|>Overall , I think the paper studies an important and interesting problem and presents a good solution which is theoretically sound.<|endoftext|>This paper studies the problem of learning good representations of such nodes using inductive GNNs. Paying more attention to tail nodes and/or nodes with fewer neighbors is important and has been neglected in a lot of previous studies on GNN. It is also of practical value, e.g., cold start in the recommendation system. I suggest that the authors discuss these studies and compare the experimental result with these methods. The experimental studies only validate the performance using GCN. I wonder if the performance will be influenced by different GNN.<|endoftext|>This paper proposes Cold Brew, a new method for learning cold start node embeddings in graphs. Experiments are conducted to show that the proposed method outperforms some baseline methods. 2   The problem is relatively new and interesting. 2   Experiments could be improved. This work studies an interesting problem.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; This paper aims to provide stronger upper and lower bounds for the RD function of arbitrary sources. The authors handle unknown sources by requiring only i.i.d.samples.Specifically, the authors derive an upper bound to the RD function using a $\beta$ VAE like generative model, which has some similarities to the Blahut Arimoto (BA) algorithm with two distinctions: 1) they do not restrict the source to be a low dimensional discrete data, and 2) they use gradient descent to predict the parameters of the variational distributions $Q_{\hat{X}|X}$ and $Q_{\hat{X}}$. Finally, the authors provide experimental results to show that the proposed upper bound is, in fact, exact for random Gaussian samples. The authors provide a tight upper bound on the RD function of arbitrary sources without limiting the sources to discrete data or restricting the dimensions. The authors use the dual formulation of the RD function to find a lower bound, which, to the best of my knowledge, is a novel approach. The experimental results on random Gaussian samples show the exactness of their upper bound (for this particular source). For the banana shaped source and its high dimensional projection, the authors show that upper and lower bounds are tighter than what was proposed in prior work. I find the experimental results on natural images interesting as the authors essentially set a (somewhat loose) limit for future work in lossy image compression without actually providing a compression algorithm. This makes the current lower bound not applicable to high dimensional data such as images. The lower bound is evaluated only on a banana shaped source. It would be nice to see the tightness of the upper and lower bounds on a few other sources. The figure number in the last paragraph is also missing.<|endoftext|>This paper considers the problem of estimating the rate distortion function R(D) for arbitrary sources with unknown distribution. An upper bound is established using a variational distribution and is estimated using an iterative coordinate descent algorithm inspired by the Blahut Arimoto algorithm. A lower bound is established using a parameterization of the dual form of the rate distortion function by Cisszar. These are then evaluated using neural network encoder/decoders on Gaussian and Banana sources, and tested on natural images suggesting that there is room for improved compression rates achieved by current state of the art image compression algorithms. While there has been some work on extending this in the past (Riegler et al, 2018; Harrison & Kontoyiannis, 2008; Gibson, 2017), none have tackled the problem to the breadth of this paper. While the experiments on the Gaussian and Banana sources give credence to the tightness of the bounds, these are rather simple examples. In particular, the variational distribution used for the experiments is Gaussian and contains the source distribution in its family. Do the authors have examples of synthetic experiment in which the source distribution is significantly different from the variational/latent distributions (Q_{Z|X}, Q_Z) as well? The results on image compression suggesting a possible 1 PSNR improvement seem to be significant. Due to the weaknesses in the "significance" part of the main review I am marking the score as a 5.<|endoftext|>The paper proposes to use ML to establish lower and upper bounds on rate distortion for general sources, thus going beyond the Blahut Arimoto algorithm (which assumes discrete sources with known PMF, and has complexity exponential in the dimension of the data). Some numerical results are provided. To construct the upper bound, the paper starts with the unconstrained variational objective of Blahut Arimoto. The distributions Q(\hat{x} | x) and Q(\hat{x}) are parametrized by the parameters of a NN; minimizing the objective function provides a point on the upper bound of the R(D) curve. Based on this a projection to a lower dimensional space is considered. The establishment of the lower bound is less clear. .While some detail is provided for the Gaussian sources (results shown in Fig 2), very little detail is provided for the banana sources (Fig 3) or the natural images (Fig 4). The authors consider the problem of obtaining upper and lower bounds for the rate distortion curve for source distributions that may be continuous, discrete, or mixed, and may not be known (only data are available).<|endoftext|>The paper claims to propose "upper bound" and "lower bound" on the R D function in lossy data compression. I will outline major weakness, not covering all the waeknesses (as all pages are weak). (1) The paper claims to propose upper and lower bounds on the R D function of unknown memoryless information source. There are always nonzero probability of the "upper bound" being lower than the true R D function, because the data samples are probabilistic. There is no reason for not using the standard statistical framework. If one wants to use the Baysian counterpart of the interval estimation, one must have a prior probability of the unknown memoryless information source, but such a prior probability is not provided in the manuscript. It is clear that the discrete and Gaussian sourcesare OK, but they can be handled by the Arimoto Bluhat algorithm in 1970s, after estimating the probability distribution by standardstatistical methods. (4) With any conficence interval estimation algorithm, there always exists a positive probability of the event that the true parameter does not belongto the confidence interval generated by the algorithm with data samples obtained. It is acceptable that confidence levels of the proposed "upper" and "lower" bounds are also algorithmically produced givendata samples. Experiments are only conducted for the Gaussian distribution and so called the "banana" source. If the authors claim wider applicabilityof their proposed algorithm, it must be supported by experimental or theoretical evidences.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; In addition, with different model sizes, model architectures, and different noise level of the training data, the scaling law still holds. This means the effect of suboptimal model architectures and noisy data can be compensated with simply adding more data. Reasons for score:I have major concerns about the contributions this paper adds compared to [1] and [2]. The scaling law of dataset size has also been thoroughly studied and verified by [2]. [2] has already shown the effect of larger models on data scaling laws with models with different numbers of parameters. In addition, some conclusions from the paper are also not convincing:  The paper only conducts experiments on a single language pair (English   German), which doesn t necessarily mean all the patterns found in the paper would generalize to other datasets. When testing for different models, there is only one data point for a type of model at a specific dataset size. The authors didn t discuss how they decide these hyper parameters and how they affect the scaling law. The contributions of this paper are incremental and the conclusions are also not convincing.<|endoftext|>Specifically, it investigates the following factors empirically:  model architecture  model sizes (different numbers of encoder or decoder layers)  data filtering  different types of manually added noises. The paper has performed a large number of studies to demonstrate some empirically useful findings for training the NMT models. A large number of empirical studies are performed to investigate the impacts of different model & data choices on the final NMT scaling laws. Weaknesses:  The studies are mostly focused on one language pair (English to German), there isn t clear evidence whether their findings could generalize to different language families. Since the paper is mostly an empirical work, there s lack of novelty in methodology contributions. However, I am not sure if we should count too much on the methodology contribution for such scaling laws papers. Some findings don t seem very informative, e.g., the scaling laws on model sizes and model family. Different noise reduction filtering methods (e.g., CDS and Bicleaner) can also be fit into the shared component. Overall, these findings emphasizes again on the importance of data quality for NMT models, which is somehow known to the community. The paper further quantified such importance with the data scaling laws.<|endoftext|>The paper studies model and data scaling laws for Neural Machine Translation. It studies how the test cross entropy loss scales with 1) Different seq2seq model architectures (transformer seq2seq, transformer LM and a transformer LSTM hybrid) and asymmetric scaling of the encoder and decoder 2) (a) Training data size and composition   backtranslated data from models of varying size, filtered data using cleaning tools, synthetically introduced noise in the source and target languages (b) In domain vs out of domain test data. Pros:1.Well executed empirical study that complements the work of Ghorbani et al.2021 by considering different architectures such as transformer LMs, transformer LSTM hybrids, and data filtering along with results on publicly available web crawled corpora. Cons:1.A lot of the analysis and scaling results are already present in Ghorbani et al.2021 which this paper borrows from. Overall this is a very thorough empirical study of NMT scaling that is sure to benefit the NMT research community immensely when read alongside Ghorbani et al.2021.<|endoftext|>Architecture: Experimenting with two different encoder decoder architectures, namely transformer encoder decoder and transformer encoder LSTM decoder, the paper shows that marginally worse architecture can be boosted up by adding more data. The scaling laws can fit both independent noise and dependent noise (via back translation)**Strengths:**  The paper has strong empirical experiments at scale to validate the scaling laws. Compare with previous work (Gordon et.al), the training data is much larger, up to 512M sentence pairs for English >German. Could the authors elaborate the size of these models? There are some interesting findings in the paper when scaling up the training data. The experiments with data noise are carefully designed and controlled. **Weaknesses and Questions**  As the authors already called out in the paper, one major weakness I found is the use of log perplexity as a proxy for language generation quality. While the paper focuses mostly on test log perplexity in scaling laws, I think there are other aspects of translation quality that are important for making deployment decisions. I understand that this is not the main focus of the paper, but I think it might be useful to have this angle in the discussions when deployment decisions are rarely made based on perplexity and BLEU scores. They are not mentioned in the following paragraph. Some part of the paper should have been proof read and cleaned up. In the implication paragraph, could the author ellaborate where the addtional amount of data $O(D^{−5/4}) $ and $O(D^{ 2})$ come from?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper proposes a new approach for macroscopic traffic flow forecasting in which the traffic forecasting problem is reformulated as a key value pair matching problem as opposed to the conventional way of using a neural network to predict a sequence given past input. The new approach is novel, technically sound, and well motivated.<|endoftext|>It proposes a neural memory module to model the spatio temporal traffic data and designs a new traffic forecasting model based on the memory module. Experiments on a few public datasets demonstrate the effectiveness of the proposed scheme. S3 The paper is well written and easy to follow.<|endoftext|>Weakness:One of the major concerns is that the paper is not well presented, and critical information on the model and problem setup is missing. The paper is not well presented, and critical information regarding the problem and model setup is missing, which makes the paper difficult to understand. Cosine similarity is a continuous value. In the problem setup, the definition of traffic patterns is not clear.<|endoftext|>How to sample the patterns with a time window? 4.Many traffic forecasting works use MLP for prediction directly, does the proposed decoder performs better than this baseline? The writing of the paper should be carefully improved;2. While using pattern matching for traffic forecasting is reasonable and the design contains some new ideas, the paper should be carefully improved on the following points:1.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper aims to provide an approach to train a robust model, where the model is trained without adversarial examples. Moreover, the inference time defense strategy may give false robustness. The feature smoothing method is proposed for training robust model, but it also is used in the test phase. The motivation for applying this strategy in the test phase is unclear. This kind of approach is not novel [1]. Besides adaptive attack, the criterion suggested by [2] is also necessary to verify the robustness. [1] Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks.<|endoftext|>The authors should perform an adaptive PGD attack where each gradient is computed as follows: Draw multiple samples of random noise, add each of them individually to your current adversarial example iterate, run Algorithm 1 for each of the noised examples, plug the resulting images into the model to obtain a cross entropy (also try CW) loss value, and finally backprop through the entire Algorithm 1 to compute a gradient with respect to the adversarial example iterate. Additionally, there are numerous grammatical errors and misspellings which should be corrected before publication. The most significant concern I have is that the “adaptive attack” is not, in my view, a sufficient try at breaking the proposed method. I am highly skeptical that this defense will stand up to scrutiny, and I don t think the current experiments demonstrate that it will.<|endoftext|>This paper proposes a defense against adversarial attacks that does not involve using attacks in the defense process. The defense consists of two parts: first, a feature smoothing loss is added to the main objective function during training. Then, when defending against attacks, noise is added to the input followed by additional perturbations to the input designed to decrease the feature smoothing loss. The proposed defense is novel, but some ideas are similar to ones previously proposed in the literature. Moreover, understanding the role of noise in the active defense via an ablation experiment will be important to see which aspects of the method actually improve robustness. If the authors can address these points, the paper would be significantly improved.<|endoftext|>The paper introduces a new empirical defense against adversarial examples. Specifically, the paper proposes adding an additional loss to the standard classification loss which “smoothens” the feature maps of a specific layer in the model. At test time, random noise is added to each input, then the input is smoothened out (in an attempt to remove the adversary), by minimizing the Smoothening loss introduced earlier with respect to the input. ### Strengths:  The introduced defense is sound and novel  The defense does not depend on adversarial training, so it is quite fast to train in practice compared to adversarial training. The defense is evaluated using standard white box and blackbox attacks and compares to SOTA robustness benchmarks. This matters for assessing the practicality of the defenses. But I didn’t really get what this means. What does it mean that two losses are “comfortable with each other”? The new defense is faster at training time than adversarial training, but slower at inference time.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; The paper proposes a new memory model following the research line of Kanerva Machine, DKM, and Kanerva++. It shows clear advantages to the previous works. The meanings of "temporary read out" and "dynamic" weight were not clear in the beginning. Minor  It may be beyond the scope of the paper, but showing the actual benefit of this memory in an RL agent will make the paper more complete. The proposed idea is valid and interesting.<|endoftext|>The only section with some detail on related work is section 2.2.1, and connections are only drawn to DKMs and the EM algorithm. These models are both deep generative models that maintain a hidden state similar to recurrent neural networks called memory and have ways of writing into and reading from this memory. I find the results for VAEs, IWAEs and the "Richer priors" section in Table 1 superfluous, can the authors clarify why these were included? The authors propose a novel deep generative memory model called Generative Pseudo inverse Memory (GPM) that extends Dynamic Kanerva Machines (DKM, [1]).<|endoftext|>However, as I ve expressed in the main review, some of the claims need further justification and the paper clarity needs to be improved. On a more general note, a dedicated literature review will greatly help. (ii) The text, however, is a bit hard to follow at times and needs to be severely improved both clarity wise and grammatically. Could the authors clarify this mismatch?
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; The paper presents continual federated learning (CFL) to address the issue that data at clients in FL may vary over time. **Limited novelty:** The technical novelty of this paper is relatively limited. The theoretical analysis is somewhat interesting but it relies on strong assumptions such as convexity (more details below), which does not hold for deep neural networks. Furthermore, the result is a bit cumbersome and difficult to interpret, and the analysis technique and result appear to be quite similar to Yin et al.(2020b).**Unrealistic assumptions:** It is evident that objectives related to deep learning models are not convex, so it would be good to analyze the convergence of non convex objectives as well. Usually an upper bound of the gradient drift across clients is sufficient for the analysis. It would be worthwhile to see whether some of the terms can be simplified in the $\\mathcal{O}(\\cdot)$ notation. I do not understand how Section 4.2 is related to time varying scenarios. In particular, it is not clear why setting $p_{t,i} 1$ would eliminate $R$. I m not sure why the core set needs to be maintained collectively across different clients, and if this is the case in the core set method evaluated in the experiments, it may cause privacy issues. It seems sufficient for each client to separately select and manage its own core set. In general, the use of replay buffer (e.g., in the core set method) for continual learning is quite standard. While the topic is interesting, the paper has limited novelty, unrealistic assumptions, and hardly interpretable results.<|endoftext|>The authors study Federated Learning for time evolving data. * Assumption 2 and 3 involve conditional expectations given parameter vector \omega. My main concern is about the novelty brought by the paper: * As the first main contribution, authors list a novel CFL framework. I assume that authors refer to Eq.CFL as the "CFL framework". * As the second main contribution authors list a convergence analysis of a gradient method for (CFL). * what is "...challenging non overlapping time varying heterogeneous data" ? It is not clear to me how Table I shows that the proposed convergence rates are substantially better than rates of known FL methods. However, i do not see how the current numerical experiments verify these bounds on the required rounds for achieving prescribed accuracy. * what is a "cross device setup"? ps provide more justification for this "non approximhatability" * Definition 3.1 talks about approximating loss functions but Eq.(3) is about gradients * "To analyze Equation (2) and Algorithm 1 in depth, we propose the Gradient Noise Model..." It seems a bit unusual to use a gradient noise model for analysing an  algorithm. The gradient noise model might be prescribed by the application setup (data access model) * should the LHS of (4) and (5) involve the gradient of local loss functions instead of the global loss function ? * "...and in some conditions, μ convex."<|endoftext|>This paper proposes to study time evolving heterogeneous data, and proposes Continual Federated Learning (CFL) to address this problem. Their analysis achieves this by introducing time drift to capture data heterogeneity across time. Convergence results are presented followed by numerical results on time varying and heterogeneous settings. STRENGTHS:  The paper introduces clearly the problem and contribution, while clearly stating differences with state of the art in FL and CL. Time evolving patterns could have a common component across clients. Given the time evolving nature of data, it would be good to see different metrics (not only final accuracy) that represent how the algorithm is able to maintain accuracy as time shifts occur. My score is based on above comments, but I think several of these can be addressed during the rebuttal.<|endoftext|>This paper introduces the Continual Federated Learning framework that allows capturing time evolving heterogeneity of FL. The authors introduce a new formulation of the problem and propose a carefully chosen approximation to work with the new problem. Also, the authors provide theoretical analysis for strongly convex and general convex settings. Some assumptions are questionable and analysis can be generalized. This paper proposes a novel framework and considers the important problem. I think that novelty of the paper is significant, but the paper has some issues. It is easy to follow the narration, all assumptions are clearly described. The motivation of the problem is explained in detail. It might be useful to use superscript. It is not clear what is the CL rate of Yin et al.(2020b).Assumption 1 is not well described. Assumption 2 is limited. In the paper https://arxiv.org/pdf/1909.04746.pdf, all results do not require a bounded variance assumption. Assumption 4 is questionable. This paper has a wide set of different experiments. It might be interesting to compare with a large number of federated learning algorithms. The main issue of the experimental part is the lack of convex and strongly convex models.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper considers distribution shift and feature missingness problems. Authors verify their methods in synthetic dataset and small benchmark problems. The authors did not use an approximate MMD distance, so the proposed method has limited scalability. The paper has a limited applicability to large scale problems. Considering the modern methods and tricks has already dealt with the large scale problems on a more diverse set of distribution shifts (such as Hendrycks 2019 ICLR), the results in the paper seem not significant.<|endoftext|>The method is supported by several evaluations on synthetic data and real data. *Strengths*  The paper considers model adaptation under different missing patterns in the source and target domains and/or covariate shift. Proposed algorithm based on MMD and intensive empirical evaluations*Weaknesses*  The paper lacks a formal definition of the problem and proposed solution. The proposed method lacks theoretical analysis on the test error bound when adapting the model. *References*[1] Kirchmeyer, Matthieu, et al."Unsupervised domain adaptation with non stochastic missing data."<|endoftext|>Authors suggest a new MMD based methodology to improve generalization in case of both distributional shift and missing data. For the missing data, authors propose to generate a new mask (or missingness indicator) on the training data to mimic the missingness patterns in the test data. The closeness of missing patterns in training and test data can be measured with MMD. The proposed methodology to address both missingness and distributional shift makes sense.<|endoftext|>The masking model is learned by minimizing the maximum mean discrepancy between the distributions of  $(x \_{tr}, \hat{i}\_{tr})$ and $(x\_{te}, i\_{te})$. These three approaches are benchmarked and compared in one synthetic dataset and two applications (Bike Sharing and IEEE CIS Fraud Detection). Furthermore, it pushes the idea of thinking about the distributions of feature representations under covariate shift and of an end to end learning workflow (vs. common two step approaches in the liteature). # Related work on missing data:At least half of the contribution of the paper pertains to supervised learning with missing data, yet no connection to existing approaches is made. The paper addresses an important topic and is interesting. * The authors would need to provide additional detail to facilitate the reproducibility of the results.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The paper shows that the existing solvers are susceptible to adversarial attacks. As an example, the authors aim at perturbing SAT and TSP problems and evaluating the robustness of the corresponding state of the art neural solvers on the perturbed problem instances. As a result, I believe the paper offers a solid contribution that should be interesting enough for the community working in the area of neural solvers for combinatorial problems.<|endoftext|>The paper pinpoints the problems with neural combinatorial solvers and studies the notion of adversarial robustness for the scenario. The evaluation of the work is also sufficient.<|endoftext|>In this paper, the authors propose to evaluate and improve the robustness and generalization of neural combinatorial solvers with adversarial examples, that is, perturbed inputs that fool the neural network to generate outputs with high loss. 2.The paper shows that adversarial training with the adversarial data may be a useful practical method for improving neural solver s generalization performance. The paper is dealing with an important and interesting problem.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper describes a way of leveraging the low rank structure of the generalized Gauss Newton (GGN) matrix, and a library that extends BackPACK s features in order to efficiently compute:   the spectrum (eigenvalues, eigenvectors) of the GGN   per sample directional derivatives and curvature of the GGNThey then demonstrate the library on a 900K parameters x 10 classes deep network on CIFAR 10. They also propose a damping scheme for second order that only have access to noisy estimates of the curvature. I would however not deem it as new as it is e.g.discussed in [1,2], but arguably it is interesting to emphasize how to use this low rank structure in deep learning algorithms. I would have appreciated some more insights with more standard networks (e.g.ResNets)**Proposed adaptive damping scheme** The information about mini batch variance is nicely exploited in the proposed adaptive damping scheme for second order methods. I think that the proposed adaptive damping scheme can however make for a nice follow up.<|endoftext|>This paper leverages the GGN’s low rank structure, which allows for efficient computation of eigenvalues, eigenvectors,  per sample first  and second order directional derivatives, and parallel feature computation. The empirical performance is encouraging. Weakness:1) The technical novelty of using low rankness of GGN s seems a bit limited. It is suggested to highlight the difference as well as technical difficulty with existing works in exploiting the low rank structure for fast computation of quantities like eigenvalues. This paper proposes to use the GGN’s low rank structure for fast and sound model learning.<|endoftext|>This paper highlights how the low rank structure of the generalized Gauss Newton (GGN) approximation of the Hessian can be used as a computationally efficient tool to study the loss landscape of deep neural networks. Through the lens of the GGN spectrum, authors make observations on the geometry of the loss landscape and its evolution during training, and propose an adaptive damping technique for second order optimizers that utilizes the GGN curvature information. Since the low rank structure of the GGN has been investigated before by others (as the authors also pointed out), the main contribution of this paper is the compute/memory efficiency and scalability of the proposed tool. In case the method does not work well in these scenarios, the applicability of the tool is limited. Therefore, I recommend rejection of the paper in its current form.<|endoftext|>This work proposes a curvature model VIVIT based on generalized Gauss Newton (GGN) approximation for the training of neural networks with a convex loss function. The low rank structure of VIVIT allows for efficient eigen value decomposition, which also gives per sample directional derivatives and curvatures. As an application example, VIVIT is used to provide noise aware directional damping which improves the stability of second order methods. It is novel that the proposed curvature model VIVIT extracts full spectrum from the exact GGN matrix (on each mini batch) and provides per sample directional derivatives and curvatures, which may be of interest to future work on their own. 3.Overall, the paper is well organized and clearly written. It would be better to have a discussion of the quality of such GGN approximation or demonstrate it in some experiment.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper considers the problem of activation normalization in neural networks in the context of training on non i.i.d.data in continual learning. The authors showcase the issue of the "cross task normalization effect", where the data from a given task is normalized by statistics biased towards another (latest) task. Based on this phenomenon, they show that commonly used normalization schemes, such as Batch Normalization or Group Normalization, suffer from certain problems in this setting (high forgetting, low transfer). Strengths:  The paper focuses on an important problem of activation normalization which to my knowledge has not yet been properly explored in the continual learning setting. On the other hand, the paper would benefit from a more thorough investigation of the problem and fixing certain issues in terms of empirical evaluation. As such, I would say that at the moment it is marginally above the acceptance threshold. Continual Normalization is very easy to use and adapt to existing architectures, which makes the paper useful for the community. The proposed solution was evaluated empirically with satisfactory results and it s very easy to implement, making it a useful tool for CL researchers and practitioners. Why do the "forgetful" properties of BN vanish after applying GN? In Table 3 you consider many different normalization schemes, but after that, you only show results with BN and CN. I think that the overall results are satisfactory, but at the same time, there should be a discussion of the limitations of the method. ", but as the appendix does not have space constraints, it would be useful to add this information. Unfortunately, this is not discussed.<|endoftext|>It argues that batch normalization (BN) is important to CL; however, BN in the current form introduces a bias towards current task, leading to catastrophic forgetting. In particular, it combines spatial and batch normalization into one layer that is suitable for CL. The authors conducted experiments to evaluate the performance of the proposed method. Strong Points* The paper takes one of the most import issues in continual learning: non stationary online CL setting. * The proposed adaptive normalization layers can be a plug in replacement of BN. * The authors made extensive comparison between CN and prior work such as BN, IN, and GN. * Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty, the method has been well motivated by pointing out the limitations in SOTA methods. * Table 2, in DER++ setting, CN s FM score (for catastrophic forgetting) is not as good as GN s. Please explain the reason for that. My major concern is about the limited novelty of the paper and the performance of CN on catastrophic forgetting (see weakness above).<|endoftext|>The paper studies the role of normalization layers in continual learning. The main argument is that vanilla BatchNorm is not very suitable since the statistics of data change across tasks. The experiments compare different normalization schemes across various CL benchmarks. ### Strengths1  I like how the paper is structured. It is well motivated and well written. 2  The arguments are sound, supported by experimental design. For instance, in Table 1, it would be interesting to see what would the performance metrics be if we don t use BatchNorm at all. 2  It would have been more interesting to disentangle the role of normalization from replay buffers. Table 1 suggests the benefit of BN* diminishes when we are using the replay method, and Table 2 doesn t report the results on the Naive Finetuning (or Single in the paper). 3  While the arguments about normalization layers are sound and intuitive, the reasons behind the benefit/drawbacks of different normalization layers are still not studied beyond the fact that the statistics change. I believe it is important to study "how" these changes in statistics change performance.<|endoftext|>This paper proposes a novel normalization layer called Continual Normalization for continual learning. In the paper, the authors point out the problem of global moment bias of Batch Norm. Comprehensive experiments on various benchmarks and continual settings demonstrate the effectiveness of the proposed method. The paper is well written and easy to understand. The intuition of why an "adaptive normalization" at test time could reduce forgetting (or the cross task normalization effect) should be elaborated better. 3.It s nice to see the method worked with replay based method (Table 2, 4). However, I still wonder the performance of CN when there is no replay involved. As replay in some sense already alleviates the cross task normalization effect, it would be better to decouple the influence of CN and replay. 4.(minor) According to the paper, the method can combine the merits of both BN and GN under the continual learning setting. But the method itself is not limited to continual learning problem. The proposed Continual Normalization is novel, however, the reasoning and intuition behind the idea should be elaborated better.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; A modified version of the optimizationstrategy for adversarial perturbation vector generation (Szegedy 2014)is used to obtain an adversarial subspace. A distribution of thedistance of decision boundary (equi logit points) to the test samplesis computed in the adversarial subspace. The distribution shows thatthe distances grow slowly as larger adversarial subspaces areconsidered. This explains the success of adversarial attacks. Basis ofthe tangent subspace is ordered based on curvature. The subspace pullback is defined to measure the susceptibility of the image to anattack. It is observed that the subspace pull back distribution issupported by most of the directions. This is a viable explanation of thesuccess of adversarial training strategies. I m leaning towards accepting this paper because it conducts an extensive analysis with existing approaches towards elegant explanations of adversarial robustness,<|endoftext|>This paper proposes methods to understand the geometry of decision boundaries of ANN classifiers by using adversarial perturbations as a tool. They introduce two methods: (1) a method for finding adversarial subspaces (where input sample has minimal distance to the decision boundary); (2)  a method for measuring the curvature of a decision boundary. These methods allow for new findings such as (1) distance to the boundary grows with increasing dimensionality of adversarial subspace; and (2) decision boundary is more curved in adversarial subspace than random subspace; and (3) insights into why adversarial training works well. <Updated after the author response: Thank you for the detailed response, clarifying comments, and additional experiments. Using adversarial robustness to explain how and why deep networks perform well is an interesting idea, but it wasn t clear to me if this method can be used to explain the performance differences between a large array of standard deep networks. For example, if ResNet152 works better than AlexNet, can we use approaches introduced in this paper to explain their performances, or is the approach here limited to the comparison between adversarially trained vs. vanilla networks only? Empirical demonstration of the proposed approaches on standard deep networks would be helpful, as well as their limits and generalities. My main reservation about this paper is that the proposed interpretability metrics are demonstrated in a sanity check style manner on only a small set of models.<|endoftext|>This paper studies the geometry of adversarial subspaces around input samples, with a focus on their spatial properties and relation to the decision boundary. The principal curvature decomposition is applied to find the principle curvature (the degree of linearity) of the decision boundary in adversarial supaces. The study of an interesting problem. 2.Some of the findings are not new but do have some insights. The ideas are poorly motivated. 2.There are so many different ways of studying the adversarial subspaces, why do the authors choose to study geometry in certain ways? 3.The findings in this work may not be completely new, some of them are caused by the smoothing effect of adversarial training. 4.The dimensionality of inputs/representation space has been thoroughly studied in existing works [1,2]. How the dimensionality investigation in this work is different? Is there a difference in studying the input space vs the representation space? It seems to me some of the studies make more sense in the representation space. 5.The geometric property of adversarial subspaces has been studied in [3], one well known adversarial subspace characterization work. What is the difference between this paper to [3], are the subspaces studied in [3] the same as here? In [3], they define adversarial subspaces to be the space around adversarial examples, which is quite intuitive. How about incorrectly classified natural examples, i.e., natural generalization error? For example, the loss landscape understanding [4,5], decision tiltiling understanding [7], boundary smoothing [8] all look quite relevant. Without a thorough discussion of the existing understandings, it is hard to assess the novelty of this work and how the findings add to the adversarial robustness. 7.It is unclear how the understandings developed in this paper can help future research. This paper raises more questions than answers. It is hard to say the findings are reliable, accurate, or novel. The observations for adversarially trained models are more like a side effect of boundary smoothing, which is what adv training is designed for. Min max optimization gives us a better explanation [8].<|endoftext|>The paper investigates the geometry of the decision boundary surrounding test examples which are subjected to successful adversarial attacks. It also finds "adversarial subspaces" in the input space, which are spanned by orthogonal vectors pointing from the test example to the closest adversarial examples. The empirical results are obtained by randomly sampling the adversarial subspaces. Using this method, the authors investigate the impact of adversarial training on the decision boundary: its distance from test examples and its curvature. The strengths of the paper are:  fully taking into account the boundedness of the input domain when finding adversarial subspaces  principled and (almost) fully rigorous way of defining and measuring the curvature of the decision boundary  empirical investigation of the geometry of the decision boundary, leading to interesting (although not shocking) resultsI have two doubts about the paper, which might require more discussion in the paper (or I didn t understand something):1. We know that the activation functions used in typical neural network models are not smooth. Hence, the decision boundaries will not be smooth either (but will be locally smooth). Is the local curvature representable of the large scale geometry of the differentiable manifold if the manifold is not smooth? If we sample its local curvature randomly, we will "see it" as a straight line. Is sampling e.g.100 random vectors in a 50 dimensional adversarial subspace going to give us a representative sample? Since this analysis is a major selling point of the paper, I cannot recommend accepting it at this stage. Some other comments:1. 2.The first sentence in the caption for Figure 1 is hard to understand. (1)?Can the Authors provide some proof that this approximation works? Something to consider. However, this may be seen as an open question to be explored in future work.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; This paper proposes to introduce attention masks to incorporate spatial locality into part of self attention heads, while keeping the remaining heads to capture global dependencies. Experiments show improved accuracy on ImageNet in comparison with some baselines. The authors should define symbols clearly and specify the configurations in the experiments. 3.The masking strategy, which is the core component of the paper, has serious technical problems. The paper proposes a heuristic strategy to add masks. However, in this paper, all the heads assume to share the same mask which is not reasonable. More explanations are needed. Overall, I think the submission is not complete at the current stage and needs to be significantly improved.<|endoftext|>Experiments on ImageNet show that the proposed approach does matter for lifting the model performance of the original DeiT model. Though there have been many papers discussing the importance of introducing locality into vision transformers, this paper starts from using a mask to improve the self attention. Experiments on DeiT show that the proposed approach performs better than the baselines, especially when two masked heads are used as shown in Table 2. In my view, it should be the locality is not explicitly introduced in transformers. I think the authors should take a correct attitude towards paper writing. After the DeiT paper, there are a large number of papers working on vision transformers. The improvement over the baselines is also not significant. However, the improvement shown in the paper is lower. I do think a comparison with other methods for vision transformers should be added. I cannot find any tables showing such a comparison. Based on the concerns listed above, I think at this moment this paper does not reach the status for publication in ICLR.<|endoftext|>The paper proposes a method that increased DeiT performance by 1%, where the proposed method introduces attention to incorporate spatial locality into self attention heads. By doing this paper claims that local dependencies are captured with masked attention heads along with global dependencies captured by original unmasked attention heads. strengths:  the paper proposes a masking strategy for attention mechanism, to incorporate the spatial locality into self attention heads. weaknesses :  how is the proposed method differs from the multi scale transformers or hierarchical vision transformers   novelty for masking strategy is minimal. it hard to understand how the masking strategy addresses the spatial locality based attention mechanismIn summary the proposed method has few strengths and weaknesses, it would be easy to understand the effectiveness of the proposed method, if the paper provides experiments showing the improvements when masking strategy is introduced to other transformer networks<|endoftext|>This paper proposes to bring locality into the attention module of vision transformers. Basically, the attention masks are binary and is likely to restrict the attention to the local field of a token. Pros:1.The paper is well written. 2.The experimental results shows that improvement of the proposed method compared with the baselines. Yet, using attention masks does not have this additional benefit. 2.Thus, whether the proposed method could be applied to other transformers with locality in the attention module is questionable. 3.In the experiments, other transformers with locality mechanism (Swin transformer, LocalViT, T2T ViT) should be compared. The main concern of this paper is the usefulness of the locality mechanism due to the existence of Swin transformers which are more computationally efficient.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; in particular, it investigates baselines for policy gradient under the dr settings such that the gradient estimate can have a lower variance to ensure better policy updates and learning. it is technically novel and the proposed algorithm works well in practice compared to baselinesweaknesses/questions1. this work targets the variance of the gradient estimates. so why not directly measure the variance of the gradient estimates? what are your opinions on this? what s the connection or insight?<|endoftext|>The paper derived an optimal state/environment dependent baseline and a variance reduced domain randomization approach for policy gradient methods. The idea of extending the state values as baselines to the additional parameterization of the environment variations is natural. My main concerns are about the experimental results: I think the analysis is too weak and not enough baselines are compared with. I do not believe these are all the baselines that should be considered, for instance there s no comparison with meta learning or robust RL methods.<|endoftext|>The idea is to derive a bias free and state/environment dependent optimal baseline for domain randomization. This manuscript is a re derivation of the control variate given that the randomness in the environment is partially artificial (domain randomization). Does the cluster method influence the experimental results? Why are other cluster methods like k means not considered? 7.The paper uses the uniform domain randomization as a baseline, which is not the proposed method in [Mehta et al., 2020] (cited by the paper).<|endoftext|>The authors prove that the policy gradient variance can be further reduced by learning a state dependent baseline for each environment parameter compared to state dependent baselines. It consistently accelerates policy training. As authors noted, the baseline in Section 3.2 is a special case of the input driven baseline derived by Mao et al.(2018).And it s not discussed how proposed method is different from Liu et al.’ s per task control variate. The clarity of the paper can be improved. Examples:        On the fifth line after Eq.(1), the meaning of equation E_{P, \mu, \pi}[g]   .. :  E[g], and the meaning of g in Eq.(9) are not very clear.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Different from previous works that mainly focus on manipulating distributions in latent space, the proposed method directly computes the OT map in the data/ambient space. This paper sheds new light on using OT maps as generators by exploiting the OT map from latent space to data space with unequal dimensions. The proposed method does not add a sophisticated mechanism to compute the OT map compared to WGAN. A discussion on the singularities of the generator/OT map would be helpful for a deeper understanding of the proposed method. Overall the paper is well presented, with novel theoretical and empirical results. I would like to recommend accepting this paper.<|endoftext|>This paper proposes a min max optimization algorithm to apply OT maps directly in ambient space. And that extends the method the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. Contributions of this paper have three aspects: 1) the end to end algorithm is given to fit OT maps for Q embedded quadratic cost, i.e., if two distributions located on the spaces of equal dimensions, the identity embedding Q(x)≡x (Wasserstein 2 distance); if not, choosing Q embedded quadratic cost. 2) The theoretical analysis the error bounds is proved. 1) the proposed method can apply OT maps directly in ambient space, and fit OT maps for Q embedded quadratic cost between located on the spaces of unequal and equal dimensions. what is the relationship between it and the generated mapping?<|endoftext|>This work introduces a method for doing generative modeling, and conditional generative modeling (e.g., image restoration) using the optimal transport map between two distributions. The paper also discusses some of the work that does show the OT map but only into latent spaces due to the intrinsic complexity of computing the OT map on the original space. Indentation and presentation of equations (8) to (11) is confusing. This paper introduces a mathematical formulation to recover the OT map that maps two distributions for doing generative modeling and (unpaired) image restoration tasks. The main weaknesses of the paper are the presentation and experimental validating the approach. This work addresses an important problem and proposes an interesting solution using the OT map between input and output distributions. What s particularly interesting about this mathematical formulation that enables it to work on the original dimension? Lemma 4.1 seems the relevant one. The technical extension to unequal dimensions seems straightforward if you have Q. The paper does not provide intuition or a mathematical justification why the proposed formulation should work better than the other methods when addressing the problem in the original dimension.<|endoftext|>The paper proposes to learn the optimal transport map from the latent distribution to the data distribution directly by optimizing the W2 distance. Experiments show that the proposed method works well and learns the image distribution successfully. The experimental results are solid and convincing. Cons:  I am not sure if equation (10) and equation (11) is really equivalent to each other. Without this constraint, which requires $T$ to be a measure preserving map, equation (11) may goes to $\infty$. The optimal transport map should be included in the feasible set of equation (11) if it is large enough, but without the measure preserving constraint, it s hard to prove that the learned map $T$ is measure preserving. My main concern comes from the deduction from equation (10) to equation (11). The authors may add more explanation to show the equivalence.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; Quantifying the uncertainty of neural networks is a challenge. In this paper, the author(s) proposed a method for calculating prediction intervals (PIs) based on three neural networks (PI3NN). In addition, as demonstrated by the author(s) that empirical experiments showed superior performance of the proposed method over state of the art (SOTA) approaches. And the theoretical analysis and proven properties of PI3NN are provided. I would appreciate it if the author(s) could give a response. How to determine $c$ in Section 3.3 for empirical experiments? In addition, this paper is generally well written, but some places (some issues) in this paper should be further clarified/fixed. As claimed by the author(s), this paper addressed some of the limitations of the existing approaches.<|endoftext|>**Summary**The paper proposes a new method, PI3NN, for predicting confidence intervals with neural networks. The method trains three neural networks with different loss functions, which can then be combined without retraining to give intervals for with different confidence levels. With an additional adjustment to initialization and training, the authors propose a simple way to perform OOD detection with the predicted intervals. **Strong Points**  Producing confidence estimates along with predictions is clearly an important problem that deep learning methods struggle with. The method is more efficient than existing approaches that require retraining for different confidence levels. PI3NN outperforms the baselines considered in their experiments. Also see the question about the use of Equation (12) below. u_\theta and l_\xi are written as functions of x in Section 3.2. It seems like you are comparing their performance based on the metric you define in Equation (12), but I’m unclear on whether this is reasonable. **Additional Feedback**  The assumption that the noise is homoscedastic should be discussed in the paper’s introduction. **Recommendation**If my questions and concerns below are addressed, I think the paper is interesting enough for acceptance.<|endoftext|>The authors propose neural networks based method which calculate prediction intervals for uncertainty quantification. The authors suggest that their method can calculate prediction intervals without retraining neural networks and the estimated prediction intervals can avoid the crossing issue. The targeted goal is interesting and meaningful and the authors achieved this by calculating prediction intervals for confidence levels. The proposed methodology is well described and introduced experimental settings are properly designed with multiple public datasets. The introduced experimental settings seem to be reasonable to show the effectiveness of its method in calculating prediction intervals and the OOD identification capability. Although the targeted problem is interesting and the proposed method is mathematically correct with reasonable experimental results, I think more analysis is needed to make their method more interesting. *ResponseAlthough this paper has a good motivation and the authors improve the quality of experiments, it s still a bit hard to say the proposed method is novel enough as it is a combination of pre existing methods.<|endoftext|>The paper proposes a new prediction interval method which is called prediction interval based on three neural networks (PI3NN). The motivation is to have a method that neither requires retraining neural networks for different confidence levels nor involves highly customized loss functions. In PI3NN three neural networks are trained once and a linear combination of their outputs is used for predicting intervals. Also, a variation of the method is proposed for OOD detection. The paper has considered OOD detection and proposes an idea to help with that along some experiments. Concerns:  One of my main concerns is that there seems to be a disconnect between the theoretical justification and the algorithm. For example, based on the assumptions in Section 3.1, the expectation in (5) is independent of x. Going to step 3 in 3.2, the term is considered to be dependent on x. (9) does not seem to agree with (5). There is no experiment on other types of data like images. For example, given or at a specific x is inconsistently dropped or included. There is no discussion about the scalability and run time of the method. Minor:  There seems to be a typo in Theorem 1 (> and < flipped)The paper is well motivated, but the theoretical analysis does not appear to be solid enough for the proposed method and the experiments are limited.
Reject; rating score: 3; rating score: 3; rating score: 8; 2.*No theoretical guarantee or experiment*: This paper introduced merely a conceptual framework instead of a working system. The proposed idea is interesting but without supporting proofs or empirical experiments, it is very hard for ICLR community to justify its correctness and judge its applicability to real world problems. I strongly recommend the authors code the proposed meta SCM and SAM in a training architecture and validate their correctness in both theoretical and empirical perspectives and submit the to future venues.<|endoftext|>This paper proposes an extension of SCMs that allows modeling cycles in the causality. The main contribution of the paper is providing definitions for the meta SCM via the concept of active mechanisms and effectively connecting the data to mechanisms. 2.The idea of **sufficient representation** is interesting and can be further developed. The authors need to show that they can solve a real world problem using the proposed Meta SCM. It is unclear if tying the definition to data will make the definition flexible or not. 4.Continuing #4, there are no experiments to validate the proposed definitions. * However, they are still not well understood. They also need to show that their proposed meta SCM has enough novel methodology contributions.<|endoftext|>This paper presents a new lens on causal graphical models from a lens of fuller generality. The authors address one of the most fundamental problems in causal modeling: sufficient representation. I found the model proposed to be both intuitive and general enough to address many concerns such as feedback, equilibrating systems, etc. My largest concern is in practicality: from reading it is not clear to me how one would either learn or parameterize these models. In short, this paper is a refreshing though provoking piece of work that I believe could be the basis for many interesting future directions. Interesting, though provoking paper that proposes a novel paradigm for causal modeling in general systems that are often unrepresentable in current causal graphical frameworks.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; With the data compression method,  more old training samples can be stored in the memory to better capture old data distribution. However, there is a trade off between the quality and quantity of compressed data, the authors propose to use determinantal point processes to determine the quality of the data compression. The idea is simple but makes sense in the context of memory based continual learning. 3.The experiments are also extensive which show the benefits of the proposed approach. The motivation of the proposed method is not that clear. 2.In continual learning, we would like the samples in the memory to represent the old data distribution,  why the proposed objective based on DPP can achieve this? In this paper, the authors propose to compress the data for memory based continual learning.<|endoftext|>In this work, the authors propose memory replay with data compression, which is both an important yet neglected baseline and a promising direction for continual learning. Since the compression quality is highly nontrivial for the efficacy of memory replay, the authors provide a novel method based on determinantal point processes (DPPs) to determine it efficiently, and validate their method in both class incremental learning and semi supervised continual learning of object detection. The novelty of this paper is incremental but the experiment of this paper is sufficient.<|endoftext|>This paper studies the problems of classification and object detection from natural image datasets in a continual learning setting, whereby the different classes/objects to be learned are not observed together but sequentially. The strategy used by the paper is to fill this memory buffer with compressed data samples (with JPEG) rather than the original images. Such compression introduces a quantity quality trade off, which this paper empirically analyses. Additionally, the paper proposes an automated way to select the amount of data compression based on determinantal point processes. Based on the description in this section, I would think that this paper deals with domain incremental learning (https://arxiv.org/abs/1904.07734) or the new instances setting (http://proceedings.mlr.press/v78/lomonaco17a.html), but from the rest of the paper I understand it deals with class incremental learning / the new classes setting.<|endoftext|>In this paper, the problem of catastrophic forgetting for continual learning is explored. JPEC method has been explored empirically to determine an optimal compression rate through solving an optimization problem. Experiments on three benchmark datasets are performed to demonstrate that the method is effective. 2.Unnecessary complexities in the implementation of the idea. The justification that the authors have provided is that grid search is of "huge computational cost". This is what we do in most cases when we want to tune a hyperparameter. As a result, the idea contribution of this manuscript is highly limited. 3.In experiments, it is mentioned that 20 samples per class are stored. The appropriate way to do this is to consider a memory buffer with a fixed size and then discard samples as new tasks are learned to replace a portion of old samples with new samples. 5.It is not very clear how much advantageous data compression would be for continual learning. An additional analytic experiment can be to show that how much memory can be saved by storing less compressed samples while getting no considerable performance degradation.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes learning a disentangled representation of spatio temporal mobility data using a VAE based architecture, which essentially tried to decompose spatial and temporal features and model them independently.<|endoftext|>This paper proposes a variation of the Variational Autoencoder (VAE) model to learn disentangled spatial and temporal representations from ST raster data. Spatio temporal representation learning is an important problem. Overall, the paper studies a topic in an important domain. The biggest concern for this paper is the motivation.<|endoftext|>The authors propose a novel neural architecture to structurally learn disentangled spatio temporal representations in the context of mobility forecasting. This is in apparent contrast with Figure 1. The authors approach an interesting and relevant problem such as the one of mobility forecasting. Specifically, the authors focus on how to leverage the disentanglement of spatial and temporal variability in order to get improved forecasting accuracy.<|endoftext|>The core idea of the paper is very interesting for the problem of mobility forecasting but as it is the description of the model is too broad to reproduce the architecture and the experiments. The paper proposes a VAE model for mobility forecasting. It focuses mainly on the disentanglement of spatial and temporal features by modelizing explicitly two groups of features in the VAE schema, a group of spatial features that are time independent and a group of temporal features using a sequential prior.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The authors propose the addition of simple, but structured, classification methods to an existing unsupervised spectral clustering algorithm. I appreciate the main point of the paper, that simple models can perform surprisingly well on standard graph benchmarks, and I find such methods interesting. Also, the empirical results are presented honestly, which is greatly appreciated. If it is the latter, then I do not find the argument of the paper either novel or convincing. However, this would be a very interesting direction for the authors to pursue in the future. should be consistent in its indexing. Do the methods fail at this scale or are there other problems?<|endoftext|>This paper reviewed the application of spectral clustering embedding on node level classification problems. Then it introduced neural network based classifiers which utilize the embedding for classification tasks. Their benchmark results validated that the neural network framework showed certain advantages in accuracy. This paper is very clear and easy to follow and understand. Though it concretely introduces lots of methods, the paper itself does not propose any new methods or insights, such that it does not qualify as a "new method" paper. Still, I will treat this paper as a review paper and below are some of my unuseful comments. 3.The "orthogonal projection", "radial projection" and "geometric partitioning" can be unified in the classifier section. The author could consider compare spectral clustering embedding with other embedding. But it lacks originality and also did not derive new insights.<|endoftext|>This is a thoughtful investigation spectral clustering in classification problems for graph based data. However, there are several concerns regarding missing forms of analysis such as deeper investigations into why certain methods work well on certain datasets. The empirical results indicate that even compared to GCN / deeper approaches, the less parametric spectral clustering based approach can be competitive or better at classification tasks. This paper investigates the use of spectral clustering based approaches for node classification in graph data. Overall this paper provides a clear and thoughtful presentation the spectral clustering based methods. This paper does an excellent job in its thorough investigation of these spectral clustering based approaches. * I think the paper would be improved with some discussion of tradeoffs empirically from a wider variety of GCN approaches.<|endoftext|>The paper proposes a study about spectral clustering techniques. The experiments show that such a simple approach can produce state of the art results on social graphs. WEAKNESSES NOVELTY AND CONTRIBUTIONThis work does not present any particular methodological novelty, and I see this paper more as an experimental one. Why on "arXiv" outperforms all other methods? c) I think that considering only the quantitative performances tells only part of the story, and a deeper analysis should be provided (e.g., error localization, timings)PRESENTATIONApart from the background, I find the rest of the paper a bit verbose, and that could be improved in its communication. I think the visual aspect of the paper could be significantly improved and serve as support to the analysis. While I like the general idea and I appreciated the effort in explaining the underlying theory, I think there is no significant methodological novelty and also a significant lack in the experimental and analysis settings. I do not consider it ready for publication in the present state.<|endoftext|>This paper puts forth a methodology for supervised spectral clustering, by leveraging existing theoretical foundations of spectral clustering, in particular drawing insights from seminal works such as Lee at al on multi way Cheeger inequalities. leads to an improved pipeline for supervised spectral based classification, and is a nice insight. This work demonstrates that with the right pre processing steps in place (like the radial projection onto the unit sphere), spectral methods could be brought back to the comparison board.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This papers considers stochastic convex optimization (SCO) in the shuffle DP model. In particular, the authors consider two models for shuffle DP: A "sequential" model where the analyzer operates in rounds, and in each round a new set of users participate in a local DP protocol, and a new "full" model in which the analyzer can request a specific subset of users to participate in each round, which allows users  data to be queried more than once. The authors show that in the full model, one can retrieve excess population loss bounds matching the best possible bounds in the central DP setting. Furthermore, they show even the sequential model offers improved excess population bounds over the best possible bound of sqrt(d/n) in the local setting. The authors also show that in the weaker sequential model, one can still get improved excess population loss compared to the local setting.<|endoftext|>The authors deal with the stochastic convex optimization problem under the shuffle differential privacy constraint. Under these interactive models, they propose the shuffle private SCO algorithms with the analyses of the excess error bounds. The authors introduce a novel analysis for the scalar sum, revealing the instance specific privacy guarantee. This technique enables us to obtain the lower dimensional dependency on the error of the vector sum protocol. In all cases, the first terms in the error bounds match the lower bound on the non private first order stochastic convex optimization. The second terms in the strongly convex and smooth case in the sequential model and all cases in the full model match the known best result in the central model. It is likely to be tight in these cases. The discussions about the tightness of these bounds are helpful for the reader. One possible weakness is unclearly in the tightness of bounds in the sequential model.<|endoftext|>The paper provides shuffle DP algorithms and population loss upper bounds for stochastic convex optimization (SCO). You require $\epsilon \leq 1$ for privacy of your vector summation protocol, right? In light of the work of Erlingsson et al.(2020) and Girgis et al.(2021) on shuffle DP ERM, SCO is a natural next step. The authors do mention that in practice, sometimes only one query per user is possible, which provides some motivation for sequentially interactive algorithms. They also use acceleration and smoothing for some of their results. Intro: $O(\sqrt{d}/\epsilon \sqrt{n})$ is optimal in the local model is true, but I think Duchi et al (2013) only proves it for sequentially interactive pure DP algorithms, right? Definition 2.7 seems unnecessarily abstract; would be good to explain intuitively  "allow the analyzer to adaptively choose which subset of users to query.." is not the main/important difference between full and sequential interactivity; sequential algorithms also choose which subset of users to query in each round (but not adaptively). I would either delete this part of the sentence or emphasize "adaptively"; the main distinction is in the second part of the sentence (repeated querying).<|endoftext|>The paper studies stochastic convex optimization in the shuffle model of privacy. The algorithm for SGD is pretty straightforward and the analysis is also standard in the literature. In total, the "proposed" new contribution of the paper is the scalar sum protocol. So, I would request the AC to consider this remark over the score given in the Recommendation section. The paper studies stocastic convex optimization in shuffle model; however, to my understanding, most of the results are known or are folklore. This number is a random variable that depends on the privacy noise. However, more importantly, they give an algorithm that meets the bounds one get by shuffling or sampling (up to $\log n$ factor; which I think can be improved using better data structure).
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposes a method for mean variance trade off optimization in RL. The proposed method can mitigate the double sampling issue that appeared in prior work, and can be combined with any existing policy gradient based algorithms. I agree that for this problem, it is still useful to better optimize the mean variance trade off, but I think the authors should clarify this better to avoid confusion. Moreover, since the authors propose this algorithm as an RL algorithm, it would be interesting to test the performance of the algorithm on a more realistic RL benchmark. The experiment section is not satisfactory enough.<|endoftext|>Experimental results are provided to demonstrate the effectiveness of the proposed EQUMRL. After Rebuttal I read the authors  response. Minimax regret bounds for reinforcementlearning. I more or less understand that it is hard to analyze a policy gradient based RL algorithm with sophisticated parametric models like neural networks. Overall, I think that the studied mean covariance problem in this paper is very interesting. However, this paper lacks theoretical analysis, and thus I could not judge whether the proposed approach achieves good theoretical results as well.<|endoftext|>In addition, authors need to improve the writing and update literature review to provide better context of the contribution of this work. 3.For the real world dataset, it is not clear how the authors have trained the model. Since returns increase with increasing variance, I would expect the constraint to be binding. Weakness: I have concerns on results presented for both the examples illustrated in the paper.<|endoftext|>The paper proposed an alternative objective (expected quadratic utility) to the mean variance objective in an episodic RL setting. The proposed expected quadratic utility objective is easy to work with. In the experiments, it seems that only the mean performances are shown. What are the standard deviations for the performances of the methods? side points: (1) I am not sure how the traditional MVRL objectives can be rephrased as the constraint optimization problem with *equality* constraint. Could the authors provide more details on that? The paper was nicely written and provided a simple alternative (the expected quadratic utility function) to the mean variance objective, which has nice interpretations and avoids the double sampling issues.<|endoftext|>The paper is well written and easy to follow in general. The authors provide detailed discussion of the related literature and comparison of the present work with existing ones. Can the authors provide intuition of why this is the case? I have read through other reviewers  comments and authors  rebuttal. I would like to maintain my score.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; * The introduction is very well written. My recommendation is to accept this work. The text is clear and well written. It would be helpful for the authors to include even a few sentences on this.<|endoftext|>The paper presents a strong and clear theory for the proposed reparameterization. I have the following questions for the authors. Is it due to the scalability problem of LSSL? Post rebuttal edit  I thank the authors for their rebuttal. It is clear that this is both empirically and theoretically a strong paper. This is theoretically and empirically a solid paper.<|endoftext|>The paper seems well written both regards to clarity and citations. Contentwise, the theoretical and experimental parts are interesting and relevant. 2) Related to the previous question, it turns out the authors only use the HiPPO matrix as an initialization (although as a very sensible one), but then the algorithm is free to learn any NPLR matrix. Do the authors expect this would negatively affect the results? This is a solid paper with a novel technical contribution that utilizes nontrivial insights for efficient matrix computations, and with a strong experiments section.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes a method to predict turbulent dynamics of coarse simulations. Beyond the proposed architecture, the paper presents an interesting range of test cases from 1D KS to 3D compressible turbulence. The results contain a nice variety, and it is impressive to see predictions of full 3D turbulence simulations. The paper also contains an interesting evaluation of generalization for different initial conditions and simulation domain sizes. "Efficient turbulence" sounds very generic to me, being more specific in the title is a good idea.<|endoftext|>The authors evaluate some different architectures of neural nets for learning turbulence. Meanwhile, the main claim of the paper is somewhat vague because the difference between the proposed architecture and the baseline architectures is not explained in full detail.<|endoftext|>It also shows that the proposed model can simulate turbulent dynamics more accurately than classical numerical solvers at the same low resolutions across various scientifically relevant metrics. The authors revised the paper based on previous reviews. The demonstration doesn’t clearly show the benefits of this method. Isn’t the error is larger in Dil ResNet in fig 2b? It’s hard to show significant advantages with these low accuracy simulations.<|endoftext|>As shown in the results, on coarse grids, the proposed method predicts more accurately than the classical solvers, especially on preserving the high frequency information.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper aims at imputing missing data which are MCAR and MAR. In the experiments, the proposed method, EMFlow is compared with GAIN, MisGAN, and MCFlow. But the assumptions and main idea of the work can be lack justification, i.e., the relationship between the dependencies in the latent space and observation space needs to be elaborated and clarified. The work adapts the online EM algorithm for the missing data imputation.<|endoftext|>This paper presents a model named EMFlow, which performs data imputation in the latent space using the online EM algorithm together with the normalizing flow models. Evaluation using ten UCI datasets, MNIST, and CIFAR 10 datasets show impressive improvement against baseline models and the convergence is faster than MCFlow. The quality of the paper is generally good. The performance is generally better than baselines including MCFlow. However, the novelty is somewhat limited, especially when compared with MCFlow.<|endoftext|>This paper presents a novel imputation method for high dimensional datasets that typically serve as benchmarks for machine learning methods. EMFlow is applied across regression tasks in datasets in the UCI machine learning dataset repository, and standard image classification datasets, MNIST and CIFAR 10. EMFlow has strong empirical results compared to the baseline of MCFlow. The choice of a multivariate gaussian latent space distribution leads easy conditioning and marginalization, and thus lends itself well to the online EM algorithm presented for imputation. High dimensional image data are thought to lie on lower dimensional manifolds.<|endoftext|>The authors propose a novel architecture EMFlow for missing data imputation. The authors also show the results of various experiments with multivariate and image datasets. The study is well designed. Perhaps need to show the performance on those datasets. Therefore the novelty remains limited.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The experimental results show the advantage of learning the inverse skill dynamics and the proposed semi parametric imitation learning approach. The experiments show that this approach successfully solves unseen tasks with a few demonstrations and outperforms the behavioral cloning and skill prior RL baselines. Is this better than executing $H$ actions from the sampled skill $z$?<|endoftext|>The paper presents an approach for skill extraction from an offline dataset of demos/interaction data, for use in few shot imitation learning of previously unseen long horizon tasks. I have some questions about the experiments, but if addressed I think the papers results are solid. The experimental results show that the method seems to perform SPiRL significantly across a range of environments. Overall the paper presents some technical extensions over SPiRL, and seems to have significant performance improvements.<|endoftext|>This paper tackles the problem of learning generalizable long horizon tasks from offline human demonstrations with the below two insights:  (1) The long horizon task learning is enabled by extracting behaviour priors Z as basic skills from offline data, which has been seen in previous publications  (Pertsch et al.2020).(2) The generalization is enabled by matching the new states from adapted envs with the states from demonstrations using a pre trained distance function, which is common in few shot imitation learning. Overall, this paper is quite interesting to read and the presented problem is worth being investigated. As a result, I recommend a weak acceptance.<|endoftext|>The paper presents an approach for skill learning and fine tuning. There are two main novelties in the paper:  Learning of an inverse skill dynamics model that infers which skills should be used given the current state and a future state  Learning a distance function between states that can be used to select states from the demos as "target/future" states for better generalizationApart from the technical novelties, the paper presents ablation studies with intuitions on which of the parts of the new approach (FIST) are important and why. IEEE.The paper was an interesting and nice read.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; It divides the system into two parts, multiple containers, and one centralizer. Containers are trained with trajectories generated by their own actors interacting with the environment. Furthermore, this paper proposes a new loss function encouraging different containers to be diversified to promote exploration. Finally, I think it s unfair to compare CMARL and other baseline algorithms with the same training time. Edit: Thanks for providing the clarifications and new experimental results. I only take QMIX as an example, and other baseline algorithms also face the same problems. I believe that the paper could be considerably improved. This paper is also well organized and clearly written. I think this paper introduces a new distributed value based multi agent reinforcement learning framework to ease the demanding data transfer and accelerate the training process, which is a benefit of large scale training. Meanwhile, based on the analysis in (1), Eqn.<|endoftext|>The authors propose a framework for distributed multi agent reinforcement learning. By grouping actors, replay buffers, learners, and a queue manager into containers CMARL can be used to scale learning to the requirements of the multi agent setting. ~ Positives   The paper is well written and conveys its ideas clearly. Sharing (some) parameters between the global and local learners is an interesting idea and is motivated in the paper. The authors provide code with their submission which makes the experiments reproducible. The paper presents clearly some interesting ideas that improve the training time of MARL algorithms. At this stage, I cannot recommend this paper for acceptance.<|endoftext|>This paper focuses on an interesting and important question: how to perform distributed multi agent deep reinforcement learning? The author first proposed three challenges to be considered: 1) Demanding data transfer. 2) Inter process communication. 3) Effective Exploration. From the current version, this part of the policies is in different containers. 7) It is also the experimental part. The current version seems to be that the author’s method uses more resources for training.<|endoftext|>This paper proposes a distributed containerized multi agent reinforcement learning(CMARL) framework that addresses three challenges in MARL: intra agent observation data transfer, inter process communication,and efficient coordinated exploration amongst the agents. Using a container that collects environment experiences from parallel actors into buffers and learns local policies, CMARL demonstrates notable performance improvements with respect to time as compared to state of the art benchmarks.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper considers a popular approach to semi supervised learning based on iterative pseudo labeling the unlabelled data and refining the model parameters thereafter. The reviewer likes the approach proposed in this paper; however, I would like to express several concerns regarding the theoretical power and limitations of the result. Any bound known from the literature? Why is it so? A similar concern regarding Figure 6. However, in the paper Overall, I believe the paper is interesting, but it can not be accepted as is unless the authors  resolve these comments. The results are interesting; however, it is not clear to me how one can extend the results beyond the Gaussian mixture model considered in the paper. The bounds are needed for model complexity penalization. Figure 1: the upper bound on the generalization error, which should be in between 0 and 1, is about 14 at convergence.<|endoftext|>This paper considers one common semi supervised learning algorithm, pseudo labeling, and studies this problem from theoretical point of view. Specifically, it derives an information theoretic upper bound on generalization error in each iterative update of pseudo labeling. The paper starts with the motivation that labeled data is limited, and pseudo labeling is shown to be helpful, but studying the regime where we have growing number of labeled samples does not completely intersect with that motivation. Based on Figures 7, 8, and 9, there is a sharp reduction in generalization error after only one iteration, but after that, it is not necessarily decreasing. For example, in figure 9, we have reduction in generalization error for the first iteration, then we have a jump in the generalization error followed by another sharp decline. It would be helpful if authors could explain these ups and downs in the plots. This is no surprise to me that more than a few rounds of fine tuning will not improve the generalization error. For section 4, it would be better to subdivide it into a few subsections. The analysis of the paper seems interesting and correct.<|endoftext|>The paper considers the problem of semi supervised learning where pseudo labeling is used to iteratively assign labels for unlabelled data batches to enlarge the labelled dataset for subsequent re training of the classification model. The paper first adapts the recent results of Bu et al (2020) and Wu et al (2020) to this set up and provides a general information theoretic upper bound for the generalization error of such a learning algorithm. Additional experiments are performed on the more practical datasets with deep neural network classifiers. **Weakness**Although the work is interesting and has some merit, to this reviewer, the theoretical results (Theorems 1 and 2) are quite far from the setting in the real world in which deep neural nets are used as the classifier and the training is done via SGD. Compared with Castelli and Cover (1996), the current paper does not seem to offer deeper insight or stronger results. Agreeably it may not be fair to compare the two papers, since the considered learning algorithms are different. The paper presents new generalization bounds for semi supervised learning under iterative pseudo labeling. But the results are distant from settings of practical interest.<|endoftext|>This paper provides a generalization error bound for iterative semi supervised learning (SSL) algorithms using information theoretic principles (see Theorem 1). It is shown in bGMM that when the class conditional variances are not too large, the upper bound on the generalization error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results on the simple model are corroborated by experiments on MNIST and CIFAR datasets, where similar phenomena are observed, i.e., the generalization error improves after several pseudo labelling iterations, but saturates afterwards. I have a question regarding to the SSL algorithm considered in the paper, splitting of the unlabeled dataset into \tau disjoint part is something unnatural to me. However, such a phenomenon is expected, and something more useful would be how to use such bounds to predict the performance of SSL algorithm and further improve it.
Reject; rating score: 3; rating score: 3; rating score: 5; SummaryThe paper studies the evolution of cooperation in SSDs which are essentially more complex versions of the (repeated) Prisoner’s Dilemma. This evolution is now doubly fragile because there is an incentive to free ride on punishment – that is, not to punish the punishers. The authors introduce a mechanism where agents also try to seek out those who have similar strategies as them, which reduces second order free riding. However, I’m not sure there is enough in this paper to warrant publication at ICLR. Possible ExtensionsI do not want to sound too negative, I think this is an interesting start, so I now discuss some potential extensions to increase the meat in the paper.<|endoftext|>This paper considers the instability of the cooperation in MARL. Is the proposed method just a trick, which does not allow the agents to adopt 2 nd order defecting. Then they propose a loss function called “homophily loss” that can resolve these second order dilemmas in MARL. But the provided results cannot convince me that the proposed method is not just a trick.<|endoftext|>This paper studies the problem of promoting cooperation among self interested agents. As far as I understand, the main contribution of the paper is using homophily to solve the second order social dilemmas resulting from the incentive mechanisms other than temporally extended mechanisms. This doesn’t seem to be super realistic in practice given that all the agents are self interested. There seems not a good guidance for these questions.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper extends the hypersolvers framework (Poli et al., 2020a) to the setting of optimal control. The hypersolvers framework combines a cheap numerical dynamical system solver with a neural network trained to approximate the truncation error of this solver, which together yield a cheap and relatively accurate solver. The authors further provide an augmentation to this framework, called multi stage hypersolvers, to further account for misspecifications of the dynamics. Weaknesses: The systems that are tested on are relatively small scale/low dimensional. However, providing experiments on larger dimensional systems would in my view significantly strengthen the paper by more fully demonstrating the potential benefits of the proposed method.<|endoftext|>In this paper, the authors apply the idea of hypersolvers to numerical optimal control. The idea is to approximate the dynamics with a low order ODE solver and learn the truncation residual dynamics using a neural net. Overall, the paper proposes to use hypersolvers to speed up ODE solving for numerical control. In experiments, the method is shown to perform on par with the higher order ODE solvers.<|endoftext|>The paper descibes the application of hypersolves, which warm start ODE solvers with neural nets that have distilled many precomputed solutions, to optimal control ODEs. I thought this paper on the whole was nicely written and easy to understand. I woudl be willing to change my score if the reviewers and AC disagree. This is reinforced by the experimental settings being quite simple and lwo dimensional.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposed a novel geometry evolving graph transformer for protein interface contact prediction, named DeepInteract. In particular, it predicts partner specific protein interface contacts (i.e., inter protein residue residue contacts) given the 3D tertiary structures of two proteins as input, showcasing its effective use in learning representations of protein geometries to be exploited in downstream tasks. The experiments on challenging protein complex targets also demonstrated the proposed method achieves SOTA results for interface contact prediction. The comparison of benchmark methods is also interesting to read. Although the author stated in the paper the new modules of Geometric Transformer, including EDGE REPRESENTATION INITIALIZATION, CONFORMATION MODULE, NODE REPRESENTATION INITIALIZATION, and INTERACTION MODULE, it will be more convincing if the author can compare with Graph Transformer. In the experiment comparison, if the author can add an ablation study experiment, it will be more convincing. In addition, from the experimental results in Table 1 and Table 2, Geometric Transformer does not seem to be significantly improved compared to Graph Transformer.<|endoftext|>This paper proposes a state of the art deep learning architecture for predicting mutual contacts between proteins. Specifically, this work presents a Geometric Transformer for rotation and translation invariant protein interface contact prediction,given 3D protein structures. 2.This work is uses a novel geometric neighborhood representation which considers edges as pseudo nodes, with geometric feature updates. The results are not compared with standard benchmarks, and require porting different results for comparison. 2.The network trained is relatively small and may not scale well. In summary, the proposed deep learning architecture for protein interface contact prediction is somewhat new,though aspects of this contribution exist in related work. The performance metrics used and baselines may be improved.<|endoftext|>########################After an interesting discussion with the authors and inclusion of further results, I changed my evaluations twice and now recommend this paper for acceptance. I would advise the authors to work more on the writing of the paper using the supplementary for the equations and emphasizing more the novelties of their methods compared to previous work (backed up by ablation studies disproving the usefulness of these technical choices). Weaknesses :   Even though the approach is novel, there is not enough evidence that it is needed. This paper pursues two complementary aims. A first one is building a predictor for contact residues using the structures of both proteins interacting. It also includes using edges as pseudo nodes in this graph.<|endoftext|>This paper aims to improve the prediction performance of the protein protein interaction, which involves predicting partner specific protein interface contacts (i.e., inter protein residue residue contacts) given the 3D tertiary structures of two proteins. The key contribution of this paper is not clear. 2.The method section as well as the figures is hard to read. (2021).Deep geometric representations for modeling effects of mutations on protein protein binding affinity. The proposed architecture seems not novel, and I have some serious concerns regarding the experiments.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; For example, I would imagine that there will be a large sim to real gap in vision. It provides extensive experimental evaluation of the approach in terms of performance and safety metrics, both in simulation and using real world experiments. + The proposed method is validated on a real robot. Neither of the key components of the paper are novel: RL for locomotion and Transformers.<|endoftext|>The method is extensively evaluated in simulation and on a sim to real transfer tasks. ## Strengths and Weaknesses### Things I liked about this paper  **a powerful framework**: fusing visual and proprioceptive data for quadrupedal locomotion using transformer architectures is an interesting and also valuable approach that works well and sets an excellent opportunity for future work. There are some minor details that may further improve the quality of the paper but I see this as a strong submission which I can recommend for acceptance.<|endoftext|>[Strength]This paper tackles an important question of how to incorporate visual information in learning policies for quadrupedal locomotion, where most existing learning based control of quadruped robots in the published works only considered proprioceptive information, and the robots are essentially "blind." How well does the method work compared with the built in controller of the robot? The real world demo from sim to real transfer also provides concrete empirical evidence on the practical use of the proposed method.<|endoftext|>The sim trained policy has been demonstrated on the real A1 hardware. The main strengths of the paper:(1) Proposed a novel transformer based architecture that can train visual locomotion policies end to end, and demonstrated good navigation/obstacle avoidance/uneven terrain walking results in the simulation. (2)  Zero shot real world transfer to a A1 robot and demonstrates walking + navigation behavior in various environments. As the authors cited, there are many approaches to tackle visual locomotion + navigation problem besides end to end training. The authors also deployed the trained policy successfully to the real robot.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Example sentences are:a) "It achieves a win rate about 12% instead of 1/4 · 1/3   1/12"b) "In the fedual reinforcement learning (FRL) formulation, both the manager and the worker subject to Markov Decision Processes (MDP)." Limited novelty and improvements but significant gains in performance although unclear because the writing isn t clear. There seems to be quite a large gain in performance with the FRL model that generates sub goals in a backwards manner. There are several things in the paper that are not clearly explained at all that made this hard to follow.<|endoftext|>But there are some concerns about what is done so far in this paper:a. The modeling of the language seems out of date, using LSTMs etc. The models are described, experiments are described (mostly in the RTFM but a callout to Messenger is there in Appendix). This is a natural approach and intuitively might be generalizable to other contexts.<|endoftext|>However, the experiments in Messager are only preliminary and only shown in the appendix. The manager issues a plan which is executed by the worker. The worker, instead, uses the plan from the manager (in terms of coordinate to the target ($X_target$)), the observation $E_{obs}$, and the position to the other player ($X_{pos}$) (not sure I understood correctly this) to interact with the environment. Moreover, it is very hard to read/understand Eq.1. and 2.I suggest the authors rewrite it and better connect them to the figures.<|endoftext|>The manager and the workers are trained separately. This seems like an inefficient way to collect successful trajectories by letting an agent execute random actions. Moreover, collecting successful trajectories is not possible in more complex environments. I would have liked to see the failure cases of the model and why the authors think that the model fails on these scenarios. Moreover, I would have liked to see a description of the Messenger benchmark with a figure (like that of RTFM) and a description of the different splits used to evaluate (in the appendix).
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; A latent transition model is trained contrastively. * results are shown with a fixed set of loss weights which shows robustness. weaknesses:* the method is very complex and hard to reproduce. Even if the code is made available, building on top of this method will not be trivial. * the ablation experiments are done on domains that favour good flow representations and lack baselines (dm control)* the crux of the state representation problem is that it should make credit assignment easier but the hard exploration games are either missing from the experiments or improvements seem marginal. remarks:* i think structured representation is a misnomer here. This is because the image lattice is the weakest form of structure you could have and because structured representations in the literature refer to objects and parts. If the community is to be convinced to embrace such a complicated approach experiments need to be more compelling i should think.<|endoftext|>My main concern, however, is that the results the authors chose to put forward are not as good as I d have liked, which makes me doubt the overall significance of the method:  in Atari, the proposed S³R performs significantly better than SPR in 12/26 tasks, and underperforms significantly in 7/26 tasks;  in Control suite at 100K steps, S³R significantly outperforms DRQ in just one task out of 6, and underperforms significantly in 2, and both are more or less tied at 500K steps;Since S³R seems significantly more complex in terms of implementation and computational cost, it seems unlikely that a practitioner in the field would bother compared implementing it rather than using a much simpler variant. I also don t think it s necessary for the paper to make this claim. Following up on that, could using both Z^w and Z^q as input to the RL head bring an improvement? Notations: the paper (eg fig 2 and the paragraph at the top of p5) seems to imply that a_k is computed from I_k to I_{k+M}, should this be I_{k M} to I_k instead? The paper combines existing self supervised architectures (self supervised flow estimation and InfoNCE like similarity loss) to learn visual features in a rather novel and intriguing combination, but the overall increment in RL performance does not seem to justify the added complexity compared to much simpler counterparts.<|endoftext|>The proposed approach clearly shows promise over prior approaches. The paper is well motivated on a conceptual and mathematical level. 2.It s difficult to conceive how this method could apply to different input modalities (i.e.audio, text, joint positions). The flow based approach is very much tied to image inputs. 3.While there is improvement in quantitative performance over baselines, the margin of improvement is fairly small. The authors propose an approach to self supervised learning for reinforcement learning which is very similar to SPR. Because of this, the generality of the approach is limited to image inputs. However, the results are promising and hold across different domains and agent setups.<|endoftext|>The major idea is to force neural networks to encode optical flow between consecutive frames and impose regularization that the local embedding predicted by warping and a state transition model (given action) are both similar to the extracted embedding from the next frame. Although the current work does not attempt to explicitly model objects, the flow information likely captures some object information implicitly. I find the results generally convincing and the idea appears novel and interesting. However, I am a bit worried why the internal flow loss in equation 4 will help the network extract useful information. If my understanding is correct, the flow for warping internal representation is the same flow as the one for warping image (equation 2). How can we guarantee this the extracted query feature does not simply become a fixed linear transformation of the local image patch? As long as the representation in equation 1 can be learned to predict external flow that satisfies the image warping cost, it seems to be able to guarantee the cost in internal flow to be low as well, even if query feature is a simple linear feature of local image patch. Although the paper has demonstrated good performance in the RL task, it may be interesting, if possible, to provide some visualization of what kind of local feature is actually learned, in the appendix.
Reject; rating score: 5; rating score: 6; rating score: 6; The proposed method can speed up the finetuning by almost three times. The main idea of this paper is to introduce adversarial perturbations to improve the convergence rate of large batch fine tuning of sequence models.<|endoftext|>In this paper, the authors propose ScaLA to speed up the fine tuning of large pre trained transformer language models.<|endoftext|>This paper proposes an adversarial training algorithm to speed up the fine tuning of large Transformer models while retaining strong performance. I vote for a weak accept for the written quality and the rigorous analysis of this paper.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; In the paper, the authors study stable architectures for RNNs. On the theoretical side, the authors present a series of conditions such that a weight matrix of an RNN is contractive. On the modeling side, the authors propose RNN architectures that have contractive weight matrices. The theoretical results seem interesting, although not very surprising. It s unclear to me how the subnetworks are combined. I can only infer from the "recursive construction" in the title and Figure 2 that the resulting weight matrix is a block matrix. Furthermore, the idea of parametrizing orthogonal weight matrices by exponentiating skew symmetric matrices is not novel and has been explored in expRNN [1]. The writing of the introduction section could also be improved. Instead of discussing AlphaGo and modules in evolution, the reader might benefit from a more thorough literature review of the RNN trainability and long term dependence. "Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group." International Conference on Machine Learning. However, the presentation of the proposed model is not clear, and the model itself does not seem novel. Overall, I think the paper needs improvement to meet the acceptance threshold.<|endoftext|>The authors studied contraction properties of continuous time recurrent neural networks. They further showed that a network of provably stable RNNs (net of nets) can be trained to reach competitive performance on several benchmarks, including sequential CIFAR10, even when only connections between modules are trained. StrengthHow to assemble a network of RNNs is an interesting problem. Reporting results from many individual AxB networks seem unnecessarily. (2)	The authors showed performance comparison with other types of networks in Table 1. I think it would be quite informative to show performance of networks where everything is kept the same, except that the RNNs are no longer provably stable. (3)	It would also be good to know what happens if all connection weights are trained, not just the connections between modules. Does the performance actually decrease despite having more parameters? (4)	The provably stable part is kind of separated from the training modular network part. Overall, this is an interesting paper that takes a less common approach to RNNs: provable stability and net of nets. The results are at places more difficult to read, but overall it is clear.<|endoftext|>The submission proposes new theorems showing the stability of a class of RNNs. ## Experimental claimsThe experimental section, while interesting, seems to lack a main takeaway. Also some very dubious choice of reporting in the table. * The authors claim that for the result shown on the Tab. 1, they run the Perm MNIST trial 4 times and the results fall between 96.65 and 96.94. I was frankly shocked to find that they choose to only report 96.94 on Tab.1! (If one were to only report the best case, one could get much better performance than is achievable on average by running the experiment many many times.) * Some claims are backed up by only a single data point. For example, the claim that increased modularity benefits performance to some point is only backed up by the fact that 44x8 performs better than 22x16 in Sec.3.2.1.To draw a significant conclusion and demonstrate a trend, the authors can perhaps look at 50x7 and 39x9. In 3.1 increased size makes the network better monotonically but in 3.2 it is inverted U shape. What is the conclusion to be drawn here? For example, the discussion of stability and contraction are clear but then in the paragraph at the top of page 3, the authors use the symbol g for two different thing in the same paragraph. * Theorem 7 should be slightly reworded so that it is clear that the first inequality is a condition and not a statement (this is rather obvious in hindsight but for a new reader it is very confusing). * I would suggest an overall re read of the paper to maximize readability. The theoretical results are novel and noteworthy. Unfortunately the experimental results lack a clear conclusion and at times do not follow best practices (i.e.reporting only the best run out of many).<|endoftext|>This paper is primarily a theoretical contribution to the construction of assemblies of recurrent neural networks. Then, using fixed RNNs generated according to these constraints (leaving the connections between them as antisymmetric learnable parameters), the authors show that their sparse combination network is able to achieve SOTA performance on sequential image classification benchmarks with far fewer learned parameters and the previous stability guarantee. The authors first thoroughly investigate various permutations of their modular sparse combination network framework (# RNNs vs size of each using absolute value weight constraints) and do another investigation of their alternative SVD weight constraint network (which doesn’t perform as well or train as quickly). There has been a lot of recent work in networks with many individual recurrent components, such as the aforementioned AlphaGo or the more general recurrent independent mechanisms (RIMs) framework, but for the most part, they rely on intuitive explanations and empirical results over theoretical guarantees. The proofs in the appendix are well done and easy to follow, given a sufficient math background. Weaknesses:  This paper is very dense and difficult to follow. The appendix is a mandatory read as are some of the references. I also feel like my familiarity with AlphaGo and other methods gave me more of an insight into how this would help in practice than the actual paper did. As much as I liked the empirical results that were provided, they’re all of a kind: sequential image prediction. I would have liked to see at least one application in a different domain (NLP, RL, continuous control, etc). Overall, I would accept this paper. Although it was difficult to follow and required a lot of consultation with the literature, I do ultimately think that this is a direction that DL algorithms are going in and that the theoretical and practical results from this work could be quite powerful.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper aims to improve the learning effectiveness and data efficiency for long horizon control tasks under sparse rewards with demonstrations. Specifically, instead of selecting goals from the learned agent’s rollouts, the new technique only selects goals from the successful rollouts (and demonstrations). Strong Points+ The final task used in the paper (“bimanual insertion”) is challenging enough to convince readers of the authors’ claim that the proposed algorithm is a meaningful improvement on learning long horizon control tasks. + The empirical results are comprehensive and convincing. + The paper is well motivated.<|endoftext|>This paper contributes a method called HinDRL, which tackles the important problem of sparse reward robotic RL tasks when supplied with demonstrations. By combining the goal selection strategy with goal conditioned rewards which are either engineered or learned through temporal time consistency, HinDRL is demonstrated on simulated sparse reward robotics tasks where it achieves improved final performance and demo efficiency compared to HER. The strong points of this paper are:A more comprehensive ablation study of topics such as encoder quality, effect of number of demonstrations on method performance, and goal distribution sampling, which are often important but unclear aspects of implementing these types of goal conditioned methods. Additional baselines would make the empirical results more convincing, for example, to Reinforcement Learning with Imagined Goals (RIG). Additional questions:How are the number of relabeled goals for each task selected? My recommendation currently is to reject the paper, because I feel that the experimental results seem to indicate that the engineered features are contributing to the difference in performance and thus creating an unfair comparison, and the novelty of the goal selection method is not currently apparent to me.<|endoftext|>The experiments are quite thorough, but there are some missing details that seem critical to evaluating the performance of the method compared to prior work. This paper studies long horizon manipulation tasks given sparse rewards and a few demonstrations from a hindsight relabeling based approach. Then, the rewards are relabeled in hindsight based on a thresholded distance in the latent space. Strengths:  The paper studies a difficult problem in robotic manipulation, and shows results on tasks of varying difficulty. Leveraging demonstrations as task relevant goals is intuitive and straightforward. It would be nice to include a sensitivity analysis of this hyperparameter.<|endoftext|>This method is inspired from HER + LfD methods. The results are also well analyzed and HindRL shows strong performance when compared to HER and other RL + LfD approaches. Weaknesses: I think the main weakness of this paper is the lack of diversity in the empircal evaluations. This approach is evaluated on a set bimanual cable insertion tasks, where it outperforms prior approaches such as HER and is able to learn with fewer demonstrations than RL + LfD.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The idea of the proposed method seems interesting. Indeed, the paper shows that the exploration of gradient based search approaches allows to select policies more efficiently in a huge search space. The paper is clear and well presented.<|endoftext|>Wouldn t it be misleading for the readers as they would expect to see a Class wise algorithm that outperforms all other methods? Although the paper has some great (but minor) contributions in terms of the novel augmentation for EEG data, there are still many aspects in which the paper can be improved. The work done by authors in the field of data augmentation for EEG data is novel and interesting. I think this would help to understand the novelties of (C)ADDA over DADA.<|endoftext|>THis paper proposes a special version of AutoAugment to search class wise data augmentation policies for EEG data. Weakness:1) This paper is very hard to follow. I think it would be really great to have a method/approach section to talk about the proposed method. This is an interesting paper in machine learning and healthcare. However, the paper requires more work to improve writing quality.<|endoftext|>Pros:+ This is an interesting work. + The authors designed some data enhancement methods for EEG. Please clarify it. + Although the author focuses on solving problems in the field of data enhancement. I hope that the method proposed by the author can be compared with some of the latest sleep staging methods, such as [2] and [3].
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The authors present a method to incorporate constraints into the output of learnable PDE models. I think the method is sound, and up until the experimental section the paper is well laid out and straight forward to read. They achieve this by representing the PDE solution in a basis, as is common for the variational method (e.g.pseudo spectral method, and finite element method). They cover point wise, differential, and integral constraints. To implement the constraints, they have two methods: ‘soft constraints’, whereby a constraint breaking penalty is applied to the neural network output and ‘hard constraints’, whereby the output of the network is projected on to constraint satisfying solutions. In most of the experiments, comparisons to other methods in the literature are missing and it is therefore hard to gauge the significance of this work. The notation and underlying assumptions here should be explained more precisely. Advantages  The submission is written well, is clear up until the experimental section. The experiments show that the addition of constraints help to learn a model that does not violate said constraints. The central method is sound and simple enough to be reimplemented. That said, I think some experimental details are missing and I could not reimplement the experiments. Queries  Equation 1: can dynamics F also explicitly depend on t?<|endoftext|>The paper introduces and studies several variants of introducing soft and hard constraints into learned models of PDEs on unstructured meshes. In general, the paper is easy to read. As the paper mentions in the introduction, using soft constraints (often on regular grids) in training is an extremely common practice in physics prediction papers. This paper differentiates itself from these methods by a) using unstructured grids, and b) providing a formalism for solving hard constraints. And second, hard constraints are presented as one of the contributions, and the description spans quite a bit of the method section. However as the authors state themselves, their method for solving hard constraints is only feasible for very small systems, and is hence not used for any of the paper s main results (only for a minor example in the appendix). Similarly, the paper introduces mitigation techniques to make quadratic basis functions (PWQ) work, which turn out to perform strictly worse than a simple linear basis in all settings. I d encourage the authors to work further on the method (e.g.make hard constraints work for nontrivial systems), and choose good experiments to showcase the strengths of constraints on irregular grids. I also think constraints for adversarial methods that the paper touches on could be worth exploring further.<|endoftext|>The paper proposes a two folded method to enforce constraints of different natures (differential, integrals….) on a statistical model learned from physical data. The method can be summarized as follows: Learn the model $F_\theta$ that fits the data constraints (regression, gan etc....), hence the solution at “grid points” $\mathbf{u}$. (linear, quadratic, etc..) to interpolate between the prediction on which can enforce specific constraint, then a fortiori constraining the learned $u$. The experiments are well conducted and present various use cases useful for the whole community. However, the main weakness of the paper is the clarity in the presented interpolation method. How well can the interpolant $u_f$ approximate the original to the problem ? If  not, then the interpolation simply concerns the $\phi$ part ? Remark on the presentation:Despite interesting ideas, there is room for improving the presentation and the clarity of the paper. a) Since the main focus of the experiments concerns “soft constraints”, I suggest the authors develop the section treating the soft constraint components (Perhaps including an algorithm to show practical use) and how constraining $u_f$ constrains the learned $\mathbf{u}$ (or more specifically how the losses on $u_f$ impacts the $\theta$ of $F_\theta$), by detailing explicitly the link between the interpolation and the learned solution. The experiments are well conducted but the method’s presentation lacks clarity (see main review) notably on the estimation of the interpolation coefficients and their link to the learned solution function?<|endoftext|>This paper proposes a method to enforce physical constraints in deep learning models. It provides a nice summary of local, differential and integral constraints, and frames them in a Lagrangian setting. This is intuitive to me, as the physical constraints could nicely stand on their own. The paper presents a series of tests which follow a somewhat unintuitive order. Not much detail is given, and the test only confirms that the constraints give very similar results. While the first CH case is very simplistic, the latter cases contain interesting setups and show some interesting results. I would recommend to rephrase this. Also, stylistically, citations shouldn t be used as nouns, and the related work seems to brief to me (e.g., generative models for physical problems are missing). It is definintely an interesting direction, but in its current state the paper applies existing methods from regular grids to Lagrangian discretizations. This is a good idea, but not a very fundamental step forward. The presentation of the work could also be further improved.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; However, these questions should be easy to address in the rebuttal. The submission proposes a method for transfer in reinforcement learning building on estimating a small set of factors describing a system and modelling the agent as a dynamic bayesian network. The method is evaluated on variations of a cartpole and a pong domain (both from images) and evaluated against a set of recent, competitive baselines. The paper investigates the question of transfer via identifying a small number of changing variables to maximise data efficiency. First, random data might not cover enough of an environment s state space such that the estimated model is not accurate for later stages of policy training when a different part of the state space is visited. In particular the second point could be investigated by iterating between model estimation and policy training. In both cartpole and Pong, occlusions do not occur. If not, it will be helpful to emphasise this aspect to describe which aspects are unobserved. While overall well written, parts about equations and the general model are hard to trace and a visual representation of various parts would be helpful. A strong submission on improving transfer learning which leaves a couple of open questions regarding its evaluation.<|endoftext|>This paper proposes a method that learns structural relationships between variables of the RL system so as to be able to adapt to changes across domains, and learn a new policy in a new domain with few samples. I know it is referenced later in the paper, but it should also be discussed in the introduction when other representation learning methods (such as (Zhang et al.2021)) are being discussed. Some more details below. Why are they conditioned on different parameters? I think this is an interesting paper, but it falls short in the empirical evaluations. I look forward to reading what the other reviewers think, as well as the authors  response.<|endoftext|>Instead of implicitly updating the policy using data from the source domain, learn a particularly structured latent model and the elements of variation and learn a policy that performs pretty well on some set of the elements of variation. I think this is a good paper but could benefit from some clearer writing as discussed in the clarity section below. The ablation on the masks is good and helps demonstrate that all the components of the method are needed. It seems to be interpretable because you know what the factors of variation are a priori; would it still be interpretable if you didn’t? On page 2 in the final paragraph you use the notation s_{2,t} but this $2$ index is not defined at this point in time.<|endoftext|>The method is based on learning a latent representation with "domain shared" and "domain specific" components. The method is evaluated on modified versions of Cart Pole and Pong domains. The authors test different settings where parameters of the environment as well as rewards functions are changed within source domains and the target domain. The method is compared with latest transfer learning methods and the results show that AdaRL performs better compared to other methods The paper is proposing an interesting method for an important problem. The writing and the presentation of the paper is very well. The evaluated domains could be more complex, otherwise the results support the claims of the authors.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The paper is well written. 2.The paper proposes a certification framework against poisoning attacks in deep reinforcement learning, which is non trivial. It advocates two certification criteria: per state action stability and lower bound of cumulative reward. 3.For each certification criteria, it provides bounds for different proposed aggregation protocols. 4.In addition to theoretical results, the paper also presents numerical results. I appreciate the authors  providing numerical results and the theoretical studies. There are a few related works on certified robustness in DRL.<|endoftext|>This paper studies the problem of certifying a policy learned via an offline RL algorithm is robust to poisoning attacks. The authors propose two certification criteria: per state action stability and lower bound of cumulative reward. Generally, the paper was well written, although there are some organisational problems I would like to see addressed. How were the trajectory lengths chosen in Section 4.2? 9.Could the authors comment on the relation between this work and [1]?<|endoftext|>Two criteria including per state action stability certificate and cumulative reward lower bound certificate are proposed. Based on these two criteria, the paper designed the COPA, a general certificate framework, which provably achieved certain level of certificate. The authors should discuss them in the related work. (2).The applicability of the COPA certification framework is limited in the sense that it only applies to poisoning attacks where the attacker poisons a small fraction of training trajectories.<|endoftext|>The authors propose a framework named COPA to certify the number of tolerable poisoned trajectories. There is only the result of the computed reward lower bound in Figure 2. As far as I know, this is the first method in certifying both per state action stability and cumulative reward against offline poisoning attacks in RL. I would like to see the authors focus on the most interesting one in the main paper and provide detailed explanations, then put the other two variants to the Appendix. This is the most important result in the paper, but the presentation is really confusing.
Reject; rating score: 3; rating score: 3; rating score: 6; The authors propose a variant of Wasserstein for distributions supported on different Euclidean spaces (e.g., different dimensions). + Weaknesses:It seems that the contribution is quite incremental. It seems that the main problem is that supports of distributions are living in different spaces. In case, one can learn the embedding as in Alaya et al., 2020 (e.g, about \phi, \theta embedding)   which is also used as the main part of the proposed approach, the remaining problem is the optimal transport between distributions supported in some "latent" space. 2.As in the setting, distributions are supported in different spaces. 7.What are the stopping conditions for GW and RISGW in experiments?<|endoftext|>Unless the authors can convince me in the rebuttal that they have something fundamentally new in their paper, the paper does not appear novel to me. Do the authors also have the experimental results for real data? The HWD is a direct application of the idea of [1] Nguyen et al.[2020].The formulation as well as the theoretical results are also direct from this work. Both the formulation of HWD and its theoretical results do not appear to be novel.<|endoftext|>The HWD is built on ideas from the sliced Wasserstein distance and measure embedding : the two distributions are projected into a one dimensional space, in which the classical Wasserstein distance (closed form in 1D) is computed. The HWD is a novel discrepancy for distributions on heterogeneous spaces, based on slicing and embeddings, and in this sense, is not very original. A precise justification of the chosen formulation of the problem in this paper would be appreciated. Therefore, HWD is a computationally interesting tool to compare probability measures supported in spaces of different dimension.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; The codes are not availabel, it is difficut for others to re produce the results. This paper presents a neural relational inference model with node specific information. This paper is well written and the idea is clear. Experiments on real world datasets validate the merit of the proposed method.<|endoftext|>The paper introduces the concept of node specific information (NSI) to model that nodes in a graph may have private information that other nodes cannot have access to. The paper uses Neural Relation Inference (NRI), a framework published in 2018 based on variational inference, to uncover the hidden relations of nodes in the graph. However, it is not entirely clear to me if the private information can be propagated to other nodes (question 1) and if the model can even be applied in the envisioned scenario (question 2). I still think the applicability of the idea may be problematic, but I think the idea of NSI is sufficently interesting and novel to be accepted to this conference. The paper is well written. It is easy for the reader to understand what the contributions of the paper are. Weaknesses:  The paper argues that in many real world examples, "there is a set of hidden features that affect the way entities interact with each other." However, it is not the hidden features that affect the interaction of the entities, but only the public nodes, which are affected by the hidden features. It would be great if the authors or other reviewers could clarify this point.<|endoftext|>**Summary**: This paper introduces a neural relational inference model that makes use of the hidden features of each node in a variational inference framework. Specifically, the hidden/individual information is modeled as private node in the graph. Importantly, the task assumption made by the authors is that these individualized features cannot be observed by other entities. ## Weaknesses* The task assumption of the paper does not look straightforward to me. Is this a well recognized setting? * Given the existing work of NRI and the variational inference framework, the paper is not clear about the contributions. The concerns I have are about the assumption of the approach and unclear novelty points.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper addresses the problem of deriving the correct answer when contradicting examples are presented to the model. The dataset, ContraQA is built on SQuAD, and it contains contradicting contexts produced by humans and neural models. To study how QA models behave with contradicting examples, this work evaluates the performances in a scenario where the correct and the fake contexts are presented to the model. 3)	The setup proposed in this paper is not simulating a real word scenario, as instead claimed in the introduction. Thus, the distribution between real/fake presented in this work does not reflect a real scenario. Contribution of the paper.<|endoftext|>This paper releases a new dataset with human and machine generated contradictory contexts for QA pairs from SQuAD 1.1. The authors experiment using BERT, ROBERTA and SPAN BERT and report a drop in performance in both experimental settings. However, I am not sure I am clear about the goals of the paper   I elaborate further in the rest of the review. This is just another way of distracting a QA model but without having any reason for it to believe the information is "fake". 3.What could perhaps have been interesting is to also see if a model could "detect" contradictions and says, that it should not answer. Why is this surprising?<|endoftext|>The authors studied how contradictory information affects the accuracy of QA systems. It s well written and provides a new dataset. They also designed a BART FG model to automatically produce such context by replacing spans with model generated value. Strength.It calls our attention to a very realistic problem, that misinformation could affect the QA systems, which a lot of people may blindly rely on. The experiments are very detailed. The paper itself is well written and easy to follow. Reviewer is very curious about what the classifier had learned, which is not discussed in the paper. How did the authors make sure it s contradictory?<|endoftext|>**Strengths**:  To my belief, this is the first work that sets up contradicting contexts for closed domain QA. Another interesting baseline to compare with ContraQA would be SQuAD + N Most similar (tf idf) context passages (instead of random). 2.The fourth guideline for fake context creation by humans: "The modified paragraph should be fluent and look realistic, without commonsense errors. If so, how do you expect the miss information at source to play a role in confounding the QA systems? However at the same time, it does leave things to be desired.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper proposes to use a conditional GAN as a more expressive model of thedenoising step in a denoising diffusion model, allowing to reverse the diffusion(sampling) much more efficiently, while maintaining the high quality of diffusionmodels. The authors start by showing with a toy example that the denoising conditionaldistribution can be heavily multi modal for large diffusion steps, highlightingthe limitations of the typical Gaussian model, which can only be assumed forvery small steps, thus leading to an expensive multi step sampling process. Theauthors overcome this by using a conditional GAN to model the denoisingdistribution under large steps, and experimentally show that this can indeedmodel very multimodal distributions. While taking this idea to the extreme would lead to thestandard one step GAN, interestingly the authors show that their methodoutperforms standard GANs, and the provided justification is that in themulti step approach the distribution mapping in intermediate steps is lessaggressive than direct Gaussian to data distribution, and thus easier to learn. I think the proposed idea is clever and very well executed.<|endoftext|>**Strengths** I think this work and the proposed combination of adversarial and likelihood based training is a natural and novel extension of DDPMs towards faster sampling times. In summary, the paper provides a nice combination of adversarial and likelihood based training and, in my opinion, represents a natural and novel extension of DDPMs towards faster sampling times. The results on CIFAR 10, CelebA HQ and LSUN Churches are competitive with previous state of the art methods. The paper should address previous work that also attempts to solve the trilemma by reducing the number of steps required for a trained DDPM model, e.g., DDIM [1] or GGF [2], and compare with them. Do the results presented in Tab. How does the model perform on other typical but more complex synthesis tasks such as LSUN Cats or class conditional generation of ImageNet? Just out of pure interest: Have you tried using the UNet as a discriminator as well (similar to [7])?<|endoftext|>The main question is whether such an approach can really combine the advantages of the different models. Thus, I recommend acceptance. The main reason why diffusion models require a lot of iterations is because they model each of the steps of the reverse diffusion process with a Gaussian distribution, which is only valid for very small timesteps/a large number of steps. The main evaluation on CIFAR 10 indeed demonstrates a favorable combination of sample quality according to FID and IS, sampling speed and data coverage as measured by Recall. The paper motivates the problem it is addressing well through the mental image of a trilemma. Approach and evaluation  The approach is relatively simple and provides good results. In fact, a change in this weighting has been one of the break throughs in [Ho, 2020] that lead to the current success of this class of models. The choice of this weight in the presented approach is not discussed. Choice of generator architecture: While it is nice to see that a similar model as used for diffusion models also works as the conditional generator in the presented setting, Tab.<|endoftext|>This paper proposes to break the Gaussian assumption in denoising steps by parametrizing it with a multimodal conditional GAN. It is currently non saturating GANs. Does the proposed approach also work better with advanced conditional GANs? 2) The results of FID should be specified with the evaluation dataset or both (FID (train) and FID (test)). The minor issues are in the experiments. Therefore, I recommend to accept this paper.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper addresses the problem of learning an opponent model in a multi agent environment. The paper proposes an algorithm, called MBOM, that operates by simulating the reasoning process of the agent in the environment and considering different types of opponents. The process is recursive and uses the environment model to predict the policy improvement of the opponents. This raises the issue of what happens in situation when there is more than one opponent. It would have been useful to have at least one application with multiple agents to understand how well the proposed algorithm addresses the multi agent opponent modeling as claimed. Some explanations would help. 7.The paper does not include any theoretical results, and, given the comments made above, it is hard to understand when the method will work and how well it will work.<|endoftext|>This paper proposes a model based opponent recursive reasoning method for MARL problem. The recursive reasoning takes multiple levels of estimation to generate a number of opponents. Based on this, the value is not exchangeable. Also, when there are more than one opponents, would the recursive reasoning becomes more complicated, because when use a model to generate rollouts, the best response rollout should be selected by considering other opponents  actions? Moreover, as the number of levels of IOPs and the length of rollouts k are important factors in the proposed method, these hyper parameters should be studied with sufficient empirical results, which are absent from the current contexts. However, these results are not considered in the current experiment section.<|endoftext|>The paper proposes MBOM, a scheme for multi agent reinforcement learning, which flexibly adapts to opponents with different or unknown learning abilities by relying on recursive reasoning. However, the policy inferred using conditioning on the best anticipated move of the opponent is  is for Alice to choose the second bar withprobability of 0.55, for the expected reward of 1! The paper address a significant problem of multi agent reinforcement learning in the setting when agents are 1) heterogeneous 2) learn/reason about each other and adapt their policies based on observations of other agents  behavior. The agent behavior may seen reasonable in the complicated settings of the proposed domains, but in general I suspect that the algorithm is not sound. Based on the above, I would appreciate a formal theoretical treatment and an empirical evaluation to be improved in a revised version of the paper. An interesting line of research but theoretical analysis and empirical evaluation should be improved in a revised version. 1) The empirical evaluation is not well aligned with related work. It is easy to show that it does not always converge. This is a competitive setting.<|endoftext|>Agents learning in systems with other agents that may be simultaneously learning presents the original agent with a dynamic learning problem. One such solution to this problem is to design a learning algorithm that accounts for how the other agents may update their policy from the shared experiences, effectively eliminating the aforementioned problem. Their proposed algorithm, Model Based Opponent Modeling (MBOM), learns parallel opponent models that are trained using various depths of recursive reasoning. Then, when played against an opponent, the agent can select the most likely opponent model (level 0: non learning opponent, level 1: learning opponent, level k: k 1 level recursivly learning opponent). Simpler baselines should be included to help understand the benefit of the additional sophistication. Train a BR directly against a single opponent, mixture of the opponents, etc.. The proposed major contribution of this work is a learning aware learning algorithm that is independent of the other agent s learnin algorithm. This isn t true. This is not generally true. The effectivness of a method s generalization to held out opponent policies depends on how well the fixed set of agents covers the strategic landscape of the game.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper deals with end to end learning of heuristics for combinatorial optimization problems. 1.Generalisation is only illustrated (and claimed) with respect to the size of the instances. It would be interesting to see the results on other distributions shifts (e.g.applying EAS to a model trained on TSP100 on instances of TSPlib)1.<|endoftext|>The paper proposes a new method of updating deep neural networks forcombinatorial optimization problems during search using reinforcement learning. Someexplanation of this would be helpful for the reader to understand what exactlyis going on there. Theevaluation is thorough and fair, the results are convincing. This is a goodpaper that should be accepted.<|endoftext|>This paper studies deep learning methods for solving combinatorial optimization problems. For the scheduling problem, the improvement over active search is a bit more modest. In Section 3, it would be helpful to clarify what exactly an “action” corresponds to in this setting.<|endoftext|>The paper builds upon Bello et al.(2016) on using reinforcement learning to generate solutions for combinatorial optimization problems (e.g., TSP). The novelty of the paper is to optimize only a subset of the model parameters. However, I feel that the results of the experiments should be reported in greater detail, i.e., to compare with the original active search in different performance metrics such as memory and CPU time usage.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper applies the Integrated Gradients technique to the problem of creating adversarial examples in the black box setting. This produces results that are better than the state of the art methods for most of the networks tested, and it can be used in combination with those existing methods to produce even better results. Strengths:  The application of the Integrated Gradients technique to produce adversarial examples is novel  The evaluation of the method is comprehensive, and demonstrates state of the art performance in almost all casesWeaknesses:  The technical novelty is not enormous, in the sense that it consists purely of a straightforward application of an existing technique, and the method section mainly consists of a justification of why the method makes sense in this domainThe paper is overall quite good, with thorough experiments demonstrating good performance.<|endoftext|>The authors propose a simple technique named Transferable Attack based on Integrated Gradients (TAIG) that combines all these three approaches. Unlike the existing systems, which leverages other methods by adding additional terms to the objective function, TAIG integrates them into a single objective. The authors propose a simple but effective technique for generating adversarial perturbations in a black box setting. In addition to this, the core component of the proposed technique (the Integrated Gradients and the Random Path Integrated Gradients) can be used to improve the existing techniques. I would encourage the authors to additional experiments to justify the claims made in the paper. How imperceptible are the examples generated by the proposed technique? For instance, even though most black box adversarial have higher success rates, they generally forgo imperceptibility. How good is the proposed technique in evading the recent class of adversarial example detection methods?<|endoftext|>The experimental results are good. This paper proposes a novel algorithm to generate adversarial examples, and the results are good. 2.The idea of updating along the integrated gradients is new and has theoretical support. The most important weakness is the high computational cost. 2.In section 3.3, the authors claim that sign(IG(f,x)) approximates sign(\nabla f). Will the similarity between sign(IG(f,x)) and sign(\nabla f) be higher if more sampling points are used to calculate IG?<|endoftext|>This paper studies adversarial attacks to deep neural network. To achieve good attack performance, this paper proposes the attack to the integrated gradients and the method (TAIG) is shown to be highly transferable to other black box models. However, I have some concerns about this paper:1. 3.There are still some extant works that generated the adversarial examples based on the sensitive feature in the image. 6.There are some typos and grammar errors in the paper, it is better to revise them. The paper proposes the integrated gradients and the method (TAIG)  to attack black box models, which has shown good performance. But there are still some place to improve.
Reject; rating score: 5; rating score: 5; rating score: 5; However, if the paper is interested in machine learning conference, some more contribution of machine learning novelty and more quantification of the methods can strengthen the paper. The paper provides clear description for the audience to understand the data and the problem (e.g.Fig 4 and Fig5). Some questions: (1) Although the three modules framework is interestingly designed, it seems to me that the contribution of machine learning novelty seems not very clear.<|endoftext|>The design choices overall seem sensible, and the results appear to be strong. However, I have two main critiques, and a number of minor comments as well. Without such comparisons, it s impossible to evaluate the practical impact of this work. I think this means that V is the number of peaks, but then it s also the number of intensities. The fragmentation inference task in Section 4.4 is not well described.<|endoftext|>This model takes the experimental mass spectrum and the corresponding fragmentation tree structure as the input for the transformer to predict the SMILES of the molecule in an end to end manner. The strength of this paper is that they incorporated extra chemical knowledge of the spectrum via its fragmentation tree to enhance the input feature. With the current paper, I this it is marginally below the acceptance threshold.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper studies valuation problems for cooperative games. While it is hard to argue (both in theory and in practice) that one valuation method is better than the alternatives, the empirical results seem to be reasonable. The authors then present a gradient ascent algorithm to compute this decoupling. Classical valuation criteria like the Shapley value and the Banzhaf index can be recovered as special cases or modifications of the algorithms iterates.<|endoftext|>This paper introduces some very interesting ideas about the use of EBMs and mean field VI in the context of player valuation for cooperative games. Is the main practical impact, then, proposing the variational index as an alternative to Shapley/Banzhaf values for valuation problems? **Applying existing Shapley value approximations to the variational index. I have a couple questions/comments that I hope the authors will consider for improving the paper. How can that be, given that the problem is non convex (stated earlier in the paper)? **Cooperative game theory in ML.<|endoftext|>This paper studies valuation problems from cooperative game theory. The goal is to use this function $F$ to define an importance vector $\phi(F) \in R^n$. They then phrase the problem of defining an importance vector $\phi(F)$ as a decoupling problem. Strengths:  The authors show that one can derive the Shapley value and Banzhaf index using one step of gradient ascent using specific initializations. 4th paragraph of Section 1: At first, I was confused about what the “valuation problem” is.<|endoftext|>The paper presents some interesting theoretical connections, but does not provide sufficiently compelling empirical results showcasing utility for applications in ML. The present paper presents a probabilistic treatment of cooperative games, and shows that two classical valuation criteria can be seen as a one step factored approximation to maximum entropy solution to the game. The writing is accessible, but can be improved to better motivate the subset/feature selection applications, especially for an ML audience, and provide more elaborate intuition early on for the "decoupling" approximations that the classical criteria seek to compute. Pros:  I find the observation that classical criteria such as Shapely value can be seen as one step "decoupling" approximation to the maximum entropy probabilistic assignment to be quite interesting.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a novel transformer based detection algorithm (FP DETR) that could benefit from large scale pre training for model robustness and generalization on small datasets. Overall, I like the paper but I feel hesitant to accept this paper at its current status . However, I found the current experimental protocol is not solid enough to support some of the claimed contributions. Pros:  The paper is well written and easy to read. The intuition behind the algorithm design is sufficiently discussed. I like the idea of decreasing the model discrepancy between upstream classification tasks and downstream detection tasks through an encoder only transformer. What’s the performance of Deformable Tranformer with Encoder pretrained？  The ablation analysis on the lite version (Table 3) is not as strong as one in the base version. The paper proposed a novel architecture to empower a transformer based object detector with smoother pre training and fine tuning paradigm.<|endoftext|>This work proposes a new transformer framework for detection task by masking better use of features in pretraining phase. The proposed framework achieves competitive performance and becomes more robust and generalizable. Authors conducted ablation studies to verify the different settings. In Table 7, it shows the FP EncDec with 300 tokens performs better than that with only 1 token on pre training and fine tuning tasks. In table 2, is it possible to have an ablation study in which we have FP DETR with ResNet 50 backbone? At the current stage, I prefer to accept this work.<|endoftext|>This paper argues that recently developed detection transformers didn t employ pre training on transformer layers and not benefit from pretraining. Strength: + Encoder only transformer design without the need of extra visual backbone+ Lower parameters compared to concurrent worksWeakness:  The effeteness of pre training is not well justified. The paper claims "large scale pre training on ImageNet helps FP DETR learn more generalizable representation." However, the author only conducts experiments on imagenet 1k, which barely counts a large scale pre training. I believe this paper has some merit and worth accepting for a poster. Hence, I have raised my score.<|endoftext|>This paper proposes a new method of pre training transformer based object detector (FP DETR). Experiments on MS COCO show that the FP DETR achieves competitive performance and generalizes well on small scale datasets. According to the section 3, the authors first pre train the encoder part on the ImageNet. However, the class token domain has a large gap between the visual domain with visual contents. As a solution, the authors propose a task adapter (which is an self attention layer or a Bi LSTM) to mitigate the gap. In such sense, it would be confusing whether the task adapter is used to better model the object interaction, or to adapt the pre training image classification task to object detection task. According to the findings in section 4.2, pre training on ImageNet 21k has lower performance than that from ImageNet 1k. It may need more efforts to solve the scalability problem. However, there is one novelty not fully verified. Besides, the method may have some problem in scalability.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; **High Level Overview**This paper proposes a neural rendering technique to learn how to estimate an implicit surface and its UV parameterization from a set of 2D images. This paper does a good job demonstrating the applicability of the proposed neural rendering technique to the problem of document unwarping, both in the formulation of the problem (learning the UV grid *is* unwarping the document) as well as in the empirical evaluation (OCR). In other words, while textures can be rendered an implicit surface via IDR, no explicit surface parameterization is created, so the appearance is fixed. Additionally, although document unwarping is an interesting application of this neural rendering technique, it is also very niche. **Simplified Problem**This paper shows the proposed approach to work quite well for document unwarping. It would be helpful for the authors to articulate how this method would generalize beyond documents unwarping and what potential pitfalls may exist in trying to do so. Additionally, it is not entirely clear what to make of an improvement of 0.05 MS SSIM. Runtime improvement should not be considered "out of scope of this paper" (Sec 12, supplementary). *Qualitative Results*The qualitative results highlighted in Figure 4 are not particularly discriminative. Figure 5 does a better job showing where the proposed approach outperforms DewarpNet. **Other Notes*** The OCR evaluation (Sec 10, supplementary) is very relevant for the document unwarping task and is the quantitative result that best distinguishes the proposed approach.<|endoftext|>The paper presents a multi view method for undistorting documents. The method significantly outperforms SOTAs in the experiments. The method seems to be primarily based on IDR with following similarities:  3D surface is represented by a Signed Distance Field (SDF)  Appearance is learned from a neural network that maps 3D surface geometry (location and normal) and viewing angle to radiance values  The networks are trained with an overall loss including an RGBA rendering term and an Eikonal term for regularising SDFIDR can handle specular materials but at the cost of introducing so called  shape radiance  ambiguities. This means geometry cannot be possibly recovered due to view dependency and is only regularised by neural nets  inherent priors (c.f.NeRF++).In this paper though, it seems document pages are all diffuse and I think it would make more sense to drop the view direction term $\mathbf{v}_p$ from Eq.3.This would disambiguate geometry and could improve robustness. I like the overall idea of this paper and the results look really convincing. However I would not recommend acceptance to ICLR given its significant overlap with an existing technique (IDR).<|endoftext|>Strengths  The paper is well structured and clearly written which makes it easy to follow and reproduce the method. It is not clear why the authors did not include the "best view" setting of the proposed method too, it would be a more fair comparison to the "best view" of DewarpNet, as now both methods would be able to use the oracle. Was there a reason for not including this comparison? Could the authors then first align the GT and the unwarped image by optimizng for the best translation and then evaluate the LD and MSSIM to show that the drop in precision is indeed due to the transformations? Could the authors comment on what happens if the the terms L_E, L_F, L_G are turned on from the beginning and the term L_{uv} is not used at all? The paper introduces a creative way to combine an implicit shape representation with parametric functions and a neural volumetric renderer to deal with document unwarping. The paper is written in a clear way, I believe it should be easy to reproduce, and the qualitative results seem compelling.<|endoftext|>This paper presents document unwrapping based on optimization of neural networks. Similarly to NERF, the target example is approximated by training the MLPs using differentiable rendering. The proposed method is reported to be par with the baseline method (DewrapNet) in quantitative comparisons and outperformed the baseline in qualitative comparisons. Thus, I am not sure the comparisons are fair. Based on the above reason, the practicality of the proposed method is not clear. I think this paper is interesting for an experimental study, where implicit differential rendering is applied for document unwrapping. My concern is that the paper deeply depends on the supplemental document. Overall, I would like to positively evaluate the trial of implicit differential rendering for document unwrapping, and showing detained results in supplemental document.
Reject; rating score: 5; rating score: 5; rating score: 6; The paper considers the behavior of a series of algorithms for learning in a batched bandit setting,where parameters are arm/action specific, rather than shared across arms/actions. However, to my knowledge, these formalisms are quite new to online settings like bandits, thus making the presented approach appealing. I believe [1] could be a great basis for such experiments. However,I believe the central imputation idea gets lost in the many variants considered. However, I believe that there isn’t sufficient clarity on how the actual imputation is accomplished, and what is the impact of such imputation.<|endoftext|>This paper addresses batched contextual bandits, with a fixed set of actions, and separate unknown parameters for each action. For the approaches with and without sketching, the authors derive a $O(\sqrt{dMT})$ regret bound, where M is the number of actions, and d is the dimensionality of the parameter vectors, which broadly matches what is expected in the non batched setting. MAJOR COMMENTS ON CONS:My main issues are that the explanations of the main algorithm are not sufficiently detailed to convince me that it is as effective as suggested. This is non trivial work. Are there infinitely many solutions and a particular choice of \eta parameterises a specific one? If this happens to be a good action there would be a concern that this could lead to sticking in suboptimal actions and incurring more regret than algorithm which does not impute on the basis of biased estimates.<|endoftext|>The paper considers the contextual batched bandit setting and introduces the idea of imputation utilizing the non executed actions in each batch. The introduced sketching error is further tuned by appropriately setting the sketch size to provide a sublinear regret of O(\sqrt{MDT}).
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; rating score: 8; This is interesting, and potentially useful for practitioners. In my opinion, the main issue with the current paper is that some theoretical arguments are presented to attempt to justify the approach, but the mathematical content of these results are lacking. I believe the paper will be much easier to understand if focused entirely on empirical results and studying their implications. Below, I outline some issues that I have trouble understanding while reading the theoretical parts of the paper. Can the authors comment on what this particular definition means? 2.Proof of theorem 1: It is very hard to unravel what is being proved here. The overall logic appears to be (and the authors may wish to clarify, if this is not the case)   1. 2.Second, even if we take the definition 1 as correct, there is no statement in the proof to show that the condition holds, e.g.what is the explicit estimation of $\epsilon$ in terms of the network weights, say at typical initialization values? 3.Third, while it is the case that there exists weight values for which the RNN dynamics is not stable, such can be said for most parametric dynamical systems. However, what needs to be investigated is whether for a typical initialization, and subsequent training, the stability condition holds. This is not shown in the proof, nor in the statement of the theorem. 3.Implication of Corollary 1: I do not see how it follows that *“Corollary 1 suggests that well conditioned gradients of an ODE RNN can negatively impact the modeling capacity of these models and prevent them from learning the dynamics of the training data”*. Corollary 1 says that if the Jacobian is the identity, then the dynamics is trivial. In the nonlinear case, Jacobian’s role in stability is even more limited, as only the stability of small perturbations around an equilibrium point is governed by the Jacobian at the point. 4.Proof of theorem 2:    1. The Euler discretization argument here is not rigorous, and not needed. The proof presented here is using the Euler discretization, which has nothing to do with Picard iteration. I suggest authors carefully revise the statements here to avoid confusion. The key issue is that this can *also* be done for recurrent neural networks without gating. Then, the same statement can be made that at and close to initialization, the stability condition holds with no exploding or vanishing gradients. In Appendix equation (7), $f_\theta$ should be evaluated at time points according to the Butcher Tableau as well, not just at the final $T$. I believe this is a typo.<|endoftext|>The authors outline how the ODE RNN model suffers from the familiar exploding/vanishing gradient problem that we expect from RNNs. They introduce an improvement "mixed memory RNNs" whereby the problem of exploding/vanishing gradients can be mitigated, at least early on in the learning process, resulting in what should be a more stable model for long time series. It is presented in a palatable way without too much mathematical detail in the main body of the text. The problem is interesting and timely, the proposed solution (if truly a delta over previous work, see below) that controls the error at the start of training is useful and simple for practitioners to adopt. ## Cons / Queries  Throughout the paper I think the authors are too strong in their wording with regards to how well their model mitigates the EVGP. Its not that the model can outright avoid or is immune to the issue, rather, initialisation can control the error propagation at the start of training which should enable it to learn longer into the future without issues, but this is not fully avoided. A query about Corollary 2. This corollary states ODE GRU/LSTM also suffers from EVGP. The question is does mmRNN enable better control of the error than a well initialised GRU/LSTM? Unless I am missing some fine point, it seems incorrect to note that ODE GRU would suffer an EVGP as if mmRNN doesn t, when it does, it just suffers it less bad, but the authors do not give us insight on the improvement an GRU makes from an RNN so we can compare against the mmRNN. Similar to the above, why have the authors not tested against ODE GRU/LSTM?? #### Typos"Experimental settings are given in Appendix""Although , the update" (under eq 3, the space before the comma)This analysis is only of real interest if it is solving an issue not already solved by using an LSTM/GRU gate. If this is the case and can be explained and evidenced with experiment by the authors in the paper then I would opt for accept, for now I am leaving my review as a weak reject.<|endoftext|>This paper proposes a mixed ODE RNN plus discrete time LSTM. It is shown that this mixture approach is superior to many other ODE based RNNs and to discrete RNNs specifically designed to combat the exploding/ vanishing gradient problem. From the theoretical side, combining two well established approaches is certainly not a big leap, but the empirical results appear to justify this procedure. So my main comments focus on those:1) At least in the main text, there is no information on how hyper parameters of the different approaches were tuned, and whether the different networks are roughly comparable in terms of trainable parameters. The differences between mmRNN and other methods are sometimes quite small. 2) Along the same lines, I wondered whether the specific irregular task settings may have created an unduly bias for some of the models (those based on continuous time): Why are events aggregated through temporal intervals (“dense coding”), instead of just keeping them as they are but separating them by irregular intervals? Doesn’t this mean that the discrete RNNs are really learning a different type of task since, in contrast to the continuous RNNs, they need to process time aggregated information, or did I miss something here? 3) In general, I’m not sure any of the used tasks really contain significant long term dependencies in the sense that the RNN needs to get back to information presented much earlier in the stream to issue a correct output. For the activity recognition the authors point out themselves that long memory is not needed. Here I think it would be good to add established long term tasks in which performance can be charted as a function of temporal delay, like the multiplication (Hochreiter & Schmidhuber) or copy task. 1 is not a proper definition of the exploding or vanishing gradient problem as used in the literature, which implies that gradients should exponentially go to 0 or infinity as $t \rightarrow \infty$. The Jacobians in Def. 1 are time dependent, so having any of them >1 or <1 is not sufficient for either explosion or vanishing, while having all of them is not necessary. Also, for $\epsilon>0$ gradients could still exponentially vanish! Assumptions and conditions are not properly introduced in the theorems. Also not sure what ‘non trainable constant dynamics f 0’ means: As defined, the ODE still contains a decay term, so dynamics is not ‘constant’ everywhere, and either way with f constant, gradients may still explode or vanish. More generally, I wondered whether it wouldn’t be more elegant, and potentially efficient, to combine two ODE based mechanisms into the same architecture (e.g.ODE RNN and coRNN), rather than combining discrete with continuous time methods. In sect.7 the authors discuss the drawbacks of heterogeneous architectures, but the same arguments I would think apply to their construct as well. Minor issues:  Strictly, mathematically, LSTMs neither avoid the vanishing nor the exploding gradients problem, just alleviate it. Don’t understand some of the notation: For instance, right in eq.1, h should be a continuous function of t but is indexed by t, and f seems to take as input $x_{t+T}$ a future value, making this some sort of delay DE. Fig.3: How were these graphs produced?<|endoftext|>The authors approach the problem of modelling irregularly sampled data with continuous time RNNs. They first show, through a series of theorems, that continuous time RNNs are expected to suffer from vanishing and exploding gradient problems in both automatic reverse mode differentiation, and using the adjoint method. To be transparent, I was a reviewer on a previous submission of this paper where the decision to reject was borderline and significant discussion between reviewers and with the authors led to enlightening improvements. As for the previous submission, I very much enjoyed this paper and believe it contributes very interesting ideas to model irregularly sampled time series. The theoretical derivations are sound, the experiments are detailed and well rounded, despite some issues (see below), and the writing is clear. 1."In iRNN (https://openreview.net/forum?id HylpqA4FwS), an ODE based RNN, the authors clearly proved that no vanishing/exploding gradients exist in iRNN." How does the proposed setting differ from this one, and how does this explain the discrepancies in findings about gradient explosion and vanishing ? A discussion about this point, even a brief one, is still lacking. Given that experiments show some small margin with other models in some cases, it would be important to add details about hyper parameter tuning. If the best parameters used in original papers were used, a motivation as to why this is a good practice in this case should be articulated. It is unclear, given the specific nature of the tasks used here, if these choices of hyper parameters give a fair comparison between models. Unless I missed this, details about this point are not present in the main text or in the appendix. These concerns are based on extensive discussions between reviewers in a previous submission cycle, and I would like to better understand how the authors built on these remarks before I adjust my score.<|endoftext|>The authors prove the ODE RNN and many of its variants suffer from vanishing/exploding gradients. They show that this problem persists even when employing some of the previously proposed methods to overcome it. Moreover, the authors suggest a new method to mitigate this issue while keeping the ODE RNN properties useful for irregularly sampled sequences. They demonstrate that their theoretical model works in practice better than previous methods, including against a "simplified" non continuous RNN augmented with a time input. * Proposes a new method to overcome it, proves this, and demonstrate empirically that it does work better in practice by a significant margin. * Also compares against a basic baseline of non continuous time. This is important, because it shows the advantages of the continuous formulation. The more common definition that I am aware of is that the absolute value of the derivative dL/dh_t grows/decays exponentially for some paths of neurons. Specifically, why 1 ± epsilon is sufficient for explosion/vanishing in the given neuron? I can see how a sequence of such "slightly" small/large gradients can accumulate to vanishing/explosion, but why would a single i for a specific time be sufficient? Shouldn t it be required to hold at all points in time? * While most of the paper is written in a very clear language, the introduction to ODE RNN is a bit brief. Extra background on the subject (perhaps as part of the appendices) would be very useful. Note: I am afraid that I am not sufficiently familiar with ODE RNN to verify the correctness of the proofs. Overall this is a strong submission. However, because I am not as familiar with the specifics of ODE RNN and could not verify the correctness of the proofs, I am not fully confident in my score.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; 1.The paper tackles object detection by using the encoder decoder structure of the language model, Pix2Seq.2.The authors argue that the proposed method leverages prior knowledge for object detection less than existing object detection algorithms that exploit box regression, intersection over union, and so on. Inference TimeThe main concern is inference time. Since the model has sequence prediction (“generate one token at a time”), it can take more time compared to existing models. 4.Cross Attention Maps are really good. This is one of the major concerns in DETR and may inherit the proposed method. I believe it will get a huge attention and lead to a big change in computer vision community. It can be relaxed by using different architecture such as deformable DETR (Zu et al.2021) or other efficient DETRs. In general, the proposed method is novel and gets good results. The authors tackle the exploitation of prior knowledge of object detection in other methods. I hope the authors resolve my concerns. 4.EOSSince the model uses a language model, it is possible to emit EOS between some coordinates or class tokens (e.g., y_min, x_min, y_max, EOS). How does the model deal with this?<|endoftext|>Pix2Seq provides a simple approach for object detection understood as a sequence generation task, in this case, of the coordinates of the bounding box and the object class. The manuscript proposes a scheme to discretize the bounding box coordinates in histogram bins and proposes data augmentation for the sequence, which  address two observed limitations when predictig sequences from images: early EOS and repetition of objects. STRENGTHSS1 The study is novel and scientifically interesting in order to further understand the potential of the language model task beyond textual inputs and outputs. S3 Proposes a approach to encode the continuous coordinates of the bounding boxes to discrete tokens. This simplifies the decoder architecture, compared to DETR. S4 The work does not require the non maximum supression post processing, as in DETR. If so, then the whole output sequence of 25 tokens is the result of reading row by row ? More guidance to the reader may be helpful. C2 Given the sucess of seq2seq models for image generation (eg, iGPT) when trained with large amounts of data, one woders whether training by just more data results would actually reach state of the art. The proposed ideas are novel and, from my perspective, valuable enough for their publication despite not obtaining new state of the art results for the task. Pix2Wav simplifies DETR and improves its performance a bit.<|endoftext|>This paper proposed a language modeling framework (pixel2seq) for object detection. The authors cast object detection as language modeling tasks that use a sequence of tokens (x1, y1, x2, y2, c) to describe the bounding box and train an auto regressive decoder to generate the target sequence. I really appreciate the authors advocating a totally new approach for object detection based on the intuition that *``if a neural network knows about where and what the objects are, we just need to read them out *. Compared to Faster RCNN, pix2seq gets rids of bounding box proposals, ROI pooling which is highly customized for detection tasks. This is a huge improvement towards a more unified model for vision tasks. I wonder what is the percentage of different synthetic noise sequences used in the paper? I wonder is there any specific reason to do this? What is the inference time of the proposed model. It is known that the auto regressive model is slow at the decoding stage, but comparing it to the Detr model will be informative for the readers. The idea is novel, the proposed approach is simple and elegant and the paper is well written with most of the technical details with strong experimental results.<|endoftext|>This paper presents a new framework for object detection by casting the problem as an (auto encoder based) auto regressive sequence prediction using a CNN based backbone as the encoder to encode visual features and transformer based encoder & decoder (c.f.section 3.1) as the decoder to predict each bounding box sequentially. All the bounding boxes in an image are generated auto regressively conditioned on the image features from the backbone and previous predictions (c.f.Eq.1).The key idea in this paper is to output each axis aligned bounding box as the set of tokens representing the possible bin locations for its two corners and then to use cross entropy loss (SoftMax) along with the class token to predict each detection. The best model is trained by the random ordering of bounding boxes. The submission has merit and a potential to receive considerable attention by the research community due to having an unconventional (but not necessary novel ) view point to the object detection problem with good results. Limitation of this formulation should be clearly elaborated and the ablation study should contain insightful results for the contribution of each proposed module (formulation, architecture or loss). To this end, I rate this submission borderline<|endoftext|>Given an image, the model first uses a visual encoder the obtain a feature map and then sequentially decode the coordinates and label through a decoder. 5.In the proposed method, the decoder predicts the quantized coordinate tokens given input feature map and preceding tokens. I think this paper proposed a novel idea to reformulate object detection to a sequence generation problem. It provides us with a new perspective to think about conventional vision tasks. However, from the paper, I do see several drawbacks of the proposed method. Pros:1.This paper proposed a novel idea for object detection. DETR decodes the predictions in parallel while the proposed model does it sequentially. My impression is that such a sequential model may introduce more time cost than parallel ones such as Faster R CNN or DETR. This leads to the same question asked above. I would like to hear more from the authors about what could be potentially applied to further improve the performance.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; How is $p(x_o, \theta)$ computed, since we are in the likelihood free setup? The supplementary materials are thorough and include helpful experimental details as well as further discussions. Provided that my question above can be answered, I recommend this paper for acceptance. Accuracy is maintained while reducing the computational cost of inference.<|endoftext|>SNVI sequentially updates a surrogate likelihood model and an approximate posterior model in high posterior density. Finally, the approximate posterior is sampled with importance resampling. ## Pros:  The paper is well written. Improving the efficiency of sequential SBI methods is important. I am sure there are good motivations for this but this is probably something that should motivate. As you say that your method could be useful when we have many observations it would be interesting to show how does something that other methods cannot do. I recommend weak acceptance as I think the method described in this paper may be interesting for the SBI community.<|endoftext|>How does the proposed method compare to this simple approach? On page 4, should  $q_\psi$ be $q_\phi$? On page 4, what are the subscripts of \theta in the equation? In sum, this paper proposes a practical way for the posterior inference with unknown likelihood. The method is simple and natural, the writing is mostly clear.<|endoftext|>This paper presents a form of variational inference (VI) approach for likelihood free inference (LFI) named Sequential Neural Variational Inference (SNVI). Finally, the authors propose an SIR step from previous work which claims to enrich the variational family with minimal computational cost. The paper is clear from a method recipe perspective. Specifically, each of the three parts to the SNVI approach, as well as the three variants of the variational objective used for the posterior model part of the approach, the SIR step, and the fix using calibration kernel, have already been well established by previous works, which the authors do clearly cite. As a minor and related weakness, it is not clear how one would choose between the three variants of the variational objective here. There are alternative choices for each component of this technique that seem sensible but can be shown to be worse than this combination of techniques. I would lean towards acceptance.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors seek to use a small RNN to generate extended, time varying outputs given a cue indicating which output to generate. They focus on the problem of generating a sequence of outputs, cued in series, without the end state of one interfering with the subsequently cued trajectory. The authors then show that by biasing the activity of the network towards the origin in the absence of an input (which they suggest is inspired by neuroscience), RNNs more readily learn sequences and more consistently generate accurate sequences in series. As it stands, the authors visualize cumulative distributions of a subset of the states rather than performing statistics. Primarily, the authors fail to test obvious alternative hypotheses of ways that the problem domain they design could be solved by a learned system, without the need to add the inductive bias they add. Moreover, and more fundamentally, the relevance of such a small toy system (300 units, trained to generate just 10 trajectories) for machine learning is unclear. My enthusiasm for the work is relatively low for a variety of reasons. The authors discuss two important alternatives to the approach that they ultimate pursue, which would involve attacking the problem with learning rather than with building in inductive biases. The networks are small and seemingly toy: 300 units, 10 motifs. This is smaller than MNIST. I m not one to think that everything needs to be shown on ImageNet and achieve SOTA to be a useful contribution, but the scale of these experiments are so, so small that I worry that anything learned in this domain would transfer or inform work at far greater scales   particularly when the conclusion is that in such a low data regime, greater inductive biases are required rather than learning. Minor issues:  The authors assert that thalamic activity may change the effective connectivity of motor cortical activity, but they don t make clear why that change would result in a bias towards the origin. In what sense is biasing the network to the origin motivated by neuroscience? Also, how is preparatory activity being the same across all motifs brain like?<|endoftext|>This paper considers the problem of RNNs learning arbitrary sequences of “motifs” (continuous time functions, discretized) after having learned the individual motifs separately. *NeurIPS 2020* The general topic of this paper is relevant, but I found the presentation confusing and difficult to understand. It then moves to propose a new model inspired in thalamocortical interactions in the brain: an RNN equipped with a preparatory module that imposes a beneficial bias when following a specific training protocol. Overall, given the current state of the presentation, some issues with the details of the continual learning experiments, and some concerns related to the existence of a trivial solution that would need to be addressed, the contributions of this paper remains unclear to me and I suggest a rejection. In this sense, the study of some of the architectures presented here (e.g.inspired on Schuessler et al.2020) are interesting. Also welcomed are explicit addressing of the continuous time problem, which is relevant for many fields including control. If the models are to be analyzed in this setting, it is important to see a robust description of the task and the metrics (see, for example, Chaudhry et al.2019).Also, although I recognize the value of synthetic tasks, I also would have preferred to see in addition a task that encodes a “naturalistic” dataset, specially given that the authors are interested in a general noise robust solution. Second, there is a problem that the authors seems to be well aware of as per the second to last paragraph in section 3. If I understood it correctly, the task described in section 2 admits a trivial solution, that of a hard resetting to an initialization state before processing a motif. Also, no discretization method is specified, for example, and the information about initialization is scattered. However, after reading the section, it is not clear what the essential components are. This should also go in the main section of the paper (it could be short, but it should be complete). Also as a good example of what I mean is Schuessler et al.(2020), which opens section 2 with a very concise description of the task.<|endoftext|>This paper presents the problems in chaining nonlinear motor pattern generators and proposes a resetting mechanism inspired by the thalamo cortical network. Strength: The proposed architectures have been shown to work for "zero shot learning," or recombination without re learning. Weakness: It is not convincing whether the particular resetting mechanism is the most reasonable one, either computationally or biologically. Computationally, any strong inhibitory input to bring the network state to the origin should suffice.<|endoftext|>The article proposes a biologically inspired architecture which increases the degree to which simple recurrent neural networks may be able to chain together elemental output sequences. The authors show that the proposed "predatory module" allows networks to perform on par as if the network were starting from a set initial state, while also providing a smooth transition between motifs. Pros  The modification that s required to the "control" network in order is extremely simple, and has a clear theoretical goal of decreasing "residual" activity between motif transitions, without quenching activity in response to the new control signal. The level of experimental detail is sufficient such that the experiments  in the main text can be replicated by an independent reader. Cons  The methods in the main text are clear, and lead to convincing results in the main section. However, without digging much deeper into appendix 4 & 5, there s no clear reason why the learned preparatory modules are able functionThe preparatory module appears to function well, and the quenching activity is reasonable well tied to biological thalamocortical loop actions. To my knowledge, the motif transition problem is not solved in switching recurrent networks, however I am not overly familiar with that literature.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposed a model for learning physical parameters from video. The model takes in a video with a foreground object and background, and predicts the motion of the object in the video. Weakness:  The whole framework is built upon an assumption that the video can be (near) perfectly decomposed into foreground objects and background, which is a very toy assumption and cannot be used in any complicated real video data. This paper assumes knowing the underlying physics dynamics (in this case pendulum), which is an unreasonable assumption. Other dynamics, if it exists in the video, will not be able to be modeled. The experiments are very weak. These are not shown in the paper at all.<|endoftext|>This paper proposes to infer from a single video, the underlying physical parameters of a system. The underlying novelty of the project is also rather limited. The synthetic pendulum results seem very toy, with the pendulum rendered on a fake background. In general, the evaluation is the paper is insufficient   only pendulum scenes are consider, and in that only a single video of the real world. Nor is the approach shown to generalize in any way to a new datasetI would like to see a comparison where a non continuous neural network is used to predict the underlying dynamics of the approach. The paper is also missing references several additional implicit representations for modeling dynamic scenes which are relevant [2 4]. The paper has a variety of issues which do not (in my opinion) make it fit for presentation at this time.<|endoftext|>This paper proposes a novel method for inferring a physical model from a single video. Their approach estimates the physical parameters and initial conditions of an ODE that describe the dynamics of the object, using neural implicit representations to render a video sequence based on the physical parameters. The authors perform experiments using simulate and real world pendulum videos and compare to a Lagrangian VAE baseline.<|endoftext|>SummaryThis paper represents a video as a canonical implicit representation and a set of object motions that are described with an ordinary differential equation. This paper represents nonlinear object motions with an ordinary differential equation, which enables them to recover interpretable physical parameters from videos. The paper says that it can learn physical models from videos. However, there are only experiments on a nonlinear damped pendulum model. 2.The comparison experiments only compare the proposed approach with one baseline. 3.The ablation studies are not sufficient. There are some ablation studies that can be conducted, such as the iamge pyramid scheme, validating the decomposition of foreground and background, and the accuracies of the approach for captured objects in different motion states.<|endoftext|>The paper proposes a method which is capable of obtaining physical models of motion using only visual observations of the motion from a single video (no training data). This is done by combining a differentiable ODE solver with a coordinate based network which reconstructs the video frames based on the ODE solution. Since this system is fully differentiable, the synthesized video loss can be used to infer the physical parameters and initial condition of the physical model along with the coordinate based network parameters which represent the video. However, the amount of evaluation seems limited (only on the case of a pendulum), the comparisons to competing methods seem limited, and it’s not immediately clear to me why the coordinate based network needs to be used for learning from a single video. In my opinion, the strengths of the paper are as follows:1. Figure 3 shows that the proposed method generalizes to unseen frames significantly better than the baseline method. Is it because rendering from the coordinate based network is differentiable, allowing gradients to propagate back through the ODE solver and physical parameters?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The authors first derive several theoretical results that can be easily derived from the existing works on CNN, e.g., Wu et al., 2020a and Foret et a;., 2021 in section 3.. In section 4, the authors show that if they directly apply large $\rho$ as suggested by (3), they will encounter the gradient vanishing phenomenon during the training. Based on this phenomenon, the authors propose two ways to solve this. The contribution as I can see is that the authors are the first ones to conduct extensive experiments using AWP on graph dataset. Strengths: I am satisfied with the numerical experiments in section 5, although they largely follows the routes of existing work. Section 3: i) The theoretical results are very incremental based on existing works e.g., Wu et al., 2020a, which can be easily derived or directly used. This is work i) is quite incremental compared to the existing works, ii) does not explain/justify/describe the algorithms well. I do not think this paper is good enough for ICLR.<|endoftext|>The authors extend the line of work related to adversarial weight perturbations: first showing that a vanishing gradient issue in standard AWP can hinder training. Past this, none of the results are strikingly novel or groundbreaking. The authors then focus on using AWP to train graph neural networks, demonstrating a minor boost in both clean and robust accuracy. This theorem doesn t add add anything to the story.<|endoftext|>The paper designed and tested WT AWP, a new adversarial weight perturbation approach, on graph neural networks. WT AWP reliably increases GNN performance on a wide range of graph learning tasks, including node classification, graph defense, and graph classification. This paper focuses on extending AWP to GNN. This paper analyzes the vanishing gradient issue existing in AWP and gives a more detailed theoretical proof. The experimental part of this article is comprehensive, and the experimental results also verify the advantages of the proposed method. is confusing for me. However, some statements and equations are not clear enough. I hope the authors can further polish the paper in detail.<|endoftext|>The authors propose a variant of adversarial weight perturbations / sharpness aware minimization for graph (convolutional) neural networks for node and graph classification. In particular, they make two adjustments: “truncating”, i.e., limiting the weight perturbation to specific layers, and weighting the sharpness aware loss with the regular loss during training.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes a different density function for popular density based clustering algorithms, i.e.truncated symmetric and asymmetric Gaussian kernels for DBSCAN and DPC. It could become a stronger paper after major revision by motivating and explaning their approach more carefully. However, the authors need to more clearly explain and motivate their approach. The introduction, related work or preliminaries would be a good place. Should there be error bars? In page 5, "some intuitions that why..."  In page 5, "process grouping of all the data..."  $\hat{g}$ and $\hat{\rho}$ in the appendix are not defined. This paper has promising experimental results, but is not ready for publication at this stage.<|endoftext|>The empirical results certainly exhibit that potential for strong performance may be there, but I have reservations...My primary concerns relate to the experimental set up and some uncertainties relating to the presentation, and also the clarity in relation to some of the technical points. Does this have any relevance in density clustering? If only in the proposed approach, then it is not clear if the improvements in performance are actually a result of the new method, or simply down to the choice of kernel. However, some issues about clarity in relation to the technical parts of the paper. If the latter, then these performance comparisons are not indicative of the actual comparative performance. Otherwise the paper appears to be correct.<|endoftext|>In this regard, I would ask for a more complete picture of density based clusterings (including spectral clustering), their theoretical background, and how the proposed density measure can improve the clustering performance (also in dependence of the employed clustering method). As a result, since there is already some literature about kernel diffusion maps and spectral clustering (as cited by the authors), the relationship between these clustering methods and what is particularly novel in this work should be made clear. The proposed idea is sound and the experiments indicate that the proposed measures are able to improve the performance of clustering methods.<|endoftext|>The paper presents a methodology for defining the density reference function required in density based clustering methods such as DBSCAN and DPC. However, there are several concerns to be addressed: There is no theoretical justification that the proposed method is able to capture the local characteristics of the dataset. Therefore the empirical conclusions are conditioned on this value. This is the typical measure. Moreover, for the datasets of Table 1, it is suggested to present results using typical clustering methods (eg.k means) as happens with the face datasets.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 8; This paper researches the lottery ticket hypothesis for networks as a deep image prior or deep generative prior. The specific approach is to (1) train deep networks to reconstruct multiple images for DIP (Ticket ﬁnding objectives), (2) conduct iterative magnitude pruning to the trained network, (3) obtain the pruned mask and reset the model parameter to initialization weight and (4) perform deep image prior to new (or training) images. The experimental results are somehow interesting that (1) different pruning methods actually performs differently and (2) there are winning tickets in the studied problem. The point I am more interested in is that some networks are more suitable to be used as deep image prior than others. It seems that the author only discussed the existence of these networks, but did not discuss the possible reasons why these structures is superior to other structures in the deep image prior task. The lack of this part of the discussion is regrettable. If possible, some experiments can also be conducted. It is not clear whether the cause of the effect on GAN CS and the result on DIP are the same. Another minor problem is that this paper is difficult to follow. Especially in terms of specific methods, I think it is difficult to quickly understand the author s approach. Overall, this is an interesting paper. If the author can discuss these issues and improve writing, the paper can be accepted.<|endoftext|>SummaryThis submission studies lottery ticket hypothesis for deep neural network based inverse imaging. It considers two scenarios: 1 inference for compressed sensing based on pre trained deep generative models, 2 deep image prior where all network parameters are fit to a single image. The empirical results suggest that with a high level of sparsity not only the model size can be made much smaller, but also the generalization can improve which suggests pruning as a regularization. OriginalityThis work studies the LTH for low sample deep inverse imaging for the first timeSignificanceStrong points  for the first time studies LTH for deep image prior and pre trained deep generative compressed sensing  provides empirical results for several low level image restoration (as well as image classification) tasks and datasets  the results are interesting suggesting LTH exists with high degree of sparsity that not only makes the models much smaller but also improves the generalization performance as a result of sparsity regularization  the results also suggest transferability of LIP subnetworks across data and image restoration tasksWeak points  The paper presentation is poor. The organization is poor and confusing. This work is purely empirical study and it s not clear for example why LIP can transfer across image restoration tasks, and dataset? The experiments only report PSNR. This is more important for the pretrained GAN model. Isn’t this one of the important points of pruning as advocated in the introduction? This is empirical work studying an important problem.<|endoftext|>This paper exploits the LTH in the low level vision tasks, including image restoration and compressive sensing image reconstruction. Through some experiments, it identifies the existence of LIP in low level vision and also gives some interesting conclusions on the transferability of the proposed LIP. 1.This work shows that there exists a very sparse subnetwork whose performance can surpass the original dense network. To find such sparse network, it directly minimizes the MSE relying on the ground truth image. In practice, however, the ground truth image is always not accessible. 2.In my opinion, the goal of the research on LIP is to simplify the DNN architecture in DIP based on the pre known knowledge on the sparse subnetwork. As for the loss function of algorithm 2, whom does $E(f(x_a;\theta \odot m))$ denotes the MSE distance between the network output $f(x_a;\theta \odot m)$ and? Even though this paper is interesting, I still argue the usefulness of the proposed LIP. This paper only attempts to do some exploration based current existing LTH related works, which weakens  the novelties and the contributions of this work.<|endoftext|>The paper shows the effectiveness of such a lottery image prior which trains subnets in isolation. Positives+ The paper addresses an important problem of image restoration using deep prior which has been quite effective in the absence of labelled examples. + The work deserves merit due to the novel application of the lottery ticket hypothesis to this problem and has been well presented. + The paper conveys several new ideas and is shown to induce sparsity on the network while achieving impressive results. Concerns  The paper does have a lot of typos such as the line   "that can be training in isolation" in two different places. Experimental evaluation and comparison for deep image prior tasks lacking rigour. they have not been compared with. 1.Gandelsman, Y., Shocher, A., & Irani, M. (2019). " Mastan, I. D., & Raman, S. (2021). DeepCFL: Deep contextual features learning from a single image. Hence, it is just below acceptance threshold.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The work applies and adjusts manifold mixup in the subject area of zero shot cross lingual transfer. To take care this, the authors propose a X Mixup method that interpolates the representations between the source and target languages, and give this compromised representation for target language prediction. The paper is well written and the motivation is clear.<|endoftext|>The paper proposes cross lingual mixup, a technique which performs manifold mixup of source and target sequences. Strengths:  x mixup is a clear idea and seems straightforward to include on top of existing models like mBERT or XLM R.  The improvements over strong baselines on multiple XTREME tasks are convincing. Weaknesses:  One big downside I see is that the authors use mixup at different layers for different tasks. This leads to improvements in several XTREME downstream tasks over strong baselines.<|endoftext|>This paper proposes X Mixup, a model that considers the source languages and target languages together for cross lingual transfer. The designed model takes a pair of sentences (or the translated sentences) in a source language and a target language as the input and computes the cross attention between them. The proposed idea is interesting.<|endoftext|>This paper proposes a technique which the authors call *cross lingual manifold mixup* or *X mixup*. The paper is well written and the authors perform a nice analysis/investigation of smaller research questions in Section 6. The general idea is to combine the hidden representations corresponding to the source language input with the hidden representations corresponding to the target language input (in a smarter way than, e.g., simply concatenating them).
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; StrengthThis paper studies generalization of deep neural networks, a central topic of theoretical research in deep learning. This paper is written well and easy to follow. WeaknessMy major concern is that the key findings of this paper are either not novel which have been revealed in previous works, or lack comparison with existing works so it is unclear how significant the results are. In particular, (1) the finding that target functions corresponding to fewer leading eigenfunctions of NTK are easier to learn have been revealed by the generalization bound in [a] (Theorem 5.1 of [a]). (2) The “no free lunch” theorem presented as Theorem 1 is based on the new notation of “learnability” defined in equation (2), and it would be important to connect this new notation to existing works so that one could understand the significance of the results. For example, the pessimistic result of Theorem 1 could be connected to the result in Section 3.2 of [c], where it was proved that there exists a difficult test data set which fails the trained ReLU neural network (the difficult test data and the training data can be generated from the same target function). There are some other issues, for example, only asymptotic analysis (the width of the neural network goes to infinity) is presented. Note that all the referenced works [a c] are based on neural networks with finite width, which is the case closer to the practice. While this paper studies an important topic of deep learning, generalization capability of deep neural networks, the key findings of this paper are either not novel or lack comparison with existing works. I suggest the authors perform a thorough literature review in the recent progress of theoretical research in deep learning, and then carefully place their work under the context of the literature.<|endoftext|>Section 2.1: In the opening part there is no assumption about independence of outputs (m of them). Judging by the claim on page 3 (the last paragraph), the paper does assume outputs are independent? To see that these kernel based problems are quite different, please check out "On learning vector valued functions" by Micchelli and Pontil (2003). Unless one takes the absolute value, I fail to see how this quantity is bounded in [0, 1]? Section 2.5: The discussion immediately after Lemma 2. This is about the dependence on the sample size and, thus, not sure why this is specific to neural tangent kernel? The empirical analysis is interesting and should have been the main focus of the work. In this regard, the ablation study with input spaces that have the closed form expressions for eigenfunctions is quite informative. The paper should have focused on its strengths which is the empirical study that links the NTK spectrum to generalization in neural networks. The theoretical results apply to kernels in general and, thus, do not see much novelty in this regard. The related work does not adequately cover prior developments in kernel methods and fails to place the claimed theoretical contributions within this scope. I would recommend a major revision with a proper review of prior work on kernels. The focus of the paper should be empirical and presentation of its empirical findings should receive more attention/pages. I also fail to see sufficient theoretical contribution relative to prior work on kernel methods.<|endoftext|>It conjectures that the same results will also apply in the finite width regime as well. By analyzing kernel regression and by defining a measure as the “learnability” of a given target function, the paper proves a “no free lunch” theorem which implies that improving a network’s or kernel s generalization for a given target function must worsen its generalization performance for its "orthogonal functions". The paper then analytically predicts two phenomena: worse than chance generalization for hard functions and non monotonic error curves in a small data regime. It also provides some simulations to corroborate the analytic results. The topic of the paper which is on the NTK approach of analyzing generalization error of neural networks is interesting. The paper continues the line of work of Bordelon et al 2020 which seems a promising direction. Moreover, the paper is well structured and has plenty of simulation results. However, on the theoretical aspect, I believe the paper can be improved. I especially found the problem setup section of the paper and its notation hard to read. I think the theoretical results of the paper is a bit incremental with respect to the Bordelon et al.2020 and Canatar et al.2021.In particular, Theorems 1 and 2 demonstrate simple facts and have very short proofs. Moreover, the setup of the paper is for finite set $\mathcal{X}$ with $M |\mathcal{X}|<\infty$ with data sampled from a uniform measure. The justification for the reduction of output dimension to m 1 in page 3 is also not entirely clear. In the conclusion section, it should be noted that the results of the paper concern the generalization property of neural networks in the “infinite width” regime, and that the paper merely conjectures that the same theory holds for finite width networks as well. I am not very familiar with the recent prior work that this paper cites and builds upon its approach and I have not completely checked the mathematical correctness of the claims of the paper. But, overall, given the importance of the problem analyzed by the paper, i.e.generalization performance of neural networks using NTK approach, and due to mix of theoretical results of the paper with empirical results, I am inclined towards accepting the paper.<|endoftext|>But I think this is really hard to assess given the experiments are also on unrealistic data and learning tasks. By making a number of approximations, they are able to find a closed form expression for this matrix and study its first and second order statistics. Moreover, the authors also introduce a new metric, which they call "learnability," that they use to prove a new no free lunch theorem whose content is that when averaging over a complete basis of functions the learnability is independent of the kernel. I think the results are useful, intuitive, correct, and bring new insight that let us understand better how kernel regression works. This paper provides a really nice theoretical analysis of generalization in the setting of kernel regression. The metric of "learnability" is intuitive *and* useful, and the new no free lunch theorem provides new insight into the relationship between the size of the training set and the success of regression. The results may very well be relevant for explaining generalization in deep neural networks, but the theoretical analysis and experimental evidence for that are not particularly convincing as of yet. I think a major weakness of the paper is the desire to extend the regime of these results to make claims about deep learning. In particular, deep neural neural networks are only kernel machines at infinite width, and we expect deviations from this limit for realistic finite width networks. If some of these statements are revised (or perhaps supported with more evidence), and/or if there s more discussion of when they expect their results to apply to DNNs and when they expect it to break down, I will substantially increase my score and recommend an acceptance. ### After Author ResponsesThe authors have addressed a number of my concerns, and I have updated my score accordingly. Relatedly, the specific experiments performed are on very artificial datasets. but it also gives me pause with the claims made about deep learning in general. One particular problem is that the spectrum of eigenvalues for realistic data often obeys a power law. Along these lines, it seems that the most relevant question is why is the inductive bias of fully connected networks, which could be assessed by looking at the particular combinations of parameters and activations that make up the NTK, often useful for learning and generalizing the datasets and functions that we typically care about for AI function approximation tasks. First, as described above, it s not clear that the learning tasks are particularly realistic. A width 20 network of 1 layer might be very much like kernel regression and be in a regime of applicability of their results, but a width 20 network of 20 layers will definitely not be.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The paper proposes a more realistic setting (called *i Blurry*) for continual learning (CL) that generalizes the *blurry* and *disjoint*  settings proposed in prior work. The disjoint setting assumes that there is no class that appears in multiple tasks and the blurry setting assumes that no new classes are seen after the first task. In the *i Blurry* setting, one can have overlapping classes across tasks as well as new classes appearing in each task. This setting is also **online** and thus the paper is interested in continuous model evaluation i.e., any time inference too. For instance, what if the task was not restricted to be online. For the new configuration, the paper proposes a strong baseline and a new algorithm called CLIB. CLIB is a memory based CL method that refines its memory by throwing out *least important* samples and updating it with more important ones. * Section 4.1 (page 5): For the LR scheduling, other **CL methods use either (1) exponential decay** or (2) constant LR ...* Last line on page 4 is missing a period (full stop)    * Also, Table 3 $A_{avg}$ on CIFAR 100 has the same numbers for the last two rows, which might be a typo. CLIB is also equipped with a data driven adaptive learning rate scheduling scheme which provides some additional performance benefits. Also, some decisions regarding hyperparameters need to justified, given the *i Blurry* setting is online and the main motivation of the paper is to propose and solve a more practical CL setting. An ablation study shows the main benefits of the proposed method come from the sample importance based memory scheme. Some justification for the decisions is required. 3.Lots of supporting experiments are provided in the appendix that further corroborate most claims. ## UpdateThe authors clarify almost all of my questions pertaining to the paper. A discussion regarding this is necessary as it is likely that similar ideas present (beyond CL for instance) in literature. **Personally**, I think dedicating a portion of the main text (Section 4) in discussing these would probably be more beneficial than describing memory only training and adaptive lr, as they seemingly play only a minor role (as indicated by the ablations). 5.Since the i Blurry setting is an **online** one, the choice of hyperparameters are likely to affect the results greatly.<|endoftext|>In this benchmark protocol, the class distribution is class incremental and has blurry task boundaries, and the training is online. They also propose a new method, CILB. It is important to analyse the results when some of the classes are not disjoint in continual learning tasks. The authors provided extensive experimental results. The proposed method is technically sound and proven to be effective by the empirical results. The paper is well organized and easy to follow. For the first component, the main idea is to find the optimal exemplars that can minimise the total loss so that it could be regarded as a simplified version of “mnemonics” (Liu et al., 2020). ***This paper is not self contained. ***The authors only provide experiment results on small scale datasets. *** Most continual learning papers, such as iCaRL (Rebuffi et al., 2017) and BiC (Wu et al., 2019), provide the results on large scale datasets (e.g., ImageNet 1k). *** In many continual learning papers (iCaRL (Rebuffi et al., 2017), BiC (Wu et al., 2019), etc. ), the number of tasks will significantly influence the continual learning performance. However, I don’t find an ablation study on the number of tasks in this paper. I don’t even see an explanation about how the authors choose that hyperparameter. Overall, I think this is an interesting paper. Nevertheless, I still think the paper might be useful for the continual learning community. &nbsp;### Post rebuttal updateThe authors addressed most of my concerns in the rebuttal.<|endoftext|>The paper presents an experience replay method for continual learning (CL) whose main innovation is a memory management algorithm that updates the memory when new samples arrive in a way that preserves the most "useful" samples (wrt to their effect on the loss) while promoting class balance. The main contribution of the paper, I believe, is the memory update method, which (as stated above) updates the memory when new samples arrive in a way that preserves the most "useful" samples (wrt to their effect on the loss) while promoting class balance. And the authors use a learning rate scheduling method that also gives some advantages. The memory management method is novel, as far as I m aware, and the overall method performs very well. Evaluation is carried out in the class incremental, online, task free setting, which is a good decision in my opinion. So many other methods fall short under these conditions. So I think the paper deserves to be accepted. The authors make a big deal out of their "blurry" sampling method for evaluation. I think it s a good approach, but isn t really a big contribution in its own right. Presumably, following memory update, the batch that is trained on is sampled uniformly from the memory. Space permitting, it would be nice to see a more detailed comparison with related experience replay methods.<|endoftext|>The paper proposes a new problem setup in continual learning. As the title suggests, the paper focuses on online, task free, class incremental, task blurry learning with any time inference. The authors also came up with new baselines and importance based memory management. They empirically tested their methods in the proposed problem setup. The paper has excellent plots for the new problem setup. However, without further clarification of the following concepts/comments, the significance of the new setup could be weak. * **The paper claimed the new setup is both “task free” and “class incremental.”** In my opinion, the two setups are not compatible with each other. ** Only one baseline demonstrated the incapability of making any time inference in Table 1. The significance of any time inference needs more evidence. * **Thm.1 may suffer from catastrophic forgetting. ** Thm.1 can be seen as a performance driven memory management strategy. However, it has nothing to do with catastrophic forgetting. In the long run, it is hard to tell whether the proposed method will outperform the baselines. This performance drop could be attributed to the catastrophic forgetting of Thm. Without addressing them, the proposed problem setup and method could not be that significant for continual learning.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper investigates a permutation based sparsification technique for distributed optimization, and prove that, under a mildly tighter condition than L smoothness, it can achieve better communication complexity than the existing random sparsification technique. The method is largely under the larger construct of MARINA, a distributed optimization scheme by Gorbunov. A (1+d/sqrt(n)) factor improvement is shown with the permutation vs randomization technique when the Hessian variance is small, and several promising numerical results are given. Overall, the paper is interesting, and the analysis in the  main paper is careful and clear. The assumptions are not too outlandish, and it makes sense that sparsification should be easier when data / Hessians are more uniform. (I suspect that is what is happening in the MNIST example, for example.) The paper seems strong, but I am not familiar enough with this specific area to comment on its novelty or impact. Weaknesses: Since the paper seems mainly to put forward the theoretical advantages of different schemes, I question if the proposed methods are better because of better proof techniques, or if they are really always like that in practice. For example, I suspect a fully correlated scheme where every worker uses the *same* sparsification (but randomly regenerated at each iteration) should achieve good performance as well. It is unclear to me, at least intuitively, why a permutation approach would be better than random sampling. I also think (and this is less easy to address, so it s not really factoring to my decision of the review) that the few experiments given may not be exhaustive enough to provide general conclusions as to the superiority of each method. Also, since the appendix is about 40 pages long, I did not have time to read it. If the authors feel there is one specific proof that they are particularly proud of, I d be happy to take a look and give a less lukewarm review.<|endoftext|>With a new assumption on the Hessian variance, the paper extend the original analysis of MARINA to accomodate correlated compressor, which avoid the independent compressor setting and can lead to improvement to the communcation complexity in a case when the number of dimension is larger than that of component. Improving communication complexity is an important topic in distributed optimization. The paper tried to extend the analysis of the SOTA algorithms in a more detailed case to refine the complexities. 2.For the experiments, does the autoencoder example satisfy the extra assumptions? 3.For the proof, the difference seems to lie in the Lemma 5 here and derivation above Eq (21) in MARINA paper, where AB inequality and Hessian variance work to bring some difference in the coefficients in my opinion, and the claimed difference of using correlated compressor is incorporated into the coefficient computation of AB inequality, the remaining things are similar, e.g., the Lyapunov function $\Phi$. Can the authors kindly provide more intuition on the benefits of the new analysis, with the additional assumption? Even though closely related to MARINA paper, but as a separate independent submission, to make it self contained, I may suggest that at least authors can consider to add a formal description (e.g., an \algorithmic environment or the main update rule) of the algorithm into the main content (rather than mentioning it in Appendix B), I think, at least now, readers should not be required to be familiar with MARINA as they are with SGD before reading the submission. I hope to have more insights from authors on the importance and practicality of the additional assumptions. I will appreciate the authors to address my confusions, and definitely reconsider my decision. Thank you very much. The paper improves the analysis of MARINA algorithm, but the introduced new assumptions need to be further justified.<|endoftext|>This paper extends the theory of MARINA to support potentially correlated compressors and refines the original analysis of MARINA based on a new quantity called Hessian variance. They show their proposed compressors, random permutations, have improvements in theoretical communication complexity in the low Hessian variance regime. The paper proposes some new techniques (e.g.AB inequality and input variance compressors) to relax the original assumption of independent compressors. 2.The paper proposes a new quantity called Hessian variance to refine the analysis of communication complexity. The advantages of the new theoretical results are based on the regime Hessian variance is 0 or small. But these seem to be very simple cases. For more complicated models, it is not clear if and when the Hessian variance is small. 2.In the big data case ($d \leq n$), when $d \ll n$, there is no improvement. In Figure 2, the difference between PermK and TopK is very small, which does not show the theoretical improvements. Although not perfect and build upon existing works, the paper proposes some new techniques and quantity to refine the analysis of distributed non convex optimization algorithm. The theoretical novelty is sufficient.<|endoftext|>This paper extends the analysis to an existing algorithm called MARINA, and proposes two results: 1) Correlated compressors among the workers, and one realization called PermK; In the analysis, an inequality called AB inequality is used to study the compression variance. 2) A new metric called Hessian variance to refine the results of MARINA. The proposition of studying correlated compressors in a distributed environment is good, but I m a little confused on the motivation   why is the hypothesis limited on MARINA but not on other algorithms? To me the hypothesis is fairly simple and interesting: in a distributed setting, workers follow parameter server architecture and communicate via compressed gradients, how will the correlation in the compressors affect the convergence? Same question for the Hessian variance part: could you elaborate why this is specific for MARINA? Understanding a newly proposed algorithm is great, but the contribution of a paper seems limited if the results only hold on one algorithm. The idea of PermK, where unbiased sparsifier is constructed via some random state and selected coordinates are enlarged, seems to be overlapped with (Jianqiao et al., 2017). In that paper, unbiased gradient sparsification is guaranteed in a similar manner. The similar idea can also be found in (Wang et al., 2018). It s unclear to me about the novelty on this method. The experiments are done on linear tasks, but it s unclear what hypothesis they are verifying. They only match when the same compressors are used in each run and number of bits in each case grow linearly with the iteration at the same rate.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; However, the overall contribution may not be sufficient enough to reach the acceptance threshold. Main advantage of the proposed method is that it s compatible with many existing learned video compression frameworks and the increase of computational complexity at the decoder side is negligible. The paper is well written in general. Weakness:The overall novelty seems limited since the instance adaptive method is from existing work with no primary changes.<|endoftext|>Basically, it is a borderline paper and I am open to other reviews. Novelty.This paper extended the previous work Rozendaal et al.(2021) to video compression and may have limited novelty. The major modification is the prior model for entropy coding, which is not enough.<|endoftext|>How many overhead bits are consumed due to the parameters  update? The contribution and novelty are limited. Overall speaking, the reviewer thinks that this paper stands at the borderline, but marginally above the acceptance threshold. Limited novelty and contribution.<|endoftext|>The author s main contributions are:a. It can be applied to any neural network video codec model. The method of this paper is lack of innovation. How does it compare with FVC on the HEVC Class C and HEVC Class D ? e The reference format of the paper is not uniform.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper presents a direct method for generating a molecularconformation conditional on the molecular graph. I also didn t understand the first two of the three future directions:I am not clear what the authors mean when they suggest combining thismethod with flow based or Langevin dynamics models can they clarifythis point? I think that this paper is an advance on previous work and probably representsthe state of the art in generating molecular conformations conditionalon the graph. Importantly, it should attempt to clarify if the improvement is mainly due to model size changes or due to algorithm changes.<|endoftext|>The authors propose the generative model that constructs a conformation (set of 3d coordinates) of the molecular graph with a variational autoencoder (VAE) framework. **Originality:** The work suffers from the lack of originality. The described model shares the same idea as ConfVAE and the unnamed model, proposed in [1] and [2] respectively   the autoencoder based framework for conformation generation, the iterative generation procedure, and RMSD objective were proposed earlier. The paper is written in a hard to read way. **Significance:** The novelty of the paper is minor.<|endoftext|>The paper proposed a new VAE based generative model for generating molecular conformations from graphs. Personally, the paper seems an extension of the recent progress of protein folding to the small molecule setting, but I appreciate the authors’ efforts to make it work.<|endoftext|>Strengths:* The results are strong and the presented methodology is clear. * The ablations shown in Table 5 are very nice to see and I think are a useful part of the paper. This would be nice to have been further expanded upon. Could these all be shown? * The text is at times a bit difficult to follow, but it does not detract from the ability to follow the work as a whole. The work presents strong results on molecular confirmation prediction. The results shown are very competitive, though I would like to see slightly broader comparisons and more exposition on various points that are raised in discussion.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 3; (In particular, pre training on emergent language performs on average better than synthetic hierarchical data, but not quite as well as a different natural language, and all of these pre training methods do better than training from scratch.) This is the newest of a small but growing body of literature that seeks to connect emergent communication with genuine NLP tasks. Steinert Threlkeld (2020) "Towards the Emergence of Non trivial Compositionality" makes similar points and could be cited here as well. * Why are the lines for "from scratch" flat in Figure 2? * Related work: I would also include a discussion of this Lazaridou et al paper where they compare ways of combining EC with non EC learning signals (e.g.image caption training): https://aclanthology.org/2020.acl main.685.pdf* Ethics statement: I appreciate this statement and agree with the possible positive impacts.<|endoftext|>This paper proposes a new way of using techniques from the emergent communication literature, where agents are trained to develop languages for communication on some shared task, to improve more typical supervised learning tasks in NLP, namely language modeling and image captioning. Results on permuted EC corpora could be explained further      One outstanding question issue I have with this paper is the results on permuted EC corpora. But regardless, it s clear that training on permuted EC results in impressive gains. The permutation experiments show that it s not syntax/sequential structure. This paper proposes a simple and interesting way of using the emergent languages for something more productive and more of interest to people actually working on real NLP applications. The idea of pretraining on emergent corpora neatly brings together new ideas about transfer learning and modern studies of emergent communication. There is surely a point at which EC corpora are too simple (e.g., imagine just a single token and a very small vocab). There is a very rich space of future experiments to try in this area. Table 3   how many data points actually go into the computation of correlations here?<|endoftext|>This paper studies whether output from emergent communication systems might be useful as pre training data for natural language tasks. Strengths:  I think this research question is important and interesting, and that the study of what EC agents output and how they might relate to natural language is important for both EC and NLP  The setting was made very clear. The claims, particularly those about language modeling, are evaluated on a variety of settingsWeaknesses:  Some of the claims don t seem entirely justifiable. Using a particular method for evaluation requires some degree of confidence that it holds generally, and I m not sure that the results here convince me of that. The broader research question is important enough that I feel like even initial work in this direction is perhaps of interest to the ICLR community and worth accepting.<|endoftext|>This paper explores the use of pre training models on corpora of emergent languages to improve performance on downstream tasks (in this case language modeling and image captioning). So I m curious how the authors reconcile this? For instance, even at the end of the paper "We present a new perspective for studying emergent communication by linking the corpora of emergent and natural languages in two ways." So again, for accuracy, it s important to clearly define the scope of the work as pertaining to this one particular scenario. For language modeling, the authors show that indeed the EC pre training is useful for the downstream language modeling task, but the results are not very convincing that this would ever be useful in a practical sense. Back to the research question, yes, EC languages *can* be used outside the game, but it s not apparent that it would ever be best choice nor the easiest.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; These insights are used to design attention reuse across layers, and shown to maintain the quality while improving the speed on Librispeech. StrengthThe analysis part of the paper can be a good contribution to the speech recognition community. It would serve authors better to identify other more correlated tasks. The analysis proposed in this paper can be a good contribution to the community. Overall the analysis is beneficial to the community, and can be considered for acceptance.<|endoftext|>The paper analyses the multi head self attention behaviour in CTC based ASR models. I think this is a very interesting experimental analysis of the properties of the Conformer/Transformer models that can help to understand non autoregressive end2end ASR models. They identify the working regimes, namely, phonetic and linguistic localisation. Based on these findings the authors propose to share attention maps across layers. This yields some modest improvements in terms of WER but obtains larger speeds up with respect to a basic Conformer model. The paper is very well presented with a large battery of carefully designed experiments and conclusions. In practice, this is just a semantic issue as there is strong evidence for the emergence of such diagonal pattern in the paper and previous works. In addition,  Figure 5 is an average and it would be interesting to contrast the phonetic properties of each head to understand whether there are emerging patterns. 3   computing the PAR sharing the attention maps.<|endoftext|>Second, based on the above observations, the paper proposes a method to exploit this division into the phonetic linguistic layers. The proposed method is to reuse the attention maps across several layers. This modification allows for more efficient architecture both in terms of latency in train and test time. # Correctness and SignificanceThe paper is logical and methodical in the investigation. The proposed measurements and experiments make sense. The phoneme analysis is conducted well. Session  > SectionIn summary, this is a good exploration into the transformer architecture for speech recognition. The paper provides valuable insights into the transformer s attention map structure and connects it with the phonetics. Then the paper uses these insights to improve the model. Or some discussion on this.<|endoftext|>The paper explores how self attention maps in conformer based ASR models are distributed across layers. Overall, the authors present an interesting analysis of the attention maps, which in the reviewer’s opinion is the strongest part of the work. It is interesting to categorize lower layers as phonetic, and upper layers as linguistic. * The authors are able to reduce the inference computation by a factor of 1.96 for a 30 second audio, with marginal improvements in WER. The presented results are also lacking; it is unclear if the method is very specific to CTC and whether it generalizes to other more popular end to end techniques. It would be useful to have some of these comparisons to the presented method. What about LAS [5] or RNN T [6] models, which are widely used as well? "Transformer transducer: One model unifying streaming and non streaming speech recognition."
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper describes an RL based method for learning to solve the weighted maximum cut problem. Especially, the authors specifically addressed scaling to larger instances by making the architecture more efficient, which is an important and challenging topic. The policy makes use of a so called  peek , a one step lookahead of the result of each flip action on the current objective. **Strengths**  Well written paper, it was nice and easy to read  The paper presents a sensible approach and good motivation, where it specifically designs an architecture to minimize the computational burden of the algorithm  The paper addresses scaling, which is an important challenge in neural combinatorial optimization  The paper applies reasonable baselines such as the heuristic strategy (MCA/MCA soft)  The paper takes care to conduct fair comparison (e.g.own implementation with optimizations also for ECO DQN baseline)**Weakness**  The paper only considers maximum cut, while title suggests general combinatorial optimization. As multiple aspects (flipping, peek feature) are specific to max cut it is unclear how the proposed method can generalize to other problems. To avoid confusion, I think the authors should use  best RL based method  and keep the term SOTA for the best exact/heuristic/RL solver in general. The experiments do not support the strong claims: generalization is only compared against ECO DQN and not against heuristic baselines. The theoretical novelty is limited as the  learning to explore  idea and max cut setup is from ECO DQN (Barrett et al.). I think it would be helpful to list both original and reproduced results for clarity in Table 1 (unless they are very similar)   Heurisitcs  typo Table 1  It may be good to mention other solution strategies are possible as well, e.g.rounding a relaxed solution (https://www.cl.cam.ac.uk/teaching/1617/AdvAlgo/maxcut.pdf)  How does MCA and MCA soft perform in Table 2? Given the focus on max cut only, I think some more related work on max cut (non RL) could be included. Overall, I like the paper, which is quite nice to read and presents a sensible and effective approach for solving the maximum cut problem using RL.<|endoftext|>Compared with current baselines, time complexity is reduced from O(|V|^3) to O(|V|^2). Although this paper has excellent motivation for improving the efficiency of current GNN based RL for CO problems, I have some concerns about the approach and experiments. For this reason, the title is much larger than the content of this paper, unless the authors can show the generalization of their method for other CO problem. According to experimental results, the paper states the performance is better than ECO DQN. (3)	In the experiments, the paper only compared with ECO DQN and two simple heuristics, which cannot support to claim that the proposed method is better than current SOTA of approximate algorithms. Some of them are efficient for large scaled problem too. (4)	If a max cut problem has multiple best solutions, I wonder whether the convergence of proposed approach can be guarantee with limited learning steps? The authors used 2|V| for small scaled instances and 4|V| for large scaled ones. If could not, the algorithm is hard to be used in practice.<|endoftext|>This paper presents a RL based algorithm for solving the graph based combinational optimization problem of Max Cut. The key idea is to formulate the problem as an MDP, where the state is the embedings of all the vertices and the actions are the vertices to be flipped. The proposed algorithm, named ECORD, is shown to be comparable to the previous state of the art ECO DQN and be significantly faster than ECO DQN. Postiive points:+ Combinational problem is very general and could have many applications. + The paper is very well written. The Max Cut is based on graph and only considers two labels. It is unclear whether ECORD can be applied to problems without graph structure or with more than two labels. It is unclear whether Max Cut is hard enough to use RL. In table 1, even greedy heuristics can have very strong performance. In terms of efficiency, the greedy heuristics should be even more efficient than ECORD. It is unclear how ECORD improves the efficiency of ECO DQN. The improvement over ECO DQN or heuristics is not significant. It is unclear how ECORD improves ECO DQN in terms of efficiency, which is the major contribution of this work.<|endoftext|>This paper proposes a scalable deep neural network (DNN) based solver for the maximum cut problem. The main idea is to use (1) a single graph neural network (GNN) pass to acquire node embeddings and (2) use sequential decoding (without using GNN) to acquire the maximum cut solution. Pros:1.The proposed approach is solid. I think this algorithm can easily extended for solving combinatorial optimization problems other than the maximum cut problem. 2.The empirical comparison is thorough with respect to the considered baselines. See [1] for an example. Gset (considered in this paper) is a popular benchmark for the maximum cut problem and there are several non DNN based results that can easily compared with the proposed solvers. For example, the authors can compare with [2]. This comparison is important since [2] is also a GNN based solver with running time linear with respect to the problem size. They use a single GNN pass and decode the solution. My current understanding is that [4] pretrains a GNN using supervised learning and ECORD use end to end truncated backpropagation. Ablation studies on using DQN over M DQN would be useful to see whether if the empirical improvement comes from the proposed scheme, or simply using a better RL algorithm. However, at the current state, it is hard to access the significance of this paper since baseline algorithms used in real world is missing. Furthermore, I would like to see description on the proposed algorithm s difference to existing work of Manchanda et al., (2019) to further support novelty of the proposed method.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper proposes a surrogate loss learning method named ReLoss. Hence, the authors aim to learn the surrogate losses by making the surrogate losses have the same discriminability as the evaluation metrics. The idea is straightforward and is easy to implement by using ranking correlation as an optimization objective. * The experimental results look good. By keeping the same backbones and training strategies, the proposed ReLoss gains improvements on various tasks, including CV and NLP tasks, and outperforms the state of the art methods on human pose estimation and machine reading comprehension tasks.<|endoftext|>The authors introduce a relational surrogate loss learning method (ReLoss) for replacing the original losses. The proposed approach is well motivated and makes sense. The paper proposes a method to learn surrogate losses, compared to related works, the proposed method gains significant improvements on various datasets. Without doubts, the proposed correlation based optimization that introduces differentiable Spearman s correlation coefficient has looser constraints than approximation based methods.<|endoftext|>This paper provides extensive experiments that demonstrate the effectiveness of the proposed method. The performance and efficiency compared to existing surrogate loss methods are significant, and the performance compared to original losses is seem to be significant on various tasks. I think the proposed method is well principled and provides meaningful improvements on various tasks. E.g., on human pose estimation task, ReLoss outperforms the state of the art method with only replacement of loss function.
Reject; rating score: 3; rating score: 3; rating score: 6; This paper proposes a method to select a group of pretext tasks from a given set of tasks for optimising the network training during self supervised learning phase. The weights for the given set of tasks are learned based on Hilbert Schmidt Independence Criterion (HSIC) using a few data samples. Using 2 downstream tasks, the authors show that their approach could benefit the learning of features relevant for the downstream tasks. Authors show, for example, that using a subset of pretext tasks benefits the accuracy of ASR model. Also, results do not show the impact of picking other subsets which are not strongly related to the downstream task. comparison with wav2vec 2.0 is weak, can this approach beat the SOTA? or they fail when the downstream task is changed?<|endoftext|>This paper attempts to improve self supervised (SSL) speech representations by determining what linear combination of SSL targets will perform best for downstream tasks (audio SSL targets like fundamental frequency or MFCC). The technique learns to weight these targets according to a Conditional Independence criteria: a target is more valuable if the SSL target is more independent of the data given a label. The second is that they manage to improve LibriSpeech, Voxceleb, and IEMOCAP performance over a wav2vec2 model by adding SSL targets weighted according to their algorithm. It would be incredibly useful to the community if the weighting of many SSL targets was important, and one could be found to make multi task training paradigms competitive. 2) The theoretical result seems interesting. I have not seen Conditional Independence be used as a criteria for weighting SSL targets. To support the claim that the authors’ algorithm is useful, they need to show that their algorithm also improves on wav2vec + naive mixing of multitask SSL. 1b) I worry about the presence of IEMOCAP in experiment #2 but its absence in experiment #1. The idea of using Conditional Independence to rate SSL targets, and the technique for calculating CI, is novel. However, the empirical results do not support the usefulness of this method (experiment #2) or are not compared to strong baselines (experiment #1). Without stronger empirical results to support the usefulness or relevance of this method, I do not believe this paper is strong enough for acceptance.<|endoftext|>This paper describes authors  proposal for selecting pretext tasks (pseudo labels) for multitask self supervised learning, to improve self supervised learning for speech representations. It leveraged the Hilbert Schmidt Independence Criterion (HSIC) for selecting pseudo label loss weights and used softmax and its sparsity version to realize it. Experimental results on speech, speaker and emotion recognition showed effectiveness of the proposed approach. This paper proposes a new approach to combine multiple tasks during pre training for speech representation learning, and it leads to improved performance for downstream tasks including speech, speaker and emotion recognition. Though authors mention not to directly compare with downstream tasks  performance reported in literature, it still helps to provide some justification to show the numbers reported in Table 1 & 2 are improvements over strong baselines. The proposed method is technically sound, though additional justification is needed to for experimental results reported in this paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper extends the training objective of EsViT to make it better suited to visual relationship detection. The novel queue based formulation is tested in various experiments, both for visual relationship detection and visual question answering. For example, the basic training signal in HICO already includes the concepts used to build the dictionary, therefore it can not be represented as a form of weak supervision. Is this desirable or not desirable? Can you add a discussion about this? The confusion arises also in other parts of the paper. Unless major issues arise in the discussion with other reviewers, e.g.the method is not as novel as I thought due to related works that I am not aware of, my recommendation is to accept.<|endoftext|>Thus, I think the rating of 6 is suitable for this version. I will adjust my rating according to the response and the discussions from the authors and the other reviewers. For example, in HICO, one image usually has more than 2 h o pairs. The comparison about the "complex pre processing stage". Thus, a discussion about the alleviation of the long tailed bias would make this work more solid. And the results on the two benchmarks look promising.<|endoftext|>The contribution is mostly about the use of their concept feature dictionary as a visual knowledge base to support retrieving information on the fly. It would be fair if the authors also evaluate their concept guided approach with the ResNet 101 backbone or actual visual objects given by Faster R CNN. ViT does not actually rely on actual “visual objects”, for example, RoI poolings in Faster R CNN. The paper appears to be limited in novelty and contribution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The authors conjecture that the reason for this is that the third STE variant breaks the scaling symmetry w.r.t.the weights of the first layer. This is possible due to the simplicity of the model and the assumed data distribution. On the other hand, they show that at least one of these global optimisers is also a stationary point for the third STE variant. The loss studied there was the MSE loss w.r.t.a ground truth model with the same architecture. In particular, it is shown there that the properly derived ST estimator is unbiased for models studied in the submitted manuscript, albeit for a different type of loss. It is clearly seen that their derived unbiased ST estimator breaks the scale invariance.<|endoftext|>The claims are weak and it is not clear if they are specific to the specific setting chosen. There is no numerical validation. In other words, the authors claim that some minima of the population loss of the original model *may* also be minima of the surrogate model with cReLU STE if the weights are appropriately rescaled. The single setting explored, the one with the ReLU teacher, doesn t seem enough to support general claims though. Moreover, there is a complete lack of experimental validation and some poor writing.<|endoftext|>In the paper this discussion is not coming back, but I think it would be interesting to discuss whether the other estimators (ReLU, cReLU) are biased and whether one is more biased than the other. The main drawbacks are that there is no empirical data to support any of their claims, several aspects are unclear based on the manuscript and the paper could improve in its overall clarity. In the current version and my understanding of it the drawbacks outweighs the strong points.<|endoftext|>The authors further analyzed the stationary points of a simple misspecified model with Gaussian inputs to confirm their observations. I d like to thank the authors for the revision, although it does not resolve my concerns. However, you mentioned this nowhere and it is even not stated what $m$ and $n$ is. Also the work does not go beyond one hidden layer network as it claimed in the abstract. I vote for rejection mainly due to the questionable analysis (lack of rigor) and the difficulty to interpret the relevance of the result.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; In this paper, the authors provided a study on how to effectively utilize the existing active learning (AL) methods. Specifically, the authors conducted experiments on the performance of different AL approaches with different training strategies and data manipulations. 2) The conclusions drawn in the submission can be used to design and improve the performance of AL methods. Cons:1) The paper is more like a technical report where different experiments are conducted and corresponding results are reported. Minimal analysis or insights are given to discuss why the results are obtained. For example, the authors give a conclusion where LL4AL should use a warm start. But what exactly affects the warm start to be effective for different AL methods and datasets? What results could be expected on other datasets that are not included in this paper? If so, it would be interesting to see the authors using the conclusions to construct an AL system to achieve better performance. 2) Please consider to put different shapes or use different line types for the methods in comparison for better presentation. To sum up, the paper can be employed as a practical guidance for researchers to conduct AL for image classification. Thus, my initial rating on this submission is borderline reject.<|endoftext|>This paper provids a codebase for fair comprisons of existing Active Learning methods, and performs a lot of comparative studies on the various influence factors for Active Learning. Some new observations are yielded and help figure out the optimal practices of evaluating AL methods. The experiments are extensive and valuable for the better understanding the existing strategies of training Active Learning models. Compared to previous codebase, this work seems to show some different observations and insights. Meanwhile, the conclusions about  various factors in AL methods are insightful and useful for guiding the development of AL strategies. Is it possible to extend these experiments into more real and complex image datasets, like ImageNet? 2.The conclusions or observations are straightforward, but scattered. Is it possible to provide an overview of the importance of various setting, like training setting, dataset configuration? 3.It would be better to discuss the potential research direction, considering the properties of existing AL methods and datasets. Overall, the experiments are extensive and produce many findings that could help understanding of existing pool based AL methods. The new codebase could provide solid benchmarks for making fair comparisons. Although some adjustments may make a better contribution, this paper is marginally above the acceptance threshold.<|endoftext|>Effects of factors such as choice of backbone, data augmentation, optimizers, learning rate, cold vs warm starting are studied and the conclusions are provided as best practices. Strengths:    Recent active learning methods show results using different settings, and this paper tackles the important job of evaluating these methods in a consistent manner. The analysis w.r.t using additional modules for complementary tasks, and unsupervised pre training is interesting and useful. The paper is clear to understand and the most results are presented effectively. Weaknesses:   When comparing different optimizers, the optimal learning rate for different optimizers can be different. Hence, the argument of using the learning rate which is better for one optimizer (SGD), also for the other optimizers (Adam and RMSProp), is not precise. This is especially concerning because the relative ordering of methods might change, and the conclusion w.r.t optimizer might not hold. Similar concern as above for experiments with different backbones, as different backbones might need different learning rates to obtain the best performance. It would be valuable to see how the performance of these methods would change with datasets having different number of classes, which is another important factor. The three datasets studied here all have 10 classes. al.Parting with Illusions about Deep Active Learning Some typos:* Introduction, paragraph 1: it exists a large unlabeled U → there exists a large unlabeled set U * Introduction, paragraph 2: include the for training → include them for training * Introduction, paragraph 2: divers way → diverse way * Section 7, unsupervised pretraining of the network: In such as setting → In such a setting * Section 7, unsupervised pretraining of the network: Figure 10a show → Figure 10a shows  I believe more thorough experimentation is needed for backing up conclusions in some parts like the effect of optimizers and backbone. It would be very valuable to compare popular semi supervised methods in the section on using unlabeled data.<|endoftext|>It also highlights the main factors that can influence the performance of AL methods: the construction of an initial training set following a certain strategy instead of random sampling, and pretraining the backbone of the network in an unsupervised manner. In addition, it provides solid benchmarks to compare new with existing methods in sections 5 and 6. In addition, it discusses the merit of recent trends in AL such as using unlabeled data, initial set construction, and unsupervised pre training of the network. There are a few parts of the article that might benefit from some attention in revision:      The greatest concern is with the assessment of classification accuracy. In contrast, this paper does not explicitly show the labeling efficiency of selection algorithms with respect to random sampling. In addition, it is not clear how many fewer labels a selection algorithm needs in order to achieve that test accuracy with respect to random sampling. I doubt this problem can be removed by the use of labeling efficiency plots and motivate the need for studying labeling efficiency in future works as a means to more holistically understand AL selection algorithms. I suspect that the codebase of this article would be a useful resource for others to build on this work and help give this article impact on the community. This paper reproduces state of the art results of popular AL methods with different datasets and settings typically used in the literature. They also provide a new Pytorch codebase that will allow future researchers to evaluate and compare different strategies in a fair way.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The quality of the pseudo labels is taken into account as follows: datapoints for which the model produces a pseudolabel with large uncertainty are downweighted compared to datapoints for which the model has high confidence on the pseudolabel. The proposed training strategy is used to train models on different versions of QM9 and PC9. The first case considers training on all the labeled data in the training set of QM9, and the data of PC9 is used as unlabeled data. *The empirical results seem encouraging. The paper can be significantly improved in this aspect. * Comments on experiments:      what is the explanation for why using pseudolabels deteriorates performance on the $\epsilon_{homo}$ prediction for SchNet in table 3 for 1% labeled data? Although I appreciate the fact that the authors did an ablation study, several things are unclear. However, as explained above, the paper could be greatly improved in terms of explaining the method.<|endoftext|>PseudSigma is a simple, effective, and model agnostic algorithm with robust empirical improvements. From the technical point of view, the PseudSigma is promising. Both the strengths and weaknesses of this paper are obvious. (2) The method part is well explained, even for readers like me who are not working on the self training problems. I expect it can be generalized to more general problems, in addition to the QM tasks. (4) Almost consistent performance improvements can support PseudSigma. (1) There is a gap in why using PseudSigma on QM tasks. First, the authors claim that PseudSigma is simple, effective, and model agnostic (termed SEM for short). And how is this formulation connected to SEM? If so, authors should point this out explicitly. Otherwise, authors can provide different view points? (2) Lack of self training baselines. For example, `the perturbational noise in 3D conformation can lead to drastic energy differences`. The authors can consider adding it into baselines.<|endoftext|>The paper is focused on improving quantum mechanical properties of molecules in a setting similar to semi supervised learning, where X is known for samples without Y. In an iterative fashion, the current model is used to predict pseudo labels for those samples, then pseudo labels together with labeled samples are used to improve the model. Strengths:  Empirical results show improvements over competing methods  Connection between evidential uncertainty and entropy minimization is interestingWeaknesses:  Contribution of the work is not clearly stated  The technical novelty of the method is very limited: 1) Instead of regenerating pseudo labels every epoch, they are regenerated less often (after an "episode": a set number of epochs).<|endoftext|>In this paper authors propose learning a single model with time to time regenerating pseudo labels and incorporating these pseudo labeled data according to the model uncertainty. Having all this in mind I recommend to accept the paper. Moreover it demonstrates improved results for out of distribution data. **Strong points**  At first time, authors adapt and study pseudo labeling training for quantum calculations problem which has its own challenges and motivations  Authors propose to use evidential uncertainty to estimate uncertainty of the pseudo labels and demonstrate that proposed adaptive weighting mechanism could be viewed as parts of the entropy minimization framework (or entropy minimization implicitly uses adaptive weighting)  Demonstrating that algorithm works for different model backbones (SchNet and DimeNet++)  In speech and image recognition it was demonstrated that the key components of iterative pseudo labeling success are model noise (e.g.dropout) and data augmentation. This paper demonstrates that even with no noise (neither in data nor in model) pseudo labeling can give significant performance boost. Any idea why for 1% train data for HOMO pseudo labeling doesn’t work with SchNet? These works are relevant in the way how authors performs iterative training with continuous model training. Does “ uncertainty” ablation diverge actually? As model fully converged on supervised data?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; One of the key quantity that the authors find to give significant insights is the largest eigenvalue of the Hessian matrix. Further investigation to various architectural choices   such as normalizations and initialization techniques indicate that the value of \lambda behave somewhat consistently after the initial training period for well training models. I think the analysis that has been done across multiple architectures and models provides a good basis for the authors claims in the text. Strengths:* The paper contains a very solid set of large scale and in depth evaluation, which is something well needed for this kind of empirical work.<|endoftext|>The paper is a mix of several empirical observations: (Section 4) One of the main observations is that the necessary (not sufficient) requirement for a "successful" training is to have *relatively* low max eigenvalue through the training process, where *relative* is roughly determined by the inverse of the learning rate. Strengths:* The paper includes quite a lot of experimental results and definitely provides valuable empirical observations (see them in the brief summary above). Here are the questions for each section:* Section 4. I d guess it s the optimal (peak) learning rate per batch size, but it s not fully clear. So I may be missing something here.<|endoftext|>Can one predict this factor from the number of layers in the model ? This could be connected to the question of lazy vs feature learning regimesAlthough the results presented are not particularly groundbreaking, the paper is rather well written and easy to follow, and gives some potentially useful insights on an important topic : learning rate scheduling. The observation that lambda_1 follows 2/lr in presence of warmup could be better connected to Cohen et al.with a sentence such as “Cohen et al.showed that lambda_1 increases up to 2/lr then plateaus at the edge of stability, in the constant lr setup : we show that this holds even in presence of scheduling”  The figures are extremely long to process for the various PDF readers I tried.<|endoftext|>They empirically studied the evolution of the loss sharpness as they vary the learning rate, warmup period, initialization, and architectural choices. And different methods such as initialization, learning rate warmup, and normalization all enable higher learning rates to be used by reducing \lambda_1 during training. The paper presents the necessity of low curvature at the early stage of training for stable neural network training with large learning rate, some notable observations includes “the mid training conditioning is determined by the learning rate, not on the initialization method used”; “Learning rate warmup can match the performance of recent advances in initialization research”. If so, why not study them as well? On the other hand, the DenseNet experiments says that the models with Batch Normalization actually start out with higher curvature than the non BN variants, which suggests that curvature may not be able to explain the effects of BN quite well and no smoothness benefits are observed at initialization. I would hope there are more investigations here as BN is still a critical module and the authors observation contradicts previous belief.
Reject; rating score: 5; rating score: 5; rating score: 6; The authors introduce an approach for learning identity preserving transformations from data. First the low dimensional manifold structure of the data is learned using an autoencoding neural network, then the transformations (the Lie group operators and their coefficients of combination) that map between perceptually similar image pairs are learned. The main contribution of the paper is that the learned operators can transform the data semantically. The authors address these challenges using two auxiliary networks. I m not convinced the use of CAE and beta VAE as baselines is appropriate. I would have also liked to see a comparison against another approach that uses Lie operators (e.g., [1, 3])A common dataset for analysis of disentangled representation learning algorithms is [4]. 5154 5163[3] Connor, Marissa, and Christopher Rozell. I m not convinced the baselines or datasets used in the empirical analysis are most appropriate for this work.<|endoftext|>The manuscript proposes a method for learning group action on data using features from classification networks. The idea is natural and easy to follow. The proposed way of learning local transport also makes sense. The additional step of combining MAE together with another encoder network for local operation makes the decomposition even more ill posed. It would be nice to have a better explanation of why the desired decomposition (i.e.latent variables, group action, local action regions) can be obtained.<|endoftext|>This paper proposes to learn natural transformations in datasets, with manifold auto encoder (MAE), where the underlying identity preserving transformations are not easily identifiable. By using Lie group operator, this problem reduces to learning paths/motion in the latent space of MAE. The main challenge in training MAE is how to choose a transformation pair, a point and its identity preserving transformed counterpart, in an unsupervised way. This paper introduces the idea of using penultimate layer  of a pretrained classifier on Imagenet to choose such point pairs and learn such MAE more effectively. The proposed method is validated on three datasets. Three stage training phase for MAE is already something and on top of that, this paper introduces to exploit a pretrained Resnet classifier and a coefficient encoder network with Laplace distribution prior on the coefficients.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The authors introduce a framework to evaluate the picture word interference in CLIP which is known to exist in human cognition. It is designed to test whether language biased decisions occur across different category levels, and the extent to which picture word interference in CLIP depends on the semantic similarity between superimposed words and images. [Overall] This is a very interesting piece of work that analyzes whether the state of the art artificial vision and language model; CLIP, resembles human cognition or not. [Writing]  I though that CLIP is trained by inputting image and words separately, but according to the authors statement in Section 5: "how CLIP acquired the ability to read superimposed words", it seems that it models language information from the image (superimposed text). Is this true? Please also introduce in detail the task settings so that this kind of confusion will not occur.<|endoftext|>This paper introduced a benchmark task for evaluating how semantic processing of visual word form interfere with the recognition processing of an object image in word embedded images. By placing category word (e.g., superordinate category like  electronic  or basic category like  laptop ) in the center of an image, the authors evaluated the 1) the misclassification rates of CLIP; 2) the semantic/spelling similarity between original and superimposed labels under different conditions; 3) the change of model prediction probability; 4) RSA on images with different types of picture word interference. The evaluation results brought novel insights to understand the recognition process of the CLIP image encoder, showing its distinction on visual representations of word forms and object images. In general, this work provided interesting new insights on how language information bias the image classification in visual language models like CLIP. Overall this paper is well written. The authors are trying to figure out how the visual representations of word form and object images interfere with each other and bias the classification behavior. This is an intriguing question driven by observations and theories from cognitive science. In general, I agree with the authors  claims. Does it imply that the representational space of "*words in visual form*" should have similar structure (in the sense of pair wise similarity) as the representational space of "*word in textual form*"? What s authors interpretation on this? 7.What does the "semantic compositionality" in the paper title refer to? I feel this term is weakly connected to the main text. The benchmark was tested with the latest model CLIP.<|endoftext|>This paper proposes a testbed for picture word interference in image classification models. The authors specifically investigate the performance of CLIP to understand whether such language biased model shows similar interferences to the ones observed in humans. The dataset consists of images superimposed with words, representing two different category levels (basic and superordinate). Further analysis shows that CLIP image representations are different from the ones of superimposed words, while this does not happen in two ImageNet based CNNs. Figure 1 could also be improved to depict an example of the task itself (Fig A1 is already better), which is unclear solely by the figure and potentially confuses the reader. However, as also mentioned by the authors in Section 5, CLIP was trained on a different dataset. It would have been instructive to train CLIP on ImageNet and investigate whether the language biased modelling is still different from CNNs. The paper presents a useful dataset to evaluate word superimposed image recognition models. The analysis on CLIP shows how it behaves for different categories of superimposed words. Further experiments on ImageNet based CLIP would make the paper stronger.<|endoftext|>The authors investigate the behavior of CLIP when its handed imagessuperimposed with text. They construct a corpus of {12 superordinate}x {138 basic} x {~150 images with FMRI data} by superimposing labelsover each image and then measuring CLIP s response. The connectionto Rosinski s work is interesting, and understanding how CLIP dealswith the ambiguity of this situation is perhaps interesting from theperspective of adversarial examples. I had a few fundamental concerns with this work. The authors motivate their considerationby citing Rosinski s work, in which he showed children pictures ofvarious objects with correct and incorrect labels, where the childrenwere specifically tasked with labeling the image (and not thetext). But, CLIP has no access to such asymmetric direction, so tocall transcribing the text incorrect, to me, is misleading. The evidence: label switched"misclassifications" (where the model predicts the written word ratherthan the object depicted) don t depend on the semantic distance to thedistractor word (as measured by word2vec vs. the true object class)nor the spelling (as measured by Jaro Winkler), as they do withhumans. But, my concern regarding the ambiguity of the setup remains: I don tthink it s fair to simply call the text transcription "incorrect". Andso, without accounting for that, I don t think I buy the arguments theauthors are making with respect to comparison to humanexperiments. But: because the "correct" answer (or a task description)isn t ever given to CLIP (as it was to the children in Rosinski swork), I can t bring myself to be convinced by any of the experimentalclaims.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 3; The authors propose a backdoor technique for NLP models that is agnostic to the downstream task or dataset when the poisoned model is used with transfer learning. Empirical evaluation on a variety of downstream tasks and datasets demonstrates the effectiveness of the proposed approach. Apart from some cosmetic changes mentioned above, along with the issue of a more vigorous evaluation of potential defenses, I feel the paper is in good shape and would make an valuable contribution to the conference proceedings. 3.Section 5.2: "...by just checking the training performance of the downstream tasks." The authors can probably restructure the paper to focus more on experimental findings and less on re iterating the same points. It feels as though this defense would have the potential to work well, and it would be worthwhile for the authors to include this. 3.Section 5.2: "For simplicity, in our experiments, all the values are...". 2.Section 4.2, "However, they are not very useful for specific practical NLP tasks." Please provide a reference for this, or elaborate.<|endoftext|>The authors state that a backdoor attack should satisfy the following properties: effectiveness and generalization, functionality preserving, and stealthiness. The authors evaluate their method by pre training BERT models and fine tuning them on 10 different downstream tasks. However, the backdoor triggers show to be effective against all fine tuned models as performances drop drastically for malicious inputs. The proposed method is simple yet effective. The paper is well written and the experiments are extensive and technically sound. Remarks:* It would be helpful to briefly elaborate on the practical applicability and relevance of backdoors attacks in NLP. Why do they pose a threat? How can they be used maliciously in practice? * It would have been interesting to see whether BadPre would be as effective against other variants of BERT (e.g., robustly pre trained models). It would be interesting to provide some insights into whether the entire pre training dataset needs to be poisoned for the attack to be effective, or if poisoning only a fraction of the data is sufficient. Overall an interesting paper and a relevant contribution. I recommend acceptance to the conference.<|endoftext|>This paper proposes a task agnostic backdoor attack against the pre trained NLP models. Experiments evaluate the attack effectiveness of BadPre from the perspective of functionality preserving, effectiveness, and stealthiness, and demonstrate that BadPre can attack a wide range of downstream NLP tasks in an effective and stealthy way. This paper is overall well written and the method is simple and easy to follow2. The task agnostic backdoor attack is an interesting and less explored problem, which can be a real practical threat. Otherwise, it is clear if BadPre can indeed improve upon prior work. The authors explain that it is due to the “the conﬂict of trigger words with the clean samples”. Could the authors provide some qualitative examples? c) In Figure 2, the authors show the same attack success rate when injecting one or two trigger words, which is a bit counterintuitive, as it is expected to see a much higher attack success rate when injecting more trigger words.<|endoftext|>The paper proposes a task agnostic backdoor injection paradigm BadPre for pretrained NLP models. This model can then be finetuned on any downstream task and data regularly and can be attacked by injecting the trigger in the input. However, I am not fully convinced about the empirical evaluation and why the proposed approach should work in practice. **Weaknesses:**1) The idea proposed in the paper has very limited technical novelty. 3) There is no reasoning or explanation provided as to why the pre training tasks (MLM, NSP) which are very different from the downstream tasks should transfer the backdoor triggers to the downstream task. This is a very important aspect of the paper, and there is no empirical evidence to this claim as well. While these attacks may require information of the downstream task, it is still important to compare how the attack strength and functionality compares with previously suggested attacks. I disagree with the claim made in the paper about the stealthiness of the BadPre attack.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper considers the linear regression estimator, and gives the exact formula for the test risk in the asymptotic regime for the rank 1 case; for a general rank r, the paper also gives a formula, but it s not entirely rigorous. Below are some major questions/issues. This needs to be explained clearly. This assumption means that the datapoints are not independent, which makes it unrealistic for statistical learning. 4.Even the introduction section is quite chaotic. This paper extends double descent (wrt data size) to a low rank denoising setting with linear regression.<|endoftext|>The theory is presented for the linear case, whereas the experiments cover also neural networks and MNIST. I see that these works are cited but I think these should be presented already on page 2 where sample wise descent is motivated. The paper does not have a conclusion part. * Figure 7 caption: for the our formula  > remove theThe paper proposes a low rank data model and studies the double descent phenomenon for denoising tasks.<|endoftext|>Some of the figures can be removed to make room for it, in my opinion, such as Figure 1 and 6. The paper is clearly written and is easy to understand. They experimentally show double descent in nonlinear networks as well. If the results are similar then even that is convincing, even if it is empirical. Overall, I based my score on the fact that the paper is incomplete and rough, not because the work has major problems. **Related work:**The related work section is very small and needs to be expanded in order put the author’s contribution into context. This is a weakness.<|endoftext|>This paper considers a denoising setup for a linear model. The authors provide an asymptotically exact formula for the generalization error for rank 1 data and an approximation for higher rank data. The following are some detailed comments. 1.Authors claim that the noise in the training samples can be controlled. 2.The explanation of the double descent with respect to the number of training samples is not satisfactory.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The authors of this paper proposes a DLGN (Deep Linear Gated Networks), a novel class of deep networks, inspired by a recent dual view where the computation in DNNs is broken into two parts: learning in the gates and learning in the weights. DLGN’s performance recovers 83.5% of SOTA DNNs. In particular, the current study overcomes the limitations of NTK framework, by proposing NPK (Neural Path Kernel) for DNNs with gating and dual pathways. Perhaps the connection to these lines of work can be discussed more clearly. Is there any way to reduce the number of parameters in gating network, and can this be used to explain why the performance of the DLGN is a bit worse than SOTA? Can Fig 1 be more detailed? Perhaps it should be stated as an interesting direction for future study. Admittedly, exploring the expressivity and capacity of DLGN is an interesting question in its own right and it s unclear why this is  emphasized only at the end of the paper. The goal and the motivation of the paper, and the connection to the existing prior work could be made more clear, but once those are addressed I am willing to change my score.<|endoftext|>By theoretical analysis and empirical experiments, the paper argues that the neural network is learned path by path instead of layer by layer. Disentangling neural networks in the path space is a really interesting perspective, which may give new insight to understanding neural networks. 2.By theoretical analysis, the paper proves some novel propositions (e.g., ResNets have an ensemble structure, NPK is rotationally invariant) which might be useful to understand deep learning models. Otherwise, it would be hard to follow for readers who are unfamiliar with DGN. However, the "input 1" experiment and the "layer permutation" experiment on DLGN only prove that **DLGN can** be learned path by path, and this conclusion doesn t necessarily hold for the general neural network. In Table I, why is the performance of models whose layers are permutated slightly higher than the original models? But some of its claims are not well supported or well explained. The writing of the paper is ok, but the prerequisites of the paper can be more comprehensive.<|endoftext|>This paper deals with the entanglement in the DNN through two steps. First, replacing the rectified linear units (ReLU) in the traditional DNN with Deep Linearly Gated Network (DLGN). Second, demonstrate the weighted network is disentangled in the path space. The view of this paper is interesting since the success of the DNN is its non linear ability. 2.And both the theory and experiments are sufficient to support their claims. It is a little bit hard to read for me.<|endoftext|>This paper proposed deep linearly gated networks (DLGN) for interpreting DNNs with ReLU activations based on a dual view. The proposed framework is able to completely disentangle the ‘gating network’ and the ‘weight network’. Finally, the experiment results demonstrate that DLGN can achieve good performance for classification on two benchmark datasets compared to the existing DNNs. This work also offers some theoretical analysis to better understand the proposed method. 3.Compared to the prior work on dual view, this paper presents more insights and new results for convolutional networks with global average and ResNetCons:1. I am curious about what experimental results they will be. 2.This work did not discuss the impact of $\beta$ in the soft gate on the experimental results3. It is hard for the audience without background to understand some theorems. The idea of using deep linearly gated networks (DLGN) with dual view looks novel and interesting. I think this paper should be above the acceptance threshold.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper looks at how to aggregate time series inputs to LSTM models, and recommends to use non uniform aggregation, with small recent bins and larger older bins. And how to apriori know which partition may be helpful for what kind of time series (maybe based on auto correlation / spectral properties). The main idea in the paper   of using non uniform aggregation of inputs to LSTM is reasonable, and I am not surprised that it seems to show promise. Unfortunately, experimental results are flawed, and only partially convincing. It s not really an  exponential  partition. I assume it s the split point. What happens when the input window is not length 48, and when you re not predicting 12 steps ahead?<|endoftext|>The paper studies different windowing schemes and pooling schemes for feeding data into an LSTM encoder for time series forecasting. The windowing schemes considered are different uniform divisions of the history and exponential window sizes where data further back in the past is aggregated over larger windows. The paper does not discuss or compare with well known advances in architectures that are specifically designed to attend to longer histroy length.<|endoftext|>However, I do not think there is sufficient novelty or new ground being broken in this paper to warrant acceptance. In this paper, the authors introduce an exponential partitioning, whereby the time series is split into bins but the bin sizes increase as we move further from the current time. Tables and graphics are non consistent and not nice on the eyes. The authors test the approach for a few different aggregation functions (mean/median/min/max) and show good improvement of the proposed method compared with the simple baseline.<|endoftext|>In this study, authors propose an exponential partitioning procedure to minimise the vanishing gradient and exploding gradient problems that can occur when forecasting time series with long time series. Based on the current status of the manuscript, I am recommending to reject this paper. Major Weaknesses   In the abstract, authors phrase, “Long short term memory (LSTM) models are the state of the art for time series forecasting”, which is a very strong claim to highlight given the recent success of other machine learning architectures in the time series forecasting space. The results of these competitions are often used as a yardstick to measure the quality of newly proposed forecasting methods.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes an adversarial training method, noise FGSM(N FGSM), which improves the RS FGSM by removing the clipping perturbation and increasing the magnitude of adding noise. 2.The experimental results indicate that  the proposed N FGSM can achieve better performance than RS FGSM and GradAlign in terms of adversarial accuracy(Robustness). 2.The motivation of this paper is unclear. The authors raised the CO problem but turn to the two "key components". The two so called "key components" of adversarial training, clipping and random steps, are unconvincing. By eq(4), the adversarial perturbation will be definitely larger than the baselines. Theorem 1 is obvious and hard to be the contribution of this paper. And the table is very hard to understand. Does it represent the magnitude of $\epsilon$ for training? I suggest a clear rejection for this paper.<|endoftext|>This paper aims to address a failure mode in the traditional single step adversarial training known as catastrophic overfitting. They show that compared to the common practice of generating the adversarial perturbation, adopting larger random initialization and avoiding clipping the perturbation can effectively mitigate catastrophic overfitting. The effects of these two techniques are analyzed empirically in detail, followed by a comprehensive comparison to other methods. The paper is overall well written and easy to follow. I think more (empirical) analyses will be helpful to understand why this is true. * Consider include some numerical results in the main paper. I am willing to increase my score if the authors can better clarify the proposed method and provide more intuition.<|endoftext|>Based on the empirical findings, this paper discovered that the absence of clipping as well as using stronger noise could help avoid catastrophic overfitting. The author further proposed Noise FGSM, utilizing single step FGSM and noise augmented samples to generate adversarial examples for training. Strengths:+ Overall, this paper is well written. + Empirical results look fairly good. The choice of very large $\eta$ (e.g.2$\epsilon$) in Algorithm 1, further equipped the adversarial perturbation with a larger attack budget. It will be better for the authors to present the real $\ell_p$ norm of perturbations generated by N FGSM. In all, although the method proposed in this paper beats the baselines, it may not be a fair comparison and there are some fundamental errors in the experimental settings. I did not penalize its simplicity (this is fine to me), however, its effectiveness should be further justified either empirically or theoretically, e.g., whether or not the proposed design is applicable to fast adversarial training using TRADES type loss, or other similar and in depth studies.<|endoftext|>Different from previous intuitions, this paper find that not clipping the perturbation around the clean sample and using a stronger noise is highly effective in avoiding CO for large perturbation radii. ##########################################################################Pros:  1. The paper attempts to improve the efficiency for adversarial training. Although the authors give theoretical analysis to understand the role of noise insingle step approaches, the theoretical analysis on that increasing the noise magnitudeand not clipping prevents catastrophic overfitting is not provided. Overall, I vote for accepting. This paper find that not clipping the perturbation around the clean sample and using a stronger noise is highly effective in avoiding CO for large perturbation radii, which is interesting. Hopefully the author can address my concern in the rebuttal period. The main drawback of this paper is the lack of theoretical analysis. The reason why it works is not clear enough. I believe that the paper may have potential but as its current form has some weakness.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper focuses on designing efficient deep networks with a proposed novel parameter free layers. The first attempt to investigate built in parameter free layer as a building block for network efficiency. 2.Good and convincing empirical studies.<|endoftext|>The author shows that using the parameter free operations (e.g., max pooling) can also achieve a good performance. I think the author could work on that and show how much speedup it has. But the results in the paper are not suprising yet, even with the deform_max. This seems to be a pretty normal accuracy efficiency trade off.<|endoftext|>Still, the empirical results are intriguing, showing that simple max pooling ops can be used in place of convolutions much more than I might have expected. This paper argues for more extensive use of max pooling layers in image classification networks, as an inexpensive substitute for convolutions. While the most specific claims are supported by the experiments, I think the overall framing as a demonstration of parameter free layers is over generalized for what has been demonstrated around max pooling. How many layers were replaced at a time, and with which operations?<|endoftext|>+ Very extensive experiments to support authors  claim on the effectiveness of the parameter free operations. This paper shows a simple yet effective finding and validate this with extensive experiments.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper introduces a differentiable kernel based estimator of differential entropy, named KNIFE. Estimation of (differential) entropy and the related mutual information are topics in machine learning of fundamental importance, which drive a wide range of applications. 2.The proposed method is relatively easy to implement. There is not much novelty in this work. The proposed solution is a simple plug in estimator based on adaptive kernel density estimation.<|endoftext|>This paper provides a new approach to estimating differential entropy called KNIFE that is also applied to mutual information estimation. The authors define their estimator using a parametric model based on estimating a KDE. The authors provide some theoretical analyses of the estimator and multiple empirical experiments where the proposed estimator outperforms several other estimators. However, many of these estimators are based on plug in approaches similar to the KNIFE estimator. Thus I believe that they can be compared to as well. The authors should include these in the discussion of prior work at the very least and compare the empirical results to the ones that are also differentiable.<|endoftext|>The paper proposes an estimator $\widehat{h} _ {\text{KNIFE}}$ for differential entropy suited for applications in deep learning. A set of desirable requirements is specified all of which are satisfied by $\widehat{h} _ {\text{KNIFE}}$ but not by other commonly used estimators. The paper tackles the important problem of estimating commonly used information theoretic quantities like differential entropy and mutual information. This paper uses a modification of standard kernel density estimators to estimate $p$, and therefore gets$$\widehat{h} _ {\text{KNIFE}}    \frac{1}{n} \sum _ {i 1}^n \log \widehat{p} _ {\text{KNIFE}}(x _ i; \theta). The paper needs some rewriting of the technical sections, but otherwise is a good paper.<|endoftext|>The authors prove consistency, and demonstrate that on various downstream tasks where MI based regularization is needed, the proposed method outperforms previous work on entropy / MI estimation. It is argued that, in ML tasks where the data $x_i$ are the learned representations and change during training, the modification allows the estimator to be both flexible and efficient. The strength of the method mostly lies in its practicality (in representation learning tasks): it does not provide proper lower or upper bounds for mutual information, which is the central quantity of interest in the downstream tasks. The novelty is limited, but this is fine if there is consistent improvement in empirical performance. For example, from a quick scan it appears Mahabadi et al (2021) used a very heuristic estimation of MI, by fitting multivariate Gaussians on the joint and conditional distributions, yet demonstrated a similar level of improvement in GLUE as here.<|endoftext|>This paper studies differentiable proxy estimators for density function, which is in turn used to compute various information metrics such as (conditional) entropy and mutual information. The main contribution of this paper is that it generalizes previous kernel based density estimators by parameterizing the anchors and mixture probability of kernels. [Weakness]\The technical significance of the proposed method seems incremental: The Schraudolph estimator proposed before already includes the covariances as learnable parameters, and there does not seem to be many technical challenges of making the anchor and mixture probability learnable as well. The background information provided in the statement of novelty improves my view on the matter, though I hold to the opinion that extending the Schraudolph estimator does not seem to be particularly challenging. Overall it is a paper of quality.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper proposes to convert residual network architectures to equivalent "plain" networks after training. If the functional forms of the residual and plain networks are equivalent, then any pruning operations on one form should have an equivalent expression in the other. Why is it necessary to first convert to a plain network, instead of simply considering pruning of residual connections in the original form? In fact, the proposed approach seems to add computational work (more convolutional filters, Dirac initialized) for the sole purpose of taking advantage of fast implementations of convolutional layers.<|endoftext|>This work focuses on removing residual connection in network via  reserving and merging (RM) operations on the ResBlock. The author has tested the approach on classification tasks on several networks with skip connection, e.g., resnet, mobilenet v2, etc. 3.Beside the RM steps, there are also other additional operations like pruning operations, it would be good to see the improvements from different operations. In general, the paper has showed the strength in removing the residual connections on the classification tasks, it would be much more convincing to have more experiments on other popular tasks.<|endoftext|>This paper presents an improved method (RMNet) for removing residual connections from a ResNet   type neural network after training. Main Review (+   positive comment, o   neutral comment,     negative comment) \+ The strength of the paper is the relatively easy to implement method that can be applied to many ResNet variants in order to remove the residual connections. The results and discussion on finetuning, pruning etc. are also quite useful.<|endoftext|>The motivation of this paper is clear. This paper is also well written and easy to follow. 2.The method of re parameterization is very useful in model design and network pruning. However, the re parameterization has limitations such as the difficulty of removing residual connections across non linear layers. To overcome the limitation, authors propose RM operation to equivalently remove residual connections from ResNet based architectures which enables the plain models to keep higher performance for deeper networks. It is known that ResNet is harder to be pruned compared to plain model because of the existence of residual connections. The paper puts forward a novel method: RM Operation, which can equivalently remove residual connection across non linear layer in ResNet based architecture and shows great power in network pruning.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This work presents a number of empirical comparisons for the metrics on uncertainty estimation performance such as AUROC, ECE with 484 ImageNet classification models and has drawn some observations such as the superior properties of distillation over vanilla training, pre training and adversarial learning, the superiority if ViT over other networks, and so on. Strengthes:  This work has extensive numerical studies on the metrics of uncertainty estimation performance and has drawn some observations. There is no new method, no new metric to be proposed in this work. In other words, the contribution of this work may be purely empirical. Many observations were lined up, but what are the main contributions? In addition, how did the authors implement g in experiments? It seems that g is important to define in details. It is clear how MC dropout measures the uncertainty, but it is not clear how selective model does. Some of the observations are interesting from extensive experiments, but there are a number of concerns on the metric to be used (e.g., selective model for uncertainty).<|endoftext|>The paper presents an empirical study and focuses on evaluation of uncertainty with respect to different model architectures and training schemes. Using metrics capturing calibration and accuracy, the authors conclude that distillation based schemes are efficient in uncertainty estimation and ViT performs the best uncertainty estimates among the architectures considered. The paper conducts extensive experiments with 484 models to gather insights into training schemes and model architectures for capturing uncertainty. Uncertainty due to class out of distribution should be grouped into epistemic uncertainty, which by definition describes the uncertainty stemming from lack of knowledge. I think this result was not surprising given knowledge distillation s connections to ensembling, which is a state of art method for uncertainty quantification that captures epistemic uncertainty. I recommend adding a discussion on this point. Overall I recommend a rejection of the paper.<|endoftext|>In this regard, it is recommended to review the information and results included in the main paper, compared with the content of the appendix, and include the necessary introduction to the concepts and definition, making it less for readers necessary to jump into the appendix and back to the paper. The capability of models to reflect a calibrated uncertainty is interesting for different applications that require a reliable confidence indicator for the model predictions. However, some points regarding clarity on the document can be commented on. Models are classifiers trained on ImageNet.<|endoftext|>The paper provides an empirical comparison of uncertainty estimates obtained from 484 deep neural networks (DNN), trained for image classification tasks on the ImageNet dataset. 3.For OoD detection, I would like to see a comparison of * baseline models such as ResNet* Models with advance architectures (such as ViT)*Feature density based models that use data likelihood estimates to identify OoD samples. The authors provided a thorough comparison of different architectures and training strategies. 7.The paper demonstrates the strength of ViT architecture in providing better uncertainty estimates and predictive performance. In International Conference on MachineLearning, 2020. Average confidence is a measure of softmax entropy and hence, is not sufficient to capture epistemic uncertainty.<|endoftext|>They not only provide a critical evaluation of the different metrics and their task dependent strengths and weaknesses, but also discover previously unknown empirical patterns in different architectures and techniques. I also really like this paper as an example of how to make use of libraries of pre trained models but nevertheless do extremely thorough empirical work. In general it might be good to comment more on the limitation of relying on ImageNet, and add some consideration of how this might differ for other datasets/data modalities. Title of Appendix G should be `Effects , not `Affects This is a great paper. I have some quibbles about structure, but the basic results are extensive, interesting, and important.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper empirically demonstrates and theoretically investigates an important problem of representation learning in general and contrastive methods in particular. This problem was previously widely known for the “completely collapse” case, where every representation is mapped to a single point, hence the introduction of explicit negative pairs in contrastive methods to avoid this trivial solution.<|endoftext|>The paper discusses the dimensional collapsed problem in contrastive learning, and analyze two reasons to cause the dimensional collapses solutions. According to the analysis, they provide a simple method named DirectCLR to solve the dimensional collapse problem. The analysis in toy model (e.g.linear layer, additive noise) is convincing.<|endoftext|>This paper theoretically proves that the contrastive learning results in the dimensional collapse in the feature representation space. Such as,        * How this  result interpreted with one of the claims? * The claims are theoretically well described and the process of proof is easy to follow with the supplementary proofs in the appendix.<|endoftext|>They thus propose a new framework called DirectCLR to solve the dimensional collapse in CL. This paper is interesting. The experiments are also exciting, which shows that the projector may not be necessary for contrastive learning.<|endoftext|>This paper investigates the collapsing problem of contrastive learning. The experimental validation is quite limited. The authors may want to compare with them.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper proposes a hybrid SGMCMC algorithm, a Langevin type algorithm that operates on a modified energy function based on the variational inference (VI) approach. 1.The proposed algorithm is not well motivated: the VI approach aims to obtain a point estimate, SGMCMC aims to draw a sequence of samples from the target posterior, but the proposed algorithm is to simulate a sequence of samples from a modified distribution. It seems that they are ``comparable in wall clock time , while the baselines pSGLD and SGHMC might not be the state of the art. This paper proposes a hybrid SGMCMC algorithm, but the underlying theory is not fully developed and the performance of the algorithm is not fully explored.<|endoftext|>Overall I enjoyed reading the paper and I think the proposed method has merit, but I also do have some concerns. For example, the ESS in table 1. 3)	Please note, that in the recent paper [1] it was shown that ESS and “other standard MCMC metrics which do not account for sample bias are not appropriate diagnostic tools for SGMCMC”. This paper [1] proposes to use the kernel Stein discrepancy metric for the assessment of SGMCMC methods. I would recommend the authors include it in their analysis. 4)	On page 4 the authors assume that “This partitioning structure is assumed to be known a priori.” when talking about factorization of the parameters into mutually independent groups. 6)	On page 3 it states: “The gold standard for approximating the entire posterior distribution is by deploying Markov chain Monte Carlo (MCMC) algorithms. The paper, however, does not provide theoretical guarantees of convergence while empirical evaluation metrics were not chosen optimally (only univariate metrics which are also not optimal for SGMCMC methods as shownin [1]).<|endoftext|>However, I can only find the accuracy metric in the experiment, which is different from the predictive likelihood. The paper presented an interesting approach to incorporate independent structures into posterior inference. However, there are still some ambiguities in motivation and theoretical soundness. To better support the claims, some metrics or experiments regarding uncertainty quantification should be added or discussed since the proposed methods also seem to underestimate the variance. It address some of my concerns. However, for my last concern regarding the uncertainty (which is one of the reasons we consider MCMC over variational inference), the author did not mention it at all. However, I cannot see direct reasoning behind those claims, could you elaborate more on this? I thought the author mentioned that a **single** sample $\tilde{\theta}^{(t,i)}$ from the **current** timestep is used for Monte Carlo approximation. Another concern is the theoretical soundness of proposed algorithms. In terms of the empirical evaluation, I wonder about the uncertainty quantification ability of the proposed algorithms, which is an important metric for SGMCMC methods, especially since the proposed methods also seem to underestimate the posterior variance.<|endoftext|>However, this structured approximation is computationally expensive and requires the same number of evaluations of the approximation as there are parameter groups. This computational burden is alleviated by a dropout scheme, where instead of sampling from every parameter groups, parameters are masked using a dropout distribution, and the number of stochastic masks is a hyperparameter that controls regularization and fidelity to the structure imposed in the factorization. Experiments show that this is a viable way to impose structure on a variational distribution, and that mixing times are improved. This is a very well written, clear paper, with nice experiments to guide intuition. The effort shows. If the paper "attempt[s] to hybridize MCMC and VI", why not compare to structured variational inference? If so, it could be stated in a sentence, which would help me be less confused.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper carries out a variety of studies, both theoretical and numerical, on numerical solution of a Schrodinger equation using deep learning inspired methods. Valuable technical advances on an important and popular topic.<|endoftext|>This paper establishes statistical lower bounds and upper bounds for (a modified) Deep Ritz method and PINNs based learning of solutions of PDEs when the estimators belong either to a class of sparse neural networks or lie in truncated fourier basis. However the authors point out that they get better bounds due to the strong convexity of DRM and PINN loss.<|endoftext|>2.The authors proposed a modified DRM. In general, this paper is a good paper and provides solid theoretical results on solving PDEs by deep neural networks. The authors derived faster upper bound for PINN and DRM than existing results.<|endoftext|>Applying deep learning (DL) to solving PDEs numerically has been a very exciting research directions. The paper studies approximation power of DL based PDE solvers, mainly the Deep Ritz Methods (DRM) and Physics informed neural neural network method (PINN). DL based PDE solvers are important and exciting direction of deep learning.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 6; In this paper, the authors propose a method that applies DP SGD to NLP tasks. As a DP algorithm on a deep learning model, this algorithm achieves good performance on fine tuning the language models and several real world applications.<|endoftext|>This paper investigated the problem of privately fine tuning large language models for downstream NLP tasks, including sentence classification and language generation. Overall, this is a fairly empirical paper on an important problem and shows good performance.<|endoftext|>These two are important in boost the performance of the final models. Reason to accept:Simple DP algorithm with demonstrated reduction in memory consumption and effectiveness. 3.The experiments show strong results on both classification and text generation tasks, better than the prior DP methods in terms of memory consumption and accuracy/generation quality.<|endoftext|>This paper adapts the widely used DP learning algorithm, DP SGD, to language models. Pros,1) It achieves a remarkable performance of DP algorithms on NLP tasks with a satisfactory level of privacy protection.
Reject; rating score: 5; rating score: 5; rating score: 8; rating score: 8; This paper aims at improving the computational efficiency of the attention mechanism by leveraging the specific sparse pattern supported by sparse tensor cores of NVIDIA A100, with a particular focus on delivering practical running speedup. To achieve this, the authors proposed DFSSATTEN, which shows both theoretical and empirical advantage in terms of performance and speedup compared to various baselines. In particular, DFSSATTEN yields 1.27~1.89x speedup over the vanilla attention network across different sequence lengths. Strengths:This paper offers a simple method with clean code implementation to leverage sparse tensor cores for attention speedup. It demonstrates that the 50% sparsity pattern can retain model performance very well, and it yields 1.27~1.89x practical speedup over the vanilla attention across different sequence lengths. * The proposed method is very specific to the sparse pattern offered by NVIDIA A100, which is not generalizable. Also, the proposed attention method doesn’t solve the quadratic attention complexity issue. It would be great if DFSSATTEN could accelerates the training from scratch, but this is not studied in the paper.<|endoftext|>This paper presents DFSSATTEN, a fine grained structured sparse attention mechanism. Experiments show that the proposed method achieves speedups in A100 GPU. Why quality of Attention lottery is a good indication of softmax matrix approximation $A$, instead of the error between true softmax matrix $A$ and approximate softmax matrix $\tilde{A}$? Why Eq.6 shows an equality? LRA benchmark has 5 tasks. Why only 3 tasks are used for comparison? Recommend to include state of the art efficient self attention methods on LRA, such as [1], [2], and [3]? The authors only compare the self attention part, which does not necessarily represent the speedup of the proposed method for Transformer (forward and backward). The authors should report the running time and memory consumption for a vanilla Transformer with respect to different sequence lengths and compare with other baselines as in [4]. But the analysis and experiments have some issues (see weakness). I will give a borderline due to those concerns and change it accordingly based on rebuttal.<|endoftext|>The paper proposes a method that exploits the 50% structured sparsity supported by tensor cores on modern GPUs. The authors evaluate the proposed method on LRA and MLM tasks to show that the method can provide practical speed ups especially for moderately long sequence lengths where other proposed efficient attention mechanisms struggle due to overheads. Strengths:   Overall, I found the work to be interesting and the speed ups quite significant especially for moderately long sequences ($\lt 512$) where most efficient attention mechanisms struggle due to the overheads involved. Some questions and concerns:   While the empirical evidence showing that the proposed structured sparse attention can approximate the full attention is very convincing, I am not sure how well does the assumption $\frac{QK^T}{\sqrt{d}}$ follows i.i.d.$\mathcal{N}(\mu, \sigma)$ (proposition 4.1) hold in practice? This is important as a lot of the claims regarding the quality are hinged on this. It would also be useful to compare attention distribution for models finetuned with full and DFSSATTEN. I am concerned if using only memory access for theoretical estimates is a good approximation. It would be good to have a plot similar to figure 4 comparing random sparsity, and fix sparsity for few different values of $s$. Specifically why $\sigma \approx 1$ if higher magnitude edges are more influential? References missing in the table 4The paper showcases speed ups for a large range of sequences as well as the ability to do both training from scratch and finetuning with no loss in performance. Moreover its compatibility with other efficient attention mechanisms makes it a good choice for a wide range of applications.<|endoftext|>This paper focus on the dynamic N:M fine grained structured sparse attention implementation on transformers. Firstly, the authors analyze the theoretical efficiency of Top K Sparsity, Fixed sparsity and dynamic 1:2/2:4 sparsity, which demonstrates the dynamic N:M sparse attention can achieve considerable speedup and high quality approximation. Then, the authors evaluate the accuracy and the practical speed of the transformer model from the Huggingface model Zoo using the DFSSATTEN module and explain the detailed implementation using the cutlass library. The authors provide the source code with clear comments. The paper is well written and easily follow for industrial deployment. cons:In section3.1, the reason why this paper started from stage1 confused me, Chen focus on Nvidia Volta Architecture without sparse tensor core, this paper conduct experiments on Ampere Architecture. what about the speed of QK^T with N:M structured sparse on Ampere Architecture? Related work, https://arxiv.org/abs/2102.04010 which was the first research paper to train N:M fine grained static sparse weight from scratch  (including transformer) should be mentioned. So, according to the reasons, the speedup of dynamic sparse attention may be difficult with a higher sparse ratio (e.g., 2:8) in the future? The paper presents dynamic sparse attention to accelerate the attention module.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper considers the problem of Federated Learning (FL) with a random selection of clients in order to minimize $\sum_{i} p_i \mathcal{L}_i(\theta)$, where $\mathcal{L}_i$ is the loss function of client $i$ and $p_i$ is its weight, which is often proportional to the dataset size on client $i$. The authors first prove a decomposition bound (Theorem 1), which establishes a simple recursion for the distance to the solution. As far as I can see, this result is never used to establish a convergence rate. In addition to Theorem 1, under smoothness, bounded variance, and bounded dissimilarity assumptions, the authors study convergence rates of unbiased client sampling in the nonconvex regime (Theorem 2). First of all, most of the results are straightforward. The results from Appendix A require no more than computing expectations and simple summations. Theorem 1 is not even a convergence result but rather a simple one step recursion that can be obtained by expanding norms and scalar products. Theorem 2 is proved using standard techniques from prior work. In addition, I do not see any significant insight into the convergence of FedAvg. The impact of client sampling has been studied before and optimal sampling strategies were proposed by Chen et al.(2020), and other works, for instance (Fraboni et al., 2021), have studied more forms of unbiased sampling. This work gives a bit more detail into how it all affects training but it all seems trivial, but without new improved strategies for sampling, it is hard to see the value of this study. Most results follow by combining some simple identities for expectations of random variables with convergence results for Local SGD from prior literature (Wang et al., 2020a), (Li et al., 2020c). Adding a global stepsize also does not change almost anything in the proofs. While Federated Learning is an important and challenging problem, and client sampling may play a big role in its efficient use in practice, the results in this paper provide us with little new insight.<|endoftext|>The author(s) studied the impact of the client sampling for FL. In particular, the author(s) proposed a decomposition theorem and used it to obtain an improved convergence analysis of FedAvg. **Pros**:  The paper is well written  The theory is well developed and is correct to my knowledge**Cons**:My major concern is on the novelty of the theoretical analysis of clients  sampling. Clients sampling in FL and mini batch sampling in SGD are closely related to each other. Related works on the sampling strategy for SGD are not well address, for example [1, 2]. The major difference between FL and mini batch SGD is that FL runs more steps during the local update, but SGD performs a single step. But this difference should not cause too much trouble. It seems that the analysis of the sampling scheme for SGD can be directly applied to the context of FL. Could the author(s) elaborate on the challenge of applying importance sampling to FL? The discussion on unbiased sampling doesn t seem to be something new. The authors emphasized the importance and novelty of Theorem 1. I read the proof, and it seems to me that Theorem 1 is a detailed expansion of the variance term (instead of just bounding the variance by some constant in the literature). It is also straightforward that the variance can be decomposed as the summation of covariance. My comment on Theorem 2 is similar to the above comment for Theorem 1. The result is from a detailed expansion of the variance term, and I am not sure if it is significant enough. Some minor comments:  In eq.(1), is it necessary to introduce "I"? The author(s) said that MD has an additional O(n) cost to construct the distribution and could be expensive. I doubt if constructing the probability density for MD will introduce non trivial overhead. Stochastic Optimization with Importance Sampling for Regularized Loss Minimization. [2] Dominik Csiba, Peter Richt´arik. JMLR 2018. But I am not convinced by the significance of the theoretical analysis of this paper.<|endoftext|>This work studies the effect of client sampling in the convergence of Federated Learning. This work clearly formulates the setup of client sampling and studies the effect of client sampling to FL progress. 1.The main theorem 1 (decomposition theorem) is somewhat too microscopic (as it only studies one round) and more like a direct corollary of the equation (3), and is independent with the other procedure in FL (e.g., local optimization steps). Hence, I would recommend calling theorem 1 "decomposition theorem/lemma for client sampling", rather than the "decomposition theorem for the convergence of FL" 2. This makes it hard to interpret the trade off of the three terms in $Q(\theta^t)$. 3.The paper argues that MD sampling should be used as default case, while uniform sampling is superior only in the special case when clients have the same amount of data. I cannot find a theoretical justification of the first part of this sentence. It is only shown in corollary 1 that Uniform is better than MD in certain cases, but not the reverse side. From Theorem 1, it is clear to me how to argue when MD can be better than Uniform Sampling, as the third term in $Q(\theta^t)$ is always larger in MD. Is it possible to establish  the convergence without assuming sufficiently small local step size $\eta_l$? This claim does not sound accurate. The only necessary condition we can read from (7) is the negative inner product. Also, an optimization algorithm does not need to always make progress in distance to optimum every step to "improve" the model. 2.Table 1: the notation $\alpha$ appears much earlier than it was first referenced in the main text (before §3). My main concern of this work lies in the significance of the results as well as the interpretation. As this time I think the paper is marginally below the acceptance threshold, but I am happy to re evaluate the paper if the authors can address my concerns.<|endoftext|>This paper analyses two client sampling strategies for federated learning algorithms. This work compares Multinomial distribution and Uniform distribution. The authors provide a decomposition theorem, that gives some insights into the impact of client sampling. Moreover, they provide convergence guarantees for the general non convex case under some additional assumptions such as Bounded Dissimilarity. An experimental comparison of two sampling schemes is done at the end of the paper. This paper is well written and it has a clear structure and narration. All assumptions are described and all variables are well defined. Proofs seem to be sound. Does it mean that K SGD steps? The first assumption in this paper is Unbiased Gradient and Bounded Variance. Assumption 1 (Unbiased Gradient and Bounded Variance). Every client stochastic gradient $g_{i}(\boldsymbol{x} \mid B)$ of a model $\boldsymbol{x}$ evaluated on batch $B$ is an unbiased estimator of the local gradient. Instead of using the Bounded Variance assumption, it is better to use the Expected Smoothness assumption:Assumption (Expected smoothness). This means that analysis does not cover arbitrary heterogeneous cases. In https://arxiv.org/pdf/1910.14425.pdf it was shown that this assumption is also limited. The first one is the decomposition theorem. This theorem provides several insights and it is an interesting result, which can lead to further improvement. However, FedAvg can be modeled not only by the local SGD method. There are many other approaches such as Federated Random Reshuffling (https://arxiv.org/abs/2102.06704, https://arxiv.org/pdf/2110.10342.pdf). It might be interesting to formulate similar theorems for other methods. However, there is no epsilon complexity and comparison with other results. Additionally, bounds for strongly convex and general convex cases are also desirable. In this section the comparison between Multinomial and Uniform Distribution. However, it is also interesting to have a comparison with other methods used in Federated Learning. It is not clear why n \in {10, 20, 40, 80}  are used and why only m   n/2 is considered. Moreover, only two distributions are compared, additional comparison with other distributions is needed.<|endoftext|>Finally, experimental results on Shakespeare back the theory and demonstrate the effectiveness of one sampling choice over another based on the loss function setup in a FL framework. Strengths:  The paper is very well written and very easy to understand and follow. The paper studies a framework for studying the impact of client selection strategies for unbiased aggregation at the server. Through the framework, the authors provide theoretical and empirical evidence as to in which scenarios weighted sampling is preferrable over weighted sampling. Weaknesses:  As showcased in Wang, et.al., "Tackling objective inconsistency in federated learning" (FedNova), the convergence rate in FL is affected by the objective inconsistency as well. It is not clear from the decomposition theorem, whether such an inconsistency is factored in. It is not clear from the experiments what sort of data distribution was considered for the clients. It is not clear if the claims made in the paper continue to hold for both iid and non iid distributions. Moreover, the number of clients selected is roughly 50% of all clients present in the experiments. It is not clear if the number of clients are selected at a rate 5 10% clients per round or even lower, which is characterized by zero bias and high variance would have a different observation. With variance being the main deciding factor now, is there still a noticeable difference between sampling strategies? Also, how noticeable is the difference when each client undergoes just one step of SGD or likes of that as opposed to 50 steps of SGD. I would recommend the authors to add more experiments with only 5 10% clients being selected in each round. In such a case, how would one go about sampling clients and ensure unbiasedness of aggregation schemes? This paper proposes a new framework to study the effect of client sampling strategies based on weighting of clients/samples in a federating learning setup. While the framework is new and provides new insights, there are some clarifications required in terms of the theoretical convergence results. The experimental results are also incomplete and not up to the mark. The paper is in a good shape, but without the improvements above it s not a strong paper yet.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The authors consider the training dynamics of neural networks under a modification of the mean field parameterization in the high dimensional (d >  n) setting. This one also appears novel in that there is the connection to kernels which was unexpected. There were a number of points of clarification I m hoping the authors could comment on. The authors claim at top of pg. But it seems to me that this is quite a different scaling than the one there, and also the standard mean field scalings. What happens when the scaling is 1/D vs. 1/sqrt(D)? Regarding the negative result of (65; Wojtowytsch & E  20), please give more details on what precisely the negative result is, and how your experiments relate to it. The paper makes a novel contribution for convergence of NNs in the mean field regime with a clean and simple proof.<|endoftext|>This paper establishes the global convergence analysis, with a linear convergence rate, of gradient flow for the neural networks in the mean field regime which possesses a feature learning aspect. The contribution of this paper is to show that the positivity of the Gram matrix of input (random) features is sufficient for guaranteeing global convergence. There are typos in the definition of pseudo L layer NN in Section 3.2.2:The description of $\phi_j$ seems redundant. [Contributions]Recently, the mean field setting of neural networks has become an important topic in the context of global convergence analysis of neural networks because of the presence of feature learning whereas the kernel (lazy) regime basically describes the local behavior of the dynamics of training. Specifically, the positivity of NTK can be reduced to the positivity of the Gram matrix of input (random) features and $d \geq n$ by decoupling the parameter dependent part from the NTK. Loss landscapes and optimization in over parameterized non linear systems and neural networks.<|endoftext|>In this paper, the authors studied the optimization problem of shallow and deep neural network. One important difference with the existing NTK and mean field literatures is that a different scaling factor was used in this paper. Under certain condition on the Gram matrix, it can be shown that GD converges to 0 training loss. Empirically, the neural network indeed shows the ability of feature learning. There seems no discussion about it. It is understandable that there might be technical challenges when analyzing the case of training all layers, so I would not view this as a major limitation.<|endoftext|>This paper studies the optimization of a pseudo three layer network. The inputs are fixed random feature embeddings. They proved that when the second layer is sufficiently over parameterized (wrt the number of samples), GD flow converges to a zero loss solution exponentially fast. In particular, the authors claim that, unlike the NTK regime, their setting exhibits feature learnings. In contrast, in Theorem 1, they only require m (the width of the output layer which is in LNN scaling) to be larger than $\log(n)$. Therefore, I cannot recommend accepting this paper.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper is very well motivated. It has convinced me that being invariant to exogenous noise is important and that the EX BMDP is useful abstraction for making this problem concrete. The argumentation is clear, and the flow of the paper is quite natural. This is a small point, but it is not initially obvious why inverse dynamics fails on the combination lock problem. A well argued paper that sets up an important problem and introduces a novel algorithm (PPE) to solve it. This is somewhat undercut by PPE only working in a restrictive setting. Some discussion of mutual information approaches should be made.<|endoftext|>The paper proposes an algorithm to find a policy cover with sample complexity that depends only on the size of the endogenous state rather than the observation or exogenous state. However, as far as I can tell, there is no indication of how you perform this planning in PPE in the main text. In my opinion, the contribution of the paper is strong since it analyzes an important problem setting, presents an analysis of representations learned using inverse dynamics, and shows that it works in practice.<|endoftext|>STRENGTHS(S1) The problem of efficiently learning in high dimensional observation spaces is a good one, and the authors provided an excellent discussion of the technical components of this problem in the early parts of the paper. Even looking at Algorithm 1, I find myself with a number of important questions unanswered, chief among them being "what about $\phi^*_e$"? It strikes me as odd that nowhere in this algorithm are the inverse mappings from observation to exogenous/endogenous state represented or used. My understanding from the paper is that one seeks some sort of "covering" of the state space in what sense is it practical to obtain and/or store this covering? I feel this is important enough that it should appear in the main paper.<|endoftext|>This paper proposes a new class of decision problems, the exogenous block MDP (EX BMDP), and an algorithm for acquiring minimal state representations for EX BMDPs. The requirement for near determinism in the endogenous dynamics is unfortunate, particularly given that it applies to the initial state distribution as well (often you want to perform well over a distribution of similar but not identical tasks). The fact that it is possible to obtain a sample complexity bound completely independent of the complexity of $\Xi$ was surprising to me given the relatively complex form of exogenous noise in an EX BMDP, and shows that the EX BMDP class is at least tractable. The takeaway seems to be that PPE drives accuracy to \~100% eventually on a less toy problem.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 6; Experiments show a heavy tail distribution on running time in some cases; that imply abundant cases where the problems are solved quickly –in polynomial time– or take a long time. Policy guided heuristic search with guarantees. 5.The observations about the distribution depend on the distribution of the original problem. I will recommend rejecting the submission as this needs further work. The experiments are in a region of a relatively low number of expansions. Please add a reference. 1.The abstract does not mention that this empirical work is for one domain in particular: Sokoban. Two theorems are stated in the paper but without proofs. ## Questions to be answered in rebuttal. 2.That leaves only two interesting cases in the experiments: PHS and PHS*. *and page 2: *We identify heavy tailed runtime distributions of PSPACE hard planning problems*This holds for Sokoban with these particular heuristics. page 2: *We study the interplay of the policy and value networks in A* based deep RL. The negative empirical result does not imply that the problem is more or less challenging.<|endoftext|>This paper studied A* style best first search with deep networks guided value and policy functions. Theoretical framework was created for the analysis of heavy left  and right  tails problem in running time. This paper also claimed that according to the experiments policy network, random restarts, and uncertainty aware networks are import contributors of solving hard planning problems. This paper created an abstract model which explains the right  and left  heavy tail issues for best first search algorithms. 2.Experiments seem to demonstrate the effectiveness of the proposed combination of policy, valuation, random restart and uncertainty networks with no or small modification to the original proposal in the literature. Please clarify. Also the notation of p in section 4.1 seems to be conflicting with section 4.1 and a few other places. This paper needs significant improvement in the writing clarity, mathematical soundness, and experimental validity to reach publication status.<|endoftext|>* This paper proposes an imbalanced tree model similar to the one shown in the context of SAT/CP and makes some statements on the running time in Thm 1 and 2. * The experiment section compares the coverage of the problem instances and statistics on the search space from randomly selected instances. This paper focuses on the Sokoban domain covering more than 10,000 instances available in the literature and presents some evidence that the running time distribution of A*+DNN may have left/right heavy tails. * intuitive explanations that may explain the results with the imbalanced tree model. How does the observation made in this domain applicable to AI planning in general? 2.In the paper Figure 4. would be the only data that supports the empirical statement, which seems not sufficient. 4.Can you provide the details on the network? * What are the typical problem statistics? * Dropping the admissibility, A* is a more or less greedy search algorithm. This paper shows interesting experiment results. However, I think experiments don t support the claims well. What was the time limit for the search? In this sense, the experiment may cover the cases when the search doesn t expand too many nodes.<|endoftext|>This paper considers A* planning with learned policy and value function approximations. After presenting empirical results for heavy tailed distributions of guided search runtimes, the authors propose an abstract model to explain these distributions. Also, [5], but that one is not published. Empirical results are in Sokoban. I think this is a nice idea and could have general applicability. * The number of random seeds or trials does not seem to be reported. Was only one model trained? On the one hand, there are many more interesting ideas than is typical for a single paper. On the other hand, there are many more experimental details missing than is typical even for rejected papers.
Reject; rating score: 5; rating score: 6; rating score: 6; The paper considers point forecasting of hierarchical time series, i.e.multivariate time series with hierarchical aggregation constraints. Reading the paper raised a lot of (unanswered) questions. While dimension reduction in hierarchical forecasting is an important problem, the paper lacks clarity and justifications for all the design choices. The experimental setup is also questionable (loss function, etc). Why imposing the hierarchical constraints on the embedding is equivalent to constraining the forecasts?<|endoftext|>In this manuscript, the authors proposed a method for hierarchical time series forecasting, consisting of twocomponents, the TVAR model, and the BD model. The paper is well written and easy to read.<|endoftext|>The paper introduces a method for hierarchical time series forecasting. The ‘Problem Statement’ paragraph is a must have in every paper and helps to prepare the reader for the ‘Problem Setting’ section and the suggested model section. It’s fine that the paper doesn’t deal with it, but it would be worth if the paper touched on this point.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 10; Is it possible to do it? Thus the main motivation of the paper is appropriate and might have a significant effect on the literature. This paper extends this matrix language in order to determine the expressive power of GNN which goes beyond the 3 WL. The way of evaluating expressive power by tensor language is not new at all. I see that experimental work such that proposing a new GNN is not the main idea of the authors. Breaking the limits of message passing graph neural networks. But  above 5 points should be addressed. Therefore I would recommend clear acceptance for this work.<|endoftext|>The paper proposes Tensor Language (TL), a language with which popular GNN models can be uniformly studied, so as to yield insights about their expressiveness and separation power. Beyond expressive power, TL is also used to quantify the function approximation power of GNNs, and also shows that GNNs, characterized by a TL fragment, can learn functions with separation power upper bounded by a refinement algorithm corresponding to this fragment. Furthermore, TL establishes a new set of results, thereby addressing some open questions in the field. Hence, the paper seems to be a valuable addition to the literature on GNNs. Therefore, I strongly suggest a more through comparison with related work. This section required multiple reads to be properly understood, and is quite dense. Therefore, a more simplified presentation, supported by examples, would be beneficial. [1] Barcelo et al.The logical expressiveness of graph neural networks. All in all, the paper makes good contributions, but some of its claims, namely the simplicity of TL translation, as well as its novelty, should be better explained.<|endoftext|>It showed that their expressive power is identical to (suitably parameterized) color refinement algorithms or (vertex/graph) WL algorithms. Then, by translating existing GNNs into the tensor languages, this paper provided upper bounds for the expressive power of GNNs, which recovered several existing results. In this sense, the approach of this paper is novel. [2] This approach allows us to analyze expressive power independently of particular GNNs. In addition, it is relatively easy to derive the expressive power of a GNN because it is sufficient to translate the GNN into the tensor language. Therefore, I think this approach is significant. [4] The paper gives positive answers to the unresolved issues raised by existing studies.<|endoftext|>This paper introduces a new approach to study the separation power and approximation properties of graph neural networks (GNN). In Section 4, they show the separation power of TL and relate it to color refinement algorithms like k WL. This is a theoretical paper without any experimental results. Strengths: the paper is very clearly written and makes a clear new connection between programming language and deep learning. The main reason is that the authors consider the discrete topology on the set of graphs so that any function is continuous. I think the authors should address this issue in Section 6.2 by explaining the possible limitation of their approach. Very nice contribution making a new connection between programming language and GNN to study their expressive power.
Reject; rating score: 5; rating score: 5; rating score: 6; This work presents a method to efficiently sample from a pre trained DDPM by solving a dynamic programming problem that can maximize the log likelihood of the data samples given a fixed computational budget. Weaknesses:* The main weakness of this work is that the method appears to overfit the ELBO objective without improving (and potentially reducing) the visual quality of generated samples. See "Other Comments" below. Why does the method not work for arbitrary continuous time steps if the model is trained with discrete time steps? The final conclusion is therefore somewhat unsatisfying because an ideal DDPM schedule would be short and efficient, able to produce high log likelihoods, and able to produce low FID scores compared to other methods.<|endoftext|>This paper presents a dynamic programming algorithm to sample from diffusion models. You write a bit on page 2, but I would make it even more clear, as it is important for reading the rest. Also: what is an ELBO path? Is it not possible to both scale and translate the timescale? How restrictive is this really? Can the authors think of other regularization methods that break the approach?<|endoftext|>This paper demonstrates that (on a pretrained model) the optimal ELBO may be obtained via a dynamic programming algorithm for the location of the steps. Overall the dramatic reduction in steps feels "too good to be true"   a sentiment that is largely borne out by Section 5.1, in which it is demonstrated that improving the ELBO does not necessarily imply improving the FID. With some refinement I could see the techniques this paper proposes being of great utility. As it stands it presents a dramatic speed improvement that may or may not produce compromise the final model. Overall I recommend acceptance. This is not at all the same thing as using the same Brownian motion. The authors have not released code so I cannot see what library they are using themselves, but the above procedure may easily be done using the `BrownianInterval` of the `torchsde` library [1]. I think that a derivation would be a meaningful improvement to the paper.
Reject; rating score: 5; rating score: 5; rating score: 6; [3] A closer look at few shot classification, ICLR 2019. In representation learning, it adapts the stochastic weight averaging (SWA) as a regularizer for learning more generalizable features. This work achieves comparable results to SOTA, however, they are major concerns that need to be addressed. However, there are some major problems in the opening remarks of the paper, the explanation of the algorithm is not clear enough, and the few shot classification performance of the proposed method is not fairly compared with recent methods. What do you exactly mean by this? Considering all these problems, I think the authors need to rewrite the introduction part and make it more reliable, considering the recent studies in few shot learning. Although it is mentioned that the purpose is to find a low rank approximation, it is not clear that how do you finally deploy it in your algorithm. The algorithm is unclear and seems to be lost in the explanation of various concepts! Overall, I found it very hard to follow the methodology as in some cases, the main flow of the algorithm is lost due to overexplanation of other concepts.<|endoftext|>This paper presents a few shot learning method based on the reprehensive pre train learning using stochastic weight averaging (SWA). And as claimed by the author, this is the first few shot learning work can works for classification and regression. As a result, it is not very supervising that SWA achieves better results in transfer learning for few learning learning. It will be more interesting to provide more analyse why SWA lead to the better performance compared with other solutions. Moreover, the flat loss surface contribute to the adaption is well discussed in some existing incremental few shot learning research. The result shows that the performance is improved. However, the theoretical link is still not clear. Is the low rank representation criterion general for all reprehensive learning to have better result? 5), Similarly, for other transfer learning based few shot learning method such as Baseline++, it is straightforward and fully compatible to employ the SWA in its pertrain, the comparison should be analysed if MFRL better than (Baseline++ with SWA) to demonstrate the effectiveness of the MFRL.<|endoftext|>However, there is a disconnect between the simplicity of the idea and how it s presented by the submission, and overall there aren t a whole lot of new insights to be gained beyond the observation that SWA works well for few shot learning. **POST REBUTTAL**: The response addresses most of my concerns. Then, stochastic weight averaging (SWA) is applied to the model by continuing training for a certain number of epochs and averaging the parameters obtained across those additional epochs. The writing is easy to follow. The conjecture that it fosters lower rank representations is supported by empirical observations. I feel that the presentation would have been better and more straightforwardly characterized as an application of SWA to the few shot learning setting, and I d be interested in hearing the authors  opinion on this. In the classification setting, MFRL only really stands out when SWA is applied. Are there other aspects to MFRL that positively impact performance? Being such a small implementation difference, it also reinforces the idea that MFRL reduces to SWA, and naming it is not necessary. Presenting results on model calibration is good, but the paper feels lacking in details.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper presents a novel adaptive second order method:  OASIS , for large scale optimization problems (convex and nonconvex). The search direction is obtained by preconditioning the gradient information with a matrix obtained by approximating the Hessian diagonal matrix (via Hutchinson s method with a momentum term). The learning rate is updated adaptively by approximating the Lipschitz smoothness parameter. For instance the theorems 4.6, 4.9 seem to be worst case bounds. Moreover, the bound in theorem 4.9 depends on $\hat{D}_k$ which is a random variable. Finally, extensive empirical results are provided which show that the proposed method achieves comparable, and sometimes better results than other existing methods. Pros  The paper has the following strengths. The paper is written in a very clean manner, and is easy to follow. So the novelty aspect is a bit limited in that respect. 1) There is currently not much discussion on the interpretation of the bounds appearing in the convergence analysis. The empirical evaluation is quite extensive and satisfactory in my view. 2) It would have been helpful to have provided some kind of proof sketch of the theoretical results, or atleast an overview of the key steps which I believe might be common to more than one theorem. For instance, how do these results compare with those for existing second order methods (e.g., AdaHessian)? Because in the AdaHessian paper, they also consider a spatial averaging step for better estimation of the Hessian diagonal. 4)  As mentioned on pg 4 in the discussion on literature for adaptive LR, the present paper draws upon ideas from the literature on first order methods for adaptive LR. Is there any conceptual reason behind this?<|endoftext|>It provides deterministic as well as stochastic versions, which either fully compute the gradient or sample it. The algorithm estimates the diagonal elements of the Hessian via Hessian vector products. The algorithm makes use of this information for finding a better search direction and the step length, eliminating the need for a line search. It provides convergence guarantees and a number of experiments on classical ML problems as well as on deep nets. Empirical evidence is given that the algorithm outperforms comparable approaches like AdaHessian, etc. Weakness:  The empirical evidence/experiments are rather limited. (Yes, it is stated in the paper that comparison to only diagonal preconditioners is made but in general, there are many more methods to solve this case, e.g., quasi Newton methods or trust region Newton CG methods which are also used for computing the optimum in the provided code.These methods make use of the same information as the presented method and hence, a comparison to these methods would also be useful for a better global picture.) Stochastic case: Again, only a very limited number of experiments is provided here. Having not to tune the learning rate is an enormous plus here and it would be nice to verify the algorithm s robustness on a number of different problems/nets. Especially that one does not need to tune a learning rate can be very beneficial. I did not fully read the convergence proofs though they seem sound. According to theory and experiments, one should always use this algorithm. It would be nice to justify this claim by a more comprehensive study, e.g., more problems, datasets, and other algorithms in the deterministic case and more nets and data sets in the stochastic setting. Only then one can tell if it is superior to state of the art approaches. If such experiments were provided in the paper I would have given a higher score.<|endoftext|>This work proposes OASIS, a second order method which approximates the diagonal of Hessian matrix and uses the information to rescale the gradient vector. The main difference between OASIS and the existing method AdaHessian (Yao et al., 2020) is on the ways they approximate the diagonal of Hessian, that AdaHessian uses a formula similar to Adam and OASIS uses an exponential average. The authors established the convergence guarantees of OASIS under various settings including convex, strongly convex and nonconvex cases, using various learning rate schedulers such as the adaptive learning rate, fixed learning rate and line search. The topic on how to effectively leverage the Hessian vector oracle in large scale machine learning tasks is definitely important and interesting. For the main ideas, the authors show in Figure 1 that OASIS approximates the diagonal of Hessian much more accurate than the Hessian momentum in AdaHessian, which is the main point made in the paper (I have the feeling that the Hessian momentum is not solely for approximation?like the first order momentum vector may not be an accurate approximation for the gradient vector, but is effective for acceleration). It seems that the adaptive learning rate can also be incorporated into AdaHessian by choosing a different weighted norm. However, I was hoping for more insightful discussion on these results, such as how the theorems would suggest a better parameter choice. Currently, they are only convergence guarantees, which could be far from the practical performance. The theorems in Section 4.1 generalize the results in (Mishchenko & Malitsky, 2020) in the deterministic setting while there seems to be no theoretical advantage of such generalization (BTW, is there any bound on the scale of $Q_k$ in Theorem 4.6?It seems that it can be of the order $O(k)$ which kills the convergence). Typo in the citation "Adaptive gradient descent without descent. (2018).Lectures on convex optimization (Vol.137).Berlin, Germany: Springer International Publishing". I appreciate the authors  efforts on the comprehensive analysis and empirical evaluations of the proposed OASIS. The paper is also very well written. However, both the theoretical and practical results seem incremental to me. Moreover, OASIS still requires parameter tuning in some of the experiments, and thus is not "fully adaptive".
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; I feel that adding an algorithm of the proposed method and signposting it to each section would go a long way towards clarifying the method. The main claimed contributions of the paper are that it formulates HPO as an MDP, and that solving it using MPC with some lookahead is better than greedy selection. The fact that non myopic selection strategies perform better is not novel   this is effectively an active search problem, and it has been shown that non myopic strategies perform better greedy selection (e.g.see Garnett et al (2012)). This is what existing surrogate functions do as well. As the authors mention, Jomaa et al.(2019) also uses the same formulation, with the one difference being they use an LSTM to model it whereas the author of this paper follow the deep sets formulation. 3.I am not very clear on experimental details. What is a run and a trial here? 4.The authors compare the results of lookahead at 3 vs 5 steps and it seems like there is no clear winner. I would like to see the results for 1 step lookahead, i.e.greedy selection, and if the lookahead actually helps here at all. 5.I find some of the assertions in the paper to be quite baffling   I am not sure if this is just due to poor wording or some fundamental misconception about MBRL. `HPO … can be seen as a special use case of model based reinforcement learning developed under the guise of some idiosyncratic terms.` What is the basis of this statement? Bayesian optimal active search and surveying.<|endoftext|>The paper presents a novel transfer learning approach to hyperparameter optimization (HPO) by formulating the problem in a model based reinforcement learning (MbRL) framework. Strengths: + The paper proposes a novel search strategy based on model predictive control for HPO by utilizing the existing meta data. There is not enough details on the approach and experiments to reproduce the results. Comments:  One of the main concerns about the paper is the MDP formulation and characterizing the approach as planning in a MbRL framework, when there is no structured way to learn a policy and the setup does not conceptually match an RL problem. The planning strategy selects k samples without exploiting the orders between different HPs, evaluates their immediate improvement (using the trained transition function), and picks the best performing one, which is more of a search scheme rather than planning ahead, and resembles Monte Carlo tree search or a contextual bandit approach. The literature on HPO is not properly covered, there are several works, particularly in the bandits domain, that are not discussed: *Li et al., A novel bandit based approach to hyperparameter optimization, 2018  *Falkner et al.BOHB: Robust and efficient hyperparameter optimization at scale, 2018  *Tavakol et al, HyperUCB: Hyperparameter Optimization using Contextual Bandits, 2019  The authors describe the HPO as a sequential decision making problem, while based on my knowledge, this is not very common, and they also do not provide a supporting evidence or citation for this claim. The clarity of the paper needs improvement as the ideas are not well organized and several parts mentioned in the paper, e.g., algorithm 1 and some details about the data and experiments could be better explained. Do you compute the true validation loss for the current HP and replace it in the next state or you keep using the predictions only? The paper could benefit from a proofread.<|endoftext|>This paper proposes a method for hyperparameter optimization (HO) drawing inspiration from model based RL (MBRL). They recast the HO problem in a markov decision process (MDP) formulation. ## Strengths  Clearly described connection between HO and MDP, which was interesting (though perhaps not entirely original, given Jomaa et al, 2019). Good experimental results in comparison to baselines, and a thorough set of baselines appear to be considered. ## Weaknesses  Related/prior work could be discussed a bit more clearly in the context of this paper. Would help clarity significantly to have the Algorithm description (or at least a summary) in the main text, rather than appendix. I understand the motivation for the use of ensembles, but it would help a lot to have an ablation over this to understand its importance empirically also. Minor: Tables 1 and 2 could be made clearer by highlighting that 15,33,50 refer to num. of trialsOverall this paper is interesting and has I think some original ideas. If the authors are able to do this, I would consider raising my score.<|endoftext|>The authors establish the equivalency of hyperparameter optimization (HPO) and model based reinforcement learning (MRL). On one hand, hyperparameter optimization is seen as an optimization of a sequence of actions (hyperparameter candidates) that improves a reward function. On the other hand, planning replaces the acquisition function of HPO. The sequence is constructed by a transition function driving the optimization towards its extremum. Within this frame, they explore the effect of planning on the performance of this approach on the task of hyperparameter optimization. In the related work section, they miss reporting other related work coming from RL, which is in turn briefly discussed in section 4. This is not very concerning, since the authors later compare to this body of work in the experiments section. However, they don t evaluate their efficiency or stability (in the context of non transfer scenarios). In the transfer learning setting, the use of this type of surrogate is clear. At some point, this brings into question the utility of a surrogate at all, since it would make sense to actually train and evaluate complete models. Shedding a light in this direction and possible limitations would be beneficial. The paper is well written, the work is presented in depth and clearly. I believe that the presented work is solid and will contribute as a point for constructive discussion between the HPO and RL communities.
Reject; rating score: 5; rating score: 6; rating score: 6; This paper proposes a novel strategy for performing exploration based on distributional reinforcement learning. They highlight the fact that existing approaches cannot distinguish between epistemic and aleatoric uncertainty. Finally, they validate their approach in their experiments. As far as I can tell, their main strategy is to use sampling from a distributional perturbation rather than optimism. I would have expected the theory to support the benefits of using their exploration strategy instead of an alternative.<|endoftext|>However, the paper lacks in quality of writing and empirical analysis. The paper motivates the need for risk neutrality from the family of works applying OFU (optimism in the face of uncertainty) to distributional RL   it states that prior works in this space induce a one sided risk tendency (risk seeking or risk averse) which is undesirable as it leads to “biased exploration”. Grammatical errors are prevalent throughout the paper and the related works, motivation and presentation of the proposed method all seem to be lacking in clarity.<|endoftext|>This paper considers distributional reinforcement learning with the goal of seeking risk neutral optimal policy. I think that the overall idea is intuitive and makes sense to me. Since committing to specific criterion induces a one sided tendency on risk for the agent, it seems natural to instead randomize the risk criterion so as to reduce the bias and gradually shrink the ambiguity set to obtain converged results. The writing can be improved. While there is room to improve, the theoretical and empirical contributions seem to be relevant and of interest.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper proposes a novel method to tackle the more practical problem of semi supervised learning with imbalanced data.<|endoftext|>Attacking semi supervised learning with the MNAR setting is an interesting and important topic for practical learning tasks.<|endoftext|>This paper formulates the problem of semi supervised learning where the likelihood of an example being labeled is dependent on the class of the example. Given the paper proposes the MNAR setting, it would help strengthen the justification/interest to point to a real dataset/setting that matches this assumption as the experimental results all are based on simulated class dependent labeling. I am in favor of accepting this paper as the problem and solution are well written and the experimental results seem reasonable.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper aims to train a GHN model that can predict weights of each layer. The architecture is represented in adjacent matrix format, where a node represents one layer and an edge indicates whether two nodes are connected in given architecture. Each type of layer is represented with a one hot vector as their node attribute. 2..The trained GHN model can be generalized to unseen architectures. 3.Extensive evaluation on three real datasets demonstrate the superior performance of the proposed approach for tackling the heterogeneity issue in federated learning. When the datasets on local devices are highly non iid, the performance of this method might be significantly dropped. 2.The dimension of the final output is fixed, even though it can be solved by concatenating or slicing the final output. Copying layers and concatenating them together to form a bigger layer is too brutal and may not be a good choice. In practice, the parameter of one layer is not mirrored. 3.In the experiments, what does  without graph  mean? 4.The authors fail to provide the convergence analysis for federated learning, especially the convergence of GHN under FL settings. 5.The paper is more like a straightforward application of the Graph Hyper Network from the centralized training to the federate learning.<|endoftext|>The authors study methods for federated learning with clients using different neural network architectures. Graph hyper networks are used to predict useful weights of client specific neural network architectures. The proposed method addresses a key challenge in federated learned which is data heterogeneity. The approach is explained in good detail. The main weaknesses in my opinion:   The novelty seems limited. I do not understand this argument   "...from some predefined family of models..." what exactly is a family of models? The authors use a direct application of existing methods (graph hyper networks). "... see supplementary for more ... "   " ... and trained in a standard FL." " ... when local data is ..."   "...when local data percentage is at ..."   "...our GHN can immediately populate it ..."   " ... we have proposed a new setup ..." pls refer to the relevant equation, algorithm or figure First of all, the underlying modeling assumptions on the datasets are not clear. * It is nice that the authors write down two research questions at the end of the third paragraph Section 1. However, it is not very clear how and where these questions are answered later on. How can we tune the input parameter of Algorithm 1 ? Should the client architectures \mathcal{A}_{c} be listed as input to Algorithm 1 ? "all clients need to share the same network architecture" pls provide more justification for this claim.<|endoftext|>This paper proposes a method to tackle the problem of architecture agnostic federated learning. **Heterogeneous data**. As shown in Appendix G, the approach to use GHN may lead to worse performance (compared to standard FL) on a balanced test set. The main insight of this method is that neural architectures can be represented as graphs and their weights can be outputs of a larger GNN. The authors may want to justify the results. Specifically, in practice, we cannot guarantee that the test data distribution at each client is the same as its training data. The paper is clearly organized and written. I can follow it without much effort. The paper has its merits. The writing is clear and easy to follow. I am willing to raise my recommendation of this paper upon further clarifications. Then, does the adaptation result depend on the data upon which to perform fine tuning? The result does not seem very supportive of the authors  claim of generalization. Following the previous drawback, I am considering how the proposed FLHA GHN can deal with very deep networks, and here is why. **Number of clients**. I am curious about how big the graph hypernetwork used in the experiments is.<|endoftext|>The authors addressed this problem by sharing a graph hyper network that can populate model parameters based on different model architectures. The authors adopt a graph neural network with a MLP as a hyper network to generate model parameters  for each client. 2.The proposed framework is generalizable to unseen architecture. They are all variants of ResNet with close size. 2.Experiment results on CIFAR 10/100 do not show evident advantage of the proposed method over baseline method. Previous works on federated learning either focus on the mechanism of parameter aggregation or the aspect of privacy. This paper opens a new direction in FL where clients may not be willing to share their unique model designs. From this perspective, I think this paper has promising impact on the research field of FL.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Standard Bayesian type decision making models have that given some signal X, our decision makers choose an action A to maximize utility given the signal X. Rational inattention models say that the there is limted processing capacity in the agent and model this by requiring that A | X has a capacity constraint (i.e.which under some extra assumptions essentially becomes maximum amount of mutual information). Pros:Injecting more “psychologically plausible” models of human behavior into RL is very important for many applicationsAdding the information channel is a simple way to add rational inattention into RLCons:It is unclear to me in which cases rational inattention is the right framework to use. However, the simple model presented in experiment 1 is absolutely tractable. The second experiment may not be, but it is too simple to be a “real world example” and though it is more complex than the first experiment, it’s not clear what new insights we are supposed to gain. Overall, I think this paper needs more convincing results that tell us something general either about PA+RI problems or just RI itself.<|endoftext|>The paper proposes a RIRL, which incoporate rational inattention suboptimality into the optimal RL model. The authors simulate RIRL in a single agent and a multi agent setting. The paper is clearly written. I find the model interesting and the authors discuss some emergent behavior from the model. At the same time, as ICLR is still a more ML venue, the lack of more empirical results can be a big minus. The experiments are all simulations based. The paper can benefit from some empirical results, though this might not be reasonable to ask given the short rebuttal period.<|endoftext|>The proposed RIRL framework is evaluated in two specific Principal Agent problems. Strength:  Using MARL to model human bounded rationality is interesting. Moreover, extensive experimental results are presented to show that the RIRL framework can reproduce rich information about bounded rational behaviors. The paper is generally well written and easy to follow. For the experiments, while I appreciate that the paper presents a quite extensive analysis on the principal agent problem, I need to say that I can t evaluate the significance/novelty of the results as I am not familiar with the literature on the principal agent problem. (2019).Modelling bounded rationality in multi agent interactions by generalized recursive reasoning.<|endoftext|>The paper proposes Rational Inattention Reinforcement Learning (RIRL), a multi agent reinforcement learning framework that incorporates the behavioral model of rational inattention. They empirically evaluate RIRL in two principal agent problems, building on classical models from the economics literature. The idea of bringing rational inattention into multi agent reinforcement learning setups is very conceptually interesting. Due to the conceptual novelty of incorporating rational inattention into multi agent RL, I recommend weak acceptance. I would encourage the authors to provide a more thorough discussion of these comparisons in a future version of the paper. Thus, I keep my assessment the same.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposed to estimate the (log) density ratio, $\log [p/q]$ by introducing an intermediate density $m$, and rewrite it as $\log [p/q]   \log [p/m]   \log [q/m]$. The paper requires a lot of work to improve. Theoretical guarantee is not from empirical analysis.<|endoftext|>This paper addresses one of the essential tasks in machine learning, the problem of density ratio estimation (DRE). This paper attempts to contribute to solving an important problem in the context of DRE. While the attempt is novel and very interesting, there is insufficient comparison with existing methods and justification of the proposed method. The proposed method seems to be an extension of Rhodes et al.(2020), but has a novelty.<|endoftext|>The authors propose a novel method for density ratio estimation (DRE). However, parts of the paper seem inaccurate or not very rigorous. This is quite confusing, since C seems to be just the number of classes (and not a density). Can you make the argument somewhat more formal? The paper is not theoretically strong, but offers a useful and practical approach which can be impactful.<|endoftext|>This work estimates the ratio of two densities using intermediate densities that have sufficient overlap with the target densities. + The paper is written clearly. Both Lemma 1 and the idea of telescoping are not new. Assume that two densities have disjoint support, then wouldn’t the density ratio estimation be a trivial problem as the ratio is always 0 (or infinity) ? Most experiments are on normal/Gaussian distributions.
Reject; rating score: 5; rating score: 8; rating score: 8; This paper theoretically analyzes a student teacher setting for which epoch wise double descent occurs. The inputs are d dimensional and there are n training samples. Exact analytic expressions for the test error can be derived by utilizing the replica method in the limit of n, d  > \infty. The case analyzed in most depth is a setting in which F has two scales for its singular values (with high degeneracy)   c.f. The intuition behind this is that when the scales are well separated and learnable (e.g.regularization not too strong), overfitting of the faster features occurs (leading to a rise in the test error) before learning of the slower features (which subsequently decreases the test error). Strengths: The analysis of a simple model that exhibits epoch wise double descent is illuminating and worthwhile, so the topic is of general interest. Weaknesses: Numerous typos in the main text and appendix that need to be addressed & divert the flow of the calculations for the reader. See more on this below. This also made it a bit challenging to check the calculations in full. I am not certain as to whether there are notable technical advancements in the paper. I would like to better understand some of the prior results this work relies on for the analytic calculation; a self contained discussion of the exact results drawn from older literature in the appendix would be helpful. For instance, regarding Eq.(17) and the probability distribution induced by SGD. What justifies then the time dependent distribution in Eq.18, since there isn t a notion of equilibrium in the finite time case? Some of the writing and terminology seem too imprecise to be useful. The authors also write about the "interaction" of different feature learning speeds, which makes me think of a precise notion of interaction in the sense of physics, although I believe the authors simply mean the "presence" of different scales. It is common to average over the draw of finite size training dataset. (It seems these are the teacher weights, W  > W*?) Eq.(28): Z^n should be labeled Z^r. As with earlier equations, I assume this equation refers to the teacher network but don t see input z referenced. My slightly lower score is based on the following factors: Since the focus of this paper is not empirical (e.g.the observation in realistic networks has appeared before), the contributions come primarily from the theoretical side. In this respect, I am not sure if similar insights in linear models & control of multiple scales for epoch wise double descent have appeared in earlier works and would appreciate if the authors could comment on this in detail.<|endoftext|>Recent (and old) theoretical work has demonstrated that even simple models like linear regression exhibit the same non monotonic behavior as a function of model and dataset size. The authors of this work build on this literature by demonstrating that linear teacher student models trained with gradient descent can exhibit double descent as  a function of training time. Using replica theory, they derive a closed form expression for the generalization error of this model as a function of training time. This insightful work demonstrates that another apparently exotic behavior of deep neural networks, non monotonic generalization error over the course of training, is already present in simple, analytically tractable linear models. The paper is clearly written, the theory is well motivated, and the results are neatly presented. I only wish that the authors dove more deeply into interpreting the behavior of their analytical theory: (1) what happens when features of more than two scales are present? is there triple descent in the generalization error? (2) how does epoch wise double descent differ from model size double descent? one difference seems to me that linear teacher student models trained on isotropic data exhibit model size double descent but apparently not epoch wise double descent, which requires anisotropic data. What explains the difference? It would be interesting to discuss this further. (3) How do the results change as a function of the the overlap between the teacher and the anisotropy in the data? Note: I believe the color code in Fig.2 is incorrect, and is misleading. I think the colors of the large and intermediate regularization strength curves should be flipped. I think this work is a valuable contribution which captures an interesting feature of the training dynamics of generalization error in deep neural networks in a simple, analytically tractable model.<|endoftext|>This work studied epoch wise double descent using linear model as a proxy. Basically, authors proposed a linear teacher student models to established analysis and used random matrix theory (rmt) to interpret the learning dynamics. Some simulation on the well posed (n>p) linear regression problems backups the theory. The implicit regularization effects of SGD/GD on least square linear regression may perform as a Ridge regularization. (Ali and Ray Tibshrani s work at ICML 2020), where the number of learning epochs/steps are connected to the inverse of lambda (strength of Ridge effects). It is not superise that when you tune the lambda of Ridge, certain double descent would appear in testing accuracy. In such setting, the analytical results of SGD convergence might be different (inclusion of pseudo inverse). In that sense, the result would lead to Ridgeless regression. Shall authors discuss the connections between this work and those works? It is a solid work with theoretical analysis and empirical evaluation. Though it may connect to many pioneering works in the field, authors should discuss these connections.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper presents PCT, a tree structure that integrates several packing heuristics for addressing the online 3D BPP. It is also well illustrated and easy to follow. There exist a couple of heuristics and some of them were summarised in:      A generalized reinforcement learning algorithm for online 3d bin packing. 2.The authors claim that the proposed method is efficient. Post rebuttal   Most of my concerns have been well addressed during the rebuttal period of time.<|endoftext|>This paper proposes a new method for online 3D bin packing problem (3D BPP), with the newly designed packing configuration tree (PCT) for representing the state and graph attention networks (GAT) for learning the representations.<|endoftext|>This paper addresses the problem of online 3D bin packing where the order of objects is out of the model s control and it must make placement decisions one object at a time.<|endoftext|>This work proposes a tree based learning method for online 3D packing problem. Clearly, it is different from the continuous action space in reinforcement learning. It is important to handle Large scale packing problem with continuous action space.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper gives an exact sampling algorithm to sample from DPPs based on non symmetrical, low rank, kernel matrices of the form $L VV^\top + B(D D^\top)B^\top$ where $V$ and $B$ are of size $M$ by $K$ and $D$ is square, non symmetric and of size $K$ by $K$. The marginal kernel of such a DPP may be written in the form $K ZWZ^\top$ with $Z$ a $M$ by $2K$ matrix and $W$ a square matrix of size $2K$ that is not symmetric. The empirical results tend to validate this intuitive argument. In passing, I find the proof hard to follow and suggest to the authors to do their best to find ways of simplifying it as much as possible for the camera ready version.<|endoftext|>Furthermore, this paper develops a scalable sub linear time rejection sampling algorithm for a subclass of low rank NDPPs that are called ONDPPs. Through experiments, it is shown that the predictive performance of ONDPPs is not degraded compared to NDPPs. This paper proposes two scalable sampling algorithms for NDPPs, one for low rank kernel, and the other for low rank orthogonal kernel. However, this might not be a big issue because it is shown in the experiments that its predictive performance is not degraded.<|endoftext|>Their work can lead to more applications of NDPPs in practical settings. This is the first paper to give efficient algorithms for exact sampling from NDPPs (which can be used in a variety of real world applications like recommender systems etc) and thus this paper can have a good impact.<|endoftext|>This paper studies the problem of sampling from non symmetrical Determinantal Point Processes (NDPPs); in particular, this paper focuses on exact sampling for low rank NDPPs. The main contributions of this paper are:1. This paper is overall well written, easy to understand and both the theoretical and empirical results presented in this paper are very exciting.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper is about feature selection in a weakly supervised setting using a background and target dataset and the goal is to learn features that are specific to the target dataset. There is a background latent vector z and a salient vector s. For the background model, the salient vector s is fixed where it has variations specific to the target. The general idea of this paper is to learn an encoding function that takes the input data and generates a low dimensional embedding g_z to captures the variation in z and not the variations in s. We define a loss function that takes this z along with the features to reconstruct the original target points. The paper presents different methods to learn these functions, and the validation is done on a semi synthetic dataset and a biomedical dataset. 2) The proposed algorithm does not use too much supervision and only relies on the knowledge of whether a data point belongs to background or target datasets. Cons:1) One of the main weaknesses of the proposed method is the choice of k. It is not clear as to how one would choose the number of salient features. 2) I have doubts about the basic problem formulation. Stronger baselines would make the paper strong. The poor reconstruction could be due to many other reasons in addition to capturing salient variables.<|endoftext|>A new formulation of selecting the top k informative features, when two sets of samples are available: one containing background patterns and patterns of "potential" interest, and the other having background patterns only. Proposed objective is quadratic error minimization between the original example and vector obtained using reconstruction function applied on background embedding vector concatenated with k selected original features. Three different ways of learning the reconstruction and background functions was proposed and evaluated on several datasets (synthetic and real world ones) against unsupervised baselines. Both the formulation and solutions proposed appears novel. Proposed approach (in two out of three flavors) showed superior performance in terms of accuracy. It would have been interesting (and highly relevant) to see how would the method compare to the supervised feature selection approaches (e.g using target and background sets as a binary classification problem). Even though I think the approach can be valuable, my main concern is the motivation to use it when background and target sets are already known. How does its utility compares to supervised feature selection methods?<|endoftext|>Summary.The paper considers the problem of feature selection in the contrastive analysis setting. In particularly, the authors are interested to discover features that reflect salient variations enriched in some target dataset compared to a background dataset. A new method, called Contrastive Feature Selection, is proposed. The reported accuracy of the experimental results is extremely good. In general, there are a number of unclear points in the paper. Do you mean that $g_z$ is not useful? Not clear sentence: "A diverse array of prior work exists on unsupervised feature selection, with one major differentiating factor between methods being how to define the “usefulness” of a feature." The feature selection is performed using the existing stochastic gating layers, so, the feature selection novelty is absent. The final framework is quite complex, is not extremely clearly described. Am I right that this part is not drafted on Figure 3? The paper presents a combination of an existing feature selection method (called stochastic gating layers) and a classifier (to test the models performance). There is a lack of novelty in the paper.<|endoftext|>The paper presents a weakly supervised feature selection algorithm which selects features which have different amounts of signal compared across two datasets (a background or control dataset, and the dataset of interest). It uses a concrete selection layer to find relevant features with three different approaches for separating them from the background features. Comments:  The evaluation only uses extremely randomized trees as the classifier, which is not ideal for a feature selection evaluation as the randomness means that it s hard to know if the whole feature space was useful. Or even an MLP given the feature selection system is in pytorch. Could the authors expand on the difference between the pretrained and gated systems with reference to the experimental results? It s not clear in what circumstances one technique should be preferred over the other, neither experimentally nor theoretically. Presumably the pretrained method took longer as there were two separate training runs, but there are no runtimes reported either. Overall the presentation of the contrastive analysis problem is well motivated, the dataset choice and descriptions are helpful and the paper is well written. Are the grass images sampled with replacement or without replacement, and if without replacement is it the same sample across both background and target datasets? Are any grass images repeated? The experimental study has weaknesses, but still justifies the proposed technique.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The proposed contrastive loss, call ReCo, is applied to semi supervised semantic segmentation task with two cases: full labels and partial labels. Thus, I improve my rating score from  6: marginally above the acceptance threshold  to  8: accept, good paper . The performance was validated by re implementing all existing methods in the same backbone architectures and training strategies for a fair comparison.<|endoftext|>The paper extends semantic segmentation models with a contrastive loss on "representations" at different locations in the image. The effect of the "active sampling" method is particularly interesting. These provide some evidence that the representations make sense, independent of the effect on end to end accuracy. iii) Strong accuracy results.<|endoftext|>The proposed method showed state of the art level performances in various settings and various datasets. 3.The experimental results and analysis presented by the authors align well with the authors  claims. Although this paper lacks novelty, I think that it will provide a good future direction for many researchers in this field, if it is verified that the proposed method generally works in more settings. The strong baseline [ref2] is missing.<|endoftext|>The method can achieve high quality semantic segmentation results by using only a few labeled samples (e.g 5 examples of each semantic class). Strengths:* The proposed method can achieve high quality semantic segmentation results with only 5 examples of each semantic class. * The figures in the paper look nice. [1] is missing in the relate work. It boosts the scope for selecting the positive & negative pairs.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors show its effectiveness in supervised learning tasks and establish a SOTA like result over Procgen generalization, and also provide several empirical ablations and visualizations over the method to understand its inner workings. Pros:* As the authors have mentioned, this is a very simple regularization method which achieves good empirical results. From the point of view of judging it alongside other image based regularization techniques (e.g.cutout, data augmentation) and even RL specific upgrades, this method seems particularly effective. This makes me somewhat suspicious that one hypothesis is maybe CLOP just affecting the policy s action outputs, leading to random actions during training and thus an exploratory behavior/"data augmentation" that s leading to the higher test performance. It is not obvious to me if CLOP would work here. Previously, I was concerned that the CLOP method might be a bit too "hacky" (for lack of a better word), but I think the paper s simplicity and strong empirical results may also lead to how we redefine RL generalization. Perhaps Procgen tests too much on "visual generalization" and thus this paper might be providing some evidence for new benchmarks.<|endoftext|>This submission proposes a simple new regularization technique for convolutional neural networks: "channel consistent local permutations" (CLOP): at a deep layer in a CNN, it randomly swaps some pixels with their neighbors in the forward pass during training. I do appreciate that that is not the aim of the authors, but since the proposed method itself doesn t have much to do with RL, and the impact of the paper would be broader if it added more thorough supervised learning experiments too, to establish the proposed approach as the state of the art for CNN regularization in general vision tasks. In a latent feature space many layers deep into the CNN, we don t know what is represented. How does the layer to which CLOP is applied affect performance? Besides the prescription of applying it to the last convolutional layer, some empirical results addressing this would be useful. I am in favor of accepting this paper, but await the author responses and other reviewers  comments to form a stronger opinion.<|endoftext|>In this work, a novel regularization method for deep neural networks is introduced. By locally swapping dimensions of intermediate feature maps, the authors report generalization improvements in supervised learning and on several reinforcement learning benchmarks. STRENGTHSI want to start by acknowledging that the authors provided a version of the code used in their experiments which was very easy to navigate through and understand. The main strength of the regularization method introduced in this work is in its simplicity. Another important contribution of this work is in the improved generalization performance obtained in a supervised learning as well as a RL setup. However, concerns regarding scalability and robustness of the proposed method should be addressed by the authors. The main contributions of this work are the introduction of a new regularization method (CLOP) for image based RL and supervised learning tasks as well as a detailed empirical analysis and evaluation on several standard benchmarks. I agree with the claim that the type of spatial regularization that CLOP tries to inject works at higher layers of neural networks. While this is not a fundamental weakness, I think the paper would benefit by the inclusion of this experiment. This should be mentioned in the paper, possibly in a footnote.<|endoftext|>Empirical evaluations show that CLOP can improve the generalization of convolutional networks on both supervised and reinforcement learning settings. The presentation of the method and the intuition behind it is clear and easy to follow. It would be good to get clarification from the authors on this. Weak experimental results    Compared to previous baselines, the proposed method is only better on 7 out of 16 Procgen games. The authors mentioned that "Compared to these methods, the CLOP layer offers a direct and easy way to augment the RL agent’s ability to generalize to unseen environments, with the benefit of being entirely complementary with each of them." Because of that, I believe the paper deserves a spot in the conference. UPDATE:Thanks the authors for their response. Even though I still think that the improvements of CLOP over previous baselines are not that large, the insights provided by additional experiments and empirical analyses can be quite valuable to the community.<|endoftext|>This paper introduces an augmentation technique: channel consistent local permutations (CLOP), to help address observational overfitting issue in RL. The proposed method is evaluated on Procgen benchmark and outperforms other methods. **Weaknesses:*** The paper aims at generalization in RL. For example, "the important information is often spatially scarce and very localized" also applies to image classification. * It would be better to use the same experiment protocol as in the Procgen paper (full distribution of levels for testing instead of 1000, for hard difficulty). In addition, it would be better to include comparison with other methods that also use hard difficulty such as mixreg. * To make the results more convincing, it would be better to conduct ablation experiments (Figure 5 and Figure 7) on all environments. Overall, I think the proposed augmentation technique is simple and effective when compared to original PPO and other data augmentation techniques, but it does not significantly outperforms state of the art IDAAC. I have some concerns on its connections to RL and some experiment details.
Reject; rating score: 5; rating score: 5; rating score: 8; This paper studies how adaptive methods help performance in GANs. nSGDA is evaluated on several datasets and the results demonstrate that nSGDA is more stable than SGDA. Although the paper finds that the adaptive magnitude of Adam helps the performance in GANs, the proposed nSGDA is still inferior to Adam. The study does not analyze why Adam is superior to nSGDA. 3.For the optimizer, the current experiments are not comprehensive enough. I think the proposed optimizer should be evaluated over different network architectures, different resolutions, and different GANs. Although the topic and findings of this paper are interesting, I think the contributions of the current version are limited, especially for the performance of nSGDA and the experiments.<|endoftext|>In the second experiment it is shown that an adaptive gradient magnitude helps to train a good model compared to an adaptive gradient direction. The normalized optimizers including Adam outperformed SGDA. [3] based on the results of [2] have shown that the results are also valid in the GAN context where the slower agent is the generator and the faster agent the discriminator. As the learning speeds of the agents depend not only on the learning rates, e.g.the complexity of the architecture is important as well, one has to be careful to select the correct settings. Stochastic approximation with two time scales. The importance of balanced updates of the generator and discriminator in GAN training is already well known. Using normalized gradient update steps for convergence is plausible, however not enough as shown in [2,3,4,5].<|endoftext|>By doing so, authors found the Adam produces higher quality solutions relative to SGDA in GANs mainly due to its adaptive magnitude and not to its adaptive direction. This paper presents a great study that dissects the success of Adam in GAN training, which covers insightful observations and comprehensive analysis both empirically and theoretically. The findings of this paper, such as "it is the adaptive magnitude of Adam that matters" and "the learning objective of GAN converges does not imply the generator synthesizes high quality samples" will definitely encourage more exploration along this direction, helping us better understand the optimization process of similar non convex non concave min max problems. Related work should be included in a separate section. The findings are insightful with comprehensive empirical and theoretical analysis.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The authors leverage meta learning to make the shared supernet weights adaptive to randomly sampled subnetworks. Experiments are conducted on NAS Bench 201 and MobileNet space. StrengthsI generally agree with the proposed idea. Is this for the purpose of randomness during meta training? I am curious if the authors have ever tried other meta learning methods, such as the original MAML? I generally like this idea, although seems straightforward, applying meta learning during training the supernet is a reasonable and correct strategy.<|endoftext|>This paper addressed the multi model forgetting problem in supernet training by a supernet learning strategy based on meta learning. The paper is well written and the analysis gives insight. If the authors could present the comparison of training (search) time in the main Table, that will be better.<|endoftext|>The paper proposed an improved training strategy for oneshot NAS supernetworks. The key idea is to view the training of each subnetwork as a "task", and then to apply an MAML/Reptile style meta learning scheme to ensure efficient cross task adaptivity. This makes me wonder whether Kendall’s tau is the right metric to measure the effectiveness of the proposed method. While the authors did conduct ImageNet experiments as additional evidence, results there do not appear strong as compared to existing methods.<|endoftext|>This paper focuses on improving the predictive ability of supernet in one shot NAS, which considers a meta learning strategy to tackle the multi model forgetting issue. This paper utilizes a simple while interesting meta learning strategy to train the supernet in one shot NAS, and the experimental results on the NAS Bench 201 and MobileNet show the effectiveness of the proposed method. However, it seems that the proposed method is a direct application of MAML on one shot NAS.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper presents a model capable of generating CAD sketches with constraints. The model is implemented as two Transformers, one to generate the sequence of primitive, and one to generate the sequence of constraints conditioned on the primitives. **Strengths**   The paper is well written, easy to follow. My main concern about this paper is that it appears very similar to [Ganin et al.2021, Para et al.2021], which will both appear at Neurips. Checking the ICLR reviewer instructions, I see that these papers are indeed considered concurrent (*if a paper was published (i.e., at a peer reviewed venue) on or after June 5, 2021, authors are not required to compare their own work to that paper. But I remain uncomfortable accepting a paper that shares so many similarities with papers that have already been reviewed and accepted, even if shortly before the deadline. I suspect that the authors have been inspired by these concurrent works, maybe in the development of their method, its description, its evaluation, which is why I have a hard time ignoring them in my evaluation. To be clear, if [Ganin et al.2021] and [Para et al.2021] are to be ignored, then the technical novelty is of level 4, but if they should be considered, then the technical novelty drops to 1 or 2 and I would argue for rejection.<|endoftext|>The work builds on the SketchGraphs dataset that contains CAD sketches, consisting of primitives and constraints. Assuming the authors are willing to add (a lot) more details, I would suggest that this paper should be accepted. This paper looks at both of these questions. There is already important work in this area, i.e.Ganin et al.(2021) and Para et al.(2021), that has substantial overlap with this submission. A strong inspiration for this (as well as the other papers) was PolyGen from Nash 2020 which is appropriately cited multiple times in the paper. The paper is generally very well written (language wise) and nicely illustrated. It seems that Ganin et al.and Para et al.have more content devloted to clarify details, but this submission does not have a corresponding appendix with details (e.g.appendix A and B in Ganin et al.). 6 bit quantization appears a bit low and I wonder if that is really a suitable choice to achieve very good results. Maybe there could be an ablation study.<|endoftext|>Details (in order of importance):  A comparison to prior work (SketchGraphs and ideally also Willis et al.) While both have a different generation approach than the proposed method, it should be possible to compare the constraint NLL and the full sketch NLL to SketchGraphs (as shown in Para et al.and Willis et al.), the primitive NLL to Willis et al., and the distributional statistics of generated sketches (possibly augmented with a few additional statistics to batter capture primitive and constraint quality) to both SketchGraphs and Willis et al.The qualitative results look quite good, so I would expect the quantitative results to be favorable as well. ", but the authors don t mention how this is done. For the primer conditional generation, it might be good to discuss briefly if the primitives of an incomplete sketch need to be given in a similar order as during training (the typical order a designer would create the primitives in). The proposed methods contributes a new generation approach and explicit generation of both primitives and constraints over prior methods; and reconstruction of CAD sketches from hand drawn images over both prior and concurrent work. However, as a big minus, the result are not compared to any related work.<|endoftext|>The transformer based model is able to generate both the primitives as well as the primitive constraints that make up a sketch, and its output can be then imported into standard CAD software. The paper is well written, and the shown results look good given the dataset that is used for training. My main concern about this paper has to do with its technical novelty. On the other hand, while the results are nice, I don t think they are ready to be put into a production level system, in part due to the relative simplicity of the generated sketches. Some of the related work that s noted to be concurrent is in fact prior work. It would be important to include a comparison to this work or at least a more detailed discussion. The authors mention that the main distinction is that Willis et al.do not produce the constraint graph as part of their output. This paper is well written and tackles an interesting problem in a sensible way. However, due to limited novel technical insight and comparison to past work, I think it requires some improvement before it is ready to be published.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper tries to address the big model on the server side in federated learning (FL) via knowledge distillation. The key issues of existing methods and the challenges of the proposed method are not clearly stated. The technical depth seems weak. However, major concerns are existing in the current form of this paper:  The contribution is not clear.<|endoftext|>This paper aims to enable training of higher accuracy models by training a high capacity model at the server and lower capacity models at the clients. Experiments illustrate the promise of this approach. The problem considered in this paper is interesting and important. This paper suggests an interesting approach towards overcoming this issue, and the experimental results suggest that the proposed approach is promising. 9.The objectives (2), (5), and (6) all depend on the parameter $\epsilon$ that balances between cross entropy and distillation losses. However the paper has a number of limitations. This substantial difference, without any remark from the authors, leads me to question how much faith to put in the experimental results.<|endoftext|>This work proposed a KD based FL framework for training a larger server model securely. The problem (training a large scale model) by utilizing scatter data from large number of edge devices is well motivated. Overall, it s a good empirical paper, but it s better to address some of the above concerns before applying the proposed framework in practice. The public and private data is 1:1. It would be better to do an ablation study to understand the performance in different ratio. 5.Another issue would be the scalability. Does FedML support the implementation? 7.It s highly appreciated when the authors can discuss some limitations.<|endoftext|>The paper proposes an empirical technique to train large server models with  selective knowledge fusion. The authors tackle multiple challenges in federated learning with their approach.Their results seem to show improved accuracy for both hetero and homogeneous settings. The algorithm is also shown to work well in resisting poisoning attacks. What will happen if the server model overfits and predicts everything or most of the samples in the public data correctly? For example, the paper could show the effect of larger and smaller server models and the change in accuracy for different numbers of clients during poisoning attacks. This paper proposes a novel approach to tackle several problems in federated learning using selective knowledge fusion. However, the algorithm proposed is largely empirical and needs more thorough ablation studies to strengthen its core claims.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes a mechanism for dynamically assigning query key attention masks (searches) to one of several value matrices (retrievals), avoiding the rigid assignment of search to retrieval inherent in standard multi head attention. ### Strengths  The paper identifies a current limitation of multi head attention, and proposes an interesting method to overcome it. The paper shows results on a wide variety of experimental tasks. (I understand that is a fraught task given different hardware optimizations/settings between models, but it is still an important consideration)  On a related note, how might Compositional Attention scale to larger models with more searches and queries and more layers?<|endoftext|>The proposed mechanism disentangles the search and retrieval from the transformers architecture, where the retriever is no longer tied to a specific retriever that is in the same head   instead, there are multiple searches available, and multiple retrievers. Intuitively, the suggested method makes sense: it provides additional flexibility in the attention mechanism, since now the same retrievers can be re used by different searchers, and since a search is not confined to a single retriever. For some of these datasets, models are evaluated on OOD samples. Nevertheless, experiments convincingly show an improvement in almost all cases. My main concern regarding the experiments is that in all cases, parameters were shared between all transformer layers. This paper proposes a modification to the transformer s attention mechanism, which shows improvement on multiple evaluation benchmarks of various types. I find this proposal to be convincingly useful, although evaluation on more o.o.d splits and experiments with non shared parameters could have made results stronger.<|endoftext|>The paper first analysis the potential drawback of the rigid mapping between search and retrieval in standard attention heads, and then proposes compositional attention that disentangles search and retrieval and composes them in a dynamic and context dependent manner. To evaluate the proposed method, the author conduct experiments on the standard Transformer model and Vision Transformer. Strengths:   The problem studied in this paper is interesting and well motivated. The paper points out the shortcomings of rigid search and retrieval coupling in standard multi head attention. The paper should also compare with other variants of multi head attention mechanisms. The paper provides a somewhat novel insight into the shortcomings of standard multi head attention, and also proposed a compositional attention mechanism. However, there are still some concerns about the evaluation.<|endoftext|>This paper proposes a Compositional Attention to replace the standard multi head attention. This leads to redundant parameters and less generalizability. Instead, they propose to consider all pairs between query key and value. Weakness: They argue that the complexity overhead of compositional attention is lightweight. The appendix shows the overhead analysis on the contextual retrieval task. Multi head attention has been widely adopted in a lot of scenarios, such as pre training, fine tuning, various tasks with sufficient data or very little data.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 10; This paper proposes an implicit neural representation for 3D geometry, named implicit displacement fields. The method is consists of two SIREN networks as baseline models to approximate: a coarse base shape representation (low frequency) and an implicit displacement field (for high frequency details). Different from previous works, the displacement field is implicitly represented by a SIREN network. The coarse and fine networks design also enables applications such as shape manipulation. Strengths(1) To represent both the coarse surfaces and displacements implicitly by two neural networks is new. The results show that the proposed method can keep detailed 3D geometry while remaining a reasonably small network size. Ablation studies also demonstrate their effectiveness. Weaknesses(1) I would like to see more experiments and discussions about how the parameters ωB and ωD (ωB   15 and ωD   60) are chosen in the paper. Despite having some uncertainties (see weaknesses), the reviewer holds a positive opinion on this paper.<|endoftext|>This paper proposes an implicit representation for shapes, which utilizes two SIREN networks of different frequencies representing base surface and displacement respectively. In addition, the authors introduces transferable implicit displacement fields, which enables the transferability of displacements between aligned shapes. The evaluation results show that this method could represent shapes with sharper details while using less storage compared to baseline methods. The idea of implicitly representing both base surface and displacement map with neural networks is new and elegant. Transferable implicit displacement field (IDF) also adds to the novelty of this framework. The transfer experiments are impressive and show great potential in applying the method to downstream shape processing tasks such as detail modeling. Weakness  It would be helpful to explain how some of the hyper parameters are chosen e.g.attenuation factors $\nu$, and frequency factors $\omega_B, \omega_D$Other  Is it possible to perform hierarchical displacement mapping / shape modeling based on this idea? Any discussion is welcomed. The paper proposed a new method for implicit shape representation.<|endoftext|>The paper addresses the problem of obtaining a 3D surface reconstruction from point clouds. S3: Several experimental results are presented, including ablation studies. I feel that the method will fail with noise because of things like the need for normal to the points. This is a very relevant fact, and it is not described in the document. There are small tests with noise in the appendices, but the level of noise added is almost 0. However, it is not clear how this new representation will work with noise, and I believe it would be beneficial to clarify this. However, if the other reviewers think the paper is ready to be accepted, I will not be against it.<|endoftext|>The network is based on two SIREN architectures that separate low and high frequency information in the model. One returns an SDF of the rough shape while the second returns a displacement field at the surface of the SDF. The general idea was already introduced for single view reconstruction (in Li & Zhang as mentioned by the authors), but this papers evolves the setting to work for general geometry representation, including an inverse displacement and making the displacement transferrable by using a kind of feature based UV map. The results are both smooth in areas where this is correct, as well as able to show details where necessary. This could be incredible useful for applications based on implicit 3D representations that struggle with training complexity of highly detailed models. This makes it impossible to exactly reproduce the results. I do not know if this is part of the conference style guide but I would recommend to change it. In addition, the paper presents some elegant solutions for efficiently evaluating the loss in this new setting, and using a feature based UV mapping that can be transferred to similar shapes. The quantitative evaluation is a bit lacking, and the final version of this paper should include an exact list of shapes that were used in the evaluation.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper proposes a gradient based affordance selection, i.e., action/option selection, in addition to the value equivalent modules for tree expansion procedure(s) in planning. These affordances are later used for expanding the look ahead search tree. The key idea is to compute the gradient of performance loss with respect to the affordance model s parameters through the planner. # Weakness  The claim that the paper introduced "the idea of discovering affordances for planning in continuous domains" seems inconsistent. In contrast, GrASP uses gradients through the planning process to discover a discrete set of affordance mappings which are used for building the tree. How is the discrete set of affordance decided from continuous action and state space? While the authors mention "GrASP algorithm is not the SOTA model based RL in continuous domains", it is still difficult to say how significant the contribution of the proposed approach is. This seems like a relevant comparison, however some observations like "in *Ball In Cup Catch* it seems to be learning relatively faster than Dreamer" seem inconsistent with the plots shown. The result section highlights which of the different variants of GrASP perform best in different scenarios.<|endoftext|>This paper proposes a gradient based method for selecting affordances in planning. The planning procedure is aided by a learned model with an encoding, dynamics, reward, and value module. Most prior work uses the gradients through the trajectory under the learned model to update the policy/actions, whereas this work is about using them to update the collection of K potential actions to select from. Strengths:  The paper approaches the problem of continuous control from a discrete planning approach, and shows how the discrete set of actions/options can be updated with gradient descent. The paper acknowledges that it is not state of the art for model based RL in continuous domains, but it would nonetheless be useful to include the comparisons to DREAMER in the main set of results, i.e., in Section 3.1 and Section 3.2. This could be because the action space is already narrowed down and the planner only needs to operate over a lower dimensional action space, e.g., the tasks in Section 3.1 have 2 D or 4 D spaces. It’s not clear whether TD3 is performing poorly using primitive actions or with the pre trained options. In the implementation of the affordance module, is there some regularization to ensure the output actions/options do not collapse to the same output? This paper proposes a gradient based approach for planning in continuous control problems. However, the experiments currently lack relevant comparisons to understand the significance of the proposed approach.<|endoftext|>The paper proposes dealing with tree search planning in continuous action spaces by learning affordances. The authors propose an architecture composed of several modules, where they are able to plan ahead in a tree search manner by learning a module that expands K discrete possible affordance from each state node. The results suggest that the proposed technique is able to deal with difficult tasks in which standard tree search methods would not be easy to apply. Strength:  As far as I am aware, this is a novel contribution and an interesting idea overall. The paper is clearly written and is easy to follow. It could be possible that the initial set of affordances predicted by the model are quite similar, and it gets trapped in some local optimum that prevents the model from trying out other options.<|endoftext|>In this paper the authors claim two contributions (1) the incorporation of affordance consideration into the planning process and (2) an approach for discovering affordances which the authors have called GrASP as it is a gradient based approach. The paper is well structured and well written. The proposed approach is articulated well. The authors have done a good job of explaining the challenges as the approach was developed. While others in the multi agent simulation community and the RL community have looked at incorporating affordance inspired concepts into agent models there are aspects of this work which is different and novel from other previous work. The consideration of how affordances will affect the generation of the actions branches in a planning tree is important. My criticisms of the paper are by no means major. I would have liked to have seen1. a more detailed motivating example as to why incorporating affordances in this way is important. 2. the experimental domains selected could have been more sophisticated.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This allows an end to end gradient based learning to optimize the internal metrics of graph clustering, namely the Dasgupta cost and Tree Sampling Divergence. This paper proposes a novel graph clustering method with solid theoretical supports and strong practical performance.<|endoftext|>This paper presents a hierarchical clustering over graphs using a probabilistic model via continuous relaxation of the discrete tree based hierarchies. It uses two metrics the Dasgupta cost and the Tree Sampling Divergence. Overall, it may be an interesting addition to ICLR if the authors can improve on the weaknesses I pointed out.<|endoftext|>The paper proposes a probabilistic clustering method based on two losses: Dasgupta loss and Tree sampling divergence. In the current state, it is very hard to find support for the claims in the paper experimentally. The idea of hierarchical probabilistic clustering inference from the trees seems novel and interesting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; To construct the metric the method uses partial information decomposition, which decomposes the the mutual information between the latent variables and the labels into a sum of other information terms with specific roles:  Redundant information  Unique information  Complementary informationEach of these terms are precisely defined and an intuitive explanation os given. The proposed UnibBund metric is defined similarly to MIG based on the above terms. The 3 main information terms are well defined and explained. Minor:The notation could be a bit better. The authors clarified some details and notations. I keep my score and recommend the paper for publication.<|endoftext|>The authors leveraged partial information decomposition (PID) for analyzing multi variable interactions in latent representations. It would be better to find and show the cases similar with the redundancy and synergy attacks by analyzing learned features. I thus find it difficult to argue for acceptance of the work. The paper is well backed up by a comprehensive supplementary material that seems to contain all the details required for reproducibility.<|endoftext|>Or at least those qualitative measures can be elaborated in the main text. Using Partial Information Decomposition (PID) framework, the authors attempt to characterize the interaction among latent factors, correlation/disentanglement, for more than two latent variables. However, I am on the borderline between acceptance and rejection. I think this is a good submission and the proposed information theoretic framework is persuasive. The paper attempts to make a connection between the existing literatures in information gap theory and partial information decomposition to improve the entanglement analysis. I expected to see detailed ablation studies, beyond the suggested entanglement attacks. There is no analysis for robustness of partial information measures (the proposed bounds).<|endoftext|>This paper proposes a metric for evaluating disentanglement inspired by Partial Information Decomposition (PID). Pros:1) The problem that the authors want to address is important. Cons:1) The paper misses important references and discussion of prior work. 3) Experiments are limited to only 2 toy datasets and do not really highlight the advantage of the proposed metric. 1) Novelty:  The idea of an information theoretic analysis framework for disentangled representations was proposed by Do & Tran [1] but is not discussed in the paper. Please check [3] for more detail of these metrics.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper targets to a 3 dim metric: alpha Precision, beta Recall, and Authenticity, that quantifies the fidelity, diversity, and generalization performance of a generative model. This sample wise metric can audit models by judging individual synthetic samples by their quality. Weaknesses:  The technical novelty is marginally revised from Sajjadi et al.2018.The experiments are incomplete and therefore unconvincing. The testing datasets are a bit toy like. For mode collapse evaluation, please consider using attribute classifiers pre trained on CelebA, as well as IvOM [3]. The testing generative models are out of date. Imagine our community works on a new GAN paradigm and targets to outperforming StyleGAN3 [1], are we convinced to use the proposed metric which has only been validated on toy data/models? [1] Liu, Ziwei, et al."Deep learning face attributes in the wild." [2] Karras, Tero, Samuli Laine, and Timo Aila. "A style based generator architecture for generative adversarial networks." ICLR 2017. NeurIPS 2021. "Generating diverse high fidelity images with vq vae 2."<|endoftext|>This work addressed an interesting problem and a very relevant one   how to audit generative models? The most popular metric to evaluate generative models is FID but it is extremely opaque and population based. While the definition of these metrics are well grounded, their measurement process itself   which is done via trained NN classifiers   is a bit unconvincing. The authors demonstrate that the proposed metric can recover the real ranking of generative models on a synthetic data generation task and can discover issues like mode collapse. Strengths: The paper addresses an important topic that is time critical and relevant to the generative modeling community and the broader sphere of AI. It is well written and motivated and a pleasure to read overall. The authors clearly point out the issues that are lacking with existing metrics and motivate the need to define the new versions of precision / recall metrics. Moreover, the metrics enable a new use case of model auditing which can be very relevant is safety critical applications. Weaknesses: 1. The experiment with the predictive model depends on the downstream classifier as well   the performance of the generative model is tied to the performance of the classifier   hence the hyper parameter settings and training settings for the classifier should be explained clearly. 2.Another aspect that is missing from the evaluation is the application of model auditing on real life critical data. The idea of model auditing becomes useful mainly in cases where we need to filter at the sample level. Even though there are some open concerns regarding the evaluation, the fact that the current approach is one of the first to facilitate model auditing for generative models makes this a good contribution.<|endoftext|>A 3 dimensional metric space is proposed: fidelity (output quality), diversity (coverage of expected variability of output) and generalization (to what degree model avoids memorizing training data i.e.is truly generative) is proposed. Particularly the latter element is novel and the manner in which it is may be evaluated by developing an `authenticity’ measure for the task. The paper contributes useful thought and discussion for how best to argue the holistic performance of a generative model, contrasting with FID and other domain specific approaches. There are novel perspectives in this paper that are worth sharing with the community, particularly those contributing to discussion around generalization vs. memorization (copying) where it is shown that prior metric ignoring this aspect otherwise indicate performant models (c.f.time series NeurIPS challenge data). Indeed the brief literature review is relegated to the paper Appendix and was not helpful in arguing for the novel contribution of the paper in context of such work. The paper also lacks any reflection or conclusion on the limitations of the proposed metrics. A use case on high dimensional data e.g.images beyond a toy MNIST example would have better demonstrated the practical utility of this work for data where the need of such performance metrics is clearer. I am on the borderline tending toward accepting the paper given the interesting discussion around authenticity and memorization, but I am not fully convinced this is a practically useful paper.<|endoftext|>The authors propose a mechanism to calculate the "goodness" of a generative model on a per sample level; this method is model agnostic. This can be used to audit the model post hoc. Well written, easy to follow. 3.Discussion of NeurIPS challenge is important. I did not learn something substantial from reading the paper. 2.The claims are too strong. The generalized definitions of precision and recall are effective markers of understanding synthetic data quality. However, empirically they do not outperform other distribution measuring mechanisms (such as FID, PW, W) and I suspect methods like KL divergence or MMD, or TV distance will also perform comparably. 3.The appeal in this work lies in the fact that the proposed measures can be calculated on a per sample basis, unlike prior works. 4.Empirically, the authors consider few issues which may result in poor fidelity/diversity (mode collapse being one of them). If the models did not generalize during training, how do these methods capture them? 5.Could the authors clarify how the proposed method provides interpretability? However, recent work by Feldman et al.argues that overfitting (or memorization, in some cases) is essential for generalization. Can the authors comment on the same?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper revisits the old of idea encodings that have higher density around a particular point and lower density as one moves away from a particular analysis point. The authors are honest about the shortcomings (such as large memory requirements) of their method. While the exact architectures proposed by the authors are not directly implementable with standard techniques in the literature, one can come awfully close. Now, a counterargument might be that in such a dilation based architecture, the network might not learn to put together filters in a way that mimics the log polar structure produced here. For example, a strong baseline would be, "how could I build an architecture, using traditional methods, that would allow filters to be learned that incorporated dense sample around a point combined with sparse sampling far from the point?" In short, the argument that this is a compelling new architectural element has not been made. And if the network is not more expressive, then the question becomes whether it is significantly faster to train or does it require less memory? Apparently, this network offer none of these advantages and even some disadvantages. As such, it seems unlikely that anyone would adapt it for a specific task over current state of the art models. The paper is well written with a somewhat interesting idea, but ultimately, it seems unlikely that such an architecture provides advantages over strong baselines, which are not really explored.<|endoftext|>This paper introduces a new formulation for the convolution operator. Instead of defining convolution kernels on a regular 2D grid, this paper proposes to define convolution kernels on a circle divided by angle and radius. Note that the radius is divided equally in log scale to account for the observation that content near the center should have more impact on the outputs. Also, it can be implemented in existing frameworks easily using a customized pooling operator plus standard convolution. The authors claim that they use the same meta parameters for all methods for a fair comparison. However, this claim is problematic. It is well known that the meta parameters can significantly affect the performance of machine learning models, and proper tuning of meta parameters is a necessary step in practice. Also, the authors seem to report the best result in ablation study as the final performance, which optimizes the model performance on the test set and may lead to overly optimistic estimation for the performance. Therefore, the effective capacity of the model is larger than a model with plain 2D convolution. Also, the authors should also consider other aspects of the model such as the computational cost, memory usage, etc. Because deeper models may learn the effective receptive field better, the proposed method may have diminished gain as the model becomes deeper or even harm the performance. Also, more recent architectures such as transformer and active convolution may be able to learn the appropriate receptive field directly. Therefore, the performance gain might not be very meaningful as the gap may reduce as the baseline models are properly trained. However, some important aspects are not carefully considered in the evaluation.<|endoftext|>This paper proposes a Log Polar Space Convolution (LPSC) for general image recognition tasks. The proposed convolutional kernel distributes in the form of log polar spaces, and therefore the receptive field can expand exponentially as the radius grows without significantly increasing the learning parameters. The proposed LPSC are integrated into several popular CNN backbones like ResNet, VGGNet, MobileNet, etc., and the performance of modified networks has exceeded the original structure on several image recognition and segmentation datasets. The ability to expand LRF is a major claim, however, the current experimental results may be insufficient to support the claim. The authors only evaluate their methods on light weight network structures like ResNet 18/20, MobileNet, etc., while the receptive fields in those models might not be dense or broad enough and therefore can be compensated by the proposed LPSC. However, what about those more complicated models like ResNet 101 that may already have a good LRF coverage? Will LPSC still work on them? 3.The computational inefficiency can also be a concern here. The authors have already discussed that the spatial complexity can be a limitation (which I appreciate), but I think this also indicates an increased time complexity since the mask has to be pre computed. I think a discussion on the data distribution can be necessary. The authors only compared their method with dilated convs in segmentation experiments, but I think it can be important to also compare with other related methods like DenseASPP [a], Deformable Conv or Self Attention module. Overall, the idea of this paper is interesting and reasonable, while the proposed LPSC can be potential for dense prediction tasks. However, there are several limitations of this method, while the experimental results are not very convincing at this stage.<|endoftext|>The paper proposes a log polar space convolution in which elliptical kernels are convolved over the input. Actual implementation is achieved through use of log polar space pooling which can be used to create a set of pooled featuremaps to which a regular convolution can be applied. The advantage of such an approach is that it allows for much larger receptive fields without the need forr more weights. Positives Overall this is a nice idea, and the experimental results are fairly convincing in showing that the proposed method does not hurt in existing model architectures. Some mention of this in the text would be wise, and it may be better to for example change the title of the paper to something like "log polar space convolutional layers" to make it absolutely clear that the paper is about CNN layers rather than more general convolutions. Further, the authors need to clarify the use of the bias term in their implementation   it is not clear from the math/text if there is one or not. I realise that this might exceed the computational resources available to the authors to acheive, however the marginal gains are deserving of a much more detailed discussion. This is important as it would completly change one s interpretation of the results. The number of FLOPs to apply a standard convolution and a dilated convolution with the same number of weights should be identical. Further, I suspect that this analysis would only tell part of the story about complexity. More specifically one would want to understand the actual latency of the different approaches because the memory access patterns are going to be significantly different. In terms of wall time, how do the different methods compare during training and inference for example? Overall, I think there are some interesting ideas emerging from this work, but in its present state it s a bit borderline.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes training a VAE in parallel with a classifier in continual learning and learning a Gaussian Mixture Model in the embedding space for sampling pseudo data points in experience replay. 2020.The contribution of this work is rather incremental and there is not enough discussion with related work, the experiments are not convincing either. There are quite a few prior works that have proposed to apply VAE in continual learning and there are also some works that have proposed to generate pseudo samples for replay. The comparison with other baselines is insufficient either as mentioned above. 2.No enough explain of the GMM model, is it isotropic Gaussian? 3.Fig.1 is confusing as the pseudo points from the decoder should be fed to the classifier for replay, but the arrow is pointing to the encoder.<|endoftext|>In this paper, the authors propose ICLA, an approach to tackle the problem of incremental and continual learning. Particularly, they adopt an encoder decoder style architecture to learn an embedding space that serves two purposes. (i) The embeddings from the encoder serve as an "internal" representation of data whose distribution is explicitly estimated using a Gaussian Mixture Model. When new classes are incorporated (as in incremental learning), additional components are added to the parameterization of the GMM. The authors provide some theoretical guarantees for error bounds and conduct empirical evaluations on MNIST and FMNIST. (iv) Other clarifications on the method:(a) It is true that with an increasing number of samples/classes ICLA doesn t suffer from a memory bottleneck. This could prove to be a computational bottleneck on larger scale tasks. (b) "However, we can conclude that ICLA has a superior performance when the network size is small and using a memory buffer is not possible"  > Model parameter complexity results are not presented in this study. How did the authors arrive at this conclusion? (e) "This is because FMNIST data points are more diverse. I am willing to update my score if the authors are able to provide a convincing response!<|endoftext|>This paper proposed an algorithm for learning concepts incrementally taking inspiration from Parallel Distributed Processing and Complementary Learning Systems. The core idea is to use Gaussian Mixture Models and update them incrementally plus exploit a generative model to perform pseudo rehearsal. Generally, I like the strong theoretical background of the paper and approach. The paper builds on important theories that are central in lifelong learning (PDP, CLS) and mathematical approaches GMM, WD. My main concerns are two. The main result for continual incremental learning in Fig 4 are not compared with the baselines that are instead shown in Fig 3. The paper addresses in important challenge in lifelong learning, i.e., to be able to generalise across a variety of classes that are learned sequentially through different tasks, meeting both new classes and drift, when the distribution of one class changes. However, I feel that the paper could be improved significantly in clarity, readability, and presentation of results. It s not clear to me what are the main claims of the paper, and while some results appear to be good, the final set of results (fig 4) are difficult to interpret. In short, this paper has a strong potential, but requires a better presentation and a better illustration of the results and baselines for the last experiments in Fig.4.<|endoftext|>The submission proposes a new unified methodology for continual and incremental learning based on Parallel Distributed Processing theory and Complementary Learning Systems theory. The authors prove that their methodology optimizes an upper bound for the combination of previous tasks and also demonstrate the effectiveness of their approach empirically. The submission is interesting and well motivated. The main novelty of the approach is fitting the latent space of the auto encoder structure with a Gaussian mixture model which is then used to help with both generation of pseudo samples and to prevent concept drift. To be more specific, the main innovation in this paper is the use of a GMM to interpret the latent space of an auto encoder. However, the authors do not explore the consequences of this assumption in cases were it is not satisfied. Neither of these ideas are new. The submission is a step forward in implementing approaches that can learn in a continual manner.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This work proposes Self Ensemble Adversarial Training (SEAT), which utilizes the states of history models during training to efficiently improve the adversarial robustness. # Strength:This work proposes a novel ensemble method. It also performs theoretical analysis, which compares the difference between this new self ensemble method and existing ensemble methods. The exponential moving average (EMA) is used in this paper, which is also widely used to improve the test performance of models. Could the authors also report the results of each component of AA? My concerns are well addressed. The added experiments compared to vanilla EMA further demonstrate the effectiveness and novelty of the proposed SEAT. Additional experiments on each component of AA (one type of adaptive attack) present the reliability of the robustness. The theoretical analysis and proof are easy to understand and thoughtful. The experiments show this method has good potential.<|endoftext|>This paper proposes an adversarial training scheme called SEAT, where the final robust classifier is united by averaging states of every history models through the adversarial training process. Strengths:* The proposed method is well motived, and authors provided multiple experiments to support their claim and justify the performances of their proposed method. Weaknesses / discussion questions:* Deterioration is a very interesting topic and desired more studies. I believe the paper could be improved if a clear explanation can be studied and explored. what loss function used for SEAT? The authors are suggested to report more ablation study discussing different combinations (e.g., SEAT by TRADES, SEAT + CutMix by TRADES) as well. * On Table 1, I found at least TRADES is not sufficiently trained, its performance over NAT and AA should be at least better than 84% and 49.5% respectively. * Although the paper is overall easy to follow, it seems to be written in a rush. Although the idea of self weight ensembled method is intriguing, I am not that convinced by the current experimental settings and found it is lack of several competing baselines and performance on larger dataset.<|endoftext|>This paper proposes the Self Ensemble Adversarial Training (SEAT) which averages weights of history models for robustness. Compared to individual models or an ensemble of predictions from different classifiers, they argue that the proposed self ensemble method provides a smoother loss landscape and better robustness. The paper is well written with theoretical propositions and extensive experimental results.<|endoftext|>In addition to this, the paper experimental evaluations requires more work. In addition to some providing some theorems concerning their proposed method, the authors empirically show that the trained SEAT models are competitive with current methods for improving the adversarial robustness of the method. Please find my review of the paper *"Self Ensemble Adversarial Training for Improved Robustness"* below. If I understood correctly, the proposed method is a combination of both adversarial training and model ensembling, where in this instance the ensembling is performed by maintaining a moving average of the weights of the model at past time steps. ### Formality theoretical contributionsThe paper proposes a number of theorems and propositions that I personally found to be written in a slightly weird form. A clear example of this is found in Theorem 1, which ends with: *"SEAT will hardly be approximate to the average prediction of history networks". * In this case it is unclear to me what is meant with *hardly be approximate. ### Experimental results are lackingWhile the authors did an admirable job by providing a exhaustive list of baseline methods for improving the adversarial robustness of a classifier, I believe the experimental evaluation still to be somewhat lacking. Specifically, I believe it to be important to study adaptive attacks that would be able to deal with the smoothness of the loss landscape. The theoretical contributions of this paper are still not clear to me. However, based on the discussion with the authors and other reviewers, I will increase my score.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This work studied the limitation of conventional Federated Adversarial Training approach, and proposed an \alpha weighted relaxation for Adversarial Training in the federated learning setting. Weaknesses:(1) The motivation of the relaxation on Federated Adversarial Training is counterintuitive. (5) Some related works of adversarially robust federated learning are not discussed. For examples, some of them pointed out that local clients have limited computational resources to afford the local adversarial training. "Adversarial training in communication constrained federated learning."<|endoftext|>This paper introduces the alpha Weighted Federated Adversarial Training algorithm. The key of the idea is that in the aggregate step, the center prefers the local machine that yields smaller lost. Some theoretical results are delivered with numerical experiments. But the adversarial x tilde is purely determined by the current estimation of theta, and during the training, no local machine knows the optimal theta. In the discussion of figure 2, I don t understand why hard to train local machines may "over optimize" their model. But the authors clearly say that the alpha weighted mechanism doesn t work for federated standard training. Several notations are unclear, for exampleL smoothness: l(.,.) What is the meaning of this expectation?<|endoftext|>The authors explore the adversarial robustness of federated learning. The authors also experimentally establish that federated learning models are most susceptible to attacks when clients are using non IID training sets. The paper tackles an interesting direction in federated learning, and is well organized. Overall, I enjoyed the paper itself and the presentation of ideas, but I do think the contribution is incremental. PGD 20 or PGD 10? 2.The novelty of this paper is incremental. Besides, only a slight improvement can be seen in Tabel 2. The motivation for the specific weighting chosen was not very clearly explained.<|endoftext|>This paper tackles an important problem in federated adversarial training: robustness accuracy significantly drops at the later stage of training. The authors first raise their assumption for the cause of this phenomenon: Adversarial training amplifies the heterogeneity of data distributions across different clients, and overfitted local robustness can not well generalized to other clients. One piece of suggestion is that, if your assumption on the cause of the robustness drop phenomenon is correct, maybe combining "benign adversarial examples" [1] with \alpha WFAT can even further improve performance. 3.The proposed \alpha weighted federated adversarial training method is technically solid, easy to implement, and empirically achieves good performance under multiple different adversarial training and federated learning settings.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; Overall this paper is an analysis paper of 3 self proposed metrics on posthoc analysis of spurious signals. The metrics turn out not to be accurate enough for the tasks at hand, especially when the spurious signal is implicit or unknown. The paper offers an analysis on three metrics for explaining post hoc reliance of models on spurious signals. Contributions: the paper proposed 3 metrics that as admitted by the authors are not performing as well as one would like.<|endoftext|>The authors present work that aims to test, whether post hoc explanation methods can detect unknown spurious signals. To improve clarity of the figures, I would suggest adding arrows to Figure 1 to direct the readers attention to the spurious signals. Overall, the manuscript seems to be quite packed. p6: “We results report for the small DNN model in the paper”  > “We report results”  p3: “which of these associations are be spurious”  > are spuriousOverall, this paper raises important questions about the usefulness of current post hoc explanation approaches for NNs and provides a range of empirical results to support these claims. Strengths  This work raises very important and timely concerns with regard to using NNs for critical application domains like health care. What population was recruited for the study, is this a population of medical practitioners, machine learning engineers, or random subjects? Briefly, when participants detect a spurious signal with one method, will this not influence their judgement based on the next method.<|endoftext|>The paper aims to validate whether post hoc explanation methods are effective for detecting unknown spurious correlations. The authors claim that they find that these methods also seem to attribute to the spurious signal (FAM > 0.5) even when the signal is not being relied on by the model. It is unclear whether "the model" here is the normal models or spurious models. Why do the authors think that the CCM measure indicates that all these methods do not indicate the presence of spurious signals even with both high and low CCM? Overall, this finding suggests that TCAV is less susceptible to false positives". The overall idea of the paper is good.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The authors manually constructed a dataset that aims to train and evaluate model’s capabilities in naïve logical reasoning. Strengths:A dataset was built to train and evaluate model’s capabilities in naïve logical reasoning, which consists of 10,296 instances. Weaknesses:All the instances in the dataset were selected or written by humans, and there is no valuable or useful method proposed in the dataset construction process. There are even many grammatical errors in the examples given in this paper. Besides, it is not clear how they guarantee the quality of translation. It seems that the main difficulty of answering the questions in the constructed dataset lies in semantic parsing rather than logical reasoning.<|endoftext|>This paper proposes a new dataset that tests the capability of logical reasoning through multiple choice reading comprehension. The collected questions are difficult for BERT large and RoBERTa large which show lower accuracies than the human baseline of college students. I agree with the authors that the naive logical reasoning needs to be explored as an independent research topic, but I think the authors need to prove that the proposed dataset is useful in some sense in relation to other NLU tasks, datasets, and systems. I m not sure if the authors can use "naive" in the sense that is referred to in the context of Naive Set Theory by Halmos (1970). There are grammatical errors or typos in the paper. Questions and feedback:  It is unclear how the authors translate Chinese and English examples. Are there any quality controls over the translation? Could the authors flesh it out?<|endoftext|>The main contribution of the paper is the construction of a dataset in Chinese and English (termed NAIL) with items for training and evaluating such naive logical reasoning, where the task in the dataset is cast as a standard multi choice (4 choice in this case) question, with only one correct answer. I am not fully convinced about motivation to have this sort of logical reasoning embedded in the model   these NAIL tasks seem more like  enigmatic  puzzles, and a stronger motivation is needed to justify why pretrained language models should even need the ability to solve such puzzles: can they be fully trained (with larger datasets) to do well on the puzzles? Another question is, where I expected a deeper discussion: do we need AGI to solve puzzles like the one presented in NAIL? Is it a data problem or truly a human like reasoning problem? Only performance of BERT and RoBERTa is attested in the task   I would very much suggest to try out other language models (e.g., T5) and think of possible (auxiliary/intermediate) tasks which could prepare off the shelf models to become better at this type of task. This is done through the introduction of a new task called naive logical reasoning. I feel that more work is needed (see the main review).<|endoftext|>This work introduces NAIL, a bilingual (English and Chinese) a benchmark for naive logical reasoning, inspired in the kind of questions, involving this aspect contained in standardized exams such as ChineseNational Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT). NAIL E, which are actual examples from both CNCSE and LSAT, translated to English and Chinese respectively, and NAIL I, a data augmentation approach that uses the structure from existing examples and replaces the three aspects of the problem, subject, predicated and objects in order to create new instances. The paper is well organized and written. All the decisions taken by the authors are properly described and justified. My only concern on a paper of this quality is the lack of experimentation using NAIL as a pre training step for other logical reasoning datasets. If learning the logic of this dataset bring naive reasoning still into a model, that should be reflected into the performance of other logical datasets like LogiQA, ReClor, RACE, but also in CLUTRR, MAthQA, AQuA, HotpotQA among others ...
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper takes a fully Bayesian approach to Gaussian Process (GP) active learning by using MCMC sampling to consider multiple model hypotheses from a full posterior, from which it selects examples for GP regression using two new active learning strategies   Bayesian Query by Committee (B QBC) and Query by Mixture of Gaussian Processes (QB MGP). The authors evaluate these strategies against several baseline active learning methods on several simulators. When I first read the abstract, I was intrigued by the idea of the bias variance tradeoff being neglected in active learning, how in GP this tradeoff corresponds to the length scale and noise term, how “the optimal mode of the joint posterior of the hyperparameters is equivalent to the optimal bias variance tradeoff”, and how B QBC and QB MGP directly incorporate this tradeoff in their acquisition functions to mitigate bias variance issues. However, I believe the paper is not ready for publication. Specifically, my reading of the paper is that the proposed methods are designed to select the correct full posterior mode as quickly as possible. *** Section 5 ***While I appreciate the multitude of simulators tested in the experiments, I am not convinced by the experimental results.<|endoftext|>This paper considers Bayesian treatment of the hyperparameters in Gaussian process based active learning, based on whichtwo novel acquisition functions are proposed. Strengths: This work is interesting and well motivated. The proposed B QBC seems to exploit more of the available information. How is the experimental validation different from Bayesian optimization? This is an interesting work that considered Bayesian Gaussian processes for active learning.<|endoftext|>This paper introduces two new active learning strategies for Gaussian process regression, based on a fully Bayesian treatment of GPs. Instead of using fixed hyperparamers obtained by maximizing the marginal log likelihood of the GP model, the authors suggest learning a posterior belief over GP hyperparameters, and make use of the full distribution to design more effective active learning methods. 2) a variant that combines a notion of disagreement with the predictive uncertainty (QB MGP)These methods are then compared to others on 8 synthetic datasets. **Weaknesses**  connections and comparison to prior work is incomplete  experimental results are not very conclusive; no experiments on real data  parts of the paper are unnecessarily convoluted and unclearThe problem of overfitting to the marginal likelihood problem is well known in the GP community. How does this paper s approach differ? The experimental results on synthetic data are quite comprehensive and insightful. How does their approach compare empirically?<|endoftext|>It seems that the authors introduces two novel acquisitions functions for Gaussian Process models:the first one is a mode seeking Bayesian variant of Query by  Committee (B QBC), and the second is simultaneously mode seeking and minimizing the predictive variance through a Query by Mixture Gaussian Processes (QB MGP) formulation. The authors also present several simulations. The rest of the paper, including abstract and introduction, is quite confusing in my opinion. The concepts of "bias variance trade off" and "active learning" are often referred but it is very difficult to understand the connection. Please, improve your explanations in all the work. Please, include these kind of acquisition functions in your discussion.
Reject; rating score: 3; rating score: 5; rating score: 5; The paper proposes a self supervised embedding learning method to learn embeddings of various sized neural network architectures. Each network is first represented as a low rank projection of a Jacobian matrix, where the rows are Jacobians (output averaged if multivariate) evaluated at various inputs at random initialization time, called EDJM. (3) but $(y,y )$ in Eqn. Minor presentation issues:+ It d be great to see the training time direction in Fig.4 .The problem of embedding various architectures into the same fixed length vector is an interesting one. However, the method is not described clearly, and some of the algorithmic choices are not well motivated. Given these considerations, I don t recommend acceptance in its current form. I hope the authors can help clarify in the rebuttal period. If so, are they fixed for the same architecture, or across all architectures? I am not convinced by both motivations. (5) can be used to achieve roughly uniform volume assignment. But isn t that given by Eqn. The notations here are a bit confusing.<|endoftext|>The premise is interesting and the implications are exciting. Section 4.3   The predicted accuracy looks quite strong. Random Forests can overfit on data really easily, especially in this case since you ll have only 3 features as input. In Section 4.4, I found the premise of the experiment interesting, but I m unable to conclude anything interesting based on Figure 4. Can you present the data in a more readable format? 5.1, 5.2   I appreciate the simplicity of the experiment by not confounding the results by including the hyper parametrs of TPE and DE as part of the search space. However, the results in this section are mixed. Also, as was acknowledge by the authors in this section, computing Jacobians can be slow. Are you using the contrastive learning network from one dataset and using it zero shot on the other, and then regressing on the accuracies using Random Forests? More details are needed about how the Random Forests were used.<|endoftext|>This can be achieved by  first computing the data jacobian of the network with respect to datapoints sampled from different neighbourhoods. This jacobian matrix is then input to a contrastive network which produces architecture embeddings. The contrastive views in this case are different initializations of the same network, which in turn must yield the same embedding. I am assuming that the contrastive learning is as good as predictive power of the input2. How would you decide which images from the dataset are used to compute the jacobian matrix? The experiment conducted on NATS BENCH is not enough to substantiate the claim that the embeddings can generalize across various search space. In order to make it more compelling, they must add more empirical evaluations requested above.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper addresses on policy errors in model based Reinforcement Learning. While I am not familiar with the related literature and I might have missed something important (hence I lowered my confidence accordingly), I have the impression that this work is solid and would bring a nice contribution to the conference.<|endoftext|>The paper considers model based reinforcement learning (MBRL) of the MBPO flavour, where model free RL methods are accelerated using rollouts of a learned model. THis paper proposes to alleviate (off policy) model bias per rollout using the (on policy) data and calls this approach on policy correction (OPC). There appears to be many discrepencies between Fig.3 and FIg.<|endoftext|>The authors introduce a model agnostic technique for improving estimates of on policy data and show improvements on MuJoCo and PyBullet domains. This is a common experiment in OPE literature. Do the authors have any insight as to why this technique matters much more on the PyBullet domains?<|endoftext|>The paper proposes a way to combine observed data with a learned model to mitigate the cost of each choice separately   the observed data is over fitted to a specific policy while the learned model has approximation errors. The idea of correcting samples with the model sounds immediate, which means more is required of the analysis: (a) Since there is a trade off between model based and sample based, how to implement\calculate this trade off in the proposed correction?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper extends the Lottery Ticket Hypothesis(LTH) to a more challenging case and proposes the concept of Dual LTH: Any randomly selected subnetwork of a randomly initialized dense network can be transformed into a winning ticket. 2.The construction of the paper is clear and easy to follow. As formulated in the abstract, the DLTH demonstrates that any ticket in a given lottery pool can be transformed into a winning ticket. And the definition of winning tickets is a subnetwork, which can be trained from scratch and match the performance of the dense counterpart. But on CIFAR 10/100 and ImageNet with a 50 98% pruning ratio, the performance of all the subnetworks still remains a unignorable gap against the dense model. Personally, I think it is more meaningful to investigate the proposed method in the setting when subnetworks identified by both LTH and DLTH are winning tickets. The RST method follows the same layer wise sparsity ratio derived by GraSP. 3.The related work about sparse network training needs to be re organized, which only tells several works without any logic. The caption of Figure 3 covers part of the y label. Overall, it s interesting to investigate the proposed Dual Lottery Ticket Hypothesis.<|endoftext|>This paper studies the Lotter Ticket Hypothesis (LTH) and proposes a Dual Lottery Ticket Hypothesis (DLTH). DLTH describes that any ticket in a given lottery pool can be transformed into a winning ticket. The idea of exploring a transferable LTH is interesting, which is complementary to the original LTH. The empirical study is clean and shows promising results on some datasets. The general writing is good. However, I have a few concerns about the current version:1. The empirical study is only on small backbones and datasets, how about larger ones (ImageNet, etc.)? 3.The paper ignores some related but important works, such as [a,b,c]. It would be better to compare with them, e.g., RigL. a. Rigging the lottery: Making all tickets winners. c. Drawing early bird tickets: Toward more efficient training of deep networks. Overall, this is an interesting and good paper. I will consider making the recommendation for the paper. Most of my concerns have been addressed by the authors.<|endoftext|>The paper proposes a new pruning method that resembles distillation   keeping pruned connections within the pruned network and using them when generating the models  output. The method works better than the currently popular topic   LTH, and works better than some pruning at initialization methods such as GraSP. The paper is overall clearly written, and the concept is easy to understand. The experiment results show the superiority of the proposed method compared to other methods like vanilla LTH. Cons:  I am not sure if this work can be seen as a complement/supplement/extension to the LTH. One key point in LTH is "trained in isolation", which means that the sparse sub networks should be extracted out. In this work, however, the subnetwork emerges late, and all the connections in models are alive during the training. Also, there is also no such phase as re training. Based on the comments above, I think this paper is more like a pruning paper aiming at compressing models and training models at the same time. More specifically, pruning after training methods seem to be the correct way to look at. For example, the \bar{\theta^*} was never pre defined. Minor: Some citation formats should have parentheses. My final score will be mostly based upon discussion with authors regarding my points outlined above. Currently, I think the paper is somehow off topic, but I am open to hearing from the authors and willing to change my view if I had made mistakes.<|endoftext|>The paper claims that "Any randomly picked subnetwork in a dense randomly initialized neural network can be transformed into a trainable condition, where the subnetwork can be trained in isolation and achieves better at least comparable performance to LTH and other strong baselines." In order to demonstrate how such a transformation may be achieved the paper introduces RST which gradually penalizes the magnitude (L2) of the non selected parameters during training (upon which they are zeroed out explicitly) in order to arrive at the randomly selected sparse sub network. Major Pros:That randomly selected sub networks, can attain same performance as dense and comparable sparsity IMP derived LT has reached significant attention. The main idea presented here at tackling how a random network may be  transformed  via RST is interesting. Specifically, for high sparsity levels   performance in iterative magnitude pruning depends strongly on number of iterations (I assume this is what is meant by cycles?). Style and clarity (not affecting score):Throughout the paper much is left to the reader to infer burdening the clarity of the paper and its decipherability:  At the onset of the main claim: what is a " trainable condition "  Information Extrusion mentioned but not defined (as early as the abstract), only to be exemplified via implementation in section 5. The updated manuscript changes the erroneous original claims ("any network can be transformed to trainable condition") and precisely modifies and scopes them ("random ticket wins"). In particular, giving room for a key novelty in the paper in the form of RST. However, with the updated discussions and revisions as well as the correction of the core claims, the paper rises to the level of acceptance   assuming addressing the above issues, which the authors demonstrate willingness to do. The paper s main claim as presented seems to be false   it is expected not to hold true that *any* randomly picked subnetwork is transformable to a network as performant as the original dense network if trained. For the particular method presented (RST) dependency on cycles leaves utility / insight unclear.<|endoftext|>This work proposes the so called Dual Lottery Ticket Hypothesis which claims that every ticket in the ticket pool, i.e., every random sparse subnetwork in a randomly initialized dense neural network, can be transformed into a winning ticket with admirable trainability. This "transformation" proposed in the notation of Random Sparse Network Transformation (RST), in practice, comes in with the form of a squared L2 norm regularization on the masked weights, making them still involved during training while extruding the information contained in them and transferring into the unmasked ones. The empirical experiments show good empirical performance of RST compared to the vanilla LTH, pruning at initialization methods and LTH variants such as EB LTH. (3) and (4), Random Sparse Network Training seems to a better name for RST as far as I can see because it looks more like a general (potentially a uniformly better one) training method for sparse neural networks from scratch. For example, we could for sure apply RST to the winning tickets found by IMP directly. Will combining IMP + RST give us a better performance than RST + any tickets? This experiment will give us a clue of which one of the following is more true (or both not true):1) a winning ticket achieves the best performance regardless of the training method and any transformed ticket with RST can not achieve better; OR2) the winning tickets are still better subnetworks and can also be improved in terms of accuracy with RST? If the first case is more true, then the contribution of this work would be much more interesting to me. Let alone that the core regularization technique in RST is not new. I think this issue is also related to my first concern about the posing of this work. My evaluation for this work is that it is below the acceptance threshold, but I tend to give a score 4 (but I don t have option). Major reason is the posing of the DLTH claim and the contribution.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper proposes a new Kernel for Bayesian Optimization to incorporate  Landmark meta features  of groups of tasks, so that the Kernel can be used to improve the efficience of the Bayesian Optimization on a new related task. The Kernel is based on 2 nested neural networks that are jointly trained via first order meta learning method REPTILE. The authors show that the method is overall better performing on the benchmark HPO B v3. Hyperparameter is expensive and transfer learning is a promising solution to reduce the cost of HPO. I do not see this discussed explicitly in any of the experiments. If it is important, it should be supported by the experiments. The proposed solution is most certainly more computationally expensive than random search, or the fast Bayesian Optimization algorithm ABLR. The standard deviation on Table 1 is way too large to be able to conclude anything. I could run again the same experiments with different seeds and observe an upward trend instead. The paper does not discuss how to chose these hyperparameters and whether tuning them is important for the performance of the algorithm. It is not defined in section 2.<|endoftext|>The negative transfer problem is also not once mentioned in the experimental evaluation. This paper introduces DKLM, a novel approach to hyperparameter optimisation based on transfer learning. I m inclined to believe the validity of these results, although I wish the authors would have included code / more details of the setup. "Observed" makes it sound a bit like an event in nature. Why are you using REPTILE? Further, the lack of clarity unfortunately leads to open questions about the proposed approach itself. It would be great if an appendix with this information and/or code of DKLM were provided by the authors. However, the hyperparameters in the GP (but not the GP itself) are often inferred by maximising the evidence. In any case, I feel that this would need to be exposed much more clearly. The proposed method, DKLM, for transfer learned HPO seems interesting and is, as far as I m aware, novel. They construct a neural network architecture that take as input a literal dataset (with all rows/datapoitns and columns/features) and outputs a learned representation. The exposition leaves me with too many open questions as to how DKLM actually, as well as some of the experiments, actually work. Further, one of the main contributions of the paper – avoiding  negative transfer  – is not discussed sufficiently and not evaluated at all, and I have trouble following the authors in why it is appropriate to say that DKLM learns "landmark meta features".<|endoftext|>The authors focus on the important problem of more efficient hyperparamter optimization through (meta) transfer learning. More specifically, the author propose a new method (Deep Kernel Gaussian Process surrogate with Landmark Meta features (DKLM)) that embeds a meta feature extractor model within the kernel function of a deep GP. Results included error bars (Table 1). Weaknesses:  Experimental setup could be made clearer as to which tasks are used for training and eval in the transfer setting (i.e., which tasks are used to originally train the meta extractor, and on which tasks is it transferred). Minor:  p. 3: In the fourth line from the bottom, the reference "2020" needs to be fixed. It is a reasonable paper for acceptance.<|endoftext|>This paper focuses on doing transfer learning from related tasks for the optimization of ML model hyperparameters. These meta features are then passed to a kernel function that, finally, parameterizes the Gaussian process used during Bayesian optimization. Experimentally, the authors compare their method to several baselines, both in the transfer and non transfer learning settings; aggregated metrics show that the proposed method (DKLM) outperforms other baselines on average. The clarify of the paper could be improved. In particular, the experimental setup was not entirely obvious to me. Comments/questions   You mention the "negative transfer phenomenon" several times as a major motivation for this work; I would recommend describing it in more detail in the paper. Did you compare DKLM to a GP parameterized with the learned metafeatures from Dataset2Vec? This is an interesting paper, but its novelty is limited.<|endoftext|>Experiments on the HPO B v3 benchmark show this procedure equals or beats 10 baselines when using 20 to 100 trials. It shows how various existing techniques and ideas (deep kernel Gaussian processes, landmark meta features, deepset) are combined with meta learning to combat the "negative transfer" phenomenon. * A mean function $m$ is introduced, but not used, and Eq (1) shows the mean as $0$. * In Eq (2), $y$ vs. ${\mathbf y}$  It would be nice to keep the colors consistent between Figures 2 and 3 for the common curvesOverall good, well written, self contained article that presents a novel combination of existing techniques, and addresses the issue of negative transfer when using transfer learning for Bayesian HPO. The experiments conducted are well designed and support the conclusion.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The paper proposes a new version of transformer called concept transformer with the aim of improving the interpretability of its attention by computing cross attention between the inputs features and a set of concepts. * **Writing quality**: The writing clarity of the paper is good, the idea and content are easy to follow, the motivation is clearly explained, and there are good visualizations that help understanding. At the same time, overall the related work section gives a good coverage of prior works about interpretability and attention. I think that’s a nice advantage of the proposed approach. Overall, I think the paper proposed a nice interesting model on an important problem, motivates the goal of the model in a compelling manner, presents the model with good detail and clarity, provides a nice mix of experiments of different types and on different datasets, and can be of benefit to the research community, and therefore I recommend its acceptance and wish best of luck to the authors.<|endoftext|>The authors present their work in a variety of domains and show how it aids model decision interpretability. This paper presents a fairly straightforward yet novel modification to a vanilla Transformer that introduces concepts, which the user has to provide during training time. The authors provide a few examples where a misclassification happens and they show how the model attended to the wrong concepts, but is it possible this is a coincidence? A weakness of the work is that the concepts provide an explanation of perhaps why the model is wrong but what I expected was the ability to do some sort of fix of the model s attention so that it can be forced to output the right answer.<|endoftext|>The paper introduces a transformer based architecture for (deep) concept models. Here, the authors suggest using the attention mechanism. This work investigates the important field of interpretable deep models. Furthermore, distinguishing between local and global concepts by the design of the architectures is a very interesting approach. Can you elaborate on this further? The introduced ConceptTransformers seems to be a technical extension to „simple“ linear classification heads of Concept Bottleneck Models.<|endoftext|>This paper interestingly focuses on addressing the relationship between post hoc  explainability and inherent  interpretable model. Quality and clarity: The quality of the paper is good and it has a good motivation. The targeted goal is interesting and meaningful and the authors achieved this by linearly enforcing domain knowledge to weight allocations of attention mechanism. The writtenresults seem to be reasonable. Originality:Although interesting, It’s a bit hard to say the introduced technique is quite new, since a simple linear combination of attention weights and concepts patches (domain knowledge) in Transformer is the main contribution.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper describes an approach to learn visual concepts with a few examples. The paper considers a problem consisting of paired images and sentences, optional concept descriptions, and the target question answering task for a new concept. The proposed approach utilizes the object detector (He 2017) to map visual objects or relationship into the box embedding space (Vilnis 2018), and also utilizes a neuro symbolic program to identify the referring object as well as relationship among concepts. Section 2 concisely summarizes the context of this work. ## WeaknessI do not have a major concern on this paper. The paper uses a pretrained Mask R CNN for extracting objects in a given image. I wonder what is the effect of detection errors and the influence of on what dataset these detectors are trained. The paper proposes a novel framework for fast visual concept learning that builds on a neuro symbolic program executor, box embeddings, and graph neural networks. Given the novelty and the effectiveness, I believe the paper makes a solid contribution.<|endoftext|>This paper tackles concept learning problem. As long as the author could address my questions for the setting, datasets, and details of the approach, I would recommend accept. It can also mediate the concept embedding using additional text explanation. To verify this approach, this paper follows a meta learning setting. Specifically, the concepts are split into training, validation, and testing. The model first learn the base concepts from the training set and then quickly infer the concept embedding for the testing split. The paper s idea is very interesting and novel. For the dataset (sect.4.1), if I understand the approach correctly, the approach first needs a paired image and sentence to learn the base concepts. In this setting, the sentence should be a descriptive sentence that describes the object/concept in the image. How to do the semantic parsing of the sentence? Second, it is pretty rare to see a paper that makes neural symbolic machine works for the real language. The neural symbolic machine works very well for the template language. For the setting, I wonder what is the setting for CUB?<|endoftext|>The paper provides a method for learning new concepts from a few examples with both images and natural languages. It is a relatively novel task to learn the new visual concepts from a few images and sentences. In the experiments, the authors validate their methods in both real world and synthetic datasets. The proposed method cannot recover from the errors made by those components. It would be great to explain in a bit more details about the failure patterns of the proposed method. I recommend a rating of “weakly accept”.<|endoftext|>This paper presents a unified meta learning neuro symbolic framework for fast visual concept learning from diverse data streams. It introduces a new embedding prediction module to integrate visual examples and relations to infer novel concept embeddings. It’s meta learning continuous learning approach also uses supplementary sentences to relate concepts to one another. The writing and paper structure needs work. The problem I have with the paper is that it introduces (1) a new problem formulation, (2) a new meta learning model, and (3) uses a geometric embedding space. But none of these are really justified. I would have expected to see a paper that answers: (1) Why is this an ecologically valid problem formulation? (2) why a meta learning approach is the best formulation to tackle the problem? And (3): Why the box embedding space?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 5; rating score: 10; In Figure 1, it took me a minute to figure out that the pictures of the 2048 game were not pictures of a VQ VAE embedding. Would be better to use backgammon or something else less misinterpretable. I think that, as a matter of correctness, my 1st point needs to be addressed for the submission to merit acceptance. I also think that the 3rd point should be addressed, toward making the submission s experiments more reproducible. This work can largely be summarized as plugging the ideas from Ozair 2021 into a MuZero training pipeline. My main comments are on the experimental side:1) Throughout the paper, the submission claims that Stochastic MuZero retains the MuZero’s performance in deterministic settings. Maybe Stochastic MuZero’s network architecture could be tuned such that each simulation is only half as costly? 2) One weakness of the paper is that the results on games with stochasticity are not on active (at least in a relative sense) research benchmarks. This was the approach taken by single agent SPARTA, for which code has been open sourced: https://github.com/facebookresearch/Hanabi_SPARTA. The appendix gives a number of details about the Stochastic MuZero implementation, but few regarding the AlphaZero implementation. 4) A fourth, again smaller point concerns the model learned by Stochastic MuZero. Does Stochastic MuZero end up learning a deterministic model in deterministic environment? How do the shape of Stochastic MuZero s search trees look compared to those of AlphaZero and MuZero?<|endoftext|>The authors extend mu zero to MDPs with stochasticity by adding after states to the tree and using a VQ VAE model. It’s not essential since it clearly works, but I think it’d be interesting! I suspect it doesn’t completely do this as this is the only explanation I am able to come up for why you need a larger sampling budget for stochastic mu zero? To the former point, why do you need a larger sampling budget for stochastic mu zero in the Go games? This also does not affect my score nor am I asking for additional experiments on this, I just think such an experiment would be interesting. Is the notion of after states strictly needed? Why is it advantageous to explicitly split the deterministic and stochastic components instead of having each node be stochastic? It would be good to include the hyperparam grids that were searched over to get the results in the appendix, or if none was done, to say so. I assume you mean to say that chance outcomes are drawn from sigma but the construction of the sentence makes it read like you are drawing $\sigma_t^k$ from $\sigma_t^k$. I think it would be worthwhile, at least in the appendix, to be clearer about the process by which the chance variables are actually sampled. Given that the VQ VAE model is a key contribution, I really do think clearer explanation of how the “novel” variant of VQ VAE works could be given. I don’t have a substantive solution, I just want the authors to be aware that this section was slightly confusing to read. A very good paper with a couple of areas that need some work in rewriting to make the method clearer.<|endoftext|>This paper aims to extend previous work on value equivalent MBRL, such as MuZero, to stochastic environments. This is important to communicate explicitly to the reader, since it is the primary environmental feature that experiments depend on. To this end authors consider the MuZero algorithm and advocate for learning a stochastic model with a VQ VAE, and they modify MCTS so that it can be used with their stochastic model. ​The authors propose Stochastic MuZero. In contrast to conventional work in MBRL that fit transition models to be consistent with environmental observations, this line of work fits transition models to improve the accuracy / utility of a downstream value / policy. The notable addition is in incorporating afterstates in these functions which allows for incorporating chance outcomes. There is an afterstate dynamics function that predicts a latent after state given a state and action. This related to the empirical methodology. A similar methodology was used in an additional experiment, using the deterministic, perfect information game of Go, and here the question was whether their algorithm s learned stochastic model could match the performance of a learned deterministic model. For instance, Figures 2, 3, and 4 all seem to represent a single trial. Some attempt was made to characterize the variability of results in Section 5.4. Each experiment was repeated under unknown random conditions three times for a small fraction of the data shown in figures 2, 3, and 4. There are two issues with this methodology:1. 2.The trials should be run with the same amount of data as the reported results.<|endoftext|>This decomposes the modelling of the stochastic environment dynamics into a deterministic model s_{t+1}, r_{t+1}   M(as_t, c_t) and modelling chance outcomes p(c_t | as_t). The chance outcomes are modeled as a discrete categorical variable (1 of M) and learned using a VQ VAE like setup. The paper shows how the proposed model achieves ~SOTA on two stochastic environments: 2048 and backgammon, and retains SOTA performance on a single non stochastic environment, Go, although, using twice the computational budget. Strengths:    This paper represents a significant contribution to reinforcement learning, by extending the state of the art MuZero to stochastic or partially observed environments. Expectimax is known to be optimal, but computationally infeasible in many applications. What about environments with continuous stochasticity? Also, how large are the random outcome spaces of the environments tried, and how well does the one hot chance outcomes scale with larger random outcome spaces? How well does it scale to partially observed environments, like e.g.poker?The paper proposes an extension of MuZero to stochastic environments, and achieves ~SOTA results on 2 stochastic environments, 2048 and backgammon, while retaining SOTA performance on a hard non stochastic environment, Go. The proposed method is a significant step towards a strong, universally applicable, reinforcement learning algorithm.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper describes a novel way to do bayesian inference with deep learning models by employing a meta learning approach. This is done by creating a synthetic prior over datasets, training a deep learning model on samples from this prior, which results in an approximation of the posterior predictive distribution over the datasets. I think the approach is very novel and the results surprisingly good – especially for the tabular data with BNN priors. This method has a lot of potential for bridging the gap between deep learning models and small data regimes, all the while adopting a Bayesian perspective. It also opens the door to very interesting future research, for instance on the question of design of priors for a particular problem. The experiments are convincing and quantitative. If it is the case, that is very impressive, and I wonder why does it work so well?<|endoftext|>In this paper the authors show a simple architecture (simple on top of transformer pieces) that can learn to perform supervised learning on new datasets given collections of existing datasets. The model is trained on a collection of supervised learning datasets, and, at test time a novel training dataset and the corresponding test inputs are given and a single forward pass through the network produces the predictive distributions for those test inputs. I found the results of this paper pretty impressive, since it shows good performance across many different modalities of data and it is able to generalize well to new datasets. However, I find the main claim of the paper, that "Bayesian inference" is happening, unsubstantiated. This proves that the Bayesian posterior is being recovered _for this particular case_. The results would be very similar to those in Fig 3. Although I understand that that time can be amortized, it would be good to have a sense for it in each of the experiments. If so, it s not a unit "cube". Probably mainly useful for fast inference in small datasets.<|endoftext|>This paper presents the posterior inference framework named Prior Data Fitted Networks. The theoretical background is to approximate the posterior predictive distribution (PPD) with a proposal distribution and optimize the KL divergence between the proposal and the PPD. The proposed framework is clear and sound, and it works well for some well known models (e.g., GP and BNN). In other works, for more stochastic case ($|D|$ is far smaller than $\mathcal{D}$ or a much larger model with more number of parameters), how about the performance of the proposed method? In the experiment, only small datasets are discussed. 2.In this paper, the authors emphasize that the transformer encoder can be adapted to fulfill the suggested PPD approximation. However, I didn t see the necessity or justification to use transformer as the tool for PPD approximation. In summary, the paper is clearly written and proposes an interesting method for approximation of posterior predictive distribution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper mentions that "While none of such items are individually novel, exploiting their ensemble effect in this task is indeed a significant novel contribution." Moreover, the results do not show any significant improvement over those of the existing methods.<|endoftext|>The novelty of the paper is weak. The organization and writing are poor. The validation needs improvement. In the experiments, more state of the art methods should be compared.<|endoftext|>The main strength of the paper is represented by the results that it is able to obtain on standard datasets and how it compares to the state of the art. On the downside, the main weakness of the paper is that none of the individual contributions is individually novel, e.g.:  casting the low light enhancement problem as an unpaired image translation problem  using geometric and illumination consistency  using a contextual loss for semantic similarity  using multiscale color, texture and edge discriminatorsThe paper can be therefore seen mainly as an incremental step in the field, and therefore not significant enough to permit its acceptance in this venue.<|endoftext|>They also separate their discriminators for color, texture and edges. It also improves object detection performance somewhat, but it was not compared with previous works. They fail to convince the effectiveness of color, texture and edge discriminators, as well as geometric and lighting consistency. The proposed model seems to lead some performance improvement in some benchmarks. Also, it is hard to get a clear idea on their methodology in detail, as the paper is poorly written.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Authors then run experiments outside of the NTK regime, on CIFAR 10 and CIFAR 100, to compare the robustness and calibration performance of square loss with cross entropy. Most realistic experiments in this paper is on CIFAR 10/100, however I am not sure if the experiments included in the submission are convincing. In general results using white box adversarial attacks can be hard to interpret, as the performance of white box attacks depend heavily on implementation details and attack hyperparameters. For the purposes of this submission, adversarial attacks are further concerning since the hyperparameters of these attacks were designed and optimized for cross entropy loss, but the authors apply them to square loss here without re optimizing them. I acknowledge that AutoAttack contains black box attacks, such as SquareAttack, which may not suffer as much from this problem. However, it is not clear to me how much of the improvement of square loss on AutoAttack robustness is due to its improvement of SquareAttack. For robustness evaluation, I would recommend a simpler black box noise such as Gaussian noise on the input with several noise scales. To evaluate the significance of this improvement, I would suggest including comparisons to ECE measures reported by published papers for the same model. Theoretical analysis is limited to toy models. Empirical studies should be improved demonstrate that square loss leads to more robust and calibrated models.<|endoftext|>This paper considers the non parametric convergence rate of neural network (NTK regime) under tsybakov s noise condition (low noise with large margin) and discussed the potential application of the theoretical result on the robustness of neural network and calibration results. Results of model calibration are impressive. The paper don t answer the question of how l2 loss and CE loss is different. Is it tight? It s slower than the convergence rate of the Excess risk? The paper gives positive results for L2 losses,however, the paper didn t show any results that the cross entropy loss is not good. The convergence of CE loss can be slow in the terminal phase of deep learning training [6] (O(1/log t)), but if you train longer, maybe all the properties you proved can appear. **Adversarial robustness** The gain of SL interms of the adversarial loss is actually not significant, I would to suggest the author to try more norms(l2 norm robustness as example) can be tried in this setting. In the experiment I can t see what s the norm the author is using for PGD. For l2 robustness, you can further try randomized smoothing [5]. [1] Nitanda A, Suzuki T. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime[J]. [3] Schmidt Hieber J. Nonparametric regression using deep neural networks with ReLU activation function[J].<|endoftext|>The paper studies wide neural networks trained with the square loss in the neural tangent kernel regime from a theoretical perspective. In particular, it provides generalization error bounds, robustness and calibration results and makes a connection to supervised contrastive learning. From my understanding, this not an issue. Misc.Proposition 4.2 has already been proven in [3] and in a more general case in [4]. **Detailed comments:**  Presentation of the results:    Crucial information is missing from the main text and can only be found in the supplementary material, most importantly, the assumptions under which the theorems hold. That being said, the paper is difficult to navigate, as crucial information lies in the appendix and the experiments do, from my perspective, not really corroborate the theoretical contribution. For example, Theorem 3.1 is a asymptotic equality for some function $\hat f$, which is not specified in the main text. It would also improve the readability, if the definitions of $\Omega_1$ and $\Omega_2$ are restated in the theorem or in its proximity. From my perspective, a comprehensive on the extent to which the theoretical predictions hold would have been more valuable.<|endoftext|>This paper provides a theoretical analysis of the squared loss for classification where cross entropy loss is often the standard choice in both theory and practice. The authors show under some assumption of the data and the NTK regime, GD on squared loss enjoys fast convergence, strong robustness and good calibration. + The results hold for a non separable case using the Tsybakov noise condition. + Additional insights on the robustness and model calibration. Cons:  The analysis under the NTK regime is somewhat standard and may not be practical. For example, when mu 0, GD operates under the kernel regime, which do not explain some phenomenon observed in practice such implicit bias. There are a number of restrictive assumptions, for example, C.1, C.2 and C.3. When is the definition of separability (when \eta only takes either 0 or 1 in two separate sets) more general than the linear separable case? 2.How does this result compare to other results of implicit biases of gradient descent for separable and non separable data? Does the analysis carry over if \mu is exactly zero? The objective in (2.2) gives the impression that A is optimized, but it is not. The analysis would be very different with a joint optimization over both W and A. 3.The color bar in Figure 1: how probability P[y 1] can be negative? I don’t see any color difference in each of the plots.<|endoftext|>This paper provides a theoretical guarantee for square loss in the neural tangent kernel (NTK) regime. Whether classes are separable or not, the convergence rate is proved to be improved. The theoretical analysis of this paper is rich, including generalization and robustness theory. When introducing the theoretical results, we should make a detailed comparison with the existing cross entropy loss results. The current writing method cannot reflect the advantages of square loss. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model? In this regard, there is a lack of references to some relevant literature. Based on this, [Lyu, 2018] designed a square type margin distribution loss to improve the generalization ability of DNN. If the author can provide the theoretical comparison between SL and CL and supplement some missing citations of related work, I will more recognize the integrity of the paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposed a personalized federated learning algorithm which takes into account the similarity of gradient of different users to update the model.<|endoftext|>There is no question that personalized federated learning is an important and practical problem. This paper doesn t provide any theoretical guarantees (and the motivation for the proposed approaches is not always clear). There are a lot of questions about the experiments, which reduce my confidence in the overall conclusions. Does the server update the client parameters $w_i^{t+1}$ and $w_j^{t+1}$? Is this additional memory overhead practical? And again, it does not appear to be possible to perform such similarity weighted updates in a privacy preserving manner.<|endoftext|>This paper proposed two personalized federated learning algorithms: SPFL, PLGA. The paper is overall not well written and I recommend rejection. This requires maintaining a potential large matrix, maintains the identity of each client, and cannot be applied to new clients.<|endoftext|>This paper proposes gradient based weighting strategies for synchronous (SPFL) and asynch (PLGA) personalized Federated Learning (FL). Table 1: Factoring in standard error, it is not clear if there is a clear gain for their algorithm except in CIFAR 10 with non IID setting. Negative results are also good, as long as the experiments and methods are well thought out. Eqn (3): Could enhance the similarity metric S(i,j) by a weighting hyperparamter \gamma. Although the main idea seems promising, there needs to be a lot work done on presentation and evaluation.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; In the linear settings, they learn the model invariance states by directly applying ICP and prove that an optimal policy can be learned over them. In the nonlinear settings, they instead propose a practical method to approximately learn such model invariance states. On the experimental side, the baselines are not convincing enough to judge the validity of the proposed approach. This is misunderstanding about ICP, because the representation is allowed to vary across environments. I feel that the theoretical results presented in the paper are not quite new, all of which are immediate results of or simple extension to previous work. However, this is not the case in the nonlinear settings.<|endoftext|>This work presents a method for learning representations for model based reinforcement learning by leveraging assumed sparsity in the causal graph to improve the generalization performance of the learned model in the low data regime. Furthermore, they provide a bound on difference between the optimal value function on the learned representation and the true optimal value function as a function of the approximation quality of the invariant model. Overall, I found that this paper has a good, theoretically motivated derivation to improve low data generalization in MBRL, which is a clear bottleneck. The paper presents in interesting, useful idea that causality plays in important role in generalization performance in model based reinforcement learning, and proposes an interesting method to leverage this. While in many cases, it is useful to derive theory in a linear case to inform practical algorithms for the nonlinear case, but in this work, the practical approach put forth in the paper seems disconnected from the linear case.<|endoftext|>Inspired by theoretical results, this paper also proposes a novel method for learning model invariant representations using neural networks. The paper is well written and easy to follow. Empirically, this paper shows that in continuous control tasks, their model learning algorithm improves the performance of MBRL algorithms in the low data regime. This paper is well motivated, the empirical results are impressive.<|endoftext|>This paper points out that the causal structure is important for the generalization ability of a dynamics prediction model, and empirically show that learning a model invariance representation can implicitly learn the causal sparsity and improve the generalization ability of the learned model. In general, I think that the paper is well written and I like the core idea of this paper because it points out an important direction to improve the data efficiency and generalization ability of the dynamics model. The paper is also well written, but the analysis and the experimental support are not strong enough, so I give 5 in this procedure, but I am willing to increase my score if the authors fix them in the rebuttal process.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; The paper proposes a new training algorithm to defend against membership inference attacks (MIA) in machine learning models. Motivated by the connection between MIA success and difference between training and test loss distributions, the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it.<|endoftext|>The study tackles the problem of defense against membership inference attack, with a focus on (1) decreasing the performance of the attack, (2) maintaining the classifier’s performance, (3) assuming the blindness towards the attack model. By discarding the learned information of the network about the non ground truth classes and manually replacing the values, my intuition is that there is a good chance we are overfitting (i.e., we can t expect good performance on the test set). The poor testing results in Table 1 for purchase100 data, which has the largest number of training samples, might be hinting at this overfitting. Figure 1 c, the (E,Val) pairs should be linked to the corresponding distribution. The approach is clear, and the extensive experiments performed on variety of datasets against reliable baselines proves the merit of the proposed defense algorithm.<|endoftext|>To defend the common membership inference attacks, this paper proposes a relaxed loss with a more achievable learning target to reduce the attack accuracy and also help with the generalization gap of learning models. Extensive evaluations on five datasets with diverse modalities show that the proposed method can achieve higher membership inference protection. Also the gradient descent and gradient ascent are used alternatively to balance the member and non member loss distributions. The authors give an analysis for "RelaxLoss increases the variance of the training loss distribution" in appendix A.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper presents a new self supervised learning framework for audio visual speech. “When only modality is used”   > "only one modality". Especially, with a small amount of labeled training data (30 hours), their method achieved comparable results with a previous state of the art method which trained with a huge amount of labeled data (31,000 hours). The authors propose a powerful method to learn speech representations from multi modal data which can be utilized for both ASR and VSR, in a self supervised manner. “we rely on the joint decoder module”. What does the joint decoder mean? ‘iterative pe training’  > ‘iterative pre training’6. Even the proposed AV HuBERT shows impressive performances, but it is an expansion of HuBERT to work in multi modal data. [R1] Chen, Yen Chun, et al."Uniter: Universal image text representation learning." Springer, Cham, 2020. 2.For the loss function in equation (4), their final performance seems obtained by setting the alpha as 0. Have the authors re examined hyper parameters (m_a, m_v, p_m, p_a, alpha) for ASR task?<|endoftext|>This paper presents a multi modal (audio visual) pre training approach that results in models that can be fine tuned to achieve state of the art performance on both lip reading (visual only) as well as speech recognition (audio only) on the LRS3 dataset which is the largest public lip reading benchmark. Very well written paper, presenting an effective approach for learning embeddings for multi modal data. Pros:a) Simple but very effective extension of masked self supervised training to multi modal tasksb) State of the art lip reading and ASR results on the LRS3 benchmarkc) Demonstrates value of providing multi modal input to the model even when only a single modality is present at test time.<|endoftext|>The paper proposes a strategy for self supervised pre training for lip reading. The authors perform experiments on both limited and full resource settings, on both of which they demonstrate strong performance. The authors take good advantage of a recent advancement of pre training in ASR (HuBERT), and applies it to lip reading, aka visual speech recognition. The visual only HuBERT is a simple adaptation, but the cross modal iterative refinement is a useful addition. It would be good to also show the effectiveness of the embeddings for other downstream tasks, such as audio visual speech recognition and speech separation. As an application paper, the paper might be better suited for CVPR or Interspeech, but I still vote for acceptance since the contributions are significant and the results are state of the art.<|endoftext|>The authors further propose the modality dropout and masking by substitution module to further fine tune the design. Extensive experiments and ablation studies are carried out on the LRS3 dataset, which validate the effectiveness of the proposed method. ## Strength:++ This paper is basically well written with detailed supplementary. ++ The idea of leveraging Bert for self supervised training is different from the traditional audio visual alignment pertaining. ++ Detailed improvements such as the audio visual stream formulation and Masking by substitution have been proposed. ## Weakness:  This kind of formulation, though novel on the topic of audio visual speech recognition, has been used on other topics. Why use only the LRS3 dataset for evaluation? While I understand the limitation in resources, the authors are suggested to show the multilingual results under the full setting. Overall this paper introduces techniques that the field of audio visual speech recognition has rarely leveraged into this field, and makes task specific modifications. The idea is not thoroughly new but the application is good.
Reject; rating score: 3; rating score: 3; rating score: 3; The authors propose a probabilistic framework to improve the classification accuracy in instances when there exists missing data in the multi modality datasets (where one of the modalities is the predictive label; however, this label is not assumed missing). However, there are a few concerns about the approaches and the evaluations in this paper:* The most significant concern is about the baseline comparison. * In Fig.4 (c) and (d), the classification accuracy of the “happiness” emotion by ZP is higher than that by the proposed method.<|endoftext|>Two emotion recognition datasets are used in experiments to make comparison with several baseline methods. At the same time, the comparative methods in this submission are less persuasive. There is a mismatch between the title and the content, given that the proposed methods are only verified in one application scenario, i.e., emotion recognition. The authors may consider to extend the methods in order to match the title or change the title to a specific area.<|endoftext|>This paper deals with multimodal learning with missing modality in training. Given this lack of comparison, I believe that the claim of authors “……which lead to the information of the modality missing data not being well exploited to” was not justified as well. It is also important to mention that the proposed method was tested only for categorical emotion recognition, while emotion datasets are typically multi labeled (as humans cannot elicit only one emotion at a time) and also include continuous values therefore, regression task might be targeted too 	I also found the used datasets limited in terms of their size. There are also lack of information regarding how the data is being processed. c)	“On each processed dataset, we split all data into three parts: training set, validation set, and test set.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The goal of this paper is to improve the generalization ability of the few shot model on novel domains. To this end, this work introduces a learning manner named domain switch learning (DSL). [ ] Summarization of the datasets (domains) used in the experiment could be included in the appendix. **Weaknesses**   **[Working mechanism needs more discussion]** 	The proposed Domain Switch Learning (DSL) includes only a single domain into a training iteration and switches to another domain for the following iteration. **Second**, ***the evaluation is well clarified. *** ***One suggestion*** is the main paper should clearly mention the experiments in the appendix. In addition, I notice that the experimental results on the ***newly introduced test sets (Aircraft and Traffic Sign) also show the effectiveness of DSL***. This also makes the model favors domain general knowledge. ***It would be better*** if the authors could add them in the revised paper (e.g., appendix), which helps understand the effectiveness of DSL. ***Second***, In Sec.3.2 the work includes a sentence to eliminate the concern of the contradiction when using mini ImageNet images at each iteration. In order to learn well from these images, the model tends to learn common information. The major concern is the working mechanism of DSL is not well explained. *** Post Rebuttal *** After reading the response and revised paper, my major concerns are well addressed by the authors.<|endoftext|>The authors propose to improve domain generalization capability by training with a fast switching manner. Strength:* the high level intuition of incorporating adaptability into the training scheme is heuristic to the research community* the proposed method is applicable to the real life scenario of few shot domain shift* extensive experiments and ablation studies are conducted to demonstrate the performance of the proposed method* the paper is very well written, easy to understand, well organized, and supported with nicely drawn figures Weakness:* since the proposed method strictly requires multiple source domain, it is confusing how single domain baselines are relevant here. (it is included in all the experiment tables)* the authors should consider adding more discussion on how the proposed method of domain switch is better at extracting general information than domain mix methods. Since in the domain mix setting, different domains are given, it can be easier for the model to learn the general information shared by these domains, which is not possible in the domain switch setting. * comparison with GNN would be helpfulbased on the above discussion, I recommend borderline accept for this paper.<|endoftext|>This paper proposes a novel training scheme termed Domain Switch Learning (DSL) to pursue the few shot learning problem under the cross domain scenario. In practice, DSL uses the data from one domain for a training iteration and switches to another domain for the next iteration. During the switching, two regularization terms are also introduced to balance the domain specific knowledge and domain general knowledge. The extensive experimental result demonstrates the effectiveness of the proposed method on several standard benchmarks. Strengths+ The paper is well written and easy to follow. Compared with TPN+ATA, the strength of the proposed method would be limited, especially for the Places dataset. Could the authors give some explanation and analysis about that? Actually, the Tabel3 made me confused about the experimental results. How to ensure the fairness of comparison？ （2）In Table3, I can t align the performance of the domain mix scheme with other methods. As shown in Table3, even though the accuracy of domain mix was lower than domain switch, I am still curious about the performance of domain mix+teacher regularization. For the last one, I am still concerned if the performance gain is provided by the switch scheme or just from the carefully designed BKLD loss and RCE loss.<|endoftext|>This paper aims to address the problem of cross domain few shot classification. To this end, it uses multiple domains during training – it only trains on a single domain at each iteration and switches the domain in consecutive training iterations. In addition, two constraints are proposed to further improve the generalization. First, a domain specific prompter module is introduced, so that the model is not overfitting to the domain in the current iteration. Second, a domain general teacher module is introduced, so that the model does not forget the already learned knowledge of other domains. Paper Strengths:The authors tackle an important and challenging problem of cross domain few shot classification. The proposed approach is simple. Experimental evaluations clearly demonstrate the effect by introducing the domain switching training strategy. I was wondering how does the proposed domain switch strategy work on more diverse, heterogenous datasets, like Meta dataset [Meta Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, ICLR 2020]. The current statements sound like they are already proved facts. How is the performance if the domain ordering is completely random in the entire training procedure? In that case, probably the design of the prompter and teacher should be modified as well. 6) Domain mix and domain switch can be viewed as two extremes, where domain mix uses all the domains at each iteration, while domain switch uses only one domain at each iteration. For example, at each iteration, some domains are randomly combined as a compound domain, and there is no overlapping domain between two consecutive training iterations.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; This paper introduces a novel variance regularization to reduce the statistical variance of the  variational based estimators for f divergences and Renyi s divergence. The proposed regularization is based on the well known delta method which provides the asymptotic variance and  depends on the specific from of the variational bound under consideration. Although the main idea of exploiting the asymptotic expression of the variance (based on the delta method) to introduce a penalty term that controls the variance is not surprising, the application within the framework of estimation of diverges based on variational bounds and the effectiveness of the method seems to be interesting. ** weaknesses **  The main misconception of this work relies on the fact that the synthetic experiments provided in Section 4 are too simplistic to be able to validate the proposed method. More precisely,  (i) the regimes under which the estimators are investigated are rather optimistic since a large amount of samples are considered (e.g.512K); (ii) there is no experiments on multimodal (mixtures) distributions or other distributions than Gaussian distributions (e.g., there are many possible examples where the underlying information measure does not change but the the distribution of the samples can be very much different by introducing complex transformations); (iii)  there is no studies analysing the effects of the batch size, the effects of the underlying optimisers used to obtain the variational  lower bounds (e.g., what about the use of batch normalisation?; (iv) there is no investigation of the varice reduction under realistic high dimensional datasets (e.g.SVHN, imagenet, etc.) For example, CLUB has been improved in [A] and see references in therein for other missing methods and possible additional non Gaussian scenarios to study. Indeed, this estimator is also applied to the problem of disentanglement and thus, it is relevant to compare with the results obtained on Section 5.2. In particular, comparison with state of the art estimators are clearly missing, the synthetic datasets are too simplistic  to validate the proposed method, the regimes (in terms of samples) are not relevant and many other effects related to the choice and impact of the optimiser, batch size, etc... are missing. I cannot recommend the acceptance of this paper.<|endoftext|>The paper proposes to adopt a variance penalty term to manipulate the divergence estimators. Experiments first investigated the correlations of the variance penalty and the bias and variance trade offs. Then, the authors tested the proposed approaches on two real world application (biological and speech synthesis) datasets. The results show that the proposed approach can effectively reduce the variance as compared to systems without including the variance penalty. Theorem 2 could be inappropriate and insufficient to be considered as a theorem. The authors defer the details in Appendix B., but the proof which the author indicated only proves the divergence property of Renyi divergences instead of other divergences. Also, since $\alpha$ is used by Renyi divergence, the authors should denote the parameter of Adam by another symbol other than $\alpha$. In addition, it is necessary to explain why the authors choose MedAE instead of MSE. (7) The trade off between bias and variance is unsatisfactory. Moreover, to greatly reduce variance, DNE VP exhibits large biases. (8) The second synthetic experiment seems problematic. (10) The authors did not introduce the model architectures and datasets used in the speech synthesis experiments (for both speech synthesizer and automatic speech recognition systems). Without such information, readers cannot reproduce the results easily, and thus the results reported in the paper are not convincing. The proposed idea is reasonable. However, some key information (including notation definitions and experimental setups) is not well described, which may make readers difficult to follow the main ideas and reproduce the results.<|endoftext|>In this paper authors propose to an new neural based divergence estimator using variance penalty. In this paper a highly topical problem is tackled, how to obtain low variance divergence estimator. Idea is quite good and synthetic experiments do seem to validate that reduction works. I have only following points:   About Fig 2, can you comment about why for CLUB VP does not seem to help so much as for DNE? Variance reduction, at least visually, seems to be quite minimal. It appears to be that there were no other variance reduction techniques compared, why not to compare against for example control variates? Good contribution on an important topic.<|endoftext|>This paper takes the variational view of the statistical estimation of statistical divergences. Standard variational formulas suffer from high variance when estimated with finite samples, this paper proposes to address the problem by adding a variance penalty regularisation term to the objective functional, thus trading variance for bias. From this new formulation, the paper derives a new low variance Neural Divergence Estimation Algorithm with the variance regularisation technique. **Strengths:**The paper clearly motivates the problem of high variance in the estimation of the statistical divergences . Despite being heavy on notation, the manuscript is well written and easy to follow, and the proposed solution is clearly laid out. The theoretical results are also a useful addition. The experimental section assesses some of the properties of the proposed estimator sufficiently. **Weaknesses:**One of my concerns is that this paper does not compare to other variance reduction methods. The paper mentions that there are some other methods in the literature that apply techniques such as antithetic sampling to reduce the variance, but I did not see a direct comparison to such methods. How would they fair against the variance penalty method? Can the VP be used to supplement them? If yes, I think this point can be explored in the experiments as a “best of both worlds” approach. In my opinion, this work is original and well presented. The paper discussed the presented method in depth and the experimental evaluation was well constructed. I think, overall, this work is a useful and significant contribution.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This work tackles the problem of active learning. StrengthThe paper is well written. I believe using Wasserstein distance to bound the core set loss is a unique and interesting way of dealing with the active learning problem.<|endoftext|>What is the computational complexity of computing the mentioned Wasserstein distance? The paper is well written and easy to follow. I think overall the approach makes sense, but I didn t find significantly novel contributions, either theoretically or empirically (maybe the authors could point out their novel contributions).<|endoftext|>This paper proposes a (batch mode) active learning method that chooses a subset of data points to be labeled through approximating the whole dataset in terms of Wasserstein distance, which is an upper bound of the generalization error. The selection is formulated as a large scale mixed integer programming, and the authors propose to solve it by the GBD.<|endoftext|>The paper frames active learning as an integer optimization problem, that minimises the distance Wasserstein distance between unlabelled pool of data. This is done in a feature space (in this case trained using self supervised methods all the data). Skeptical that performance would be good, or that the method would scale well. Or did you continue to use the VGG16 features that was used in Sener and Savarese? Would like see some additional results, but this a good paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper proposes to train large Transformers for proving theorems on Lean by using the auxiliary training objective built with proof terms. The main idea of using proof terms for self supervised training is reasonable. The experiment details are thoroughly demonstrated and the results look solid. Overall, this paper advances the techniques of training deep networks for theorem proving by co training on proof terms.<|endoftext|>The system has already contributed theorems to Lean library and has a potential of significant impact on the Lean community in future. PACT significantly improves the theorem proving success rate on held out suite of test theorems. * This work makes the following contributions. I recommend acceptance of the paper. I feel that the conclusions are in general well supported by the results. An ablation study is carried out to rule out the possibility that the benefits from PACT come from simply regularizing the model. * I am wondering why the authors chose Lean for this this work (as opposed to say MetaMath).<|endoftext|>This paper addresses the general setup of using transformer models for interactive theorem proving (ITP) tasks. The ITP engine considered here is Lean. TacticZero by Wu et al 2021 is missing from the related work.<|endoftext|>See e.g.[4 7].[1] Cezary Kaliszyk, Josef Urban:Learning assisted theorem proving with millions of lemmas. %%%%%%%%%%%%%%%%%%%The work is an important practical step for Lean, but there are major evaluation and presentation issues. The major issues with this work remain to be: 1. This has been to various extent used in workssuch as [1,2,3], where the benefit of learning from additional prooftasks was also demonstrated.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes a simple yet effective multi agent offline reinforcement learning algorithm, called Offline Multi Agent RL with Actor Rectification, which combines CQL (first order optimization) and evolutionary algorithms (zero order optimization) to enhance the applicability of the CQL algorithm on multi agent tasks. The paper is clear and easy to understand, and the approach is simple and effective. So it is not surprising that MA CQL performance is poor. The authors claim that this is due to the local optimum, but there is not sufficient experimental arguments to illustrate that. So I think a key ablation experiment to illustrate the contribution of zero order optimization to offline reinforcement learning is performing zero order optimization based on single agent CQL. 3.There are some details about the methodology that are worth discussing. 3.2 The authors argue that solely regularizing the critic is insufficient for multiple agents to learn good policies for coordination in the offline setting, but in fact, the authors do not analyze coorparation explitily. So, I consider the paper to be at the level of borderline for now, and I recommend marginally below the acceptance threshold (5).<|endoftext|>This work considers extending conservatism based algorithms to offline RL with multi agents. The paper is overall clearly written. The intuition is discussed and the main idea is easy to follow with enough explanations. It is a bit surprising to see that such a simple combination of first order and zeroth order methods is able to deliver a good performance. However, I do not see how "safe" the policy is. It s fine that the theorem is not strong since I consider this as primarily a practical paper, but some explanation is needed to avoid over claiming the results..(2) Practical: I think the empirical evaluation is not adequate. In particular, in the beginning, it is motivated that standard methods fail when there are more and more agents.<|endoftext|>This paper considers the offline multi agent RL setting, first demonstrating that optimization is more likely to find bad local optima than in the single agent case. Zeroth order optimization has been considered in the RL literature, so its application is not hugely novel in this context. The experiments are pretty comprehensive although admittedly of very limited scale. What stops me from giving a higher score is mostly the somewhat disconnected nature of the proposed contribution to the theory of MARL, which is at this point mostly based on intuitive/empirical findings.<|endoftext|>This work looks at the problem of training multi agent reinforcement learning with continuous action space in an offline setting. The authors proposed a simple but seemingly effective method for improving MARL with offline datasets. The paper is overall well written and easy to follow. However, if the local Q value gradient direction is opposite compared to the selected sampled action, would the actor policy get stuck in some suboptimal actions (e.g.in the middle of the two high Q value points). 3.Does this value function saddle issue happens only in the offline setting? Does the low optimal issue go away when you have more data points?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper studies the "Benign Overfitting" phenomena in the setting of linear classification of sub gaussian mixtures. The latter two papers also worked on overparameterized models under d/n   \Theta(1) regime, which is slightly different from this paper (their results are stronger in certain aspects and weaker in some other aspects). I think a detailed comparison with these works is very necessary and may help readers understand the contribution and novelty better. However, the analysis in this work (see theorem 4.4) requires d>  C*n^2 log(n), which is quite heavily overparameterized. (2) There doesn t appear to be much clear take away from the theoretical results. Besides, some of the behaviors that appeared in this work contradict empirical evidence for deep neural networks. For example, one of the main messages conveyed by this paper is that overfitting is benign in adversarial training.<|endoftext|>Previously, Chatterji & Long (2020) had studied the non adversarial classification case, so the main new result is an extension to the adversarial case. The main claim of the paper is that benign overfitting occurs in the adversarial case as well that is, even though the training data are overfit perfectly, one can still attain reasonable generalization error. However, in practice it is well documented that overfitting is indeed worse for adversarially trained models, so I think an adequate analysis in this case would explain this phenomenon. The paper does not include a discussion of what new technical contributions or proof techniques it contributes, so it was difficult to assess this aspect. I did not feel the paper made significant empirical or explanatory contributions, and was unable to assess the theoretical contributions.<|endoftext|>Surprisingly, the authors report that the benign overfitting phenomenon can be observed even in the case where we use adversarial training. The authors  approach is reasonable and should serve as a reference for future research. The result just implies that the upper bound under adversarial risk matches that under standard risk, but this does not imply the "benign overfitting" because there is a possibility that the upper bounds are not tight. This paper studies a very interesting setting. However, I could not understand the extent to which the authors  claim of "benign overfitting" is supported.<|endoftext|>This paper studies the clean test and adversarial test error obtained using   _Gradient Descent Adversarial Training_ (GDAT). However, benign overfitting still exists though a larger $\epsilon$ for the adversarial training hurts clean error. The result shows that adversarial error is certainly worse off than clean test error. StrengthThe behaviour of overparameterised models in the presence of noise when trained with adversarial training is certainly a very important problem as both of these components(overparameterisation and noise) are important components in the ML context. al.2020, which raises questions about the technical novelty of the paper. al.2020.The paper also deals with a simple linear model and it is not immediately clear how it relates to overparameterised neural networks. While it is important to first understand a complex phenomenon theoretically on a linear model, in the absence of  a huge amount of novelty in the theory, I find this second drawback rather glaring.
Reject; rating score: 3; rating score: 3; rating score: 6; The paper proposes a neural network based algorithm for edge rewiring. While rewiring an edge sounds straightforward on a graph, it is tremendously difficult in a physical network, e.g.adding a new road or a new power line. Furthermore, the functionality of a physical network is beyond nodes and links. The problem is not very motivated.<|endoftext|>The paper proposes a neural approach that increases network resilience by edge wiring. The novelty of the proposed method is unclear and the experiments need major improvement. + The paper is easy to follow. The paper could be improved by focusing on specific challenges (such as scalability) that the rewiring problem has while working with this framework. "Reinforcement learning for combinatorial optimization: A survey."<|endoftext|>My main concern is the empirical performance of the proposed framework. after response The updated results in the revision resolve some of my main concerns. Questions and suggestions:1. This paper proposes a new RL based framework together with a new GNN architecture for improving network resilience.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes DMAC, a general off policy actor critic framework with entropy based divergence regularization for multi agent reinforcement learning to better coordinate multiple agents in a complicated environment. Strengths:Theoretical derivations have been included in the analysis of the algorithm in addition to the experiments. Weaknesses:The motivation for the target policy rho is not well explained. Neither theoretical or empirical evaluations seem to address these concerns. The update rule is also standard in RL literature, and it would be ideal if the authors can further emphasize how their method differs from existing benchmarks.<|endoftext|>This paper suggests a DMAC learning framework by introducing a divergence penalty between the learning policy between a target policy into the RL framework instead of the commonly used entropy penalty. 3.I really appreciate seeing that the proposed methods can be really combined with multiple algorithms and gain improvement naturally. The authors state in the introduction section that "DAPO cannot be trivially extended to cooperative MARL settings". 2.Experiments are not sufficiently convincing. (2) The paper utilizes COMA as its on policy baseline. What is the motivation for this choice?<|endoftext|>For cooperative multi agent reinforcement learning, they propose an off policy(?) learning method that is reqularized by the KL divergence to some target policy, which, in contrast to classic max entropy RL, allows to circumvent(?) The paper is strong in presenting both theory (I took a brief look at the proofs, the authors seem to know what they are doing, but I did not go through details) and experiments. What I m not so sure about is how deep the contribution is compared to previous work, in particular Wang et al.2019, or, for instance, (Haarnoja et al., 2018), which is cited in the proof of Lem 2.<|endoftext|>Specifically, the main contributions include the developments of the divergence policy iteration for general cooperative MARL settings and the off policy divergence regularized multiagent actor critic framework (DMAC). Empirical results show that DMAC can improve the performance of existing MARL frameworks in various multiagent domains. The paper is well written, organized, and explains the main insights well. As pointed in the paper, the theoretical study in Section 4.2 is assuming the fixed target policy. 3.What are the possible future directions of DMAC? Initially, I vote for a score of 6. After reading the authors  responses to my questions, I am open to raising my score.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper proposes a new attack strategy for adversarial examples in the case where transductive defense strategies are employed. A new strategy for adversarial training is also proposed yielding more robust models in the transductive setting than existing defenses. Strengths:  I appreciate the formalism of the adversarial game for transductive learning introduced by the paper, which also seems novel. The paper covers a wide range of topics: formalizing the transductive attacks and defenses, breaking transductive defenses thought to be strong, experimenting with domain adaptation as defense against the attack proposed in the paper. The experimental section seems to follow standard practices for robustness evaluation. The paper is well written. Weaknesses:  The topic of the computation cost of the proposed attack is not discussed. The proposed method seems to be a variation of PGD (limited novelty), but considering that the paper has other contributions and GMSA works well in practice, this work is arguably worth publishing. Other comments / questions:  The paper could benefit from additional proofreading. The transferred adversarial samples produced with PGD and AutoAttack might be weaker than some generated with the proposed attack, thus not resulting in a proper evaluation.<|endoftext|>This paper proposes adaptive attacks against transductive robust learning methods. These methods aim to improve the robustness of models to adversarial examples by updating the model using unlabeled test data. However, this paper notes that previous evaluation of transductive robust learning has not considered attacks that are truly adaptive, i.e.the attacker should be able to utilize its knowledge of the transductive learning algorithm to craft better examples. The experimental results on two existing training methods and two newly proposed ones, one of which is a strawman, demonstrate that the proposed adaptive attacks render transductive robust training to be no more robust than standard robust training. It clearly articulates issues with the previous evaluation of transductive learning algorithms for learning and lays out the threat model that should be used in the future when evaluating these algorithms. It also delves into the various aspects of the multi round game that results from an attack on transductive robust learning. The empirical study is fairly thorough, tackling 4 different types of robust learning algorithms across two standard datasets, using the two different attacks proposed in the paper. Specifically, when the transductive model is run for T iterations, is the robustness reported using the best adversarial example set over all iterations, but evaluated with respect to the final model with all test data? In fact, in a number of cases the simple FPA algorithm had the best performance. I urge the authors to consider improved methods to circumvent the bilevel optimization problem. While I still believe more innovation is possible for the attack, I have decided to raise my score to a 6 since the paper is likely to lead to interesting follow up work.<|endoftext|>This paper intends to break a specific type of adversarial defenses. These defenses choose to dynamically modify the model for the purpose of robustness and defenses. My biggest concern is about the importance of the studied topic in this paper. Specifically, the so called transductive defenses are not very popular and have not been fully recognized by the community so far. In fact, I happen to have read the paper of RMC when it was published in ICML and thought that the authors of RMC should have tested some adaptive defense aware attacks in their work, which they did not. Meanwhile, I also served as a reviewer for Dent, which was rejected at the time and has not been accepted by other conferences according to my knowledge. Thus, to me, it seems that the proposed attack GMSA origins from the obfuscated gradient work and only breaks a few defenses that are somewhat not canonical. I may change my score after I exchange my opinions with other reviewers.<|endoftext|>This paper study the adversarial robustness in transductive learning, where the defenders can update the models during test time by interacting with the adversary in multiple rounds. The result show that the proposed attack can decrease robust accuracy more than other baseline (the static AA, DENT AA). The writing needs to be improved. 2.The are some questions/places not clear and require clarifications from the authors, see below. ### QuestionsQ1: How practical is the problem setting of adversarial robustness in transductive learning? I assume it works as follows:adversary generate a batch of adv examples (adv_0) based on model_0 (original model)  > defender get adv_0 and update the model to model_1  > adversary generate a new batch of adv example (adv_1) based on model_1  > defender get adv_1 and update the model_1 to model_2  > repeat to the end  Are my above understanding correct? In this case, would the proposed method work (it seems that the proposed method is a white box attack setting)? What happens if the adversary also know about the randomness in $\Gamma$? The attacker should be able to develop a stronger attack? Can the authors show some experiments for this? Then why AA (RMC) does not work? In summary, this paper propose a simple yet effective GSMA attack to evaluate adversarial robustness of models in the transductive learning setting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The approach of the algorithm makes sense and seems potentially promising, and targets an important set of problems. The experiments are restricted to synthetic data of a limited scale, and to perhaps the most basic sparse recovery setting (linear regression, while the introduction makes the point that the method is more broadly applicable). While I appreciate the theoretical analysis and its validation on synthetic data, the utility of learning based algorithms hinges to a significant extent on whether they can be substantiated on real data, where the correspondence between the training and test data is not as structured and clear as in the synthetic case. There seem to be parts of related literature that are not mentioned. These include some additional learning based algorithms for latent parameter recovery (e.g., [a,b], though they move away from sparsity), and generalization bounds for learning based algorithms [c,d]. Pending some concerns raised by other reviews, which would be resolved in the internal discussion phase, from my point of view the paper can be accepted.<|endoftext|>The algorithm is based on a deep unrolling of a non convex optimization procedure from a prior work (Wang et al, 2014) in the literature. The paper contains some theory (for capacity and generalization) and experiments, establishing the efficacy of the algorithm. The paper aims at an important problem and provides a novel solution. I believe the work could have future impact. I have a few concerns regarding the theoretical claim and experiment. The author(s) should be careful about the scope of this claim. The paper applies this framework to the specific setting of learning based sparse recovery. The claim is that "there exists a set of parameters θ .... such that the estimation error is small". 3.In Section 4 on generalization, the author(s) should clarify the distributional assumptions here. 4.The paper does not compare PLISA with other classic algorithms in the sparse recovery literature, for example, convex relaxation or linear sketching methods. It would to nice if the experiments are more comprehensive. There has been a few works on unrolling or differentiable learning of classic algorithms. This is very closely related to the nature of this work.<|endoftext|>It is not a good assumption that there exists a supervised dataset of sparse recovery instances available for tuning the parameters of the proposed algorithm. The fact that all experiments in this paper are based on synthetic datasets and no real datasets are used in this paper confirms my concern. So even after fully training the parameters, just evaluating the algorithm on an input instance is several times slower than the original path following algorithm. The complexity is even much worse during the training phase because one needs to evaluate the whole algorithm multiple times and compute its derivatives with respect to the learnable parameters. The paper does not analyze the runtime complexity, unfortunately. In particular, the parameters need to be found by solving some optimization problem but it is not even clear if this optimization problem is convex or not. It needs to be defined more specifically. Score after revision  The revised version looks better now and seems to address my concerns.<|endoftext|>The paper presents a deep unrolling method that unrolls the path following algorithm of Wang et al.(2014) for learning to regularized sparsity estimation. Theoretical guarantees on sparsity recovery and generalization performances are provided. A simulation study is carried out to verify the theory and effectiveness of the proposed algorithm. This result seems not surprising at all as PLISA by definition naturally mimics an existing iterative sparse recovery method with similar guarantees. Continuing the above comment, the generalization analysis is also of limited interest and novelty. First of all, since the generalization is about the hyper parameters involved in meta optimization, most existing uniform convergence analysis techniques in classic learning theory are expected to work for bounding the generalization gap. Last but not least, I am seriously doubting the practical usage of the proposed approach to real data sparse learning problems because it is only designed for well defined sparsity models. The algorithm is somewhat interesting but the theory is of limited relevance and novelty. The implementation details and practical usage of algorithm are largely unclear in the current stage.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper considers the out of distribution (OOD) problem. cons1.The technical novelty is limited since bilevel optimization framework has been widely used in machine learning. 2.Precisely, the formulation of the proposed method is not bilevel optimization framework but triple level like optimization framework since in the outer loop it is a min max optimization problem. 3.This paper lacks theoretical supports including the perspectives of the generalization and optimization convergence. As mentioned in 2, the optimization problem is different from ordinary bilevel optimization problem.<|endoftext|>The paper proposes a new method called BLOOD for the problem of domain generalization. Major concerns:   This paper is primarily empirical. While the results on the three datasets included in the experiments look promising, comparison on other datasets can strengthen the empirical efficacy of the proposed method BLOOD. Alternatively, it would also be interesting to include more details on the diversity of biases in the groups obtained with different shallow learners at least on the toy MNIST dataset. The choice of shallow models used for bi level learning is not clear in the experimental section. How are hyperparameters tuned for Algorithm 1? https://openreview.net/pdf?id lQdXeXDoWtIOverall, the paper is clear and proposes a novel method.<|endoftext|>The paper studies the problem of OOD classification: the test data and training data distribution can have different spurious feature class dependencies. There is no formal analysis or explanation   most explanations are ad hoc. the formulation/algorithm is not really a bilevel optimization formulation/algorithm.<|endoftext|>Strengths:The procedure proposed in this paper is a natural choice, the implementation is easy and can be incorporated in many off the shelf machine learning training algorithms. There is no novel contribution in the technical aspect. For instance, the authors claimed, "... spurious correlations are commonly characterized as simple surface patterns".
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper performs a study on the adversarial robustness of vision transformers. It provides some insight on vision transformer from the robustness perspective. (3) The paper has a clear structure and is easy to follow. Weaknesses: \\(1) The insights of this work cannot be exploited for understanding or improving the adversarial robustness. This is the main reason that I believe this work might be below the ICLR bar. \\(2) Why is ViT more robust than CNN? It would be interesting if the authors can provide more concrete insight into the mechanism behind the reported phenomenon.<|endoftext|>This paper studies the adversarial robustness of ViTs. It would be good to see my detailed comments below. Second, because this paper is still not yet published and currently there has been a number of paper workings the same ideas of this paper, it would not be appropriate for the authors to overstate this is the first work in ViT robustness, considering many of the concurrent works provide more deep insights in this topic. The current version is lossy. (Negative) It would be good not to say that this work provides the first study on the robustness of vision transformers (ViTs) against adversarial perturbations. There have been other works discussing the ViT adversarial robustness. (Negative) The following observations are natural and not new: "we find that ViTs possess better adversarial robustness when compared with convolutional neural networks (CNNs)."<|endoftext|>For example, it is important to give a reason why ViTs have better adversarial robustness than convolutional neural networks. Does this advantage come from the splitted several tokens in ViTs? 2.More general Transformer architectures should be explored in the paper. The authors mainly focus on ViTs, and the observations are mainly for ViTs. It would be better to explore more types of Transformer and draw conclusions for the more general cases of Transformer. It would be better to provide more explanations for some findings, and further improve the experiments for more general Transformer cases and different types of attack methods.<|endoftext|>This work studies the adversarial robustness of vision transformers comprehensively. However, some concerns listed in weaknesses above remain to be addressed. Therefore, I rate this paper blew the acceptance threshold. The paper is clearly written and easy to follow. The authors state that ViT is more robust than CNN. However, the experiments are conducted on CIFAR10 datasets.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Risk objectives have long been investigated in reinforcement learning (RL). Hence, the authors consider a new risk aware objective. Following some derivation, they compute a sample based estimation of the gradient of this new objective w.r.t.the policy parameters. The correctness of this approach might be trivial for the authors, but not for me. What should be the idea of using the CPT objective instead of other measures? Looks like that this work is focused on the aleatoric component (which is fine). WEAKNESSES:The paper lacks a proper background, clarity, and precision. Is this due to the log likelihood derivative (as in classic REINFORCE) or there is an additional motivation caused by the new objective? The authors can just cite classic works on baseline subtraction.<|endoftext|>The authors evaluate different objectives obtained by employing a Wang weighting function, with different values of the parameter $\eta$. The author included also the distribution of the returns, which helps understanding the impact of the risk averse optimization. This is not fair in principle, since the proposed approach is actually optimizing a different objective. It would be interesting also to see how the algorithm performs w.r.t.the objective it is optimizing, including its learning curves. While the paper analyses an interesting problem and its experimental analysis is sound, it is not clear how novel its derivation is. Moreover, the author did not provide a clear picture on the policy gradient method they propose, for which further proofs should be included.<|endoftext|>They use this distributional objective in conjunction with policy gradient methodology to propose a distribution policy gradient method for risk sensitive RL. ### Concerns:  I think the clarity of some parts can be improved. I think this is an integral component of the work and it should be clarified in more detail. The authors claim that the expression in Eqn 11 has a lower variance than expression in Eqn 6 but there is no accompanying proof. I think the paper presents a really interesting approach to distributional DRL in context to risk sensitive RL.<|endoftext|>This paper considers a generalization of the policy gradient method to optimize for arbitrary utility functions with weightings that depend on the entire CDF (rather than the expected reward). The paper derives an expression for the policy gradient and also generalizes the standard variance reduction baselines. * Figure 5 captions says 6 environments, but there is only three in the figure. Other comments:* How robust are these results to the more common deep RL benchmarks outside of the safety gym?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper presents, at its core, a case based reasoning (CBR) centered approach to solving text based games. Strengths:  The core idea is well defined and motivated. In my opinion, such an analysis would likely prove to be the most valuable contribution of a work like this.<|endoftext|>This paper describes how to apply  a combination of case based reasoning and RL methods to improve performance of agents on text adventure game type tasks.<|endoftext|>This paper proposes an approach to combine case based reasoning with reinforcement learning for text based games. Strengths:  Solid approach combining CBR and RL  Experiments are thorough and convincing. In the retriever module, how is the threshold $\tau$ chosen?<|endoftext|>In this paper, the authors propose to improve the existing policy based RL algorithms for the text based world environments by incorporating knowledge via a case based reasoning module. This improves out of distribution performance. The proposed model is more sample efficient and generalizes better.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; Pros:  The idea of this  paper is interesting. The motivation and presentation of this paper are clear. It is easy to follow for readers. In addition, as described in the late part of the experiment section, training on images with higher resolutions can improve the model performance. In Page 7, the authors report that training with a longer schedule benefits the model performance when utilizing the proposed approach. If the concerns can be well addressed, I would like to lift the rating.<|endoftext|>The paper is well written and easy to follow. 4.ViT models are accelerated by a large margin. The gain is more significant with more input tokens and a longer training schedule. Thus, the discussion of the vanilla model’s training instability and the improved training efficiency lacks enough justification. I recommend accepting this paper for its technical novelty, the model acceleration, the performance gain (when it is trained with a long schedule), and the potential of improving more with a better attentive score.<|endoftext|>In this paper, an EVIT method is proposed for vision transformer speedup. The inattentive tokens are reorganized as one to support attentive tokens. Experiments have shown on thebenchmarks for visual recognition. As have explained in Sec.3.3, this might be because the attentive token selection is not stable at the beginning of the training. The proposed method improves ViTs from the efficiency and accuracy perspectives.<|endoftext|>The results are fine. Given a ViT model, is the design of the proposed method arbitrary for all the layers or some fixed layers? Where to drop and fuse tokens within a ViT model deserves further discussion. These two aspects are evaluated in the experiments.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper theoretically studies gradient based algorithms for two player zero sum Markov Games (MGs), an important problem in multi agent reinforcement learning. The main contribution of this paper is a sample based policy extragradient algorithm for finding an approximate Nash equilibrium of the MG, with improved sample complexity guarantees over existing policy gradient based algorithms. (Feels like this may be resolved by presenting a head to head comparison between the population results of Cen et al.and Wei et al.)? Finally, I am very suspicious of the motivation of “private policy updates” in the abstract / intro, as many prior algorithms also have this feature (as the paper mentions in Table 1) and this is quite common for any sample based decentralized learning algorithm. Update: I thank the authors for their response and revisions to the paper.<|endoftext|>The paper proposes an entropy regularized policy extragradient method for zero sum Markov games in the tabular setting. By using low variance value function estimators, and entropy regularization, the algorithm is shown to achieve improved sample complexity for a given target duality gap for a Nash equilibrium solution. The paper investigates a multi agent RL problem from a policy gradient perspective, which is an interesting problem. The paper is well written. Below are my comments and questions regarding the paper. In the single agent case, entropy regularization with coefficient $\tau > 0$ has two effects: improves convergence rate in the regularized MDP, and encourages exploration (Mei et al., 2020; Bhandari & Russo, 2019; Shani et al., 2019). In particular, entropy regularization makes $\min_{s,a}\pi(a|s)$ bounded away from 0, which seems to be important to prove convergence. The paper assumes that the minimum steady state probability under each policy pair throughout the trajectory of the policy optimization is bounded away from 0, which is a strong assumption as it imposes a condition on the nature of the policies. I would be glad if the authors could provide insights on these. The assumption that the minimum steady state probability under each policy pair during the policy optimization steps is strong. It would be good if further insights could be provided on these.<|endoftext|>## Summary This paper studies a decentralized stochastic policy extra gradient algorithm for solving two player zero sum Markov game. In comparison to the standard policy extra gradient algorithm, this algorithm uses a set of stochastic estimators to estimate the value functions involved in the stochastic updates. Improved sample complexity guarantee is also derived. Overall I find the paper well written and provides good contribution to the current line of works for solving two player zero sum Markov games. The paper also presented clearly the intuition of the algorithm and the new technical techniques that were involved. Post rebuttal update: I have read the authors  response and will keep my original score.<|endoftext|>This paper focuses on the two player zero sum Markov Game setting and proposes a stochastic version of the policy extragradient (PE). The PE algorithm, which solves an entropy regularized minimax matrix game problem by predictive update (PU) [Cen et al., 2021], has been well studied under the deterministic case in the work of [Cen et al., 2021], proven to converge exponentially to a unique solution. This improvement is a valid contribution to the understanding of the Markov Games. While both techniques are previously analyzed in [Cen et al., 2021]. I am not very sure if the novel design of the estimators is a natural adaptation to the PE algorithm, or does the two approaches: the sampling from the state action frequency to the state frequency, and the accurate original policy, have also contributed to the improvement in the sample complexity. The techniques of extending from a deterministic PE algorithm to a stochastic PE algorithm through Markovian sampling are similar as in [Wei et al, 2021], with novel design on the estimators and necessary adaptation to the PE framework. I am leaning towards accepting this paper but have some concerns about the novelty.<|endoftext|>The authors consider two player zero sum Markov games. With entropy regularization and new estimators, the authors show the stochastic policy extragradient algorithms have these properties and the sample complexity improves the sate of the art. The problem the authors consider is important in competitive RL, where the authors show some improvement on the state of the art result, so in general I think this is a good paper. Comparing with Wei et al.I think authors should discuss more on the contributions of each component to their improved bounds. Regarding the technical contributions, I think that is also one of my concern, First, the authors use Assumption 2, but I think Wei et al.didn t need this. If that is the case, I think the paper, strictly speaking, is not comparable with Wei et al.Secondly, I didn t notice the authors explain the technical difficulty when analyzing this algorithm in the main text. Lastly, as highlighted in Wei et al., their algorithm is rational (converging to the opponent’s best response when it uses a stationary policy). It is an interesting paper, which improves the best known bound in the field. However, I have some concerns regarding the novelty, comparison with the previous work, and technical significance. If the authors can address my concerns, I am happy to increase my score and recommend acceptance. After the discussion period, my concerns are addressed so I changed my score accordingly.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; The paper introduces a predictive state representation (PSR) based MARL framework. The framework uses a graph representation to model the interactions between agents. Performance bounds are given for the learned predictive state representation. The experiments results show the advantage of the proposed method compared with the baselines experimented in the paper. The paper gives solid technical contributions and substantial experiments.<|endoftext|>The key advance in this work is to apply this existing PSR framework to networks of agents, with 3 types of agent network: static complete graph (all agents affect all others experience); static non complete graph (only some agents affect one another); and dynamic non complete graph (agents affect one another in a time varying way). A number of theoretical results are presented, including PAC bounds for the approximators in the framework. The paper presents a series of experiments based on environments encoded in the  OpenAI Gym MAMujoco system.<|endoftext|>This paper extends predictive state representation (PSR) for POMDP to multi agent reinforcement learning (MARL) setting. This paper presents practical algorithms for MARL by utilizing the data driven dynamic modeling by PSR. The two versions of the algorithms, MAPSR1 and 2, performed well in MAMuJoCo benchmark. Regarding statements like "PSR is more compact than POMDP": POMDP is a problem setting and PSR is one way to solve that.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; The paper proposed a new penalty term which is expected to make the final prediction modelrobust to distribution shift of test data. The proposed penalty is based on thedistributions of local curvatures of two sensitive groups and enforces these two distributions to be similar. The proposed idea is motivated by robust learning methods where the prediction model is robust to distributional shift. Strength    The idea is new and interesting. The paper is well written. Weakness    I do not understand the fundamental claim that the similarity of the distributions of local curvatures  ensure the fairness robustness. Even there is no explicit definition of fairness robust to distribution shift. I agree that local curvatures affects the robustness of a prediction model but I do not understand why and how  local curvatures affect the degree of fairness robustness. Numerical results do not  seem to support the main claim. For standard fair learning algorithms (notrobust to distribution shift) including AdvDebias and LAFTR, the degree of unfairness (e.g.$\Delta E_{OPP}$) even decreases as noises are added in the test data. Adding a small noise would not be sufficient to see the effect of robustness. As  (Ensuring Fairness Beyond the Training Data, NIPS2020) is done, the worst case distribution shift could be considered for numerical comparison. However, it the authors explain successively why and how the two concepts   prediction robustness and fairness robustness, are related and how the proposed method utilizes this relation. Instead of intuitive explanations, some equations with rigorous definitionswould be helpful for understanding the value of the proposed method.<|endoftext|>The model claims to be fair in robustness against distribution shifts with several experiments being conducted. Points in favor+ Fairness is a hot and timely topic. + Selection of problem domain, i.e., fairness under distribution shifts. This paper does not propose a new fairness goal and the first fairness metric under distribution shifts. Rather it extends equalized model performance to distribution shift settings. Similarity, the  new  learning algorithm is basically a combination of existing techniques such as Hessian matrix and adversarial learning frameworks. The authors should tone down these claims. It is also a stretch to argue that the proposed method does not achieve "much more robust fairness" and is insignificant instead. The statistic test is therefore suggested to verify the importance of these results. The paper does not discuss and compare proper state of the art. The related work discusses pre , in , and post processing approaches while studies focus on fairness under distribution shifts are not cited and compared. To give concrete examples, fairness drift are considered in "FARF: A Fair and Adaptive Random Forests Classifier" as well as reference therein. I urge the authors to compare their method with recent works that consider fairness drift and discuss at least incomparability otherwise.<|endoftext|>The paper is motivated by a common problem in real world applications of deep models, distributional shifts, which can cause unreliable behavior in the deployed models. In particular,  state of the art fairness algorithms would be affected by such distributional shifts in the test data. This poses the following question studied by the paper: how can one achieve fairness when there exist unseen distributional shifts? Towards this end, the paper proposes a new objective: Equalized Robustness (ER), that imposes equalized model robustness against distributions shifts across majority and minority. Comments: * Although the ideal model would indeed be robust against any unforeseen distributional shift, in certain applications (e.g., demographic changes in a student population applying for college), these changes are less dramatic over time (altough not necessarily predefined as described on page 4, Section 3). Would the algorithm still be applicable with some modification to take into account this knowledge (and how)? * In terms of writing, the paper is well written and precise but remains less accessible to readers who are familiar with the fairness literature but not other related parts (e.g., robustness and smoothness). Does the algorithm bring new ideas (of potential general interest) or just adapt existing robustness techniques to this particular fairness related application? If it is the latter, what potential difficulties and special conditions one needs to address (that make the problem challenging)? * I found the experiments extensive enough. On page 7, the paper mentions that Gaussian and uniform noise is used. First, I think that these shifts are not necessarily representable of the true shifts in a population (if not, are there some real cases that justify this assumption?). It think it would make more sense to perform the same experiments on a dataset with observations over multiple years or after a policy change. The idea is to test how a model trained on old data performs on the most recent ones. I suspect that the changes might be not be very dramatic (since we are talking mostly about demographic changes that are slower). Minor comments: * the bright green color in links is difficult for reading* I am not very familiar with the ICLR format, but Section 2 reads like a mix of math preliminaries and related literature. I think it’s better to shorten and remove unnecessary mathematical definitions. * Typo: “the the stability”, p.1The paper tackles an emerging problem (fairness under distributional shifts) and proposes a new objective and algorithm. I also have concerns about the value of the experimental evaluation as explained in my comments above.<|endoftext|>This paper proposes a new fairness metric, Equalized Robustness (ER), to assess model robustness for sensitive subgroups. ER measures the maximum mean discrepancy (MMD) between the loss curvature of the two sensitive groups. The paper shows that existing in distribution fairness promoting methods do not achieve parity with respect to the new metric. A new method, curvature matching (CUMA), is proposed to achieve both in distribution and robust fairness. + The paper is very well written and well motivated. The notation is clear and the authors provide intuition about the key concepts. Does matching curvatures only guarantee that the groups will have similar level robustness which may be low or high robustness? Or does it specifically encourage high robustness? Would be helpful to be more specific about distributional shifts. It appears these are covariate shifts—i.e.shifts in the distribution of features/predictors, but assuming there is no shift in p(y |x). Do we expect the simulated distribution shifts (Gaussian and uniform noise) to be representative of real world distribution shifts? It is important to acknowledge the problems of a task like predicting "attractiveness". Minor  Would be useful to discuss the choice of kernel for MMD and what that may impact  Empirical results should include uncertainty estimates  I found the explanation of how hyper parameter S impacts the MMD to be confusing. Would be helpful to include dimensions in introduction of the setting in section 3. E.g.x is a p dimensional, Hessian is p x p matrix, etcThe ability for fairness properties to generalize is an important problem tackled by this paper. This paper makes a couple novel contributions: 1) a fairness metric that captures robustness to distributional shifts; 2)  learning method to achieve this notion of “out of distribution” fairness as well as standard in distribution fairness. The main technical areas of improvement are 1) include analysis on how well this method works for real world (as opposed to simulated) distributional shifts and 2) provide error bars to support the empirical claims. I would also encourage the authors to exercise more care/caution around a task like predicting "attractiveness".
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The grounding to the target image region is achieved by recovering the corresponding color text from the masked token in the query template. The idea of connecting images and texts with the concept of color is quite neat. What if it is not that case? Furthermore, even for the visual grounding task, CPT is only evaluated on RefCOCO and its variants, making the claims less convincing. While the idea of this paper is neat, there are a few details missing. The empirical studies lack comprehensiveness to support that the proposed method can generalize. I would be more than happy to change my score if the concerns are addressed well.<|endoftext|>This paper proposes a colorful prompt tuning for pre trained vision language models, with color masked regions and masked word tokens, on three reference tasks (refCOCOs), it shows promising results in zero shot and few shot settings. It shows promising results on three refCOCO tasks. Weakness:There are some questions or missing parts  1. This question may relate the motivation of this work, if the coverage is not too high, it might be quite trivial for this method. For sure, refCOCO is a natural task for this method. This is an interesting work to utilize the grounding between image regions and text words, as a prompt tuning way for pre trained vision language models, and shows promising results on three refCOCO tasks in zero shot and few shot settings.<|endoftext|>Specifically, CPT applies a unique colorful mask to each visual region in the input image and then utilizes a pre defined template to wrap the input text, where the modal needs to identify the color of the corresponding region that contains the described object. 2.Extensive ablation study and discussions are involved in the paper. As discussed in Section 4.5, one limitation is that the added color masks would disturb the original colors in the input images and eventually mislead the model. It is believed that such cases are common in real world data, such as “man in red shirt” or “white cat on the couch”. 3.The authors should visualize and compare the results of visual grounding derived by the proposed CPT and zero shot/fine tuning/fully supervised models. Pls see the details in main review.<|endoftext|>Generally, CPT is an interesting, simple but effective prompt based approach, which reformulates the problem to fit the downstream task. The experimental results and ablation studies are good and provide many insights. However, there are a few concerns of the paper:1. I doubt that CPT is a generalizable approach that can fit other V+L tasks. But for general task such as VQA, captioning, text image retrieval, I do not think CPT can be applied. It is only for visual grounding tasks. I would appreciate it if my concerns can be addressed in the rebuttal session.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This manuscript enhances the yarn level cloth simulation to a differentiable level by introducing alternative differentiable operators in place of originally discontinuous/indifferentiable counterparts. The yarn level simulation was proposed for more accurate collision handling and detailed presentation. I believe the implementation of this paper is respectful and non trivial.<|endoftext|>This paper proposes a new differentiable physics based model for simulating composite materials such as fabrics in cloths. The underlying principle may be extended more generally for other related tasks. Experiments show the efficacy of the proposed model, confirming the model’s capability for simulating heterogeneous materials such as fabrics. Strengths: This is the first fine grained data driven yarn level model for describing internal forces for fabric.<|endoftext|>This submission proposes a differentiable parametric force model for fabric, at the level of yarns, by modelling interactions forces between yarns. Strengths:  The proposed model allows to estimate physical parameters efficiently, ie. Yet the experiments fail to prove it is useful to fit general fabrics in a less controlled setting.<|endoftext|>This paper studies the fine grained cloth simulation using differentiable physics models. The paper is largely based on the prior work on cloth simulation [Liang et al., In NeurIPS 2019] but with the domain specific yarn level formulation. Specifically, it proposes to represent cloth as two perpendicular groups of parallel yarns. Experiments have been conducted on simulated data for material estimation and model based control learning. Strengths:* The paper studies a challenging topic in differentiable physics, namely, the fine grained differentiable cloth simulation. * The paper contains detailed problem formulation and derivations.<|endoftext|>This paper introduces a model for yarn based fabric simulation that is differentiable and hence particularly well suited for solving inverse problems, such as deriving model parameters from reference video input. At least not more than between Ground Truth and Ours. The contribution is a simulation that serves as forward model component in a regression framework, so I think that should qualify it. As the authors acknowledge, neither the specific problem nor the general idea of differentiable simulation is new. Still, I think this is reasonably nice work to warrant acceptance.<|endoftext|>This paper proposed differentiable yarn level dynamics for fabrics. Compared to the previous differentiable cloth model, this paper is more specific and fine grained. The experiments show that their method can learn the control policy and material parameters successfully. The fine grained model is accurate.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The draft of the paper especially Sec.3 is very hard to read and understand. The paper works on a novel problem of interpreting and explaining structured output models. Example, in Eqn.<|endoftext|>The paper proposes a technique for identifying what input variables are most relevant for determining the value of a single, given output variable in structured output (MAP) inference. Interesting research direction but unconvincing text, technical contribution, and empirical evaluation This is fine.<|endoftext|>I can’t see the novel point for the proposed methods as the method seems to be a combination of pre existing methods in model interpretatility. The authors validate their method on synthetic and public datasets.<|endoftext|>This paper propose a method for interpreting structured output model. In general, I found the paper quite hard to read and motivation of the paper was not very convincing.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; It can be difficult to follow, and greatly detracts from the quality of the paper. Along this note, it seems like PER is a pretty important baseline to compare to in the space of methods which select which transitions to sample from the replay buffer  can the authors comment on why PER was not compared to, and whether the claims of being "state of the art" are reasonable when only comparing to vanilla experience replay? With sufficiently large replay buffers and enough changes made to the policy, it de emphasizes updates on really old/stale transitions that the optimal policy would rarely visit. The paper suffers many spelling, grammatical, and clarity issues.<|endoftext|>The paper presents Experience Replay More, an algorithm that prioritizes certain transitions in a replay buffer for replay. Experience replay is a really important component of many RL algorithms right now. While I understand that the idea of the paper is to more frequently replay states that are more important in some way, the paper does not communicate how those states are identified or what makes them more important. Given this, the AN2N algorithm needs to be far more precisely described and justified. Even if the AN2N paper were more clearly written and its hypotheses clearly evaluated (from a brief skim this does not seem to be the case), it is important that *this* paper be sufficiently self contained that the reader can understand the significance of the contributions. Each line on the plot is associated with a particular Q value (I think??), so what does the y axis represent?<|endoftext|>Weaknesses:* This paper proposes a different sampling scheme than uniform sampling. However, in the experiments, no other sampling schemes are compared at all. Even though the paper mentioned prioritized experience replay in the Introduction, it is not considered a baseline in the paper. In the experiments, we only see the comparison of normal RL algorithms and RL+ERM (for example, DDPG vs DDPG+ERM, SAC vs SAC+ERM). To answer this, the authors should include the learning curves of DDGP+AN2N, TD3+AN2N, SAC+AN2N. * The paper lacks the discussions on how sensitive the proposed sampling scheme is to the extra hyperparameters that it introduces. How about other more difficult benchmarking tasks such as Humanoid? * The writing of the paper needs to be improved substantially.<|endoftext|>The paper proposes a new method (ERM) to bias the decision over which transitions should be replayed more often. In particular, using ERM, some states are deemed as key and they are replayed more often (following also a recency bias). This new method is then tested in 4 environments on the DeepMind control suite by using 3 different RL algorithms (DDPG, SAC, TD3). The paper is very difficult to read as it contains many typos and fragmented sentences. It is also very unclear how the similarity between states is calculated. Again, the authors claim that SAC+ERM is better than SAC in half of the task, but this conclusion is not supported by the plots. Also the paper is not well written, with lots of typos and fragmented sentences.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a variant of sliced Wasserstein distance, named augmented sliced Wasserstein distance. This paper is overall well written, and ASWD is well motivated. (b) The second question is about the cause of the difference in performance.<|endoftext|>Empirical evaluations on simulation datasets and on generative modeling highlight the potential of the proposed method over existing approaches. Overall, the paper is well written. The new distance is supported by theoretical analysis and empirical experiments showing its effectiveness. To guarantee the injection property, a neural network (NN) is considered and $g$ is get as the concatenation of the input and the output of the NN. The idea inspired from DenseNet is tricky and presents a clear advantage of avoiding learning complex NN. Intensive empirical evaluations are conducted.<|endoftext|>Several experiments are conducted on generative modelling (CIFAR10, CelebA,  MNIST, color transferring). #### Strength: The paper is well written and the approach is mostly well presented. It is probably that the fact for a small $lambda <1$ ASWD outperforms the sliced based Wassrstein distance is linked to the choice of this special parameterization of the injective NN.<|endoftext|>The proposed distance is shown to be a metric. Moreover, given that the optimal choice of the nonlinear maps is rather computationally intensive to obtain, an approximation based on neural networks is proposed. Several experiments are shown where a better performance is obtained with respect to existing methods. Strengths  The paper is well written and results are easy to follow.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; 2.The runtime efficiency of DocHopper is one of the main contributions that the authors claim in their paper. The research problem of this paper is the inefficient computation time of existing QA models. Therefore, comparing IRRR and DocHopper is not a fair comparison.<|endoftext|>The proposed method achieves strong performance on those datasets, reducing the computational cost at the inference time. Claiming state of the art results on those variant settings may not be fair, especially when the baselines to be compared are not specifically designed for the new setting.<|endoftext|>Evaluation results show improvements on 3 of 4 datasets. The proposed approach is interesting and more efficient compared with cross encoding approaches at inference time. Most prior results in the literature suggest that separate encoding approaches work inferior to cross encoders, thus results in this paper are encouraging. In this regard, the comparison is not entirely fair. One of the stated benefits of the proposed approach is that it is more efficient by reducing the need for cross encoding the query and context at inference time.<|endoftext|>The paper proposes a simple attention based model for conversational and multi hop QA tasks. I do think it makes novel technical contributions. 2.The process when attending to a paragraph vector (i.e., unpack the paragraph by reusing dot product attention, then pack it again with the extra learnable attention) seems tricky and cumbersome but the intuition behind is not clearly illustrated. 4.The performance of the proposed model lags far behind MATE on HybridQA dataset. Minor comment:Strictly, the softmax term in Eq.3 (and argmax term q_t c_m) should be s^i_j q_t^T, as q_t and s^i_j are both 1*d matrices according to the notation at the bottom of page 3. Trivial method, contributing little scientific knowledge.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper considers the problem of probabilistic certification of robustness, that is, showing that the probability that a random point in some hyperrectangle changes the classification. A simple baseline would be to use a stronger verification method (e.g.https://arxiv.org/pdf/2010.11645.pdf) with Q 0, which might actually outperform the proposed method even allowing for larger Q.<|endoftext|>For both PROVEN and I PROVEN I find the use of convex relaxation a questionable choice due to several limitations. In particular, the authors expand a known methodology (PROVEN) for computing probabilistic robustness and show that in many cases their methodology is better than that of PROVEN. I think the pros of this paper are its clear exposition, goals, and methodology. Further the goals and method of the paper are clearly explained.<|endoftext|>I find this paper marginally below the acceptance bar because the problem setting is not that well motivated, and the approach itself is incremental over the prior work PROVEN. Could randomized smoothing be applied to the threat model of probabilistic robustness that authors consider in this work?<|endoftext|>***updates after author response***I am still feeling that, while the observation of union bound is cute, the overall technical contribution is below the bar (the union bound is the only new analysis). However I do encourage authors to further explore the power of the bound, likely among other things, to get a stronger algorithm.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper is well written and easy to follow, it does a good job summarizing the requisite group theory, and the numerical experiments indicate the potential of the proposed strategy. What is its complexity class? Figures 1 and 2 have no caption which makes it hard to interpret them. This fact is not surprising to people familiar with group representations (I would like to say that it is known although (perhaps) not commonly written in this exact form.<|endoftext|>While I liked the paper overall (especially its general discussion in the first half), I think it is not quite ready for publication without extensive experimental results.<|endoftext|>The main problem is that when you try to implement this algorithm by yourself, you have to calculate the characters by yourself, and the set of characters is not finite in general.<|endoftext|>I have to confess that I was not able to understand the main thesis or contributions of this paper. The presentation of the paper could be significantly improved. I understand this may be due to my limited knowledge of group theory, but I also think the authors should try to present the paper in a way that a broad audience in the machine learning community could understand and appreciate.<|endoftext|>Therefore, it seems inappropriate for this paper to be accepted for the conference. The paradigm proposed in the paper is very interesting, and I liked the manuscript, especially the Appendix, which explains the mathematical concepts in an easy to understand manner. As the authors mentioned, the paper does not mention the application of the proposed paradigm around machine learning field.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper propose Head2Toe, a method that exploits intermediate representations of DNNs to improve performance, and OOD generalization of transfer learning. The key idea of the Head2Toe is to augment traditional linear probing with intermediate representations. An important finding is that Head2Toe improves OOD generalization. 2.The paper is generally well written and easy to follow. 3.The proposed method is simple, technically sound, and makes sense. The overall performance of Head2Toe shows some improvement compared to linear prob and even fine tuning. The key difference between Head2Toe and ELMo s approach is that Head2Toe selects a subset of intermediate features after learning the weights. In the VTAB benchmark, in natural and structured category, the performance of Head2Toe is only comparable with linear prob but worse than fine tuning. The overall improvement of Head2Toe is because it performs well on Specialized category and it has more tasks. 3.The claim in the title and abstract that Head2Toe improve OOD generalization are not well supported by the experimental results. The main experiments only show that Head2Toe improves transfer performance. The paper is generally well written and the method makes sense. However, given that the novelty of the method and some concerns about experimental results, I think the paper, in its current form, is slightly below the bar.<|endoftext|>The main focus of this paper is the study the use of intermediate layers in a deep pretrained model on downstream tasks. Authors argue that the the fact that the good performance while fine tuning on downstream tasks even if data scarce is due to the prior existence of useful representations deep in the model which are brought up during training. Head2toe, which consist in utilizing intermediate representations, while freezing the weights of the network for training for downstream tasksExperimentation is carried out starting with a ResNet 50 and ViT B/16 models on a variety of datasets collected on the VTAB collection, showing that the approach where Head2toe  outperforms linear finetunning (training a classification head on top of the model, while freezing the rest of the weights) and matches the performance of finetunning the whole model. The proposal is indeed interesting, although utilizing intermediate representations has been seen in the past. My main concern regarding authors claim "We conjecture that FINETUNINGbetter leverages existing internal representations rather than discovering entirely new representations; FINETUNING exposes existing features buried deep in the net for use by the classifier".<|endoftext|>This paper explores the utility of intermediate layers for linear probing in transfer learning. Experiments on the VTAB benchmark shows that Head2Toe matches performance obtained with fine tuning on average, but critically, for out of distribution transfer, Head2Toe outperforms fine tuning. Strengths:  The paper is very well written and easy to follow. The overall idea of Head2Toe for transfer learning is interesting. How about selecting a common set of intermediate features for all the tasks? How the proposed method comparable to finetuning on other tasks besides image classification, e.g., video classification? How is this method comparable to existing transfer learning methods besides Finetuning on VTAB benchmark, e.g., selective finetuning or some of the methods mentioned by the authors in the related work? I would like to see authors response on new experiments and discussions during the rebuttal as I think the experiments are limited and somewhat unconvincing in the current version of the paper.<|endoftext|># SummaryThis paper proposes a method for using intermediate representations from a pre trained model for better transfer to other tasks. A linear classifier is trained on features from multiple layers. The proposed method works slightly better than fine tuning with a ResNet model, and slightly worse with a ViT model, although the comparison is not completely fair, as discussed. There are different performances on different categories of images. # EvaluationThis is a well written paper with a simple but clearly motivated idea. The preliminary experiments and discussion are helpful in motivating the approach. [1] Dalvi et al., 2020. There is a comparison with several reasonable baselines. While I m not very familiar with the transfer learning literature in image classification, the idea seems common enough (as discussed in the related work) and pretty straightforward. See the main review for more questions and comments. How would the results of the present approach compare to others that use intermediate representations? What exactly is the novelty?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper studies several properties of the landscape of memoryless planning in infinite horizon POMDPs. The paper considers an important and challenging problem, with a set of new interesting theoretical discoveries.<|endoftext|>This paper studies the geometric property of memoryless policy optimization problem for POMDP. I enjoyed reading the paper.<|endoftext|>In this work, the authors study the problem and structure of memoryless stochastic policies in POMDPs. The paper appears to be novel and well written. Commentary on the implications of some of the statements would benefit the paper. In this regard, while theoretical properties are always welcome and of interest.<|endoftext|>This paper studies the problem of finding the best memoryless stochastic policy for an infinite horizon partially observable Markov decision process (POMDP).
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; The paper addresses the problem of monotonic value representations for non monotonic true values in MARL. Is $Q$ factored? and the results in Figure 2 (are there siginificant differences in return?). **POST REBUTTAL**Thanks to the authors for their clarifications.<|endoftext|>The authors proposed a new additional condition for value decomposition in MARL, True Global Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author s claim, there are lots of explanation to understand. In the paper, the authors used true $Q$ function and joint $Q$ value function, but reviewer doesn t find the definition of joint $Q$ value function.<|endoftext|>The proposed method (GVR) attempts to ensure both Individual Global Max(IGM) and True Global Max(TGM) conditions without learning a completely expressive (CEC) value function (which can represent all joint  actions). ### Strengths:* The ideas in the paper are novel, and experimental results validate the equations as well. The destabilization of the non optimal greedy actions through the superior replay buffer is interesting, but the authors should empirically compare against other efficient exploration based MARL approaches such as [1,2,3,4]. However, the authors rely on superior experience replay to destabilize the non optimal greedy actions.<|endoftext|>The paper studies the problem of value decomposition, which decomposes the joint Q function into some linear or monotonic transformations of individual factored Q functions for each of the agents. The True Global Max condition is interesting, and the paper proposes relatively simple techniques to satisfy the condition, which is a plus. The experiments are also interesting and the ablation studies in the supplemental are informative.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper presents a neural net based bandit approach with a novel exploration strategy. Specifically, the solution uses an exploitation network to estimate rewards for each arm and an exploration network to predict the potential gain compared to current reward estimate. I have detailed my questions in the previous box. Can authors provide this comparison analysis?<|endoftext|>This paper studies the neural contextual bandit problem, and proposes a neural based bandit approach with a novel exploration strategy, called EE Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward.<|endoftext|>This paper studies neural network based contextual bandits. Different from existing works which utilize UCB or Thompson sampling for exploration, the exploration component of the proposed method EE Net is also modeled with a neural network. **Strengths**The idea of utilizing neural networks to model potential reward gains for exploration is very interesting; it seems quite different from rule based UCB and Thompson sampling. > In real applications, it is not possible to tune hyper parameters in this way primarily due to the bandit feedback. The idea is interesting and as far as I know it is novel. See Main Review for details.<|endoftext|>This paper proposes EE NeT for contextual bandits which contains two neutral networks: one for learning the underlying reward function and another for learning the potential gains of arms if explored. This paper also shows that EE NeT outperforms existing linear and neural bandit approaches. The reviewer has the one concern about the exploration network:      the proposed exploration net is mainly based on the error bound derived from Ban et al.2021.This paper uses neural networks to learn the complicated function. What’s the point of training a neural network to learn it instead of learning the unknown parts in that equation?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This admits a formulation as a linear program and may be solved efficiently through its dual. However, recovering the primal solution is nontrivial, and there exists no rate analysis to date. The crux of this method is an alternating block coordinate ascent algorithm, which is then combined with an extrapolation/Nesterov acceleration scheme to accelerate convergence. However, what is unclear is why the entropy regularization is necessary, or in what sense it helps. I agree that strong concavity helps with rate analysis, especially as it is necessary to obtain improved rates from Nesterov acceleration. On the practical side, a few different use cases of EOT are discussed in the introduction  such as cake cutting, multi type resource allocation, internet minimal transportation time, etc. This seems like a major drawback of the manuscript. In summary, this paper develops some interesting theoretical insights about optimal transport with entropy regularization, but the innovations mostly borrow from existing results that are standard in numerical optimization, machine learning with regularization, and while the rate analysis is presented for a novel context, it does not itself employ any novel techniques or technical innovations. On top of that, the experiments are extremely simple and do not convincingly demonstrate that EOT is useful in a problem that arises in practice. This work is well written but not particularly technically innovative, and is missing a thoroughly convincing use case.<|endoftext|>The contributions of this paper are on the complexity analysis, as well as new algorithms for the EOT problem. The paper is well written, apart from minor typos, and easy to follow. However, I did not checked every line in all proofs. Weakness.This was a missed chance to really motivate why EOT is an interesting problem. The numerical experiments are toy examples, of low dimension. Is a pity that there is no provable accelerated for the proposed accelerated method. Correctness: The techniques use for proving the results are rather classical in primal dual methods. It show it works but really does not provide any additional insights or interesting applications.<|endoftext|>In particular, EOT is a linear programming (LP) problem, which is expensive to solve in practice. However, there are two main issues with PAM: that is its convergence is unknown and it only provides a solution in the dual domain. The authors precisely address these issues by providing a convergence rate for PAM and proposing a novel method to compute a feasible primal solution from the dual solution. Strengths:(a) The paper is well written as easy to follow. (c) The authors further provide a novel rounding scheme plus a variant of EOT. Weaknesses:(a) My main concern is about the novelty and the level of contribution of the paper, which is mostly providing analysis of an existing algorithm for an existing probem. I also have difficulty interpreting the results reported in Fig.2.Overall, the paper provides some novel theoretical results for the PAM algorithm that is used for solving EOT. The paper is well written and its theoretical results are strong. I am personally not familiar with EOT applications, but if other reviewers believe that EOT is a sufficiently interesting problem, I will be willing to vote for acceptance.<|endoftext|>For this formulation, researchers have proposed a projected alternating maximization algorithm (PAM). The authors of this paper provide a convergence analysis of the PAM algorithm. I followed the proofs and did not identify any concerns with the arguments. The paper also contributes a variant of PAM that leads to numerical performance improvements. Numerical experiments are provided for a synthetic dataset. The convergence analysis is facilitated by the introduction of a novel procedure for extracting the primal solution. The authors cannot theoretically establish a faster rate, but the numerical experiments indicate a substantial improvement. While it is certainly sufficient considering that the primary contribution of the paper is the theoretical development, more extensive experiments (and associated analysis) would strengthen the paper. Comments:The paper provides a useful and novel theoretical result for the EOT problem. I think the paper could be improved by some further discussion of the main result. For example, there is an observation in the contributions section that the rate is the same as that of Sinkhorn’s algorithm for computing the Wasserstein distance. There is not a strong tie between the theoretical results and the experiments.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The setting is very appealing, and the paper is clearly written, but I m concerned about the MADT Online method (see above).<|endoftext|>Novelty:This paper is not novel as it seems to be a straightforward combination of transformer and MAPPO. Thus I recommend the rejection. Significance:It is obvious that using transformer improves the ability of representation, and the results are not surprising.<|endoftext|>I suggest toning down the claim. However, this part should be elaborated. The contribution and novelty of this paper, however, are limited.<|endoftext|>As a result, I lean towards rejection at this time believing that this paper can make a much larger impact to the community following another significant round of revisions. The proposed approach was not clearly related to the multi agent literature at all and popular approaches were not compared to even on a theoretical/intuitive level.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper studies the dependency of SGD convergence on order of examples. For smooth loss functions, the paper bounds SGD convergence based on averages of consecutive example gradients. Specifically better the average of consecutive example gradients approximate the objective gradient, faster the SGD convergence is. The paper recovers (and in some cases improves) the convergence results of various example selection schemes. Based on the framework. Their framework also helps develop two new example selection algorithms.<|endoftext|>The paper also proposes a technique which greedily tries to optimize the metric in the proposed condition. The paper proposes a general metric which can be used to prove convergence rates for a range of stochastic optimization algorithms such as Random Reshuffling, Shuffle Once, SGD with replacement, or any technique that uses sequences of examples for computing the gradients. Overall, the paper is well written, and presents new and intuitive ideas, and useful results. The convergence rates in prior work for shuffle once do not have a dependence on dimension $d$, whereas Proposition 2 has such a dependence. "Closing the convergence gap of SGD without replacement." I think the authors should provide some more details in the comparison. This paper presents new and intuitive ideas, and useful results, and is a step in the direction of unifying convergence analysis for different sampling schemes of SGD. For example, if $f \|w\|^2$, and one of the functions $f(w;x) \frac{1}{2}\|w\|^2$, then this assumption is not satisfied.<|endoftext|>The paper studies the convergence of SGD, Random Reshuffling (RR), and, more generally, algorithms that process data examples in an order satisfying a certain assumption, which is a variant of bounded variance assumption with variance decreasing as $O(m^{ \gamma})$, where $\gamma$ ## MainI am somewhere between "weak accept" and "accept" for this paper. But the main concern that I have is that all rates for RR and SO seem to be strictly worse than the rate of standard gradient descent. This is in contrast to the results of Theorem 1 of Mishchenko et al.(2020), which shows that RR and SO can be $n$ times faster than gradient descent as long as $\sigma^2$ is small and the loss functions are strongly convex. What I really like about this work is the new variants of data selection proposed by the authors. It d be very nice if the authors could study this phenomenon on a non synthetic problem.<|endoftext|>1: This paper theoretically builds the following connection: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. This indicates that the convergence rate obtained through the analysis of average stochastic gradient error is tight enough, and should be useful for analyzing new sampling strategies in the future. 3: Two algorithms are proposed based on this theory. The latter problem is clearly much easier to address.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Given some monolingual data, the authors use GPT 3 to generate zero shot translations, which then get used as few shot examples for a smaller generative model to generate a synthetic translation dataset. For example, MASS [6] pre trains using only a denoising autoencoding objective and yet achieves strong results comparable to the ones found in this paper (accounting for model size) and additionally does not rely on clever prompting. al.Do all Roads Lead to Rome? The initial translation performance from GPT 3 already demonstrates that language model can serve as an acceptable substitute to denoising autoencoding for kick starting back translation. al.Language Models are Good Translators[8] https://github.com/openai/gpt 3/blob/master/dataset_statistics/languages_by_word_count.csvIn summary, I don’t believe this work in its current shape is of the adequate quality to be published at ICLR. The choice of language pair also hinders this study.<|endoftext|>This paper proposes to solve the machine translation task without any bi text data and using only a pre trained generative language model to bootstrap the process. It is not clear what the advantage of the proposed method over such an approach would be except as a curious application of existing pre trained models. Could the authors elaborate on this point and, if they agree, compare to the methods described above.<|endoftext|>The paper proposed an iterative finetuning method to adapt pretrained LMs to translation tasks. The model is finetuned by backtranslation. Experiments show that the trained system achieves the better performance than existing similar methods, showing that the effectiveness of the proposed method. It would be not reasonable to say state of the art by comparing its accuracy with other "unsupervised" models trained from only a specific corpus. Since pretrained LMs are trained using a bunch of resources available from the Internet, its training data possibly includes the training/evaluation data used in this experiment, meaning that there would be some suspicion about data leakage.<|endoftext|>This paper investigates the way of constructing the UNMT model with a generative pre trained language model, such as the GPT X model. This paper only verifies the effectiveness of the proposed method on the English French language pair. I like this idea of constructing UNMT system with the GPT X model and the proposed method is very interesting. This paper merely verifies the effectiveness of the proposed method on the English French language pair, making the experiment results not convincing enough. 2.In fact, this proposed method could be applied in sequence to sequence pretraining model, such as mT5.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; **Pros:**  This paper shows provable speed ups on the second order methods introduced in previous works, and further demonstrates with experiments. The previous works are properly surveyed and discussed in the context of the proposed methods. Why does (not )squaring matter if $S$ is solved by just minimizing the loss $\mathcal{L}(S, A_i)$ itself? It may be nice to show these numerics: the convergence of first order methods in FIgure 1 4 as a baseline, and how the spectrum of $A$ looks like. I would suggest the authors address these ambiguities in both the author response and the revised version.<|endoftext|>In this paper, the authors extend a line of work focused on sketching the Hessian for convex problems to help accelerate second order optimization methods. ## Strengths:The paper is well written and gives a nice background of existing literature with strong motivation. The technical detail is of good quality and the numerical experiments provided show that the method performs well on selected data sets when comparing number of iterations required. This is common for LS but is it often observed elsewhere that makes it more useful for non LS problems. 2.For learning the locations of the non zero entries as discussed in 3.1, how is the oracle trained to predict these heavy rows? It clear indicates the sixe Since the paper primarily focuses on the application of a learned sketch to an iteratively solved LS problem (both components well established elsewhere) the contribution seems marginal.<|endoftext|>This paper considers how learned CountSketches can be used in a variety of optimization tasks. The authors propose a method for predicting rows with high leverage score based on the training data. Why are “learned(value only)” and “learned(combined)” not included in Figure 3? Learning of sketch matrices is an interesting twist to the more standard random sketches out there. It is unclear how useful these learned sketches actually are. What does that mean? Q10.One of the questions you mention in the introduction is “(2) should we apply the same learned sketch in each iteration, or learn it in the next iteration by training on a data set involving previously learned sketches from prior iterations?” You then say that you will answer this question, but I didn’t see this discussed in the paper. W2.The datasets in the experiments are very small scale, and can be solved very easily using deterministic methods. You should put it as a proper section heading. The core idea of learning sketch matrices is interesting. What am I missing?<|endoftext|>This paper studies matrix sketching algorithms where the sketching matrix is `learned , or generated adaptively. The authors do do a good job addressing this related literature, but their results are mostly direct consequences of matrix concentration bounds. The experimental results are interesting: I m not aware of such evaluations of test errors in previous works. On the other hand, these studies mostly take place on small to moderate sized data where the performance gains from sketching is unclear. So I feel they serve mostly as proof of concept of the utility of such sketching, and perhaps also point out that different formulations/objectives are necessary for giving better test errors.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper presents an approach for few shot (FS) learning using Voronoi Diagrams (VD). In particular, it relates the objectives of existing FS approaches to VD, and shows how Cluster induced Voronoi Diagrams (CIVD), a variant of VD that allows multiple centers in a cell, can be used in for FS learning ensemble method (DeepVoro). It is well written, and I found it easy to understand the main points of the paper. My main concern is the lack of details provided about the implementation, as I can see this approach would be useful in many future FS research techniques. It would also be valuable to add a discussion of how outliers would affect the method, computational complexity of the proposed technique. Mathematical link is presented between FS classification and voronoi tessellation, description of a novel ensemble FS method based on  CIVD.<|endoftext|>The paper proposes a CIVD based approach to few shot learning. CIVD, cluster induced Voronoi diagrams, are a known technique that is used to categorize / describe different types of few shot classifiers. In the experiment section DeepVoro(++) is shown to perform superior to other methods on three datasets. Terminology is sometimes used in unusual ways, e.g.I would identify data augmentation rather with what is said under point 3 in section 1 than GANs and VAEs in the top of the page. Figure 1 is confusing. The text below uses Dirichlet tessellation as a special case of PD, whereas before (3) they are identified. The datasets used for the comparisons should be described in the main paper, not the supplementary material. How are these modifications motivated from theory or experiments? Important descriptions, in particular of the proposed methods, are missing. Post rebuttal:most concerns have been addressed and the correctness has improved.<|endoftext|>This paper provides a new geometric point of view for few shot learning (FSL). In this view, the widely used ProtoNet can be regarded as a Dirichlet Tessellation (Voronoi Diagram)in the feature space. Furthermore, the authors use the recent Cluster induced Voronoi Diagram (CIVD) for FSL and propose an ensemble approach to achieve a stronger FSL model. Extensive experimental results on three standard benchmarks demonstrate the effectiveness of the proposed method. This paper makes a bridge between computational geometry and FSL. 2.This approach achieves the new state of the art performance over three standard datasets, including mini ImageNet, CUB, and tiered ImagenNet. The main idea is novel and interesting, and the experimental results demonstrate the superior performance of the proposed method.<|endoftext|>The authors introduce the use of Cluster induced Voronoi Diagrams to few shot classification, and show that it can be used to combine feature and surrogate representations with various types of few shot classifiers (eg nearest neighbour, linear classifier and cosine) and various types of heterogeneities (eg feature level, transformation level and geometry level) into a single, coherent, mathematical formulation. Positive:  The use of the Cluster induced Voronoi Diagram and its variant introduced in this paper are novel to FSL (to the best of my knowledge). The resulting voronoi diagram formulation is geometrically elegant, and allows the integration of various classifiers, heterogeneities and types of feature representations. The results appear to be state of the art (at least relative to the compared works) and standard datasets used by the community. Furthermore, the classifiers it integrates and the various heterogeneities are not novel, and neither is the cluster induced Voronoi Diagram (which itself is a relatively straightforward generalization of voronoi diagrams). I overall like the paper and appreciate the geometric formalism and results. That being said, from a mathematical point of view, I am unsure about the amount of novelty the paper would have had, had the same approaches for integration been used without the overarching geometrical point of view.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The work proposes an algorithm like TRPO and PPO inspired by Mirror Descent. The contribution of the paper is primarily algorithmic and also empirical; there is no supporting theory for the proposed algorithm but I believe this is fine for this type of paper. The paper proposed an algorithm inspired by Mirror descent, much like TRPO and PPO, both of which are well known and used. The method appears to work convincingly better than the state of the art in the proposed experiments. For the on policy setting, an interesting difference with TRPO is the way the trust region subproblem is solved.<|endoftext|>This paper focuses on reinforcement learning with the tabular Markov decision process setting and proposes the mirror descent policy optimization for off policy and on policy situations. Furthermore, the experiment result shows that this new algorithm outperforms the previous algorithm in both off policy and on policy situations. It seems that this paper this work is a combination of mirror gradient descent with the previous TRPO or PPO structure, and the technical contribution is limited. Based on the previous comment, it is marginally above the acceptance threshold due to the excellent experiment result.<|endoftext|>Summary:Inspired by recent theoretical analysis of TRPO and PPO that use mirror descent, this paper proposes two new algorithms that directly minimize a mirror descent objective by taking multiple gradient steps. In general I thought the entire paragraph starting with “the main idea in algorithm 2…” was poorly written and confusing. “If we write the definition of KL and use the reparameterization trick in(16), we will rederive the loss function(11)used by our off policy MDPO algorithm” I don’t think this is true, wouldn’t it be the same as (16) but without the current policy term? The authors did a great job describing the differences between MDPO and TRPO/PPO/SAC, but there are many other related methods out there that should be referenced and talked about, for example Trust PCL. Could you put a vertical line in the results to separate on policy and off policy algorithms?<|endoftext|>The paper connects the optimization method, mirror descent, to the study of the policy optimization method. Based on the mirror descent principle, the paper proposes the MDPO algorithm, which updates the policy via approximately solving a trust region problem. The connection between PPO, TRPO, and mirror descent is well known (see, e.g.[11, 19, 20, 24, 28] and [A]). Overall, the paper is well written, and the empirical study well supports the result.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper proposes to use energy based models (EBMs) for task based molecule generation. The energy function is parameterized using relational graph convolutional networks (R GCN) which have the permutation invariance property. 2021.Overall, due to the weakness in novelty and experiments of this paper, I recommend rejecting it in its current form. Pros：  This paper proposes to use EBMs for task based molecule generation, which is exploring a different direction from the most popular autoregressive based generation. The paper is well written and easy to follow. Experiments are comprehensive in the sense that the authors conduct both single objective optimization, constrained optimization, and multi objective optimization to verify the proposed method.<|endoftext|>This paper proposed a graph energy based model based for molecular graph generation. The energy function is implemented by graph neural networks, and a molecule is generated by the Langevin dynamics that maximizes the trained energy function. The experimental results show the effectiveness of the proposed method on benchmark molecular graph generation tasks. This paper propose an energy based generative model for molecule generation, which is a novel contribution of molecule generation literature. 2.This paper is well written, and easy to read. However, the modification of loss function is not justified properly and the improvement is too marginal.<|endoftext|>The paper proposes a new energy based generative model for molecules. The idea is straightforward and relies on using R GNNs (relational graph neural networks) to parameterize the energy function. The authors show how to achieve single  and multi objective generation via scaling, and composition, respectively. Overall, the paper is well written. C1: Some of the contributions listed by the authors (in Section 1) derive naturally from the use of EBMs. Therefore, it seems like the paper s main contribution is to use GNNs to parameterize energy functions. The main positive aspects of this paper are: $i$) first EBM based model for molecules; $ii$) a model that leverages the node permutation invariant nature of graphs/molecules.<|endoftext|>The paper proposes an energy based molecular graph generative model. The paper is well written and the presenting idea seems interesting and novel. They propose to use contrastive divergence to learn the energy function and generate samples from it via Langevin dynamics. weakness: As the main goal of the paper was to overcome the challenge of permutation invariance property when we generate graphs, it would be good to have a discussion in the paper the computational cost of the proposed method when compared to other baselines that use different tricks to handle the permutation invariance.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposed an uncertainty aware offline reinforcement learning algorithm based on a Q network ensemble. The paper provides the theoretic interpretation of their uncertainty penalization. **Cons**  The proposed method requires additional computation costs and memory for Q network ensemble and OOD penalization. There is a recent paper [1] that is highly related to this submission. This paper is well motivated and clearly written with sufficient theoretical and experimental backups.<|endoftext|>Bootstrapped Q functions are trained, and the standard deviation of their estimates is used for the uncertainty quantification. This uncertainty quantification is then used for pessimistic bootstrapping. Also, in contrast to the existing methods that only considers in distribution target, PBRL optimizes Q function even for out of distribution actions with the pseudo target that is penalized by the uncertainty quantifier. A theoretical analysis is provided that PBRL is provably efficient in the linear MDP setting. Overall, I think the paper made a solid contribution. The proposed method is well backed by theory and the empirical results are convincing.<|endoftext|>Namely, Lemma 1 in the main text asserts that the bootstrapped uncertainties in the algorithm are equivalent to LCB penalties at the heart of recent theoretical works. To remedy this, the paper proposes an offline RL algorithm in which an ensemble of critics is trained with an objective composed of (1) a TD error based on actions seen in the dataset with target value penalized by standard deviation over the ensemble of the next Q values, and (2) a squared error on the Q values of OOD actions regressing to those same Q values penalized by the standard deviation over the ensemble. The proposed algorithm is paired with a theoretical analysis connecting it to recent theoretical offline RL papers. The algorithm is evaluated on the D4RL benchmarks and shows favorable performance compared to baselines.<|endoftext|>This paper presents a model free pessimistic bootstrapping approach for offline RL. Specifically, the paper considers an actor critic approach with an ensemble of Q functions and utilizing disagreements in their predictions (measured as standard deviation) for learning the Q functions. Comments on why this is needed or how it can be relaxed? The paper offers theoretical support in the linear MDP setting and has compelling empirical results in the D4RL suite (with both MuJoCo and Adroit tasks).
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; Finally, I found the writing style of the paper to be very hard to follow and confusing at times. This can reduce the complexity of the agent and it can allow to remove undesired actions for probabilistic or online learning scenarios. The contribution of the paper is the used of a monte carlo method to find the size of the action set, which is claimed as a Shapely inspired algorithm, which is correctly spelled Shapley. The paper is not ready to be published and needs a thorough revision.<|endoftext|>This work proposes a method to reduce such action space exploration. The method separates actions into two categories: dispensable (the action can be ignored) and indispensable (the action must be taken). While the paper is well motivated, the current state of this work is unsatisfactory. Finally, no other baselines are provided at all, thus it is impossible to properly judge the performance of the method. This is an incomplete work. The method is not explained in detail and is not evaluated in a sufficient manner.<|endoftext|>The paper proposed a data driven method for optimal action space selection in a reinforcement learning problem. The case study demonstrates that the Monte Carlo sampling based algorithm reduces action search space by 81% and then it creates a list of ranked action set. The presentation needs to be improved significantly. I also had a hard time in understanding Figure 3 and 4. However, I am afraid that the presentation of the paper is not up to mark at this moment; I failed to understand the key technical contributions and empirical evaluation methodology.<|endoftext|>This paper empirically considered the impact of training action space for reinforcement learning in a case study. An empirical study of this problem appears to be the main contribution of this paper. For example, it is quite confusing to me what the dispensable action set and indispensable action set truly mean. The authors also stated that "the possible action space grows exponentially with the number of training actions". The paper is not ready for publication with its current shape.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; The paper casts the problem of 2d fluid flow simulation as an image to image to translation task. Is it the velocity field? The model is evaluated in an autoregressive setting on the problem of fluid flow around a rectangle, and the results are evaluated with image metrics (PSNR).<|endoftext|>The paper uses a cGAN to model the fluid flow of a Karman vortex street in image space. This paper uses a standard implementation of a cGAN on a standard benchmark task for physics prediction. These images correspond to 8 bit discretizations of the velocity and pressure fields.<|endoftext|>This paper reform the task to solve a PDE into an image translation problem and used cGAN to solve the PDE. Although the authors reformed the problem as image translation, using a cGAN (instead of GAN, or other architecture) to generate PDE s result seems to be a trivial idea. In addition, there are other critical issues in the paper. The paper is poorly written with fancy words.<|endoftext|>The idea of fluid simulation as an image to image translation task is really interesting. Moreover, the experiments are conducted on a small dataset, are likely to be overfitting, and are unfair to the selected baseline. The writing in the paper could also use some work. The Navier Stokes equations require both pressure and velocity to be known globally, yet the network is only given pressure and local inflow velocity. The trained network may simply be a "Karman vortex sheet simulator."
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 8; In this paper, the authors propose ProoD which merges a certified binary classifier for in versus out distribution with a classifier for the in distribution task in a principled fashion into a joint classifier. Although this paper proposes a principled method ProoD for robust OOD detection and has some interesting theoretical results, the experiments conducted are not enough to show the effectiveness of the proposed method. Also, the theoretical result (Theorem 1) that the proposed joint classifier gets provably less confident in its decisions as one moves away from the training data is not entirely novel since it is mainly based on the previous theoretical results in the literature; 2. The authors may need to acknowledge this limitation; 3. For example, in Table 2, the performance of ProoD on CIFAR 10 vs. Smooth is worse than that of ATOM and GOOD in terms of AAUC metric, and in terms of GAUC metric, the performance of ProoD is also worse than that of GOOD. The GAUC metric for ATOM and ACET seems meaningless since it is a lower bound and is equal to 0. So it is unclear whether the performance of ProoD is better than that of existing methods or not. 5.I think the authors should give more details about attacking ProoD.<|endoftext|>This paper presents an approach to combine OOD detection and classifier to develop a robust OOD detection framework with high classification accuracy. The authors provide confidence bounds for the noise perturbed and adversarially attacked OOD samples. 2.Authors provide a confidence bound for the classifier which is useful to develop certifiable robust OOD aware models. 3.Experiments on the benchmark datasets show that the proposed approach achieves comparatively high accuracy while maintaining a guarantee on the AUC score. It is not clear how the equations in the figure map to equations in the draft. c. In the experiments section, "semi joint training" requires more details. For example, how are the several models trained with binary shifts? Metrics in figure 2 are not clear from the associated text. 2.Authors claim to achieve better guarantees than the existing approaches (e.g., CCU, GOOD). What are the authors  comments on this. 5.Authors are encouraged to compare with the more recent OOD detection approaches a. Liu et al., "Energy based out of distribution detection"b. Lee et al., "Training confidence calibrated classifiers for detecting out of distribution samples." The proposed approach of combining OOD detection and classifier through joint training is an interesting approach. However, some parts of the paper lack clarity and thus, it is hard to evaluate the contributions.<|endoftext|>This paper aims to detect out of distribution data in an adversarially robust manner. To this end, the authors incorporate a certified (binary) classifier to model in and out distribution and jointly model predictive distribution $p(y|x)$ by conditioning with the binary classifier, i.e., $p(y|x, in)p(in|x) + p(y|x,out)p(out|x)$. **Strength**\The proposed method is sensible and shows consistent improvements.\The paper provides experiments on several detection benchmarks. Rather than that, I suggest utilizing a certified robustness measure with $l_\infty$ extension (Zhang et al., 2021). Additionally, considering AutoAttack (Croce et al., 2020) to attack the confidence $\max p(y|x)$ will be more convincing (as an empirical robustness measure): calculate the maximum confidence over the ensemble attacks in AutoAttack. Due to this paper, it was hard for me to believe that adversarial detection is (almost) free: as it is known to sacrifice the clean accuracy to obtain adversarial robustness (Zhang et al., 2019). * The main technical novelty of this paper is to (1) model predictive distribution with in and out conditional distribution (2) utilize IBP for modeling in and out distribution to model certification of OOD robustness. * However, I believe (1) can be found in (Hsu et al., 2020), and (2) corresponds to (Bitterwolf et al., 2020). *The AUC of ProoD disc seems to be low*. This implies that the discriminator does not capture the in and out distribution probability well. The robustness tends to increase by the network size (Xie et al., 2020).\Is it possible to report the GOOD (Bitterwolf et al., 2020) result in CIFAR 100 and R.ImgNet? I believe it is the main baseline to consider. I believe the evaluation is somewhat questionable and also needs some rigorous discussion with related works.<|endoftext|>The proposed method, ProoD, merges a binary classifier and a multi class classifier in a clever way to produce an OOD detector robust to adversarial perturbation. The presented empirical results seem promising. There are multiple points in the paper that needs to be made clearer. In Table 1, "High clean OOD" is supposed to be "High clean OOD detection performance". Does it hold for all $y$ s? Also, the first inequality needs to be explained. The paper can be made more self contained by including some background knowledge. For example, clear definitions of GAUC and AAUC should be provided. Probably this is why the clean AUC of ProoD Disc in Table 2 is somewhat low. From Eq.(7) and the text nearby, $p(y|x,i)$ is not trained for robustness and the only component that is robustified is the binary classifier. Adversarial attack can also be performed on inliers so that it is misclassified (as in conventional attacks). How does ProoD respond to such attacks?<|endoftext|>The paper proposes a method to provide certified adversarial robustness on the out of distribution. However, IsoMax+ loss outperforms OE in some cases [6]. The authors use the word "robust" in the paper to mean "adversarially robust." Apparently, adversarial robust training is not applied to the multiclass classifier, preventing it from presenting classification accuracy drop. 1._"In contrast to (Bitterwolf et al., 2020) this comes without loss in test accuracy or non adversarial OOD detection performance as in our model the neural network used for the in distribution classification task is independent of the binary discriminator. Hence, we believe that "(almost) for free" may a bit be misleading. In contrast, our loss in Eq.(4) is significantly simpler as **we just have a binary classification problem and therefore only need a single bound**. Moreover, GOOD [3] presents _certified_ adversarial robustness on the **in distribution data**. Maybe we are missing something. For example, provide classification accuracy on adversarially manipulated in distribution data (e.g., see [2]). A drawback of the proposed method is the need to design an ad hoc binary classifier. We believe the authors could be more explicit about these limitations. We also recommend that the authors combine their approaches with the IsoMax loss [4,5] and the IsoMax+ loss [6] rather than SoftMax loss or even OE to start with an improved OOD detection baseline.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; For me, necessary experiments to prove the point of the paper in product quantization are presented, but the gain seems marginal. The paper would be a lot more interesting if there were deeper treatment on this front. There is no need to do an expensive diagonalisation (I think the authors should elaborate the need of diagonalisation   it is not clear from the paper).<|endoftext|>The experiments show that the proposed algorithms are much time efficient and lead to performance improvement in an end to end training of embedding indexes. strengths:A new method to learn rotation matrix in approximate nearest neighbor search in proposed to replace the SVD based solution. It would be great if the authors can find other baselines beside the Cayley for Section 3.2. The paper is clearly organized and easy to follow.<|endoftext|>However, there are some flaws in the technical and experimental parts. One is the lack of discussion of the number of rotations in one step. It seems the authors fail to provide convincing explanations. ## Pros1) It seems novel to consider multiple Givens rotations in one step. It would be better to mention the related methods as well. It is more like a proposition. The paper is well organized and easy to follow.<|endoftext|>Therefore, their proposed algorithm is able to learn the rotation matrix more effectively for the end to end training. In particular, the authors avoided adding some definitions that could have helped a wider range of audiences to understand the paper. The preliminaries notation has been introduced in the 2.2 section but it would be better if it was at the beginning of the method section. Although, the context that they used this method seems novel the method itself is not considered novel. Although the approach is interesting the technique is not that novel.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; ** After rebuttal **Thanks for answering my questions in the rebuttal! (2) to tackle the architecture search in such a setting, this paper propose makes the following contributions. * The paper states that not all candidate models need to be evaluated by the neural predictor, but only those that are “in the search space along the gradient direction”. Based on what I can conclude based on the manuscript I see this paper as a very solid contribution, and I think this paper should be accepted.<|endoftext|>This paper organized a multi task NAS benchmark including 4 widely used datasets and more than 20,000 models. The overall assumption of the NCP is that the search space for model code e is continuously differentiable. The authors trained a three layer MLP as a predictor (function: f: e >y). And how about the performance? The NAS benchmark (NAS Bench MR) contains valuable large scale experimental results and be used to benefit other NAS research.<|endoftext|>Network Coding Propagation is proposed in the paper for NAS on multiple heterogeneous vision tasks. 2.Extensive experiments are conducted. However, very little understanding or intuition about the network coding space is provided. Can we do interpolation between two codes? 2.The paper NAO (Luo et al., 2020) also propose to search on a continuous space. The difference between the proposed method and previous NAO needs more discussion.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces Task Conditional Neural Network for continual learning. The method is based on mixture of experts. Each expert is independent of other experts and therefore the model is not susceptible to catastrophic forgetting. The experiments do not report any indication of the size of the models for the different methods (e.g., number of parameters). The proposed method requires at least one expert per task and is therefore computational inefficient compared with other baseline. On the subject of baselines, the baselines reported in this work are weak. It is unclear to me why some baselines were not reported despite being cited in the paper. Progressive neural networks. Overcoming catastrophic forgetting with hard attention to the task. The results reported (in Table 2) are not competitive with the state of the art by some margin. Nice idea but weak experimental baselines and results.<|endoftext|>This paper studies the task agnostic continual learning problem by introducing a model called task conditional neural networks. The proposed method relies on the mixture of expert (MoE) approach to handle dynamically varying tasks and introduces “probabilistic layers” to predict the task index/assignment probability for each sample. The idea of learning task assignment probability is also reasonable to handle the task agnostic CL problem. If the exact task number K has to be given, it will be a very strong restriction for a model to handle the task agnostic CL setting. 2.It is not clear whether the model can explicitly identify the task boundaries via learning the task probability. If not, the approaches in (5) seem an ensemble of the MoE models training with different augmentation, and the task probability prediction works as the “heuristic attention”, which limits the novelty and significance of the work. 3.Experiments on larger datasets (at least like tiny Imagenet) should be conducted. If I understand correctly, the code link on page 4 should not be included. The basic ideas are well motivated, reasonable, and interesting.<|endoftext|>This paper attempts to proposed a new method called Task Continual Neural Network to address the problem of task identity inference in continual learning. The proposed method estimates the task likelihood by constructing a probabilistic layer based on the idea of mixture of experts (MoE). Experiments on benchmark datasets demonstrate the effectiveness of this method. Strengths:The main idea is easy to understand. Task incremental learning has been proven a much easier problem than class incremental learning. Why a MoE like formulation is good for modeling task likelihood? We can have a MLP with a softmax activation to model task likelihood as well. For example, a task classifier in [1] works also quite well. 5.For the experiments, it is not clear of how to set the augmentation functions and the parameters of the architecture is missing. 6.Also, an ablation study will be very helpful to understand the method. For example, changing the number of experts per task, changing augmentation functions, etc. Overall, I think the novelty of the method is limited, and critical insights and analysis are not enough, both theoretically and experimentally.<|endoftext|>This paper provides a new method to infer the task identity without directly accessing the old data distributions. The proposed method can learn task specific experts with task specific kernels to decide which expert should be chosen and activated under different tasks given to the model. 2.The proposed method takes advantage of Mixture of Experts (MoE), which can use a shallower network for each expert. Cons:It is difficult to infer task identity without directly accessing the old data distributions, and the proposed idea of learning task specific kernels might be an effective solution. The evaluation is on small scale images, i.e.split MNIST and split CIFAR100. The evaluation of more complex domains with a larger scale is necessary since it will be more difficult to learn the data patterns and infer the task identities. 2.Replay based methods are generally more advantageous in class incremental scenarios since they directly recover old data distributions. 3.The authors claim memory and computation efficiency as a key advantage of their method. However, since the entire model is expanding in continual learning, a comparison of memory and computational cost with other baselines should be provided. Post rebuttal:After reading the reply, the author has done partial experiments that show the effectiveness of the method. Thus, I keep my score.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper provides a method for combining contrastive learning and clustering (prototypical probabilities) for three knowledge distillation tasks   supervised model compression, self supervised model compression, and self supervised learning with self distillation. The experimental study justifies the usefulness of the paper. Strength:(i) The proposed method is well described and seems to work as shown by sufficient experiments in various distillation settings.<|endoftext|>Concretely, it models the critic of a contrastive objective by the prototypical probabilistic discrepancy between two features. The authors then carry out extensive experiments on supervised model compression, self supervised model compression, and self supervised learning through self distillation. The empirical results show that the proposed method outperforms other strong baselines. This paper proposes to combine knowledge distillation and contrastive learning for distillation tasks, which is also well motivated. Extensive experiments on benchmarks validate its effectiveness.<|endoftext|>This paper proposes a prototypical contrastive predictive coding by combining the prototypical method and contrastive learning. Experiments are done on applications such as supervised/ self supervised model compression and self supervised learning by self distillation to validate its effectiveness. This paper is well written and experiments are done thoroughly.<|endoftext|>This paper studies a new contrastive loss function for knowledge distillation and self supervised representation learning. The paper is clearly written and well presented. I m not sure the name ProtoCPC is well chosen. 2.The title also could be improved: there is no mention of distillation in the title yet the rest of the paper is all about applications to distillation.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; Specifically, it tests the assumption whether the previous observations about languages with higher morphological complexity are difficult to language model, using a controlled experiment with a n way parallel corpus across 6 languages of interest. The key finding of this paper is a simple one : that the choice of input representation is highly important, given everything else is kept constant, and that differences in language model perplexities can be normalized and made to disappear by choosing the same representation (word/character/byte)This paper contains a series of sound, controlled experiments to study the role of input representations in language modeling. However, I do have a few concerns that prevent me from giving a higher score. Specifically, the work of Gillick at al. ### Edit   22 November, 2021I spent some time reading the discussion between the authors and the other reviewers, as well as revisiting v0.2 of the paper.<|endoftext|>nomenclature for experiments is unhelpful to readers. Figures in this paper are largely unreadable and do not help as visual aids. The main motivation of the paper is exciting, the methods are well controlled, and the findings are interesting. Once this can be fixed, I think its great work, and it would be interesting and directly useful to many people in NLP. My biggest problem with the paper before, were the figures, but I think that the authors addressed my comments to the best of their abilities by adding a lot more clarity to the textual descriptions of the images, in the paper and in the captions.<|endoftext|>The paper asks an interesting question, one that appeared eternally relevant for NLP: Are languages harder or easier to contextually language model by Transformer in light of their varying morphological complexity? Strengths:  Very nice writeup and structure, the paper is for the most part a very enjoyable read. Bold in tackling important questions from a fresh angle. Weaknesses (may be obsolete, see discussion):  The contributions are overstated in light of the experiment setup. I would be much more inclined to accept this (undoubtedly insightful) paper should it insert a clear Limitations section which would reflect on the preliminary nature of these findings given the dataset constraints. Do share with the community!<|endoftext|>Thepaper presents a series of experiments with 6 languages in the UNparallel corpus, using different units (words, sub words, characters,bytes) for training the conditional language models. Existence of "inherent" difference in difficulty of NLP for differentlanguages is interesting for theoretical reasons as well as theapplied NLP. Examples include:      There are some unclear statements like "To which extent is      morphological complexity necessary in computing?" The same goes for the discussion of ZH as      "high resource" language on page 6: it is not clear to me why      this is relevant here, and what is the relation to the overall      aim of the paper. Although the data used is a standard data set, it would be useful  for most readers to have more detailed description of the data  (e.g., the type of documents, size of the overall data that the  parts used are sampled, ...). Footnote marks should be place after punctuation.<|endoftext|>Maybe statistical bias will be a more    better choice. I want to thank the authors for the interesting paper. I have never seen    this parameter in the context of transformer based language models. The weakness of this paper and questions about the paper is listed asfollows:### Not enough novelty compared to prior worksAn important prior work of this paper is Mielke et al.(2019).I findthis paper highly resembles the work of Mielke’s from some aspects. I    understand that using the same set of hyperparamater is for    \"controlled experiments as basic research for scientific    understanding\", but sometimes the paper attributes the performance    fluctuation to the fact that they did not search for better    hyperparameter. What is the morphological complexity of the    remaining three languages? The models used in this work are transformer based conditional    language models, while Mielke et al.(2019) used RNN based    (unconditioned) language models.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; This paper proposes a computationally efficient method to detect adversarial examples in reinforcement learning models. The detection method is based on the curvature of the loss landscape around the inputs, which is shown to have larger negative value for clean examples compared to adversarial ones. The experiments on Atari environment models show the effectiveness of the method. The paper is well written and backs up the experimental results with mathematical intuition and analysis. It also seems that the arguments and analysis presented in this paper could be directly applied to the classification problems as well. In fact the baseline of Roth et al.and all the attack methods used have been designed for image classifiers. With the current attack formulation, the curvature of the adversarial example could end up being lower than that of clean examples, which would be flagged as adversarial.<|endoftext|>The local curvature of the neural network is considered in a lot of works such as DeepFool and it is not a novel idea. Applying it simply on RL domain sounds like a marginal contribution. The mathematical formulations in the paper sounds more decorative rather than adding value to the paper. The simulations and experiments are not comprehensive. The reviewer expects comparison with more baselines. Although they claim that they are the first to apply such an idea (detection of the adversarial example in RL domain), the contribution is marginal as it borrows a lot from the literature.<|endoftext|>This paper provides a fast method to detect adversarially attacked states during reinforcement learning. They also show the method resists even adversary aware attacks fairly well. The paper was well written and it is good that they considered not just a detection method but also tried to attack it. It seems the best adversary aware attacks of SO DATA still performed about as well as adversary unawareattack on the Roth approach. Was the number of random perturbations for the Roth method given? The paper provided and adversarial detection method and a good attempt to break the method. While strongly connected with methods in non reinforcement settings, I m unaware of a similar approachfor reinforcement learning. I enjoyed reading this paper.<|endoftext|>This paper proposes a method on detecting RL adversarial attack. In particular, it uses the curvature of the cost function. The insight is that adversarial states have larger curvature than the normal states. It further provides empirical evidence on the observation and some theoretical motivation. In the end, it evaluates the proposed method against two baseline methods on seven tasks under seven different kinds of attacks (including two adversary aware attacks) and show superior performance. This paper addresses an important topic: detecting adversarial attacks on RL models. Given RL models have been increasingly widely used, it is important to make sure of their robustness. I like the observation on the difference between the adversarial example and the normal examples in terms of the loss curvature this paper makes and how it leverages this observation to develop the method. Sec3.3 further provides a nice theoretical motivation for the use of negative curvature as the criteria for detecting the adversarial examples. The experiment results especially those on detection aware adversaries show the effectiveness of the proposed method. Questions:What is the intuition behind the superior performance of SO DATA compared with Roth et al.?
Reject; rating score: 5; rating score: 5; rating score: 8; This paper considers the ridesharing matching problem and builds the solution upon NeurADP. The main contribution over NeurADP is that the action values of each agent (vehicle) takes into account the impact of its action on the neighboring agents within the same cluster, which is obtained through clustering of the intersections on the road network. The paper presents an incremental step forward on top of NeurADP for the matching problem. Weaknesses and comments  The paper uses the number of completed requests as the problem objective. The proposed estimation of agent interaction effect involves a handcrafted conditional agent probability based on a softmax over distance between destination pairs. I suggest the authors do a more careful ablation study to separate the effect of multiple algorithmic differences. How did you tune it exactly? The paper claims that the algorithm can be executed in real time setting. Once is good enough, and we all know they are well known ridesharing companies. While there s some merit in technical contribution in this regard, there are a number of major issues in algorithmic justification and empirical validation.<|endoftext|>This paper studies an RPM problem (ride pool matching problem) for on demand transportation services. A recent breakthrough, NeurADP by [Shah et al.2020], has shown a good performance, but the proposed approach in the paper, CEVD, achieved much more performance gain (reported as 3.8% 9.76%), which has a significant impact on the ToD service. The idea of considering the effect of other agents seems to be correct and valuable for multi agent decision making problems like ToD systems. I guess that such additional information is helpful for readers. ## Cons  Some notations are not clearly defined, and it is hard to follow the details of the CEVD and its idea. It is hard to follow the details of the proposed method. I want the authors to give more explanations and clarify some parts to help readers, which is also important to increase my review score. (Page 6).Please give an example of such issues. Please clarify the property of this optimization problem.<|endoftext|>This paper focuses on the ride pool matching problem to efficiently allocate combinations of user requests to vehicles online under quality constraints and matching constraints. Intending at this, the authors come up with a conditional expectation based value decomposition (CEVD) method. The proposed approach in the paper considers the impact of other agents actions on individual value by computing conditional expectations to improve the overall performance. The experimental results verify that the CEVD is an effective method for improving the overall requests served by 9.76 compared to the baseline, which seems promising. This needs to be explained clearly and it is crucial to experiment setups. In Section 6 conclusion, I highly recommend to the authors to add more open issues and future directions of their work. In general, the paper makes very solid work and is suited to be published in ICRL.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; This paper investigates the inconsistency problem in LOLA: each LOLA agent assumes the other agent as a naive learner, resulting in LOLA agents not converging to SFPs in some games. COLA empirically shows that COLA finds the consistent solution when HOLA converges and finds more stable solutions when HOLA diverges. **Strengths:** 1. My main concern is COLA s benefit against SOS (Letcher et al., 2019). While this paper finds interesting empirical observations to related works, I am unsure how it improves over the state of the art approach in the literature, such as SOS.<|endoftext|>It displays several issues in previously existing methods, proposes a new methods, and demonstrates some of the features that appear to be superior to the existing methods. The paper focuses on an interesting challenge in learning in the presence of opponents. On the other hand, the contributions the paper makes do not meet the claims made throughout the paper. What if it is not? Second, this is one of the main claims that the paper is built on. Too much is left to future work. The paper is also filled with similar vague language. As stated, only rigorous proofs would answer the questions (and the paper lacks those). Similarly, the discussion of the observations from the empirical results overgeneralizes. After the author response:Thanks to the authors for the response. It unfortunately does not change my assessment.<|endoftext|>This paper deals with the problem of learning in differentiable games. The main contribution of the paper is to point out a flaw in the existing claims regarding correspondence between competitive gradient descent and iLOLA. The paper proposes a new algorithm COLA and shows that it find more consistent solutions empirically. This seems to be an important correction to existing claims. However, empirically COLA does not converge to a a Tit for Tat strategy as desired (or as LOLA does). It seems that the original claims from the CGD paper is series expansion of CGD recovers HOLA (high order LOLA). While I have other minor points, but I think the claim regarding the original claim from CGD paper and its correction should be first made clear.<|endoftext|>The paper investigates HOLA convergence, demonstrates that CGD does not correspond to high order LOLA in general, and proposes COLA to directly address the consistency problem. The proposed method COLA seems more robust to different look ahead values where HOLA diverges. The authors also find that COLA is still sometime susceptible to the ‘arrogant’ LOLA behavior, opening questions for future work. ### Strengths  Paper is well written and provides a well motivated investigation into LOLA’s failure to preserve SFPs and corresponding ‘arrogant’ behavior. Explanation of cases where CGD is not equivalent to iLOLA in general sum games seems important and significant. ### Weaknesses  More thorough proof for CGD argument would strengthen the paper, as well as including it as a baseline in more of the empirical evaluations. Even with explicit consistency loss the authors still find that we still find that the arrogant behavior remains, so the insights from this investigation seem relevant for future work and open questions in this area.
Reject; rating score: 1; rating score: 5; rating score: 6; rating score: 6; The authors introduce a quantum pyramidal circuit to achieve an orthogonal layer of a neural network, which is fast and can maintain orthogonality. The authors implement the orthogonal NN on simulators and quantum machines to demonstrate the effectiveness. 2.The orthogonal quantum NNs are implemented on real quantum machines. Major: The contribution and novelty of the paper are debatable. Training rotation phases in the unitary parametrization space with various robustness/discretization considerations already exist in the literature [5]. Prior work also exists to solve gradient explosion issues in RNNs using those unitary operators [7]. 2.Major: The robustness of the constructed orthogonal layer is debatable. 4.Minor: Training such triangular quantum unitary layers using backpropagation consumes considerable memory as each stage needs to store intermediate results, which is not quite scalable or efficient.<|endoftext|>Comments and concerns:  Reproducibility states "We have explained in detail each part of both algorithms’ implementation, it is in our belief that anyone with classical software skills can re implement the classical algorithm." The authors claim to achieve 98% accuracy on a binary mnist classification task on the real quantum computer ibmq_bogota. This paper uses the latter. The overall idea of pyramidal circuits as neural network layers may have some novel significance. My overall understanding of the evidence supporting the claims in this paper is weak. Despite my greater confidence in understanding implementations, I believe its reproducibility is questionable, and the experimental descriptions are lacking. The experimental results appear to be weak, such as achieving an 85% test accuracy on MNIST. Perhaps I m not understanding the significance of this, or reading it wrong altogether. Section 3 states "Therefore, for any given neural network’s orthogonal layer, there is a quantum pyramidal circuit that reproduces it".<|endoftext|>On quantum devices, it is implemented using 2 qubit, 2 level gates (RBS gates), and on classical devices as a sequence of planar rotations. In both cases, there are $n(n 1)/2$ trainable parameters that also only need $O(n^2)$ in the backward pass. Strengths:  single, unified architecture that can be implemented on classical and quantum devices  simple data loading procedure for quantum computer that does not require assumptions about qRAM  good error characteristics of the unary representation  experimental results showing similar accuracy for the classical implementation, implementation on a quantum simulator, and implementation on a quantum computerWeaknesses:  as the authors note, computing $|Wx>$ for $x$ with $n$ features uses $U_W |x>$ where $U_W$ is a $2^n \times 2^n$ unitary, i.e., one needs $n$ qubits for an $n$ dimensional dataset  literature review for the classical orthogonal NNs is missing some prior work similar to the pyramidal circuit. [1] uses Householder projections while [2] uses Givens rotations, essentially the same building block as the RBS gate.<|endoftext|>This paper replaces usual weights and orthogonal matrices in orthogonal neural networks with an equivalent pyramidal circuit made of two dimensional rotations. The novelty of this paper is high. I think this is a good contribution to the deep learning community. However, the experiments are not very convincing since they lack the comparison of running times. This paper proposed a training method for orthogonal neural networks that run in quadratic time, which is a significant improvement from previous methods based on Singular Value Decomposition.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The authors propose a new convolutional operation designed to optimize the internal number of groups, and kernel sizes for a given model size and performance tradeoff. The authors compare their results with similar seperable convolution methods, and evaluate their methods and show performance gains on common benchmarks. It would be interesting to see this evaluated. The work shows improvement in common Cifar and ImageNet benchmarks. Cons:  The paper is overall somewhat difficult to follow and could benefit from improved explanations throughout. The paper could benefit from more advanced network evaluations. While I found the proposals by the authors to be appealing, I personally found the paper difficult to read, and would encourage the authors to further improve explanations of concepts and overall paper flow, as it was rather difficult to follow at times.<|endoftext|>They evaluate their proposed operator on CIFAR10, CIFAR100, and on ImageNet. I believe this paper provides an interesting analysis suggesting how to configure the hyperparameters in efficient convolutional operators. The novelty from my reading of the paper was mostly confined to the analysis and conclusions around group number. Overall, I would argue that this paper should be a borderline reject. I believe the CondenseNets (CVPR18) may also have a learned group convolution. I understand that your method is more than just determining the number of groups, but especially in the   o  case, not   so , it seems the major contribution is the group number. Also, the experiments are interesting comparing depthwise to optimized, but it seems like all the of architectures are custom.<|endoftext|>This work proposes an optimized separable convolution by optimal designs for the internal number of groupsand kernel sizes for separable convolutions, thereby achieving better trade offs between the model complexity and task performances. (1) Even if we do not discuss the NAS net methods, the author should compare with other efficient CNN backbone designs, not limit the scope to separable convolutions and handcrafted optimization, for example, the IGCV series [1],  the Shift operations [2][3], learning the group strategies [4]. (2) The author should consider more criteria for the "efficient" architecture design, for example, the exact inference speeds (which are more important but listed in the appendix), the memory and time consumption during training [5]. 2018.[3] Chen, Weijie, et al."All you need is a few shifts: Designing efficient convolutional neural networks for image classification." 2018.Overall, the idea and content of this paper are quite clear, the main concerns from the reviewer are the novelty of the proposed method and practical implementation for the CNN community.<|endoftext|>This paper proposed an optimized version of depth separable convolution. The proposed method extend common separable convolutions to more generalized & improved separable convolution. 2.It demonstrated that generalized separable methods surpassed previous convolution methods in performance with a small number of parameters compared with existing separable methods. 2.The authors claimed that by introducing the proposed convolution, the model size could be reduced. 3.In appendix section E, we can see that the GPU inference time of the proposed method is slower than other methods. The structure and representation of their work is good but empirical evidence is not sufficient.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; The paper presents a method for constructing neural networks where the architecture and the weights are derived analytically from the structure of ordered inputs instead of iterative training to provide arbitrary accuracy on training data. Authors make a number of unconventional arguments about their architecture being resistant to catastrophic forgetting, and generalisation ability of the proposed model. So my question is   how is the choice made in the method proposed in this paper? Doesn t that imply that some points (presumably at least the two before and after the new point in the sequence) need to be known for the adjustment?<|endoftext|>This paper propose two new NN architectures, namely TNN, and SQANN. They claim that these networks are resistant to catastrophic forgetting, are interpretable, and are highly accurate. This is confusing and limits the applicability of the neural network.<|endoftext|>This paper proposes two new neural networks (NN) construction schemes that aim at better interpretability, in the sense that (1) the NN should always memorize the training data; (2) the NN can roughly tell if a new test data has any similarity to any data in the training sample. Strengths:The idea of making neurons reflect training data seems new to me. 2.As new as TNN and SQANN look, I highly doubt if constructions alike are really helpful for interpretability issues. The authors also tried hard to give readers enough intuition to understand what they are trying to achieve in this paper.<|endoftext|>3.These two neural networks are resistant to catastrophic forgetting. Contributions claimed by authors: 1. TNN and SQANN are proposed. The contributions are not novel or even not supported. This paper provides two new proof of the universal approximation theorem by construction. Settings of the claimed theorems are different from that of catastrophic forgetting. 2.1.Interpretable constructions for the universal approximation is not novel.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes an approach for intelligent weight sharing in offline RL when the dataset available consists of two groups with slightly different MDPs, and where group identity is known/observable. This work provides an interesting perspective and intuitive approach for solving it. Premise (2) is also plausible, but also poorly justified. That being said, I do think there are some interesting directions here for future work, specifically in settings where there is more than one group and group identity is unknown. This discrepancy is not mentioned, but should be. Indeed, _none_ of the instantiations of NFQI proposed by the authors carry such a convergence guarantee. In the discussion of the related work, the authors claim that their approach differs from Meta RL and Multi task RL by arguing that these approaches deal with different tasks _in addition to_ different MDPs, but the NFQI approach only handles modeling the _same_ task with different MDPs. To that extent, one relevant comparison here would be to compare Multi task RL and Meta RL approaches to the proposed NFQI approach.<|endoftext|>This paper introduces a learning paradigm to handle related MDPs (called Nested MDPs) that share the same structure and definition, varying only in their dynamics. Which was it? The paper is well structured and well written for the most part. *Weaknesses* I have several concerns about the necessity of this approach as proposed by the authors and presented in this paper, as submitted. Second, there is a large body of work on transfer within RL that has been overlooked and should be cited appropriately. Is it the method itself? The specific action definition is also not very clearly outlined.<|endoftext|>This paper considers decision making in nested MDPs where there are two distinct groups having partially shared (but unknown) state dynamics. Extensions to more than two groups could be more generally useful, but are only mentioned in future work and not discussed concretely. Could you elaborate whether this approach will also provide benefit in the online setting, and if not, what makes it special for offline RL? The baselines were not clearly described in the main text: what is the “transfer learning” baseline (first appeared on page 4)? I also hope to see some theoretical support for why $f_{NFQI}(\\tilde s, a)$ is a “good” choice of function class. I would recommend reading and citing the following references [4][5] (plus possibly their followup works) and comment on how the proposed approach is related and where it differs.<|endoftext|>The paper proposes an off policy RL called NFQI (Nested Fitted Q Iteration) as an extension of Fitted Q iteration to estimate group specific policies and account for group structure in the data having two pre defined groups of observations (background and foreground). The NFQI method is validated on a nested cart pole environment where the background is the original environment and foreground includes a constant force that pushes the cart to the left. Separate bars would be better for clarity. The proposed training approach is very similar to transfer learning, where  shared and specific components are trained.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper includes interesting control experiments disentangling the effects of different components. Pros:  very clear and well written paper  very strong results on ProcGen  informative ablations and insights  novel application of model based methods to procedurally generated environmentsCons:  limited novelty of methods   the authors do not mention code release in their reproducibility statementIn section 4.2, is there a reason why MuZero is only trained for 50M while the baselines are trained for 400M steps? Overall this is a strong paper and I recommend acceptance. In addition to strong results on ProcGen, the ablations are quite informative in understanding the effect of the different algorithmic components. My main issue with the paper is that the authors do not mention code release in their reproducibility statement.<|endoftext|>This paper measures the generalization ability of model based agents, i.e.MuZero, in comparison to model free agents. However, they find that these factors do not necessarily provide the same benefit for task generalization. The paper systematically investigates how MuZero, a SOTA model based agent, performs on procedural generalization (in Procgen) and task generalization (in Meta World). However, the experiments and results testing the effect of planning and data diversity for task generalization were hard to follow.<|endoftext|>The work is very clearly presented, easy to understand and presents a number of ablation cases on some important considerations for RL agents, e.g.model free, planning and model learning. The authors present a systematic empirical study of the effect of planning and model learning on generalization performance, using the MuZero agent. Strengths:   The paper is very well written and the motivation and findings are clearly presented and easy to follow. Weaknesses:  The main weakness of the paper lies in that the scope is somewhat limited to an empirical study with only a small number of take away learnings for the community, which are not necessarily very surprising.<|endoftext|>This paper evaluates how well model based RL, specifically MuZero, generalizes in comparison to model free RL. The paper concludes that self supervision is a promising approach to improving the generalization of MBRL agents in procedural environments, but perhaps it does not improve task generalization. Perhaps the results on task generalization are simply due to this? Since multi task RL and generalization seems to be an active research area, I think this is a valuable contribution.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper introduces HyperDQN, a novel exploration technique for deep reinforcement learning, which extends the ideas from an existing method, namely RLSVI, to the deep RL settings. HyperDQN optimizes a linear hypermodel, which is used to compute state action values as a function of extracted features, to work as a module that converges to a sample from the posterior distribution of the Q value function. Empirical study demonstrates significant improvement compared to the competitors on several benchmarks. + The paper is very well written, clear, and easy to understand. The supplementary materials provide most of the information needed for acquiring a better understanding of the paper. + The experimental results show superior performance of HyperDQN compared to state of the art algorithms. Weaknesses:   The proposed approach provides incremental contributions w.r.t.the existing ideas from the previous work. Comments:   The main ideas of the paper has been previously introduced in the literature and is further extended to the deep RL scenarios which make the presented contributions only somewhat novel. The paper can be improved by providing some information and\or intuition about the generalizability of the proposed approach to the other domains. Additionally, discussions about the challenges in extending the proposed idea to the areas such as continuous control, offline RL, etc. One of the main contributions of the paper is addressing the major problems with RLSVI which are restricting its applications in deep RL. While addressing these limitations regarding computational costs of RLSVI is discussed extensively, providing a specific cost function to jointly optimize the feature extractor and the hypermodel might lead to stability and convergence problems. Hence, it would be beneficial if the authors provide further insights into this matter. Additionally, it includes valuable theoretical and experimental discussions that provide further support for their contributions. I thus vote for an accept.<|endoftext|>This paper proposes a new posterior sampling based exploration method, called *HyperDQN*, for deep reinforcement learning. Conceptually, *HyperDQN* is modified based on randomized least square value iteration (RLSVI) and it resolves the limitations of RLSVI on feature engineering and high computational complexity. Technically, *HyperDQN* extends the Hypermodel technique, which is only used for bandit learning in its original paper. The performance of *HyperDQN* is evaluated on Atari 2600 games, SuperMarioBros and deep sea environment. ### StrengthsOverall, I think it is a good trial to extend the idea of Hypermodel to the realm of deep reinforcement learning. Meanwhile, the paper provides a large amount of experiment results to show the superior performance of *HyperDQN*. Meanwhile, the paper is also written in a clear logical flow. One of my major concern is on the novelty. Given the existence of Hypermodel, the *HyperDQN* seems to be a very straightforward application of it on deep reinforcement learning without more insights. Another concern I have is about the performance of *HyperDQN*. Specifically, although the experiments compare *HyperDQN* with BootDQN, it seems that this paper uses the version of BootDQN based on [2]. However, the version of BootDQN in [3] has much better performance than that in [2]. **Honestly, I m not very experienced in running experiments, so I apologize if the above comments on experiments contain some flaws. **### Questions  Does *HyperDQN* contain other insights that are not presented in [1]? Did the comparison experiments use BootDQN in [2] or [3]? When you ran BootDQN, did you use number of ensembles significantly less than the one used in [3]? Hypermodels for exploration. In Proceedings of the 8th International Conference on Learning Representations, 2020. Deep exploration via bootstrapped DQN. Randomized prior functions for deep reinforcement learning.<|endoftext|>The paper presents HyperDQN as a practical algorithm of Randomized least square value iteration (RLSVI). HyperDQN consists of two parametric models: first is a base model that is similar to that of a DQN agent and the second a meta model that parameterizes the last layer of the Q network as a function of a latent variable. The purpose of this architecture is to generate posterior samples using the diverse Q value functions represented through this architecture. The paper begins with a strong motivation towards computationally implementing RLSVI in Deep RL using a base and meta models, and also points out reasons why this is non trivial. Also, the main differences between BootDQN and HyperDQN are also discussed, which is important because BootDQN is a highly relevant prior work in this domain. Overall the paper is well written and has strong motivations that are relevant to the RL community at large. The following are some of the questions based on my understanding of the paper and would be great if the authors could clarify some of them: 1. The meta model in HyperDQN is considered as a linear parameterized model and there is a theoretical justification for this. I am inclined to think that the representation power for the posterior distribution would be better and thus should produce improvements overall. 3.The Atari performance curves summarized in Fig 3 is not sufficient to demonstrate the utility of the presented approach. Also, why are the agents only trained for 20M frames? It is conventional in Atari to run the agents for 200M frames (Machado et al.2018).This makes it easier to compare the performance of HyperDQN wrto the prior methods in Atari. A minor comment: why do some curves in Fig 3 start at 0 and another starts at a value lower than 0? In NeurIPS, 2016. 4.How many random seeds were used to produce the learning curves in all the experiments (Atari, Mario and DeepSea)? It would be useful to add this information in the main text. Also, are the error bars reported in each of the presented learning curves? Overall, I think the contribution would be important to the RL community. My main concerns with the paper is that the presented approach claims to be helpful for exploration but the empirical results are not demonstrating this: One specific way to demonstrate this would be to show and emphasize that HyperDQN is able to achieve better returns on Atari games that are classified to be hard exploration games. This would be a more powerful argument for HyperDQN.<|endoftext|>The novelty of this method is rather limited as the method is exactly the same as the one proposed in a prior work. The empirical evaluation results appear to be less convincing to me because the results on Atari 2600 are only shown for intermediate model trained after 20M frames and the full training scores (inferred from the learning curves in Fig 12) appear to be much inferior than another variant of randomized exploration model which is the noisy net. The principal of HyperDQN is to combine DQN with a probabilistic hypermodel which could generate posterior samples for the parameter values of the weights at the last linear layer of the Q function. This paper proposes a new randomized exploration method termed HyperDQN. It aims to improve the popular Randomized Least Square Value Iteration (RLSVI) method by addressing RLSVI s major limitations of computational burden and its requirement of known good features in advance to adopt the linear setting. Overall, the proposed method could outperform Bootstrapped DQN, Double DQN and another recent bootstrapping baseline termed OB2I in majority of the testified domains. Overall the method of combining hypermodel with Q network is quite sound and very general to be applied on deep RL problems. The empirical evaluation results are very extensive where relevant baselines are considered for comparison. So it is unclear if the method could really be the next state of the art method for Atari 2600 test suite. Only limited number of baselines are considered. My primary concern is about the novelty of this paper. In [1], a more general formulation for the hypermodel has been discussed which includes linear models, neural models and additive prior models. Thus the contribution of this paper is not in terms of introducing a new method but is to show additional experimental results for an existing method on several deep RL domains. I feel such contribution might not be significant enough to be published as a full paper in ICLR. I feel that many other variants of UCB type algorithms, such as noisy net [2] might be able to outperform O2BI or HyperDQN greatly with 200M training budget. For instance, prediction error based exploration methods could effectively progress on the extremely hard exploration task Montezuma s Revenge while O2BI and HyperDQN make no progress on it.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The authors derive exact and non asymptotic (in size or time) expressions for the expected test loss of a linear model during the SGD training dynamics. The implication of the main theorems on batch sizes and learning rates and the connection with neural tangent kernel theory are carefully explored. Numerical experiments are extensive and well thought. Is some Gaussianity assumption made also here? The paper is a nice contribution to the theory of SGD in the quite fundamental linear regression setting. Therefore I recommend acceptance on ICLR without esitation.<|endoftext|>This paper addresses the problem of characterising analytically the dynamics of stochastic gradient descent (SGD) in problems where the data have features with arbitrary covariance structure. All the results are valid for linear models (e.g., random features, neural tangent kernel) trained on the mean squared error loss. Some typos are present both in the text and in the equations. Therefore, I think that at the present stage this work is not ready for publication. The authors apply their theoretical findings to derive heuristic estimates of the optimal batch size and learning rate and study their dependence on the features and target distributions. I recommend the updated version for publication and I have therefore raised my score to 6. Overall, it is well written and the results are presented in a clear way. On the contrary, in this paper the final expressions for the test and train losses are nice and simple.<|endoftext|>The paper studies the learning dynamics of stochastic gradient descent on simple linear models with structured features. In practice, this model seems to be able to predict the training/test error of small neural networks on real data. I believe the presentation of the paper could be largely improved by first explicitly stating the existing work (rather than deferring the related work to the end of the paper) and then introducing the novel contributions of this work. It is important to clarify this assumption since the authors use this model to predict neural network training dynamics. The discussion on the relationship between optimal batch size and feature covariance is new. [1] Which algorithmic choices matter at which batch sizes? Rederiving the loss dynamics with different data structures makes the paper a bit incremental.<|endoftext|>This paper studies the relation between the test error of stochastic gradient descent on training linear models and the structure of the data distribution, the iteration number, and the batch size. Therefore the authors should discuss how the results in this paper are different from these existing works. This paper claims to cover the training of deep neural networks. The theoretical results are correct, and the experiments are also reasonable.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The novelty of the work lies in incremental changes to existing methods, and I think the modifications are substantial. The paper presents a new way of molecular optimization that is based on chemical fragments. These metrics are directly optimized by the proposed model. However, the experiments, in my opinion, do not demonstrate well the impact of these changes.<|endoftext|>The proposed method is shown to be able to generate high quality molecules on single property and multi property optimization tasks. The paper is well written and easy to follow.<|endoftext|>It d be good if it s well motivated and empirically shown. The overall framework is novel, however, it relies on many existing methods.<|endoftext|>The proposed method is evaluated by ligand generation tasks, where generated molecules are evaluated in terms of success rate, novelty, and diversity. However, I am not very convinced by the RL component, because of three concerns raised above. My humble suggestion is to set the generative model of fragments as the main contribution and the RL components as minor ones (or even re using the existing RL component), because the generative model of fragments itself seems to be novel enough (at least to me) to be presented to the community.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposes C MINHASH to improve vanilla MINHASH. I think the paper can improve in the following perspective. Moreover, experiments should also be conducted to compare with these works. The storage cost may not be a concern as one can generate random permutations on the fly without storing them. However, the paper lacks a clear justification of the benefits of C MINHASH and a through comparison with related works.<|endoftext|>This paper proposes an effective approach for MinHash by permutating data vectors. Besides, this paper is well structured, and the theoretical background of the proposed approach is well described in the paper. Furthermore, it experimentally confirms that the experimental results follow the theoretical results. However, I am concerned about the experiment since the paper compares the proposed approach to only the original approach of MinHash. Therefore, it needs to compare the proposed approach to the previous approach to demonstrate its usefulness. It needs to compare the proposed approach to the previous approaches.<|endoftext|>This paper proposes Circulant MinHash (C MinHash) to approximate the Jaccard similarity in massive binary data. The authors also systematically demonstrate that the C MinHash can provide a smaller estimation variance than MinHash. However, the experiments and the related work are a lit weak. Thus, it will be more convincing to show more baselines in the numerical experiments in Section 4 rather than using MinHash only. I suggest the authors discuss more related works and illustrate their difference to C MinHash. In International Conference on Machine Learning, pp.<|endoftext|>~However, I strongly feel that this paper should probably be merged with a separate paper by the same authors on reducing two permutations to one to tell a full story. This is an unbiased estimator. The proof seems straightforward with laborious calculations, which I did not check carefully.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; It is further shown that a method from the literature (DSPN) is multiset equivariant; to make this method perform better, this method is improved with an implicit differentiation approach. The paper is very well written and easy to understand. Technically, it builds on top of the nicely carved out, yet somewhat subtle difference of multiset equivariance and set equivariance. More empirical investigations would be great, but I feel the paper is well rounded as is.<|endoftext|>Deep Set Prediction Networks (DSPN) are identified as the only used architectures for set prediction that satisfy exclusive multiset equivariance with a specific choice of encoder that employs sorting. # Strengths  The paper is well written and understandable. I agree that very similar elements will be a problem for many set equivariant networks that try to approximate functions such as push apart, however, there are also set equivariant networks for which this is not the case.<|endoftext|>The paper proposes the usage of multiset equivariance as a weaker constraint for deep sets. The authors then modify DSPN to be multiset equivariant and introduce the usage of Jacobian free implicit differentiation to speed up computation. Specifically, the push_apart function only serves to show what multiset equivariance is, but I m not sure how this motivates any real world example.<|endoftext|>The authors examine the problem of building multiset equivariant neural networks, and note that there exists multiset equivariant functions which are not set equivariant. Additionally, I was not convinced that multiset equivariance is of practical importance in general problems of interest. The authors propose a strategy to construct multiset equivariant functions (which are not set equivariant) through the Deep set prediction network framework. The authors tackle an intriguing problem in modelling functions on multi sets.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper makes use of the NeRF idea on RIRs (coined here as NAFs). Weaknesses: There are a lot of questions that arise in this work, and I was disappointed to see space being used for unnecessary fluff (wave equation?!!?) Why it is that it works better than using the time domain data? It isn t clear from the writing, but I assume that by "log spectrogram" you mean log( abs( STFT)) and not log( STFT). Clearly a lot of the most useful RIR information is in the phase component. The experiments are really weak. I don t see how these experiments strengthen any point in this paper, they are simple more elaborate variations of your MSE estimate. Minor things: "in principal" ⟶ "in principle"Just as one can learn NeRFs, the authors show that it is possible to also learn (some version) of RIRs. Representing RIRs is a big deal, and this paper can potentially be the first one published to use this approach. If, on the other hand, I m mistaken and you are actually estimating the phase as well, then I would like to see some results with more realistic audio experiments and potentially some quantitative listening numbers.<|endoftext|>The manuscript considers the problem of encoding room impulse responses in a room. Overall, the manuscript considers a very interesting problem, but I am not convinced that the proposed method would be sufficient to provide a solution as implied in the Introduction. To address this challenge, the authors propose a neural network architecture that not only takes into account the speaker and listener positions in a room but also some geometrical information as well. Here are some more specific comments : 1. After all, for each individual room, you need to have many prerecorded room impulse responses to train the neural network. 2.Page 2, "... we propose to condition NAFs on the local geometric information present at both the listener and emitter locations..." : I understand you provide a grid with the emitter, listener locations, as in Fig 2, but how do you form the grid exactly   you write about this verbally, but can you be more specific about the details? 5.It s a bit confusing to say that $\Phi$ in eqn (4) is your NAF   it s the version without geometric features, isn t it? 6.For $\Phi$ in eqn (4), you try to predict the logarithm of the magnitude   how do you recover the phase? 9.Eqn (8) : Isn t the idea to obtain $p_i$ from the NAF?<|endoftext|>This paper introduces the concept of Neural Acoustic Fields, which is an implicit representation to capture how sound propagates in a pysical scene. The idea is new and interesing, but there are multiple overclaims in the paper. The paper overclaims its contribution and there are multiple places in the paper that needs more clarifications or analysis or writing improvement. The background of the room impuse response and environmental reverberation is nice, but the dataset of room impulse responses is captured/introduced in another paper (Chen et al.2020), which is not the contribution of this paper. This is overclaimed, because this is not enabled by the design of the proposed neural network. It is enabled by the dataset, which uses simulation to model room impulse response. Why not just directly use the dataset instead? For equation 6, it is mentioned v is a single scale value taht represents the intensity. But isn t *v* should be the room impulse response? I assume these maps can also be directly generated using the dataset. There are multiple places in the paper that are not clear or need more details.<|endoftext|>This paper proposes an neural acoustic field (NAF), which is an extended version of neural radience field applied on impulse responses defined between listener and emitter positions. It predicts an impulse response given the listener and emitter positions so that one can simulate the impulse response in arbitrary positions. Strengths:The proposed idea is novel and expected to promote other related researches in the future. I assume those parts will not be trained and results in poor prediction. The idea is seemingly novel but it is not enough for publication in this status. I recommend reject. But I m willing to raise the score if the authors answer the questions with more details and write them in the paper.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper presents generalization bounds for meta learning (Corollary 1) that obtain O(1/sqrt{m*n} * log{covering number}) convergence rate. Unlike previous work, the proposed bound can utilize all m*n training samples (n tasks, m samples per task). This is done by imposing the task relatedness assumption on the task environment. The paper makes an important and meaningful contribution to the theory of meta learning. The paper is well written, and the ideas presented are sufficiently novel.<|endoftext|>The paper presents novel generalization bounds for meta learning based on a notion of task relatedness that allows one to compare two tasks by notably allowing a mapping only in subregions where the similarity can be measured in a sense. The contribution is theoretical. The spectrally normalized bounds are interesting and give more practical insight on the performances of some models.<|endoftext|>In other words, when tasks are similar, then meta learning algorithms should be able to utilize all data points across all tasks. The paper is well written and comprehensive.<|endoftext|>The paper:1) proposes a new notion of task relatedness for meta learning termed "almost $\Pi$ relatedness" that assumes isomorphisms between two tasks in the environment. With the big caveat that I am not knowledgeable on recent theoretical work for meta learning to provide a very educated critique of this paper, I felt that proposed $\Pi$ relatedness notion and corresponding PAC generalization bound involving m*n samples is novel enough to make this paper interesting to the ICLR community.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 10; SMN improves GNNs by modeling the joint distribution among discrete node labels; SMN is a promising alternative to CRF as it provides efficient learning and inference procedures. Empirically, SMN achieves better node level and graph level prediction accuracies than several existing models, with minor additional runtime overhead. The model inherits advantages of CRF, such as describing the dependency of node labels and providing probabilistic interpretation. The model is tested on multiple datasets and shows very promising results. The paper is well written and easy to follow. Weaknesses  The model is proposed for modeling discrete node labels on a graph. For neural models that can describe node label dependencies, the paper compares with GMNN only.<|endoftext|>2021.While the paper is well written and the results show consistent improvements, the paper is lacking in terms of any connection made to piecewise training (which the best version of the SMN essentially boils down to, as far as I can tell) and an analysis of why optimizing the proxy problem is superior to the original learning problem. This paper proposes a CRF for classifying nodes in graphs where the CRF has a potential for each node and edge in the graph. For general graphs, computing the partition function is intractable, so approximations are used during both learning and inference. Learning draws from prior work and combines learning pseudomarginals of nodes and edges with GNNs and optionally some steps of "refinement" by optimizing a maximin game equivalent to likelihood maximization. GNNs are used for computing the potentials, one GNN for node potentials and one GNN for edge potentials. The method is general and appears very promising, but currently the paper is very much written for graph problems, so I fear that researchers outside of the graph community may overlook it. Original review follows:Strengths:The methods are well chosen for the task. Why might they generalize badly? No connection is made between the proxy problem and piecewise training. The SMN results are consistently better than the corresponding CRF ones.<|endoftext|>Authors study the problem of node labeling in the inductive case, i.e., at test time the goal is to label all the nodes of a given graph. For that problem, several variants of GNNs and CRFs have been proposed in the past, and the authors propose a Structural Markov Network that uses GNNs to model the potential functions of a CRF with the subtle difference that a proxy optimization problem is solved to make learning more efficient. Strengths* Paper is well written* The proposed method is a novel contribution that combines ideas from CRFs and GNNs. The technical contribution is limited as it builds on old ideas from graphical models. My understanding is that the key to efficiency relies on solving the proxy problem, which, as the authors stated, is an idea that dates back to at least the early 2000s in the context of graphical models. In my opinion, the paper contains good contributions that are worth publishing at ICLR. A score of 7 would reflect better my evaluation of this work as I find the technical contributions to be okay but somewhat limited.<|endoftext|>The paper targets the task of graph node classification in the inductive setting, taking an input node and edge feature representations, and inferring a categorical label for each node. update after rebuttalsI have read other reviews and the authors  replies, as well as extra experiments. The problem under consideration is important, and current solutions have the limitations pointed out in the paper (limited expressive capacity of CRF vs only marginal node classification for GNN models). The model and algorithm descriptions are excellent. The experimental methodology is appropriate, well implemented, described well. The two evaluation metrics (node and graph level accuracy) are adequate and it is an advantage that the proposed method can optimize for either at inference time. Experimental reporting is very good, with error bars. Having to choose between 8 and 10, I have decided to go for 10, as I consider the paper to be much better than just "good". The paper is very good under all aspects. One might argue that the innovation is slim because it consists of a single technical contribution; for this reason this might not be a game changing paper, but it certainly improves on the state of the art by solving a tricky problem.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes a special Gaussian process (GP) named dense GP, to model a mapping between dense local deep features and their corresponding mask values. Based on this dense GP, a few shot segmentation method named DGPNet is proposed. To support this they conduct series of experiments on PASCAL 5^i and COCO 20^i. The authors claim that \mu_{Q|S} and \Delta_{Q|S} are the predicted mean and covariance of the mask of query, i.e., y_Q. However, the prediction of y_Q is fulfilled through DFN (a deep CNN), which means it s not ensured that the predicted \hat{y}_Q can be sampled from a Gaussian distribution with a relatively high probability. 2.Since the input of the decoder is a concatenation of the GP outputs and shallow level query image features, it s necessary to conduct another ablation study to show the influence on the predicted result cast by the GP outputs, i.e., the effectiveness of introducing the GP outputs. 3.The use of 1 4 background foreground weighting in the loss function actually performs hard example mining, which is empirically found beneficial in binary segmentation tasks. However, an ablation study is lacking to prove the effectiveness of the dense GP module alone.<|endoftext|>This paper proposes a dense Gaussian Process approach for few shot segmentation scenario. Overall promising results are achieved comparing to the recent efforts. *lack of clarification of the major contributions. *It is also surprising that despite being a much simpler method as shown in Fig.2 when comparing to existing methods such as SAGNN (CVPR 21), and despite the internal Gaussian representation is very coarse scale and lacking of details as in Fig.2, the final results of this paper are much better. Is there any intuition of why this is the case? *The authors should also present more visual results of e.g.their 5 short segmentation in the appendix/supplementary. *The authors should provide the implementation publicly available. Otherwise it is difficult for others to validate and reproduce the same results and performance.<|endoftext|>Authors propose a novel few shot segmentation method by adopting dense Gaussian process (GP) regression to capture complex appearance distributions. Weaknesses:  The idea that utilizing Gaussian processes in the context of few shot classification has been exploited in [r1,r2]. Although the proposed method focuses on the segmentation task, it seems like a dense classification setting of r1 and r2. The idea of adopting dense Gaussian Process (GP) regression to learn the mapping from local deep image features to mask values, as well as considering uncertainty for the final segmentation is interesting. Although the proposed method seems like a dense setting of the existing methods, the paper is well written and the results achieved are very competitive. I d like to enhance my rating if the authors address all my concerns.<|endoftext|>The paper strives for solving the few shot segmentation problem, they propose to incorporate the Gaussian Process(GP) into the framework of few shot segmentation. Except that, they also exploit the high dimensional output space for GP, the result reaches state of the art in two benchmarks,  one bonus is that  the segmentation quality scales gracefully as increasing the support set size. Extensive experiments to validate the results on different benchmarks  State of the art on two benchmarks, and gracefully extend to high shot segmentation. The author should mention this in the paper. The detail is missing here. For me, it is still not clear how the input are interplayed in encoder. Visualization for 5 shot is not provided, what s the main improvement from 1shot to 5shot to 10shot? The paper overall presents a novel idea with solid empirical validations. Except some minor concern I raised in previous parts. In total, I am inclined to accept this paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper introduces CrossBeam, a method for guiding the synthesis of bottom up search for solving program synthesis problems. The main novelty of CrossBeam is a neural architecture that leverages the search context to decide which program to evaluate next in the bottom up search. Empirical results in string synthesis problems and ILP show that CrossBeam is superior to other search methods in terms of number of programs evaluated. The paper has two main weaknesses: the proposed method is much slower than other methods from the literature and the search algorithm can stall during search and not make progress, despite being allowed more computational time. I agree with the authors and also see value in the kind of work that is presented in this paper.<|endoftext|>The paper presents crossbeam, a bottom up program synthesis with neural guided search. Training is done on policy using beam aware training. Thorough experimental analysis shows very promising results across two important benchmarks from the literature (string manipulation and inductive logic programming). Are these generated them using a beam search with a beam width of 50,000? Overall, I think the paper presents a novel and interesting approach that shows significant improvement, however there are still engineering challenges related to batching.<|endoftext|>The paper proposes a method for program synthesis from input output examples using a bottom up search method. I vote to accept the paper considering the strong empirical results and the interesting method. This policy uses all of the available information at each step of the search, including the sub programs identified so far, to decide which sub program to explore next. The authors evaluate the method on string manipulation and  inductive logic programming benchmarks, and show strong empirical results compared to prior work.<|endoftext|>The paper introduces CrossBeam – a variant of bottom up enumerative search algorithm for program synthesis guided by a pointer network. An interesting and well written paper that suggests a solution to a long standing problem in program synthesis from input/output examples. The neural network is trained on policy, utilizing search histories and intermediate program executions as context. The method is evaluated on the program synthesis task (programming by example setting) in two different domains: string manipulation and logic programming. CrossBeam significantly reduces program search space compared to state of the art, and improves accuracy compared to previous art. I believe the search history can be maintained with left to right models. It would be interesting to compare CrossBeam beam aware training to an RL based on policy training method like e.g.proximal policy optimization (PPO) method.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The proposed algorithm, CDS via unsupervised kernel classification (CDSK), then minimizes the between cluster discriminative similarity. In fact, the maximum margin based clustering methods are implicitly based on such a framework, and it seems nice to explicitly formulate this framework in this paper. This paper also provides strong theoretical results for the generalization error bound of unsupervised similarity based classifier using Rademacher complexity. I particularly appreciate the detailed discussion in Section 5, which places CDSK in a clear position in the literature and explains its significance. Weakness: It could add more value to this paper if the authors provide more examples showing why the derived discriminative similarity is better than conventional similarities, such as the regular kernel similarity and the similarity used in the sparse graph (especially subspace) based clustering literature.<|endoftext|>This paper proposes a discriminative similarity clustering method via unsupervised classification and provides the generalization bound for the similarity classification. The paper is technically solid and it provides the theoretical analysis of the generalization bound. It would be better to compare with some more recent state of the art clustering methods. Maybe the authors want to compare with other methods w.r.t.computational complexity? But I do not find such comparison. It would be better to show the comparison results of the  computational complexity and running time on the used data sets. Moreover, in the Caption, it says "$c$ in the left column is the cluster number", but I do not find $c$ in Tables 1 and 2. Although there are some concerns, the paper proposes an interesting and effective method and also provides some theoretical analysis.<|endoftext|>In contrasts with kernel similarity with uniform weights, the induced discriminative similarity with learnable weights enhances its capability to represent complex inter connection between data. Positive:(1) Under the framework of CDS, discriminative similarity is induced by the generalization error bound for unsupervised similarity based classifier. The authors conduct a complete and detailed theoretical analysis, and the results provide theoretical guarantee on the discriminative similarity can be induced from kernel density classification. (2) Moreover, based on the CDS model, the authors develop a clustering algorithm termed Clustering by Discriminative Similarity via unsupervised Kernel classification (CDSK). As is known to all, there exists a lot of classic similarity learning paradigms, such as metric learning methods and subspace learning methods. It is suggested that the authors make a deep analysis on the related work and summarize what are the key differences between these efforts and the proposed method in this paper. The paper contributes some new ideas and the motivation of this paper is clear. This paper proposes a new clustering framework termed Clustering by Discriminative Similarity (CDS).<|endoftext|>This paper proposes a clustering algorithm by learning from the weighted similarity of each data partition. Then, the upper bound of excess risk of this classifier is provided by Rademacher complexity. They show that the weighted kernel similarity is a special case of the general framework, and they apply it to discriminative clustering by iteratively optimizing the class labels and the weights of similarity. The authors also provide theoretical results to support the propose algorithm and give the strict proofs. The expected loss in Eq.(4) seems to be not matched with the corresponding empirical loss. Moreover, the authors should explain how the empirical loss in Eq.(4) can be transformed into Eq.(5).The detailed derivative process should be listed in the appendix.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper presents a model over sequential structural tree edits, used for program repair based on ASTs. The significance of this is a matter of reviewer opinion; I did not personally find it sufficiently novel. In short, while I appreciate the significant effort that went into this paper, I found it comprised of many parts that fell just short of convincing execution. This feels marginally interesting   and the ‘CodeT5 small’ model seems to perform very similarly to the model in this paper, with about the same order of magnitude of parameters. As it turns out, the performance of all of the better performing pre trained models seems to be about the same between these.<|endoftext|>The paper proposes a new approach to abstract syntax tree based automatic program repair. The novel technique called deleted subtree reconstruction is based on dropping parts of the syntax tree and training the model to grow them back. I am missing a discussion on why the presented work is still relevant despite that. * The anecdotal examples are not compared to the kind of results other approaches give, so it is unclear what to make of them.<|endoftext|>This probably deserves a comparison. Some concerns about the scalability given the need to encode the entire tree at each intermediate step. The latter probably deserves more analysis if this is to be considered a part of the contribution of this work: did you attempt to pretrain with removing larger subtrees? Please focus on these in the response/future revisions. The first main contribution over Hoppity is the use of a multi head scheme in the GNN encoder. The eventual results are not particularly strong compared to many other baselines, which, while equipped with more parameters, also use models that scale better with larger parameter budgets. A similar problem is present in E.3. Overall, the paper uses a natural approach to this task, but much of it is similar to prior work, and the new components feel inadequately analyzed. Besides these salient issues, several more problems, some minor and others moderately significant, are identified below.<|endoftext|>The paper presents GRAPHIX, a graph edit model for program repair. The work has also proposed a pre training task to improve model performance. Empirical study covers many recent works and is fairly comprehensive. I lean towards accepting the paper. The problem is much worse on medium sized data. Here are two questions:	 1. Is it possible to add such an experiment? Can we add more diverse tasks to let the model to predict a variety of edits? The last paragraph in Section 4.1 shows that there is a single head "base" model, which is not mentioned in any experiment in the main paper. I found the "base" in Appendix but it would be better if you move this description into Appendix as it is not that relevant.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper considers the problem of how to provide a collection of counterfactual explanations for a binary linear classifier such that an informed choice can be made on how best to actualise a new input based on individual preferences such that a different classification is likely to be made, even under potentially changing model parameters.<|endoftext|>In particular, the authors focus on a linear classification setting, based on which, they provide a lower and upper bound on the probability of joint feasibility of a given counterfactual plan, a correction method to improve the lower bound, and a framework to construct a counterfactual plan that satisfies certain optimality. I have a question about originality. Moreover, in this paper, the authors only study the joint feasibility. Minor issue: The conclusion section is missing. It would be better to have it. Post rebuttal:Thank you for the feedback.<|endoftext|>Also, can the authors please add intuition as to what it means for the distance in distribution (mean, covariance) of model parameters to be bounded up to a distance of $\rho$? Overall I found the paper interesting and relevant. It is also not used later, at least not directly ($\mathbb{B}$ is used).
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper proposes algorithm for learning coarse correlated equilibrium (CCE) and correlated equilibrium (CE) of multi player general Sum Markov Games, the propose algorithm has polynomial dependence on the number of state and the horizons. Strength: The paper presents the first sample efficient algorithm for multi player general sum Markov game and prove its convergence to CCE and CE. Though there are many *similar* types of paper in the field of theoretical reinforcement learning (i.e., a new setting with new sample efficient algorithm etc.), I found this one interesting. Overall, I think it is interesting paper with solid contribution. I vote for acceptance.<|endoftext|>This paper studies the problem of finite horizon multi agent general sum Markov games. The proposed algorithm CE V Learning achieves $\epsilon$ coarse correlated equilibirum (CCE) using $\tilde{O}(H^5S\max_{1\leq i \leq m}A_i/\epsilon^2)$  episodes, and $\epsilon$ correlated equilibrium (CE) using $\tilde{O}(H^6S\max_{1\leq i \leq m}A_i/\epsilon^2)$  episodes. The sample complexity of  $\epsilon$ CE for general sum games is significant and worth publishing.<|endoftext|>The paper studies general sum episodic Markov Games. This happens due to ``independent updates" of the agents. This is technical paper with very impressive results. Note though that the updates are not simultaneous, in the sense that one agent updates at a time. I feel this paper should get accepted!<|endoftext|>This paper studied the sample complexity for learning the coarse correlated equilibrium (CCE) and correlated equilibrium (CE) of m player general sum Markov games (MG), as well as learning the Nash equilibrium (NE) of Markov potential games (MPG). The results in this paper are quite interesting. Strengths:+ The sample complexity of learning Markov games is a timely and important topic in the areas of multi agent reinforcement learning.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This work aims to study the influences that lead to the systematic generalisation in linear models. Systematic generalisation in neural networks is not a new topic and there has been a lot of work on this subject. Thus, I find the results up to page 6 not particularly novel or interesting. The authors introduction and related work section consists mostly of very recent work so a more systematic reading of the work following Fodor and Pylyshyn s critique may be adequate. That said, I found the iterated learning section interesting, albeit rather brief and I m not sure I fully understood the setup. It is not clear to me how this conundrum is resolved. I find that the paper tackles some interesting questions but does not provide enough value to be accepted.<|endoftext|>The theoretical results demonstrate that systematicity is difficult to guarantee, but that structural modularity and iterated learning are helpful to improve systematicity. Similarly, it is not quite clear at this point what are systematic inputs vs. outputs, and systematic features have not yet been very precisely defined. These clarity issues make it quite hard to understand the implications of the plots in Figure 2. 1.The submission formalizes a simple data setting in which systematicity is controllable. My main concern is the applicability of the insights to more standard settings (W1). "...however, a number of studies have identified situations where depth alone is insufficient for structured generalization ()" missing reference? I also found the rest of this paragraph hard to understand at this point in the paper. Citations should be added as there is much work on this topic.<|endoftext|>The paper studies the relation of learning dynamics and architecture structure to the acquisition of systematic knowledge in shallow and deep linear neural networks. To this end the authors study the network behavior on a space of parametric datasets composed of systematic and non systematic features and assess the impact of training regime, architecture, and iterated learning on the learned by the network functions. Finally, beyond the linear study, litter commentary is given on how to address the nonlinear case, apart from a simple CMNIST experiment. I understand that the focus of the paper was on the linear case, however, it questions how impactful the findings are (especially in the light of the above critique) in real world scenarios. Technical questions: What is the use of k_x>1?<|endoftext|>Strengths The paper investigates the subject of systematic generalization in neural networks, which is important and not well understood yet. Similarly, the size of the hidden layer (when there is one) may have an impact on the learning dynamics, and it may be good to address it. This paper does provide a benchmark and preliminary findings, that could be interesting to build on, but I think this paper does not have enough substance yet to be published at a conference.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Then, the second focus of the paper is on exploring the behavior of LAQ in a variety of domains and settings. I found the problem setting to be both technically and conceptually interesting. The two presented theoretical results are quite compelling in their own right, if quite natural in hindsight. For instance, in the paragraph "LAQ value functions allow transfer across embodiments", it is stated that "LAQ densified rewards functions, speed up learning and consistently guide to higher reward solutions than sparse task rewards, or D3G". I include several writing suggestions below. This seems important for determining how to perform the latent action inference. [C.1] Including the pseudocode for LAQ would benefit the paper a lot. It would be helpful to state this explicitly in the Lemma in the main text. I found the idea of a "fundamental action" to be quite sharp and added a lot of clarity to thinking about the proof.<|endoftext|>Is it calculated using state value functions on a batch of data? Experiments in a variety of settings, including reward shaping and planning with low level controllers, validate the effectiveness of the approach. Is there learning in the actual environment? For the theoretical result, I find that the conditions needed to satisfy theorem 3.1 may be too stringent. The variety of different evaluations to assess the method was welcome. The paper tackles an interesting topic and proposes a simple approach which performs well empirically. Why is that?<|endoftext|>This paper analyzes the problem of learning from experience tuples thatomit the action. Ground truth actions are said to be an upper bound on the    performance of your method, but in Table 1 for Kitchen manipulation    the proposed method outperforms this upper bound. Strengths     An interesting approach to the problem of learning from action less    experience. This is in part due    to the obscure nature of the problem (of learning with action less    experience), partly because the related work is not well    contextualized earlier in the paper and partly due to the    experimental evaluation. It is not clear to me that correlation is a good    performance criterion in this case. The theoretical argumentsthat motivate the proposed algorithm are well thought out. This is true, but the state value function can be    learned without actions already. Edit: After discussion with the authors, I have increased my score to a 6.<|endoftext|>They present a theoretical result that motivates their subsequent method for finding the true Q function from just the given data. While the problem setting, i.e., state only offline RL, is realistic and practically relevant (cheaper that classic RL), and the paper does contain some interesting ideas in terms of theory and algorithm, the paper is too premature. :Motivation: I think the motivation is somewhat overstating the difficulty of the problem. Some comments on definitions, problem formulation etc.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; Strengths:+ This paper conducts a large number of experiments and presents a comprehensive instance segmentation literature review. + The figures are very clear. Weaknesses:  I don t see any technical contribution for the method presented in the paper. It simply alters the dataset with existing style transfer method and perform other literature methods on the altered dataset. The main experiment that this paper presents is to use the style tranfered dataset. I prefer to reject this paper. My main concern of this paper is the lack of contribution to the community. And the new data setting doesn t make a lot of sense since the groundtruth is encoded in the image and there s no user cases for such setting.<|endoftext|>In this paper, the authors study whether deep learning based techniques for instance segmentation are robust to changes in object texture or contour. They use a stylized version of coco but also create two different types, one stylizing only the objects and the other stylizing only the background. Finally, when the entire image is stylized, the boundaries get mixed up and there is very little signal in the image (as can be seen Figure 2), so the detector does not work in this case. Overall, it remains unclear to me if the method is adding any insights compared to what is not already expected. Also, its unclear if there is something special about using stylized transformations to make claims that object detectors are learning shape/contour specific features   the conclusion which I am taking here is “the more noise we add, the worse object detectors get”, more specifically, noise everywhere > noise on object > noise on background. Other Comments:Figure 6 is very dense and hard to read. The authors should include 20 30 examples of stylized images in the paper or the supplementary material. This will give better insights to the reader about how the dataset looks in different cases.<|endoftext|>The paper also covered many of the popular instance segmentation frameworks and architectures. My main concerns are the following:1  The paper is heavily influenced by the work in Geirhos et al.to the point that makes this paper a small incremental work. (e.g.yolact is consistently more robust against stylization compared to other architectures, yolov4 csp is more robust compared to rest of yolo models, and Swin being the most robust when combined with maskrcnn framework). 3  Since the paper is heavily relying on the stylized images for evaluation, and since the models are trained with different data augmentation strategies which might have influenced the robustness against stylization, I am surprised that the authors didn t study how data augmentation would influence the robustness metric. For instance, would we consider yolact more robust because of some data augs used during training, or is it because of something else?<|endoftext|>If the authors can well address them. Then, a large number of experiments and evaluations are performed on this new dataset to study the impact of framework, architecture and pre training. I would consider raising my score. The experiments conducted are extensive and it provides some suggestions of instance segmentation model design for environment robustness. Thus, I decide to increase my rating to 6. 3.The paper is presented in a clear structure and easy to understand. However, in real life, the robustness for instance segmentation should be more about the adaptation to novel objects [a,b] in a long tailed distribution world or how to handle overlapping objects with similar appearance in heavy occlusions [c,d].
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper presents an approach for learning diverse temporally extended and reusable options. It is based on the assumption that learning diverse options are generally useful for downstream tasks. The main idea behind the approach is to discover options by maximizing the mutual information between the options and the corresponding state transitions and demonstrates the options discovered by this approach. Also, the paper demonstrates that these options are useful for faster learning in a downstream task. The paper presents an extension of the termination critic idea, which is called the InfoMax Termination Critic. Specifically, the approach is different from Termination Critic as it involves computing the entropy over the final states as a function of the option and the start state (termination critic only computes the entropy of the final states as a function of the option). The overall approach is clear and well presented. I do have a number of questions related to this approach and would like some clarifications from the authors: 1. The Fig 2 presented shows qualitatively how the terminations for options look for IMTC and Termination Critic. It would be much easier to make a comparison with the published result as opposed to reimplementing termination critic and demonstrating the differences on a new but smaller gridworld. Is my understanding correct? This seems like a roundabout way of discovering options that are diverse. Why doesn’t the approach learn option policies where the rewards for those options are obtained through the discovered termination functions? 3.Would it be possible to make Fig 3 more prominent? 4.Because the approach is very much related to Termination Critic, I think it is important to include Termination Critic as a baseline in the large scale experiments (the plots in Fig 3 and Fig 5). Overall, I think the paper addresses an important problem in RL: discovering diverse options when there are no external tasks. However, some of the design choices made by the approach does not seem to be reasonable. Also, using VIC in this way makes it unclear as to whether the performance gains are due to the proposed approach or due to VIC.<|endoftext|>This paper proposed an algorithm, termed infomax Termination Critic (IMTC) to learn diversified options in RL. This algorithm learns termination conditions of options by maximizing mutual information between options and corresponding state transitions. The experiments demonstrate the IMTC algorithm learns diversified options and can be reused in various tasks. Strength: 1.	the method is well motivated and derived 2. Empirical study in various domains show diversity of the learned options and its usefulness in transfer settings. The method is directly derived from the termination gradient theorem from Harutyunyan et al.(2019) with reward signal constructed based on mutual information between terminal state and option. 2.Failed to mention and compare some relevant references, E.g., reference [1] also considered learned diversified options using pseudo reward constructed based on divergence between action distribution of different options. [1] Kamat & Precup, Diversity Enriched Option Critic, Neurips2020[2] Ramesh et al., Successor Options: an option discovery framework for reinforcement learning, IJCAI2019I would like to see some discussions of these references and experimental comparisons. Proposition 2 seems simply a trivial extension of proposition 1. In the experiment (5.1), why the policy over option $\mu(o|x)$ is fixed to be $1/|O|$, instead of learned? In all, this paper studied an important problem in reinforcement, learning diversified options that can be reused in various tasks. Their empirical study is able to justify the diversify of the learned options and shows superior performance in transfer learning settings comparing to some baseline. However, there are still some key references missing and should be compared against. Also, the method seems trivial extension of many existing methods.<|endoftext|>The authors propose an objective for learning a diverse set of options in which the goal is to maximize the entropy of terminating states from any initial state while at the same time minimize the entropy of terminating states given a specific option. Intuitively, this means learning options that tend to be deterministic, while at the same time are able to reach diverse terminating states. Overall I enjoyed the paper, it is well organized and well written, and it touches in a topic that I personally find very interesting which is options learning. Straight forward objective with a nice intuitive explanation. Evaluation done in a set of diverse tasks. Weakness:  Maybe a concrete definition of what s meant by diversity in this paper could help. The objective encourages the options to learn different terminating states, but not different trajectories. So, based on the objective two options that "move right" for 38 steps, and one moves "up" on step 39 and the other moves "down" on step 39 would be diverse, and would maximize the objective. Looking at the results, it s interesting to see that "reach" tasks with large action spaces is where the proposed method tends to underperform. Does the complexity of the task or the action space play a role in how difficult it is to learn options? How many times did you re run the experiments to make that claim? I would like to see some more specific numbers around these results. This is a generally well accepted hypothesis, but the way its written it makes it sound that no one thought of it before. The results look promising, but the discussion and analysis could have more depth; there are questions left unanswered on this paper.<|endoftext|>The authors propose an HRL algorithm that uses the VIC objective to discover the options, i.e., the termination condition of the options is trained to maximize the mutual information between the set of options and their terminating states. Later on there are experiments that show how an RL agent can re use these options in downstream tasks. The general idea of using the termination gradient in VIC is novel and interesting. Since the VIC paper was published there were quite a few papers extending it, like DIAYN and RVIC to give two examples. More concretely, the implementation of the option model is not clear to me and is not well justified. The theoretical derivation suggests using a model that predicts the probability of seeing s_f after executing option o from state s. However in practice, this model is not used, and it is replaced with the discriminator as being done in VIC, which is a different kind of a model. Furthermore, using this model instead of the option model makes the methods much more similar empirically to VIC and much less novel in my opinion. I think this should be clarified and discussed in much more detail, e.g., what is exactly the difference from VIC. Section 4.2 makes some decisions that are not very clear to me and I would have like to see some experiments supporting them. I don t see it as a major part of the paper though and it might have been better to put this section in the supplementary. if it is on trajectories executed by the policy, then shouldn t the state occupancy be infinity for all non transient states? In addition, including more baselines would have made the empirical contributions stronger.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper introduces a system called MT3 that performs music transcription with various types of datasets. In the experiment, the system showed state of the art performance in all the relevant metrics and on the selected datasets, which are comprehensive. Considering every note a single instrument? It s great that even Melodyne was included in the experiment. Appendix A seems to be a bit too simple to me. T1 "small" model   does it really define everything about the model? Very solid work.<|endoftext|>This paper seems to be a great milestone in the AMT research. It is probably the first unified AMT model that can take music audio with an arbitrary number of instruments.<|endoftext|>This paper proposes a multi task multi track music transcription framework. Music transcription task has mainly been tackled individually for each instrument type. However, in this work, the authors jointly trained the model using several datasets with different instrument types. The contribution of the paper is mainly written in the "Summary of the paper" part, I will write some questions in this section. I think multi instrument music transcription task can be regarded as two parts which are transcription (addressing note) and classification (instrument).<|endoftext|>The authors combine numerous automatic music transcription datasets to devise a training framework. The work may open an exciting path towards studying "data valuation" for automatic music transcription, e.g., which parts of the data are more informative, inconsistent, or erroneous. The paper is well written. The literature review is extensive   even though there can be several additions (see the specific comments in the main review). Towards end to end polyphonic music transcription: transforming music audio directly to a score. Automatic music transcription and ethnomusicology: a user study.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The comparison between InfoLOOB and InfoNCE losses (Section 2) are carefully made and technical sound (myself didn t rigorously go through the proof, though)   Good empirical improvements over CLIP. **Weaknesses**   Contribution / novelty is small: neither InfoLOOB nor Modern Hopfield Network is new   In addition to image text contrastive learning, would love to see how it applies to other contrastive learning tasks (e.g.image image)While empirical results are good, the overall contribution of the paper seems fall below the bar. It simply applies a combination of two existing techniques to image text contrastive learning, and doesn t show how it broadly applies to other contrastive learning tasks.<|endoftext|>This paper proposed a new contrastive learning method called CLOOB, which minimized the leave one out upper bound (InfoLOOB) on mutual information with the modern Hopfield networks. In particular, CLOOB is easy to be implemented in practical applications. The novelty of the proposed method seems not big enough. From this perspective, it seems like that the proposed method is just a feature engineering algorithm, whose novelty, in my opinion, is not convincing enough. Rigorously speaking, Theorem 1 and Theorem 2 in the main paper need citations as they are just the theoretical results from previous works. Authors of CLOOB could not use such theoretical results as their own.<|endoftext|>In this paper, the authors propose CLOOB, short for "Contrastive Leave One Out Boost", where modern Hopfield networks are used together with the InfoLOOB objective. This has not been investigated before. c) The authors use InfoLOOB as the objective. d) What exactly is a modern Hopfield network? Eqn.(9) and (10) tried to explain this, but they are not clear enough to me. 2) Experiments:a) From results in Figure 1, it is not easy to see clearly variance of InfoLOOB is reduced via modern Hopfield network. It will be interesting to see how the performance gap changes while making the visual backbone stronger.<|endoftext|>Positive points:  The method is clear and the paper well written and structured. Ablation study A1 does seem to demonstrate that the objective introduced has strong synergies with the Hopfield network approach. This justifies the usefulness of the different components. While it is true that the authors design a learning algorithm that functions well against their chosen baseline, overall most of the components (the InfoLOOB, modern Hopfield networks, ...) are not intrinsically novel. I would still recommend acceptance as I do not feel lack of technical novelty alone should detract from the empirical merits of this paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 3; Overall, this paper lacks technical and theoretical novelty. It would be better if the authors present the results of the common metrics as well, because existing methods are mostly evaluated on these metrics. In this sense, the proposed method is performing the worst. The baselines are rather naïve and insufficient to justify the advantages of the proposed method.<|endoftext|>Are they one hot encoded, treated as integers, ...? The paper is otherwise clear and generally well motivated, and covers an important business use case. I appreciate that the authors used a public dataset in addition to their private dataset.<|endoftext|>The novelty of this paper is limited. In this paper, the authors propose a CNN based model with multiple outputs/losses. Some figures are not informative. For example, more details should be added to Figure 3 and its caption to make it clear.<|endoftext|>This reviewer believes that this paper would be better suitable for publication in a more applied venue   as the contribution to technical novelty, particularly the methodology proposed and the application of the study is very limited. As the authors are not comparing their proposed model with a baseline CNN, there is an uncertainty in their claim that the performance boost can only arise from incorporation of the auxiliary layers. The paper is well written and easy to follow.<|endoftext|>There are two main problems that make the paper not ready. Indeed, in my opinion, the paper lacks novelty as it can be reduced to applying a CNN based approach to the problem of time series regression. Indeed, there is a continuous discrepancy between the terms multivariate time series for the data and one dimensional CNN for the model.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; Results in Table 1 seem convincing. I think it is not accessible for a wide audience. After carefully reading it for hours I still do not understand the main intuition behind the proposed approach: Why this way? The paper discusses and proposes generic approaches in a general setting, but then only empirically evaluates them in the task of speech vocoding, which is in a quite particular domain (speech) and, more importantly, corresponds to a strongly conditioned task (that is, a task where the conditioning carries a lot of information; for instance, it is known that tasks like that require much less sampling steps for diffusion models). Even with a reduced set of synthesis utterances? I think the proposal of the paper is sound and the developments are correct.<|endoftext|>This seems very different compared to the $\pi(x_t)$ described above Prop. How does this distribution then predict $x_{1:t−1}$ given $x_0$, as indicated? Since the noise schedule is learnt specifically for synthesis with fewer steps, the work addresses an important problem of denoising diffusion models, their slow sampling speed. The proposition does not explicitly compare the derived bound with the regular ELBO bound from DDPMs in terms of how tight the bounds are, unless I am missing something. I would recommend including such a section in the paper. Overall, the paper presents a new idea for learning the inference noise schedule in generative diffusion models and the experimental results are promising. My main concern is that I found the presentation and writing of the paper lacking in clarity and difficult to follow, as indicated by my questions above. However, these are different distributions (one defines a forward process, the other the backward posterior).<|endoftext|>This speech synthesis paper proposes to learn the forward diffusion process and reach better ELBO values by doing that, as well as facilitate inference with fewer steps while keeping good generation quality. The paper compared the proposed method to some strong baselines, including WaveNet, GANs and two recent diffusion methods: WaveGrad and DiffWave. The MOS results for the proposed method show no significant difference from the ground truth (same as DiffWave) starting from 7 steps while being and an order of magnitude faster than the best baseline DiffWave. Pros:1.The idea to find the optimal forward process is interesting and novel (modulo concurrect work like VDM https://arxiv.org/abs/2107.00630 which attempts to reach the same goal). 2.The experimental results persuade that the method indeed works well with few inference steps. There are a lot of technical details which make the paper hard to read. I am inclined to accept this paper, because it contains novel ideas and the experiments are solid. Writing would profit from simplification though.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; This paper proposes an importance sampling method for estimating the likelihood of observations in stochastic differential equation (SDE) models. Given the numerous advantages of the proposed method over existing works, I am inclined to recommend acceptance of this paper. ** Low level details necessary to reproduce the results from this paper are not all included. ** The main technical novelty comes from the application of Theorem 1, as summarized in Algorithm 1. In particular, the authors propose to estimate the original SDE of interest with an linear SDE, which is an attractive alternative since it is easier to estimate their probabilities and expectations with importance sampling, and has the added benefit that the sampled paths pass through the observations.<|endoftext|>The method relies on Girsanov s theorem, which requires evaluating the density of the observations under Wiener process. The experiments show that the method indeed learns accurate drift and diffusion functions much faster than standard integration based approaches and has much smaller gradient variance+ The paper indeed addresses a timely topic. + The results seem pretty impressive. However, the experiments are done only on very few dimensional systems. A few notes to further improve the readability:      What does the sentence in line 48 mean? More details on the GP SDE model in Sec 3.3 would be nice. The proposed approach concerns an interesting topic and provides excellent improvements over existing approaches. However, I m not able to mathematically verify Theorem 1 and the derivation in Section 2.2. This is why I preliminarily vote for a reject but would be very happy to re consider my score if additional details are provided by the authors and discussions with other reviewers.<|endoftext|>The paper proposes an importance sampling method for the estimation of probabilities in SDE models, applicable to SDEs with state independent (or transformable to state independent) diffusion. Results also suggest that the proposed method is faster than integration based methods and the gradients have lower variances. Below are my specific comments. * The scope of the work is limited to state independent or transformable to state independent diffusion. * Why gradient variance results is shown on a separate dataset than learning curves? * Line 46: "Models that implicitly describe f as a Gaussian process"  please provide examples and include citations. * Theorem 1: please spell out the conditions of Girsanov s theorem explicitly and what exactly should hold for function f and sigma. In the current form, it s not clear how exactly the experiments are done.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper suggests some interesting modifications to the resnet architecture. However, the changes aren t very novel, and the paper did not convince me that these new adjustments are beneficial. The results in the paper appear to be (slightly) negative, but there is no proper ablation study for them to be informative The paper suggests an adjustment to the resnet architecture, which enables successful training despite having deterministically initialized weights. Weaknesses:* (Significance/ novelty) Getting rid of batch normalization was already achieved by fixup, in a very similar method to what the paper presented. The experiments do not show a benefit for using the new modified resnet over fixup. Other works, like [SkipInit](https://arxiv.org/abs/2002.10444) and [ReZero](https://arxiv.org/abs/2003.04887) also offer similar methods to improve convergence speed and get rid of batch normalization. * (Significance) The paper pride its initialization on being deterministic, but this is poorly defined. Comparing standard deviation over 5 seeds does not give you a statistically significant result, this could easily be the result of random chance. * (Novelty) Initializing residual networks to only zeros and constant was already done in [this work](https://arxiv.org/abs/2007.01038). This paper cites this work as requiring "random noise to improve the performance". As far as I can tell, the degeneracy problem there is a direct result of the choice of padding $\mathbb{I}_{1,2}$ with zeros when the network is wide. I am also confused by why this problem is different from the first dead neuron problem (Skip connections obviously can t solve dead neuron problems for neurons it is not applied on).<|endoftext|>In this paper, the authors propose some modifications to the classic ResNet architecture and initialization scheme, so that the network can be trained with fully deterministic initialization, even without any normalization layers. The paper introduces a novel method to initialize a ResNet like architecture deterministically with binary weights (except for the Hadamard transform where additional rescaling is required). The paper contains some novelty in its model design, has good clarity, but does not show convincing practical significance compared with previous work. For example, identity initialization of convolutional kernels was presented in Xiao et. #### 2.The experimental results do not show performance advantage of the proposed method over previous ones such as the original ResNet and the Fixup initialization (in the no normalization setting). ##### 1.2 What are the initial weight variances of different initializations (as it could be that they start from very different distributions)?<|endoftext|>To initialize a ResNet with only zeros and ones, this paper analyses the problem of all zero initialization and then proposes a  method named ZerO to achieve the goal by augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms. However, it seems to introduce a new structure rather than only initialization, and empirical results are not really good. Pros:  This paper is easy to follow. The proposed method is interesting and somewhat novel. Therefore, the proposed method is not a pure initialization for ResNet. There is no description to mention it. In addition, ZerO performs worse than Fixup. It is suggested to change notation like $W^2$ to $W^{(2)}$. Additional Questions:  As a fully deterministic structure, the initial weights are the same with different seeds. Why is the std not equal to 0? What else makes an effect on the training? Therefore, is it weird that Kaiming is worse than Xavier in Table 2?<|endoftext|>This paper proposes a new initialization scheme for ResNet. In Section 2.2 2.3, the authors explain in a similar manner why Hadamard transform can deal with degeneracy problem caused by the dimensionality expansion. The authors compare several initialization schemes on the standard ResNet models on CIFAR 10 and ImageNet and show theirs achieves competitive results in terms of convergence speed and test accuracy. The motivation of the zero and one initialization is unclear. What are the theoretical advantages of removing randomness in initialization? Empirically, despite the authors have shown some comparison with He and Xavier methods, the zero and one initialization does not show *significant* advantages. There is no unique way to avoid the dead neuron problem. For a trivial example, one can simply add a shortcut connection from the dead neuron to a output neuron and one from a input neuron to the dead neuron. The extra shortcut connections and Hadamard transform result in additional computational cost and model complexity. As they are cheap operations, I consider this issue a minor one. However, it is contradictory to the claim in the original paper of He et al, which states "We argue that this optimization difficulty is unlikely to be caused by vanishing gradients." The paper is technically correct but lack of strong motivation, theoretical advances and empirical performance.<|endoftext|>The paper proposes a deterministic weight initialization scheme called ZerO for residual networks. ZerO works by first augmenting existing ResNet architectures with extra skip connections and Hadamard transforms and then initializes network weights with only zeros andones (instead of random weights). The authors show that the proposed scheme has various benefits such as improving reproducibility, training networks without batch normalization, and improved performance. One drawback of ZerO is that it can only be applied to residual networks. 2.Can the authors also provide a comparison with the baselines for space time complexity? I think that the number of skip connections added is significant which may add to the complexity. However, there are some concerns such as the space time complexity of models and limited applicability.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies the role of activation functions (via their multiplicity at the origin) in the condensation of neural networks at the initial training stage. Condensation can be viewed as a feature learning process, where the wide network can be described effectively as a narrower network and the input weights condense on isolated orientations during training. Moreover, using polynomial approximations, the paper provides theoretical support in two cases: when the activation function is of multiplicity one and when the input is one dimensional. Strengths:   Provide an interesting explanation of the condensation mechanism through the lens of multiplicity of activation functionsWeaknesses:   Theoretical analysis is limited to the case of one dimensional inputs and activation functions of multiplicity one   Experimental demonstrations are somehow limited   could be further improved by considering how similar phenomenon occurs for a range of other datasets and for other loss functions (cross entropy, etc.) Some experimental demonstrations are provided to support the claim but the theory is somehow limited. I believe that the paper could be further improved and will go with a weak reject for now.<|endoftext|>Essentially, it is found an empirical link between the multiplicity of the activation function and the number of condensation directions. My major concern with this paper is that I found the experimental session not fully convincing. These assumptions should be made very clear in order to avoid overstating and confusion in readers. Second, the paper focuses on claiming to analyze the behavior of the neurons activations at initialization and early stages of learning, with small weights. In this respect, I could find that the experiments use a number of 100 (or 1000) epochs, which seems something different from initial stages of training. [edit after the revision]I would like to sincerely thank the authors for the effort in improving the manuscript. I am going to increase slightly my evaluation of the paper. Thanks.The quality of the paper is hampered by  not fully convincing and unclear experimental analysis  low readability due to several typos in the manuscript  unclear improvement wrt the work in Luo et al, 2021<|endoftext|>This well written paper takes a step forward in understanding the implicit regularization in neural net optimization. Broadly, this work is intriguing, but could stand to benefit from a few improvements, suggested below. The biggest thing that I would like is a better characterization of the experiments they run. It is important that the authors did indeed show that at least for CIFAR10, a network with small initialization achieves comparable performance to a more standardly initialized net and does so in a similar amount of training time. Presumably this condensation on one or a few axes is not seen in these cases, but I d be curious about the authors speculations about this. The fact that beige was an anti correlation, not orthogonality, took me longer to realize than it should have. A few small typos:  pg 2: "quantitatively theory explanation"  > ?? pg 4: "full batch expect for..."  > "full batch *except* for..."This intriguing paper demonstrates, through experiment and analysis, a connection between the implicit regularization of certain networks and the multiplicity of their activation function.<|endoftext|>It showed theoretically and empirically that the maximal number of condensed orientations in the initial training stage is twice the multiplicity of the activation function under the small initialization of weights. Strengths:  The paper is well written, organized, and easy to follow. The experimental part is comprehensive and a detailed visualization of condensation orientations is provided. The conclusion that the number of condensation orientations is twice the multiplicity of the activation function gives an possible explanation why small initialization works as implicit regularization at initial training and is insightful for diving into the nonlinear dynamics of NN. Weaknesses:  For experiments on real datasets such as MNIST and CIFAR10, the output dimension $d_\text{out}$ is still 1. However, as Figure 2 displays condensation orientations for activation functions with the multiplicity>1 such as $x\tanh(x)$ and $x^2\tanh(x)$, I think the analysis of $p 2,3$ can make the conclusion more stronger. While it can be improved by conducting more experiments as well as analysis of more complicated cases, it is above the acceptance threshold.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; In this paper, the authors introduce a novel audio visual dereverberation approach. They propose a Visually Informed Dereverberation of Audio (VIDA) model for dereverberation. The authors also create synthetic/simulated datasets and real world data for experimentation. Finally, they show the impact of the proposed VIDA model on several speech tasks including, recognition, enhancement, and speaker verification. The results are encouraging. The main contribution of this work is the use of visual information as an auxiliary input for dereverberation. While authors suggest visual information is helpful, it isn t reflected in the results. In Table 1, if we compare the "Audio only dereverb." This raises the question about the effectiveness of the proposed approach. I feel the majority of the dereverberation is still learned by the audio model. This should be investigated. I believe the manuscript in its current form offers a very limited contribution to the research community, and the contribution of visual information in dereverberation is minimal.<|endoftext|>This paper examines an audio visual approach for dereverberation, where dereverberation of speech is conditioned on RGB and depth images (either field of view or panoramic). The method is evaluated using PESQ, WER for speech recognition, and EER for speaker verification on synthetic and real data. The audio only version of the model outperforms several baselines from the literature on synthetic data, with mixed results on real data. S2) The proposed dataset seems quite useful. Overall, I vote for marginal acceptance. This is an interesting novel task (audio visual dereverberation), and the proposed synthetic and real data are quite interesting and useful for the community. W4) The improvements by adding visual information are marginal compared to the audio only model. To benchmark the audio only baseline against the literature, it would be interesting to compare to competitive systems in e.g.the REVERB challenge. I m also not very convinced that visual input really helps that much.<|endoftext|>Based on the observation that a visual scene captured by a camera conveys information that is related to room characteristics, the authors propose a visually informed audio dereverberation method that aims to extract clean, anechoic speech from reverberant speech. They then train deep neural networks that take as inputs both visual data (RGB and depth images) and audio data (reverberant speech), and output clean speech. Through the experiments with several downstream tasks for speech, they showed that the proposed audio visual dererberation method outperforms the baseline models, both for synthetic and real world test data. With or without using a reverb visual matching loss was included in the ablation studies, but the effect was not significant compared with other methods. The performance was better with the audio only model even in speech quality metric (PESQ). Another point that I hope to be added is the comparison of the model efficiency. This paper proposes a multi modal learning framework that is applied to speech dereverberation.<|endoftext|>This paper describes an audio de reverberation method which integrates (panoramic, rgb+depth map) visual input with a spectrogram U Net to estimate a de reverberated spectrogram and recover the original clean signal. The proposed method is compared to several baseline (audio only) de reverberation methods on speech enhancement, recognition, and speaker verification problems using both synthetic and real data. There is a brief mention of astronomically low p values (1.56e 60?) which I expect are primarily driven by the size of the test set, and not the size of the effect. If this is the case, it would be interesting to see how performance (table 1) differs (if at all) in the presence or absence of a direct path. I think some of the reporting could be done better, but this is a minor complaint.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; View it as a kernel paper. They should clean up their experiments a little, but this is overall encouraging as a pragmatic way to reduce the time and space complexity of real kernel algorithms by large constant factors. The experiments are largely convincing. This suggests the GMM assumption in the theory was just a simplification to make the math tractable. The experiments suggest that TRF is notably faster and uses less space than RFF. Just want to bring up a possibly interesting connection existing in the literature.<|endoftext|>I like the ternary quantization idea and it seems to work great in the practical experiments. The theoretical results are interesting but they work under quite restrictive assumptions. I have concerns on the quality of presentation of the theoretical results.<|endoftext|>I feel the authors should spend more space to argue that. The main concern is the importance of this finding (have a constant scale improvement in computation time and storage memory).<|endoftext|>Finally, they validate the accuracy and efficiency of the proposed random features by several experiments. The asymptotic results are interesting, and the proofs seems to be correct. The authors are expected to provide more examples to illustrate the applicability of these assumptions. 2)	The presented asymptotic theory requires the dimension of input space to approach infinity $p \to \infty$ is unfamiliar in practical. In the existing random features literature, $m$ is crucial to the approximation ability and generalization ability. 4)	The kernel hyperparameters usually determine the performance of kernel methods, but the proposed random features approach seems to be independent from kernel hyperparameters and only depend on the kernel type.
Reject; rating score: 3; rating score: 5; rating score: 5; Weaknesses:  The proof does not bring much new insight that are relevant to explanation robustness. Problem studied are good, but theory and experiments have room for improvement. This gives the "best case" robustness in the explanations given perturbation in the input x. In fact, the definition of the predicted/estimated astuteness is not given.<|endoftext|>The definition of astuteness introduced as a criterion for assessing the reliability of explanations seems to be a natural and useful definition.<|endoftext|>The paper focuses on the reliability of explainers, where an explainer should give similar explanations for similar data inputs. The paper essentially talks about the robustness of an explanation method that has been extensively studied in the XAI literature [1,2. However, the notion of stability/robustness/astuteness has been extensively studied in the existing literature and the novelty of the work is thus a bit unclear.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper presents (to the best of my knowledge) the first convergence analysis of IBP training, a method commonly employed to train networks that are certifiably robust to adversarial examples. While the assumptions that lead to the results are fairly restrictive, I believe the results are of great interest to the community and could lay the ground for further work in the area. In particular, relying on similar assumptions and derivation to (Du et al.2018b), the authors extend the proofs to the case of the upper bound to the robust loss as given by IBP. It is proved that IBP training converges to zero certified robust loss with high probabiliy.<|endoftext|>This work provides a theoretical analysis on the convergence of IBP training on overparametrized networks. This paper explores the training dynamics of IBP training and provides a convergence analysis. The authors then proceed to establish various relationships with between the (time dependent) network weights, and eigenvalues of a matrix governing the training dynamics. Eq.12: Clarify that the ‘ in l’ denotes the derivative.<|endoftext|>Several of the recent techniques involve or extend the interval bound propagation (IBP) technique. However, there is no work analysing the convergence of IBP even in its simplest setting. In this paper, the author(s) analyse the convergence of IBP in a simplified setting for the first time. Although I am aware of the line of research “certifiably robust training of neural networks”, I have not seen any work on the analysis of convergence of the gradient descent algorithm, especially in the IBP setting. The mathematical steps look correct to me. ** Overall, I believe the paper does not introduce the literature thoroughly, includes multiple typos, and has mistakes in the terminology. Even in the presence of these assumptions, the results are “with high probability”.
Reject; rating score: 3; rating score: 6; rating score: 6; It applies this network to the problem of learning word analogies. While the paper is generally reasonably clear, I feel that it takes a long time to get to the main idea, which is on page 5 6 of 9. So it s surprising to me that in the methods section, \phi, a critical component of the model, is left undefined. This paper describes a simple and interesting method for modeling unordered data that has some nice theoretical properties, like generalization from smaller to larger sets. Experimental results are mixed, with the proposed model performing the best on BATS but not on the Google analogies dataset.<|endoftext|>This paper presents a method of learning word analogies by training a network to explicitly model analogical relationships using an architecture based on invertible projections, which they show induces an abelian group on the input space. The approach is motivated well, the proposed abelian network is very interesting and could have applications for other tasks. The authors only consider only word embedding setup (word2vec) which is several years old and many other better performing embeddings (on word analogy task) have since been proposed such as fasttext or glove. 3.Why is this problem interesting in itself?<|endoftext|>The authors introduce Abelian group networks (AGN) that explicitly model the operational relation between elements in an Abelian group. Theoretical proofs for the claimed properties regarding the AGN s ability to model Abelian relations and multisets. I think the authors could improve the strength of the work by any of the following. Do we have a better dataset, or even a better task, to showcase the benefit of the proposed approach? How are the examples chosen? However, MLPs used in the paper seem to be particularly poor.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 10; In this paper, the authors introduce a novel approach for training a score based generative model. For example, would the method work if they had an acceleration term in which the noise was injected? In the present paper, the authors present a novel forward process, where diffusion is run in a joint data velocity space. The paper shows that the novel scheme (CLD) yields higher quality for image generation when compared to prior models of similar capacity and number of neural network evaluations.<|endoftext|>This paper proposes a critically damped version of score based diffusion models, by extending the inference process to an augmented state space and diffusing the data coupled with an auxiliary velocity variable. The authors further proposed a hybrid score matching loss for training the reverse generative process, which provides an empirical advantage of learning a conditional score function that does not blow up (e.g.to infinity) and therefore stabilizes training. My recommendation is on the lower band of 8: accept, good paper. Strengths of the paper include * Novelty: although it is a straightforward extension, the authors explored different aspects of the central idea of augmentation/acceleration and proposed a new loss that is smooth and a numerical method that works better for certain settings. 4.In 3.3, Leimkuhler & Reich, 2005 was cited for Euler’s methods not being suitable for Hamiltonian dynamics.<|endoftext|>The paper proposes a novel class of continuous time diffusion based generative models. Thus, I m inclined to accept the paper. In my understanding, dropping weights for training diffusion based generative models prevents modeling unbounded scores at $t$ s close to the data, where the weights are extremely high. However, observing that training the proposed model via DSM is unstable, the paper proposes a modified objective called hybrid score matching (HSM). In addition, as the reverse time diffusion is also a Hamiltonian SDE, the paper proposes a new integrator for generations, benefitting from the symplectic structure of the Hamiltonian dynamics. Thus, the proposed integrator will have a better quality of samples, esp. However, it is unclear how important is this "mixing" in the context of diffusion based generative models. 1.First, the paper proposes a novel class of continuous time diffusion based generative models, called critically damped Langevine diffusion (CLD). 3.Third, for the proposed model, the paper proposes a variant of denoising score matching, called hybrid score matching (HSM), observing that the training of CLD based models is unstable by using DSM.<|endoftext|>The authors propose critically damped Langevin dynamics (CLD) for score based generative modeling. This consists of a higher order Langevin dynamics scheme with particle velocity and position coupled to each other, as in Hamiltonian dynamics. A corresponding score matching objective is derived as an objective, with proof given that it is simply necessary to approximate the score of the velocity given the position. [2] Mou, Wenlong, et al."High order Langevin diffusion yields an accelerated MCMC algorithm." I assume this is a typo?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper deals with complete verification of neural network, based on a branch and bound framework. In addition to that, the paper also propose new branching heuristics:  Active Constraint Score Branching, which uses the lagrangian variables associated with the constraints of the multi neuron relaxation to prioritize branching. This numbers come from the experiments that are listed in Table 2, but this is only a comparison to different ablated version of the authors method. I am familiar with seeing verification for eps 0.1 or eps 0.3. This paper combines existing techniques to improve the performance of complete verifiers. The contributions to the bounding process (incorporating the additional constraints with Lagrange multipliers) are relatively minor.<|endoftext|>The paper presents a novel framework for neural network verification by combining two popular paradigms in literature: tight multi neuron relaxations [1][2] and branch and bound [3][4][5]. Previous branching heuristics make branching decisions on expected bound improvements. What were the results? I agree OVAL does not support residual architectures.<|endoftext|>The paper puts forward a method for the formal verification of ReLU basedneural networks. Page 2. greater 0  > greater than 0.kkkWhilst the paper introduces some novel branching heuristics for the completeverification of ReLU neural networks via branch and bound, the overallcontribution lacks in my opinion the sufficient novelty to merit an ICLRpublication. Experimental results are reported that comparethe procedure with related methods on a number of  MNIST and CIFAR10 models. Whilst the authors claim that the efficacy oftheir method is better observed in bigger networks, they do not includecomparisons with related tools on bigger networks.<|endoftext|>This paper presents a branch and bound algorithm for neural network verification that uses a dualized variant of an existing approach for generating multi neuron relaxations. * p2: I think you need to add a "sound" to "A method is called complete if..."? The "novelty" of the work is somewhat incremental, but the new methods show their efficacy, and the paper is a worthy contribution to the literature. Can the authors clarify?<|endoftext|>In general I like the directions that this paper is taking to push the state of the art of neural network verification. The contributions are novel to my knowledge. The two ideas on branching heuristics are also interesting. However, I m not fully convinced of the efficacy of the rest of the proposed techniques. 4.What are the number of ReLUs in the ResNet architecture and in the ConvBig/Small networks? Overall, while I find the proposed techniques promising, a more thorough evaluation is needed to show their benefits.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper studies time series outlier detection. The analysis suggests that non recurrent methodologies should be prefered as they perform marginally better while requiring fewer parameters and training. Is the contamination repeated in each fold? How is the contamination performed? The current figures do not show clear performance gain from any method. Is one approach to be prefered if static methods are used. The authors present a substantial experimental work that offers interesting insights on time series outlier detection.<|endoftext|>The paper compares non RNN and RNN methods in the outlier detection context, and concludes that non RNN approach is suited to point outliers while RNN approach is suited to collective outliers. Relevant existing non RNN methods for outlier detection on time series are not included in the study. I find this not sufficient.<|endoftext|>This paper presents a comparative study of the performance of non recurrent models and deep recurrent models for time series outlier detection. The crucial concept   data complexity of time series data in outlier detection   is not properly defined and quantified. The authors claim about the data complexity in terms of the presence of some complex outliers, such as collective outliers, but detailed analysis of this complexity through some ablation studies  is missing. It is a purely comparative study with only some shallow results. Overall, the work seems to be a technical report that contains some preliminary results but has many key issues to be further addressed.<|endoftext|>The paper evaluates a set of methods for time series anomaly detection, including RNNs among the considered techniques. Nothing is wrong with that, this is just far from complete. There are dozens of methods to consider. Many questions of this form: are CNNs better? ....Preliminary results of an evaluation work for time series anomaly detection. Motivation has to be reconsidered.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This work extends the theoretical results from Cohen et.al to the input dependent setup. In particular, the derive the theoretical conditions for the validity of data dependent randomized smoothing through a similar procedure to the one in Cohen et.al. Strengths:   The main merit of this work is the theoretical contribution in section 2. It is quite useful deriving the conditions under which the input independent smoothing is theoretically valid. 2.The writing of this work along with the presented figures can be significantly improved. 3.There are several parts that need more elaborative discussion. Regarding the first point:  While the motivation of this work is the "certified accuracy waterfalls" the proposed objective in Equation (1) does not solve the problem. There are several notations introduced within text such as $a :  \|\delta\|$ and used later without referring to the same variable. al.It is clear how the objective in (1) could be used during training, but it is not clear how is it used at test time. The discussion in the paragraph below (1) is a little vague. However, there are several weaknesses of this paper that need to be addressed including the experimental setup, the effectiveness of the proposed method, and the presentation.<|endoftext|>This paper studies the input dependent randomized smoothing technique   it adds isotropic Gaussian noise with different variance on different input x to give robustness certificates. The main theorem looks reasonable to me. In terms of smoothing, I think it totally makes sense that when we are certifying a single image, the $\sigma(x)$ function should not change too much within a small radius $\| \delta \|$. Since the variance of the Gaussian noise is the only thing we can change and they cannot be changed too much, the proposed input dependent smoothing scheme in this paper is not very useful. I rate the paper at borderline because I feel the theoretical part of this paper has certain limitations and the proposed procedure is also not very effective in practice and has quite small improvements. To further improve the contribution of this paper, as I mentioned in the "weaknesses" above, the authors can study the more useful setting of applying non isotropic input dependent noise, and develop better $\sigma(x)$ function to improve smoothing performance.<|endoftext|>The authors study input dependent adversarial smoothing i.e.the setting where the smoothing distribution has noise level $\sigma$ dependent on $x$. Weaknesses:The main weakness lies in the fact that this is a negative result   it doesn t provide any real improvement over Cohen et al.2019, and the authors acknowledge that this method doesn t scale to modern ImageNet scale datasets. It would additionally be helpful to make clear that the $r 0$ case in Table 2 corresponds to clean accuracy. It s unclear how the authors derived Equation (1)   is it possible that there exists another choice of $\sigma(x)$ that could yield superior empirical results? It seems unlikely that training with a fixed noise level would be the optimal way to train input dependent smoothing models   have the authors tried, for example, changing the noise level of the Gaussian at training time to be input dependent as well? This paper presents theoretical and experimental negative results on input dependent randomized smoothing. I think it s likely to be of *some* interest to those working on randomized smoothing. But the wider community will likely be uninterested by this negative result. (Additionally the paper could use more work on the experiments and writing in particular).<|endoftext|>The authors propose an input dependent randomized smoothing method with non constant input dependent $\sigma(x)$. I thereby recommend the paper. It is challenging to formulate such input dependent RS with a valid certification, and the authors successfully address the problems. The paper is well written in a logical order. Lemma 1 and Theorem 2 describe the worst case decision region for $\sigma_0$ and $\sigma_1$. If we know upper/lower bounds on the $\sigma_1$, it is easy to obtain a certified radius. However it is not quite true as stated in Section 4 regarding the results in Table 2 and 3 (the last paragraph in page 7). Except for the above two comments, the paper has no significant weakness in its content, but there are some major/minor presentation issues that must be addressed before published. "Using non constant \sigma(x) defined in Section 2, ..." (page 2)      It is defined as eq 1. in Section 4. ### Others  Compared to other parts, the curse of dimensionality part is hard to understand.
Reject; rating score: 3; rating score: 5; rating score: 6; This submission is on modeling feature interactions for CTR prediction. It outperforms the baselines and is deployed online. Pros:  The algorithm is deployed in a real world production system. Cons:  Not enough novelty. More details about the experimental setup may be needed to assess if the setup is fair. It seems possible that the so called state of art baselines are not well tuned.<|endoftext|>The paper proposed a module DPO for CTR prediction to enhence the explicit and implicit information. The authors claimed that they provide the first attempt to extend the dynamic neural networks to CTR prediction, and experiments show that DPN (Dynamic Parameterized Network) significantly outperforms other state of the art methods. Cons:1) Even though I think the proposed method is technically sound, the results cannot convince me. For the experimental results shown in Table 1, some baselines run worse than their normal performances, and the result of Dynamic Parameterized Network seems not good enough, e.g.compared with the results in [1]. The idea is somewhat novel, but the result is not good enough.<|endoftext|>Clear illustration of the methodology as well as discussions about the relations with existing methodsWeaknesses:1. Table 7 is way too simple, and so are related descriptions for the experiments. More details would be helpful to make the A/B test results stronger. The current high level discussion is confusing. It s a methodology paper working on an important problem (CTR prediction).
Reject; rating score: 3; rating score: 5; rating score: 8; rating score: 8; This paper generalizes randomized smoothing to certify robustness against complex semantic transformations. To do that, they construct a surrogate neural network mapping images to images, to approximate complex semantic transformations and certify robustness with respect to this network. In this work, the authors first construct a smoothed classifier $G(x)$, based on smoothing the parameter of the parametrised transformation $\tau$. Then they proceed to construct a surrogate image to image translation model approximating the transformation $\tau$. The surrogate transformation then leads to a more concrete form of $M^*$ (Theorem 2), which is further simplified by assuming that $F_1$ is an affine form (Corollary 1). The main ideas of the paper are interesting and to the best of my knowledge novel, particularly the surrogate network. How do you decompose the transformation to get a resolvable and a non resolvable part? Can the same be done for GSmooth? What about the other works? Where do the authors show how they calculate their $\epsilon$ for the surrogate transformation networks as needed for Theorem 3? The authors claim multiple times (abstract and later in the paper) that existing work can not handle complex semantic transformations. I could not find a proof or some further justification for this.<|endoftext|>This paper proposes a more generalized form of certified robustness and attempts to provide new results on applying randomized smoothing to semantic transformations such as different types of blurs or distortions. The main idea is to use an image to image neural network to approximate semantic transformations, and then certify robustness based on bounds on that neural network. The authors provide empirical results on standard datasets like MNIST and CIFAR showing that their method can achieve improved results on some transformations compared to prior work. I appreciate the authors taking the time to attempt to respond to the concerns of all reviewers, and for updating and improving their work during the rebuttal process. I am glad to see that they do provide empirical evidence of improvement to common corruption robustness, compared to AugMix (one of the state of the art approaches for standard common corruption robustness) and TSS, although I can not tell how the authors derived their baselines (I can not find references to the AugMix accuracy numbers that the authors provided in their rebuttal in the TSS or AugMix paper). As the authors and I agreed upon in our discussion, the main novelty is not improvement for resolvable transformations (prior works that the authors cite perform about the same or better), but rather, is the ability to handle non resolvable transformations. I agree that robustness to non resolvable transformations is important; however, certified robustness to non resolvable transformations is not meaningful to me, because they are only being certified with respect to a neural network that is trained to approximate those non resolvable transformations. Without MTurk studies to confirm how good the neural network s non resolvable transforms are, I do not find certified robustness here meaningful, because it does not necessarily correspond to anything concrete that we can understand.<|endoftext|>The key idea of the work is to use a neural surrogate for the semantic transformations, and add noise in the latent space of the surrogate for randomized smoothing. Their approach proposes to decompose the resolvable and unresolvable parts of the semantic transformation by lifting the data+transformation parameters into a larger augmented latent space defined by an image to image network. The paper still needs some additional work however, specifically in terms of quantifying the tightness of their proposed bound against semantic attacks. The authors provide theoretical and empirical evidence for GSmooth and show improvement on contemporary methods. The topic of certifying generalized semantic transformations is extremely relevant. However, if the authors show empirical evidence of their certificates against semantic attacks, I will be glad to improve my score. 2.GSmooth is able to certify several non trivial semantic transformations which include difficult combinations such as rotational and defocus blur. 3.While the certified accuracy values are not that significant for the more complex transforms, the algorithm is good first attempt which succeeds as compared to other methods. Specifically, the GSmooth classifier should be tested against an adaptive attack that has access to the surrogate and the parameter space to find if the bounds do actually hold.<|endoftext|>This paper proposed GSmooth, a generalized randomized smoothing method for semantic transformations. The main technical contributions are:(1) Introduce the use of an image to image translation network to provide a unified framework for the analysis of non resolvable semantic transformations. (3) The empirical performance is superior to existing methods on most transformations. More importantly, the method can certify many new transformations that are hard to analyze based on existing methods. In my opinion, this work shows ground breaking results in certified robustness, by providing an elegant and general framework for robustness certification with arbitrary and possibly complex semantic transformations with performance guarantees. 2.Since the proposed method can handle composite semantic transformations, can the authors some case studies to demonstrate such an advantage? The contributions are significant for expanding randomized smoothing to more complex semantic transformations
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; This manuscript proposes HIG, which is a middle point of two popular and individually advantageous optimization methods. 2.It seems to me that the loss function in the toy example should be $l(y,\hat{y},\lambda) \frac{1}{2}(y^1 \hat{y}^1)^2+\frac{1}{2}\lambda(y^2 \hat{y}^2)^2$, or did I miss something? Also the notation (superscripts and subscripts) are confusing. This manuscript fills a blank for optimization in physical deep learning with a nice and clean method. The presentation is relatively clear to me. I would recommend an accept.<|endoftext|>The paper considers the training of neural networks for physical simulations. By distributing the burden equally between network and physics, the paper presents the half inverse gradients (HIGs) method. It is better to discuss its effect in more details.<|endoftext|>I m not convinced that is the reason for the observations. It would be very helpful if the paper was a little more self contained in the motivation. Equation (3) would be helpful to emphasize that you re using a pseudoinverse, not a standard inverse. In particular, does most of the benefit come from $\beta$? The pretraining is interesting.
Reject; rating score: 3; rating score: 6; rating score: 8; The paper shows that a small recurrent neural network can be used to predict the activity of 4 neurons in a C. elegans simulation with good accuracy as measured by RMSE. In general, I found that insufficient details are provided about how this model was built & validated (how were the biophysical properties chosen, what synapse models were used, etc). The authors state that the simulator is "high fidelity" because they "assume it reproduces with fidelity the real output of C. elegans neurons.". The model might be high complexity, but as far as I could see, fidelity was not demonstrated anywhere. In fact, the only test case, also used for all subsequent modeling work with neural networks, is "forward locomotion". Even if that s the case, more of this information should be in the main text. These are standard and well known architectures at this point, and could just be briefly introduced with references to the original papers. The reader would benefit much more from learning the details of the C. elegans simulation. That a GRU network works for modeling time series data is not particularly surprising. I found the experimental work presented in the paper far too limited to yield useful insights into using RNNs as a black box model of real brain activity. Please provide more details on the criteria you used to ensure diversity within each group. * What exactly is the input to the network? The general research direction of building reduced order models for such simulations is interesting, but the very limited empirical results presented here and lack of details about the simulation make it impossible for me to recommend acceptance.<|endoftext|>Authors show how the nervous system of C. elegans can be modelled and simulated with data driven models using different neural network architectures. Specifically, they target the use of state of the art recurrent neural networks architectures such as LSTMs and GRUs and compare these architectures in terms of their properties and their RMSE, as well as the complexity of the resulting models. Authors show that GRU models with a hidden layer size of 4 units are able to accurately reproduce the system’s response to very different stimuli. In this paper authors create models for the C. elegans nervous system with three different recurrent neural networks architectures: simple RNNs, LSTMs and GRUs. The objective is to further generate a low order description to replace the original, detailed model in the NEURON simulator. Overall its a weak accept.<|endoftext|>This paper investigates the use of recurrent neural networks as amodel reduction tool in computational neuroscience. The results show that a smallGRU based network is sufficient for achieving excellent agreement withthe starting data, which was generated using computationally demandingsimulations of a network of multi compartimental neurons. The paperalso includes an introduction on popular RNN architectures, and on theproblem of model reduction in computational neuroscience. ## Strenghts  The paper is very well written and is an enjoyable read. I think this is particularly important for this paper, as its  main audience would presumably be computational neuroscientists, so  it seems a good idea not to take for granted a deep knowledge of  (say) recurrent network architectures. ## Weaknesses  Mirroring what I wrote above in one of the "strenghts" points: the  fact that paper is a rather simple application of off the shelf RNN  architectures to a well defined modeling problem means that the  paper is not hugely ambitious, and that it doesn t bring much in the  way of theoretical or conceptual insight; however, in my opinion  this does not detract from the quality of the work. ## Minor suggestions:  When discussing related literature, I invite the authors to consider  the inclusion of a broader selection of papers that in recent years  applied deep network approaches to modeling neural activity data, as  the works currently cited in the paper investigate only fMRI data  (which is only distantly related to actual neural activity), or are  specific to C Elegans. I  apologise if this is given somewhere and I missed it.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This submission aims to reduce the prevalence of samples for which a neural network might predict the wrong answer, not just for the sample but also its nearby points in the input space. Authors call this metric robust inaccuracy. I found it difficult to understand this paper. The goal is not well motivated, the proposed terminology is confusing, and I m not sure what the takeaway is from the results. It is not even stated which benchmark this sentence refers to: "natural accuracy from 97.8% to 97.6%". I would also suggest the authors try to connect the "robust inaccuracy" aspect of their work with previous work. Thus the adversarial robustness on a sample may not depend too much on whether the sample itself is classified correctly or incorrectly by the model. My biggest concern about this submission is that I do not see the motivation of studying the "robustness of inaccuracy" on samples that are already classified incorrectly. I found that the goal of the paper is not well motivated and the presentation should be improved before it can be accepted.<|endoftext|>The paper addresses the issue of trained models being inaccurate but robust for some samples. To address this issue a flexible fine tuning mechanism is proposed. A robustness based ensembling method is introduced as well. Table 1 seems to be a justification why the approach presented in the paper is needed. However not much details are given on how to interpret the results shown and how the values were generated. The important concepts of "natural accuracy", "robust accuracy" and "robust inaccuracy" are only explained in section 3 but used to a large extend prior to that section. First introduce the concept than use it to explain the novelty or justification of the proposed approach. Why would the method be restricted to fine tuning?<|endoftext|>the sacrifice of natural accuracy, this paper proposed a new training method that aims to maximize robust accuracy and minimize robust inaccuracy. Moreover, a robustness based abstain mechanism is adopted to further boost overall robustness without sacrificing accuracy. Experiments show the effectiveness of the algorithm in terms of fewer robust and inaccurate samples and better robustness, with only marginally reduced natural accuracy. 1.It s interesting to improve robust training methods by avoiding robust inaccuracy, but the proposed method is a little bit straightforward. From my perspective, the proposed approach is straightforward and not fully explained. But I am open with my score, according to the authors  responses and other reviewers  comments.<|endoftext|>This well presented work is motivated by the relevant issue that improving model robustness while maintaining high accuracy can result in robust inaccuracy, i.e.a non negligible amount of samples get classified incorrectly, but the category remains consistent for their perturbations. Although the analysis could be stonger, it nevertheless provides sufficient evidence for improved accuracy robustness trade off over state of the art methods. The paper provides specific implementations of this term under two setups: empirical (adversarial) and certified robustness. The experiments on standard benchmarks further confirm, that the robust model consistently decreases the fraction of robust inaccurate samples (albeit at the expense to robust accuracy). Cons:  Although the problem formulation seems novel (Eq.4), its implementation under both adversarial and certified scenarios of training are closely based on prior work. Similarly for compositional models (Eq.16).This is appropriately cited, hence does not present a serious issue, but does somewhat limit the novelty. A more general concern is that the experimental results often do not yet provide as much insight as they potentially could. Nevertheless, I wonder if and how the proposed approach affects the quality of model calibration. decision boundaries to other models with only one.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Various experiments, such as synthetic examples and contextual multi armed bandit, are conducted to compare against previous works of NPs and GP. Strength:  The paper is written in clear manner. The code is also provided together for the reproducibility. Weakness:  The novelty is somewhat limited, that this is a direct utilization of Neural Bootstrapper. In Fig 1, the authors argue that the baselines (1) overfits the data from the linear regression samples, and (2) are incapable of capturing uncertainty. My opinion is that the paper is on borderline. Considering the quality of the venue, I m bit negative on the direct utilization of existing work.<|endoftext|>Experimental results show that it achieves state of the art performance on stochastic optimization problems, including multidimensional Bayesian optimization and contextual multi armed bandit. The strength of the paper is that the authors report multiple experiments in different settings that show the superior performance of NeuBANP. They are clear about the comparative advantage from ANP, BANP, and NeuBANP. The figures for experimental results also depict the superior performance of NeuBANP well. As it has been discussed well in the paper, Bootstrapping Attentive Neural Processes have been proposed in previous work. Also, Neural Bootstrapper, which replaces the sampling problem of bootstrapping with the augmentation of the loss function, has also been proposed in Shin et al.2021.This paper looks to me is an application of the ideas in (Shin et al 2021) to the published Bootstrapping Attentive Neural Processes. The paper is well written and the ideas are discussed clearly.<|endoftext|>The authors provide experimental results on nonparametric regression tasks, Bayesian optimisation, contextual multi armed bandit tasks and image inpainting (in the appendix). * The authors do provide a nice set of experiments with the appropriate baselines and the results seem promising. * Finally, in terms of novelty, while this is the first time I hear of this particular combination of models, it is a somewhat straightforward combination of previously existing ideas (this in itself should not render the work unworthy of being published, of course). * In figure 3 I would explicitly mention that the minimum variance of ANP and BANP is as large as it appears because that is the fixed minimum value. Overall I think the paper is nice, the explanations are not hard to follow and it is overall well written.<|endoftext|>This paper proposes an efficient way of modeling functional uncertainty building on recent work of NeuBoots and BANP. Authors show that NeuBANP achieves state of the art performance in benchmark experiments including Bayesian optimization and contextual multi armed bandits. Provides clean structured code for reproducibility. One of slight worry from the reviewer is that the paper’s method is hinging on two of very recently proposed approaches (Bootstrapping Attentive Neural Process and Neural Bootstrapper) where each of the merits are not quite validated thoroughly in the literature yet. In section 4.1. It would be great to include a comparison to BANP and ANP as well to see that Neural Bootstrapping is the main reason for superior performance. While authors cite Lakshminarayanan et al.2017 for bootstrapping to be a reliable approach to estimate predictive uncertainty in neural networks, the reviewers take from that paper is that bootstrapping was detrimental compared to deep ensemble.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper investigates the generalization benefits of using momentum when training neural networks with gradient descent. In contrast with existing literature, which studies momentum mostly empirically and in the stochastic setting, the authors develop a theoretical explanation for why generalization improves when optimization is performed using full batch gradient descent with momentum (GD+M) than without it (GD). Their analysis focuses on a specific structure of the data and the learning problem, and the authors argue that similar assumptions apply to the real world datasets used in practice. The authors prove two main results: one about GD learning large margin data and overfitting to small margin data and the other about GD+M successfully learning all the data thanks to historical gradients. The main strength of this paper is a rigorous mathematical analysis that exhaustively justifies the generalization gains from adding momentum to the gradient descent algorithm. However, these results reflect only a specific case of problem statement, which is the major weakness of this submission. Namely, the authors consider a binary classification problem with a two layer convolutional neural network with cubic activation and a fixed second layer. The data is generated according to a specific scheme where each data point has a single patch containing useful information (the signal patch), and all the others are Gaussian random noise with a small variance; the data is split into large margin and small margin parts according to the intensity of the signal patch. The authors claim that real world datasets possess similar properties, however, they do not provide sufficient grounding for that argument. Figure 3 exhibits that momentum indeed gives more gains if real data are artificially augmented with small margin data (which accords with the provided theory), but it does not prove that real data initially *had* such structure. In other words, even if the derived theory is correct in the particular case, it still does not suffice to explain the benefits of momentum in deep learning unless the authors prove that the considered case is relevant for practical deep learning. That becomes especially acute in light of the provided counterexample with teacher student learning on Gaussian data. It is unclear why the authors consider their setup more relevant to deep learning than the latter. "Indeed, a batch size of 1024 is known to be large enough in CIFAR training to consider the stochastic gradient relatively close to the full gradient (Cohen et al., 2021)". I am not convinced that taking a batch size of 1024 in SGD reduces it to GD. Moreover, such large batch settings are well studied in prior work (see, e.g., https://arxiv.org/abs/2006.15081) and are known as "curvature dominatedregime", in which SGD with momentum typically outperforms vanilla SGD. 2."In this paper, we ... empirically show that gradient descent with momentum (GD+M) significantly improves generalization comparing to gradient descent (GD) in many deep learning tasks". In the main body of the paper, I could only find experiments with ResNet and Wide ResNet on CIFAR datasets, which does not seem like "many deep learning tasks". 3.The authors did not provide details on their experimental setup. Is that a typo or an outlier? * "An interesting setting for this question is NLP where momentum is used to train large models as BERT (Devlin et al., 2018)". As far as I know, large NLP models are usually trained with Adam or more sophisticated optimizers rather than SGD+M. * "We suspect that this observation is due to batch normalization (BN) which is known to dramatically bias the algorithm’s generalization (Lyu & Li,2019)". This paper provides a solid theoretical analysis, which, however, applies only to a specific problem statement and thus cannot be considered a satisfactory argument for why momentum is beneficial in deep learning (unless the authors provide more evidence that their setting is relevant to real world datasets). The empirical justification does not correctly reflect the considered full batch gradient descent training.<|endoftext|>The authors provide a new perspective on the why momentum is useful for generalization in neural networks. They provide motivating intuition around why momentum is not always useful (including a great toy example), as well as empirical experiments on CIFAR 10. Finally, the authors prove a variety of theorems on a synthetic problem where momentum provably results in greater generalization. Strengths: The paper was clear and well written, particularly the toy example on when momentum hurts, as well as all of the proof sketches. Weaknesses: My complaints are primarily around significance and presentation. First some comparatively minor nits, to get them out of the way:  The loss values in Figure 1 on the left pane are quite difficult to parse you might consider making this logscale. In a couple of places you refer to "his momentum" when you probably mean "its momentum"More substantively, I m a little perplexed why the authors went to such great lengths to construct a dataset and prove statements about its training performance, but then did not actually perform any training using that dataset or that (peculiar) architecture used in the analysis of the theorems. I think the manuscript would be greatly improved by explicitly demonstrating the generalization gap proved by the authors, preferably in a plot as a function of $d$ and  $\mu$ (which seem to be the key control knobs on the generalization results). I m likewise a bit unsure of the significance of this work. It s not clear whether the mechanism proposed by the authors is also at play in other architectures, or if this is entirely a manifestation of the author s construction. That is to say, the result is clearly progress, but it would significantly improve the impact if a thread could be drawn between this mechanism, and a non synthetic dataset. These sorts of articles probably get more thorough review (and more _useful_ review) via a journal submission. The work is technically sound (to my knowledge), but of limited significance. This thread of work _could_ be of great significance if the mechanism proposed by the authors can be shown to be behind the performance of more realistic models, so I don t want to discourage the author s current line of investigation it s more that I m unsure if this is a good match for ICLR.<|endoftext|>Through some experiments, this paper claims that momentum does not always lead to a higher generalization in deep learning and such benefit seems to heavily depend on both the structure of the data and the learning problem. Then, the authors considered a certain data structure (large and small margin data) and learning problem (binary classification problem with 2 layer CNN model and logistic loss). Under this special case, the authors shown that both GD and GDM reach zero training error and perfectly classify large margin data, but GD fails to classify the small margin data while GDM can still perfect classify small margin data due to the historical gradients caused by the momentum. This paper considers a very important optimization problem in real application especially deep learning. The paper is well organized and very easy to understand. However, I still have some questions to be addressed. 1.Due to the theoretical results of GD and GD+M are built on a very special case, there is a big gap between the results in this paper and deep learning. It seems that the authors over claimed the contribution of this paper. 2.The authors claimed that they only focused on GD+M since the empirical results of GD+M can generalize better than GD (e.g., Figure 1). However, this is not realistic since the authors did not conduct GD+M and GD but large mini batching SGD+M and SGD. Therefore, it is reasonable to focus on the contribution of momentum of the large mini batching stochastic gradient (rather than "true gradient") on generalization. 3.It is strongly recommended and needed to provide experiments to verify the theoretical result. At least, the authors should provide numerical results on a simulation data according to the data distribution $\mathcal D$ (page 5) and the learning problem (page 6). The numerical results will make the contribution of this paper complete and solid. Big gap in the paper3. Lack of experiment (for verifying the theoretical results)<|endoftext|>This paper shows that gradient descent + momentum (a.k.a.Heavy Ball) provably generalizes better than standard gradient descent for training a two layer convolution network when the data distribution has a particular structure. Specifically, it considers a binary classification dataset of size N of which $(1 \hat{u}) N$ are with a large margin and $\hat{\mu} N$ are with a small margin. Most of the data points are with a large margin, i.e.$\hat{u} << 1$The paper starts by giving some empirical evidence   GD+M generalizes better for training Resnet 18 and WideResnet on CIFAR 10 and CIFAR 100 than GD. The paper also reports an experiment on a synthetic dataset in which features are from a Guassian distribution and shows that momentum does not improve over GD, which suggests that whether momentum has an advantage over GD depends on the underlying data distribution. However, the proof in the appendix in my opinion is not written well. For some of the approximations, it is hard to tell if the approximations make sense. Here are some questions regarding the analysis. The authors claim that by using Lemma H.6 one can get (174) but it is not clear at all from my perspective. Is this used anywhere in the proof of Lemma H.5? 2.(Lemma H.5): The first line on Page 48 says "by Induction hypothesis C.2, we know that $ c_r^{(t)} \geq 0$ for all t". But from Induction hypothesis C.2 (Page 16), it is only shown that $c_r^{(t)}$ is larger than a negative number. The validity of Lemma H.5 is concerning given this and the above concern. Can the authors explain where exactly in the proof did they show the "for all $t \geq t_0$" part? A related question about Lemma I.16 I.17: Should one also show $z_t \geq v$ for all $t \geq t_0$ like Lemma I.15? 5.(Lemma 5.1~5.4) We see that the iterate complexities in these lemmas all have a $\frac{1}{1 \gamma}$ factor. UNQUOTE The authors might want to expand on how Lemma 5.4 implies this. This is not transparent from the proof. The analysis is not very clear, a lot of approximations are made throughout the proof, and some places in the proof are concerning. I don t recommend an acceptance at the current point.<|endoftext|>This paper shows that the momentum conditionally improves the generalization by reconstructing a data case. I am not an expert in this area and am not very familiar with the results presented by this paper. 1.Does momentum unconditionally improve generalization in deep learning is quite important in ML and AI. This paper answers this problem negatively by a construction method. 2.This paper presents an insight that momentum helps to learn small margin data since all the examples share the same feature. 3.This paper provides plenty of detailed theoretical results. Many technical lemmas are developed. 4.The authors present the corresponding numerical experiments to demonstrate the theoretical findings. I do not think this paper is below the accept bar due to its interesting topic, novel analysisis, interesting result, complicated techniques, and good presentation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper presents a novel approach to self supervised speech representation learning, which promises to be simpler than existing methods such as wav2vec 2.0. The approach is inspired by the BYOL approach from CV, and is shown to be indeed largely as effective as wav2vec 2.0, while being significantly more efficient during training. The authors promise to release code upon acceptance of the paper, which should allow readers to verify results and build on top of it. I would like to see results with LM rescoring after the hyper parameters have been optimized, so that results are more comparable (and hopefully consistent)? This should be even more efficient on GPU? Update: the additional tests on Chime data, and the updated decoding results address these concerns.<|endoftext|>The paper proposes a new speech pre training approach named SPIRAL. The proposed method is trained by learning representation denoising of perturbed data with the teacher student framework in a self supervised manner. Compared to the state of the art speech pre training method wav2vec 2.0, the proposed method can achieve competitive or better results but with a significant reduction of the training cost. (2) The proposed method is a novel one that aims to learn the representation denoising of perturbed data with the teacher student framework. In addition, the proposed method can also be combined with multi condition training to improve the noise robustness. (7) Plenty of experiments have been conducted to evaluate the performance of the proposed method.<|endoftext|>This paper describes a self supervision training approach to pretrain speech encoders. This is related to recent work on wav2vec 2.0, and SimCLR. The goal of perturbation or augmentation invariance is well motivated in speech recognition, and also ML/ representation learning more broadly. * The results using synthetic noisy data are less convincing than naturally occurring noise. This is especially important since the synthetic noise is used during training as well. * In figure 1 it appears that the positional padding is applied equally to both the input and output of the teacher. Assuming both time and frequency are masked, is the area masked by both time and frequency replaced with zeros or gaussian noise? This is a compelling paper. it s a well motivated and technically sound perspective on self supervised training. The performance in noise conditions is strong, but would be more convincing if shown on more actual rather than synthetic noise.<|endoftext|>The paper introduces SPIRAL, a new method for self supervised pre training for speech. If yes, what would happen if the clean input is used for the teacher? Achieving similar/better WER on LibriSpeech compared to wav2vec 2.0 with 35% of the training cost; and2. The model achieves WERs similar/better than the popular wav2vec 2.0 architecture while reducing training time and model size. 5.The paper is clear and easy to follow, with sufficient discussion of related work. While the proposed model does well on clean LibriSpeech, performance under noisy conditions may be concerning (Section 5.2). These questions are of particular interest since the model is named “perturbation invariant”. 3.The ablation in Section 5.3.2  indicates that performance degrades significantly when perturbations are applied to the teacher’s input. This again deviates from standard practice in image representation learning where augmentation is applied on the inputs for both the teacher and the student, and (as astutely observed by the authors) is closer in principle to standard self training.<|endoftext|>The paper describes teacher student self supervised training, with a denoising advantage by perturbing the student. It elaborates on techniques to avoid collapse and compares results with wav2vec 2.0. In utterance contrastive loss is used to avoid learning a trivial representation. The teacher is updated as the moving average of student checkpoints. The results are somewhat comparable to wav2vec 2.0, but at lower training cost. As far as I understand, it does not require separately pre training the teacher, unlike noisy student training. The authors mention that the settings are not fully tuned   it would be interesting to know what the best results are after tuning. This can be made clearer in the abstract and introduction, while highlighting the reduced training cost.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Strengths  This paper reviews a long history of evolutionary and stochastic gradient descent algorithms. It provides a new perspective that the fine tuning process is similar to the adaptation in environmental change. The perspective of evolving the parameter itself is not new. It is better to show some results compared with SGD. Proceedings of the 32nd International Conference on Neural Information Processing Systems. Also, the writing seems not thoroughly reviewed.<|endoftext|>This paper contains a number of minor spelling and syntax errors that would benefit from an extra review. WeaknessesThis paper mentions multiple shortcomings on the usefulness of genetic algorithms in the field of fine tuning and on their specific equivalency between the Gillespie Orr GA and SGD. StrengthsThe authors provided a significant amount of generalization in their methodology using the Gillespie Orr genetic algorithm to be useful outside of the specific application they described. While including this information is still useful, the paper in general would benefit greatly to discuss how some of the discussed facets of using a genetic algorithm would look like.<|endoftext|>The paper however lacks any experimental validation of the theoretical results proposed. The following points would need to be addressed in order for this work to be considered for publication:  Either a theoretical or practical experiment, possibly both. The key argument of the paper is the equivalence between SGD and Gillespie Fischer EA / Gillespie Orr EA. The literature review should also correctly attribute credit for Transfer Learning, called "fine tuning" in the first part of the paper, which is a crucial component for some of the claims. The claim that the computational resources required for experimentation would be prohibitive should be supported by actual budgeting.<|endoftext|>This paper introduces a conceptual equivalence between a certain class of evolutionary algorithms and the stochastic gradient descent. The comparison of the SGD and the evolutionary algorithm is quite interesting but not completely new from the conceptual point of view. The paper however introduces a very specific   weaker type   of evolutionary algorithm that is easier to be formalized. The main conclusions of the paper are that the flatness of the SGD minima is an equivalent statement for the stability learning in ANN.
Reject; rating score: 3; rating score: 6; rating score: 8; The paper proposes to transfer the update matrix to a ranking matrix before clustering to detect malicious nodes in federated learning. The authors list several attacks and provide analysis of the different behaviors between benign nodes and malicious nodes. The work is rather complete. The results, AFAIK, are correct and the experiments are solid. On the other hand, I have several concerns about the methodology itself. Most importantly, the paper does give any formal robustness guarantee. The method is designed based on several known attacks. Because security should not be preserved via obscurity, if this method is applied in real world applications, the attackers can construct targeted attacks against this approach. For example, in high dimensional case, the attackers can insert contaminated records close to the benign ones but still deviate the model from converging. To achieve real robustness, theoretically strong robustness [1] should be proved so as to prevent most available attacks (even not known). Second, the method is also not compatible with secure aggregation which prevents cumulative protection. Overall, I do not think the paper is ready for publication until formal robustness guarantee is added.<|endoftext|>The paper focuses on Byzantine defense through malicious node detection in a Federated Learning setting. Namely, by ranking the gradients and then computing the mean/SD, the paper shows that the malicious and benign clients will cluster separately. Assuming that the number of malicious clients is fewer than the number of benign clients, and the clusters correctly separate the malicious from benign, the smaller cluster is removed, and training is done on the gradients in the larger cluster. Appropriate experiments are done to show the ability of the model in malicious node detection along with analysis of performance and computational requirements. (+) The main strength of the paper lie in the novel introduction of using the rank domain in order to detect malicious nodes. ( ) The weaknesses in the paper are in the number of datasets/attacks. Having more experiments on a larger range of datasets and attacks will support claims more. Additionally, the inclusion of a strong, recent Byzantine defense algorithm in the robust learning area, such as FLTrust, will also help show the performance against state of the art robust learning defense algorithms. This paper introduces a new perspective in Byzantine defense in comparison to typical robust learning defense or other detection defenses. While the algorithm and experiments done are relatively simple, the paper serves more as a starting point for research and application in the new domain. Claims are properly supported and the method s competitive performance compared to other methods is promising.<|endoftext|>The paper presents a novel technique for defending against Byzantine types of attacks on federated learning systems. __Strong points__: The paper’s strengths are its novelty (both technical and theoretical), the nature of the problem it is attacking and its clarity. The paper presents a novel technique and way to address the Byzantine attack problem in federated learning (which is a significant problem) by identifying malicious nodes. The actual technique of using the moments of gradient ranks is especially intriguing because of its simple elegance and great theoretical guarantees. The paper also does an excellent job going beyond theoretical guarantees, which hold with large numbers of samples, to show that the proposed MANDERA technique also works in practice and as good as other state of the art methods      As more of a question, really, than a comment, but the assumption that the gradient ranks, $R_{:,j}$ , are statistically independent probably deserves some more questioning. I can appreciate that the paper shows that this is empirically so, with a few, limited examples, at the bottom of section 2.3, but is this always so at least for neural network models? The paper is well written with only a few typos in it (.i.e “week” instead of “weak” in the last paragraph of section 2.3). One area where the clarity could be improved is to provide a brief 1 2 sentence explanation of the label flipping attack in the first paragraph of section 3. All of the other techniques receive a much more robust explanation in the previous section with the theoretical grantees, so it would be good for the reader to better understand why that technique was used and what it is (at a high level). __Weak points__: The paper is, overall, a strong paper with very few weak points. Mostly, the paper leaves one wondering about future work that could build on what is established within the paper, most of which the paper does comment on. For example, what about using more moments or combining rank statistics with other statistics of the gradient updates for more robust malicious node identification.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Instead, one can save in compute time while gaining in performance and robustness by initializing them from pre trained convolutional layers. 1) The novelty of the proposed method is limited as they do not always reach the performance of end to end Transformers. It would be better if the author could discuss situations in which this method is less effective than an end to end Transformer. 3) Since the authors have motioned that the proposed method can save the compute time while gaining in performance and robustness, it needs more evidence that it indeed achieves a lower computational complexity than other hybrid models. 4) Personally speaking, the organization of this paper needs to be fixed, especially the related work that needs to be reorganized.<|endoftext|>This work proposes an approach to bridge CNNs and vision transformers for image recognition. The approach is shown to improve the performance of the CNN, especially in terms of robustness to adversarial examples, corruptions in the input images, and domain shift. The actual methodological contribution, while somewhat incremental, could be potentially quite impactful. The paper is great, but the experiments are very limited, which I believe it puts it below the bar. Upon a more careful look through the supplementary material, the paper does include some interesting ablation studies. I think it would good to provide some experiments on a dataset different to ImageNet.<|endoftext|>The proposed T CNN model architecture is able to outperform its CNN counterparts and other models compared in the paper. The experimental analysis is detailed and includes many ablation analysis experiments to analyze different aspects of the proposed model architecture. ### ***Cons: ***  One of the concern includes one of the the motivation of the proposed method, which is efficient training time. Since, the training time cost is a one time cost which incurs only while training the model, it does not seem to be an important aspect of the model. The proposed model is not compared with many of the existing model architectures that try to create a hybrid model combining CNN and transformers in interesting manner. The performance comparison is provided with number of parameters vs accuracy graph, which does have its place, but additional comparison with number of FLOPs vs accuracy graph is of more importance as it also provide an important aspect of the model efficiency. Also it seems that with increase in model capacity the performance of the proposed T CNN has diminishing returns as CNN vs T CNN accuracies are very close. There are some concerns that need to be addressed raised in the main review, which should further clarify the technical novelty and motivation for the work.<|endoftext|>This paper shows that by leveraging a previous work Gated Positional Self Attention layer, we can reparameterize the late stages of pre trained CNNs and make them be hybrid Transformed CNNs. Experiments show that it can boost the performance and robustness, without training transformer block from scratch, reducing the training cost. The motivation makes sense to me and the results look good, however there are several things can be improved. The novelty is somewhat limited, the authors did not really propose new techniques but reused components from the previous paper on a not so new setting. However, from Fig.1 it seems that the Transformer based baselines do not clearly outperform the convolutional baselines (ResNet RS is better sometimes),  also, why there is a significant drop in the FGSM dataset when increasing model size or training time, do the authors have some explanation on these phenomenons? Overall I think the paper shows an interesting direction and present good results. However, the limited novelty prevents me from giving a higher rating.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; For instance, one thing I was missing because of this was that for each possible $w_\tau,$ the set $T_N$ must be resampled. The main scheme is presented modularly. I think this is a good paper, which makes an interesting methodological contribution to an increasingly relevant subfield. Finally, the paper presents experiments in the DomainNet dataset, and in the ImageNet dataset, where the shift in the latter corresponds to adversarial perturbations. How were these numbers arrived at? Finally, sensitivity with respect to these hyperparameters is not discussed. Secondly, I don t think the conclusion is as clear as is suggested by the "mean normalised size of reliable methods" metric.<|endoftext|>I hope the authors can address my concerns in their feedback. The algorithm is essentially a randomized algorithm. The method proposed by the paper is interesting and new. However, I found some claims of the theories in the paper need to be clarified or made more formally.<|endoftext|>The authors also extend this to settings where the importance weights are unknown but can be estimated from unlabeled samples. They show that their algorithm outperforms baseline methods in the literature in the sense that it maintains PAC guarantees while outputting a smaller prediction set  in expectation. EDIT: I have read the author response and my review remains largely unchanged, though I would raise the score to 7 if this were possible within the ICLR format. However, the empirical results do a good job of making up for these weaknesses, so I would give the paper a 7 if I could. Unfortunately ICLR does not give provide this option.<|endoftext|>The paper presents algorithms for PAC prediction sets under the assumption of covariate shift. However, the simulations show that the size of predictions is small than it s non robust (vanilla) counterpart. Empirical comparisons with some baselines also seem to be missing. So I recommend only a marginal accept in it s current state.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; This paper presents a method for synthesizing programs to solve MDPs without the need for teacher oracle policies by proposing a continuous program search relaxation that allows synthesizing performant programs as policies. **Method**: This continuous relaxation of the discrete program search tree is neat and intuitive, and serves as a contribution in the program synthesis for RL space. The method is also seemingly novel and performs well. Essentially one would have to decide how complex the program is a priori, and then learn via policy optimization what the weights of that program tree are. Could the authors do an ablation study on this with extra analysis?<|endoftext|>This paper presents a novel method for synthesizing programmatic policies. The first step applies a policy gradient method to optimize the probabilities defining the program space. The method iterates through all programs one can synthesize in the DSL up to a given depth. I understand that the authors were inspired by methods used to learn neural network architectures such as DARTs. However, I have a two concerns with the paper. Learning without an oracle can also be achieved without the differentiable approximation. The equation is recursive and resolving the value of the equation is equivalent to traversing the tree of programs.<|endoftext|>**Code**Please include a requirement.txt specifying the versions of all the dependencies. To this end, the paper proposes a framework that performs program architecture search on top of a differentiable relaxation of the architecture space. This allows the program architectures and parameters to be learned via policy gradient methods without RL oracle. ## Paper strengths and contributions**Motivation**  Exploring using programmatic policies structured in more interpretable representations to yield better interpretability compared to deep RL policies is promising. The idea of conducting a program search on a differentiable architecture space with policy gradient is novel and convincing. This paper presents an effective way to implement this idea. **Technical contribution**  Relaxing the discrete program architecture search space to be continuous allows for optimizing the program architecture and the parameters of program modules using policy gradient methods.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper presents a Transformer based model called SCformer to perform long sequence time series forecasting.<|endoftext|>However, this the novelty of the segment correlation attention within the SCFORMER is limited and the time complexity is incorrect. To further improve the performance, the paper proposes a dual task, which use the current time series to predict the past time series. The paper is well motivated that the time complexity of the Transformer is quadratic to the number of time steps.<|endoftext|>This paper studies long time series forecasting by using segment correlation attention. It misses the ablation study of using the longer sequence compared to using the last segmented sequence.<|endoftext|>The paper studies the long time series sequence prediction problem with Transformer. 3.In the ablation study about SCAttention, is the SCformer with canonical attention the same as the original transformer?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes an ensemble based adversarial training strategy, which could improve the worst case model robustness against multiple $\ell_p$ norm adversarial perturbations. To improve the training efficiency, the author introduces the Multiple Input Multiple Output (MIMO) strategy. (2) The problem of defending multiple perturbations is significant. (3) The insights given by the author are interesting for me. (2) The author missed some studies that also aim to improve model robustness against multiple adversarial perturbations [R1]. This is not convincing to me, and the discussion is somewhat tricky. For example, training on $\ell_1$ and $\ell_2$ adversarial perturbations and evaluating the robustness on $\ell_{\infty}$ adversarial attacks.<|endoftext|>This paper uses ensembles and more precisely multi input multi output (MIMO) neural networks for computational efficiency for adversarial training (AT), resulting in a method called MAT, short for MIMO AT. In particular, the proposed MAT generates the adversarial samples using the gradient with respect to the objective of the ensemble. Moreover, the contributions are empirical. (Major) Weak empirical evaluation. While the authors compare with existing methods on CIFAR 10, comparison on other real world datasets is missing, given that the paper is empirical.<|endoftext|>In this work, the authors propose a new training algorithm MAT that adversarially trains Multi Input Multi Output (MIMO) models. They show that ensemble models based on MIMO when trained adversarially , show ``adversarial diversity  and therefore are less vulnerable to transfer attacks. The authors also show computational benefits of their algorithm over vanilla ensemble training. The robust accuracy for $\ell_2$ attacks is significantly lower when compared to both MSD and AT. The authors propose an algorithm for adversarial training of MIMO models.<|endoftext|>[Summary]This paper proposes an ensemble method to defend against adversarial attacks. and adversarial training (Madry et al.). [Strength]1 To defend against multiple attacks, the proposal MAT requires that a) each sub model is robust against a specific attack, and b) submodels have reduced transferability. [Weakness]1 In my opinion, this paper s novelty is not strong, e.g., sub models should be diverse, sub models are adversarially robust, and MIMO training strategy. ##### Post rebuttal #### Many thanks for authors  feedback. I agree with other reviewers  evaluations such as “limited novelty“, "weak experimental evidence", etc.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces iterative decoding to improve the compositional generalization ability of seq2seq models. However, in most cases it is not the case, therefore is unclear whether the proposed method can generalize to many tasks. The proposed method is simple but intuitively make sense. This could be a significant drawback but is not mentioned in the paper.<|endoftext|>Besides, they adopt relative attention and copy decoders to improve the performance of each step, resulting in overall improvements. 2. missing baselines, A baseline is a seq2seq model trained with the original data as well as the sealed  "intermediate input intermediate output" pairs. Sealed means the intermediate output transformed to the same form of the original data. The iterative decoding proposed in this paper has limited to toy datasets. The conclusion about relative attention and copy decoders is not well supported.<|endoftext|>Weaknesses:  The biggest weakness of the paper is that it requires the problem to be broken down into iterative subproblems, and it is not at all clear how one might do this with non synthetic tasks.<|endoftext|>i.The use of “iterative decoding” in Cartesian Product seems very basic and not really representative of intermediate steps. Nonetheless, there is a substantial amount of missing related work, and the writing could greatly be improved. It seems particularly effective on PCFG. j.In general, it would be helpful to have more error analysis. Although this seems easily addressable in a revision.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper presents a wavelet based approach for deepfake image detection. The authors compare for parameters. There are works that employs analytical solutions with extremely few parameters. please provide explanation of the contribution of employing wavelet + CNN and not directly a CNN without the wavelet extraction phase.<|endoftext|>However, it would be beneficial to include a brief introduction for both type of wavelets in section 3 to better understand the intuition behind selecting these two for this work. 8.The setting of experiment on FFHQ images dataset mentions that the images are used at a resolution of 128 x 128. However, FFHQ has 1024 x 1024 images. Same is the output for StyleGAN and StyleGAN2. Therefore, using a representation combining both of them is not a very exciting idea. The paper lacks enough novelty in the technical approach.<|endoftext|>This paper leverage wavelet based representation in deepfake detection. As a result, I think this work requires a lot of extensions for publication, and rate this submission as marginally below the acceptance threshold. My main concerns are that the experiments are solid enough and the method is trivial. The experiments are not solid.<|endoftext|>This paper presents an approach to detect GAN generated Deepfake images using wavelet features. It will be good to compare those patterns and the wavelet patterns side by side. The source separation experiments are also strengths in this paper. Weakness:One main drawback of the paper is that no robustness analysis is done on the proposed method.<|endoftext|>2) The methods section is well written. What were the main challenges associated and why/how is this research work is able to overcome that? This wasn’t mentioned properly in section 1 or 2. 4) Though in section 3 the different wavelet methods are mentioned well, for the equations the symbols used are not intuitive and explanatory. Please specify them properly.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The first part of the paper addresses a theoretical question, namely whether the mixture model, on being given a prompt, that is, a prefix such as "Albert Einstein was German. If this is the main observation that the paper makes, it should be stated more openly, even if there is a danger that some readers may dismiss it as a bit "obvious" (which it is not really at the formal level). Is this related with the different color used for "Albert" and "Einstein" in Figure 1 ? * In your simulations, the fact that neural LMs such as LSTMs or Transformers are able to recover the completion of examples should be sufficiently explained by their ability to well approximate the pretraining distribution underlying the GINC dataset, without having to invoke additional ICL abilities. Do you think your theory could be extended to explain that phenomenon, or is it orthogonal to it? A true explanation would require to go into how _the hidden states of a pretrained Transformer model_ focus more and more onto a characterization of the task being illustrated by the sequence of examples in the prompt (and typically do so based on only a few examples) and are often able to solve that task even if these examples appear to be remotely related to the training distribution. This is not what the paper does, so I think the title could be reformulated in a more cautious way. The paper raises interesting connections between Bayesian learning and certain aspects of in context learning, based on a simplified formal model exploiting mixtures of HMMs. I am not fully convinced by the large significance you put on the OOD nature of the prompts relative to the pretrained distribution   technically true, but in a narrow and not very illuminating sense IMO  , and still think your title is an exaggeration.<|endoftext|>The authors propose to study the phenomenon of "in context learning" through the lens of Bayesian prediction. This paper proposes and analyzes a framework under which "in context learning" can be studied from a statistical perspective. It would be useful for the authors to give a more robust account of why we need a specific account of "in context learning" in the first place. Put differently, why should we not just consider these examples to be relatively straightforward cases of next word prediction for a model that has learned an accurate approximation to the distribution over sequences of words? In the first paragraph of $\S$1, the authors write, "Given that the distribution of prompts are quite different from real text..."   but is this really true, particularly given the enormous breadth of modern training corpuses? The prompt that precedes the quote seems reasonably close to natural (if somewhat simple) language in this case, certainly enough for a reader to reasonably ask whether there is anything particularly special here that needs explaining. How do we distinguish between "recognition and continuation of a specific concept / pattern / construction" and just "good next word prediction conditioned on a relatively plausible sequence?" However, some aspects of the experimental approach could use more discussion. One main question is: how does this toy setting help us understand the emergence of "in context learning" in models trained on vast corpuses whose contents are very unlikely to be well modeled by a mixture of HMMs corresponding to well separated "concepts?" Why do we observe "in context learning" in these relatively small LSTM and transformer models, when previously it was observed that even models as large as GPT 2 fail to demonstrate this phenomenon? Other comments:  The first sentence of $\S$3, particularly "... conditioning the pretraining distribution on the prompt infers the prompt concept to enable in context learning..." is unclear. 2.Typo in first paragraph of $\S$1: "Intuigingly"This paper contributes a framework for theoretical exposition of a complex empirical behavior observed in language models.<|endoftext|>This paper studies the question of why few shot prompting works for large language models (LM) such as GPT? The paper shows that when the pre training distribution is a mixture of HMMs, prompting works as a result of Bayesian inference. The authors show the validity of their theoretical results by training transformers and LSTM on synthetic datasets sampled from some HMMs that they devised. I think the paper does a good job in explaining why concatenating independent examples as a sequence and prompting a LM trained on HMMs gives good results, considering the prompt sequence can be OOD. There is an additional result in the appendix related to GPT 3 but it is not clear if using longer examples gives better results or because the overall prompt sequence is longer. GPT 3 uses an additional task description, which would be similar to the concept in your paper, could you explain how this can be incorporated? 2  The example sequence (O_i) is generated using the same pre trained language model. In this case, OODness of the prompt sequence only comes from the fact that examples are concatenated. I also think that there needs to be more experiments with real data to understand the gap between their simulated experiments and real language.<|endoftext|>The submission studies the phenomenon of the "in context learning" behavior of large language models (LLMs). Two results are presented: First, the submission argues that in context learning can be formalized as Bayesian inference in a mixture of Hidden Markov Models (HMMs). Second, the submission introduces small scale settings in which the in context learning behavior of LLMs is reproduced empirically. I think the authors can improve on this point by clarifying what they mean by "we can no longer factorize the examples under the pretraining distribution." Significance: The paper studies a timely question: Why does in context learning (in particular in LLMs) take place? "The canonical asymptotic tool in Bayesian methods..." The reason for introducing this should be much more elaborated. What does the tool do? I m not sure why this would be just to "simplify the notation;" is it not in place to ensure that the test prompt does not occur in the pre training set? The theoretical model considered in the paper seems to be a mixture Hidden Markov Model (HMM). "The target distribution differs from the pretraining distribution p on the distribution of $h_\text{start}^\text{test}$." This would more closely tie the "in context" phenomenon to the theoretical model. The question (S1) is highly relevant, and I think the small scale reproduction (S2) could be meaningful. The current version is suggestive of something like "An Explanation of In context Learning [in Large Language Models like GPT 3] as Implicit Bayesian Inference," which is *not* the contribution of the paper, because the paper explains in context learning in a simplified theoretical model, and provides some  empirical evidence that this captures important aspects of the same in LLMs, but does not strictly "explain" the same in LLMs. I think this should be more explicitly explained as "we model in context learning as Bayesian inference with this particular HMM."
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; So I think it would be helpful to include these experiments if the authors have sufficient resources. I think this paper provides valuable insights on the connection between local attention and dynamic depth wise convolution and I suggest accepting it without a doubt.<|endoftext|>Why author pick it as an example rather than ViL (Multi scale vision longformer, Zhang 2021)? Or what is the connection to ViL? The paper qualitatively connects local attention and dynamic depth wise convolution and they validate this connection empirically. 2.I am not quite clear about the main difference from Cordonnier et al 2020.<|endoftext|>I think this is an interesting for the community to know. Well written and easy to follow paperForms interesting connections between local attention and dynamic depth wise convolution in terms of sparse connectivity, weight sharing and dynamic weight computation.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper proposes a label refinement approach. The method tries to obtain voice characteristics which can be further used for dubbing/voice casting. Overall, I do not find the approach to be novel. but this label refinement is never really used for these tasks in the experiments. The paper merely studies the clustering through different measures and its not clear to me what the end goal here is ? In the case of the MassEffect dataset, the clustering ends up highlighting initial labels  (expected) but when another dataset is used it ends up highlighting refined labels. The paper does not offer any interesting insights on the methods as well as on the empirical end. 4.There are some recent self supervised speech representation learning approaches such as Hubert [R1]. Overall, the paper is quite difficult to read.<|endoftext|>This paper addresses the task of finding similar sound voices with application to voice dubbing (e.g.finding an actor to record dialog in English, translated from original French dialog, such that the English speaker sounds similar to the original French speaker). The paper proposes a method called "label refining". This method is based on p vectors, a prior representation found to model similarity between characters. A second experiment is performed using Skyrim data as a subsidiary corpus used to cluster, with the goal of "bringing out vocal characteristics" from the initial labels, which shifts the optimal K towards 2, which the paper claims is because the new representations start to model gender. ### Strengths:S1) The approach is simple, and seems to improve accuracy a bit over state of the art. ### Weaknesses:W1) Overall I found the paper rather difficult to read. Besides some typos and opportunities for English usage improvement (see my comments below), I think the description of the type of information that the p vectors carry is rather vague. W2) I m still a little unclear what the "non expert initial labels" are here. This needs to be made more clear. W3) I m not quite sure what conclusion is being drawn from the second experiment where Skyrim data is used in the clustering. I didn t find the paper very clear, and the conclusions are vague and not well supported by the experiments.<|endoftext|>The idea is that a voice talent s voice in the target language should match the character s voice characteristics in the source language. The method seems to work by attaching labels to data driven clusters, and refining these using a second corpus, on which the labels  value for discrimination is measured. The application is interesting and novel. It would be useful to provide examples of the labels used, and how they get refined during the process. The paper presents the main experiments as a series of pragmatic choices, but I am not sure if the proposed architecture has already been applied to other problems, and/ or if other choices would yield better results? Because of missing comparisons and ablations, there is little information in the paper that I am confident can be generalized to other tasks and/ or problems, so I am not sure the paper will be valuable to someone not working in that field.<|endoftext|>The paper tackles a voice similarity system task for voice casting problem. The authors trained voice embedding network using voice character label and cluster the embedding features and used these clusters to train final voice embedding network. Pseudo labeling or iterative training with embedding feature clustering might be of related works. Also, the author argues that the explainability of the voice recommendation system is important, however, the proposed model cannot give the controllability to the user. Backbone network is not well explained. Loss function of the siamese network used in the paper is not specified. Training details are missing. The problem definition seems interesting and the application the authors tackling (voice search system for voice casting) sounds interesting (introduction is well written), however, the proposed method seems not enough novel and some details of the experimental setup is missing.<|endoftext|>In this paper authors proposed a semi supervised learning method named Label Refining, which leveraged a clustering algorithm for computing refined labels from initial ones, for training representation extractors. The proposed approach was applied to improve characteristics extraction from actors  voices. Overall this paper clearly presents the proposed approach. I find some experimental results require further clarification. E.g.1.For results in Table 2 4, is it possible to add standard deviation to show if improvements are significant? Also in the first line below equation (1), should it be "o" instead of "e"? Overall, I think this paper needs to further improve its experimental section, with more explanation for the results.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The main issues identified by the paper are: 1) fine grained information might be lost due to the use of global features, and 2) alignment between text and language is typically done using cross  or self attention, which are inefficient. The authors proposed a dual stream method that 1) uses visual "patch" tokens from ViT models and 2) perform late fusion of the two modalities using a variant of the method from Khattab & Zaharia (2020). If yes, any idea on how to make this better? To me it is more about correlation or alignment between image and text. 2.I understand that most of these new methods are trained on a very large scale dataset scraped from the web and it is hard to share them. At the same time, it is impressive to see that the authors have created their own cleaner version of such pre training datasets. This limits the reproducibility of the experiments and also it lowers the trust on the results. Computational complexity at inference time is not clear. Does the method extract 25% of tokens of the query and the data in the search index?<|endoftext|>The paper is well written with strong results. Experiments on zero shot image classification and image text retrieval tasks show that the proposed approach is able to achieve state of the art performance. However, the proposed approach has limited novelty and some critical claims are made without strong support. 4) Experiments show strong performance on vision language tasks. The only thing the paper states  we discard the padded tokens and use average instead summation of token wise maximum similarities when computing the image text alignment, which enhances the cross modal representation learning and stabilizes training. Furthermore, we construct a large scale pre training dataset named FILIP300M from the Internet. 6) MInor: Did the authors consider   For a visual token, similarity with the full text representation for fine grained interaction (as a visual patch may be related to full text or multiple words, not only one word token)? Compared to For a visual token, similarity with all textual tokens (i.e., what used in the paper). Any comment on this?<|endoftext|>The paper proposes to utilize the fine grained alignment between visual tokens and text tokens in the contrastive loss for language image pertaining. The experiments on both zero shot image classification and image text retrieval with different pre trained datasets validate the effectiveness of the proposed model. Strong performance on zero shot classification and image text retrieval, compared with original CLIP. The proposed cross modal late interaction is inspired by [1], but the three modifications mentioned in the paper seem to be a bit trivial. 4.The authors do not mention whether to release their collected dataset FILIP300M. Ref:[1] Omar Khattab and Matei Zaharia. But this paper provides an angle to look at the token wise interaction in contrastive Language Image pre training. Moreover, the performance is quite impressive and the experiments are conducted on different pre training datasets and downstream tasks to prove the model s efficacy.<|endoftext|>(1) The core idea of the proposed FILIP method is to achieve word patch alignment by token wise similarity matrix through cross modal late interaction by modifying only contrastive loss, which is both training and inference efficient. The authors collected FILIP300M, a large scale cleaned image captioning dataset from the Internet for FILIP’s V L pre training. (2) FILIP achieves SoTA performance on ZS image classification and ITM tasks. Visualization results show its promising ability of fine grained (visual textual token) classification and localization. Stacked cross attention for image text matching.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposed a novel DL framework for periodic time series forecasting. The proposed solution is technically sound and effective based on the extensive evaluation. The writing of the paper should be improved.<|endoftext|>The paper addresses the problem of time series forecasting, esp. Does learning the periodic time encoding g_phi and the forecasting   model f_theta really have advantages? The paper isaccompagnied by a detailed appendix with further experiments,esp.<|endoftext|>The paper has proposed a new decoupled formulation of time series forecasting and formulate the forecasting problem in the model DEPTS by introducing a prior estimated hidden periodic state and a expansion framework based on residue learning. Each layer includes a local block and a periodic block for the residual expansion. DEPTS framework builds upon the residual learning and expand the learned local residuals and periodic residuals block by block. The extensive experimental results have clearly demonstrated the paper intuition (synthetic experiments), the performance improvement (real world experiments) and the interpretability.<|endoftext|>This paper proposes a deep learning model tailor designed for periodic time series forecasting problem. (2).The authors discuss the connections between the model and N BEATS.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; One of the main motivations of their work is that highly specialized hardware is expensive and hence not available to many researchers in developing countries. Also in the experiments, the proclaimed superiority of SWARM when confronted with peers leaving or joining is not tested or demonstrated. On the other hand, the section on data parallelism is quite extensive even though not really relevant for this paper.<|endoftext|>The baseline model in Section 4.2 is not convincing: it is about 2x larger than GPT 2 Small but achieves worse learning quality. It addresses an important need in the community and is likely to have a significant impact.<|endoftext|>Although the paper is well written and I liked the proposed algorithm, I found the experimental section to be lacking crucial experiments to justify the paper’s claims. The performance of the new architecture degrades as the number of stages is increased and therefore the solution is limited to a small number of stages. 2.The paper addresses an important problem that can reduce the cost of training very large neural networks and make the training accessible to more researchers. However, with that, I think this paper in its current form should be accepted. Furthermore, it is unclear why GPT 2 is not the better choice as it has less than half of the parameters and significantly better perplexity.<|endoftext|>Why is the T4&A100 s throughput much better than others? The literature review of this paper is adequate and great. Additionally, the swarm division is changeable, is it possible to make the pipeline stage division changeable too?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper presents a training method of non autoregressive text generation that is trained from scratch without distillation. During inference, it starts with a random sequence and applies several steps of denoising to arrive at the desired sequence. The main result of the paper on non autoregressive machine translation on which the authors show decent improvements over baselines on non distilled settings however slightly underperforms AR distilled models. While the improvements on "raw BLEU" are good, it is not clear why raw BLEU is interesting. 2.The motivation in the introduction does say that for unconditional generation methods, distillation is not possible where this method can be applied. Do they need to be tuned for every dataset/language pair for example?<|endoftext|>The major contribution of this paper is the unrolled denoising training scheme, which is a training step method. Subsequently, in the second step, the noisy text from the first step is first denoised by sampling from the model, and the denoised text is then used as a sample for training the denoising autoencoder. The proposed method is well motivated and looks reasonable: it provides better noisy samples in addition to the random corrupted ones for training the denoising autoencoder. Although it lists a great number of baselines in comparison, almost all these baselines were evaluated only on AR distilled BLEU, whereas the proposed models were only evaluated with Raw BLEU. (3) In the experiment section, the authors only talk about what they have done and what they have seen from the table. I personally view that MT is not a typical LG task as its outputs highly depend on its inputs. The proposed model is interesting, reasonable, and well motivated.<|endoftext|>This paper proposes a new training technique for an iterative non autoregressive (NAR) model and achieves significant improvements over the previous model. More experiments on unconditional text generation tasks further show its promising capability. The proposed method achieves new state of the art performance in non autoregressive machine translation tasks. 2.The paper is written well and easy to understand. For example, a) efficiency analysis in machine translation tasks, we all know that deeper iterations improve the performance at the cost of the decoding efficiency, b) mere quantitative results in unconditional text generation tasks are helpless to understand whether it works well. However, the Imputer model does not use reranking techniques. In addition, we also notice that the Imputer model only needs fewer iterations. Therefore, I would like to increase my score by 6. However, some concerns about the experiment still need to be addressed to understand the contribution better.<|endoftext|>The paper proposes an unrolled denoising method for non autoregressive text generation. Experiments are conducted on machine translation and unconditional text generation. For NAR models, the decoding speed (or latency) is a crucial evaluation metric, which is missed in this paper. KD alleviates the multi modality problem in the dataset so that the NAR model can be more easily trained. As for the experiments of unconditional text generation, I suggest the authors provide quantitive evaluation scores to provide a clear comparison. The proposed method achieves good results, but the flaws on contribution/evaluation prevent me from giving a higher score.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; Furthermore, the authors found that the selection of the normalization function affects the results. Although the results in the paper can be replicated, the claims are a little misleading. Beyond dSprites (which is an easy dataset) only factorVAE is reported which gives a limited comparison between the methods. So the claim of group disentanglement is misleading, there are just a group of dimensions that each individually seems to have information about a distinct factor, but the group is not disentangled at all. To summarize, the empirical observation that BYOL features are better disentangled than other methods is interesting. I suggest more complete experiments and more analysis.<|endoftext|>Questions:* It is not clear how the MI is computed for Fig.3, I believe somehwere in the Appendix it said the distribution is approximated with a histogram, but some details (or a pointer to the details that are in Appendix) would be appreciated. The paper makes the previously unreported finding that contrastive learning produces "group disentangled" representations and outperform other disentanglement models (as measured by standard disentanglement measures). It offers a nice literature review, exposition is clear and provides many implementation details that should help reproducibility. It might greatly benefit from some insight (perhaps preliminary) on why contrastive representations are disentangled when supervised/unsupervised representations would not.<|endoftext|>Even though the idea of the paper to look further into the disentanglement properties of contrastive methods is interesting, I cannot recommend accepting it in its current form due to a lack of comparison with existing methods, impreciseness in the language/results, and too few explanations about the observed results. Here, they show empirical evidence that BYOL successfully learns disentangled representations. The paper applies the contrastive BYOL method on a set of datasets, which are mostly well established in the disentanglement community. Major Results: How does your group disentanglement property differ from existing metrics in this field? That information is necessary to decide whether the results of BYOL are significantly better than those of the other methods.<|endoftext|>This work explores the disentangling properties of contrastive methods. The authors discover that contrastive methods, particularly BYOL, learns disentangled representations with just a change of normalization method in the encoder. I especially like the concept of group disentanglement and the improvement of benchmarks on a real world dataset. There are comprehensive experiments and ablation studies to support the claims in the paper: on synthetic datasets, a real world dataset CelebA (which I think is a significant step to study disentanglement), and important hyperparameters.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; But this also means a lot of effort and handcrafting in setting up the tasks, the associated rewards and so on. The paper presents an interesting discussion on task relevant supervision vs. purely unsupervised techniques. Firstly, the crux of the paper deals with a comparison between using task based supervision to learn representations, and unsupervised representation learning techniques. In that way, it is not exactly an apples to apples comparison. the authors show that TARP representations outperform usual unsupervised learning approaches.<|endoftext|>The paper proposes a representation learning approach for multi task reinforcement learning. The authors probe both what the representation looks like and what pretraining tasks give better performance. strengths:* experiments comparing with reasonable baselines* paper is clearweaknesses:* i am not sure this paper addresses a real problem. I am not sure what they are meant to convey and in which sense the TARP ones are better than other ones. For a fair comparison they should be because it is possible and would make the claim of transfer stronger if they still don t get better results than TARP.<|endoftext|>In this work, authors formalized the problem of task induced representation learning. The aim is to leverage task relevant information for learning representations that capture task relevant information. The results show that the proposed way of learning task induced representations capture task relevant information. Strengths of the paper:  The paper is very well written. The reviewer  found the paper very well written, and tackling an important problem. I like the ablations done in the paper, and the focus on "constructing datasets" for learning task induced representations.<|endoftext|>Though the authors attempt to answer many questions, they only provide analysis results on some of the methods. I found this interesting and informative. The main contribution of the paper, in my opinion, is this investigation of the different methods. Recognising that task relevant information is important for representation learning is a worthy contribution, but in my humble opinion “formalize task induced representation learning (TARP) as an alternative family of representation learning approaches that leverage task information from prior tasks“ is not really a separate contribution.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper presents a knowledge distillation method for BERT, where BERT base acts as a teacher network and  CMOW is used as a student network. To get a relatively strong student network, the authors extend the vanilla CMOW to bidirectional CMOW. The final performances of the proposed method performs worse than most baselines, e.g., TinyBERT, DistilBERT, MiniLM, BERT PKD. The proposed method has poor performances on several GLUE tasks, e.g., CoLA and MRPC, and many related baselines are not included, e.g.TinyBERT, BERT PKD, MiniLM.<|endoftext|>The paper proposes to distill a BERT model to a smaller one,  a CMOW/CBOW Hybrid. The paper is clearly written. I am curious how it performs with a single CBOW or a single CMOW, not a hybrid one. To distill a better model to a weaker model, one can try different student models, either using shallower/narrower Transformer based models or other models. If we could tolerate a big performance drop (as in this paper, the authors were satisfied that it exceeds ELMo), there are much more choices, of which CMOW/CBOW Hybrid might be a good one. My concern is that using CMOW/CBOW Hybrid is not that interesting, one might also try CBOW with convolutions, for example.<|endoftext|>The authors of this paper try to distill large pretrained language models into a bidirectional CMOW/CBOW Hybrid model. 3.The proposed method can achieve comparable results to ELMo, while having a much higher encoding speed. I encourage the authors to test the model on other tasks as well. For example, DistilBERT can also perform well on question answering datasets like SQuAD. In this paper, it seems that the author only provides 20x20 matrix embeddings for the CMOW model. The paper presents a novel and useful idea: distill a complex pretraining model into a simple CMOW model. The initial result is promising while some limitations can be further explored.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposes the post training quantization for extremely low bit neural networks. Experimental results have demonstrated the superiority of the presented method. The authors only say  $1+u(x)$ represent the noise and this is not the formal definition.<|endoftext|>This paper aims to analyze how activation quantization affects the PTQ process and provides some theoretical analysis and experimental results on the effects of activation quantization. 4.In Section 4.3, the authors claim that it is highly possible(??) Based on the empirical and theoretical analyses, this paper proposes QDrop to pursue flatness and demonstrate that partial activation quantization is more beneficial. Their performance gap is just 2% in most of the models in Table 1. 4.Established a new SOTA for W2/A2 PTQ with QDrop. The proof of Sec.4.2 provided in Appendix A is a bit confusing to the reviewer.<|endoftext|>The authors also claim a new state of the art for PTQ on various tasks including image classification, object detection for computer vision, and text classification and question answering for natural language processing. The paper presents a simple approach to post training activation quantization that should be easy to implement and test by other researchers. The authors present both a mathematical treatment and empirical results to back up their claims and I wasn t able to find any glaring error. Hence, I think the paper is good and should be accepted.<|endoftext|>The authors proposed a random dropping quantization method at the post training stageto achieve a low bit quantization network. The terms of  flatness  and  sharpness  are not well defined, it might make readers confused on these and hard to follow. 2.Experimental results on many tasks and models show the QDROP s advantages. 3.This paper is generally well organized.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; In particular, the authors note the limitations of purely convolutional adversarial audio generation model for text to speech as those fail to provide a consistent pitch for an extended duration. They also evaluate the speed of evaluation and training of their method. the authors provide extensive evaluations, including MOS. the authors provide extensive examples to highlight different types of artifactsWeaknesses:  Some remarks (see after) should be addressed. This is much more efficient than giving the pitch directly for the reasons the authors state. This is related to your work, and it could be interesting to see how HiFi GAN performs with that simple fix. because it is no longer autoregressive and only conditioned on random noise. Simple method suggested by the author to fix some known artifacts from GAN audio vocoders.<|endoftext|>The paper introduces a Chunked Autoregressive GAN (CARGAN) method for conditional synthesis which is autoregressive over chunks of audio but uses Hifi GAN like parallel generation within a chunk. The authors argue that the periodicity and pitch errors are caused by the parallel GAN generators which may disregard continuity of the periodicity and pitch when generating audio. The paper is well presented and the reasoning is well discussed. 2.Literature review is satisfactory, but some more references can be added. 3.The artifacts caused by parallel generation are highlighted in the web page with extensive audio examples. I think it needs to be cited. I believe it furthers the state of the art in this area.<|endoftext|>This paper proposes a conditional waveform synthesis (CWS) model called Chunked Autoregressive GAN (CARGAN). First, it shows that the previous GAN based non AR CWS models do not accurately preserve the pitch and periodicity of the audio signal. Especially, this paper supports the claim by conducting an experiment that compares the ability of  AR and non AR models to learn the cumulative sum operation, because the instantaneous frequency and instantaneous phase have the cumulative sum relationship. As a result, I will give a score of 5 for this work. However, I have several concerns about this work.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; PoNet addresses the quadratic time and memory complexity of Transformer with a new attention mechanism that has only linear complexity. Theoretical (Section 3.2) and experimental results (Table 2) bear this out. Experimental results on standard datasets like Long Range Arena and GLUE are very competitive with SOTA. While not revolutionary, it is a well written paper with good analysis and experiments.<|endoftext|>This paper proposes PoNet, an efficient model with linear complexity for modeling long sequences. I still recommend authors to provide code, report error bars, speedup on GLUE. This model replaces the self attention layer of the Transformer model with its pooling network. This paper contributes to a well defined but highly influential problem: Efficient modeling for long sequences. Moreover, their experimental settings are different, and the FNet gets lower performance in this setting. The paper is good but not enough. Their comparisons to baselines are not sufficient, e.g.they can be benefited from reporting GPU/TPU time.<|endoftext|>In this paper, the authors aim to resolve the quadratic time and memory complexity of the standard attention mechanism. Is the FNet pertaining also using this objective? The authors introduce a pooling based module to replace the self attention layer in the Transformer in order for fast training speed and low memory usage. Extensive experiments show the effectiveness of their methods. However, some details of their models should be further clarified in the paper (such as the number of parameters).<|endoftext|>This paper proposed PoNet, which is an efficient architecture to replace self attention in Transformer based models. The first pooling component is the global aggregation module, which is very similar to the additive attention in FastFormer. The different is that in FastFormer, the global query vector is computed by a weighted sum of the query vectors, while in PoNet it is the average of the query vectors. The architecture of PoNet is simple and the paper is clearly written. The experimental results are not entirely convincing, especially those on large scale pre training and fine tuning. 4.Personally speaking, some claims in this paper are not well supported. Are these two experiments using different settings? 3.In the LRA paper, the authors clarified that the sequences in the text classification task are trunked to length 4K.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5;   The paper considers to look for a single neural network that can inherit the knowledge from multiple pre trained neural networks and have a small size at the same time. The authors propose a model fusion framework for fusing heterogeneous neural networks, *Cross Layer Alignment Fusion* (CLAFusion)  CLAFusion consists of three parts, cross layer alignment, layer balancing method, and layer wise model fusion method. Please could the authors demonstrate some quantitative evaluation or theoretical proof, to prove me wrong ? Experiment wise, it s not convincing either, the demonstrated experiments are too toyish, and it seems only work on synthetic MNIST. I don t understand why it is not working for CIFAR, as it should not require finetuning either. In Figure 2, why the size of the the hidden layers are of 4,4,2,1 and 4,2,1 ?<|endoftext|>This paper presents an algorithm named CLAFusion. Experiments on CIFAR10 and small neural networks show the effectiveness. I have the following concerns about this paper. First, I am thinking of the practical value of this algorithm, as well as the OTFusion algorithm. Second, the experimental part only shows CIFAR10 results. Since training from scratch is not required, I would expect ImageNet results to be offered, or at least CIFAR100 can be tested. 3.Experiments are only performed on very small datasets. Overall, this paper is not well prepared for publication.<|endoftext|>This work extends the OTFusion approach, for fusing multiple neural networks into a single neural network, to handle the case of networks with different number of layers. I am happy to improve my score if the above concerns are adequately addressed. The other important drawback is that the current empirical demonstration is limited to the case of 2 networks. The lack of clarity in the presentation, at some places, does not help either. It is important that authors provide at least some, perhaps preliminary, evidence in this regard. 4.M_B aligned  is not as clear or even slightly misleading.<|endoftext|>The idea is interesting. I have two main concerns about this paper. (1)Neural networks have many layers and neural networks are non linear. It is hard to understand to fuse $f(W_A^lx)$ and $f(W_{B^{‘}}^lx)$ as $f((W_A^l+W_{B^{‘}}^l)x)$ where $f(\cdot)$ is a non linear activation function. As mentioned before, the authors claim that the same merging method does not work in the case of CNN. (2) The explanations and analyses of the proposed model fusion are not convincing.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This work proposes a VAE based hierarchical generative model (named Latent Object Models ) for scenes that contain multiple objects. The object slots are inferred deterministically from the scene latent variable.<|endoftext|>This paper proposes a structured latent variable model, called Latent Object Models (LOM), for compositional scene generation. 2.How to avoid representing the same object with multiple slots? The paper is well written and the proposed method is clearly described.<|endoftext|>For this reason, I think the paper would benefit from having experiments using carefully designed datasets that more directly tackles the problem of modelling orderless elements using the proposed method. Quantitative and qualitative results show that incorporating these two ideas allows learning better generative models that can sample scenes with plausible composition of objects. ICLR  2019The overall ideas proposed are novel and a potentially useful contribution to the space of image VAEs. The labelling in the text and the table in the appendix should be consistent.<|endoftext|>The paper presents LOM, a new structured latent variable model for object centric probabilistic modelling of scenes. It follows the existing framework of probabilistic, object centric modelling, but differs from existing systems in important ways. The quantitative results are mixed. The interpolations in Fig.4 do not strike me as particularly smooth, and contain inconsistent configurations with overlapping objects. The paper proposes an interesting solution to the issue of formulating priors over set structured latent variables.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The algorithms are novel to my knowledge, however. This paper presents a grand sounding “general theory of relativity in RL”, but both components of the theory are already known from prior work.<|endoftext|>The first main theorem (Theorem 1) in this paper is already well known and rather basic. I am wondering in what cases this low bound would be a good approximation (either in the additive or multiplicative sense) to the quantity we want to maximize? Is there any theoretical justification? In short, I am not well convinced by the theoretical justification in this paper.<|endoftext|>The so called \textit{theory of relativity} for RL is nothing but basic regret decompositions well known in the RL community.<|endoftext|>The agent uses data from both two MDPs to get a good policy for one of the two MDPs. Strengths:    1. 2.The proposed algorithms are practical. Do baseline algorithms use the same amount of data as RPO? e) pi_old and phi_old do not appear in the pseudo code while they appear in the equations.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; How close is their method to these. Do they also invoke equivariance? That is, I’m not sure whether equivariant semi supervised learning (with faithful representations) is new to this paper or whether the contribution of this paper is the combination of equivariance to some transformations and invariance to others. In table 3, it’s a bit hard for me to interpret the numbers. In that case, these results here are not that strong. Do you have qualitative results? There is a clear logical inverse correlation in which if it is hurts performance for features to be invariant to a transformation, it is likely to be beneficial to be sensitive to them. The further experiments on CIFAR 10, ImageNet, and Photonic Crystals show a clear benefit to the method. In particular, I like the combination of standard benchmark datasets and good baselines as well as interesting application domains where the proposed method may be truly be useful. This is, in fact, what the loss of eqn. The authors say DOS is rotation invariant.<|endoftext|>This paper proposes a framework which generalizes self supervised learning (SSL) to also learn equivariance behavior. A family of SSL (the authors named invariant SSL) encourages representations which learn features that are invariant to certain transformations, e.g., horizontal flips. As invariance is a special case of equivariance, this paper explores whether encouraging equivariant representation is beneficial. Please conduct an experiment without the crop/resize, to make it feasible for the memory, just uniformly sample the rotation during training, i.e., an unbiased estimate of Eq.2.# C. Misc. 1 and Table 3 to the top of the page; I think this would make the paper neater. The proposed approach achieves good performance gain over baselines on CIFAR10 and ImageNet. 3.Author promised to release code and the supplemental materials reports the hyperparameters sweeps and results. The authors should better highlight their novelty and differences from existing work. 4.Clarity: The link between equivariance and the proposed approach is not clearly explained in the paper. As the authors have introduced the equivariance definition and notation, it might be beneficial to use those in their approach.<|endoftext|>The paper claims proposing a general E SSL framework, but ends up adding an additional loss from prior work (four fold rotation prediction) to popular SSL methods. The proposed E SSL framework boils down to adding an additional equivariance objective (mainly 4 fold rotations) to popular I SSL methods (Eqn.1 & 2).The empirical results show encouraging results in CIFAR 10 and ImageNet. The main weakness of the paper is that although it claims E SSL as a more general method (which I would assume would replace I SSL), the actual method boils down to adding an additional loss (which is not novel per se).<|endoftext|>This paper presents a simple solution to improve the existing self supervised representation learning by adding an additional branch to predict the rotations. The idea makes sense to me and the experiments show good results. It is not clear how the authors did in the introduction to encourage the model to be sensitive or not sensitive to a specific transformation. The story and the title are somewhat overclaiming. I do not see any post transformation on top of the model output on the newly added branch but just labels. Overall I think this paper has its own contribution. It finds that using an additional pretext header can improve the performance.
Reject; rating score: 3; rating score: 6; rating score: 6; The paper proposes offline RL algorithms which consist of several key components: (1) use implicit policies instead of Gaussian policies (2) regularize the state action visitation distribution instead of the policy. The paper provides a theoretical analysis to show the equivalence between regularizing policy and regularizing state action visitation, and provides an empirical study on several deep RL environments. The paper is well written and easy to follow. It provides good motivation for using implicit policies and regularizing the state action visitation distribution. Moreover,  I think similar analysis has been done in the existing works (e.g., Appendix A and B in [1] or Section 4.2 of [2]). What is novelty in the theoretical analysis in this paper? 3.In the empirical section, the paper mentions that “we do not test on the random and expert datasets as they are less practical”. Therefore, I think the empirical results would be clearer with all dataset reported. After the author response, I think the paper provides a good empirical study on state action joint regularization methods, so I increased my score for the empirical novelty and significance.<|endoftext|>The implicit policy is trained by a GAN like framework, and the regularization loss constrains the distance between learned policy and behavior policy. Experiments and ablation study on the D4RL dataset validate the proposed framework and algorithmic designs. The idea of learning policy with latent/implicit structure can be promising in offline RL as the latent structure might suffer less from the distribution shift. The paper borrows some method from GAN training which is novel to offline RL and show they are effective. These methods are well motivated and tested in the ablation study. I think this study is helpful for many offline RL work. The results is very similar to this in some sense. However both of them are not really enough to support the claim that it is sufficient to require the policy distance to be small to bound the visitation distance. The the distance between $x $ and $y$ is a constant, then the regularization loss is actually optimizing the conditional action distribution between $\pi_\phi$ and $\pi_b$, which seems not so different from previous work, and it is not so clear why the form in paper is more promising. I think the algorithmic contribution in this paper can be helpful to the community. Though some theoretical claim/results in the paper needs to be better explained.<|endoftext|>One of the existing Offline RL algorithms is to constrain the learned policy, such as constraining the learned policy to be consistent with the behavior policy itself or the action distribution based on state conditions, or adopting a Gaussian policy. Experiments  The authors show the effectiveness of implicit policy, state action visitation matching, and their full algorithm in the experimental results. I believe the most valuable experiments are the toy experiments to illustrate their motivations as the findings in the toy experiments may help the policy constraint methods that need to recover the behavior policies to improve their methods further. One reason may be because the experimental setting does not highlight the proposed method’s advantages as there are too few modalities in the used D4RL offline data. I suggest the authors to verify their method on datasets with more modalities to further discover the method’s full potential. The technical novelty is sufficient and significant. The empirical results show the method reach satisfied performance on D4RL while I m looking forward to seeing potentially better results on datasets with more modalities. Overall, I recommend to accept this paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; This paper propose a point process with a non stationary kernel to model complex event data. The kernel represented by its finite rank decomposition and the basis functions (feature functions) are models using a neural network architecture. In overall the proposed method for modeling the event data is novel enough and seems to have superior performance in comparison to other state of the arts methods. Strengths:  Novel method to model complex dependencies in temporal and spatio temporal event data using the proposed non stationary kernel, which experimental results shows the superior performance of the method.<|endoftext|>In this paper, the authors introduce a neural network framework modulating the excitation functions within the self and mutually exciting count processes. I find that this paper has interesting ideas, and it addresses an important question in learning self exciting processes in the presence of non stationarity. In their paper they set T   1. This is an important piece of work, but I would like to see how they contrast their work with Chen/Hall, which I think they missed.<|endoftext|>The paper propose to replace the kernel involving in Marked Point Processes (MPP) by the product of two neural networks. Experimental part is based on mostly synthetic data set in low dimension and two real world data set is not enough to show advantages of your approach. Furthermore, the experimental part is not properly studied and does not support the advantages of their approach. Limitations of the proposed approach are not investigated. The paper could be a good paper with more complete work.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper presents a novel framework that learns how to construct spatial + semantic maps for target driven semantic (ObjectGoal) navigation. Firstly, thanks to the authors for their efforts. I encourage the others to include a discussion on comparisons with SoTA from the leaderboard. This is a very well written and high quality paper and I absolutely enjoyed reading it! More concretely, I believe the following contributions are particularly the strengths of this work:+ An active learning formulation for training the mapper by selection of informative waypoints where the mapper is expected to gain maximal amount of information+ Using the variance of an ensemble of mappers as a proxy for taking into account the current mapper’s uncertainty for selecting goal targets in the map constructed so far: the formulation is elegant and nice leads to a controllable exploitation exploration trade off. The only major complaint I have with the paper is the absence of a thorough comparison with state of the art. I also had minor issues with some missing bits of information pertaining to how the overall system is working. For instance,+ There seem to be a couple of different, but related decision making processes going on: (a) the agent has to select informative waypoints to optimally train the semantic map prediction module and (b) the agent has to select the target semantic goal location on the map built so far while being mindful of the uncertainties of the current semantic predictions. + It is not immediately clear what is causing the variance in predictions for the ensemble of mappers. One would expect a higher SPL as a consequence of that (the map predictor training strategy staying the same across both).<|endoftext|>The paper proposes a method for leveraging semantic predictions in unobserved areas to help agent navigation. The motivation and high level idea makes sense. That said leveraging these priors as proposed in the paper requires very precise semantic segmentation maps which are fairly expensive to obtain. The uncertainty estimation relies on an ensemble of models trained for the semantic map prediction. This can be fairly expensive especially with semantic segmentation models. Have the authors considered these alternatives? A simple baseline that seems to be missing is just using the projected semantic segmentation map without trying to guess the semantic layout in unseen regions to perform the navigation tasks. The high level ideas the paper is based seem natural and interesting. However, fairly similar ideas have been proposed in prior work which is cited by the paper. It seems to me that the distinctions are mostly in the details a lot of which emerge from the slight differences in the problem setup. Post discussion: The authors have addressed the main concerns about distinctions from prior work.<|endoftext|>This paper presents a method to perform ObjectGoal navigation (i.e.goto the table) based on active semantic mapping. Specifically the proposed approach contains a semantic mapping module that hallucinates unseen areas. This method outperforms prior map based works (SemExp) on the Habitat Challenge ObjectGoal navigation dataset. ### Strengths  The method is well motivated and all components perform well  The uncertainty component is used both for active learning and to select goals for ObjectNav  Ablations are done well and comprehensive  Code is provided### Weaknesses  Computational requirements. Uncertainty is estimated via an ensemble, thus increasing the computation by N( 4). Lack of test std results. ### Suggestions for improvementWhile I don t expect the authors to compare with these works as they are concurrent (one wasn t even public before the ICLR deadline), I encourage the authors also include Ye et al.(ICCV 2021) and Maksymets et al.(ICCV 2021) in their table for completeness.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper extends the max margin analysis of the gradient descent (GD) method (using an exponential tail loss with linearly separable data) to its analogous analysis of the Bregman proximal point algorithm (BPPA) and mirror descent (MD). Unlike the GD converging in direction to the max margin solution, the authors show that BPPA/MD converge in direction to a solution with a margin depending on the condition number of the distance generating function. The theory is new, but the theory does not seem to support the authors  claim that BPPA finds a better data dependent solution. Bregman proximal point is gaining interest in applications as mentioned by the authors, and understanding the property of its convergence direction (for a linearly separable data) is of significant interest. 2.This is a non trivial extension of Gunasekar et al.(2018) showing that the GD converges in direction to the max margin solution for a linearly separable data. However, I don t see what one can claim from the non max margin by BPPA/MD that is $\sqrt{\frac{\mu_w}{L_w}}$ times the max margin. The authors claim, after theorem 3.1, that the data dependent divergence leads to a better separation and margin, but I don t think there is any theoretical justification for this. Let me know if I am missing something here. Then, what does this make interesting over the classic GD that also find the max margin solution? Comments  page 2: I think (Bregman) proximal point algorithms are considered to be in the class of first order algorithms. Any references?<|endoftext|>For the mahalanobis distance, they show that the solution is a max margin solution. b) Show that the max margin lower bound is dependent on the condition number of generating function for defining the divergence. c) It shows that that above results also extend to the dual first order algorithms such as mirror descent. StrengthThe paper is a very well written paper with analysis and shows the properties of the BPPA algorithm and mirror descent algorithms to learn a linear classifier using separable data. To the best of my understanding the analysis seems correct. However, I am not sure if there is earlier work with this analysis for the particular problem they deal with. Overall my belief is that the work  has some good results but I am not sure of the relevance of these results in the present day machine learning. The paper addresses the use of BPPA algorithm and the mirror descent algorithm to find the max margin linear classifier in case of separable data.<|endoftext|>This article provides an analysis of Bregman PPA / mirror descent for classification. Their theoretical results show how the recovered margins depend on the Bregman divergence used. I find it slightly odd that you call your setting  implicit regularisation , because your results are dependent on a norm which is explicitly imposed by the Bregman divergence used. In this sense, I find the results obtained unsurprising. Moreover, many of the citations are related to implicit regularisation, and very few to the study of mirror descent. For example, Orabona, Francesco, Koby Crammer, and Nicolo Cesa Bianchi. "A generalized online mirror descent with applications to classification and regression." In my opinion, linking this to implicit regularisation is an over sell of the paper. The authors derive some interesting results on margin convergence, but similar results also exist in the literature, and proving convergence results in terms of the norm that you are strongly convex with respect to is not new.<|endoftext|>This paper studies theoretical properties of Bregman proximal point algorithm in the linearly separable binary classification problem. The main theorem shows that the margin obtained by BPPA is lower bounded by the maximum margin multiplied by a factor, which depends on the distance generating function of the Bregman divergence. Similar results are extended to mirror descent. It provides both a lower bound and an upper bound on the margin obtained by BPPA, both depending on the condition number of the distance generating function of Bregman divergence. It is a step stone for further studies about BPPA and relevant algorithms. The setting is restricted to binary classification on linearly separable data. But the difference between SGD and BPPA is not very large. It would be interesting to know which one is better in a certain situation. Tf KD\_self is a more sophisticated method than the one studied in this paper. There are also variants of SGD, e.g.Adam, which are more commonly used and probably more efficient. So comparing vanilla SGD with Tf KD\_self does not seem fair. I recommend accepting the paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Authors should go through the paper slowly and refine the writing, this is more than just minor issues on grammar and typos, the writing in the paper should be very clear and make sense, any claims should be made with supportive evidence, etc. The problem of multi task batch RL is a direction that is not adequately studied. How statistically significant are these comparisons? I recommend the authors try to design other analysis or ablations to showcase why the proposed design is a good choice.<|endoftext|>I can see several important assumptions spread throughout the paper and it’s hard to keep track of them. There could be more baseline comparisons in the experiments section 3. The main technical contribution is specific to tasks where they all share the same transition function, which further limits its use cases.<|endoftext|>I also recommend the authors to add more baselines, e.g., CDS [1] (just for example since this one is quite recent, but authors should consider some SOTA batch RL algorithms). However, the writing of this paper needs lots of improvements, the technical contribution is limited, and the final results are not solid enough to support the claims, I feel this work is incomplete and recommend rejection. More descriptions are needed for each figure.<|endoftext|>The paper describes multiple task batch reinforcement learning method, building upon a well known single task batch reinforcement method BAIL. UPDATE: as the comments (mine and other reviewers) have not been addressed yet, there are still open questions which unfortunately do not allow recommending acceptance as things stand at the moment. (2)More insight should be given into why and when it works better.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper studies theoretically how adversarially trained models can transfer better, and disentangle the robustness and accuracy on the target domain. They claim that the main reason is more of regularisation rather than robustness. The paper is well written. However, there are several big problems:1. Overselling: transfer better due to regularization is not initiated by this work as they claim, for instance, Adversarial Training Helps Transfer Learning via Better Representations by Deng et al.have theoretically analyzed the regularization effect of adversarial training. 3.The monotonic relationship between regularization and performance is not quite right, since it is common sense that too strong regularization could result in bad performance. Thus, this claim is a little too exaggerated. Details are in the main review.<|endoftext|>This paper aims to convey the message that adversarially robust models may not have better transferability in terms of transfer learning in vision tasks. Some definitions are confusing and the conclusion seems to contradict with existing literature, e.g,  Salman et al arxiv: 2007.08489. post rebuttal Thanks to the authors for the draft revision and the additional appendix C. Now the paper is clearer to me: the general idea is to say that robustness is a type of regularization, and this regularization is the key to generalization. 8.Clarity: how would your theory help in the experiments? The connection between regularization and the proposed pseudo metric is not clear. In their paper it is claimed that robustness helps generalization, which seems to be contradictory. 3) Def 2 should be compared with existing metrics. Overall this paper aims to address an important issue: does robustness help generalization? On the experimental side, it says with a bigger regularization and stronger augmentation, the generalization measured by relative DT accuracy improves. Weaknesses: This paper is not clearly written and there are many confusing parts. 3.Clarity: Prop 3.1, what is the relative domain transferability?<|endoftext|>This paper aims to investigate the theoretical connection between domain generalization (aka Transferability) and adversarial robustness in some general settings. A simple example has been proposed which shows the existence of cases, where adversarial robustness and transferability can be independent (or even negatively correlated). Overall, I believe the paper is not ready for publication yet. Also, paper proves that adding more restriction (tighter regularization) on the feature extractor stage of a learning algorithm gives better domain generalization. This paper investigates a very interesting problem from a theoretical standpoint. However, author(s) have not discussed this issue in details. The other problem is that there are numerous grammatical errors and also a number of minor flaws inside the main body. Writing of the paper should be completely reviewed. The proposed analysis, in its current shape and form, might not be ready for publication at ICLR.<|endoftext|>The previous studies show that a more robust model can transfer better. This paper argues that the true reason for the cross domain transferability is not the adversarial robustness, but the effect of regularization, which can also be achieved through other methods, such as data augmentation. This paper challenges a fundamental claim in ML: stronger robustness leads to better domain generalization, and gives a solid analysis. 2.This paper presents a theoretical framework to analyze the relationship between regularization strength and domain transferability. How to obtain robust accuracy? The empirical experiments are very interesting. This is very important, as it may alter the conclusion of this paper, especially for the norm controlling experiments.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper proposes the UIMNET benchmark for uncertainty estimation. New empirical benchmark with reproducible software but with unclear research novelty and value to community. Can the authors outline some additional use cases? What is the justification for this claim of greater "realism"?<|endoftext|>This paper proposes a new testbed to test the uncertainty of image classifiers based on a split of ImageNet. However, given the benchmark, the authors have done a great job in thoroughly benchmarking existing algorithms. The benchmarking is solid and very thorough. The authors do not appear to justify this design decision anywhere in the paper.<|endoftext|>The paper introduce a benchmark UIMNET to evaluate predictive uncertainty estimates for deep image clasifiers. Overall, I think the paper is well written and the benchmark is valuble as testbed for uncertainty estimation. For Strengths, the paper provides a solid test bed for evaluating the uncertainty estimation for deep image classifiers. The main weakness of the paper is that the uncertainty measurements in Section 4 lack the SOTA uncertainty estimation methods.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper studies zeroth order hard label adversarial attacks. They also empirically consider attacks with reduced dimensionality in practice. That being said, the paper is poorly written: without a clear hypothesis, or convincing experimental or theoretical results. Unfortunately, my concerns with the paper regarding the presentation and motivation (which are also shared by other reviewers), as well as the practical significance of the proposed method still hold. In this regard, does not seem to be a significant improvement.<|endoftext|>This paper concerns zeroth order hard label adversarial attacks on machine learning models, and in particular, how to strengthen them through leveraging manifold information. Also, it seems that this work is not completely grounded on a firm theoretical ground. The desirefor better query efficiency motivated the use of dimension reduction in hard label attacks." or the claim about MI value only supports the proposed method, intuitively? Moreover, the transitions from some parts of the paper to the next parts are not smooth.<|endoftext|>The structure of the paper should be improved. Currently, the paper is written as a stream of arguments (definitions, hypotheses, and observations), which are not well supported. In this regard, I believe the experimental study should be more extensive in order to provide a more substantial empirical evidence for the arguments made in the paper.<|endoftext|>However, there is evidence that weakens the basic assumption this paper is built on, e.g., adversarial data are just bugs. Sadly, I still find some experiments in Section 5 cannot well support the paper s arguments. [Questions]1 In the top of page 3, the author(s) stated that "The desire for better query efficiency motivated the use of dimension reduction in hard label attacks. I do not catch up with the logical link of the above statement. Could authors illustrate more to me? Besides, it seems that all red, blue, and yellow lines are not well separated. I agree with other reviewers  evaluations such as "poor writing", "unclear points", etc.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper studies the question of why the pre trained language model is effective on downstream NLP tasks. Therefore, the authors explicitly add a diversity regularizer to the linear output layer to increase diversity to improve the effect of pre training. The authors mentioned that multi task pre training is the reason why the pre training language model is effective, but the author uses the diversity of multi class for replacement. 4.Whether multi task pre training and increasing multi class diversity can be used at the same time and their combining effect is not explored. Does the diversity regularization method proposed in the paper can still work? Regularization only changes the pre training process and should have nothing to do with downstream tasks. I would like to see this part of the experiment added.<|endoftext|>The authors present a new statistical analysis aiming to explain the success of (masked) language model pretraining for NLP. Specifically, they focus on the *diversity of classes*, which they claim is similar to the *diversity of tasks* for multi task pretraining (e.g., Tripuraneni et al.(2020)).Based on their theory, the authors then propose a *diversity regularizer* to improve model performance. This paper presents a new statistical analysis aiming to explain the success of pretraining for NLP. In fact, I would argue that the author s claim "Our empirical results show great potential of this diversity regularizer." The authors investigate an important topic: the success of pretraining in NLP.<|endoftext|>The paper presents a theoretical view on how diversity of classes can help in language model pre training, and then backs up with experiments consisting of improvements in the downstream tasks due to introduction of a class diversity loss during pre training. Then, the paper explains how a diversity parameter controls the lower bound of the downstream fine tuning. The authors empirically show their method outperforms the baseline in the GLUE benchmark. I would love to hear an explanation of why this assumption is required in this setting. The empirical results, however, are only restricted to BERT which is a Masked Language Model. They support their theoretical results with that on BERT fine tuning on GLUE benchmark. Overall I find the theoretical contributions solid, but I don t know how general it is as we only see experiments in masked language modeling setup. Additional results on either pre training or on different fine tuning tasks should be able to make it a stronger contribution.<|endoftext|>This paper analyzes the effectiveness of pre trained models ondownstream classification task, showing that the "class diversity"(characterized by the least singular value of the last linear) inpre training is important for the target classification tasks afterfine tuning. The paper also suggest a diversity regularizer forimproving the downstream task efficiency. Starting with a disclaimer: I could not carefully verify the proofswith the time I can afford for this review. Although more empirical evidence would be useful, theexperiments provided also support the usefulness of the proposedimprovements (hence the effect of diversity). For example, "Devlin et al.(2019)" on page 1 should be in parentheses, while  when citation is   used as a name as in "...  proposed in (Zou & Adams, 2012)" (p.3),  it should not be in parentheses. First paragraph of 4.2: "F^pre and F^down are *sets of* linear   functions"?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper proposes a method to synthesize regular expressions by first splitting positive examples and synthesizing regexes for those and then combining these while using negative examples as an additional constraint. *Strengths*  The motivation of the work is well laid out. (v) Though the paper claims in the conclusions that their framework is " ...very effective in reducing the time complexity of regex synthesis", this has not been shown experimentally. 2020.Just in time learning for bottom up enumerative synthesis. Both have a lot of redundant information. Results on other values of timeout and other domains ( apart from Regex) will be useful.<|endoftext|>Overall, I think that the proposed idea is itself simple, which means it should be easy to use, and at the same time it could potentially be useful for the task. 1.It is not clear if the proposed approach has an upper limit on the hardness regexes it can generate. Without this understanding, the usefulness of the approach is not clear. Is it actually true that no published work put positive/negative regex examples into a neural network? I have my guesses but am not sure.<|endoftext|>This paper proposed a way of synthesizing RegEx from positive and negative sample specifications. The main idea is to split the positive samples into segments, such that one can leverage any existing RegEx synthesis tool to synthesize the sub regex out of the segments of specifications. I have several comments below: 1. 3.The model relies on the existing synthesis tools for the sub RegExpr synthesis, so the bottleneck would be the synthesis of the most difficult sub expression. Imagine if the sequence is formed from a single segment and in this case the proposed model would not help in any way.<|endoftext|>The authors have presented a "hard" negative example "aabbccc", which can be incorrectly parsed by a regular expression that is too generic, but generating this "hard" negative from positive example "aabbcc" is less likely if the size of the alphabet is larger. > "a given"> "For each regex, we randomly generate 20 positive and negative examples of length up to certain length" > "Recurrent network networks"This is a novel approach to a problem that does not have feasible exact solutions, and I think it is an interesting use of neural networks. The proposed approach is relatively light weight and demonstrates improvement in both run time and performance quality in most cases.
Reject; rating score: 3; rating score: 3; rating score: 6; This paper introduces a Transformer based algorithm for predicting the outcome of a match or a game. The main novelty of this work is the representation of each player with an embedding vector that is learned based on the player s history (previous played games and their outcomes). It would be more reasonable to split the dataset based on the players. The evaluation metric that is used is only the Pearson correlation. It seems that the performance of the proposed algorithm is decreased as the history size is increased. This work presents some interesting ideas but it is not ready for publication in its current form.<|endoftext|>This paper aims at predicting the winning probability of a pairwise comparison. Experiments show that compared with traditional Glicko2 algorithms, the deep learning method has higher predictive accuracy. Strengths:Different from Glicko which only maintains a scalar representing the player’s skill, this paper utilizes player’s embedding for match prediction. It is easy to guess that the Bradly terry model is employed, but I would appreciate it if the authors could explicitly define it as there are temperature parameters to be specified. Though it is known to many readers, I would appreciate it if the authors could expand the details instead of relying on the readers’ preliminary knowledge on this. 5  While Glicko can be used to predict the match outcome of cold start players, can this method be generalized to cold start players too?<|endoftext|>The paper presents an approach to predicting match outcomes between two individuals (teams or players) by comparing embeddings of previous match outcomes of both individuals. The paper technique is very novel and has clear practical applications. I would move my score up if the authors provide statistical evidence of improvements, particularly if using prediction accuracy, rather than correlation coefficients. This is a strong new result even if the components (transformers) are not particularly novel. Additionally, in many practical matchmaking systems it is important that predictions be calibrated. The paper would be much stronger with results reported in terms of predictive accuracy, and ideally also including reports of the model calibration curves (ex: If the model predicts a 20% win probability, what fraction of those matches are wins vs losses?If calibrated, 20% of the predicted matches will be wins.) These results should include comparisons to the other pairwise comparison methods for fair comparison.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The method is based on minimisation of Dirichlet energy. The method acts similar to a low pass filter for missing features, which complements the GNN learning algorithm. This is some interesting future work. This paper addresses a very interesting problem that is relevant to real world applications; dealing with missing node features for graph neural networks (GNNs). How true is this assumption in real graphs, and what can be done in the case where a graph is not connected? Edits:    A_u should be A_uu ?<|endoftext|>The paper address the problem of  missing node features in graph by conducting feature propagation (FP). Experiments on node classification shows the effectiveness of FP. LP also propagate the label probability, which is continuous. The global information of the graph is the key to a better performance in this case, and I doubt other algorithms using the global information can also get similar good performance.) The paper is not so convincing at the current stage. "Learning from labeled and unlabeled data with label propagation." "Wasserstein propagation for semi supervised learning." However, I am not convinced that the idea is novel enough to be accepted in ICLR, details are referred to my main review.<|endoftext|>This paper proposes a fast way to do node feature imputation in graph learning settings. In a nutshell, the idea is to consider the graph topology in the imputation by doing a feature by feature minimization of the Laplacian quadratic form. Through numerical experiments the authors illustrate the benefit of this simple approach. Using the quadratic form of the graph Laplacian (Dirichlet energy) for label propagation or graph signal interpolation has been used several times over the last decade (as referenced in this paper). From the perspective of graph signal processing, the true solution with the inverse of the Laplacian can be seen as an IIR filter and this is an FIR approximation of the filter. Under this perspective, I regard that the novelty in this paper is limited.<|endoftext|>The authors consider the case of training GNNs with missing features. The proposed framework consists of a diffusion based step prior to training the GNN. A discretization of the approach leads to what they term "feature propagation", a scalable method to impute features on the graph. The assumption is that the energy function that determines how related the features are to each other can be learnt from the data. In this paper, the authors specifically look at imputation in the context of training GNNs, and use the graph to perform the imputation. I feel the authors need to address the difference between the proposed method and label propagation a bit more. isn t it better to do imputation in a task agnostic fashion?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; They construct thes SF by an iterative attention module. They build them as an ordered set in which each element focuses on a localized yet discriminant image pattern. Auhtors show that for training, only image labels suffice. They also show that this requires a significantly smaller memory footprint to match their performance. 2) Paper is well written. 3) A comparisson with other state of the art methods. Weaknesses:Not particularly found. Main contribution: A method called: Feature Integration based Retrieval or FIRe for short. It can decomposed into 3 main parts:1) an image representation based on mid level features (super features) and an iterative module to extract them. 2) a framework to learn such representations. It is based on a loss applied directly on SF yet only requiring image level labels. 3) a set of extensive evaluations that show significant performance gains over the state of the art for landmark image retrieval.<|endoftext|>The paper proposes a novel model, denoted as super features, for image retrieval, with the following specific contributions: 1) features are extracted using an iterative attention module (the Local Feature Integracion (LIT) Transformer). The paper presents solid experimental results in public datasets and compare them against reasonably chosen baselines. + The experimental results are solid and relevant. The authors  approach outperform the baselines by a large margin, not only in mAP but also in its memory requirements. This reviewer finds very relevant the fact that a global loss is irrelevant, as it is the dominant approach in most papers in the literature. + Although it is not its strongest point, several details on the architecture and the losses are novel. + The research is excellently written and presented. My understanding is that the LIT will leverage **all** the local features to produce the super features.<|endoftext|>The paper proposes Local Feature Integration Transformer (LIT), a new architecture to extract mid level local features from images that are used for image retrieval. The paper further proposes a learning framework, called Feature Integration based Retrieval (FIRe), to train LIT using a contrastive loss with only image level supervision. A good selection of relevant related work is cited. The paper is well written, well organized and easy to follow. Detailed ablations motivate the design of the method. My main concern are inconsistencies in the mathematical formulation of the method, which I am sure can be resolved for the camera ready version. If not, it would be helpful to clarify in the paper why this is not possible. This could further motivate why applying the loss on the local feature level is beneficial. How sensitive is the performance of SuperFeatures to the initial set of templates?<|endoftext|>This paper introduces the concept of super features into image retrieval. The idea is to aggregate a set of features on the feature maps at different scales according to a set of query vectors representing different latent concepts. Hence, the proposed model somehow implements a transformer, which is called LIT in this paper. This design is just too empirical. ASMK becomes the measurement to compare two images when testing, which is a conventional option in comparing two groups of local features. The motivation and intuition of this paper are quite nice, but some parts of the details are questionable. The results are quite promising. The concept of super features is very intuitive. Some parts of FIRe are not absolutely novel. ## Pros1.The paper has a clear motivation. 2.The majority of the design is reasonable. The major results in Tab. Does it have to be ASMK?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; ## Weaknesses  How to apply the conclusion in this work to further improve the pose estimation is missing. The paper gave a detailed comparison and theoretical analysis between these two methods. A theoretical study on how these two pose estimation methods, detection based and integral regression based, work is missing in the literature.<|endoftext|>This paper investigates the performance inconsistency of integral pose regression (IPR) methods on  easy  pose samples and  hard  pose samples (v.s.argmax methods). They further propose a heatmap distribution prior loss to mitigate the shrinkage. The authors in [1] combine the heatmap loss with the integral loss and show that this improves over the pure integral loss.<|endoftext|>Integral Pose Regression has been adopted by many methods, and it is probably one of the two main paradigms for Keypoint Localization, along with Heatmap Regression. On the contrary, the theoretical study is a little basic, but it is enough to demonstrate the insights of the authors. Overall, I am positive about this paper. Getting more insight about each different method can be valuable to many people.
Reject; rating score: 6; rating score: 6; rating score: 8; rating score: 8; This paper proposes using normalizing flow based belief approximation for continous state POMDPs and learning the belief approximation via variational inference. In addition, the paper proposes learning an actor critic reinformcement learning algorithm based on the proposed belief representation. Accurate belief tracking is important but often hard for continous state POMDP. This paper proposes a normalizing flow based belief approximation, and learning the belief approximation by maximizing an ELBO. An actor and a critic can be jointly learned together with the normalizing flow belief approximation. The experiments show that the normalizing flow based belief achieves higher ELBO than a baseline RSSM on the MNIST sequence dataset, and the actor critic algorithm achieves good performance on several problems from DeepMind Control Suite. The paper is generally easy to follow, and the idea of improving belief tracking by exploiting the power of normalizing flows to represente complex distribution is interesting. However, some important aspects of the idea is not clear to me, making the correctness of the idea unclear. In particular, comments and clarifications on the following are helpful:  In Eq.(7) and Theorem 1, expectation should be taken wrt $q(s_{1:T} | \tau_{T}, o_{T})$. Where do the transition dynamics $p_{\psi}(s_{t} | s_{t 1}, a_{t 1})$ and the observation model $p_{\psi}(o_{t} | s_{t})$ appear in the belief model? If they are used for encoding the actions and observations, then these are not designed to approximate the true transition dynamics and the true observation model, and using them as transition dynamics and observation model in Eq.(12) and Eq.(13) seem unjustified. Eq.(9) actually deviates from Eq.(7), because while Eq.(7) uses a Markov transition model $p(s_{t} | s_{t 1}, a_{t 1})$, Eq.(9) uses a non Markov transition model $p(s_{t} | \tau_{t})$. In the RL algorithm, the actor and critic are both functions of the state, not the belief. How is an action for a belief calculated then? While there is a description of the learning objectives for the actor and the critic, the overall RL algorithm is not described. For example, how is training experience collected? 1?How often are the belief model, the actor and the critic updated? I also have some questions and concerns over the experiments   For the digit writing task, what s the state? RSSM is from the PlaNet paper, not from the paper of Okada et al.2020.The proposed belief model and RSSM are compared only on the digit writing task. For DeepMind Control Suite, only results for 6 out of the 20 problems are reported. This may be because the algorithms are only run for 500k steps, while the results in the Dreamer paper are obtained for 5 million steps. It would be helpful to run the new algorithm longer to see whether it can eventually match Dreamer s performance as reported in the orginal paper. 3: usally  > usually  In the description of the POMDP model, $s_{1}$ denotes the initial state, and it doesn t produce an observation $o_{1}$, but $o_{1}$ appears in multiple notations. **Post rebuttal**I appreciate the authors  effort in addressing the questions and concerns, and I have increased my score in view of the the improved clarity and more thorough experiments. A main weakness of the work is that it mainly replaces the latent model in Dreamer with normalizing flows, as noted by other reviewers too. In view of this, the paper could be improved in several aspects. * Current version is somewhat unclear and misleading about its novelty, and should provide a proper discussion on what it adds on top of PlaNet and Dreamer. * In addition, since the main novelty is in the use of a different latent model, I find the experiments not as thorough as it should be   it should compare with Dreamer on the same tasks, but results are only presented on a subset of them. * The authors have addressed most questions on clarification of the work well as far as I can see, but there are a few remaining concerns (e.g., those in other reviewers  post rebuttal updates), and these need to be discussed/addressed. Interesting idea, but there are some concerns over the motivation, explanation, and experiments.<|endoftext|>This paper proposes learning belief states for POMDPs using flexible posteriors and prior distributions for the state space model. The authors state that related work often restrict the distributions to conditional diagonal Gaussians which causes the model to underfit the data. In the context of POMDPs, the authors propose using the learned model of environment dynamics to train an actor and critic by sampling trajectories, resulting in a highly sample efficient RL algorithm. Results show the benefits of using the normalizing flow compared to baselines that only use a Gaussian distribution. Results show a consistent improvement in sample effiency of the RL framework proposed over closely related pieces of work and other standard RL algorithms (e.g.D4PG and A3C). * Weaknesses1) While most of this work is well written and the method is clearly described, I don t think the proposed contributions is particularly novel. In particular, the idea of using more flexible distributions for POMDPs have been explored (as mentioned by the authors) in the form of particle filters. While these methods are referenced (e.g.Ma et.al.2020b, Igl et al.2018), they are not compared or seriously discussed beyond an unsubstantiated claim that they suffer from insufficient sample efficiency. The authors also fail to cite an even more closely related work that directly addresses (and in my eyes, go beyond what the authors explore in this paper), see Gregor et al.Gregor et al.not only also suggests using normalizing flows, but find that using a convolutional DRAW (VAE model) outperforms flows for learning a model of complex environment dynamics. 2) The second contribution proposed, namely the POMDP RL Framework is an effective way to leverage a model of the environment, as it allows to use imaginary trajectories to train TD(lambda) actor and critics. The authors should make it clear how it differs from Dreamer beyond the use of normalizing flows. If there are no differences, then it should be clearly stated that they make use of an existing method. See PlaNet and Dreamer works for examples of algorithm description wrt. Furthermore, it would be interesting to see the performance of the method for different number N, to get a sense of how useful it is to increase the number of trajectories and whether there are diminishing returns. Some other minor issues:  3 seeds seems like a small number and some experiments still have large error bars (e.g.Cheetah Run and Walker Run). I suggest increasing the number of runs from 5 to 10 if possible. When comparing with PlaNet, it would be more informative to also show PlaNets training curve instead of converged performance after an arbitrary number of steps (1e6 steps). It may well be that PlaNet is just as sample efficient but somehow plateaued/converged a bit slower towards the end of training. **Post rebuttal update**I appreciate the willingness of the authors to discuss and address our concerns, and their efforts to improve their work. The paper is now clearer in its contributions, limitations and additional experimental results and details will make it more useful to the community. During our discussion, the authors have agreed that the method makes some assumptions that limit learning policies and values that can reason about state uncertainty (which is now better captured thanks to the normalizing flow)   it seems that this would be the main reason for using the proposed flow based model. Instead, the environments in the experiments have essentially no stochastic temporal dynamics. Another minor point, is that I find it a bit puzzling that Dreamer+multiple imagined trajectories seemingly fairs quite a bit worse than Dreamer (Fig.7)   this is a bit counterintuitive (I would have thought that at worst, performance would be the same) and I would ve liked further explanation. There are a large number of deep generative models proposed in the literature that are much more flexible and suited to different domains. The authors propose using one of these, normalizing flows, and apply it to learning a model of the environment dynamics in POMDPs settings, and in turn use it to train agents in a sample efficient way. While I think this is a good idea, and experiments indeed show the benefits of using such a model, the contribution in itself is not very novel, as prior work has suggested using richer models (see referenced works on particile filters, or the work of Gregor et al.which also refers to the idea of using flows). I think this paper would be more interesting if it compared a wide range of flexible models, and included a more extensive set of datasets. Alternatively, the contributions would also be strengthened if it provided novel ways to use the learned models when training agents in POMDP tasks. As it stands, I do not recommend accepting the paper due to its limited novelty.<|endoftext|>Belief states are a common solution to learning in POMDPs, and in recent years people have applied deep recurrent generative models to approximating belief states in complex POMDPs. In this paper, the authors go beyond the isotropic Gaussian assumption used in most deep learning works and use normalising flows to represent more flexible and multimodal distributions. The authors demonstrate that their approach is better at modelling multimodal belief states and matches/outperforms Dreamer, which is a natural baseline for their method. To the best of my knowledge this is the first study on normalising flows in this context (the authors do cite other work using normalising flows in RL which do not apply in this context). The sequential MNIST results are convincing, but the results on DM Control Suite are not so strong   having other domains where the benefit of normalising flows are more pronounced would make this more attractive than simply sticking to simpler isotropic Gaussians. The major weakness is the significance/novelty of this paper. Is it fair to say that this is Dreamer + normalising flows latent space + multiple imagined trajectories? Knowing the literature and from the experiments it would seem so, but the paper is written in a way that makes it unclear how exactly FORBES differs from prior work. On a side note, it is interesting that Deamer + multiple imagined trajectories does not seem to work that well   possibly due to the unimodal latent space. Further to this, while the DreamerV2 paper is cited with its argument for multimodality, DreamerV2 and its discrete latent space (which is also much more flexible than an isotropic Gaussian) are not discussed   which would seem an omission given the supposed benefit of normalising flows over isotropic Gaussians. On a minor note, there are several spelling mistakes throughout, e.g., "beilef", "poorfs". **Post rebuttal update:** I agree with the authors that FORBES is more general than an alteration to Dreamer, and there are theoretical contributions that are of relevance to the community. However, given my awarereness of the literature and the concerns of the other reviewers, I would hesitate to move my recommendation much higher. **Post rebuttal update 2:** The authors have put considerable work into addressing the concerns of the other reviewers, and it is now a more solid paper. Although I would appreciate a more thorough discussion of related work (perhaps in supplementary material), e.g.of why certain baselines were not included, I think it is worth accepting given the updates. The authors presented a relatively simple improvement to solving POMDPs by replacing isotropic Gaussians with normalising flows. While this does indeed work, the technical and empirical contributions are reasonably significant, and so I would recommend this for an accept.<|endoftext|>This work addresses model based reinforcement learning (MBRL) in partially observable visual control tasks. The principal novelty of the work is the use of normalizing flows (NFs) in the belief inference model. The paper derives the evidence lower bound (ELBO) for the proposed inference model, and presents a related RL framework. Empirically, the proposed model s inference capabilities are tested in the sequential MNIST domain, and its control performance is evaluated in six continuous control problems. Positives:  The paper demonstrates that normalizing flows can improve control performance and sample efficiency in MBRL over approaches working with a Gaussian assumption. While VAEs and other generative models have been applied to belief inference, this appears to be the first work applying NFs. It is not groundbreaking novelty, but seems like a solid evaluation of the idea. One of the advantages of POMDP planning is the ability to reason about future uncertainty. The sequence MNIST result where the proposed method is able to predict either 3 s or 7 s is quite exciting, since it shows the method has some capability for such reasoning. This aspect would be interesting to see even more details about. Negatives:  There are several missing baselines and comparisons in the empirical results, and some lack of clarity about parameter choices (details below). Throughout, the paper seems to conflate in writing and symbol use the belief state and the hidden state. 3.2., but should be revised overall. For conceptual clarity, the paper should in my opinion be careful to separate 1) hidden states or samples of hidden states (typically denoted s in POMDP literature) 2) belief states, or distributions over the hidden state, and their approximations (typically denoted b in POMDP literature). Detailed comments:There are a couple of omissions in the evaluation, which should be justified or a comparison added:  The paper does not compare to PlaNet Bayes (Okada et al., 2020) in the control tasks. This is the previous state of the art in the area. Since the paper does compare to PlaNet Bayes on the MNIST setting, I am wondering if it would not be possible and reasonable to compare on the control tasks as well. Igl et al.(2018) "Deep Variational Reinforcement Learning for POMDP" is also very closely related to the proposed method. In some works addressing partially observable RL, belief states are inferred by using a recurrent neural network directly on the action observation history. It would be nice to see a simple RNN baseline in the control tasks. A missing ablation is how the number of imagined trajectories N affects the results. This should ideally be done in a domain where Gaussian or other simplifying assumptions on belief inference do not work, otherwise even with a single trajectory results could be OK. I also could not find information on how many trajectories were used in the experiments. As mentioned in the strengths, it would be great to have some further details on the method s capability for reasoning about future uncertainty. For example, is the proposed method better than RSSM always, or especially for long prediction horizons? Other comments & minor:  Theorem 2 seems to not seem to add much to the paper, and does not merit a full reproduction. Figure 3 needs some improvements. It is not really clear what the colors here are indicating, or what happens at the intersections of the arrows. The author rebuttal does a good job addressing my concerns on presentation and ablations. I am raising my score to reflect this. This paper proposes to use normalizing flows as belief representations in a POMDP, addressing drawbacks of VAEs or diagonal Gaussian approximations in representing multi modal beliefs. While not highly original, the work is technically sound, and provides the necessary empirical evidence to merit acceptance. Nevertheless, I encourage the authors to address the concerns raised by other reviewers as well, and to provide comparisons to related works the authors also agree are highly relevant.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; The paper on hands tackles the problem of multi class classification in the presence of a general  background class. Thus, there is the clear recommendation not to accept the paper for ICLR! In addition, the paper is not written and structured very well.<|endoftext|>This work is motivated by the different nature of the "other"/background class in many vision tasks such as object detection and semantic segmentation. When the paper mentions it uses center loss for L_compact, it does not cite the any papers about center loss. Basic discriminability loss is the conventional classification loss for C+1 classes, intra class compactness uses center loss, and background margin loss is a margin loss acted on the center representation of all C classes and the features of background samples.<|endoftext|>This paper aims at dealing with the C+1 classification problem, where C foreground classes and a background class, which could consist of other classes that do not overlap with the C foreground classes. The paper additionally includes center loss for foreground classes and a modified center loss for background class(es), on top of the standard cross entropy loss. Overall, the paper severely lacks novelty. The writing is unsatisfactory.<|endoftext|>I enjoyed reading the paper because it tackles a well identified problem with clarity and concision. Updates: Thanks for the authors  response, which partially addresses my concerns. The problem addressed by the author is important (the background class is present in most of the vision benchmarks) but few work (to my knowledge) attempted to tackle it in a rigorous way.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposes an architecture (AGILE) for handling variable action sets through pairwise attention between the currently available actions, where the action set is free to change between episodes, and each action is described by an associated feature vector. AGILE’s benefits are most clear for the CREATE environment. WeaknessesAGILE’s margin of improvement over baselines is not large enough to outweigh the spurious effects of inadequate hyperparameter tuning (a well known issue in deep RL). Unfortunately, the paper makes no mention of hyperparameter tuning. This work has much to admire, and could make an important contribution if the uncertainty regarding its hyperparameter tuning didn’t cast such doubt on the experimental results. POST REBUTTAL UPDATE  The authors have provided much more information about their hyperparameter tuning, and have performed additional experiments which provide additional insights. These changes address my concerns, and I have revised my evaluation.<|endoftext|>This paper introduces a novel policy architecture (AGILE) for RL agents that learn action interdependence from a varying action space. Evaluation is done using a selection of benchmarks and application domains which I view as a positive. Authors address a subset of this broader problem, namely when the action set only changes at the beginning of a task instance. The main contribution and the novelty of this paper lie in the utilization of graph attention networks to learn the dependencies of actions using a fully connected action graph. Specific implementation details of how AGILE can be used in widely used RL methods (PPO, DQN) are also given. I particularly like the inclusion of the application based evaluation. For example, it would be useful for the reader if contrastive cases are given for the presented examples. The paper is also well written and easy to understand. The formulation of the architecture is clear, though some network descriptions and parameters are missing. The authors do not compare the work of (Chandak et al, ,2020b) to the proposed method (though they have briefly discussed it in the related work section) computationally. While authors do use masked action sets and utility policy as baselines, other action varying baselines are needed for a comprehensive evaluation. Another minor weakness in the paper is the lack of clarity in the action graph formulation. After the rebuttalI appreciate the clarifications made about the selection of the benchmarks and I agree with the authors here. This paper presents an approach that addresses an important problem in RL, namely how to act optimally in an environment with varying actions.<|endoftext|>This paper tackles an RL problem setting in which the actions available to an agent vary from episode to episode, and the optimal action in some states depends on the other actions that are available. The authors’ approach to this setting is to use a graph neural network to process all available actions, both to summarise the available set and to produce a relationally informed representation for each action. This work frames itself as an extension of Jain et al.2020 to consider the relations between actions rather than evaluating their utility independently. I believe the authors have addressed all of my concerns in their responses and updates to the paper. I find there is some room for improvement in the exposition of certain details, and in aligning the discussion more closely with the empirical findings. Do they use the raw action representations like Summary GAT? One question might be how relevant the quality of the raw action representations is? I would appreciate it if the authors could clarify the role of these ablations, and which conclusions may be drawn from them.<|endoftext|>This paper considers the reinforcement learning problem where the action space is fixed. Having a variable action space makes it necessary to learn interdependence between different actions (use of some actions might depend on the existence of other complementary actions), which is modelled via a graph attention network. This point is illustrated well in the paper; AGILE outperforms benchmarks that ignore action interdependence (in particular, Jain et al.) For instance,* How are the nodes of the GAT determined? However, I do not understand how this issue was resolved; the only explanation given is that "the lists were built incrementally, one action at a time." * In Section 5.3.2, the authors mention that "they collected interaction data" for the real world recommender system environment. * I think how the experiments with train vs. test actions are conducted need to be clearer. It is very clear there in what aspects AGILE differ from the existing work. * Needing a hammer to be able to use a nail is used as the main example of how action interdependences come into play. The key idea of the paper is illustrated well but many details regarding the approach and the experiments are left out.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; S2.A novel optimizing mode of grafting two different optimizers is proposed. W1.The paper structure is strange. Actually, I don’t think ADAM#SGD will be better than ADAM. In a multi dimensional space, this direction is composed of the value of each gradient and positive or negative(symbol) of each gradient. However, you change the symbol of some parameters’ gradient according to SGD. In my view, this method is more like a SGD with multiplying a large const to its gradient. If not, I think Figure~1 is a wrong example, cause M#D will step to different direction with D in multi dimensional space.<|endoftext|>The paper presents an interesting technique of grafting for the problem of step size hyperparameter tuning, and opens up questions as to the power of simple per learning rate schedules. The study also does not present theoretical underpinnings for the technique that would be helpful for understanding if indeed the results could be more widely applicable. Overall, the paper presents an interesting technique and opens interesting questions, but without further empirical results or theoretical underpinnings, the results presented are insufficient to assess how generalizable the findings might be. Then the norms of the steps each would have taken is computed, and used to combine M’s magnitude update with D’s direction update.<|endoftext|>The authors investigate the entanglements between the optimizer and the learning rate schedule and propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedulefrom a tuned optimizer to a new optimizer, preserving empirical performance. The Algorithm and the method are quite simple. This to be is the question they should address thoroughly. I think what the authors are proposing is this: If the change is in the optimizer only; how should I change the learning rate schedule? But, there are additional questions. Q1.What is the impact of this method on generalization error? If yes, with what confidence? The paper, although is interesting, lacks the technical/empirical novelty to merit publication<|endoftext|>## OriginalityThe paper is original, performing an experiment that I have not seen in the literature, and satisfactorily discusses background and related work. Instead, the results of several disparate experiments in disparate settings are reported, making it hard to reason about the generality of the results. Section 2.1: how does this deal with stochasticity in the gradient estimate? Based on Eq.1, it seems like $g_t$ is the gradient for a minibatch, but the algorithm does not specify minibatch selection. The notation of M#D and granularity could be improved; I kept having to scroll up and try to figure out which refers to magnitude and which refers to direction, and the granularity is specified separately in text. Regardless, my critiques are ultimately minor, and I do still think that the paper should be accepted.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper investigates a novel learning scenario, where the learner has limited access to the global data distribution and can share learned model parameters with a so called service provider through multiple (but limited) rounds of interactions. The motivation is interesting and seemingly useful for the scenarios described in the introduction. However, there are a few concerns/ questions in the problem setup and the proposed assisted learning protocol, please see the detailed review below. Can the provider infer the learner’s data distribution from the exchanged information? The paper investigates an interesting learning protocol, proposes an intuitive assisted learning framework, but lacks sufficient theoretical justification and empirical support for the significance of the solution.<|endoftext|>3.Theorem 1 is an asymptotic result for $r \rightarrow \infty$, but the goal of the paper is to minimize the number of rounds, so concrete bounds on $R$ are desired, i.e.$R$ should appear in the bound. The problem setting is novel and may be relevant for real life deployment of ML. 2.The theoretical analysis is rigorous. The paper considers a novel problem setting and has a promising direction.<|endoftext|>I think there may be a scenario assumed in the paper, but I m curious to see if there are any specific scenarios in which the service provider already has enough data of the distribution that the learner wants to learn. The paper proposed a simple and straightforward assisted learning framework for a learner with limited and imbalanced data. I wonder whether the experimental settings are where learning is good enough only with the data the provider has or not.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper considers Wasserstein and Sinkhorn trust region policy optimization (WPO and SPO) for reinforcement learning. Theoretically, it shows the performance improvement of WPO at every iteration, and that SPO converges to WPO. It also conducts some experiements to illustrate the advantages of the proposed methods. The paper provides a relatively complete analysis of the Wasserstein and Sinkhorn policy optimization, including closed form policy updates, performance improvement bound, actor critic algorithm, and experiments on popular instances. It is unfortunate that such connections are not recognized in the paper. The main feature of this paper is considering a non parametric policy class for Wasserstein policy optimization.<|endoftext|>This paper studies policy optimization in reinforcement learning with Wasserstein and Sinkhorn trust regions. Compare to the standard TRPO which based on KL divergence, the proposed WPO and SPO go beyond the parametric policy distribution class. The authors also derive closed form policy update, as well as theoretical performance guarantees for both problems. The paper is overall well written, the results are well organized and reasonably clear to follow. Also, the bound for $\beta^*$ for WPO in Theorem 1 seems to be independent of $\lambda$, I would like the authors to discuss some insights behind the different dependency on $\lambda$. Seems like an upper bound could be $1/\lambda$, but is this correct (or optimal if it is correct)? Small typo in Abstract: ‘extensions of policy optimziation’  > ‘extensions of policy optimization’. The numerical results also suggest that proposed policy optimization methods outperform the standard TRPO and PPO with better performance and faster convergence.<|endoftext|>This paper proposes to use two extensions of the TRPO algorithm relying on the Wasserstein distance and the Sinkhorn divergence which dont require to explicitly specify a distribution for the policy. The authors provide a theoretical analysis giving a closed form policy update for their two methods and a performance improvement bound in the case of Wasserstein policy optimisation. They evaluate their methods empirically on tabular domains (Taxi, Chain and Cliff Walking) and on some discrete locomotion tasks (Cartpole, Acrobot). The methods is theoretically grounded so I would recommend an accept but I believe the authors would need to evaluate their method on more domains to make the paper stronger.<|endoftext|>The paper introduces Wasserstein and Sinkhorn policy optimization, with three main contributions. First, the paper derives closed form expressions for updates using Lagrange multipliers. I found the exposition to be mostly fine with some minor critiques. First, it would be better to have a more detailed treatment of the prior work (TRPO is properly introduced later). They are very closely related and the text (and math) would benefit from making that clearer. "Compared to the performance bound when using KL based trust region(see, e.g., Schulman et al.(2015); Cen et al.(2020)), using the Wasserstein metric yields a tighter performance improvement bound and is more robust to the choice of parameters βt." This choice makes the relation less clear. Overall, this is not the most ambitious paper, but generally a nice contribution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This work studies the problem of ensuring node level privacy when training GNNs. Is the analysis presented here some how improving over this intuition or is there a more technical reason why this intuition breaks down? This should be made more explicit. ### UPDATE ###The paper could significantly benefit from the feedback given here and the comments they plan to make. I am still not convinced of the novelty here in the privacy analysis.<|endoftext|>They claim that providing privacy on both node feature and their connectivity is novel. The main motivation of the paper is valid and active research area. More specifically, the author defines GNN as arbitrary number of MLP on node features, aggregation operation based on graph adjacency, followed by arbitrary number of MLP on aggregated features. They used exactly one layer of aggregation but one or two MLP for both on node features and aggregated features. 7.Seems in the literature, differentially private networks results were performed for different privacy costs(Abadi et al., 2016). Bob sends this data to Alice to develop a GNN for some specific tasks. So the novelty of the paper is limited. However, graph based prediction or node level predictions are not so different to each other. Any GNN architecture can be used for either node (or edge) or graph level prediction. Transferabililty from train graph to the test graph is not the problem. 4.The authors main concentration was on node level tasks.<|endoftext|>It theoretically proves that the proposed algorithm is guaranteed to be differential private;3. This work proves the differential privacy only on 1 layer GNNs. Showing the form of DP in the more general r layer case would make the contribution more significant. 2.The experiments are not complete. 3.The introduction is a little unclear. This paper considers the node level DP problem on graphs, which is the first work on this task as far as I can see. The problem is formally defined, and guaranteed privacy is proved. However, the proof is incomplete as only 1 layer GNN is considered.<|endoftext|>This paper proposes a private algorithm for Graph Neural Networks at the node level. The extension of the DP SGD technique to the graph neural networks is very interesting, and the empirical performance seems improved. Then in the experimental sections, I feel the benefit over DP MLP is from incorporating the graph information. 3.The paper claims that the mini batch is uniformly sampled from all training nodes, which contrasts the sampling with replacement in the traditional method. What are the differences? There seems no discussion about why this modification is important for the privacy amplification results. 4.For Table 2 and Table 3, what s the privacy parameter epsilon? In summary, I think the technical contribution of this paper is not significant, but it s also worth having a DP algorithm for the GNNs.
Reject; rating score: 6; rating score: 6; rating score: 6; The paper proposes a new way of sampling trajectories, by using a policy only for a fixed number of steps and changing to another policy. While it is a novel choice to sample a trajectory consisting of a number of perturbed different policies, I am not sure whether we can compute a valid value function out of these trajectories. in Eq.14, we are learning a single critic over return estimates from many different perturbed policy samples. What policy is this value function computes value for? For accurate advantage computation and policy update in Eq.15/16, the advantage should be computed with the value function for each perturbed, policy; however, It seems that the paper is learning a single critic over all perturbed policies and computing the zeroth order gradient based on it. This may be OK for Mujoco like domains where the optimizing the short term sum of rewards is enough to get an optimal policy, but it can be problematic in the domains where long term reward is important. It is a very important point on the correctness of the main algorithm, and I may change my position if the authors can address this concern. On the empirical evaluations, it is interesting to have experiments showing various strengths of ZOAC, especially its robustness and ablations. However, the overall performance evaluations do not seem very competitive when compared to sota actor critic algorithms, e.g.SAC can achieve a return over 10000 at 1e6, and it would be much better to include such algorithms for comparison. Actor with (64,64) hidden units is also not a standard choice, and it would be more interesting to see the results with different number of units, e.g.(128,128) and (256,256).<|endoftext|>This paper developed a very interesting zeroth order actor critic algorithm that nicely integrates gradient based critic training with gradient free actor training. However, without knowing the experiment results, it is not easy to determine the true advantage of the new algorithm on hard problems at this stage. d. It is widely known that the performance of deep reinforcement learning algorithms can be highly sensitive to detailed experimental settings. The review adequately justified the technical innovation of the new algorithm. As carefully pointed out by the authors, the performance results reported in the paper introducing TD3 may not be identical to those presented in the paper introducing SAC. It remains questionable to me why the new algorithm is designed to integrate first order PEV with zeroth order PIM. I am not convinced about the technical and practical necessity of adopting zeroth order PIM instead of first order PIM. There are rooms for improvement regarding the experimental evaluation too. 2.The new algorithm introduces additional hyper parameters, including N. While the performance impact of N has been evaluated on one benchmark problem in detail, it is not clear how to set this hyper parameter properly for other reinforcement learning problems. First, the authors should consider more challenging benchmarks including Humanoid in order to clearly show the performance advantage of the new algorithm. Comparing to other first order approaches, such as SAC and TD3, is necessary IMHO. Third, based on the learning curves and the results presented in table 1, the performance of ZOAC is not significantly better than PPO. Following the feedback provided, I see the necessity of mentioning the following:a. In the feedback, the authors highlighted several advantages of ZOO, including its capability to train policy where gradient information is hard to obtain or even unavailable. This is a good point.<|endoftext|>The paper proposes an actor critic reinforcement learning algorithm, compatible with continuous actions, that combines a Monte Carlo computation of the gradient of the actor (based on workers that perform rollouts from the current state) with a standard supervised learning based critic update rule (using gradient descent for several epochs). This combination is used to compute target V Values for the critic, and the weighted average used to compute the gradient of the actor. The paper is overall well written, and both the actor and critic part of the algorithms are relatively easy to understand (especially with the pseudocode provided in appendix). The algorithm makes sense, and the results are convincing. The introduction does a great job at motivating why a zero order policy update is desirable, even though the list provided in paragraph 2 could be made more prominent. One small limitation of the paper is that two extra baselines would have been interesting in the experiments: SAC and A3C. The Soft Actor Critic has been shown to outperform PPO, and other old baselines, such as A3C, may be interesting too as they use several workers to collect rollouts (leading to better exploration). A minor remark is that Figure 1 did not help me understand the algorithm, while the pseudocode helped better. The paper presents a well motivated and elegant idea to train an actor critic algorithm, and the empirical evaluation is sufficient. I m borderling recommending acceptance (I would be happy to see the paper accepted, but could live with it being rejected in case I missed anything).
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposes FedPAGE, a federated variant of PAGE, which is claimed to attain the SOTA communication efficiency theoretically. The argument follows by the statement that running K > 1 of local steps are not worse than running K   1 step. I read the response and decided to keep my initial score. 1.Table 1: the convergence round does not account for the full participation round.<|endoftext|>The paper proposes a new federated learning method, called FedPAGE, which uses the PAGE gradient estimator in local update and provides convergence analysis for both convex and nonconvex setting.<|endoftext|>This paper provides a new federated optimization method: FedPAGE, based on the recent variance reduction based PAGE method. FedPAGE improves over variance reduction based SCAFFOLD algorithm in both smooth convex and smooth non convex settings. This is never addressed by the authors. 2.Most of my concerns about the experiments are not addressed? It is expected that authors discuss all such  cases. 6.Thanks for pointing out the technical novelty, although they are a bit limited.<|endoftext|>This paper proposes a new federated learning algorithm to reduce the communication cost on both federated convex and nonconvex optimization by applying optimal PAGE method into the federated setting and running multiple local update steps on clients before communicating to the orchestrating server. Reducing the communication cost  with multiple local updates by applying PAGE in federated optimization.<|endoftext|>And as PAGE has good convergence guarantees, compared to other distributed optimization methods, FedPAGE shows good convergence results compared to other federated learning methods with guarantees. I agree with other reviewers that the paper has flaws and issues that have not been addressed and it is not ready to be published. This is important because adding local updates is at the heart of most FL algorithms despite the fact that there is little theoretical evidence that they improve the method. Unfortunately, these points are not very clearly conveyed in the main text. And 3) the devices can drop out during any round which means that you cannot count on all devices for a round.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; Following recent works studying the effect of model scaling on tasks such as transfer learning, this paper asks whether scaling up the size of a pretrained network also helps alleviate or minimize catastrophic forgetting. For example, would you expect better or similar results with a balanced version? I also highlight that these details are important and should at least be mentioned in the appendix. Recent works on scaling laws for transfer learning primarily focus on understanding the effects of pretrained representations when learning a single new task, but this paper extends the theme of model scaling to understand its benefits when learning multiple tasks. The experiments are extensive: authors evaluate several architectures, different pretraining paradigms (supervised and self supervised), the impact of pretraining dataset size, amount of fine tuning, and also show comparisons with training from scratch. I liked the representational similarity figures, which indicated representations across classes become more orthogonal with model scaling, which is likely to prevent catastrophic interference when learning new tasks. **Weaknesses:** My main apprehensions focus on the paper’s experimental setup and some missing details in the figures. If this happens, then one possible confounding conclusion is that the benefits of scale are primarily enabling better pre trained representations. However, the two datasets have a significant class overlap and I am uncertain if one can regard training on a subset of CIFAR classes as a new task. This is also highlighted by Figure 8 of the paper, which shows that the pre trained models have orthogonal representations across CIFAR 10 classes even *without* training the model on new tasks (please correct me if I misunderstood and the models have been fine tuned on the new tasks). While I understand CIFAR datasets are standardly used benchmarks in the continual learning literature, past work in the field has either used randomly initialized networks or models pretrained on datasets that do not have any overlap with the downstream tasks’ data. * Even for the current experiments, I think it is important that the overlap of pretraining data vs. CIFAR data is highlighted. I would be happy to raise my score if the authors justifiably address these apprehensions. **Post Rebuttal**: The authors provided detailed response to the raised concerns (though primarily in the appendix) and sufficiently backed up their proposed hypotheses. Importantly, I note that I could not find exactly what set of learning rates was used for fine tuning.<|endoftext|>The paper studies the impact of large scale pre training in continual learning. Moreover, The paper shows that pre trained models suffer less from forgetting in vision and NLP tasks with various experiments. **Strengths**:  (1) The paper is well written and easy to follow. (2) The main motivation of the paper (i.e., role of scale and pre training) is significant. **Weaknesses**:(1)  I think the major problem with the arguments of the paper is that it mixes the over parametrization and pre training together. For instance, the paper mentions: “catastrophic forgetting is mitigated to a large extent by scale: that is, larger models suffer less from forgetting”, which is not a true argument in my opinion. The high performance is due to the “pre training” part and not the over parametrization. In fact, [2] shows that over parametrization by increasing depth is not very helpful for continual learning. (2) If we agree on the fact that the benefit comes mostly from pre training, then the other problem is that this has also been studied before [1] for both vision and NLP benchmarks. So I believe the fact that pre training is helpful is not a new phenomenon in the CL literature. (3) The final major problem is with the experimental setup. I believe a correct pre training setup should be what people are doing in the NLP literature and what authors have done in Section 3.4. Then, in that setup, one can argue that the knowledge of the model is related to the language model, which can be used in different downstream tasks with different data. Thus, I have increased my initial score. Moreover, the experimental design has flaws as mentioned in the main review.<|endoftext|>The authors study using ever larger pretrained models and suggest this can solve the problem of catastrophic forgetting without the use of any particular continual learning method. The similarity of the pre trained task and the task sequence is not properly decoupled in this work. Should we conclude that pre training alone is the key or pre training on very similar data distributions to the sequence that follows? I would have liked to see at least one experiment with a legitimately diverse input distribution from the pretraining distribution (e.g.different object categories from previously seen or non natural images). Many details of the  forgetting frontier points don t seem to be provided, for example the learning rates used or other optimization hyperparameters don’t even seem to be provided at all in the paper. It also will make it challenging to reproduce the results. The use of pre training seems to suggest we should evaluate forgetting on the imagenet 21k as well to get more insight into the behavior. Have the authors consider this? What if the first task is big (e.g.1/4 of the imagenet 21k data), if followed with a 2 and 3 task how is this type of sequence really different from the pre training formulation here? A more natural setup seems to be to split imagenet 21k into a series of tasks and assume the first task is the “pre trained” model. This would also end up with a situation where the CL sequence distribution does not clearly follow the pre trained data distribution by separating them via object categories.<|endoftext|>The paper explores the effect of scale on catstrophic. The length of finetuning isn t as impactful on pretrained models as randomly initialized modelsThey perform the majority of their experiments on a two task, task incremental Split CIFAR 10 dataset. **Positives:**  Overall, I enjoyed the direction of this paper. I think it was asking interesting questions, and the experiments were quite thorough with respect to the scale of the dataset and model  The results show quite a clear picture in terms of the effects of scale on forgetting in the settings the paper explores. It would be interesting to see whether these results would hold for many tasks in sequence. Many datasets used in the current literature often have anywhere from 5 20 tasks, and it s unclear how well the results would hold for those settings. This paper does a lot of good things and is fairly comprehensive in terms of trying different model sizes and dataset sizes. These results may not hold for longer task sequences that are often used in continual learning, and thus I recommend a borderline reject.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; ### Strengths* Rethinking continual learning with unsupervised representation learning is interesting, and empirical results indicate that most supervised continual learning methods can be improved by the proposed approach. * A bunch of experiments have been conducted to demonstrate the effectiveness of the proposed approach in various settings. And several visualizations have also been included for a better understanding of the learned features. Adding this result could better reveal the difference between the proposed method and CURL in terms of effectiveness. A clear explanation about this performance drop should be added.<|endoftext|>It shows that recent self supervised learning methods are efficient tools to learn image representation with lower catastrophic learning problems. The widely used mixup method is also adapted to the UCL problem. Strengths:  First the paper is well written and easy to read. I especially appreciated the experiments in Fig 2 that investigate the impact of the size of the training dataset. The conclusions are enlightening and will be very helpful to design new supervised or unsupervised CL methods. The paper shows interesting results that confirm the potential of self supervised learning methods in the context of continual learning.<|endoftext|>It shows that the representation learned with UCL is more general than the one learned with supervised CL (SCL), and investigates why UCL is more robust to catastrophic forgetting than SCL by analyzing the similarity of learned features and visualizing loss landscape. * Overall, the paper is well written. I like the idea of unsupervised continual learning and handling it by the proposed LUMP method. Hopefully the authors can address my concern in the rebuttal period. [After rebuttal]The authors addressed most of my concerns, so I would like to raise my score.<|endoftext|>Unlike most of the continual learning approaches in the literature that perform supervised training at each learning stage, the authors propose to perform unsupervised representation learning on the sequence of incoming data and then classify the samples at each stage using K NN. Experiments on standard CIFAR10/100 and Tiny ImageNet shows that the proposed method alleviates catastrophic forgetting and generalizes better in different scenarios. Strengths* The submission is well written and easy to follow. Weaknesses* I think the main limitation of using the proposed pipeline in practice is runtime and memory constraints. Overall a good quality submission with novel and interesting ideas.
Reject; rating score: 5; rating score: 5; rating score: 6; In this paper, the authors propose to use contrastive learning for matching in latent space in the Wasserstein autoencoder (WAE). Experimental results show that the proposed method, MoCA, is more stable and converges faster than existing methods. Since Table 1 shows that WAE GAN is better than MoCA, shouldn t it be compared with WAE MMD? The authors show in their experiments that MoCA achieves faster convergence than existing methods, but they do not fully explain why MoCA shows such convergence. In Section 4.1, the authors compare WAE MMD with MoCA as the original WAE algorithm, but I do not understand why they do not compare it with WAE GAN, which is also original. The authors do not seem to have verified such a thing. However, the explanation of the claim and the presentation of the results are insufficient.<|endoftext|>This paper proposes a new approach to train Wasserstein auto encoders (WAE) with contrastive learning techniques. Specifically, the paper proposes to enforce the marginal matching constraint of WAE by exploiting the fact that contrastive learning objectives optimize the latent space distribution to be uniform over the unit hypersphere. I notice this is a re submission from ICLR 2021. Thus some of my comments are based on the differences between two versions. 3.I think it would be interesting to see how to integrate the instance contrastive loss as in DC VAE [2] into the proposed MoCA. However, I still find the results on image datasets such as CIFAR10 and CelebA hard to justify the superiority of the proposed method over baselines. I lean towards weak rejection but am willing to amend my score if my concerns are addressed.<|endoftext|>The paper presents a regularization technique for Wasserstein Auto Encoders, based on contrastive learning. Simpler terminology such as just using encoding and decoding functions would be more than sufficient. 2.Some of the experiments are interesting and show the effects of the proposed regularization e.g.on the singular value distribution of the latent representation. FID is not a perfect measure and the samples from baselines should be shown side by side with the proposed approach to know whether there is indeed an improvement. I do not believe that FID is a proper measure of quality (not just for this paper but for measurement of GAN sample quality, in general).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposes a novel approach for policy learning in a context dependent reinforcement learning setting (a subset of POMDPs). The proposed method is based on an elegant approach with Dirichlet Processes, and performs well on tasks where the benchmark algorithms chosen by the authors struggle or even fail. The experiments demonstrate that the proposed method performs well even on tasks where the benchmark algorithms chosen by the authors struggle or even fail. (4) seems to be quite a simplification.<|endoftext|>Doesn’t removal of such contexts affect the Markovian property of the contexts? The main contribution of the paper is to present an algorithm for this setting, whose design follows a Bayesian approach. I have limited knowledge about learning in the CMDP setting here and the Bayesian approach employed.<|endoftext|>I could not reproduce these results from the main body of the paper alone. As best as I can tell, a contextual MDP is just an MDP where the state transitions and reward distribution shift around, possibly during training and possibly just at test time. Perhaps the authors feel that contextual MDPs is a sufficiently distance problem and prior art in these areas is therefore irrelevant. The trick used to derive equation (4) is cute.<|endoftext|>Pros:  This paper is well written and the proposed methods are discussed in detail. The literature review is pretty comprehensive and the authors did a good job at classifying the related works. Ideally, a principled Bayesian inference procedure estimated the posterior distribution for all the parameters. Minor concerns:  It would be better to introduce the full name of the methods (e.g.Soft Actor Critic) before using their abbreviations. In this work, the number of contexts is always truncated by a fixed number, is there any possibility that this work can deal with the potential infinite contexts with HDP?<|endoftext|>This paper proposes to model mode transitions (where each mode corresponds to a different MDP) in reinforcement learning with the Hierarchical Dirichlet Process (HDP) prior in a contextual MDP. During model learning, it also prunes out modes that have low transition probabilities into them to avoid modeling spurious contexts. Assumptions about the environment have to be made for their model to work well, which could be unrealistic in some setups. The paper studies a challenging problem setting, and demonstrates the effectiveness of the HDP to model this particular setting.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; In this paper, the authors propose an efficient regularizer for the trace of the Hessian. Experimental results on CIFAR 10 and CIFAR 100 are presented: the proposed regularizer helps, but unfortunately not in a statistically significant way. But even in that light, I think the idea is natural/worth writing a paper about. * I think there needs to be a careful study of hyperparameters of the regularizer i.e., prob and regularization coefficient etc. The idea is interesting, the approximation is nice, but the experimental results are too weak/incomplete to make this a convincing paper.<|endoftext|>This might or might not explain some of the degradation results we see for some of the other techniques. They present two efficient stochastic estimators of the trace, based on the Hutchinson method and their own extension to it, specific to NNs. The paper starts well, with clear and simple goal   to introduce a new regualrizer. Good and simple idea. One of the sections seems completely redundant to the narrative, unless the authors provide better explanation for their final statements there. The stochastic estimators of the Hessian through Hutchinson method are well known and used in many places in ML. Second, the main point at the end of this whole section, talking about stability of equilibrium points, is to argue that the goal of the method is to have a "more unstable dynamical system".<|endoftext|>They suggest adding the trace estimator as a regularizing penalty term for training neural networks with improved generalization. The authors then present experimental results evaluating their suggested regularization method in combination with and compared to other regularization methods on  CIFAR10 / Resnet18  CIFAR100 / Wide Residual Network  WikiText 2 / 2 layer LSTMStrengths:  The connection of Hessian spectrum and trace to generalization is an interesting and promising area of research and it is great to see results for a method regularizing the Hessian that seems to outperform other regularization methods  The structure of the paper is clear and the paper is understandableWeaknesses:  The connection between theoretical justification and practical method is not always clear and the relevant details could be expanded:My feeling is that the connection between using the Hessian / Jacobian with respect to the logits vs with respect to the parameters is not as straight forward as it is made out to be in the paper and could be explained better. This question could be expanded upon. Similarly, an explanation of the difference between the trace of the loss Hessian w.r.t.to logits vs w.r.t to the weights would benefit the paper. For example it is well known that the Hessian w.r.t.to weights for a deep neural network can have negative eigenvalues but the Hessian w.r.t.to logits for a convex loss has only positive eigenvalues. Regarding the linear stability argument the paper could be improved by empirically demonstrating that regularizing the Hessian trace leads to an optimum with a Hessian that has "less stability", perhaps by analyzing the eigenvalues at the optimum. Furthermore, is there a connection between the idea that increasing instability helps generalization to the literature that says that flat optima help generalization?<|endoftext|>The paper develops a new regularization method for deep neural networks. The proposed methods penalizes the trace of the Hessian. This adaptation uses a dropout mechanism to efficiently compute trace of the Hessian. The paper also studies the effects of minimizing the trace of the Hessian using linear dynamical systems theory, and shows that lowering the trace of Hessian diminishes the stability of the equilibrium points in the parameter (i.e., weight) space. The technical details are accessible, and it is easy to follow the mathematical steps presented in the paper that lead to the final "efficient" algorithm for estimating the trace of the Hessian. The paper contains no results using the first method.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The learning leverages the variational framework originated from VAE, with GCN as the core component for posterior parameterization. Experiments on link prediction and community detection show that the proposed method is able to achieve better empirical results. Empirical results on the selected benchmarks seem to be significant as well. However I have several concerns regarding the current draft. So no matter how good the ideal posterior is, the framework proposed by the paper is not guaranteed to learn that. For example, why one has to adapt the IBP as a prior for Bayesian nonparametric purposes? From the appendix it seems that the authors are using a 3 layer decoder by default. To justify the benefit of hierarchical graphical models, one should run the experiment with a 1 layer graphical model, parameterized with a 3 layer neural network. Having results on the latest benchmarks like OGB would be more convincing. The paper lacks a motivation and justification for the proposed model design. The experimental results are not convincing enough and larger scaled experiments on OGB are needed. # After rebuttal: I would thank the authors  effort in addressing my concerns. However the concerns like the technical contribution and experimental improvements are not fully resolved, but I do appreciate the authors  effort in making the code accessible which improves the reproducibility in the community.<|endoftext|>This paper proposes a VAE based graph representation learning model for directed graphs. 3.The learned node embeddings have some interpretability. Experiments did not compare with state of the art non VAE based GNN methods, nor discussed them. It can work on directed graphs too. Merely from Table 5 and results reported from [2], SEAL achieves better performance on Cora and Pubmed than the proposed method. However, people are switching gradually to larger datasets with millions of nodes/edges such as Open Graph Benchmark [3]. 3.No ablation study on whether the proposed $s,\gamma,\delta$ are useful. [1] Zhang, Muhan, and Yixin Chen. Overall, although the paper proposes some interesting ideas by explicitly modeling community structure, latent positions, and random node factors, the paper lacks a discussion and comparison with state of the art non VAE based baselines and uses relatively small datasets in the experiments, thus is less convincing in performance. Considering the abstract states "The experimental results on real world graphs demonstrate that our proposed model achieves the state of the art performances on link prediction and community detection tasks", I cannot recommend an accept given that stronger baselines clearly exist.<|endoftext|>This paper studies a deep latent space model for directed graph representation. Gravity inspired graph autoencoders for directed link prediction. The strong points include (1) a VAE with some well tailed designs to directed graphs, (2) efforts to interpret latent variables, and (3) seemingly promising experimental performance. The technical weakness mainly comes from the design of the model architecture and experiments. I’d like to see an intuitive explanation and experiments that show z^(i) conditioned on H_z^(i) performs better than alternative model architectures. (3) In terms of experiments, I would ask the authors to try some directed graph networks as baselines, not limited to variational or Bayesian. As the authors said, most baselines are “designed for undirected graphs and are not appropriate for learning on asymmetric adjacency matrices”, and the authors modified these baselines for directed graphs. Given the above content, I think this paper is an okay submission, and I will give boardline rejection. (2)In the first sentence of the second paragraph on page 4, the authors said four types of latent variables, but in the first sentence of the second paragraph on page 2, they said three types.<|endoftext|>In this paper, the authors proposed a new directed graph representation model. The proposed model is developed from a Bayesian viewpoint and is implemented in the framework of the variational autoencoder. The details of the model and related derivations are provided. Its learning algorithm is based on (amortized) variational inference and the implementation is compatible with neural networks. I like section 5, which provides some useful insights into the model. Cons:It seems that the proposed method is not as good as GGVAE on some datasets. It is worth doing an ablation study to demonstrate the necessity of this latent code. Overall, the methodology of this work is solid, which is a reasonable combination of several solid probabilistic models.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Besides, they design a two stage framework to train an OOD detection model by leveraging the generated pseudo OOD data. Strengths 1.This paper claims that they propose a general and much more efficient out of distribution detection model. The question is why to select CVAE as the efficient model to generate the OOD data. What is the motivation?<|endoftext|>A numerical comparison shows this method is able to outperform CSI on a couple of datasets (while it underperforms on most other ones) . ############################I thank the authors for the response. * The paper is quite clear and easy to follow. Although these methods do not apply for tabular data, but I suspect that CVAE will also not work well there.<|endoftext|>The paper is well written, covers the related works, and importantly, is well motivated. The usage of  known OOD  samples in order to improve OOD detection is very common, and while in some times there is abundance of such `known OOD` data, that is often not the case. As other reviewers have pointed out, the selection of the CVAE is not justified enough, and a serious ablation study should be added to justify it and not other generative methods.<|endoftext|>As such methods do not train an extra model, the proposed method s efficiency becomes also vague. The module consists of a classification module (classifier and feature extractor) and a generation module (encoder and decoder). In the second phase, CGA utilizes the generated pseudo OOD data to fine tune the classifier. Strengths1) Use of the class conditioned generative models to generate pseudo OODs seems novel and interesting to me. 2) The paper is well organized and easy to read. Following the analogy, I think adding more baselines that use knowledge from the classifiers can be beneficial. 3) As CGA utilizes label information for training the CVAE, I am curious about the robustness of CGA in the case of corrupted, or noisy labels.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper proposes Nuisance Randomized Distillation (NuRD) to deal with spurious correlations induced by a changing relationship between labels and nuisance variables, where covariates are also correlated with nuisance. The text is still heavy on math over intuition, but improved. The paper further describes how to find the optimal representation that has maximum information with labels. Too much technical details in the Introduction without adequate context, e.g.too early for “We show that the distribution of the label given the covariates under the nuisance randomized distribution is minimax optimal for the nuisance varying family, if the label and the nuisance are independent given the covariates” and all the text after. 2.Same for the Methods section, which repeats much of the Introduction (with some math interleaved). A huge problem in medicine is using genomic or imaging data to predict neurodegenerative disease status with age being nuisance. Would be great if NuRD works on this problem. My evaluation is based on the simplicity of experiments for scenarios where NURD worked. Post revision SummaryThe authors have nicely addressed most of my comments.<|endoftext|>This is done by fitting a nuisance randomized distribution to the data, and finding a data representation under this distribution that makes the nuisance and the label independent. Experiments show that by using a classifier learned on this representation, the method is able to improve classification performances by limiting the impact of nuisance variables. **Strengths**Nuisance induced spurious correlations are a major issue in many applications, since they lead to models that cannot generalize well to unseen data. How robust is NuRD to this? The exposition of the paper however needs to be greatly simplified by giving a lot more intuition and implementation details.<|endoftext|>This paper develops a representation method called nuisance randomized distillation (NuRD) for building predictive models with better generalizability (i.e.testing performance) without using spurious nuisance label correlations in observed data. NuRD breaks the nuisance label dependence and finds the most informative representation to predict the label for every distribution in the nuisance  varying family which is a set of distributions that differ only in the nuisance label relationship. The NURD method is evaluated on several application datasets; it produces accurate predictive models on the datasets with strong spurious correlations between the label and some nuisance variables. It provides a good theoretical and experimental analysis of the proposed method.<|endoftext|>One goal here is to train a model to be robust against these distribution shifts. The paper presents results comparing against ERM and argues that it is unique in its problem formulation compared to other methods that make assumptions about environments or specific spurious correlations. From what I understand of the NuRD method you estimate the nuisance variable using a neural network and then use this to reweight the training samples based on this. I find the evaluations lacking. The proposed formalization and method are interesting. However, the presentation is not focused on the core contribution, the method is lacking detail, and the experiments do not characterize the method well.<|endoftext|>The paper studies the prediction problem under spurious association. The idea is to construct a "nuisance randomized distribution" based on the observed distribution. The nuisance randomized distribution is constructed by reweighting the observed data. In general, this paper studies a long standing problem of spurious association in fitting a predictive function. The intuition of NuRD is clear. If we have data from a distribution where the nuisance variables z are randomized and do not cause the label y, the prediction will not suffer from the spurious association. A major concern is about the setting of the data. 2.The paper lacks sufficient clarity (e.g, Sec.2.).A main reason might be that the paper focus on the problem of spurious association, which is a causal problem. 5.Some implementation details are not presented, such as how to separate x and z, how to choose the hyperparameter $\lambda$.
Accept (Poster); rating score: 8; rating score: 3; rating score: 3; Results give good evidence of the proposed method as well. This paper would be stronger if the 3 points mentioned: exposition, other baselines, and related works can be addressed, I am willing to raise the score accordingly. The author addressed the 3 points mentioned, so I am raising the score as I said I would. The querying network is trained to generate the next observation x conditioned on the existing observations {(x1,y1)...(xk,yk)} in such a way that maximally reduce the uncertainty of the F space once the new example, (x, p*(x)) is augmented to the existing observations. ### related worksThere are very relevant prior works that studies the relationships between optimally querying examples for program synthesis from an information theoretic perspective:[1] https://arxiv.org/pdf/1704.06131.pdf[2] https://arxiv.org/pdf/1711.03243.pdfSpecifically, these works formally justify the strategy of greedily picking the next query (i.e.picking x_t that greedily "maximize the mutual information between the input output examples [[e]] and the corresponding program p", same as this work) as more than a heuristic, but is in fact (1 1/e) as good as the globally optimal solution that picks (globally) a set of k examples that maximize the mutual information between [[e]] and p. The proof involves showing that adding examples is monotonic and sub modular in reducing the set of satisfiable programs. Results are convincing to me. ## what this paper needs work onThis paper would be much stronger if the following points can be addressed. if so it probably should be justified, at least with a few words why this particular form of contrastive learning is a good choice here. Since we use the same acquisition function both in training the synthesizer and at inference time to query, the synthesizer receives the examples from the same distribution, thus, we do not run into the problem of out of distribution examples at inference time. The paper definitely makes an attempt to make this claim, but it is unclear in the current version. this is a standard approach in active diagnostics where one keeps a sampled distribution of valid hypothesis, then select the query point that maximizes disagreement.<|endoftext|>In neural program synthesis from input output examples, the goal is to generate a program which, when executed on the inputs, produces the corresponding outouts. The set of all possible programs is very large or infinite, but the set of useful programs that the user might want is smaller. In this work, the authors propose to instead have the program synthesizer interact with an oracle to determine the set of input output examples. The synthesizer proposes a potential input for the program, the oracle produces the output, and the process repeats. Typically, the set of input output examples is taken as given by the program synthesis method. A neural decoder module takes the mean/variance of this combined distribution as input to produce a new input; we can query the oracle with the input to get the output, resulting in a new input output example pair. For program synthesis from input output examples, there often is an implicit oracle for the unknown program that the user can use, even if it is not cheap to compute. The paper doesn t describe what happens for invalid inputs to programs (i.e.inputs which cause the programs to crash). Query decoder for list processing: since input examples     "additional batch normalization is added to keep the generation more stable": how was this done? "to guarantee that every query can be recognized by the program simulator, we design the output layer of the query network elaborately. ": it is not clear what was done in the output layer to guarantee this.<|endoftext|>They merge multiple unit tests using an attention mechanism into a single point in the function space and then train the unit test generator such that the mutual information between the probability distribution on the merged unit test and the probability distribution on the programs is maximized using InfoNCE loss. A synthesizer network is used to synthesize the program from generated unit tests. However I think the paper as written needs improvement before it can be accepted. This is something that should be examined. It s also unclear what assumptions they are making that guide their design choices (along with theoretical or empirical validation for those assumptions). (4) They should compare against more baseline methods. For example they mention Padhi et al.which present multiple queries to the user and let the user pick which one to label. This they claim puts additional burden on the user. But they could still compare against Padhi et al.in a way that s fair to them by selecting for example a query at random for the user to label out of the ones that Padhi et al.suggest.(5) The empirical results with the baselines they compare against aren t very compelling. They also don t give error bars for their results. The problem is interesting but the paper suffers from some incorrect claims, insufficient motivation and justification, and underwhelming empirical results.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; Does this mean that the other methods are simply not learning the correct predictions on the domain they do select? As is standard in selective classification, supervision of the value of $g^*$ is not given, and instead only $(x,y)$ pairs are provided. Does this method have hyperparameters? The scheme proposed for the same is interesting, and, taken at face value, very effective. That said, I don t think the paper makes this case sufficiently well, instead posing the setting as a sort of Huber type contamination of the data that persists to test time, but is kind enough to be entirely separate from the real data   this is quite unconvincing to me. This I think weakens the results significantly. Budget Learning via Bracketing.<|endoftext|>The paper considers a setting where the label is random noise if the input is in an uninformative subspace, provides theoretical analysis for the estimated predictor/selector that minimizes classification risk/a novel selector loss, proposes a practical iterative algorithm to approximate the estimators based on MWU, and tests on semi synthetic experiments. The methodology is supported with theoretical results and implemented by an iterative algorithm. While the problem formulation requires uninformative samples are separated from informative ones, the authors also tested more challenging settings in the experiments. There are previous papers related that are missing here. For example, "Combating Label Noise in Deep Learning Using Abstention" by Thulasidasan et al.also seems to do very similar things and has similar title, but is not discussed in the related work. The paper is well written in general.<|endoftext|>The examples are very stylized, and likely heavily leverage the fact that they match the assumed generative model. Showing how this translates to a more realistic example would have been helpful   as the analysis may have a chance to be suggestive for more practical settings as well. 2) I am surprised that using methods which allow uncertainty quantification   e.g.maybe bootstrap resampling, or say Gaussian processes would have a hard time in this setting. 5) You mention that the predictable region has to be high signal to noise. How sensitive it is to this? This paper proposes a novel formulation supervised learning with abstention.<|endoftext|>The authors analyze selective learning when part of the data is pure noise and uninformative. The paper is well written, clearly organized and easy to follow. The problem of identifying uninformative data is interesting and very relevant in modern machine learning. The empirical studies are not exactly supported by the theory as they are essentially different algorithms. The paper studies identifying uninformative data in datasets which is a very relevant problem for the machine learning community. The algorithm analyzed in the paper is supported by interesting theoretical results and can be revealing for more realistic algorithms.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The authors propose TESSERACT, an aggregation scheme that is robust to the directed deviation attack (proposed in Fang et.al.2020).Pros:a.The defense is based on an interesting observation that, for a sufficiently small learning rate, as the model approaches optima in the benign setting, a large number of gradients do not flip their direction with a large magnitude. The paper proposes a defense against a specific form of attack and does not provide any guarantees or justification about why it should generalize against other more powerful forms of model poisoning attacks. On pg.2, the paper argues (without justification) that other attacks are less damaging and essentially weaker than the attack in Fang et. 2.While the authors provide two very interesting adaptive attacks, it is hard to generalize the strength of defense without any formal guarantees, In other words, can there be no other adaptive attack that can bypass this defense. 3.Typically, for reasons of privacy, the gradient updates are usually encrypted before being sent to the server (secure aggregation scheme by Bonawitz et.al.2017, https://eprint.iacr.org/2017/281.pdf). Please see the detailed comments above.<|endoftext|>Then, it would be better if the authors tested the proposed defense method on the poisoning attack involved in the baseline schemes. In order to defend against such a poisoning attack, the authors developed TESSERACT, an aggregation algorithm that assigns reputation scores to participating clients based on their behavior in the training phase and weights the client s contribution. In particular, the experimental results show that TESSERACT provides robustness against even a white box version of the attack. Local model poisoning attacks to byzantine robust federated learning. In 29th USENIX Security Symposium (USENIX Security 20), Boston, MA, August 2020. For example, “k 1,2,…,m” should be “$k 1,2,\ldots,m$.” 5. The editorial quality of this paper is not always satisfactory. It contains quite a lot of inconsistent/non precise descriptions, as also reflected in the above comments. 8.In addition, non IID data seems also to affect the gradient direction (or value) of the client.<|endoftext|>The authors propose a novel defense against the recently popularized attack "Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks " by Yang et al.This attack reduces model availability by sending malicious updates from compromised client that maximize sign flips in the global model gradient. This defense then proposes a measure of change in gradient direction that can be evaluated for each local update and used to dynamically down weight clients with a large number of flips in direction. This submission presents a reasonable and timely defense against a strong attack against data poisoning in federated learning, which is nice. The design of the defense is well executed and the inclusion of a dynamic reputation is a good addition to its robustness. I do have a few comments,  mostly regarding the part of the paper concerning adaptive attacks:* The submission discussses *an* adaptive attack against this defense, but I would like to understand and see more discussion on why this is a strong adaptive attack specifically. It is currently not clear to me that this is a strong adaptive attack.<|endoftext|>___AFTER REBUTTAL: score increased to 6 which I will increase additionally to 8 if the authors are willing to support the claim that Tesseract is a "secure by design" defense. FURTHER UPDATE: score increased to 8, and I stand by my decision unless other reviewers point out that the claim of Tesseract being "secure by design" is flawed. Overall, the presentation of the paper is very good. The references should be improvedThe contribution is significantSTRENGTHS:+ Adaptive adversary + Trendy subject (federated learning)+ Evaluation on multiple datasets+ Technically soundWEAKNESSES  Unclear assumptions and threat model. Even if the baseline performance does not decrease, what is the overhead of the proposed method? 28th {USENIX} Security Symposium ({USENIX} Security 19). Let me elaborate on the above mentioned weaknesses, starting from the most significant ones. Does it work “only” against the “directed deviation attack” proposed by Fang et al.? **Problem or Feature Space attacks? •	In Figure, the gradient “LM_{c 1}” is out of place. The paper tackles a very interesting problem and the many security considerations as well as the experiments on various datasets and comparisons with existing defenses are commendable. The “flip score” is a measure introduced for the first time in this paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper presents an unsupervised learning model for disentangled latent representation learning. Specifically, the model works on a combined latent space including both entangled variable and separable variable. The most impressive part is the one at a time (OAT) factor learning approach that iteratively uncovers disentangled dimension and learned from reconstructed samples. However, paper clarity can be improved, which has impeded reader s understanding and rating of the paper. Contribution wise, this paper is not the first ones to use an "entangled+disentangled" latent space (eg paper named "Toward Controlled Generation of Text" in 2017), but the OAT learning approach is interesting. The paper needs more explanation on how to avoid posterior collapse and avoid the collapse of disentangled latent variables. Weird Table 1 columns and not strong experimental metrics.<|endoftext|>This study proposes a disentangled representation learning method called "one at a time", or OAT factor learning which is a VAE GAN network to generate high resolution samples and to learn variational factors in an unsupervised manner, without knowing the number of ground truth factors a priori. The authors reported their experimental results for two datasets, i.e., dSprites and CelebA. The proposed formulation of latent space that splits latent factors to two disjoint sets is also not novel. The only novel part is assuming an upper bound for variational factor, where it is not clear how the model can find the true number of factors, while $|z_1|$ is gradually increasing up to $K$ over iterations of training. Quality: The motivation of the work is good; learning a disentanglement representation is quite a challenging problem. While the authors claim that the model does not need to know $|z_1|$, it is not clear how the model learns the true $|z_1|$. The authors might need to refine the discussion on the casual factors and disentailment studies, based on the findings of this paper. I got confused. There are some typos in the text, e.g., “/mse”, “a a” at page 3. The proposed method tried to address this challenge.<|endoftext|>The proposed method is Variational Autoencoder (VAE) based. The latent variables consist of nuisance factors and disentangled factors that form the $k$ generative factors per dataset. In my opinion what is needed to improve the paper is more clarity and details regarding the training procedure, e.g.model selection criteria for the different training stages, ablation studies on the learned number of disentangled factors $k$ and a better description of their central and only results table. The authors demonstrate the performance of the proposed model on the dSprites and CelebA dataset. Although this work presents an interesting approach to learning disentangled representations, in my opinion it is not ready to be published at ICLR at this stage. To help learn the disentangled factors of variation the authors propose to use an additional discriminator and interventions on the disentangled factors. Did the authors also experience that? The authors do not write much about how they selected their models. Also, I cannot find a reference to table 1 in the text. There is no ablation on the number of learned factors of variations $k$. In section 3.4.2., step 1 the authors write that the $\gamma_i$ are set to zero and, hence the corresponding latent factors are not trained. Also in section 3.4.2, step 1, the authors describe their one at a time training approach by switch the $\gamma_i$ from 0 to 1.<|endoftext|>The paper proposes a new approach for training disentangled generative models. On top of VAE, it separates the latents into two disjoint groups: disentangled and entangled ones, and progressively increases the size of the disentangled group one at a time. To encourage disentanglement, it changes one random dimension of disentangled latent and pushes it to decode and then encode into the same one, and has a GAN loss to make sure that the changed generated distribution matches the ground truth distribution. The paper shows the performance of the proposed approach on dSprites and CelebA datasets. However, it has several issues, including inaccurate claims, insufficient experiments, and many typos. See below for more details. However in real datasets, in addition to the independent factors common to all points in the dataset, there might also be some correlated, nuisance or noisy factors pertinent to speciﬁcally only certain data points . Please fix this statement. * The paper mentions in many places that the idea of progressively learning one factor at a time is new. * If the main selling point of the paper is the ability to disentangle real world datasets with **unknown number of factors**, then the paper should conduct experiments to demonstrate that (i.e., on the settings when the approaches that set a pre defined number of factors fail, and show how your approach can improve that)*** Typos **** Page 2:  related Work * Page 3:  /mse * Page 5: missing a period after  factors are integrated out * Figure 1 caption:  so we ﬁrst group the correlated latents into one space, z_1 .
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper proposes to prune graph structure based on hashing edge features with LSH. The author evaluates the proposed method by comparing with other sparsification methods. Cons:1.The designed method is too simple: just a preprocessing and not interact with learning.<|endoftext|>This paper studies an important problem, but the proposed solution is not well motivated and the experimental results are inconclusive. he problem studies in this paper is an important research direction and a proper solution to the problem can be quite valuable. This is especially the case because as the authors mention, in 2 out of the 4 datasets in Figure 3 a random selection strategy seems to be on par with the proposed approach.<|endoftext|>**Pros**:  Graph sparsification is an important and interesting problem due to the high computation complexity of GNNs. Based on the discussion in the related work section, the proposed LSP is novel for graph sparsification. From this paper, it is not clear why applying LSH to graph neural network is the challenge. However, there are existing neural network based deterministic graph pruning/sparsification methods that need to be compared. This paper is a Local Sensitive Hashing (LSH) based sparsification algorithm, but LSH is not well introduced and is a little bit confusing.<|endoftext|>This pruning is referred to as locality sensitive pruning (LSP). Stronger motivation for LSH, over other methods of edge pruning, for example [2] (see [3] for example use of edge pruninig) and comparison with other edge pruning methods would be beneficial here, and comparison with other GNN sparsification methods would strengthen the emprical evidence for the benefits of the proposed LSP. International Conference on Machine Learning. 2019.The main concern with the paper is the lack of comparison with any other methods that sparsify the graph. I think comparison with these existing GNN sparsification methods is important as the end goal of speeding up inference time while preserving predictive performance is the same. With these comparisons, it will better position the paper.<|endoftext|>This paper proposes LSP, a graph pruning method based on Locality Sensitive Hashing (LSH) for GNN acceleration and regularization. The paper s idea is interesting, as evidenced by the experimental results. Although this idea is interesting, the primary contribution of this paper is a combination of existing techniques rather than a new method, insightful observation, or theoretical exploration. However, the theoretical depth and novelty may not be enough to meet ICLR s standards.
Reject; rating score: 5; rating score: 5; rating score: 6; In particular, data distributions are formulated as mixture of distributions (i.e., domains), and two distribution shift scenarios are considered: (1) domain shift, where the test domain and train domain are disjoint. (2) subpopulation shift, where test distribution has different mixture proportion than train distribution. It s  assumed that domain identification spuriously correlates with labels. To tackle this problem, this paper proposes two mixup strategies: (I) mixup two examples with same label but different domains; (II) mixup two examples with same domain but different labels. It s claimed that such mixup could cancel out the spurious correlations. Extensive experiments on a variety of datasets show its superiority compared to empirical risk minimization (ERM) and alternative data augmentation methods. The paper further provide theoretical analysis that under certain conditions, the proposed method has asymptotically smaller worst case classification errors than ERM and vanilla mixup. the idea is quite simple and intuitively reasonable, and empirical results seem extensive and significant, and theoretically justified to some extentsome of the results analysis is a bit confusing to me(1) in 4.1 "evaluating robustness to domain shifts", the best strategy was to always mixup same label with different domains, and the potential reason given is that the datasets actually have weak or even no spurious correlation between domain and label. Is it easy to quantify such spurious correlation?<|endoftext|>Rather than using distribution/risk matching schemes as often done by previous work, they propose to train models against mixtures of data points as a means to avoid that models rely on spurious correlations between domain and class labels, since such correlations observed during training might not hold at testing time. The proposed setting uses the idea of mixup to combine data instances in two different schemes: I combine data points from the same class but different domains, and II combine data points from the same domain but from different classes. Strengths:+ The proposed approach is simple and efficient, and can be directly incorporated in or combined with other invariance inducing approaches;+ Prediction performance is shown to improve over a number of recent baselines under challenging benchmarks. Weaknesses/suggestions:   The proposal requires assumptions that are not discussed in the manuscript. In particular, authors claim that the mixup strategies they introduce yield some type of domain invariance, which is not verified empirically. "Domain generalization via invariant feature representation." That s another case where the evaluation lacks in supporting authors  claims. However, experiments supporting key claims are lacking, and it s unclear whether the observed improvements in terms of invariance (either at feature  or prediction level) hold true since no supporting experiments are reported. Conclusions from the risk bounds provided in theorems 1 and 2 are a bit unrealistic given the strong assumptions that imply the results. Finally, regarding novelty, it seems different recent approaches introduce methods that use some sort of mixup across domains in similar settings.<|endoftext|>The paper considers the model robustness under distribution shift brought by domains and subpopulations. Specifically, based on the interpolation scheme in mixup, the authors propose two selection strategies to perform data augmentation, aim at eliminating the spurious correlations and learning an invariant representation. Strength:(1) The authors address a critical point that prevent models from generalization, namely spurious correlation. (2) The proposed method is simple and easy to implement, and the empirical results are within expectation. After Response The response from authors has addressed my major concerns, so I raise my score accordingly.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper provides a novel analysis on fair training in three different settings. Basically in this scenario local clients are not collaborating and they are just training separate global model. It is also not clear how do the authors optimize the FairBatch objective in the FL setting. I assume the step of solving the optimal global to follow the standard FedAvg algorithm. From Figure 4 it seems that the authors are summing over that loss over all the clients? This paper also compares the empirical results of FedFB with other fair FL training methods and demonstrates an advantage of using FedFB over other methods for fairness. This paper focuses on fairness in FL from an interesting angle.<|endoftext|>Thus the most assumptions in B.1 are valid. 2.The experiments in the paper are not extensive. The proposed methods have not been compared with some recent papers on this topic. The authors use the data setting in the above paper but fail to compare it. It seems the authors use the iid setting across the clients and it is not a practical setting. 2.The complete convergence analysis of the proposed algorithm is missing.<|endoftext|>This problem has been investigated in the literature for the centralized setting and the authors propose to extend FairBatch (FB) to the federated setting. This work is quite novel but its theoretical statements solely rely on very specific use cases and lack clarity. The authors speak about extending with their work FB. It would be interesting to compare FedFb with FB where every clients data is centralized on the server. With UFL, every client trains a fair model on its data. It is standard to consider that the amount of aggregations $T$ goes to infinity but not for the amount of local work $R$. Please elaborate or point me to a reference?<|endoftext|>This paper investigates how one can achieve group fairness under a decentralized setting. The authors develop a theoretical framework for decentralized fair learning algorithms and analyzed the performance of existing approaches including UFL, FFL via FedAvg, and CFL. They also propose a new federated fair learning algorithm FEDFB by letting each client share extra information about the unfairness of its local classifier with the server, which then computes the optimal samples weights for the following round of local training. The experimental results demonstrate that FEDFB achieves state of the art performance, while still ensuring data privacy. I enjoy reading this part. Question:  Overall, I recommend the paper. The writing is nice and clear, especially the discussion in Section 3.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposed a method for UDA object detection. Under this motivation, the paper proposed modules of domain adapative instance normalization, global style alignment and local content alignment. +: The key assumption of the proposed method is reasonable, however, it lacks comparison to other methods under similar assumptions. Meanwhile, there is no discussion on related works of this aspect. The proposed method improves over SWDA and HTCN, however, they are not the SOTA. Is there any evidence to support this assumption.<|endoftext|>The paper introduces a method to tackle the unsupervised domain adaptation problem, focusing on the object detection problem. In light of the aforementioned consideration, this reviewer believes that the paper has some merits but, the lack of a proper presentation, and the fact that there are not enough experimental results to support all paper claims, result in a submission that is not solid enough to justify an ICLR publication.<|endoftext|>This paper proposes a novel adaptation framework to balance transferability and discriminability for cross domain object detection. the nearest cyclist is not detected. They are like the combination of some typical modules in a generative model. The paper points out that image level style and instance level content matter in cross domain detection.<|endoftext|>This paper presents a new method for cross domain object detection, which employs the style aware feature fusion method and two novel modules to build the model. Global style alignment and local content alignment have been carefully considered. The proposed method is evaluated on multiple datasets and achieves promising results. This is an important issue given the fact that “domain shift” occurs widely in the real life applications and annotations are very costly to get.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper proposed a framework to integrate contrastive/self supervised learning into meta learning literature. The authors demonstrate that contrastive learning principles implemented in meta learning methods such as R2 D2 can achieve comparable results on various computer vision tasks. Even though the paper does not extend such a framework to the level of other meta learning literature that datasets and tasks are largely different, I believe the concept proposed in the paper may be a good inspiration for future self supervised learning research. Therefore, it has limited novelty. I do not see how this setting is different when fitting into the meta learning framework. Even though the idea itself comes from meta learning is not novel, I think that demonstrating such an idea can work in a self supervised learning framework is interesting and can probably benefit the general self supervised learning research community. 4.The authors provided all experimental details for reproducing the results. I understand that it s not possible to reproduce standard SimCLR due to computational budget but it seems that the comparison is not satisfactory. The concept of combining self supervised learning and meta learning is interesting and may have a bigger impact in the future.<|endoftext|>The paper at hand shows relations from self supervised learning (e.g., contrastive learning) and meta learning (e.g., fine tuning). Whereas the shown relationship is interesting it s not that novel and I would even argue that several teams have implemented similar ideas already. Overall the paper is quite well written and easy to understand and follow. As said above, novelty might be a bit limited but I can see the benefit of highlighting the relationship of meta  and contrastive learning for many readers in the community. Experimental results are in favor of the approach, however a more compressive analysis (e.g., confidence intervals) would have been appreciated.<|endoftext|>The paper first shows that the current popular contrastive learning for self supervised visual representation learning shares some similarity with a meta learning framework for few shot learning. This inspires the authors to propose a meta learning framework for self supervised learning. Experimental results also indicate its potential effectiveness. Experiments on the impact of iterations within the inner loop are needed to show more in depth comparison between the meta learning framework and SimCLR. Learning self supervised visual representations via meta learning is new and interesting. A core claim and also the title of the paper is "contrastive learning is just meta learning": contrastive learning can be interpreted as a special case of meta learning with a certain task distribution. However, its main arguments are not convincing to support this statement. Also, one should think about the "core characteristics" of these two frameworks, and show one is a subset of the other. For example, I did not see anything "meta" in SimCLR. 3.The idea of Large Rotation as Auxiliary Loss reads similar to prior work on classifying random image rotations as a pretext task for self supervised learning.<|endoftext|>This paper formalizes a connection between the training procedures of (few shot) meta learning and contrastive learning. The main weakness with the paper is the lack of comparison to the large body of work comparing unsupervised learning and meta learning. Most notably, Hsu et al.(2019) showed that meta learning algorithms can be used for unsupervised learning, and many of the papers that cite it have explored transferring techniques in both directions. 3.[minor] The paper title is aggressive, suggesting that we can stop doing contrastive learning and focus on meta learning, even though the only connection is in the data generation during training, not in the goals of the two learning paradigms. *Unsupervised Learning via Meta Learning*. ICLR 2019. The main concern with this work is that the main insight seems to be a somewhat more specific variant of observations made in past work on the connection between modern unsupervised and meta learning. While there are some interesting experimental results, I lean against accepting given the lack of any analysis concerning what the novelty is here compared to those papers.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; CLIORA is based on DIORA model. Overall, the proposed method is interesting and inspiring. The idea should be interesting to both unsupervised parsing and multimodel communities. Weakness:1) The image features are only used for computing the inside pass.<|endoftext|>The model was built on top of an existing unsupervised grammar induction model used for text without image information. The execution of the paper was quite good, and the results are convincing.<|endoftext|>This paper introduces a task of joint visual linguistic grammar induction from parallel image text data, presents models and metrics for the task, and shows strong empirical results. This paper introduces the task of joint visual linguistic grammar induction, and presents models, metrics and empirical results on it.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper studies the prior $\pi$ in PU learning. When unsatisfied, such an assumption will often lead to the overestimation of $\pi$. The authors propose a new CPE problem based on a new auxiliary distribution that always satisfies the irreducibility requirement. The technique is called regrouping, which transforms the true distributions to  Both theoretical and experimental results are included in the paper, showing the validness of the proposed ReCPE method. **Strengths**  The paper is well written and easy to follow. This work is well motivated and novel. The paper addresses the overestimation problem of $\pi$ in PU learning by proposing a regrouping technique. They help support the authors  claims. While I understand that the method is aimed at estimating the prior, I think that it would be beneficial if the authors can provide the PU learning results with different priors (fixed, CPE, ReCPE, etc.).A simple nnPU would suffice. Many "irreducibility" has a typo ("irreduciblility").<|endoftext|>The authors consider that the existing methods usually fail if the data distribution dissatisfies the irreducibility assumption. To address this problem, the authors introduce a method named Regroping CPE (ReCPE) which tries to transform the original data distribution to an auxiliary data distribution, such that the produced distribution always guaranteeing the irreducibility assumption. The proposed methods can be considered as a pre processing method and used together with any CPE method. This paper introduces a particular situation in which the irreducibility assumption cannot be satisfied, which has not been studied (even considered) in previous PU learning literature. The authors give a very simple and intuitive example in Subsection 3.1 as the motivation of the proposed method, which is very interesting and helps the readability of this paper. In Subsection 3.3, some theoretical results are introduced and these results can help the readers to understand the proposed "regrouping  idea. The experiments on real world data use same  hyper parameter $p$ across all datasets. It seems that $\mathbb{1}_{\{h(x) 1\}}$ is equivalent to $h(x)$. This paper considers a particular PU learning setting where the irreducibility assumption is not satisfied, and gives a new method to address this problem. This paper is well motivated and novel, and it will be better with the weaknesses addressed.<|endoftext|>Minor comments: The spelling of "real word" in Note 2 on page 7 is incorrect. The authors concluded that the existing CPE methods are based on a critical assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. This paper studies a relatively little concerned problem, that is, the problem of class prior estimation(CPE) in PU scenes. It is also difficult to prove that a certain data set satisfies this assumption. This article focuses on practical issues, and its writing style is clear and easy to read. To remove the assumption to make CPE always valid, a strategy called Regrouping CPE (ReCPE), which builds an auxiliary probability distribution, are proposed so that the support of the positive data distribution is never contained in the support of the negative data distribution. Existing CPE methods will systematically overestimate the class prior when the data does not meet the critical assumption. This is a relatively practical research topic. 2.In general, this article is well written, which is reflected in the clear organizational structure and concise and easy to understand presentation. weaknesses:1.Why is case control PU learning more general than censoring PU learning? 2.Regarding the main research scenarios in this article, that is, "assuming that the support of the positive class conditional distribution is contained in the support of the negative class conditional distribution", the article gives a theoretical definition. 4.In the ReCPE algorithm, the author proposes to find the most unlikely negative data among the unlabeled samples. The way to achieve this step is to train a binary classifier with the unlabeled sample and positive sample by treating unlabeled sample as a negative sample. If the support of the positive class conditional distribution is strongly contained in the support of the negative class conditional distribution, The two class priors should be quite different.<|endoftext|>To address this problem, this paper proposes to construct a transformed the probability distributions of input data by a re grouping operation, such as the positive prior cannot be a component (or called as a support) of the negative prior. PU learning is apparently an important machine learning problem, which has the potential to be used in many real world applications. While the proposed probability distribution transformation makes sense to avoid relationship between the prior of positive data and that of negative data, the proposed method appear pretty simple. 2.A big gripe of this paper is its writing. This sentence can be definitely written in a better way. This is an incomplete sentence, which is a mistake. Page 3: "... by creating a new auxiliary distribution $P_{p }$ always guaranteeing the irreduciblility assumption..." Here using an attributive clause is much better than using a present participle. 3.As we all know, deep learning has become the main stream of machine learning in the past decade, which is particularly true in ICLR. Many research on PU learning using deep neural networks have been proposed and demonstrated good performance. But because of the following concerns, also mentioned above, I suggest the authors to further improve this paper before publication:1. Please proofread this manuscript carefully to improve the writing.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; It s clear how to use these aggregators with a central server yet, they are not well defined in the decentralized case. The authors provide theoretical analysis in addition to a fair amount of experiments. Thanks for submitting your work to ICLR2022. Usually, Byzantine attacks are posed as the strongest attack to diverge learning. In summary, (1) the authors make several assumptions that seem unrealistic and unjustified. (3) As the main aim of the paper is to provide a computationally efficient algorithm, the paper should give clearly the computation complexity of all parts of the algorithm. This again requires consensus, right? It s important that the authors clearly state this in their assumptions. What are the practical/theoretical results of such a situation? Byzantines should be able to do whatever they want [1,2]. What is the benefit of your algorithm in this case?<|endoftext|>This paper seems to propose a new use case of an existing Byzantine fault tolerant protocol called CenteredClip in a federated learning setting. The authors need to clarify the main challenge of the existing methods and the key achievement of the paper. It seems the use of multi party random number generation for Eq.(1) is the main novelty, but its context is not clear. Although the authors state that their approach fits the decentralized learning setting, the protocol relies on some sort of leader selection.<|endoftext|>In this sense, the paper is timely and relevant. Can any peer talk with any other peer? It needs a proof that this type of attacks are not possible. The paper considers a challenging problem of Byzantine tolerant learning in the decentralized setup (without a parameter server). It will be important to give more details.<|endoftext|>The utilization of  MPC is novel, and the provable convergence guarantee is a strength of the paper. While the lack of a centralized trusted node is expected to make the protocols more expensive, a key missing ingredient of the paper is an explicit accounting for this communication cost. For instance, the paper refer s to a butterfly all reduce procedure, which seems to suggest that each node receives the sum of all the numbers (gradients) that are transmitted in the step. This can be explained more clearly. Overall solid piece of work.<|endoftext|>This paper introduces BTARD SGD as a new algorithm for distributed training among untrusted Byzantine nodes. The results of this paper are important. Parameter choices are provided and the extensive supplementary material contain the necessary details for the proofs and complementary explanations of the method. ### Weaknesses[Significance]  The assumptions needed for the algorithm, prohibit an extension of the work to a setup with decentralized data (such as in federated learning). The strict IID assumptions for the paper are limiting. Missing experimental results for the case of nodes joining.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; In this work, the authors first analyze the behavior of injecting backdoors into a well trained clean model via fine tuning it on a poisoned dataset. They propose a simple yet effective logit anchoring method for better consistency. Weakness:(1) The authors state that they are the first to propose the concept of consistency, including global and instance wise consistency. It would be helpful for the paper to be contextualized in existing discussions of instance wise consistency and the introduction of existing methods. (2) The introduction and implementation details of existing methods are not well clarified. What is the key difference between B.1 and B.2? I suggest that the authors merge Appendix B.1 and B.2. (2) [1] propose to inject backdoors into clean models via AWP.<|endoftext|>The paper proposes a novel logit anchoring method to improve the consistency between clean models and backdoored models. (2) The authors propose a logit anchoring method. (3) Theoretical insight gained from the paper is novel and inspiring. Is it difficult to inject backdoors into BERT model on the IMDB dataset? C, figures and their corresponding texts are too far, which might be puzzling. The paper formulates the concept of consistency. Overall, this paper is well written.<|endoftext|>This paper focuses on injecting backdoors into a trained clean model. The authors propose five metrics to evaluate the instance wise consistency. Experiments show that the evaluation results of these indicators are consistent in most cases. Is it possible to use other metrics, such as Pearson correlation? The paper also models the concept of consistency in backdoor learning, and proposes a novel logit anchoring method for better instance wise consistency. In general, this paper is novel and solid, and I recommend a strong acceptance.<|endoftext|>This work proposes a logit anchoring loss term for fine tuning models to introduce backdoors while maintaining consistency on clean data. The authors point out that “the surge in the usage of the large scale neural networks makes it hard to train backdoored models from scratch, since it requires a large training dataset.”  However, works which propose to backdoor models trained from scratch are typically putting poisoned data into the victim’s dataset and are not training the model themselves. Also, all the examples in this paper are ones in which training from scratch, even for a poisoner, is feasible computationally, so this motivation is also a bit off in that sense too.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The experiments could be stronger with advanced architectures such as Transformer or self attention model. In particular, the authors implement a recall mechanism through residual connections to prevent the recurrent network from losing original input information after many iterations. Unsurprisingly, using these techniques will help improve the performance. The main manuscript misses some details of the method (see questions below). Overall, I like the idea of this paper, which proposes an interesting way to exhibit logical generalization by allowing recurrent networks to "think" more during inference. However, as mentioned above, compared to the original work, the proposed modifications are marginal. The progressive loss is more interesting, yet it is unclear from the writing that this is a novel contribution to the training of RNNs (e.g., compared to other auxiliary training for RNN as in Trinh et al., (2018), what is the difference and advantage of using the proposed method). But here, the result is not impressive as there is only a 4% improvement in terms of peak accuracy. To make the paper self contained, in Sec.3 or 4, you should describe the output format and the loss used to train the model (from Appendix A, it seems that the final output is a 2d bitmap?). The procedure of selecting the inference iteration for performance measurement should be mentioned in the main manuscript. Can your method generalize to 1024 bit? In the paper, the feed forward model is a weak baseline.<|endoftext|>The modifications are: 1) Add the initial problem features to every step of the recurrent computation, coined "recall" in the paper. The paper shows that the modified recurrent network learns an algorithm that converges to the correct result with more recurrent iterations, thus overcoming the problem of "overthinking" (divergence really) as coined in earlier papers. The paper is pretty well writtenWeakness   The paper is evaluated on a very new dataset, which seems ideal for the kind of method suggested, and the only real comparison is the work they directly build on. You can always find a dataset for which your method is the best, and this paper has a bit of that feeling. Why do we need these new benchmarks? Why would you want to use a neural net on them? If the benchmarks are not interesting problems in their own right, they might still be nice benchmarks because they nicely exemplify some specific problematic environments, but then it must be very clearly explained how this ties back into (better) solving real world problems. I would very much appreciate a comparison to a RRN network in all of the new benchmarks, or conversely evaluating on some of the same benchmarks as in [1]. I suspect the performance will be very similar for the two networks. Have the authors tried simply measuring a loss on every recurrent iteration like in [1]? This would also encourage a convergent algorithm, and would be simpler.<|endoftext|>Update after discussion period: the authors included a number of supplementary results and informative additional control experiments. The analyses on convergence and overthinking in the latter part of the paper are also very nice. I think there are a few issues that need to be sorted out, but am happy to raise my score if the authors can address these concerns:  The only baseline considered is a feedforward version of the primary model (i.e.in which parameters are not shared across iterations). This seems like a good comparison, but other baselines should be considered as well. How important is the particular recurrent architecture employed here, in which the output is fed back into the model as input? Given that, I wonder whether the proposed method is the simplest or best way to accomplish this. Would it work to simply train models on a randomly sampled number of iterations, or to penalize longer processing (as is done in  adaptive computation time )? Do the models still perform just as well at generalization to more complex problems if the final iteration is used? However, on the chess problems, the version of the model with recall, but without the  progressive loss , still suffers from overthinking. What might explain this discrepancy? If so, could confidence be used as a signal for autonomously selecting how many iterations to perform, rather than having to select this by hand? The legend for table 1 of the appendix says  perfrom  instead of  perform . At the end of the main section with results on the maze task, the reader is directed to section A.3 to see an example of a 201x201 maze, but this is actually in section A.7. The proposed approach is compelling, and the results are promising, but there are a few issues that needed to be sorted out, including some additional baselines, and clarification of the selection criteria for the reported results.<|endoftext|>The paper is well written and easy to read and understand. This is an interesting line of work that could lead to useful insights, and this paper s contributions certainly to make significant progress on the challenges considered. In particular, the authors have already conducted various analyses to answer follow up questions that a curious reader might have (and I had) about the working of the proposed techniques. The results of these experiments will be very useful and instructive to readers interested in these problems. I would like to point out two "weaknesses" of this paper:1. This is not to say that the contribution of this paper isn t useful, but it should be put in context of past work that has already established strong theoretical results directly related to the behavior of interest. 2.I m concerned that the authors are playing fast and loose with the term "thinking" in this line of work. Otherwise, we can say that any classifier or detector is also thinking. The paper proposes simple techniques that clearly addresses the limitations of recent work on solving tasks in the easy to hard benchmark dataset.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This work proposed a method to trade off sample diversity for sample quality in diffusion models, which is termed unconditional guidance. Different from the prior work called classifier guidance (Dhariwal & Nichol 2021) that relies on a classifier for providing the guidance signal, the proposed unconditional guidance mixes the score estimates of a conditional diffusion model and an unconditional diffusion model for a trade off between sample quality and sample diversity. Strengths:(1) The paper is well written and easy to read.<|endoftext|>* The paper introduced a clever trick to allow sharing weights between a conditioned model and an unconditioned model, by introducing an additional label representing the unconditional case. Using an explicit classifier, as pointed out by the authors, is faster. The presented model is clearly conditioned diffusion guidance because you still condition on the labels. If the capacity of the generative models outweighs that of a classifier model, it would be fair to say the latter would be dominated by the former. Apart from the replacement of the explicit classifier with an implicit classifier sharing weights with the generative model, there is a switch from discrete time training in Dhariwal Nichol (2021) to continuous time training here.<|endoftext|>This paper proposes an unconditional guidance, which discards the classifier in previous work.Under such unconditional guidance, it is also able to obtain a trade off between sample quality and diversity as the classifier guidance. Specifically, the proposed method need to simultaneously train an unconditional model and a conditional one, while the classifier guidance method need to simultaneously train a conditional model and a classifier. Therefore, I m confused with the usefulness and the advantages of the proposed unconditional guidance. Through the experiments, the authors verify that the unconditional method can also achieve the goal of balancing the sample quality and diversity.<|endoftext|>However, contrary to [2] the classifier considered by theauthors is implicit in the sense that it is purely defined by a conditional andan unconditional generative model. The experiments are easy to read and illustrate the announced trade off between FID and IS. The experiments are interesting and clearly illustrate the trade off between IS and FID. I think the paper would truly benefit an investigation of the shortcomings of the score based generative models using classifier guidance. Then, we could fully appreciate the benefits of using unconditional guidance diffusion models. However, this does not seem to be the case. I will increase my score if the authors addressmy concerns.
Reject; rating score: 3; rating score: 6; rating score: 6; The approach is general in the sense that it involves minimization of an f divergence, for any f. The approach is mainly compared to standard deep ensembles, i.e.without controlling diversity. Additionally, the paper provides a theoretical result with approximation guarantees, which might be useful outside this specific application. PROS1.The approach targets diversity, a relevant and interesting phenomena, and gives a technically non trivial algorithm for controlling it. However, there are many disturbing typos which could easily have been caught by reading through the paper. NeurIPS 2021I am certain that the proposed method can indeed be useful and of interest for many in the field, and the method appears to be largely novel. Moreover, Figure 3 is a nice representation of the OOD detection results. Can you map it to a concept in Bayesian DL? Regarding the baselines, I think it is important to compare the method to more recent advances in order to fully grasp its capabilities. Question to the authors: how come more baselines were not considered? I see that they help devise the method theoretically, but what are f, V, B and A in Definition 1? In [1] they correlate the ensemble components by letting them share weights. Your objective function also correlates the ensemble components. [2] since, as is pointed out in Sec 5, it is "closely connected". 3.How did you obtain Figure 1? It is not obvious to me that (a) and (b) would occur.<|endoftext|>Analysis are conducted in function space, where the intuition are from and algorithm are developed. A greedy maximization of f divergence is given and its implementation for approximating Bayesian posterior are presented. The experiments on synthetic data show the ability of covering the posterior of the proposed method, compared with the deep ensemble approach. It also conducts experiments on real world data for OOD detection tasks. In general, the paper provides a novel perspective for constructing the ensemble, connecting f divergence between a distribution and a kernel mixture with sub modular functions. Submodularity is useful in exploration, e.g., submodular set function maximization. The general idea is novel and intuitive, which could be helpful for the research in deep ensemble. Some findings and analysis are interesting, e.g., a better approximation factor compared with theoretical guaranteed factor could be achieved for f divergence. 2.How are the mode seeking and mean seeking ability of the proposed method justified theoretically or empirically? How does it affect the general performance and how is this quantified?<|endoftext|>The paper proposes a novel and principled method to make Deep Ensemble Bayesian. It minimizes an f divergence between the true posterior and a kernel density estimator fitting the functions associated with the ensemble candidates. The authors formulate the learning as a submodular problem and propose to use a greedy approach to solve it. Some out of distribution detection results in computer vision demonstrate the efficacy of the proposed method. # Strengths  The problem of making Deep Ensemble Bayesian is important for the community, and the proposed method seems to be a viable approach to solve this problem. To my knowledge, the idea of rephrasing the Bayesian inference problem as a submodular one is conceptually novel. The theoretical justification is impressive and interesting. Though your modeling is applicable to arbitrary f divergence, you have not demonstrated the implications of such universality. The difference is that you train these ensemble candidates sequentially with weight decay while function space POVI trains them in parallel without weight decay? Although I know the main contribution of this work is on a new theoretical framework for Bayesian Deep Ensemble, I still want to see that this works can beat Deep Ensemble significantly in more scenarios. This paper has done a great job in making Deep Ensemble Bayesian theoretically, despite being short of empirical verification.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; It is worth noting that in many of the plots it seems that running the other algorithms for a longer time could also result in better results than FedDrop (e.g.see Fig 15); Interestingly, even in this case Fig 16 shows a large gap in favor of FedDrop which is due to the use of low E as well! Their statements are misleading/sloppy or wrong and lack assumptions. Please refer to my original review for detailed issues regarding the empirical results in the paper. The steps on adjusting the drop out probabilities are computation/memory/communication intensive.<|endoftext|>This paper proposed a dropout based method towards communication efficient federated learning, where each client will compute the dropout probability based on the update similarity between local models. The experiments show that with fewer FLOPs than traditional federated learning algorithm, FedDrop can achieve the same accuracy. Pros:1.This similarity based dropout technique to reduce communication cost in FL is new, and the authors proposed novel algorithm to approach it. Cons:1.I think the writing and organization can be improved since I am not very clear about some technical details, which I will talk about below. I was wondering for $\boldsymbol{p}$, do we optimize a single $\boldsymbol{p}$ or for each client c, we compute a  $\boldsymbol{p}_c$? Overall, the idea of this paper is new, but I think this paper can be better organized to make reader easier to understand the technical details.<|endoftext|>The dropout probabilities are computed by solving an optimization to promote similarity across agent s update. The cost of solving this problem has not been well discussed in the paper. I am concerned about the efficiency of solving this subproblem while other baselines such as FedAvg/FedProx do not need to have this additional step. Weaknesses:  The paper lacks discussion on the convergence of the algorithm.<|endoftext|>While reducing client computational costs is an important task in FL, the method proposed in the paper comes at a very high communication costs, which makes it unlikely to be practically useful while communication costs are still the primary concern in FL. Would similar results be possible for other problems such as NLP? The authors show that doing this can reduce the computational cost on clients of FL by up to 3x. If this is correct, it would be much more valuable to the reader to record the actual communication costs of each method as well as their computational costs.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper presents a new under explored problem in offline RL: the situation when during the online deployment some features that were present in the offline dataset are missing. The authors motivate this problem by the challenges in real applications. I would like to hear more about the contributions of this work compared to prior works in similar settings in online RL and sim2real. Then, they propose an extension of an existing offline RL algorithm to distil the teacher offline policy which has access to all features to a student policy which has access to a limited feature set. Finally, the authors conduct a set of experiments on 3 Mujoco control tasks where they vary different parameters of the problem, for example, the quality of the datasets (and the way they were collected), the number of dropped dimensions in the resource constrained setting. The results show the benefits of the proposed method compared to the baseline. I appreciate the experiments and new dataset construction, but I would like to see a broader set of experiments on a larger variety of tasks to be able to make conclusions about the efficiency of the method. The approach is simple, does not have many hyperparameters and, thus, practical. While this paper talks about offline online RL, I think very similar issues arise in the online RL or in sim2real transfer when either more features are available during the training time than at the deployment time or when in simulation we have access to the privileged information that is leveraged for the deployment. I don’t think that the resource constraint issue is specific to the offline RL and approaches to this problem in the online and sim2real cases are not discussed. Then, it seems to me that the main contribution of the paper is rather empirical. However, the diversity of the environments is rather limited and I would like to see a broader set of experiments on varying tasks to appreciate the benefits of the method (maybe training policies from vision?maybe some discrete control? Simple and clear approach. Could the authors explain better why the gain with a small number of available features is smaller or even negative?<|endoftext|>The proposed algorithm is a minimal extension to a popular offline RL and the experiments/results are well motivated. However, at the current state I would lean towards rejection as the paper still lacks detailed investigation of possible approaches to solve this problem as well as results on a more compelling set of benchmarks(datasets). The proposed algorithm is a minimal extension of recently proposed algorithm TD3+BC to leverages the offline/priviliged information for online deployment. Furthermore they define the data collection strategy inline with this setup which was not present in previously explored / popular datasets such as D4RL. **S2:** They propose a minimal extension on TD3 + BC algorithm for the resource constrained settings and show that the proposed algorithm was able to close the performance gap due to the missing privileged information (I.e.the additional offline feature set). **Weaknesses:**  **W1:** The motivation behind the RC D4RL dataset is not well articulated. As articulated in the paper, the available dataset D4RL is collected by a policy that has access to the privileged information, which in turn results in the behavioral policy having high quality trajectories. Hence it is important to collect the dataset with a resource constrained policy (no access to privileged information) in order to better quantify the advantages of being able to leverage privileged information. **W2:** The paper seems to be missing some natural baselines. While the proposed algorithm is quite straightforward and is able to beat a blind agent (blind to privileged information) it would be interesting to see how it compares to other natural baselines such as a predictive model baseline. Also If I understand it correctly, the baseline would simply by equivalent to Transfer(1.0, 0). Then the question becomes did we just propose an algorithm and just compared the different hyperparameter settings? **Q:** If a very natural baseline is comparable to the proposed algorithm, is the main contribution of the paper limited to the definition of the resource constrained framework? **Clarity Issues/ Comments:**  **CL1:** While the paper concludes that the performance gap between the use of online features and offline features have been highlighted, it is not well quantified in the paper, how much does the overall performance suffer due to these missing offline features. It would be nice to have them in the main paper. **CL4:** Figure 6 does not fully answer the question of how "important" the teacher s role is in training the student, were we expecting Transfer(0.5,0.5) to perform worse that Transfer(0, 1)?<|endoftext|>This paper considers a novel offline RL setting with resource constrained online deployment. In that setting, the agent has access to less information about the state in the online deployment phase than the available information from the offline dataset. Experiments are carried on in three D4RL environments with a variety configurations in the data collection protocol, behaviour policy, feature constraints. Motivation & significance: this paper introduces an interesting and novel offline RL setting and considers a simple modification to an existing algorithm for improved performance. It lists two real applications as the motivating examples. The authors do carry out extensive experiments on the three environments from D4RL, but I am disappointed that no experiments are done for any real application or a simulated setting that resembles some property of either of the motivating examples. Novelty: the proposed solution is quite simple and straightforward, adding an additional regularisation to a teacher policy with a higher quality. Therefore, there is very limited contribution to the methodology. Clarity: the idea and experiments are very well presented in the paper. However, I’m concerned that all the experiments are restricted to three simple simulated environments in D4RL and the results may not provide much evidence to how they will generalise to other environments or any real problems. Also, a very simple baseline is missing that trains a pure behaviour cloning policy to learn the teacher policy. Overall, I think it would make this work much more impactful if the authors could consider a real problem / simulated problem similar to a real problem, and study different approaches and show what the best performance gain we could obtain against an existing solution. The new problem setting could be better motivated and supported by experiments. The proposed algorithm is a simple modification to the existing method.<|endoftext|>The paper proposes an offline RL algorithm in the resource constrained setting, where the offline dataset contains richer features than provided by online interactions. The authors propose a transfer learning objective, where a teacher policy is learned from the rich features, then a policy is learned from the limited features by additionally fitting to the actions chosen by the teacher policy. The authors compare the policy learned via this transfer objective to a baseline that does not do transfer learning on D4RL tasks. Major Comments:As far as I know, the authors consider a novel setting where the features that exist during online deployment are more limited than in the offline dataset. This is an interesting setting and likely relevant to many applications. The authors only consider adding their transfer learning objective on top of a specific offline RL algorithm, i.e.TD3 + BC.I feel that since the novelty of the paper is in the transfer learning objective, the authors should have considered adding their objective on top of other SOTA offline RL algorithms, i.e.CQL.Right now, I feel like since the new objective was only tested on top of one specific base algorithm, the generality of the approach is more limited. While this is important to show, I feel that the result is not too surprising, as you are giving a lot of additional information to the proposed algorithm. As mentioned in the previous paragraph, I also think the paper would be improved by considering other baseline algorithms than just TD3 + BC, as it can show that the transfer learning objective can improve different classes of offline RL algorithms. Overall, the paper makes a first step in a novel and relevant setting. And though the results in the paper are perhaps unsurprising due to how much weaker the baseline is, it is understandable because, to my knowledge, no other algorithms that could leverage the offline dataset with rich features.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The experiments need more seeds, but the advantage of the presented method appears significant. Their policy first generates the robot s morphology and then evaluates the design with a common behavior policy that conditions on it. The paper poses the problem of morphology design for robots as RL training of one joint GNN policy. Experiments in the Mujoco simulator demonstrate a large improvement over evolutionary methods in terms of sample efficiency and final performance (although the latter is less clear, as neither method has clearly converged). **POST REBUTTAL**While the response does not answer 100% of this reviewers questions, the resulting paper is nonetheless interesting enough, and the results clear enough, to merit publication. (2) The results are based on 3 random seeds. To make the presented claims about the ablations, the authors need at least (!) The authors are encouraged to look into Relational GCN as an alternative to represent asymmetric morphologies. Kurin et al.(2021) have shown some very convincing counter examples and suggest to use attention instead of GNN layers. Better select one $a$ (over the union of the action spaces). The paper introduces the (to the reviewers knowledge) novel concept of learning the morphology with RL instead of evolution.<|endoftext|>The paper proposes an algorithm for simultaneous agent design and policy optimization. Therefore, the policy is parameterized by graph neural networks (GNNs), and it outputs i) the skeleton structure, ii) node attributes such as bone length, size, motor strength, and iii) motor control commands. Strengths: clear idea, described well, convincing experimentsWeaknesses:1) Formulations of some claims could be made more neutral. The authors are encouraged to consult the recent literature on the analysis of PG methods, e.g., [1]. Adding a discussion explaining the effects of these numbers to the main body of the paper would be illuminating. The proposed approach is novel and can be impactful in other discrete continuous optimization domains. The paper is clear and easy to follow. I am satisfied with the response of the authors and I maintain my score "8: accept, good paper".<|endoftext|>This paper proposes a transform and control policy to optimize the robotic agents  designs. Contributions include:  A novel perspective on agent design: rather than formulating agent design as a bi level optimization, this paper embeds both design generation and control into a single decision making process such that both design and control are optimized by the same RL algorithm. Strengths:  Formulating design and control co optimization as one sequential decision making problem is novel. Post Rebuttal Update  The authors adequately addressed my main concerns through extra experiments and analysis. This paper provides an interesting perspective on efficient agent design with reasonable technical approaches, and the results look empirically good. The paper is written to be easy to understand. The empirical results look stronger than existing baselines. Also, three random seeds are far below the standard. Concerns:  I can t entirely agree with the argument that this approach enables first order optimization of agent design. So, this approach is still zeroth order optimization, and it s not appropriate to claim that the sample efficiency comes from the first order nature of the method. Instead, the search space for policy is much larger in your formulation, i.e., the dimension of MDP becomes much higher. By looking at the performance curves, it outperforms NGE by a large margin even without all JSMLPs. If your approach can also effectively improve upon that and is better than other baseline methods, the results will be more solid. "Data efficient co adaptation of morphology and behaviour with deep reinforcement learning."<|endoftext|>The paper introduced a reinforcement learning algorithm that simultaneously optimizes the design as well as the controller of a simulated robot to perform locomotion tasks. By integrating the design process into the policy learning framework, they are able to design novel and effective agents to complete a variety of tasks. To support the proposed algorithm, graph neural networks is heavily used to support different morphologies. They further propose a joint specific architecture to improve flexibility of the network, which improves the performance of the algorithm. Strengths:  The resulting morphologies of the algorithm seem interesting and effective. The idea of breaking the training into multiple stages where policy first designs the agent and then controls it is novel. Paper is in general well written. Weaknesses/Questions:  Not sure how much control a user has over the design space. The training for the first two stages would involve a delayed sparse reward signal. This will help compare the condition policy approach to a bi level optimization approach. Schaff et al.The paper presents a concrete algorithm with good results in general.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors propose a model for anomaly detection on multivariate, high dimensional time series data, where there are statistical dependencies across the different time series (referred to as "constituents" in the paper). I understand that there is noise in ground truth labeling in anomaly detection datasets; however, the proposed evaluation metric to address this source of noise is difficult to interpret and frankly seems to be an afterthought.<|endoftext|>This paper focuses on unsupervised anomaly detection in multivariate time series. The paper does not make unrealistic assumptions. Overall, I would be interested to hear more about the explainable side of this work   Bayesian networks are often useful for counterfactual reasoning which makes me optimistic that these methods are not only useful for detecting anomalies, but also potentially explainable. There are some places where I think the evaluation could be improved. This is a strong paper and I recommend it for acceptance. It addresses a real problem in a manner that is both novel and pragmatic.<|endoftext|>## StrengthThe problem of anomaly detection from multivariate time series is relevant. There are also concerns regarding the experimental protocol. ## WeaknessThis paper is overall not well written, and there are several concerns regarding presentation, quality and evaluation.<|endoftext|>The paper addresses the task of detecting anomalies in multiple time series, for the setting where multiple instances of a fixed time window are available for learning. The combination of the DAG and the normalizing flow is a very interesting approach, and the application to the multiple time series anomaly detection problem is a novel contribution. More details concerning the experimental methodology are required.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper is on generalizing UGS, (Chen et al., 2021) from transductive setting to inductive setting. The main idea is to learn the importance of each edge using the MLP over the learnt embedding from GNN, and pruning them through sorting the importance score. And weight pruning is done just by sorting the magnitude of weights in GCN. Speeding up the inference? 3) what is the time complexity/running time for the proposed method? And how about  the scalability of the proposed method. 4) What is the novelty of the proposed method of weight pruning? Also according to the comments, the scalability is indeed a problem due to the lottery tickets problem, which is the main idea of the paper.<|endoftext|>This paper proposes a method to perform pruning of graph neural networks. The novelty of the paper is that the method works for inductive graphs, and thus can generalize to unseen graphs. The idea in the algorithm is pretty simple   1) learn a mask for edges using the features of the nodes connected by the nodes, and 2) apply a gnn to the graph modified using the mask in step 1.<|endoftext|>The authors introduce ICPG, a novel method for pruning in graph and node classification tasks. In Section 4.4, the authors show that ICPG can indeed generalized to unseen graphs. This is a novel contribution. The authors introduce a novel way to prune graphs graph and node classification tasks which can generalize to unseen graphs. They evaluate the method on an extensive number of tasks.<|endoftext|>In this work, the authors propose an iterative co pruning framework using a lottery ticket learning for GNNs. UGS was proposed by Chen et al.to apply the lottery ticket learning to GNNs. In Inductive Co Pruning of GNNs (ICPG), edges of input graphs and model parameters are pruned according to the importance scores for each. It is very important to extend the concept of methodologies for the transductive setting to that of the inductive setting in GNNs. [1] Edge contraction pooling for graph neural networks, F DiehlI couldn t find an issue to reject this paper but it is difficult to evaluate novelty enough.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; I feel the major weakness of this paper is with the experimental sections. Overall, I would like to thank the authors for their response. Many of my concerns have been addressed, and I am happy to increase my score.<|endoftext|>Also, more exposition on the Brownian motion baseline would be helpful. I have raised my score. This is supported by ample experiments but I have serious concerns about some of the crucial experiments and baselines that I have detailed in my main review.<|endoftext|>Instead of Brownian motion, the authorsemploy a draw from Brownian bridge by designating initial and end states,called Time Control. More generally, it is more desirable that we can condition the generation atarbitrary time.<|endoftext|>The proposed method (Time Control). In order to enforce a goal to the generated text authors by fixing a start and end to this Brownian motion the process of text generation can be modeled as a Brownian bridge. I wonder if you have thoughts about this. During the training of the decoder how do you make sure that the decoder uses the information given by the latent plan? It is a good paper to accept overall.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper improves RFF based hashing methods, and verfies it empirically. I have the following concerns of the paper:* limited method novelty: the proposed method SignRFF (eq.(7)) simply replaces the stochastic rounding into a deterministic form. * The empirical observation "the results also reveals that LSH type methods are better than data dependent and deep hashing methods when the code length is moderately large" is not new as well, [1] has the exactly same, and comprehensive study. * The adopted hamming ranking retrieval method is not very efficient. It d be better to use hashing table(s) for evaluating retrieval performance.<|endoftext|>In this paper, the authors propose a simple strategy to extract random Fourier feature based binary codes. The authors also propose a new measure, ranking efficiency, to compare the search accuracy over two datapoints for locality sensitive hashing methods. Experiments are conducted on several image retrieval datasets to validate the effectiveness of the proposed SignRFF. It is a simplified version of SQ RFF by taking out the random perturbation. 3.The paper is well written and easy to follow. While the ranking efficiency measure is computed with theoretical analysis and some approximations, the comparison is conducted empirically. Thus, I am concerned about the author s claim of this contribution to be theoretically comparison between LSH methods. 2.While the experimental results in Sec 5.2 look good, the results in Sec 5.3 (Figure 6) suggest the proposed method falls behind the KLSH. The paper is overall well organized. There are some flaws as identified in the weaknesses.<|endoftext|>This paper proposed an improved version of Random Fourier Feature (RFF) based binary feature coding method as well as a new measure(ranking efficiency) to compare different Locality Sensitive Hashing (LSH) methods. The authors also validated by visualization the consistency between theoretical comparisons based the proposed measure and the empirical results. 1.The proposed ranking efficiency measure gives an objective measure of different Locality Sensitive Hashing method. 2.The proposed hashing method was able to surpass other method. It is impressive given the simple modification based on SQ RFF;The author does a great job explaining the idea, concepts, procedures and experiments.<|endoftext|>The paper revisits the binary hashing from Random Fourier Feature (RFF) for approximate nearest neighbor search. The authors propose a simple and effective RFF based hashing method SignRFF and demonstrate its locality sensitive property. Thanks. They also introduce a new measure called ranking efficiency to evaluate the performance of different LSH methods theoretically. Extensive experiments on three real life image data sets validate the effectiveness of SignRFF and the consistency of the ranking efficiency from theoretical evaluation. 2.The new proposed ranking efficiency looks interesting. 4.The paper is well organized and easy to follow. The new hashing scheme SignRFF and the ranking efficiency make me feel like two separate works but combine together. More details can be found from Points 2 & 3 below. Therefore, it is interesting to show the LSH efficiency of LSH, KLSH, SQ RFF, and Sign RFF with respective to $c$, $\rho$, and $\gamma$, and analyze whether the observed pattern from Figure 1 is consistent with that of the LSH efficiency. Can the authors also show this curve in Figure 5? I am glad to increase my rating if the authors can solve my questions.
Reject; rating score: 3; rating score: 5; rating score: 8; It first shows that, for such problems, there exists an optimal policy that is permutation invariant, and the value function can be characterized as a function of the local state of one agent and the empirical state distribution over the rest of agents. Some numerical results show better performance compared with some existing algorithms. This paper gives some theoretical analysis on the motivation of the mean field approximation and proposes algorithm MF PPO to solve mean field MDP, which is new in related fields. However, I have some concerns with respect to the motivation. This does not rely on mean field approximation or the actor critic framework. The benefit of using mean field approximation is not clear, and the authors should elaborate more on that. [A] Computing equilibria in multi player games. Both theoretical analyses and numerical experiments are provided. However, the theoretical justification of the mean field approximation is very unclear.<|endoftext|>The authors present a principled method of solving problems with multiple homogenous agents. I am shared about this paper. I raise some concrete questions, which hopefully will clarify some of my doubts during the rebuttal. Shouldn t it be $B(\theta_{k 1}, R_\theta)$ in the equation (3.1). 3.Why the mean field MDP uses $(s, \text{d}_\mathcal{S})$. 4.Is the proof a novel one? 5.What is the exact experimental setup:6. I am confused as it suggests using DDPG, which is somewhat detached from the theoretical analysis. It is not clear what the exact theoretical contributions are and how they are related to the experimental part.<|endoftext|>This paper proposes a mean field proximal policy optimization algorithm (MF PPO) for MARL with a large population of homogenuous agents. The sample complexity is derived based on a two layer neural network approximation and the proposed algorithm is tested in several detailed experiments. A reference that might be relevant to include: Mean Field Multi Agent Reinforcement Learning: A Decentralized Network Approach (Gu et al., 2021)(2) Permutation mapping $\kappa(\cdot)$: I understand what the authors mean by "permutation mapping" and it can be any function of the empirical distribution (e.g, first moment and second moment). I am confident that the main results are correct.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The topic is interesting, but the setting lacks a strong applicative example and the formalism of the paper should be improved before it is worthed for publication. Moreover, I think that the statements sometimes are hard to understand. Again, if the author could provide a practical example, it would help to motivate the setting described by the authors.<|endoftext|>To name a few examples:(1 A) It s stated in the abstract that "However, the transition probabilities calculated from the dataset can be much different from the transition probabilities induced by the learned policies of other agents..." This really depends on the information observability of the agents. (1 D) The authors mentioned multiple times on "decentralized multi agent environment" which is confusing to me. (2 C) It reads to me that the centralized training and decentralized execution scheme could still be applied to this setting. A lot of descriptions in the submission are very vague and the statements are mathematically imprecise.<|endoftext|>Originality: The problem the paper studies is important and interesting. 2.The evaluation of MABCQ is not sufficient, the dataset seems to be "replay" in RL domain. 2.The weakness of the proposed heuristic method is not discussed, is there any other method that can improve the coordination of agents? This paper studies a novel problem in MARL area. However, the proposed value deviation method is not well motivated, the experimental evaluation is only done in replay datasets, and other marl offline paper are not cited and discussed.<|endoftext|>The paper studies an offline MARL setting, with a focus on the decentralized case, and discovers that the difference between transitions in the dataset and other agents can be very large. This paper studies an interesting offline decentralized MARL setting. However, relevant works are not cited and discussed, and experimental evaluation can be improved. Overall, the paper is well written.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper proposes a framework to train policies for tasks that are specified via language text without the use of any expert trajectories or underlying state information to engineer reward functions. The authors leverage a state of the art visual grounding model (CLIP) to ground object nouns from the text into the current visual observation in order to derive a proxy for the rewards signal. They also train their language conditioned on several tasks to obtain a generic policy that can learn to solve an arbitrary task, given paired text descriptions and trajectory rollouts from the training tasks. This could get a little confusing, consider rewording to ensure that it’s clear that one of them is your approach and the others are baselines. + The goal text descriptions seem to be severely under specifying tasks. For instance, consider grid world navigation. What would a goal description that leads to a high correlation between grounding of objects in the current observation and progress towards the goal be for the task of going to location X in the grid? + The Introductory section of the paper makes the case against reward engineering by saying that it utilizes the underlying state information. How do the authors reconcile this? + It is not clear how their approach fundamentally differs from the reward structure used here (https://arxiv.org/pdf/1904.04404.pdf). Ignoring that, the general principle of deriving a reward based on localization/grounding of a target semantic concept holds true here as well.<|endoftext|>This paper demonstrates how to use pretrained CLIP model as a reward function for an RL agent. This approach enables flexible goal specification using language. The proposed reward generation method uses CLIP to identify the relevant objects and a separate module to compute the reward based on a specified spatial relationship. So, the reward function is not entirely zero shot as the heuristics encode the structure of the goal. The use of CLIP here is closer to an object detector. While this paper showed that the pretrained mask RCNN doesn’t provide good detection, this still doesn’t show change the fact that CLIP is used as an object detector. In this case, any color detector may work as well too. 3.CLIP base result in Figure 6. It doesn’t seem likely that RL agent that uses this reward function doesn’t learn anything from the beginning. For example, turns the reward specification into classification of objects of different spatial relationships. This would remove the need for separate spatial heuristics.<|endoftext|>The core contribution of the work is in specifying the reward 0 shot. Specifically, the method parses a language instruction into an object noun phrase and desired spatial configuration. Finally a set of heuristics are used to compute the reward for a particular spatial configuration given the target object(s) coordinates. Solving 0 shot reward specification, particularly from language is essential to getting RL to work in new environments. But this likely will not be trivial in the case where the instruction is in the form of natural language. In a more interesting visual scene with more than 2 objects its not clear that the Grad CAM would provide as good object localization. Moreover there are many tasks (e.g.opening a cabinet) where its not clear how to heuristically define a reward based on the Grad CAM localization, or what part of the cabinet the saliency map will map to. It does seem that most of the reward specification is actually coming in the form of hardcoded rules, and the main use of CLIP is in localizing objects. It would also be valuable to see if this approach can handle (a) more complex natural language instructions, and (b) more interesting environments with more complex visuals than the 2 solid colored blocks.<|endoftext|>These objects are identified by parsing the task sentence. The work claims that automatically parsing the goal text and using a heuristic to get a reward function works better than using the CLIP s language encode to goal text and get the rewards with a dot product. The work proposes a simple and novel solution to the zero shot reward specification problem which is an important step towards getting RL agents to work in new environments and tasks. The experiments show promising results and support the author’s claims. Weakness:  Experiments have been performed on only simple scenarios and environments. Is it constructed in a way that is easy for the parser to be parsed? The experiments in the paper are on toy examples and the approach proposed has several limitations for it to translate to other tasks, for example, the complexity of the environment and the task specification. But the techniques proposed are simple and can be extended to more complex cases with further research.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces local permutation equivariant graph network. The main motivation for introducing these graph neural networks is to improve in term of scalability with respect to global permutation equivariant models. The contribution of this paper is unclear to me. The paper is very hard to follow. The experiment about scalability in section 6.2 is not convincing at all as it deals with graphs with a maximum average number of nodes of 40. The contribution of the paper is unclear. Provide more examples and convincing experiments about scalability which is one of your main motivation.<|endoftext|>In this work, the authors propose a framework to build GNN s that operate on local node neighborhoods in a permutation equivariant way   and argue that since LPEGN operates on lower dimensional spaces in comparison to regular GNN s the proposed technique offers significant improvements  in terms of GPU memory usage. Initial Recommendation: RejectionReason: In my view, in current format, the weaknesses outweigh the strengths of the paper. Demonstrate scalability in terms of GPU memory usageWeakness:For me, the main weakness of this paper is in its exposition and clarity that raised several questions. how many graphs are there in the dataset? Bouritsas, Giorgos, et al."Improving graph neural network expressivity via subgraph isomorphism counting."<|endoftext|>The paper introduces the framework of locally permutation equivariant graph neural networks. The authors also provide a category theory point of view of their framework. **Motivation**   the motivation for the proposed framework is a bit unclear to me. 2.**Theory**   the paper claims that the framework maintains expressivity or even can achieve improved expressivity (Section 5), but it is not clear over what other models? And there are no theoretical results which show that, other then stating it several times in the paper. **Applicability to new graphs**   how does the network handle subgraphs of unseen sizes during training? I think this paper introduces an interesting framework but fails to justify and motivate its constructions.<|endoftext|>This paper introduces the local permutation equivariant network (LPEGN). Specifically, this work proposes to apply permutation equivariant update functions locally   i.e., operating in a local neighborhood. However, I have several concerns about the paper which I will detail below:  While I appreciate the paper’s providing sufficient background about graph neural networks to improve readiness,  I am not sure I clearly understand the relation of the provided background with the proposed method. Experiments for the dataset that has the well defined local symmetry   say mesh data. While the proposed method is practical, the novelty of methods is limited when compared with existing works that also employ locally permutation equivariant update functions. However, I still would like to hear more from the authors if I have any misunderstandings.
Reject; rating score: 3; rating score: 3; rating score: 6; In this paper, the authors consider quadratic network games with payoff functions of the form$$J_i(x_i;x_{ i})   \alpha_i x_i   x_i^2/2 + \sum\nolimits_{j 1}^{n} x_i x_j$$where $\alpha_i>0$ denotes the marginal benefit of the $i$ th player from playing $x_i \in \mathbb{R}$, and $g_{ij}$ is a matrix of interactions that determines whether players $i$ and $j$ are adversaries ($g_{ij}<0$), friends ($g_{ij}>0$), or non interacting ($g_{ij} 0$). I do not believe the paper is a good fit for ICLR for the following reasons:1. The authors  model is highly stylized and I was unable to see any connection to the type of applications and/or theoretical questions that are relevant to ICLR. Unfortunately, I am not convinced and I will retain my original score. 1.The authors make no attempt to explain whether the conditions provided in Theorems 1 4 are light or stringent, and they likewise provide no intuition as to what they mean for the underlying game.<|endoftext|>The authors give conditions for which $G$ can be recovered, depending on the sets $O$ and $M$, when a single trajectory or multiple trajectories of the game are available. Strengths:    Learning the structure of games of this type has attracted some attention recently and using fundamental concepts from control theory (observability and controllability of a linear system) is an interesting view of this problem. In addition, the authors mention that "In practice, it is expected that the best response dynamics (BR) should converge to a Nash equilibrium", however this is generally not the case (even if the NE of a game is unique)   the authors should provide a reference or prove that in this particular game BR converges to a NE. Application of concepts of control theory in learning the structure of a game with linear quadratic payoff may be interesting, especially in the case of partially observable actions, however the setting of this paper seems specific without being very well motivated, there are some technical errors and not well supported arguments and the paper reads somewhat incoherent.<|endoftext|>The authors study a game theory problem in linear quadratic games (family of payoffs) with the following assumptions1) players are assumed to use the best response strategy 2) an adversary can observe a subset of the nodes  actions3) an adversary can modify the wards for a subset of the players4) one part of the payoff function that is governed by a matrix G and models the non marginal payoffs i.e., the part of the payoff that is a function of the actions taken by the other agents. Why was the name controllable chosen? The authors spend no time giving intuition behind the theorems and since the proofs are not part of the paper, I cannot vouch for the correctness.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Although its an equation from another paper, it would help to have some definition of this variable. Eq 3 needs to be explained a bit better. Thus, the reviewer is inclined favorably towards this paper. + The results are detailed and well explained.<|endoftext|>2.Comprehensive experiments and analyses are conducted. The authors propose a deformed downsampling method in this paper and mainly compare it with the previous edge based downsampling, showing better performance.<|endoftext|>The authors present a method for ultra high resolution image segmentation. I have some questions about the paper. Effectiveness of components are verified in the experiments. The authors present extensive experiments and analysis to assess the components of the method.<|endoftext|>Overall, this paper has good motivation and proposes an interesting method. Such ideas are applied in "Learning to Resize Images for Computer Vision Tasks". The proposed method is end to end trainable, and can be plugged into different backbone networks. I read the appendix and find the downsampling module consists of 3 layers of CNN with 3x3 kernel.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper proposes two ideas for improving the performance of certified training. The first idea is to use assign weight for each input based on the margin to the decision boundary. They show that using these two ideas leads to improved certified robustness on MNIST and CIFAR 10 datasets. I think the weighting method is not presented that clearly in Section 3.1. In particular, Equation 9 is quite confusing. This seems quite small, but might be in the line with recent work in this field. Typos:difition  > definitionexamples with examples with  > examples withthe the ideal boundary  > the ideal boundarylinearly grows from 1 to 0.5  > linearly decreases from 1 to 0.5I recommend rejection because the method itself is incremental over prior work and experimental results are only marginally better.<|endoftext|>In International Conference on Learning Representations. The margin defined in (12) is negative does not necessarily mean the input is adversarial vulnerable, it only means the current input cannot be certified robust. However, I welcome the authors to clear my concerns in the rebuttal period. 3.[Experiments]    * The strength of the baselines are questionable and the results of the baselines are not consistent with the previous works. I agree that the training $\epsilon$ here is a bit different, but the authors should present the baseline results in their optimal hyper parameters settings. 4.[Writing and Presentation]    * Some claims in this paper is not correct. On the effectiveness of interval bound propagation for training verifiably robust models.<|endoftext|>This paper proposes to integrate training data reweighting and sample level perturbation budget tuning to improve relaxation based robust training like IBP and CROWN IBP. The overhead is small and the improvement of the certified accuracy is +1.38% on CROWN IBP and +2.17% on IBP on CIFAR 10. However, the current experimental evaluation is not comprehensive enough and lacks several important details, so it is a bit difficult to tell whether the approach is generally effective or not. Strengths:  The proposed two tactics are novel and solid. The theoretical and empirical illustrations of the proposed method are insightful. Given these concerns, I think the paper is marginally below the threshold. Demonstrated by experimental results, the proposed method is effective. This work is simple but has shown a similar level of improvement. Provide code for replicating the experiments, or provide a reproducibility statement. Please make them consistent. Algorithm 1 in Appendix C: How does Algorithm 1 combine with the whole training pipeline? They also provide verifiable robustness.<|endoftext|>This paper proposes bound based weighted loss and epsilon auto tuning to improve the performance of certifiable training. Improving certifiable training performance is an important but very challenging task. 2.The designs of the two methods are novel and reasonable. Comprehensive experiments following CROWN IBP would be necessary to fully demonstrate the performance of the methods. Also, 0.1 for MNIST and 2/255 for CIFAR10 are important baselines as well. The proposed methods are new and the improvements shown in the paper are not that marginal compared to most of the related recent papers. However, the paper lacks comprehensive experiments to convincingly support the claims. If more consistent experimental results can be provided during rebuttal, I would vote for acceptance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes a contrastive learning framework for time series forecasting CoST. The paper shows the ability of the proposed method to disentangle trends and seasonal features through synthetic datasets. CoST was compared against different representation learning and end to end learning appraoches. Strength:  The idea of  disentangling trend and seasonal features and using the learnt representations for forecasting is novel.<|endoftext|>The current training paradigm applied in most time series forecasting approaches jointly learns feature representations and the prediction function. The goal is to learn disentangled seasonal and trend representations. Moreover, time domain and frequency domain contrastive losses are incorporated to learn discriminative trends and seasonal representations, respectively. Strengths:The idea of this paper seems novel and reasonable. The writing of the paper can be improved, e.g., several places can be more concise. 3.It is still unclear why representation learning methods can outperform end to end methods in such a large gap. I will preserve my score before reading the authors  responses.<|endoftext|>This paper introduce a framework of learning disentangled trend and seasonal representations of time series and its application to forecasting tasks. It employs contrastive losses to distill discriminative trend and seasonal representations. Empirical results show the proposed framework is able to outperform end to end trained forecasters and other representation learning methods. The disentanglement of trends and seasonalities is well motivated. 2.The introduced contrastive losses is a good proxy to learn discriminative representations. The method of making forecasts from learned representations is not presented, making it incomplete for reproducing the results. 3.Minor: $d$ should be $m$ in the sec 2. Overall I think this paper is well written with clear motivation and technical path.<|endoftext|>This paper proposes a novel way to represent time series learnt with contrastive losses in both the time and frequency domain for the forecasting task. The approach is novel and timely and is clearly described. I enjoyed reading the paper. The method for representation learning is original and clearly described and there s not much for me to argue around. My main criticism comes from the experiments and the discussion/awareness of related work. It s rather surprising that an end to end forecast approach can be beaten (by a margin) by a two step approach. In the experiments, I m missing natural baselines (e.g., NBEATS or TFT or DeepAR) and datasets that are a bit more standard, like the M5 data sets for example.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies balancing average case and worst case performance in a multi task setting. Current empirical evaluation of the paper can be furthered improved. In addition, the authors observed that the proposed algorithm only needs to update the task interaction estimation periodically during training, which reduces the computational overhead. My main concerns are with the empirical evaluation of the proposed method, which are detailed below. What would the authors recommend for such use case? I wonder if this is because weighting approach in general does not work with overparametrized networks [1]? 5.Can the authors document the actual wall clock time and memory overhead of the proposed method on their CIFAR and Multilingual LM experiments?<|endoftext|>This paper propose one step look ahead DRO for multitask learning considering the worst task’s performance as the objective. The advantage of this algorithm only appears when the top n worst tasks are very close to each other. In this case, considering task interactions might help. If the gradient is not zero, we apply the gradient. There might be a slight difference between this very simple algorithm and the proposed one. I was wondering how do they compare. I think the general idea of this paper is interesting. However, there are still several important points that need to be clarified.<|endoftext|>In this work, the authors study the problem of multi task learning in the presence of heterogenous tasks when there is an imbalance in the distribution of tasks. b) You claim that the computational expense is much higher. If that is the case and that is the main source of gain, why not report the wall clock computation times to make the point clear? Minimizing average loss can compromise one task a lot and minimizing worst case loss can focus only on one very difficult task at the expense of improvements possible in other tasks. The authors propose a look ahead distributionally optimization based approach to balance the two   worst case performance and the average performance. If the authors can effectively address these concerns, I would be willing to change my score. The approach has several problems that are not clear. 2.**Two baseline approaches** The title of the paper is about balancing worst case accuracy and average accuracy. Hence, if for each task if you compute the loss as the average loss over the data by dividing it by the size of the data for that task, this would be a useful approach to compare against. The authors claim that its much more computationally expensive. I have two main points to make herea) Baseline DRO is still a DRO based approach.<|endoftext|>Experiments on synthetic datasets and two real world datasets demonstrate its effectiveness. The empirical improvement does not seem to be very significant, especially on large models. 2.Another concern is with all the approximations, it is no longer clear whether the merit of the proposed method is still held, i.e., can the proposed L DRO guarantee that $L_{DRO}(\theta_1) < L_{DRO}(\theta_0)$, empirically? 3.Can the authors visualize the weights in algorithm 2? Post RebuttalI thank the authors for the response.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper studies several extensions of EF21 including EF21 with stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient. The paper establishes the convergence results for the six variants and conducts experiments on several small datasets. However, it is not clear to me what challenge the paper addresses and novelty the paper presents. All the six variants including stochastic gradient, variance reduction, heavy momentum, partial participation, bidirectional compression, and proximal gradient have been extensively studied in the literature. Merely combining them with EF21 and providing theoretical analyses do not seems novel to me. The aforementioned question is from theoretical side. From empirical side, Figure 7 in the appendix shows that both EF SGD and EF SGD HB converge faster than EF21 SGD and EF21 SGD HB, respectively. Only the hybrid method EF21+ SGD HB obtains higher testing accuracy. This paper provides solid theoretical analysis for several EF21 variants. But the novelty is limited.<|endoftext|>The convergence analysis and experiments shows that the proposed algorithms work well in both theory and practice. All the proposed algorithms are extensions of EF21, the overall modification seems minor to me, which weakens the contribution of this paper. 3.Even for the largest problems: Resnet on cifar10, is usually considered the smallest one in the distributed scenarios. Such phenomena is also not explained in the paper. However, I have some concerns in the contribution and the experiment settings.<|endoftext|>The main results are the convergence rates under these settings. While the authors definitely did a lot of works, as this is a 70 page paper, the novelty of this work seems to largely depend on EF21. It is not clear from the main context what are the new analytical breakthrough introduced specifically in this work. More experiments are needed to support the theoretical findings. Communication rounds does not necessarily mean fast in runtime since there are also implementation overhead for (de)compression.<|endoftext|>The paper includes several variants of EF21 such as stochastic optimization, partial participation, variance reduction, momentum, etc. This paper studies extended versions of an existing algorithm called EF21, and show its convergence complexity when combined with other techniques, such as momentum, variance reduction, etc. The theoretical analysis is detailed and given for each variant. The analysis on partial participation and proximal settings are new for error feedback type algorithms. Some other minor concerns:  In the abstract (and also in the paper), it s claimed that EF21 BC is better than a previous result from (Tang et al., 2020). So it s not clear whether EF21 BC can really improve the rate if they are compared in the same setting. Please refer to the main review for the details.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; The authors aim to study the robustness of combinatorial optimization solvers (in particular ones that are based on learning). They treat the task as a graph problem and perform attacks on the underlying graph structure. Additionally, they aim to propose a defense mechanism. Thus, the novelty in this regard is limited as well. The paper aims to tackle an important problem. Thus, unfortunately, the conclusions drawn are very questionable. Moreover, notations, related work, and experiments needs improvements. It does not tell anything about the robustness of the solvers. In this case, this is not an issue of the non robustness of the solver but inherent to the problem setting.] Given this, all follow up experiments and insights are, unfortunately, not helpful. In the current formulation, though, this is not captured at all. 3: Experiments: As mentioned above, I am convinced that the current experiments are not helpful in investigating the robustness of solvers as claimed by the authors. Besides that, I found the attack to the TSP unclear.<|endoftext|>I will give the authors credit for introducing a novel problem to study, but they do not do a sufficient job motivating why the problem is an interesting or important one. Without this, the claim in the introduction that "it is imperative to develop defense mechanisms" are unjustified. * What does it mean to "loosen part of the constraint h_i"? This paper studies adversarial attacks and defenses against algorithms for combinatorial optimization problems. The authors frame the problem, introduce RL based algorithms for attack and defense, and present computational experiments.<|endoftext|>This paper studies the adversarial attacks and defenses problem of combinatorial optimization problems. Concerns:  The defined *robustness*, *adversarial attack* and *defense* in this paper are not reasonable. The paper considers the differences between $f(S(Q^\prime)|G^\prime)$ and $f(S(Q)|G)$ as the robustness of a solver, which makes little sense. After modification, the problem becomes a different problem, and the solution to it means little to the original problem. The scope of the claims in this paper are too broad and should be narrowed down to specific tasks and solvers that the proposed ROCO can work on. The proposed solutions have limited novelty. My main concern is about the problem setting.<|endoftext|>This paper presents a framework of adversarial attack and defense for solvers to combinatorial optimization problems. In particular, the attacker can modify the problem instance by removing graph edges, while the defense side seeks to add edges to mitigate the attacking effect. The paper presents attack methods based on reinforcement learning and heuristic methods, and proposes a defense method based on reinforcement learning. Similarly, the effect of the defense should be measured by the quality of the solution to the original input, which is however unknown to the defender because it has been perturbed by the attacker. I do not see that such a setting can result in a better attack. Relaxing the candidate space does not necessarily mean that the solver should perform better, because the performance is measured with respect to the optimal solution which can have a better objective value in the relaxed space. This paper considers an interesting problem, but the adopted settings need better justifications, and the proposed methods are unfortunately not novel.
Reject; rating score: 1; rating score: 5; rating score: 6; rating score: 6; Positive aspects: 	The observations are interesting, specially for the organizers and senior members of ICLR. The methodology and analysis is clear and easy to follow. The analyses are univariate and possibly misleading (see more below). No in depth study of the observations is provided and the results raise more questions than answers. Main Comments: 	The main problem with the study is the focus on univariate analysis. The analysis done in this paper is narrowed to ICLR data only and does not relate to representation learning and broader applications. I think authors should consider the affiliations, experience, and gender of all authors of a paper and draw conclusion on the mentorship. The conference in these two years was virtual and, quite possibly, an anomaly in terms of the behavioral patterns. Authors should consider previous years as well.<|endoftext|>The authors present several findings that are likely of interest to the community and provide some potential explanations behind some of such findings. The exploratory data analysis and regression results presented in the paper contain some interesting results. As the authors correctly note in the Ethics Statement, this is an observational study. All regression models control only for the reviewers  scores, thus in many instances it s unclear what kind of effect, if any, these models are actually capturing. This p value is above any of the significance levels that are generally considered (0.001, 0.01, 0.05, 0.1), so I would interpret this coefficient (and not "impact") as not being statistical significant. However, many of the findings overlap with those of (Tran et al., 2020), which conducted a similar analysis. In addition, I raised several concerns and questions on the methodology used in the paper.<|endoftext|>This is an extraordinarily wide mandate, and the paper attempts to cover a fair amount of ground   this also results in several intuitive, unanswered questions when reading. It is very difficult in general to make strong comments about the impact of various demographic factors on conferences and the review process, and while the authors propose several important insights and hypotheses to explain differences in e.g.return rate of women, it is difficult to establish how likely these hypotheses are (or to establish causality)  It would be interesting to see how the authors account for the self selection bias of the fields/organizations that submit to or have members who attend ICLR, and perhaps compare this to other conferences (including those that are more specialized to specific fields like ACL, as well as other more general conferences like NeurIPS).<|endoftext|>There are several very interesting findings in the research, which could have real world impact on the peer review process, and on the academy at large. It is a complicated question as to whether this paper is a good "fit" for ICLR. I would like to have seen a more sophisticated level of analysis which leverages techniques developed by the ICLR community, and from machine learning at large. This work only scratches the surface of the analyses on this dataset which are possible when leveraging the authors  gender annotations. Similarly, the paper could have delved deeper into the extent to which English language capabilities are the reason for the disparities, e.g.by reporting results when controlling for the percentage of English language speakers per country. This paper does not fit the typical mold of an ICLR paper, in that it is mainly empirical work rather than presenting novel methods.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; Moreover it is shown that the truncated variant of the approach is needed to achieve a good generalization performance. While there has been some related work investigating the benefits of batch normalization on neural network training, the present work provides a theoretical characterization for training deep networks with batch normalization. The analysis is then extended for vector output networks and different architectures including L layer neural networks and CNNs.<|endoftext|>The author(s) presented a quite comprehensive theoretical study of batch normalization from the lens of convex duality. Assume that SGD with BN is converging to $w^*_{BN}$ and SGD without BN is converging to $w^*_{noBN}$.<|endoftext|>The convex reduction sparks an implicit regularization of batch normalization. But, it is easy to design input for which $p_2^* >0$. But this is not helpful for convexifying the objective. The current version is a bit difficult to read and check.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; Weaknesses:The major theoretical contribution is the proof of the convergence of PPO. This implies that the result of this paper can contribute to a very limited range of tasks. The key of PPO is the clipping technique. The paper does a good job in sharing new insights of policy optimization, and in connecting the policy optimization with classification. Reinterpreting PPO by hinge loss is also not the idea of this paper. At the same time, the improvement analysis is based on some very strong assumptions.<|endoftext|>Pros:   the paper gives interesting theoretical insight linking PPO clip with a large margin classification and theoretical analysis of a new group of reinforcement learning as classification methods. Cons:(1) while the reviewer appreciates the paper is mostly theoretical, it might be a good idea to bring some experimental analysis into the main text to show the performance of the method on a range of benchmark problems, especially given that the experimental analysis is already presented in the appendix. While the proofs are important part of the work, are they more essential for the narrative than the experiments? (2)restrictive assumptions of the proposed method: “Assumption 4. Other comments:(1) Page 5: Entropic mirror descent: could the authors give some references? Starting from 5, but happy to update the scores to acceptance if there s a convincing answer on the cons during the rebuttal.<|endoftext|>When discussing weaknesses, please provide concrete, actionable feedback on the paper. **Originality:** To the best of my knowledge, this is the first formalization which allows a theoretical analysis of PPO clip, albeit, as highlighted in the paper, not the first formalization of reinforcement learning as a classification problem. **Significance:** Since PPO clip is one of the most commonly used policy gradient algorithms, a theoretical analysis of the approach is a relevant contribution to the reinforcement learning community. **Strengths**  The paper provides a nice bridge between the existing theoretical tools for the analysis of policy gradient algorithms and the popular PPO clip algorithm. Or would an alternative, perhaps simpler, procedure yield convergence as well albeit with a different proof scheme? The fact that the experimental results are relegated in Appendix is quite disappointing.<|endoftext|>I think this is a very interesting paper that improves a commonly used algorithm and provides both theoretical and empirical evidence that support using this modified algorithm. Besides minor corrections and clarifications below, the only "weakness" is that PPO is a heavily used algorithm and the empirical results here are not convincing enough (only a part of the mini atar benchmark and a really small toy problem). However, in my opinion, the theoretical arguments provide enough value for the RL community so even though a more through survey would be valuable for practitioners, the current contents of this work are already enough for a good paper. 4.Eq 13: what happens if all the advantages for a given state are 0? assumptions 3 and 4? assumption 3 alone seems out of context. It might be helpful to hint on its role in the text as well. I found this paper to be valuable to the deep RL community as it presents an interesting generalization to a commonly used algorithm with theoretical justifications.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 10; Overall, the paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi order interaction proposed in Zhang et al.(2020) to understand the complexity of interactions in DNNs and the proposal of different loss functions to encode/penalize interactions of specific complexities.<|endoftext|>The paper looks at representation bottleneck of deep neural networks, from the multi order interaction (within subsets of pixels) point of view. The research topic of this paper is significant in understanding the representation power of deep neural networks, and is interesting to the entire community of deep learning. Are there scenarios where using the proposed loss functions outperforms standard training in any of the metric (accuracy, robustness)? The writing of this paper is clear enough, and the main contents are easy to follow. The loss functions to encourage or penalize interactions of certain orders are also novel and very natural. What about doing the opposite, i.e.penalizing high order and boosting low order interactions?<|endoftext|>High order DNN seems to have been fully explained and experimented, but not on middle order and low order interaction. The idea of describing the representation of the DNN using multi order interaction utility is good and easy to understand. Weaknesses  The authors propose two losses to encourage/penalize DNNs to learn interactions of specific orders and show the results in Figure 4. This paper introduces and proves the representation bottleneck, which is a common  phenomenon in the DNNs.<|endoftext|>This paper studies the representation ability of DNNs from the perspective of interactions (the multi order interaction). The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low order and high order interaction patterns are easy to be learned, while middle order interaction patterns are difficult to be learned. To relieve the bottleneck problem, the authors propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle order interactions.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposes an MCMC algorithm containing two key ideas: the approximation of the target density with a simpler function (proposed by [Deng, 2020]) and the parallel simulation of many chains that allow for better capturing the global properties of the target density. To be more precise, the main contribution of the paper is the extension of the algorithm by [Deng, 2020] to the case of multiple chains. Another major issue is that the authors do not compare their algorithm against CSGLD (algorithm by [Deng, 2020]) in one of the main experiments: Bayesian networks on CIFAR 100 and SVHN. My main concern is the clarity of the paper.<|endoftext|>The strength of the paper is that it proposes a new algorithm interacting contour stochastic gradient Langevin dynamics that can be shown theoretically more efficient than contour stochastic gradient Langevin dynamics. I am a bit confused with the notation here. Even though, a new algorithm is proposed, it should be considered as an extension to the previous work Deng et al.(2020), and some similar theoretical estimates can be found in Deng et al.(2020) and the technical novelty is not clear.<|endoftext|>This paper samples from distributions with complex energy landscapes by the powerful contour stochastic gradient Langevin dynamics (CSGLD) methods. In order to reduce the variance of random field function, this paper proposes a simple but effective method, that combines  random field functions from multiple chains. This paper show that compared to CSGLD which only has one chain, the variance ICSGLD with P chains and same computation budget is asymptotically more efficient. The authors evaluate ICSGLD with multiple experiments. The theoretical analysis seem to be solid too. + Could the authors compare ICSGLD with different number of chains to further justify the theoretically result? + What are the main factors affecting the target $\theta_\ast$? How these parameters are selected in different experiments? I would like to hear authors  response on how theoretical implications are connected to practical concerns.<|endoftext|>This paper addresses the problem of sampling from distributions with complex energy landscapes. The authors propose an extension of the contour Stochastic Gradient Langevin dynamics (CSGLD) sampler to efficiently simulate from big data distributions. The extension is called “interacting” since $P$ different CSGLD are updated simultaneously and the random field function is obtained as a Monte Carlo average over the random field functions of these multiple chains run in parallel. The authors prove that the proposed algorithm is asymptotically more efficient than its single chain counterpart with the same computational budget. I liked reading the paper. However, I have some minor questions that require a clarification from the authors. (i) I do not understand how the parameter $\zeta$ is tuned in practice and I would like to have more details on this procedure in addition to the comments provided in appendix B.1.3.
Reject; rating score: 1; rating score: 3; rating score: 5; The authors aimed to improve EigenGAN for conditional image generation. The whole idea is not real novel. However, in the paper, there are no obvious evidence for the superiority of such design choice. I have multiple major concerns on this paper. This indicates that the whole training pipeline is quite fragile. Based on my main review comments, I suggest "reject".<|endoftext|>This paper proposes a GAN for conditional generation. The technical novelty of the paper is limited. If space is not enough, t SNE can be resized or moved to the appendix. 4.The authors mentioned in the introduction that inner attributes are useful for unbalanced datasets. I think there is a contradiction between the authors  argument and the experimental result.<|endoftext|>This paper proposes a stochastic contrastive conditional GAN, which consists of an encoder, an attributes’ classifier and a stochastic EigenGAN generator. Details of the proposed method and experiments are important for readers to follow the main concepts. It would have been better to provide more details in the paper. The authors interpret their proposed method from an information theoretic perspective, which makes a better theoretical basis for the paper.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 5; As such, both the methodology and the experiments are well described. The special kind of embedding and its property of aiming at being performance preserving in the context of AutoML is the main contribution of the paper, in my opinion. # Update after RebuttalAll my concerns have been fixed or ruled out by the authors in the revised version. This is extremely important as NDCG values will naturally vary drastically depending on how the relevance score is defined. to existing literature on a broad level, it lacks a good contextualization on a more nuanced level.<|endoftext|>This paper focuses on the AutoML problem for tabular data and proposes a meta learning based novel solution. This method is termed Metabu. Weaknesses:  While most of the paper was quite easy to follow, the short paragraphs on the "intrinsic dimension of the space of datasets" and the corresponding empirical evaluation and discussion (in section 4.4) is a little hard for me to follow. The empirical evaluation compares Metabu to existing meta learning schemes on (i) their ability to capture the desired Wasserstein Gromov distance, (ii) their ability to find better hyperparameters via sampling without an underlying optimizer, and (iii) their ability to find better seed hyperparameters for hyperparameter optimizers.<|endoftext|>The paper "Learning meta features for AutoML" proposes an approach to learn a linear combination of data set meta features. # Update after RebuttalDue to the strong responses by the authors as well as the updated version of the paper I recommend accepting the paper. What is "the AutoML selection problem"? There is no explanation on that. # Update after RebutttalAfter reading the other reviews, the rebuttal, and the updated version of the paper, my concerns regarding the paper vanished.<|endoftext|>This paper tackles the AutoML problem. Experiments on OpenML benchmark demonstrate the power of the proposed method on boosting AutoML systems. The proposed method has some novelty and will contribute to the AutoML community. It is not well defined in the paper.<|endoftext|>Strength.AutoML is a hot topic, and the idea of using optimal transport is interesting. This paper proposes a method to extract meta features based on optimal transport to improve the performance of AutoML, which is a hot topic in the field of machine learning. The usefulness of the method is shown through benchmark data, but the theoretical argumentation is not sufficient.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The paper proposes to manually design a new 12 depth CNN architecture ParNet based on parallel subnetworks instead of traditionally deeply stacked blocks. Experiments show that ParNet is the first CNN achieving over 80% accuracy on ImageNet with 12 depth only. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy. 3) The paper proposes a new block RepVGG SSE by introducing SSE into traditional RepVGG, while ParNet s accuracy improvement from SSE is only 1.53%, as shown in Table 7. The authors argues contribution of parallel subnetworks, while the experiments show that the traditional RepVGG wise blocks adopted in ParNet contributes significantly more to prediction accuracy.<|endoftext|>The paper argues that deep networks have several limitations    (a) deep nets have a higher latency;    (b) deep nets are hard to parallelize;    (c) deep nets are not suitable for applications. Motivated by these observations, the paper aims to fill the performance gap between shallow networks and deep networks. The paper proposed a 12 layer shallow model framework, which contains proposed RepVGG SSE blocks, fusion modules for multi scale processing, and parallel streams. I have four considerations:  (a) Table 2 only shows the details for ParNet L and  XL. (c) ParNet develops the RepVGG SSE based on the work of RepVGG, CVPR 2021. But there is no comparison with this high related work. It could be better to compare with RepVGG in Table 1 and Table 2. This fact fades the significance of ParNet. It could be better to discuss this and highlight why parallelism is more important than the other two aspects for selling the work. Please erase the above concerns.<|endoftext|>The authors try to support the claim which is that low depth networks can achieve a good result by providing experiments on the ImageNet and the CIFAR datasetsPros)+ This paper is easy to follow. Cons)  I am agreeing that the network depth is a crucial design element towards model efficiency but the computational costs in including the number of parameters, FLOPs, and memory footprints seem to be overlooked in this paper: the proposed model has a larger # of parameters and FLOPs, so the overall computational costs are bigger than the baseline models. The architectural design with the proposed building block in Figure 2 is presented without providing any intuitions or design philosophy. Please clarify a multi branch architecture also has an advantage over the widened networks such as WideResNet in this case. The performance comparison should be done fairly. For example in Table 2, ParNet L is compared with much smaller networks such as R34 and R50 in terms of the computational budgets, so this is not fair. Why vanilla SE block is not suitable for the proposed model? Please specify why ParNets cannot outperform DenseNet 100 even using more parameters on the CIFAR datasets in Table 6. I am just wondering whether there is a different earning behavior of a shallow network on a particular dataset.<|endoftext|>The paper proposes a new architecture of a convolutional neural network for image classification. The authors are motivated by inference efficiency and showing that networks with about a dozen of layers can be competitive in classification accuracy with 50 layers and more. So instead of growing depth, they propose to grow width and have multiple subnetworks of the same depth within one model. Weaknesses:  architecture complexity: the authors claim outperforming ResNet in efficiency, but do not mention that the architecture is far more complex. This needs to be stressed in the introduction. weak baseline: authors use Squeeze and Excitation blocks in their architecture which is known to improve ResNet performance, yet compare to ResNet without SE layers (section 4, tables 1 2)  missing baseline: the authors introduce parallel streams in their network, which might be working as an ensemble, known to be improving classification performance on the tested datasets. A baseline with ResNet and ParNet ensembles needs to be added. However, SiLU was introduced to improve training of deep networks, which is not the case in the proposed architecture. Moreover, in the ablation study (table 7) it does not seem to bring a significant improvement over ReLU, there is also no statistical significance analysis of this result. Using SiLU seems like a needless complication of the proposed architecture. Arguable weakness:  definition of depth: the approach takes advantage of the vague definition of depth in neural networks. The paper shows a significant result that a network with 12 layers can be competitive with deeper networks on well established image classification benchmarks. However, the architecture is more complex than ResNet or RepVGG, and the empirical evaluation is unsatisfactory due to weak and missing baselines, and the lack of statistical significance analysis.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper aims to improve generalization and data efficiency in offline RL using a latent space data augmentation approach inspired by results from Koopman operator theory. The resulting method is evaluated on several environments against a similar method which uses a form of self supervision via data augmentation. ### Notes   I didn’t understand the claim “an exploration of the environment’s phase space” (section 1, page 2)  In environments that are not equivariant to any symmetry group, how would you expect the method to behave? Empirically, the method does seem to outperform the baselines the authors chose. Overall, my concerns over the clarity of the paper, the gap between theory and experimental results, and the limited comparison to existing methods lead me to recommend that the paper be rejected. These were separate from my concerns about clarity. Further, the type/definition of U(a) doesn’t make sense to me: what is the value of (U(a)(g))(s’, a’) ? should be formally defined. It’s not clear how the learned encoder/decoder/observables used in practice satisfy the assumptions laid out in the theoretical section. **The empirical results are not convincing. S4RL and CQL are both model free methods, so this seems to suggest to me that it might not be the learned latent space symmetries but rather the model learning auxiliary task that is making KFC outperform the selected baselines. I couldn’t find a description of KFC++ anywhere in the paper.<|endoftext|>I have a few questions that I believe the authors should address in their revised version to help the reader understand the method and its effectiveness. 2.The square is missing in the Bellman error in (1), (4), and (18). But since we want hat{Q}_i and Q_i to be consistent across the entire state space in order to compute the correct fixed point. This is essential in order to understand the method further because the symmetry in this computation can also be trivial because (I suspect) changing the origin of the state space is a symmetry for all environments in Table 1. 6.This paper should study the proposed method better and conduct ablation studies. For instance, the cummulative reward of KFC should degrade as more samples are drawn from the Koopman VAE. This paper proposes an interesting idea based on Koopman theory that augments the limited dataset in offline RL but it is not clear from the current manuscript whether the performance of the method comes from the proposed innovations. The authors should present results of ablation studies that convince the reader that Koopman operator is essential and a similar performance boost cannot be achieved by other augmentation methods (I have suggested a few above).<|endoftext|>The paper uses koopman theory to design principled data augmentation method for offline reinforcement learning. The koopman operator $K$ is then used to generate symmetries which are applied to both $s_t$ and $s_{t+1}$ as data augmentation during bellman error minimization. The resulting algorithm KFC and KFC++ leads to overall better data augmentation and improves for S4RL and CQL. The paper provides a principled way to generate data augmentation for offline RL (and for dynamical systems in general)2. We aim to address these limitations by learning a Koopman latent representation which allows us to infer symmetries of the system’s underlying dynamic."<|endoftext|>The paper proposes a symmetry based data augmentation technique derived from a Koopman latent space representation based on some theoretical results on symmetries of dynamical control systems and symmetry shifts of data. The authors empirically evaluated their method on several benchmark offline reinforcement learning tasks D4RL, Metaworld and Robosuite and showed that their framework consistently improves the state of the art of Q learning algorithms. The general idea and the results of this work are interesting and seem to be novel. However, I found the paper to be rather hard to understand. Also, some points and notations should be better explained. Also, in this theorem it is not clear under which conditions we can obtain eq.(12).So, more clarification in this regard is needed (this is also needed for eq.(14) in Theorem 3.6). In the proof of Theorem 3.6, line 4, what is "a"? Is it "a_t"? This paper needs to be written in a more precise way.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; I am lean to accept this paper. Overall, the idea of this paper is very clear, and I kinda like the discussion part of it. However, as I am not an expert in this field but a general reader.<|endoftext|>1.This paper is an exciting work, and it would have a solid impact on the structured based model pruning. Model pruning can help with that. 4.The evaluation metrics used in this work should also include speed, but it is missing in the current submission.<|endoftext|>A rigorous and principled approach is taken by this work to address the second order pruning issue.<|endoftext|>3.It seems that the merit of capturing global correlation and good scalability is not achieved by a single approximation method, but by the SOSP I and SOSP H respectively. The proposed approximation schemes for second order structured pruning is interesting with theoretical derivations and rigorous experimental test. Modern large scale networks usually have even more parameters.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This is very unusual, as normally we would be interested in discovering causality the true underlying process that generates the true data. More specifically, they propose a framework called CAGE to estimate generative average treatment effects (GATEs) for probing cause effect relationships in the so called latent space of deep generative models. They use the average treatment effects (ATEs) to infer causal directions for deep generative models  attributes and use CAGE to generate counterfactual data. It remains unclear what is the benefit of probing causality in a pre trained model which is not designed to capture causality. I think this paper is not ready to publish. 1.My first concern is that the authors try to probe causality in a trained model, instead from the data. The trained model may not represent the data well nor capture the causality in the true underlying process that generates the data. I would like to thank the authors for the interesting work they proposed, and tried to explain my concerns below. It is unclear why probing causality in such a trained model is useful at all. 4.I am curious about the definition on the equation in ‘step 3, page4’. What is more, by using this definition, is the true potential outcomes equal to the ‘potential outcomes’ obtained by the equation? This is a very strange equation, it would be better if the authors can show that the potential outcomes obtained by the equation is consistent with the true outcomes in theory. I don t see the noise variables were recovered in the manuscript. What the authors did in the paper is intervention, but claimed as counterfactuals.<|endoftext|>The goal of this paper is to inspect causal relationships learned by a generative model. The counterfactual for a given cause variable and datapoint is defined by changing the location of the data point in the latent space in the direction perpendicular to the linear classification boundary for the cause variable. CONS/QUESTIONS:My main concern is that this paper appears to be more about the statistical dependences of two variables in the latent space   and while some of the results seem promising   it is not clear to what extent these findings have to do with causality, and to what extent they are just conditional probabilities. This would allow the reader to see whether the method can truly distinguish causality from association. The results regarding pairs of variables are too limited to convince a reader about the ability of the method to infer causal relationships in general (and not just statistical). The paper presents interesting ideas to investigate the relationships between variables learned by a deep generative model, but to make the claim that these relationships are causal, more theoretical justification, intuition, and experiments would be needed. POST REBUTTAL UPDATE:I thank the authors for their attempt to improve the paper.<|endoftext|>This paper proposes a novel framework CAGE (causal probing of deep generative models) for inferring the causal effect relationship in deep generative models. Results show the ability to infer causal relationships of the proposed CAGE. Pros: The paper is well structured. The authors established a new method for both counterfactual manipulation and treatment evaluation. Cons: Since the hyperplane that encodes the classification boundary is obtained by linear classifier, it means that the learned attributes should be linearly separable in the latent space. How to guarantee or to proof this, like some theoretical analysis or qualitative feature visualization? 2)  How to obtain the normal vector to the hyperplane given the high dimensional latent encoding z?<|endoftext|>This manuscript proposes CAGE, a method for determining causal relationships between attributes based on the learned representations of deep causal models. The method is straightforward to apply and estimate the Generative Average Treatment Effect (GATE). A strength of this paper is that it clearly presents the methodological ideas and formulations. If you have a background in causal inference, the approach is relatively straightforward to apply in the latent space, as the methodology is well laid out in Section 3. 4.1 and 4.2 are relatively straightforward to evaluate, but on a small and simplistic scale. It is very difficult to evaluate whether the results from this model are good or not on the real data (4.3), which is a common problem in the evaluation of causal models. In particular, the different results between the different models are very challenging to evaluate, and they need to be discussed at length. They are all analyzing the same data, and the "prior" relationship is not necessarily true for the generative model (as is noted by the authors). The authors should discuss at greater length their rationale for the proposed causal relationship, and whether that appears true in the system. Additionally, I would like greater discussion of the visual samples, which don t always match what I would expect from the system.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; Maybe a missing interesting comparison would have been BYOL with a VIT model. It should be accepted for publication at ICLR. The paper is clearly written, I believe I could reimplement the method from the paper (using the appendix).<|endoftext|>The paper proposes the iBOT method. Indeed, the number of parameters and the number of epochs used is not very useful. It would be interesting to try to make a more accurate comparison.<|endoftext|>Extensive experimental results and visualization are given to show the effectiveness of the proposed mask image modeling objective and iBoT model. The authors also ablate different tokenization methods and different loss components to validate the contribution. Overall, I think this is a practical method with good experimental results. However, the proposed mask image modeling loss may not be clearly verified. As its current state, I would like to rate this paper as marginally below the acceptance threshold.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper is well organized and clearly written. It is interesting to introduce the Uniform LGI as an extension to the PL condition. The Rademacher complexity scales with the diameter of the parameter set, thus can be bounded by the optimization path length, which connects with the optimization results. This paper is well organized following the approach from optimization to generalization.<|endoftext|>The paper provides a novel generalization bound for the gradient flow equation related to the length of the optimization path. The paper is well organized and easy to follow. I am willing to raise my score if my concern is addressed in the rebuttal.<|endoftext|>The convergence guarantee is shown for a Uniform LGI loss function in Theorem 1. The paper shows several interesting generalization results for Uniform LGI loss functions. While the paper proves several insightful generalization bounds, it seems Theorems 2 5 do not explain the connection between the optimization length and generalization error.<|endoftext|>The authors study the connection between optimization and generalization for gradient flow (GF) on loss functions that satisfy a global version of the Lojasiewicz gradient inequality. The analysis is sound and presented in a clear way. This leads to the main result that shorter optimization paths induce smaller generalization gap. The way the figure is presented suggests that the length of the optimization path trivially depends on initialization, and similarly the generalization gap.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Overall, I think the required assumptions are strong and the theoretical results are not significant. The second challenge is the convergence instability caused by the stochastic training. They further conduct numerical experiments on large dataset to show the effectiveness of SLIM QN. The paper is easy to follow and addresses an important problem. This point needs to be highlighted. 2  Assumption 3 is quite uncommon and strong for stochastic optimization analysis. 3  The theoretical results, which are under some strong assumptions, do not showcase any improvement with respect to SGD or other quasi Newton methods. Intuitively, this is a valid point as they use the idea of momentum in the update of their QN method. 5  The momentum and damping techniques are used to stabilize the noise created in the stochastic setting and they don t lead to reduction of computational cost or memory. The computational complexity is the use of L BFGS method from the previous work as stated in Algorithm 1. The authors should highlight that the use of L BFGS idea allows them to reduce the computational cost, and the momentum and damping techniques have nothing to do with the computational cost reduction.<|endoftext|>SLIM QN’s practical usefulness is still questionable. The authors state that “it is easy to conduct a grid search on a small models and datasets, explore how these parameters affect optimization, then apply them to large scale model training”. But what if we train a model with a large batch size? Therefore, to motivate small batch training, it is necessary to present that L BFGS has problems in large batch training such as generalization performance degradation (as in large batch SGD [2]). Parts of the discussion of the pros and cons of K FAC and SLIM QN (L BFGS) are not appropriate. As $M 10 20$ is typically used, this is problematic especially when the model size is huge (e.g., 175B parameters of GPT 3 model [3]). This work provides interesting empirical results that a second order optimization method (other than K FAC) can achieve fast training in the ImageNet scale. SLIM QN (L BFGS) can be more efficient than K FAC if the target model is small enough to allow $M$ to be sufficiently large. However, the motivation for small batch training, which is the main target of this study, is not convincing. Moreover, the validity of the comparison between the proposed and existing methods (K FAC) is questionable.<|endoftext|>The paper improves L BFGS by adding momentum to the gradients and past parameters, and adding a damping term to the Hessian to improve the condition number. The authors run their algorithm on Imagenet with Resnet 50, and compare its performance with SGD and KFAC. In table 2, the authors compare the time and memory of SGD, KFAC against SLIM QN. The memory requirements of the proposed method are quite high, up to 20 40 times more than SGD, this would prevent this method from being used for the large models such as BERT that are commonplace in ML practice today (Resnet 50 has 23M parameters, whereas BERT has 340M). I would like to see the results on larger batch sizes anyway as I suspect that the benefits of momentum and damping may not be as significant. Overall I feel that both the novelty and impact of the paper are somewhat limited. The key techniques proposed have been studied before, and the resulting method is only practical for small problems.<|endoftext|>This paper proposes SLIM QN, a light stochastic quasi Newton optimizer for training large scale deep neural networks (DNNs). Based on previous results, SLIM QN proposes to introduce momentum in Hessian updates to stabilize the training and adaptive damping mechanism to guarantee that the approximated Hessian is always positive definite. Some experimental results are also given. The paper is well written and easy to follow. 2.The proposed SLIM QN has some computational and memory advantages. It seems useless in practice. 2.AS1 and AS3 are very strong assumptions. 3.SLIM QN is a method based on L BFGS but Table2 and experiments lask the comparisons with the same kind of methods, e.g.BFGS and L BFGS. Both the theoretical and experimental results are not sufficient. The assumptions of theoretical results are too strong and experimental results lack the comparison with the same kind of methods.<|endoftext|>In this paper SLIM QN  is proposed to  addresses two key barriers in existing second order methods for large scale DNNs: 1) the high computational cost of obtaining the Hessian matrix and its inverse in every iteration (e.g.KFAC); 2) convergence instability due to stochastic training (e.g.L BFGS).Converegence   results are provided in a stochastic setting. What is the stopping rule the authors used  for SGD ？ As it for me,  it seems to be a nice work in developing second order method for large scale machine learning. I not quite sure why SLIM is faster than sgd (as reported  the wall clock time) ?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper proposes an efficient mechanism for  image classification. It has a light weight delegator to yield coarse prediction, where early termination is possible. The so called CoE framework achieves very competitive results on the ImageNet dataset with low FLOPs and low CPU latency. ++ The expert selection is done at the model level, so that the advantage of state of the art efficient networks (OFANet, MobileNetV3) can be taken for small FLOPs and low CPU latency. Are they the same thing but optimized separately by two different constraints (Eq.1 and 6)? Also, the authors didn t clearly specify how the inference works. Why does the "Collaboration" means in the framework "Collaboration of Experts"? The selection is one hot, so there is actually no collaboration between experts? Is it also including the early termination? I think "\citep" can do this. What is the true novelty is somewhat unclear to me. Also, the methods are not clearly written and how different modules work is confusing.<|endoftext|>This paper proposes the Collaboration of Experts (CoE) framework to both eliminate the need for multiple forward passes and keep hardware friendly. A training method including weight generation module (WGM), label generation module (LGM) and selection reweighting module (SRM) is proposed to improve the expert selection. The propose CoE frames achieves good performance with little computation cost. 3).Promoting within dataset diversity in the expert selection sounds novel. Isn t Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM? 2).80.0% top 1 accuracy on ImageNet is not hard. 3).I don t know which of the proposed components contribute the most to the overall performance. Good results, but lack of ablation study of each proposed components.<|endoftext|>To alleviate these issues, the authors propose a model collaboration framework named Collaboration of Experts (CoE) along with a training algorithm designed for the framework. The training algorithm contains three components: weight generation module (WGM), label generation module (LGM), and selection reweighting module (SRM). 2.The model has achieved 80% accuracy within 100M FLOPS, achieving the SoTA performance on ImageNet. The paper mentions that the delegator needs to select an expert from the candidate experts and compares to other model selection methods. However, the related work of model selection [A, B] is lacking in the paper. Most of my concerns about the insufficient experiments have been addressed by the rebuttal. The problem of this paper is interesting, but the experiments are not sufficient to support the claim.<|endoftext|>The paper has a lot of pros and cons. Is this expected? The outcome is a tiny model that yields excellent performance on ImageNet, with performance envelopes superior to existing methods. It is practical, it performs really well, and achieves surprising results on a very crowded, high impact hot topic, which is impressive. Here are some of my issues:  What the WGM, LGM and SRM do is not clear. seems to do something very similar to the LGM. There are some phrases that are hard to understand. But what is the role of the WGM? is it balancing so that there is no collapse to a single expert? Why is it needed? However, it would be interesting to see an ablation on some of the components of the model   for example, how important are the SRM? But then, why use an indirect metric like statistics instead of using the actual loss of the chosen expert? This thoroughness is appreciated. There is no explanation regarding whether/how/when it is used, training details, etc. So, all in all, I would really like clarity improved because I think the paper really deserves it.
Accept (Poster); rating score: 8; rating score: 6; rating score: 1; This work introduces a novel scene representation for agent navigation in 2D and 3D environments. At the core of the method is an implicit neural representation of environment   implicit environment field (IEF)   which is a neural net that maps location coordinates to its reaching distance. Navigation can the be achieved by gradient descent on the reaching function or a discretized greedy algorithm. Experimental evaluation is conducted on 2D maze navigation and 3D human motion prediction, and indicate that the proposed representation performs favorably to the baselines. The paper is well written and is mostly easy to follow. Quantitatively, the method seems to be performing similarly to baselines or slightly better, while being more efficient. There is not much details provided on the architectures of the networks. Method.Authors claim that continuity is one of the core benefits of their approach, yet in practice they seem to be resorting to a discretized solution when doing trajectory search. Is there any benefit of using continuous representation in 2D? It would be great to also have an idea about how the results look qualitatively for the baselines   it is hard to judge if the quantitative improvements are significant. There are some of the failure cases presented for the human motion prediction, but not for maze navigation. a continues field  > continuousThis is a well written paper that provides an interesting solution to an important problem. I am not too familiar with the field of motion planning, thus a low confidence in the assessment. With all that in mind, I am quite positive about this work, and recommend accept.<|endoftext|>The paper proposes modeling reaching distance between any start position and any goal (subject to obstacle avoidance) with a neural network. This is equivalent to parameterizing a traditional path planning (goal reaching) continuous value function with the network, which the authors also mention in the introduction section. Because of the empirically validated generalization properties of the presented model when the 2D scene layout & goal inputs are switched, I am leaning slightly positive, but currently the concerns listed in my review are stopping me from giving a higher score. The usefulness of the trained value network for navigation and its generalization properties are then experimentally validated in 2D and 3D environments, including an interesting navigation example with two dynamically moving agents in the same scene. ## Strengths  I believe the main strength of the proposed solution is the ability of the learned value function to generalize to different goals & different scenes, which is ensured by conditioning the network on a target goal and learned features from the 2D layout of the specific scene in question (e.g.section 3.4 and Fig.3, c)).It is nice to see that an MLP can indeed generalize successfully when different goals & scene layouts are fed in, as illustrated by the multiple human navigation experiment (shown in Fig.4).Section 3 also mentions predictions are made for unseen mazes during inference, which is good. I am worried that, given the example in figure 2, the gradients (and respectively predictions) of the network in these interpolation regions (i.e.between the points from FMM) might be suboptimal. When modeling accessible regions in 3D space with a VAE (section 4.2), what is the motivation behind using the VAE model to begin with, i.e.why not directly use the empirical distribution of human torso locations the VAE is trained on? ## Other remarks  Looking at the supplementary material, there are some accidental flips of the human skeletons in the videos. Is this because of issues with the alignment of the movement primitives to the planned trajectories, or because the planned trajectories have occasional jitter?<|endoftext|>The paper proposes a method to learn an environment field that predicts the distance from any location in the map to a query location. While the general idea, having a neural network learn to reproduce the output of an A* or Dijkstra algorithm is sensible, what is proposed in this paper is simply not comprehensible. This confusion is caused by the ambiguous writing used in Figure 1 and the abstract, implying Dijkstra and the introduction, which implies A*. The description of the method itself is simply devoid of all required detail, such as architecture, training regime, cost function regularization, biases induced due to the discretized nature of the training data, etc. In many instances, the specifics of the technical detail provided appears wrong or not the commonly used form. The experiments highlight further problems with the proposed method. While in earlier parts, the paper describes the importance of a continuous representation, all experiments are performed on discretized grid environments. While the general idea is viable and something being researched, what is being proposed in this paper is simply too unclear and vague to be assessed.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper provides task aware privacy preservation to improve the task performance for multi dimensional user data for deploying a trained model. IJCAI 19.The general idea of the paper is interesting, but it seems to be an early stage of this work. Collecting and analyzing multidimensional data with local differential privacy. Strengths+ The problem formulation is simple and easy to follow to get the idea of the proposed method. + They provide an intensive and rigorous analysis for an analytical near optimal solution for a linear setting and MSE task loss in terms of task aware privacy preservation analysis. Second, the reviewer does not understand why we need LDP here and why not centralized DP? It is not clear at all in the current writing. The reviewer needs to guess what it means.<|endoftext|>The paper addresses the problem of large loss in utility due to the addition of noise in a local DP setup, especially when working with high dimensional data. The training  of the autoencoder is done over an offline phase on public data and with respect to the main task that the data is to be used for. 2020 Mar.The main reasons for my recommendation are:1. similarity to prior work, the novelty of this work and it s benefits are not well identified2. This could even circle back to interpretability and how different models might use different features.<|endoftext|>This paper proposes a task aware local DP method to improve the privacy andutility trade off for multi dimensional user data. Also, a analytical near optimal solution for a general linear encoder decoder model and MSE loss is provided. The problem of improving utility and privacy trade off in local differentially private ML is an important practical problem. 1.The local DP setting is not clearly given, which I mean, interactive [1] or non interactive setting [2]? 2.Based on my understanding of this paper,  Algorithm 1 needs $N_{epochs}$ to find the encoder, decoder and sensitivity. During this interactive proposes, the algorithm continues to query the sensitive user data samples. Please provide a comparison with [3]. 2018.This paper proposed a good analysis for task aware local DP.<|endoftext|>This paper proposes a task aware local DP approach to improve the performance on multi dimensional data with same level of privacy. The proposed approaches solves the problem in an efficient way. The motivation is clear, it is well structured and well written. In my opinion, there are three things that can improve this paper. i) The paper presents a heuristic learning algorithm, but the analysis of the task aware privacy preservation problem for approximate LDP would be more interesting. In the current version, the difference between the previously proposed methods is not so clear. This paper considers and important problem and brings an efficient solution. It is well written.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper empirically studies pre trained model on OOD generalization. Second, using smaller learning rates during fine tuning is critical to achieving good results. Third, the strategies that improve in distribution accuracy may lead to poor OOD performance. 1.The claim "models trained with smaller learning rates achieve much better OOD generalization" may be misleading/incorrect. The novelty of using pre training model to improve OOD generalization is not significant and this idea is not new. Furthermore, the intuition of using larger models and larger datasets simultaneously is unclear. 3.The idea of using pre training model is not new and the novelty of this paper is limited.<|endoftext|>Within the now common pre train/fine tune training paradigm, this paper provide an empirical examination of the influence of pre training on out of distribution accuracy *after* fine tuning, specifically for computer vision. While the conclusions (that larger models and more diverse datasets are better) are not unexpected, such detailed empirical studies are worthwhile for practitioners and researchers alike. Strengths:  While the overall conclusion that larger models pre trained on more (and more diverse) are better is not surprising, I think it is useful to confirm this finding finding with respect to OOD generalization specifically. * What specifically in the larger datasets helps? * What about the smaller learning rates make them more suitable? I have a suspicion that it has to do with the weights deviating less from their pre trained starting point during fine tuning.<|endoftext|>First, they show that fine tuning a pretrained model significantly improves OOD accuracy over the non fine tuned baseline. Finally, the conclude that larger models, larger datasets, or (preferably) both improve OOD generalization. Unfortunately, I think it lacks a strong selling point as most of the proposed insights are not novel enough. See, for example, the work of Li et al.(2020) for an in depth study on pretrained vision models, and Dodge et al.(2020) on language models. Maybe the most interesting contribution is the insight that smaller learning rates improve OOD accuracy even thought they perform on par with larger learning rates in ID data. Although the experimental study of the paper is well executed, it provides few novel insights.<|endoftext|>Overall, I think this paper provides a good set of empirical studies on an important problem: what contributes to the better OOD generalization performance of pre trained models (and relatedly, how are ID accuracies related to these OOD accuracies). This is different from the results presented in Table 8 because the models there have substantially worse ID accuracy as well. 2.The paper is overall well written and easy to understand.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; Unlike previous methods that calculate the loss function in image space, this work proposes to calculate in the simulation state space to avoid the issues (e.g.being stuck in a local minimum) when the reference video is very different from the generated video. Specifically, a rendering invariant state prediction (RISP) network is pretrained with the generated data from a differentiable renderer under various rendering conditions. In addition, a new training strategy to efficiently calculate the gradient for the loss function is proposed. The experiments have shown that the proposed method significantly outperforms the state of the arts and the proposed components contribute to the final results with big improvements. The paper is well written. This paper is a solid submission, which solves an important problem and proposes some novel ideas. I expect the authors to evaluate the method on more datasets, especially real datasets.<|endoftext|>The paper focuses on the problem of estimating dynamic parameters of a physical system from videos under unknown rendering conditions. It presents a novel idea of using a rendering invariant state prediction (RISP) network that predicts the state from a rendered image and can be integrated into a framework with a differentiable simulation and rendering engine for parameter estimation. Training this network is done using domain randomization technique with synthetic data. Strengths:  The paper is very well written and contributions are clear. This means that the parameter distribution in both training (for the RISP) and testing cases come from the same distribution. It would be interesting to see results when the train and test rendering parameter distributions are different. It would be interesting to see how the various methods would perform on real videos. The paper presents mainly two new ideas, the RISP network for state estimation and the gradient loss for regularization. The quantitative and qualitative experimental results on synthetic data show clear improvements compared to the previous methods.<|endoftext|>They do so by predicting a "rendering invariant state" that results in a much stabler loss landscape and reduces domain gap / mismatch. ### Strengths* **S1** This paper tackles the challenging (inverse) problem of directly recovering object properties (system identification) or control parameters (visuomotor control) from image/video observations. This work proposes RISP to bridge that gap (bridging this gap is crucial to enable several downstream, including real world, applications). * **S2** The paper is very well presented. Does this have an imact on e.g., where the object may lie within an image, and perhaps impact the extension of RISP to simulations with multiple objects? The authors needn’t respond to these* “Articulate body”  > articulated body* “Simulates”  > simulateThis is a well written paper describing an idea of substantial interest to the physical reasoning and the visual reasoning communitties.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper presents Policy Gradients Incorporating the Future (PGIF), a novel approach to incorporate future information during training to improve performance of model free RL agents that must overcome the challenges of planning in environments with sparse rewards. The approach involves relying on recent work in "Z forcing" in which training incorporates information about the future, encoded as a learned latent state that depends on the full not yet executed trajectory and state information. The future information helps the agent to learn effectively and overcome challenges associated with credit assignment notorious in partially observed environments; additional losses are added to ensure that the latent sate contains sufficient task relevant information and that the planner does not rely too heavily on future information. Overall, this is a solid paper, that combines an interesting and novel theoretical idea and convincing theoretical results. For example, while the reference to Appendix.F in Sec.3.2 is quite informative (and mentions that applying PGIF to the policy and value functions degrades performance), for other content, the reader *must* go to the appendices to  understand the conclusions of supplementary experiments: e.g., appendix G in which Transformers are used instead of an RNN; there should be couple sentences in the main text summarizing those results. It was only by the time that section was over that it was clear that only one would be used at a time. Could the authors include an order of magnitude number for the difference in wall clock training time for one of the experimental environments?<|endoftext|>The work proposes to run two recurrent LSTM neural networks backwards from the end of an episode, giving the rewards $r$ and states $s$ as input to the networks, and producing two separate latent state ($z$, $u$) distributions for each time step. The offline RL experimental results also are not clear to me. Once, the $z$ and $u$ are computed, these are given as an input to the Q function $Q(s,a,u)$, and the policy network $\pi(a|s,z)$ (note that the action was already sampled during the episode, but here the probability is recomputed using the new $z$). Update after author rebuttal The authors have provided additional evidence on the correctness of their baselines. I still have some concerns with the paper:  The experiments still only look at the reward curves. This is done by training a prior network $p(z|s)$ by minimizing the KL divergence to the latent state distribution obtained from the backwards network (then one can just sample a $z$ from this prior network during the episode). To aid the backwards network in learning meaningful latent representations, they use z forcing, which is a technique where the variable $z$ is used directly to predict some quantity to ensure that it incorporates useful information. I am still not convinced by the authors  explanation of why "looking into the future" is useful. It is quite possible that the experiments are performed solidly; however, this is not explained sufficiently for me to understand this. They perform experiments on Bsuite umbrella length, Gym minigrid, custom partially observable MuJoCo environments, Offline RL on MuJoCo D4RL tasks. Instead of using your approach, one can give perfect information of the future by just using the empirical return. I think it would be better to also compare to a version where the backward RNNs are not trained, but the $p(z|s)$ is just trained end to end as if it were a part of the policy with some additional latent noise $z$. The result of ~0.6 is higher than the result of ~0.5 for DQN reported in the bsuite paper.<|endoftext|>Strengths:1) The paper attempts to address the problem of credit assignment in RL which is very important. The authors build off of the existing idea of learning future conditioned policy/Q function and introduce several tricks (informational bottleneck and z forcing) to make it work. 2) The paper demonstrates that the proposed approach can be applied to several RL algorithms, such as PPO, SAC, and BRAC. The the authors attempt to differentiate their work from the prior methods, but the difference mostly comes from the fact that PGIF can be made off policy, unlike the prior work (i.e.Hindsight Credit Assignment) which is on policy. 2) It is not clear how much information about the future one can force into the prior distribution that is only conditioned on the current state. I m not convinced that this does any better than for example SAC (or DDPG/TD3) + n step returns. The empirical results are encouraging but not ground breaking. 3) Re the MuJoCo partial observability experiments: I don t think it is a fair comparison to compare SAC based PGIF against vanilla SAC (that doesn t use recurrent policy/Q). While the paper studies an interesting direction of learning future conditioned policy/Q function, which is quite exciting, I have several issues with the paper, namely:1) The incremental nature of the work. 2) The complexity of the method.<|endoftext|>It is explained in the appendix but could perhaps be a footnote in the main paper. Two technical challenges were reducing over reliance on future information and allowing the policy to make predictions without future trajectory information at deployment time. Optimization issues with latent variable models are tackled using two different Z forcing approaches. This can be tested by including the number of remaining episode steps into the state and rerunning the experiment. However, in fully observed online settings it is not obvious to me why this method improves performance, and I don t see much intuition provided in the paper. Why is the improvement in offline continuous control much greater than the online equivalent? I m not too familiar with the related work on hindsight, so my review takes the paper s comparison with this prior work at face value. Here I review the key constructive points:  Beyond the online ant setting in the appendix, I don t see much evidence that this method improves performance outside of partially observable environments in the online setting. The intuition behind why it would help in fully observed environments is also not clear from my reading of the paper. As far as I can tell the use of Z forcing and the variational bottleneck is novel in this setting. The results on partially observable and offline settings are impressive. Section 5.1 is supposed to evaluate the method on sparse rewards, testing credit assignment. But a partially observed environment is used for some of the experiments in this section.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; In this paper, it is proposed to regularize the parameters of each Q function in ensemble so that they are not similar to each other. Through various experiments, it is shown that RL methods that use ensembles of Q functions (e.g., REDQ, Minmax DQN, etc) can improve its sample efficiency by using the regularization. strengths: + A simple but general regularization method to improve the performance of Q function ensemble based RL methods is proposed. As I mentioned in the summary of the paper, a method to regularize the similarity of the parameters of the Q functions is proposed in the paper. The similarity of parameters between Q functions is evaluated based on five measures (e.g.Gini coefficients), and then added to the loss for the Q functions. The proposed regularization method is shown to be effective in five different RL methods (SAC, TD3, REDQ, Maxmin DQN, and Ensemble DQN). weaknesses:   There is no theoretical justification for introducing the regularization method based on the similarity measures. For example, it is not clear if Q functions converge to the optimal one (under reasonable assumptions) when they are trained with the regularized method. Nevertheless, I regard the study presented in the paper as an empirical one, and thus do not think that the lack of theoretical justification is a severe weakness of the paper. The quality of the presentation is not sufficiently good:     Connections among the sections in the paper are somewhat unclear. For example, in Sections 3 and 4.1, the similarity of the Q functions (neural nets) is measured by the similarity of their activation units  outputs. Experiment setups are not clearly explained. If so, what if REDQ s performance converges before 300k interactions? REDQ baseline: Is the REDQ s learning curve the one shown in Figure 5? If so, why is it much worse than the one shown in the original REDQ paper? This is probably because all the data points in the graphs are output in vector format. If you want to put graphs with many data points on the same page, it is better to rasterize the graph. I m leaning to recommend reject. I also acknowledge the authors  effort to demonstrate the usefulness of the regularization method in various RL methods and environments. However, the clarity of the current version of the paper does not meet the threshold for publication, and a non trivial revision is needed. Update after author s revision 20211125    The authors have improved the explanation of experimental setups, but the connections between the sections still have not been sufficiently improved. In particular, some of the other reviewers are concerned about the mismatch between the similarity criterion (CKA) used in Section 4.1 and the similarity criterion used in later sections. I think that replacing the analysis based on CKA with an analysis using criteria based on parameter values (e.g., equation (4)) would make the discussion in the paper more consistent.<|endoftext|>The paper considers a problem of ensemble based deep RL methods that ensembles of critic networks converge to the same point in the representation space. To address it, the paper proposes a regularization technique that forces representations of a critic network to be dissimilar from those of other critic networks in the ensemble. It is nice that one can gain a considerable sample efficiency and performance improvement by such a simple technique. S2.It is intriguing that the performance of ensemble based deep RL methods highly (negatively) correlates with the representation similarly among ensembles of neural networks. S3.I highly appreciate that the paper provides many experimental results with different regularizers, baseline algorithms, and the number of ensembles. # Weak Points of the PaperI give detailed comments about these points in the next section. W2.It is unclear how hyper parameters and seeds are chosen. In heatmaps at point A and C, (near )diagonal elements have low values, but other elements typically have higher values. Furthermore, only showing results at 4 points is not convincing to me. There are some places where the authors say "seeds are fixed". However, does this mean that you fixed random seeds, ran experiments with different hyperparameters, and pick up the best results? If you did this, there may be a maximization bias. In addition, running experiments with only 5 seeds is probably insufficient (Henderson et al.2018).Please run experiments with more random seeds. As for W3, it seems to me that REDQ s performance in this paper is significantly lower than that of original paper. of them, especially when ReLU is used. Maybe I am missing something? It is possible to backpropagate through it, right? Maybe replacing it with JPG or PNG would resolve this issue. Furthermore, the experiment conditions seem to be insufficient or not explained well.<|endoftext|>This paper proposes a collection of diversity metrics to improve ensemble diversity, which substantially improves learning efficiency for a variety of RL methods. This paper is clearly written and easy to follow. I m always happy to see simple effective techniques that can substantially improve the performances of existing methods in challenging testbeds, which looks promising to me. **Question**My biggest question is that all the criteria are evaluated w.r.t.the l2 norm of the whole network parameters, which is very counter intuitive to me. Wouldn t it be a more natural choice to use some metrics that at least involve CKA or layer wise information? Or, is it possible to provide some more experiments that use CKA directly? So it would be appreciated if the authors can provide some discussions in the related work section. Some of the references are listed here for the purpose of helping the authors to survey the literature more easily: https://arxiv.org/abs/2002.00632, http://proceedings.mlr.press/v139/lupu21a.html, https://arxiv.org/abs/2103.04564, https://arxiv.org/abs/2106.02195 In general, although the paper can be still improved, the general results and suggested techniques are promising.<|endoftext|>MED RLThis paper studies foster diversity in ensemble of DRL networks by regularization methods. The paper is an empirical one and compared five ensemble methods with and without their diversity algorithm in six Mujoco and six Atari games, and showed some results. The claim in abstract that “members of the ensemble can converge to the same point either the parametric space or representation space during the training phase “. Neural networks converge to different solutions given the initialization is different and multiple local minima. Why these environments were chosen? The claim of your conjecture is pretty big and here you have a small experiment about only one algorithm at only two time points. 3.In Alg 1, what is I()? Is it a general function that you have a few alternatives in Sec 4? 4.Sec 5.1: how do you initialize the networks in reaching the conclusion that “each neural network is trained on a separate batch from the replay buffer but still learns similar representations. “.Did you try another initialization method? Is the CKA non symmetric? There is also ACE algorithm that uses ensembles from actors. Many lines are hard to see their statistical importance especially only 5 runs were performed. Gym compatible games for reinforcenment learning. The paper had a claim that initialization of different networks is not effective in proving diversity. This is not well supported. It can give a wrong message to the literature with the small and insufficient studies. this is my main concern.
Reject; rating score: 5; rating score: 6; rating score: 6; In this paper, the authors propose a model that uses graph kernels to extend the convolution operator to the graph domain. I am not thus sure whether the results of the ablation study are significant and whether they would also apply to other datasets. The authors should consider to use another dataset that contains a larger number of graphs such that the obtained results are more valid. The proposed model was evaluated on standard graph classification datasets where it was found to be competitive with the baselines. Overall, this is an interesting paper which studies a topic that has not been fully explored yet. The authors mention in the conclusion that there are no widely available GPU implementations of graph kernels, thus it is likely that the running time of the proposed model would be prohibitive for real world applications. Even though the originality of the paper is not stellar, the paper introduces some new ideas. Furthermore, the empirical results are not bad, but they are also not impressive. Thus, the paper does not seem ready for publication in its current state. Perhaps this is due to the computational complexity of those kernels. I would like the authors to comment on that and it would also be nice if the authors could present some experimental results of the proposed model using one of the above kernels on some dataset that contains small graphs. In such a scenario, the masks could be of no practical interest. The authors perform their ablation study on the MUTAG dataset.<|endoftext|>The authors propose a method that is based on graph kernels to perform graph convolution, which add to the interpretability of GNNs. The results show improvement in some datasets. The authors provide a detailed description of the problem and related works and background. Also, it would be interesting to know if the method can be used for geometric datasets, where a more learned masks may be easier to explain. Overall, the results presented in Table 1 show that in some cases the proposed method outperforms other by quite a small margin, but in most cases it is not better methods. The authors state that graph kernels are limited by implementation. The paper is clearly written and motivated but lacks on the experimental side, both in results of current experiments, and also the scope of the experiments is rather limited.<|endoftext|>This paper proposes a novel architecture for graph processing that uses graph kernels with learnable structural masks. If this is a (well known) approach, please add a reference, otherwise, I think there should be more explanation about the method. The resulting kernel responses are collected into a vector, which is then quantized using k means clustering, and the cluster label is used as the new node label. The authors propose also Strengths:  The paper presents a novel and interesting class of learning algorithms for processing graph based data. The model is somewhat more interpretable than competitors and seems to be well apt to settings where the topological structure of the graphs plays a major role   The paper is generally well written and easy to follow, although some parts of the presentation could be improved as in the detailed comment 1. Weaknesses:  The model has some limitations. 4.Regarding experimental validation: a) I would like to see a training curve. The authors should be more clear about these limitations from the introduction. I believe the part regarding the learning technique could be clearer, more formal and with more details. However, most of the experiments have been done with the WL kernel. If so, how would it fare on the benchmark datasets? This paper is a potentially significant contribution to the area of graph ML. Moreover, the experimental section could be improved.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper investigates when to switch between exploitation and exploration and how long to stay in each exploration mode during RL learning. It proposes new ways to explore the subject, especially with intra episodic exploration variants. As positive points we can cite the wide related bibliography, from analogies with the animal system (human included) of exploring to other techniques such as the use of options. Another point to emphasize is the clear, careful and pleasurable writing that the authors developed in the paper. Perhaps the negative point is the limitation of the contribution   since it is still a study that must be deepened in order to provide effective and efficient guidelines for RL system developers working in real applications. The article is worth being divulged to remind everyone working in the RL area that there is still a lot to study, investigate and evaluate in order to have robust, efficient and effective systems. The paper brings an in depth study, with interesting results and very well written.<|endoftext|>It discusses switching mechanisms based on time ("blind switching") and based on state ("informed switching"). Studying seven Atari games, an empirical analysis of different switching mechanisms is performed. Overall, the paper is very strong, well motivated, and empirically sound. Most of my concerns are with clarifying details of the experiments and improving the exposition. These are listed below:  paragraph 2 of the introduction: the typical answer to "when to explore" is when the agent is unfamiliar with the environment or its structure, i.e., early in its interactions with the environment. paragraph 3 of the introduction: it s not clear what the connection to schizophrenia here is, other than that this was a study of explore vs. exploit; are these individuals somehow impaired in choosing between these options, etc.? paragraph 1 of section 2 (methods): not sure that the example of riding a bicycle works   is the targeted acquisition of a new skill really exploration? caption to Figure 1: the differences for D G are unclear just by looking at this figure and the caption, other than that they are different intra episode approaches; it would be better to clarify this  section 2.2, description of "blind switching": this refers to "fractional episode length", but if we don t get to choose the length of an episode, how do we implement fractional episode length ex ante? how does this affect learning and why was this choice made? section A.1, regarding no life loss signal: what exactly is an episode here if there is no life loss signal? How does scoring work?<|endoftext|>The idea of mode switching is interesting. This means that the modes that they switch between in this paper are merged homogeneously in time. A valid problem that the authors point out for the monolithic case is that the scale of the intrinsic reward signal has to be tuned and may need to change in time. But there is no comparison to these methods and the superiority of their method to the monolithic variants are not highlighted well enough. The authors try to circumvent the performance comparisons with other baselines by saying that they can obtain more diverse behaviours (in terms of exploration strategies?) If it is the latter, the diverse behaviour doesn’t necessarily translate into performance for the different games. The related work section briefly covers some similar methods, and I think comparisons are still needed with these other methods. E.g.GoExplore also focuses on the *when* question of exploration and should be included as a baseline as well as works with monolithic behaviour policies where the mode switching is replaced by a weighting problem of external and intrinsic rewards of a single behaviour policy.<|endoftext|>This paper proposes to study exploration at different levels of granularity. This paper proposes to study exploration at the intra episodic level, i.e.where the agent switches between exploration and exploitation within the same episode. Strengths:  this paper investigates a novel area, which seems very important at a high level. Some suggestions for improving the paper:  It would be helpful to have the different algorithm variants in Section 2 spelled out, especially regarding the different switching mechanisms. I think the paper would be a lot stronger if there were some tasks which more convincingly demonstrated the benefits of intra episode exploration. This could be a new task which the authors design themselves. I think including the Atari experiments is useful in that it shows their methods can improve performance on a standard benchmark, however, the monolithic exploration methods already work quite well on these tasks, and it is not clear if there is that much more improvement to be had by exploring at a finer granularity. I do agree with the authors that in the “big picture”, monolithic exploration is likely suboptimal and more informed exploration will be necessary. Introducing new tasks which measure the ability to optimally switch between explore and exploit modes would also be useful to the community in building on this work. On one hand I think it is good that the authors are exploring a new and important area and the ideas are interesting.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The paper study the problem of quantum neural networks for classical image classification. The novelty is insufficient as the embedding part and parameterized circuit part is not new. the evaluation is insufficient: no comparisons with state of the art; no evaluation on real quantum devices; no ablation studies; no justifications of why the proposed embedding can outperform others. Use FRQI to embed images with higher resolution. The proposed parameterized circuit part is also similar to that in [2]. 2.Major: Although the image resolution can be large, from 4x4 to 8x8 or 16x16, they are binarized, which will obviously lose a considerable amount of information. Comparisons with previous QNN work are completely missing. 6.Minor: the claim of "our work is the first to propose a data encoding scheme and QNN that can be used to classify realisticimages."<|endoftext|>It is not clear what are the gate and time complexities and how does itcompare to existing approachesThe authors proposed a new algorithm for classification problems combining a new quantum data loader for images andan existing trainable quantum circuit. First, the data loading procedure assumes that a binary representationof the image is enough for classification which may not be true for datasets with low contrast images. The number ofqubits is logarithmic with respect to the image dimensions but the circuit seems to have a large depth. Second, the quantum circuit used forclassification is neither novel nor close to state of art quantum neural networks.<|endoftext|>This paper studies the image classification problem on quantum computers. However, after taking a closer look at this paper, I feel that the contribution in results as well as the novelty in algorithm design are not significant for this paper to be accepted by ICLR 2022. For the circuit synthesis in Section 5, it is mostly the standard trick in Lemma 1 proposed by Barenco et al.more than 20 years ago, and for the circuit implementation the authors simply call the standard package TensorFlow Quantum from Google AI. In particular, the abstract claimed that their result “obtains accuracy comparable to classical neural networks with the same number of learnable parameters”. However, in Section 6 of the paper, the authors admit that “we observe that the QNN is unable to achieve the same performance as the classical network”. Based on the concerns I mentioned above, I recommend rejection for this paper at ICLR 2022.<|endoftext|>Additionally, the experimental section studies the use of reduced codes. The experimental validation of the proposed approach goes beyond single runs. What does it mean for data to be "inherently quantum"? Authors claim to have the ability to 8x8 or 16x16 encode images on current quantum hardware. If "these quantum systems are already approaching the limits of classicalsimulability by the world’s largest traditional supercomputer" (page 1), why would simulation experiments on a consumer laptop suffice in the paper s case? Page 5 observes that the color qbit is either $\braket{0}$ or $\braket{1}$. MNIST digits are black and white.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The author proposes a method, Critical Classification Regions (CCRs), to do this. CCRs use a nearest neighbor example to highlight similar important parts in the image explanation. The author performed a user study on a subset of the Imagenet to show improvement of the CCR method. The author has proposed the critical classification regions (CCR) method, but it is not discussed in the paper. The method section is not clearly discussed.<|endoftext|>The manuscript proposes a method to extend explanation by example (aka.exemplar based explanations) by highlighting the regions that links the test image with example(s) provided as part of the explanation. Towards this goal, different methods to compute Critical Classification Regions (CCR) are proposed where image regions/pixels linking the input with the explanation examples are highlighted. This is complemented with a user study.<|endoftext|>The authors proposes an approach to improve explanation by example techniques, by picking nearest neighbors (NNs)  based on fine grained image content that are critical for the classification decisions. The main advantage claimed is the causal role of the generated explanation. Wording Suggestions:  Critical Classification Regions  > Classification Critical Regions? The authors support this claim via quantitative evaluation and human evaluation.<|endoftext|>The paper proposes a local model specific post hoc explanation method for image classifiers (CNN) returning as explanations examples suggesting the reasons for the classification together with subparts of the images named critical regions common to the test instance and the examples and responsible for the classification. I suggest avoiding using k for both the number of classes and the k of the NN algorithm. In this last case, in fact, I suppose that the examples are extracted with the twin system. Second, Experiment 1 is biased because the way adopted to test the importance of the parts of the image, i.e., by image occlusion, is exactly the same used by SP CCRS.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; rating score: 5; The paper consults the empirical findings on the Lottery Ticket Hypothesis and proposes a kernel pruning framework with densely structured outputs. Strengths:* The paper addresses an important research problem in the field of model compression as most kernel pruning methods suffer from the curse of sparsity and are therefore not computational friendly. The intuition behind each proposed component of the framework (e.g., the proposed TMI score) is well discussed and most of them seem straightforward. The experiments are conducted on CIFAR10 and TinyImageNet only, so it is unclear whether it would work well on datasets like ImageNet. It would be more convincing if the results on other popular CNN architectures are also reported (e.g., densely connected blocks and NAS searched blocks like EfficientNet). In summary, I feel that although the proposed method seems to be novel, the evaluation part is not yet convincing.<|endoftext|>This paper proposes a new approach for kernel pruning, by leveraging group wise sparsity to obtain a compact architecture that can be parallelly computed. The approach first clusters the kernels based on the defined measurement via recent findings in the lottery ticket hypothesis (LTH), and then develop a simple and efficient greedy approximation algorithm for filter pruning. The proposed approach is technically sound in general. The idea to leverage group convolution for kernel pruning is practical. The performance gain on CIFAR 10 can be minor compared with existing baselines, e.g., ResNet 32 has 92.82% acc, which is even worse than other baselines. There are no results on ImageNet. It is not verified if the findings in Zhou et.al. I wonder if the magnitudes of grouped filters still correlate with the final accuracies. What is the time consumption for each step in such a pipeline, aside from network fine tuning after pruning? Overall this is a technically sound paper.<|endoftext|>The authors present a new metric to determine the similarity between different grouped kernels and prune the unimportant $k\times k$ slices out of a 3D filter. The empirical success of this paper may serve as proof of the existence of the Lottery Ticket Hypothesis. pros.1.The proposed method seems simple yet effective, though it has not been fully evaluated on the large dataset. The combination of the Lottery Ticket Hypothesis and structured pruning seems interesting. This paper may shed some light on this new direction. Though I fully understand the authors are stuck with limited computing resources, a solid result on ImageNet can be more convincing than CIFAR 10. It seems that the computing complexity of those steps are $\mathcal{O}(\prod_{i 0}^{K 1} \binom{C_{out} i\cdot\frac{C_{out}}{K}}{\frac{C_{out}}{K}})$ where $C_{out}$ is the number of filters, $K$ is the number of groups, which is still very large for $C_{out} 512$. 4.I have checked section A.3.1. ICCVw2019In general, I believe this work is somehow different from existing works but the current draft makes it challenging for these ideas to reach their full potential.<|endoftext|>This paper presents a novel approach in the well established space of structured pruning methods (kernel pruning) post training, which primarily consists of three stages: (i) clusters the filters in a convolution layer into predefined number of groups, (ii) prune the unimportant kernels from each group, and (iii) permute remaining kernels to form a grouped convolution operation and then finetune. It would be better if the authors can clarify this corner case.<|endoftext|>The proposed work focuses on kernel pruning. Could the authors update their results and compare against methods with high performance. The proposed work explores optimal grouping schemes for filters and after pruning unwanted filters, the retained filters are restructured and the network can be fine tuned to recoup prediction accuracy. The core idea revolves around grouping filters using a similarity criterion and removing common convolutional kernels with the group. After Rebuttal  I appreciate the author s clarifications on a number of the comments posted. After considering the revised version of the paper, I have updated my scores across multiple metrics. If so, doesn t it add more FLOPs to the forward pass? The method proposed in the work provides an alternative pruning approach that should help decrease the computational complexity. A common theme across the Related works and other sections  is the highlighting of computational cost involved in complex pruning heuristics. Could the authors clarify the magnitude increase formula , from Pg. However, this leaves the choice of windows for new dataset/DNN architectures open. Could the authors discuss their choice of layer pruning ratios ? This could add more strength to the argument of loss computational overhead. Could the authors clarify if their training and pruning setups are the same?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The method first estimate the trajectory by smoothing the observations (using any convenient smoother) before inferring a closed form (finite sequence of operations) transition function. This allows for better interpretability of the system. Authors test D Code on 5 simulated datasets, where they show high performance of the method, and on one real dataset where they use D Code to model the temporal effect of chemotherapy on the tumor volume, and compare it to Total variation regularized differentiation (SR T), the closest method in the literature. The results presented in the paper are compelling but I would want to see more comparisons to other methods, as well as results on more datasets. However, all these system have independent Gaussian noise (with different noise level). If this is not the case, I don t really see the advantage of D CODE in addition to "converting" an already inferred trajectory into an ODE for the sake of interpretability. I would lean towards acceptance of the paper. Using symbolic regression to approximate closed form ODEs is novel and very interesting, and the results presented in the paper are compelling. I am hoping that the authors will be able to to answer my concerns by presenting more results.<|endoftext|>This paper suggests a variational formulation for discovering ODE systems in a closed form. The main advantage of such a formulation is that it can find the system based on the observed trajectories (and some analytical testing functions) rather than estimating (possibly noisy) derivatives of them. The proposed method outperforms its counterparts for various ODE systems and tumor volume dataset (whose true dynamics is unknown). The paper is well motivated and easy to follow. while the baseline cannot. It seems that the authors have found non autonomous ODE for the tumor growth task, thus it will be nice if they can explain their modeling procedure briefly. * Q3.The authors state that neural ODEs should estimate the unknown initial condition (thus they are not very effective for chaotic systems), but it seems that one can rather directly use y(0)   x(0) + eps(0) as an initial condition for neural ODEs. As I mentioned above, I think the paper is interesting and thus recommend the acceptance of this paper.<|endoftext|>The paper addresses the problem of learning closed form ODEs from observational data, when the observation can be noisy and not frequent enough that instantaneous derivatives can be estimated with low enough error. The work uses a variational criterion on the solution of the ODE, circumventing the need to evaluate instantaneous derivatives. It uses existing symbolic regression techniques but instead of the regression loss, it minimizes a loss term quantifying the violation of the variational constraints. It is a work combining existing results in a novel way, but offers new perspective and there are synergies between the different parts applied. The method is clearly described, and the potential benefits are clearly understandable (see summary). However, I find the baseline comparison somewhat incomplete. My main questions are about the NeuralODE comparison. I do not agree that learning a chaotic dynamics directly with a NODE is not possible. I have some questions on the Neural ODE comparison what would need some clarification. If this concern is addressed it would make it easier to support acceptance.<|endoftext|>This paper proposes a new methodology to infer symbolic ODE representation from observed time series. The first step can be performed with the interpolation method of choice (GP or splines). The authors then evaluate their approach. on a series of dynamical system and show improved performance compared to the baselines under consideration. My main comment is that I think another baseline to investigate the importance of the proposed objective function would be the following : you optimize over an initial value and the function f and you minimize the reconstruction error. I also believe this setup would fit Theorem 1. If this is not a good idea, can authors explain why ? In Table 2 it seems that the performance differ massively between both dimensions (8) and (9). Yet, at a glance, their functional form look really similar. Because all these symbolic regression approaches make use of non differentiable optimizers, the computational cost can be quite high. In practice do you use single dimension GPs for each dimension or do you use multi task GP to model the correlations ? In Equation 3, the first integral should have $f_j$, as $f$ is not a scalar function. I would like the authors to motivate why the setup I described above is not considered in the experiments.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 8; For unsupervised feature selection, this paper proposes a two stage second order method. Knowledge contrastive distillation is also incorporated for feature learning. Cons:(1) My main concern lies in the experiments. The improvement of the proposed method over NDFS is also marginal on several datasets. Though the authors also compare with two recent methods, including CAE and InfFS, their performance is even lower than NDFS since they adopted different datasets in their original papers. Therefore, the experimental results are not convincing. (2) The novelty is not satisfying. All these modules, including the second order information, have been well studied in the area of unsupervised learning. The proposed method is a combination of these methods. The perspective for unsupervised feature selection by feature relationship learning and graph segmentation is not new from my point of view. (4) The computational complexity is very expensive, which is much higher than many compared methods. (5) This paper is rejected by NeurIPS 2021, and I happened to review this paper several months ago. The exposition could also be clearer, but the idea seems more complex than it is interesting. In all, the paper does not rise to the standards of significance and novelty of ICLR.<|endoftext|>This paper proposed an unsupervised feature selection method, SOFT, by combining the information from the second order covariance matrix with the first order data matrix. In general, I think this paper is relatively interesting, and I was initially interested by the title of this paper. I would like to increase my score if the author(s) could give convincing responses to the previous comments/questions in the Weaknesses part. # WeaknessesHere are some of my comments/questions. I would appreciate it if the author(s) could give a response. The authors demonstrated the effectiveness of the proposed method through empirical experiments. Currently, there are many new developments in feature selection, for example, [1] and [2]. If so, the effectiveness of the proposed method will be relatively convincing. Comparing Table 2 with Figure 3, it is observed that the performance of SOFT is not so stable as other methods, not only on L.Cancer as the author(s) pointed out. Thanks.Additionally, this paper is generally well written, but some places (some issues) in this paper should be further clarified/fixed. Which one did the author(s) of this paper use?<|endoftext|>This paper proposes a two stage second order unsupervised feature selection via knowledge contrastive distillation model that incorporates the second order covariance matrix with the first order data matrix for unsupervised feature selection. A sparse attention matrix thatcan represent second order relations between features is learned in the first stage, a relational graph based on the learned attention matrix is learned to perform graph segmentation for feature selection. The proposed method is interesting;2. Some presentations are not clear enough. 1.2 Why the masked matrix can be defined by Eq.(2)? Interesting idea, but some unclear presentations and motivations.<|endoftext|>The authors propose a two step unsupervised feature selection algorithm. Although the computational cost, both spatial and temporal, is very high (a correlation matrix is used), the idea of searching for strong feature correlations is interesting. However, I have some concerns regarding the contribution:1. Some of the decisions are not clearly stated. For instance: In the graph construction, why the authors delete the las 10% features? 2.The same idea should be applied to the pseudo label generation and the evaluation metrics. I expect to see how this algorithm behaves when using a more complex approach than kmeans, like DEC [1] or IMSAT [2], for instance. 3.Regarding to the problem of the accuracy drop whenever they increase the selected features (20% or higher), I wonder if there could be a different graph segmentation algorithm that could prevent it. In International conference on machine learning (pp.1558 1567). PMLR.Overall, I think it is an interesting idea with promising results, but more experiments have to be done to clearly state the performance of the algorithm, as well as a clear reasoning behing all the decisions the authors made in it.<|endoftext|>This paper considers the well defined feature selection problem. To tackle this, the authors explored the second order feature covariance matrix and proposed a two stage framework including feature correlation matrix via knowledge contrastive distillation and feature selection on the masked correlation matrix via graph segmentation. Extensive experiments demonstrated the effectiveness of the proposed methods. 2.The graph segmentation based framework is novel and interesting to me, which is different from the mainstream weight based feature selection. I have a minor question that will be posted in the Cons section. The authors compared with 10 methods including several recent deep methods on 12 datasets and demonstrated the significant improvements. The traditional unsupervised feature selection methods based on the first order feature matrix usually employs a clustering method to purse pseudo labels for feature selection. It is suggested that the authors would like to do another ablation study without knowledge contrastive distillation. 2.The motivation of this paper is to address the redundancy issue of selected features.
Reject; rating score: 1; rating score: 1; rating score: 1; rating score: 6; Motivation for this type of work is lacking. Hypothesis 1 is known, this is not a new hypothesis. It is not clear what is learned from H1. What does this dataset have to do with the benchmark dataset used? Proposed architecture is not novel. It is a modified version of VGG Face. It is also not clear what these modifications are. Only using Chinese and German faces is bias in itself. How can it be defined here? The paper makes claims of debiasing "beauty" predictions.<|endoftext|>The paper proposes a method to build an unbiased CNN for facial beauty prediction. There is severe anonymity issue in this work. Please see  ethics concern   for more details. I m rejecting the paper because of breach of anonymity. Moreover, the work lacks novelty.<|endoftext|>In this paper, the authors study the problem of bias in facial beauty prediction problem. Finally, they propose two solutions for addressing such bias. Weaknesses:  The most critical issue with the paper is the novelty. "to prove whether hypothesis 1 is true"  > "Prove" is a very strong word. I would suggest "support". 2"  > Please be consistent. The paper just confirms existing findings and does not introduce any novel method.<|endoftext|>This paper proposes an AestheticNet and a new approach to bias free machine learning tools. The proposed network has not been elaborated and lacks some ablation experiments. The latter helps to train an unbiased network with biased data for facial beauty prediction. However, since the faces of different subsets keep changing, how to ensure that the actual attractivenesses of these different subsets have similar unbiased scores?
Reject; rating score: 3; rating score: 3; rating score: 6; The paper discusses learning in biological networks, and proposes a model that combines spiking networks, local connections (convolutions without weight sharing) and reward modulated spike timing dependent plasticity. The model is tested on MNIST and on a classical conditioning task. **Recommendation**Due to limited novelty and unsatisfying results, I would recommend rejecting the paper. that all use backprop. > The proposed network is … the first locally connected SNN with a hidden layerThat’s not true. (This paper tries to do that too with SVM, however.) The main result of the paper   MNIST accuracy (Tab.3)   is very weak. 3.Local connections make it harder. I would test performance with the same architecture, but using convolutions.<|endoftext|>The authors proposed an SNN model, dubbed BioLCNet, for image classification. The proposed network consists of three layers: 1) input layer, accepting spike trains as input;  2) locally connected (LC) hidden layer; 3) decoding fully connected (FC) output layer. The LC layer is for feature extraction and is trained first using STDP in an unsupervised manner. The proposed SNN is validated with the MNIST dataset to show its effectiveness. **Strengths:**The manuscript proposed a training scheme mixed between STDP and R STDP and proposed reward functions for training the 3 layer SNN with R STDP. The validation is only limited to MNIST but with worse performance than other wildly used SNN approaches. From my point of view, it is too limited and it is not convincing enough in the effectiveness of the proposed method. As an SNN researcher, I did not really see the significant value. But, my rating is not firmed at the moment.<|endoftext|>This paper presents a spiking neural network (SNN) architecture forimage classification, designed to be more biologically plausible thancomparable existing architectures. In particular, it eschewsconvolutional connectivity in favour of local connectivity, and usesSTDP (vanilla and reward modulated) instead of gradient basedlearning. The Introduction section ends with this sentence: *"The  proposed network is the first to employ reinforcement learning with  Poisson rate coded inputs for image recognition and the first  locally connected SNN with a hidden layer. I haven t searched  more, but this should suffice to show that the claim may not even be  true (I d like for the authors to comment on this). For instance, *"in many learning problems, we do not have  direct access to the explicit label of the data. On page 3, please clarify in what sense *"Spike timing dependent  plasticity is a type of biological Hebbian learning rule that is  also aligned with human intuition"*.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; I agree that CIFAR10 dataset is more difficult than MNIST, however, we use a much larger adversarial budget on MNIST (0.3) than CIFAR10 (8/255). Some findings are already included in previous works and is not surprising. However, I have the following concerns regarding the correctness, rigor, novelty, presentation of the paper. Contribution of this paper is limited.<|endoftext|>That said, for the results on adversarial training mitigating overfitting of noisy labels, I m not sure I fully grasp what the key new conceptual insights are relative to prior work as I wasn t particularly convinced by the definition of or the experiments on the "smoothing effect" of adversarial training. I found the description of the "smoothing effect" vague, but more importantly, I wasn t very convinced that the experiments in the smoothing effect section were sufficiently meaningful. Certainly the 2D synthetic example is too limited in scope to draw major conclusions. The experiments on geometry value were interesting, and the paper asks an important question in trying to relate label noise and adversarial robustness.<|endoftext|>**Weaknesses:** The main weakness of this paper is the lack of sufficient evidence to support the claims and the use of _PGD step number_. I understand the intuition from the decision boundary perspective, but I do not think only CIFAR 10 experiments in a very specific setting (pair flipping noise) are enough to support all these claims. what are the limitations? I think more than half of the main body is giving background; the paper could be written such that the contributions are more richened and highlighted. Although the use of _PGD step number_ seems promising, there is only very limited evidence to support the claims in the paper.<|endoftext|>The contribution of this paper is two fold. The proposed method is intuitive yet effective. I am wondering if there is a confidence value or statistics number to measure the distributional difference? An interesting connection is studied and the contribution is clear. But the work lacks theoretical insight and quantitative results.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper proposed a new text to image generation method that would like to tackle the entangled textual inputs by multi tailed word level initial generation, create the region contextualized text representations for region aware image refinement, and propose an iterative multi headed mechanism to allow multiple different changes at each stage of refinement. Entangled sentence level representation and region/semantic insensitive image refinement are two noticeable issues in the text to image generation task. The disentanglement of word level attributes at the level of generating a small size image may not be a necessity, in my opinion. Word level features can be modulated to the visual features in many ways other than disentangling the sentence level input. Moreover, the performance of disentanglement was not fully validated, the proposed MTWIG looks like redundant representations of the sentence level features, rather than ``disentangled  representations. 2.The spatial dynamic memory is an extension of DM GAN with more region contextualization, its design may just be one of the choices to enable region awareness, but may not be the only way to fulfill it.<|endoftext|>The paper proposed a new method to tackle text to image generation challenge. In the paper, authors introduced a potential problem that current methods only use sentence embedding at beginning of the network to generate initial images, where different attributes may be entangled and are hard to be refined during following states. Then, these methods utilise word level information to improve the details. It is better if authors can add experiments to support this assumption by verifying the existence of entanglement and the need to use word level embedding at the beginning. Also, the following fusion operation that combines these different features into single image features. 3.The proposed spatial dynamic memory module is based on the memory module introduced in DM GAN, and the main difference shown by authors is the utilization of region contextualized representations. Could we achieve similar performance by adjusting the size of convolution operator in pixel level representation? 4.I am confused about the FID scores of DF GAN (33.29 for COCO) reported in the paper, which is different from the value claimed in the DF GAN paper (21.42 for COCO). The current version of paper may need experiments to support claims shown in the paper.<|endoftext|>This paper is motivated by that most existing text to image methods suffer from three limitations and solutions are proposed to address the different limitations. Firstly, it introduces multi tailed word level initial generation to enhance global sentence representation with distinct n gram word presentation. Second, the spatial dynamic memory module is proposed to create a separate region contextualized text representation for each image region. Finally, it introduces an iterative multi headed mechanism to make multiple distinct modifications to the prior image features. As shown in Table 4(A.2), the MTWIG(ks 1) is better than MTWIG(ks 2) in terms od FID score in the CUB dataset, but MTWIG(ks 3) is better than MTWIG(ks 1). Thus, we cannot convincingly ensure the effectiveness of different components by fairly comparing the proposed method with the SOTA methods. Authors should consider performing an ablation study on COCO to verify the effectiveness of different modules. This paper has a clear description of the found three problems, but the ablation studies are not sufficient. From the experiment, it can be seen that the stability of the model is poor. And the consideration of the performance reproducibility of training complex models with huge parameters is also very important.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; WEAKNESSES:  Figures have text that is difficult to read (text in figures should match the font that is used in the paper s body, in style and size). Following the suggested evaluation of the W space of StyleGAN2 reveals that the W space is distorted at the global level: the global method, thus, limits global consistency while in W space. Overall, they did well in this work and are high in potential. The idea is interesting and relevant.<|endoftext|>That being said, the paper is well written and the empirical results are interesting, hence the borderline/reject score. It isn t clear to me what the authors aim to achieve in this part of the paper.<|endoftext|>The idea is simple. In this work, the authors show that the principal subspace of the Jacobian may fail to capture disentangled attribute directions and result in artifacts. Low Rank Subspaces in GANshttps://arxiv.org/abs/2106.04488In this paper, the authors reviewed the related work using the Jacobians. The authors harness the Jacobian regarding the mapping from $z$ to $w$.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper focuses on an interesting and challenging setting in federated learning, where number of classes and samples are imbalanced across different local workers. Utilizes a primal dual optimization mechanism to solve the problem, provides theoretical guarantees on convergence;3. The problem is clearly stated and the algorithm is well organised, theoretical analysis is provided;2. The problem itself is interesting, non iid and imbalance is a key challenge in federated learning and real world setting;3. The experiments are mostly sound and comprehensive. More citations on non iid/imbalanced federated learning should be cited or taken into consideration as baselines. In the paper, the authors stated that "in the extreme case where every device has only one class, these methods have inferior performance than the classic Fed Avg method," it would be good if the authors can provide the results in appendix. [1] Tackling the objective inconsistency problem in heterogeneous federated optimization. [3] On the convergence of fedavg on non iid data.<|endoftext|>The paper describes an agnostic constraint learning method for handling class imbalance in a federated learning setting. However, there are 3 suggestions to improve the paper: (1) Use of more complex datasets, (2) Comparison with other FL algorithms mentioned in the paper, even under additional settings such as active learning, MAB etc. Finally, it would be interesting to see how variation in alpha affects model performance. It is also not clear on how training and deployment of the algorithm work in practice. The paper provides good theoretical and empirical justification of a unique problem of addressing extreme local imbalance in a federated data system while maintaining data privacy.<|endoftext|>The authors design a method, CLIMB, to solve the severe class imbalance issues in FL problem. In their method, they propose a constrained FL formulation (CFL) and adopt the method of Lagrange multipliers to solve this problem. They present some theoretical and experimental results to prove that their method is effective. Strengths: 1. Their method is agnostic to class distribution of the client data, which satisfies the privacy requirements of FL. 2.There is no need for active client selection in their method which is more practical than other similar work. My first concern is about the Assumption 3.4. 2.I am quite confused with the Theorem 3.2. I think the authors should provide some theoretical analysis to explain this phenomenon. “they will a higher probability to” >"there will ..."Although reducing the class imbalance in FL is an interesting topic and the method in this paper is new, I still think their method are not well supported by theories and experiments according to the above discussion. Therefore, I think it is marginally below the acceptance threshold and needs further improvements.<|endoftext|>The paper proposes an interesting new federated learning approach called CLIMB that leverages a special formulation of the FL problem as optimization with constraints. This certainly improves the fairness of the trained model, but it does not help when data is imbalanced. Therefore, a method that looks for a fair model (similar performance for each client) naturally also improves for imbalanced classes. Moreover, the approach seems not to handle class imbalance in any way if the data is not distributed in such a particular way. The second concern is about privacy. The approach seems to require sending lambda values for each client to the server, which are directly related to the difference between average performance among the clients and the client s performance. Have the authors considered this issue? In general, the paper is well written, and the algorithm presentation is clear.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposed a method to learn with PU data which quantifies the epistemic uncertainty of an ensemble of networks and selects which examples to pseudo label based on their predictive uncertainty. The authors propose to use the pseudo labeling technique based on the uncertainty of the prediction, combined with early stopping. It is well known that many important PU learning methods are not two step, such as nnPU. Second, I don t understand the meaning of designing an algorithm like this. And it labels the instance according to the epistemic uncertainty (Eq.4 6).<|endoftext|>This paper proposes PUUPL, an uncertainty aware pseudo label selection method for positive unlabeled (PU) learning. However, it is not clear the technical novelty and it lacks a simple baseline against uncertainty aware pseudo labeling. It seems that simply applying uncertainty aware pseudo labeling technique results in the proposed method. That is, the technical novelty of the proposed method seems not strong. If the authors can clarify non trivial technical contributions, it would help understand this paper well. This paper introduces uncertainty aware pseudo labeling techniques to PU learning.<|endoftext|>This paper studies the PU learning problem. It proposes a two step approach that can estimate the pseudo label uncertainty so that more reliable pseudo labels can be assigned, which improves the predictive performance. Without the two assumptions, the proposed method would have problems. I understand that the proposed approach is different and probably more advanced than those older approaches, but it will be more complete to have an experimental comparison with some earlier approaches to show the advantage of the proposed method.<|endoftext|>The authors proposed to use pseudo labels based on high confidence predictions to improve the classification performance of PU learning. Strengths+ A simple uncertainty aware pseudo labeling framework for PU learning is proposed. + This paper is easy to follow and the logic is clear. Weaknesses+ The novelty of this paper may not be enough. Specifically, the pseudo labeling techniques have been well studied in a lot of weakly surprised learning scenarios such as label noise learning and semi supervised learning. I think it is better to add more. However, I think the technical novelty is marginally significant, and the motivation of the proposed metric may not be clear.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; One major concern I have is that even if we assume reward is a linear function of features, the resulting value function is not hindered and in fact the expressiveness of value function is not limited by the linearity, as features can be an expressive and arbitrarily non linear function of observations. But this seems not the focus of this work. The authors also conducted experiments on pixel based Doom environment and show second order SF is able to linear SF. However, I am wondering if feature learning of the linear SF baseline is well tuned and have a fair comparison. The exploration idea is interesting but seems incomplete. To support the claim, the paper would benefit from comparison with related methods on “state features” and “noisy input” e.g.random network distillation, active pretraining with successor features, and noisy network for exploration.<|endoftext|>I also encourage the authors to compare against the algorithm proposed in [1] although the focus is on improving exploration with SF. The paper provides a new idea for deep reinforcement learning with successor features (SF). To reduce the burden of the encoder for learning meaningful state features, this paper extends the linear framework by proposing an additional quadratic term in the reward model, leading to an SF framework with 2nd order rewards. It would be interesting to see transfer results on Doom. + 3.c.Section 4.6 guided exploration is not clear to me (see 1.b for more comments). "Count based exploration with the successor representation." My major concerns are about (1) the clarity of the paper, and (2) the significance of the proposed method, and I suggest the authors include additional baselines and experiments in the rebuttal period (see concerns above for details). Therefore, I don’t think the paper can be accepted with the current version and vote for weak reject.<|endoftext|>The authors take the successor feature framework that separately models enivronmental dynamics and reward to use a second order reward learning formulation over the standard linear model. Weaknesses   The secondary and tertiary contributions of the paper (modelling the auto correlation matrix of state features and using the second term for guided exploration) seems underdeveloped / incomplete. Standard RL model free baselines would be interesting to see on the experimental setup i.e.where dynamics and reward modelling are not  decoupled. Paper is well written and a natural extension to the linear formulation of successor features. I wonder if the paper would benefit more from more significant transfer learning experiments uitlizing the second order model while relegating the auto correlation matrix / alternative to epsilon exploration to the appendix / future work.<|endoftext|>The paper describes an extension to the successor features frameworkwhere the rewards are modelled using a second order function. Under this formulation a second term appears that models the expectedauto correlation matrix of the state features and can be used forguided exploration during transfer. The authors provide experimental results is three domains withcomparison with the linear approach. The proposed approach requires extra effort and parameters. Pros:  a new formulation with provides a more robust reward component  the approach can be used for exploration during transfer  a clear improvement over the linear approach with less parametersCons:  it can be seen as an incremental work  the experiments do not reflect, under the same conditions, the extraeffort needed with the second order rewards
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces hierarchical reinforcement learning (HRL) into automatic disease diagnosis, which reduces the action space and improves training efficiency. Besides, the authors also expand an existing public dataset and build a synthetic dataset for evaluation. The Experimental results show that their proposed hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis than existing several baselines. 2.The paper is well organized and easy to understandWeaknesses:1. My main concern is the technical novelty of this work. 3.It would be helpful if the authors could provide some examples to understand how their systems behave. This work focuses on a meaningful task and constructs datasets for better evaluation, but the limited contribution at the methodological level makes me inclined to think that it has not yet reached the bar for a top conference.<|endoftext|>##########################################################################Summary:This works proposes dialogue policy learning by incorporating a hierarchical policy. The low models have several symptoms checkers and disease classifier. Experiments are conducted on both real and synthetic datasets to demonstrate the efficacy of the proposed work. The paper takes one of the most important task, i.e., automatic disease diagnosis. That is, it does not consider the NLU and NLg modules of the typical goal oriented dialog systems and the paper is not clear about it. 2.Experiments are provided to show that the proposed methods work well. 3.The proposes a nice application of the hierarchical reinforcement learning. However, there exists several works in RL where action space is huge, way bigger than the proposed work. I like the application side of the paper. But, technically, I am unable to see significant contribution.<|endoftext|>This paper applies Hierarchical Reinforcement Learning (HRL) to automatic disease diagnosis in task oriented dialogues setting. The authors argue that applying RL to automatic disease diagnosis is challenging because the action space (i.e., symptoms) is very large. Therefore, they propose to learn a hierarchical dialogue policy where the high level policy is for categorizing patents into different groups and the low level policy is for checking symptoms and classifying diseases within a group. Pros:  This paper is well organized and easy to follow. Therefore, the technical contribution of the work is weak. Regarding the experimental results, I found that the proposed method requires significantly higher *Averaged Turns* for disease diagnosis compared to other baselines (as shown in Table 2). I don t understand why the authors bold the highest *Averaged Turns* in Table 2. But in task oriented dialogues, accomplishing a user goal with fewer turns is better. I have some concerns about experimental results (see Main Review) which need to be clarified in the author s response.<|endoftext|>The paper tackles the problem of automatic disease diagnosis through reinforcement learning under a setting of task oriented dialogues. The authors proposed to integrate hierarchical policies with two levels (one high level master model, and one low level policy) into the dialogue policy learning. Experiments on both real life and synthetic data suggest the proposed approach is effective. The proposed HRL approach achieves better performance compared to the existing baselines. 2.New synthetic data was proposed for the task and can be useful for future study. It seems from Table 2, the existing best baseline is FIT, which is a non RL approach, so it s not clear to me why RL is necessary to solve the automatic disease diagnosis. Is average turns the higher the better? It seems it s better if the diagnosis can be solved in reasonable amount of time instead of the longer the better?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; rating score: 5; They question if this is a fundamental limitation (i.e.these very sparse solutions don t exist), or if this is a limitation of current methods in finding such solutions. None of the strong ticket methods are able to find the "planted" tickets for any sparsity level, while the weak methods are able to find tickets, but not at extreme sparsity. Further analysis reveals layer collapse in particular to be a problem. The authors conclude that rather than a fundamental limitation, current methods (in particular for strong tickets) are limited in finding very sparse tickets even if they are known to exist at that sparsity level. Proposed idea is, as far as I m aware, novel when applied to sparse neural networks. It is complementary to the many papers that explore this topic in real models/datasets empirically. The paper presents a novel take on the ability to find good sparse subnetworks before and after training, with the results suggesting that the state of the art methods for finding "strong" (before training) subnetworks do not perform as well as those for finding "weak" subnetworks even in for these toy problems — results matching the conclusion of other recent works in the field, but from a very different angle. Common wisdom for the lack of good results at high sparsity is, I believe, that at extremely high sparsity, there are just not enough weights to solve the problem. However this work shows that in cases where we know there are extremely sparse solutions, and we have enough weights, these solutions are still not found; it seems in part due to layer collapse. This result is actionable, and provides an interesting research direction for improving sparsity in neural networks, can we find methods that better avoid mode collapse and are able to discover these "planted" tickets, and does this generalize to real world problems? For example, "for which we lack ground truth information" is very ambiguous, this should read, "for which we lack knowledge of ground truth solution subnetworks" or something similar. This work shares some similarity with "Large Automatic Learning, rule extraction and generalization", Denker et al., 1987. The solutions were compared with ground truth human proposed solutions much like in this work. In particular the ability of DST methods to potentially avoid layer collapse would make them an intestine avenue to being explored (just as the authors explored iterative methods). This is a major weakness in the evaluation in my opinion. The authors should also cite previous work looking at training NNs on toy problems where the solutions are known (not necessarily the above reference, it s just the first that came to mind). There is very little space in the main paper dedicated to the results, with the results only being presented in 3 figures that are relatively small, although there are a lot more results in the appendices. The paper asks fundamental questions on the ability of contemporary methods for finding sparse subnetworks both before training, with an interesting experimental setup, and finds results (in the toy problem setting) that suggest these methods are lacking. Layer collapse specifically is identified as one of the problems leading to this issue, and the paper is somewhat convincing that the problem of finding very sparse solutions is algorithmic rather than a fundamental limitation. Nevertheless, overall it remains interesting and relevant overall, and I recommend its acceptance   although I encourage the authors to revise the abstract.<|endoftext|>This paper proves the existence of strong lottery tickets and further develops a framework to plant and hide winning lottery tickets with desirable properties in randomly initialized networks to help analyze the ability of state of the art pruning methods for identifying tickets of extreme sparsity. This paper provides a framework for benchmarking different pruning methods on their abilities to identify strong/weak lottery tickets, which can provide rich insights for the lottery ticket community. 2.The proposed framework is driven by theoretical analysis. Can the framework still plant near optimal solutions and provide useful insights in these cases? Although the paper analyzes some general trends, it s not clear whether such observations can be consistently scaled up to large scale datasets. 2.The paper is not well written and the logical flow can be improved. For example, it s not clear the "lottery ticket" in Sec.2 denotes strong or weak lottery tickets, leading to many confusions. The wording in Sec.2.1/2.2 can be improved with more structural logic. In addition, the title of Sec.2 is not accurate as "existence of lottery tickets" has been discussed in the first lottery ticket work. 3.Other questions:   For the problems proposed in Sec.2.3, are there any references? It s not clear why they can "reflect typical machine learning algorithms" as described in the contributions. In addition, a recent work [1] also shows that fine tuning the identified tickets can achieve better results than retrained lottery tickets or rewinded lottery tickets, which also aligns with the common practice in network pruning. ", N. Liu et al., ICML 21. Given the concerns about the practical usage of the proposed framework, I tend to deem this paper marginally below the acceptance threshold. I m willing to adjust my scores if the concerns are addressed.<|endoftext|>The authors note a distinction between kinds of sparse networks in the literature. "Weak tickets" require training to perform comparably to the original network, while "strong tickets" do not. They evaluate common pruning methods to find both weak and strong tickets using these tasks, and find that most methods perform well on 2 out of the three weak ticket finding tasks. They also note that the only method designed specifically for strong ticket finding performs well on these tasks. For further evaluation, they additionally share weak and strong ticket finding results for VGG 16. These both are useful contributions. From a practical, empirical perspective, I do have concerns about the impact of this result, as the authors note that strong networks are currently still difficult to find. Moreover, I also have some concerns that the outcome of using ground truth strong networks largely confirms prior work (Frankle et al., 2021, Ramanujan et al.2020) and yet relies on much smaller networks for evaluation than the networks evaluated in prior work. The VGG 16 task is presented as a bit of an afterthought and is not central to the experiments, so I am uncertain how this planting algorithm can be scaled to similarly larger tasks. At the same time, I share the author s hope that using ground truth strong networks will spur new methods for the finding of strong, sparse networks. First, the inclusion of Zhou et al 2019, would provide a fuller picture of the history of research into strong lottery tickets, as Ramanujan et al.2020 write that their work was inspired by the results shown in Zhou et al 2019. These markers are too large for the current "confidence" metric, which is not explicitly described in the paper. Organizationally, information from A2, A3, and B1 were most helpful to me personally in understanding the paper. I believe that content from these sections would better help the reader if they were presented in the main sections of the paper.<|endoftext|>Their empirical results of three common challenges are in line with the previous findings in Frankle2021. ##########################################################################Pros: (1) The paper gives a lower bound of strong winning tickets with the same depth as the target network, which reduces the dependence on the larger model depth. (2) The winning ticket planting is quite interesting and can provide ground truth for pruning before training. How was the target network selected? A further experiment to validate the effectiveness of winning ticket planting is required. What s more, as shown in "Stabilizing the Lottery Ticket Hypothesis", the initialization might not be enough to guarantee the matching performance. If this planted ticket can not match the full accuracy, it is reasonable that various pruning methods e.g., SNIP, GraSP, achieve unsatisfied performance. Will the accuracy of various pruning methods remain similar or not? This will help us understand the role of ticket planting better. (3) It s not clear the novelty of this work to me. Is the bound provided tighter than the existing works? I suppose that the matching between the target ticket after training and the initial network is difficult. ## After rebuttalThanks a lot for the response! After reading the response, the motivation and contribution are clearer to me. I believe more elaborate sentences/figures/diagrams are necessary to help explain the whole process of ticket planting, as the core contribution of the paper. I notice that I am not the only one who is confused by this planting process. I encourage the authors to add the empirical results to support the effectiveness of ticket planting as well. Overall, I decided to increase my grading to 5. The motivation and the correctness of the lottery ticket planting are not clear to me. And the empirical findings in this paper are already presented in the previous paper Frankle2021.<|endoftext|>This paper argues that one reason evaluating the strong Lottery Ticket Hypothesis is difficult is the lack of ground truth tickets. They circumvent this by embedding a winning ticket in the weights at initialization and evaluating how well different methods can recover it. I think the idea of embedding hidden tickets inside a network to evaluate lottery ticket hypothesis is interesting, but this paper is let down by the mismatch between the proposed method and experiments section:The derived lower bound applies to strong lottery ticket hypothesis but the experiments section mostly focuses on pruning at initializaton methods such as SNIP, GraSP, and SynFlow that were clearly designed for a different (e.g.weak LTH) setup. Given that one of the claimed aims of this paper is to analyze the ability of state of the art pruning methods in finding tickets, the authors should elaborate on how the insights from their approach are transferable to these methods. The authors do note this gap but claim that the insights are similar to known trends in image classification and therefore the insights are transferable. However, I m not sure how this setup can be used to make any new claims about pruning at init methods. The authors claim they construct tickets for tasks that represent "typical machine learning problems" but these tasks are not typically used in the LTH literature. It would be great if authors discuss how these are relevant to the image classification task that LTH literature mainly targets. ### Lower bound on existence probability  The authors prove the lower bound for MLPs but say it can also be applied to convolutions. Given that these are the most commonly used architectures, the authors should discuss in more detail how the proof applies to these models. Could you cite some of the references where this is common? I believe the original paper by Ramanujan did include the last layer (and BN layers). ### Planting the ticket  This seems to be a core idea of the paper but I don t think it s well described in the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The proposed method first augments the width of the tiny network to make the tiny network a bit larger. Then the gradients from the larger network are used as additional supervision. [Strengths]1) The authors show that large models benefit from data augmentation and dropout while tiny models suffer from these regularizations/augmentations. While there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. The proposed method, NetAug, is effective and simple to implement. It also works well with other techniques such as knowledge distillation and pruning. There are existing works sharing a similar idea of NetAug for large model training, which slightly hurts the novelty of this work.<|endoftext|>This paper proposes Network Augmentation (NetAug), which randomly augments (enlarges) neural networks during training to solve the under fitting issue. \S3.The proposed method is simple, so it has broad applicability. \S4.The proposed method shows consistent improvements in a variety of settings and high compatibility with other frameworks, including knowledge distillation and network pruning. \I agree with the under fitting issue in training tiny neural networks. However, even though augmented networks are larger than the target tiny networks, the capacity of the target network is fixed (i.e., constant). I wonder why and how the proposed method can find a better optimum. \There are several simple techniques to avoid under fitting. ### Minor comments  The citation formats in tables are not consistent with that in the main body.<|endoftext|>This paper proposed to train and augment a tiny network by incorporating it into the larger networks with weight sharing training mechanism. The tiny neural network is learned with the auxiliary/additional supervisions from the larger models that wrap it. With this training strategy, the tiny model can perform better than the conventional training scheme on ImageNet and several downstream tasks. Generally, I like the idea proposed in this paper. **Some of my other concerns are as follows:*****About novelty:***As I mentioned above, the overall idea and implementation strategy are both similar to the weight sharing mechanism. I think it s better for the authors to focus on the proposed method itself in the experiments.<|endoftext|>To improve the performance of compact neural networks with limited model capacity, the paper proposed Network Augmentation (NetAug) which addresses the under fitting problem in small neural networks. This is accomplished by incorporating the target tiny neural network into bigger neural networks for additional training supervision. The paper introduced a training mechanism that includes auxiliary forward flow and supervision from the very training of the very large model into the training of the tiny nets, this training mechanism improves the classification and object detection performance of tiny nets on several large datasets. I suspect that an equivalent sub network of the larger net will have similar results in comparing to the performance of the tiny network solely training on the large datasets (ImageNet, Pascal VOC etc).
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper aims to accelerate HPO optimization by allowing for retraining when new hyperparameters are proposed, while being faster and more flexible that existing hypergradient based one pass methods. This results in a novel HPO one pass technique that works with any continuous hyperparameter within a differentiable model weight update, such as learning rates. It also fits the work well in the context of related work both on standard and one pass HPO techniques (e.g., Lorraine et al.(2019)).**Reproducibility**Not only do the authors carefully provide details on the baselines, benchmarks, dataset sources, compute machines used and experimental setup, but they will also open source their code to reproduce the experiments upon acceptance. While the authors have linked to an anonymous repository, code is not visible yet. One weakness of the proposed approach is that it relies on a set of meta hyperparameters, which the authors acknowledge. Ideally, their setting should be make automatic. Is tuning on validation set required, or do they recommend some default values / can they give guidance on how to set them? The authors hint to learning rate regularisation for the high dimensional dynamics setting, but some early results on that front would have further strengthened the paper. I recommend the authors to double check those and update accordingly.<|endoftext|>This paper is concerned with tuning continuous hyperparameters of a neural network setup (model, loss function and optimizer parameters) by way of a cheap approximation to the hypergradient. It is closely related to earlier work (Lorraine, 2019), which however overlooked that the idea can be generalized to optimizer parameters (e.g., learning rate, momentum), which is effectively done here. But I feel they do not do much more. Code is provided. This paper starts from (Lorraine, 2019) and that it can be generalized in a rather straightforward manner. Crucially, this generalization allows to tune optimizer parameters as well, such as learning rate and momentum. The paper certainly excels at fixing issues with (Lorraine, 2019). Only 3 HPs are optimized over, 2 of which are optimizer parameters which the other method cannot tune, and the authors just leave them fixed. Most importantly, how do you ensure robust behaviour, and in particular avoid catastrophic failures which hypergradient approximate methods are prone to? Can you underline this with some analysis and experiments?<|endoftext|>This paper tackles the hyperparameter optimization problem with a one pass approach that alternatively optimizes over machine learning model parameters and hyperparameters. The authors are candid about the extent of their contribution and properly cited relevant literature. The writing is overall clear. I would raise my score if the authors could properly address the questions, and would suggest the authors incorporate them into the revision. As (honestly) claimed in Section C.4, the technical advancement from Lorraine et al.(2019) seems incremental: In the framework of Lorraine et al.(2019), $\mathcal{L}_T(\lambda, w)$ could still be parameterized by optimization hyperparameters in the framework of the previous work, so that $u(\lambda, w)$ could be expressed as a closed form. The mean and median performance in Table 1 and 2 may not be quite meaningful in the hyperparameter tuning context: for the methods that do not optimize over optimization hyperparameters, “Best” instead of “Mean” or “Median” may be more interesting to practitioners.<|endoftext|>Authors propose an extension of Lorraine 2019 to handle hyperparameters that control the convergence speed of the model: Lorraine 2019 "only" handled hyperparameters that were part of the model. The extension seems a little incremental, but the paper is well written (except for the theoretical part), code is provided, the bibliography is quite extensive, and experiments show interesting improvements. What do authors mean by the approximation sign in (7)? 4  Figure shows that the learning rate seems to converge toward a value around $1O^ 1$, but the weight decay does not seem to converge. 6  In the experiments I would like to see the exact formula of the optimization problem which is solved (either in the main paper or in appendix). In my opinion this would make the paper clearer. In particular, IMO, the name of the paper should be changed. However, except this part, the paper is well written, and experiments are very interesting.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The rationale is that tuning hyper parameters on the entire set of tasks off line invalidates the CL assumption of online learning, and therefore online methods are needed. The motivation for the problem of hyper parameter tuning in a continual learning (CL) setting is strong and appealing, and has been understudied in the literature2. I suggest placing this caveat much earlier in the paper and discuss the implications of this choice. However, some of the wording in Sec 3 seems to suggest that this is not the case, and the test memory used for RL training uses _additional_ memory that the base methods are not allowed to use. I wonder how specific the approach is to replay. Some of the primary empirical results are inconsistent with the motivation throughout the paper of using RL for hyper parameter tuning############## Arguments ##############The problem setting itself studied in the submission is fairly novel: how to tune the hyper parameters in a CL setting without "cheating" and looking at future tasks while tuning the hyper parameters. The authors then adequately motivate the use of RL based search as a tool for hyper parameter tuning, explaining that a replay "test" memory can be used as a proxy for CL performance. It is also never stated what the exploration strategy is for DQN. Unfortunately, I recommend the rejection of this paper. I wonder if models typically forget more easily test samples vs training samples (e.g., if training accuracy usually decays differently than test accuracy). I do strongly encourage the authors to continue improving upon their work, as I believe that it could be highly impactful. For all these positive factors, unfortunately the paper has some fundamental flaws that would need to be addressed prior to being ready for publication. I would encourage the authors to include a discussion about this in a later revision of their work, though I did not think the lack of it was a major shortcoming of the submission. The first one is the incorrect use of the term class incremental learning as a descriptor for their method. This conclusion is further validated by the fact that ER and SR with a fixed value of 3 replay iterations outperform the RL based method. And this goes against the proposed method for using task specific Q functions. If so, why?<|endoftext|>The authors present an approach for continual learning based on replay. By finding a way to represent hyperparameters of the replay learning process, specifically the replay ratio and the amount of replay iterations, the authors use a reinforcement learning approach to attempt to find the optimal parameters for each epoch. On the positive side, the approach presented for the control of the parameters is well described. The datasets used and the selection of baselines is in line with the state of the art in this problem area. On the other hand, the presented approach is built off of ER or SCR as the backbone and using DQN to perform the required reinforcement learning. So the theoretical contribution is mostly centered in the formulation of the reward signal which is then solved in a known way. The reward function under test, in this case, is also not particularly sophisticated either, boiling down to measuring the difference in total risk from one time step to another on a random holdout set. Alternatively, other additional ways to encapsulate the risk might have been good to test to characterize how this RL technique is affected by these choices or bringing in a more elaborate method of choosing the replay set would have been possible. Finally, unless I am mistaken in my reading of Table 2, but it appears that on cifar when multiple iterations are used this method does no better than plain SCR and shows almost no difference with the ER method if multiple iterations are allowed. While the paper was well written and the ideas made sense as presented, however, my concerns with the amount of theoretical novelty combined with an empirical situation in which I have some questions is going to make my recommendation at 5, marginally below threshold.<|endoftext|>In this paper, the authors use reinforcement learning (specifically Q learning) to choose certain hyper parameters of replay for online continual learning. These hyper parameters are chosen online using a DQN network as it sees new data, continuously adapting the choice of hyper parameters based on a separate test memory. Moreover, from the results of Fig.4, it seems like the DQN network seems to go towards a fixed action rather than adaptively change the hyper parameters in a complex way. This indicates that there might be a way to choose these hyper parameters using a much simpler setup. This is the key issue that requires addressing. But the authors go through a big chunk of related work, including some which are not of immediate relevance to the algorithm in the paper. This section can easily be shortened. The empirical evaluation of their method covers quite a lot of ground, and makes a convincing case that their method does have some advantage in terms of performance. Overall, the significance and novelty of the work is not so clear to me, since it seems like it s a straightforward application of an off the shelf RL algorithm for choosing parameters of online continual learning.<|endoftext|>This paper takes a particular continual learning setup and addresses the challenge of choosing two important hyper parameters of standard continual learning approaches based on experience replay: the online to memory loss ratio, and the number of replay iterations. On the other hand, the application of RL isn’t well motivated, and some crucial details are missing that may warrant reevaluation of the proposed methods, such as by achieving a computational parity with competing methods. An additional small test memory is introduced, which is used to construct the reward and states. Another strength of the work is the simplicity of the idea, which is as simple as using reinforcement learning to learn the hyperparameters. In order to formulate a problem with an MDP, there need to be meaningful state to state transitions, and the optimal policy needs to be a function of the state. To understand whether those are present in the adopted continual learning setup, one could take a simplified scenario where the overall process of the continual learning setup is stationary. Even if the optimal choices of the hyper parameters aren’t state dependent, do they have fixed points under some simplified setup? Details of the size of that replay aren’t given. Depending on how much extra computation RL is using, it can have implications on the fairness of the experimental setup. Would it be fair to compare methods with vastly different amounts of computation, especially, if the competing methods are using substantially less computation? When a task is presented, how many samples of that task are presented to the learner at a time? The paper indicates that they are sequential: “The online CL setting is more challenging: … the sampling distribution is non IID”. There could be pseudocode describing this process of interaction, not just the algorithm. There are other works that use a validation set for continual learning such as “Large Scale Incremental Learning” by Wu et al.These should be discussed and compared with the proposed approach.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper focuses on the open set semi supervised learning problem. The authors propose a modification of the classical self learning algorithms. Compared with FixMatch, which selects pseudo label based on a static confidence threshold. This paper proposes to select a threshold based on in distribution validation data. This paper focuses on the open set semi supervised learning algorithm and proposes a new pseudo label selection strategy to improve the robustness of SSL algorithms with open set unlabeled examples. Strength:1) The proposal is simple and clear and the paper is well written. The novelty and contribution of the paper are limited. 2) The authors analyze the proposal with bayesian decision theory. The results are mainly based on Eq.(4) and Eq.(5). However, the proposed algorithm does not optimize Eq.(4) and Eq.(5) directly, it seems that the theory can not demonstrate the effectiveness of the proposal.<|endoftext|>This paper proposes a modified way of doing self training that aims to improve the performance when the unlabelled data is not coming from task related labelled data with artificially removed labels. The authors validate their approach on reduced CIFAR 10, full CIFAR 10, and full CIFAR 100. I really like the goal of this paper. Most SSL and self training methods are only evaluated on a contrived dataset where the unlabelled data is actually just labelled data whose labels are removed. The obvious issue here is that the "unlabelled" data in this setup is actually very similar to the labelled data, even with the same distribution of classes etc. It is of course perfectly fine for a method to not be too novel, as long as it leads to important improvements in empirical results or scientific understanding. I think that the presentation of ODST could be significantly more convincing if it was evaluated on a more challenging benchmark such as imagenet, where noisy student had shown a significant improvement. Currently as a reader I m unsure how much of the improvement over fixmatch could be due to fixmatch not being tuned as well as ODST is, since fixmatch has lots of different hyperparameters that could need retuning when unlabelled data is from a task unrelated source. Since the methodological novelty is limited, the paper could benefit from more comprehensive empirical experiments. If that is not possible, experiments on more datasets such as STL 10 or SVHN could help, as currently the only experimental evidence for ODST is on CIFAR 10/100.<|endoftext|>The paper assumes an open world setting for semi supervised learning approaches. Strengh: The paper considers a novel mixture model for unlabeled data in an open world scenario. W2: Motivation, I doubt that the pseudo labeling strategy is in fact too noisy, owing to the misalignment of classes between in data and out data. The direct consequence is that the pseudo labeling on out of distribution classes (classes not from the same distribution as from the labeled data) might be seriously wrong. Why is OE not compared with the ODST under the same Open World SSL setup? I think OE itself is suitable for such comparisons scenarios. The supporting theorem does not link to the propagation error introduced by such pseudo labeling strategy. Since classes between labeled dataset and unlabeled datasets are significantly different, softmax function is prone to prediction artifact. I insist that the ablation is critical in this paper since the proposed method now is packaged with calibration method, pseudo labeling strategy, student teacher architecture, and the proposed loss Eq.(2).It is currently unclear which factor eventually contributes to the eventual performance improvement. I would increase my scores if the above concerns are clarified.<|endoftext|>This papers studies semi supervised learning with OOD samples appearing in unlabeled data as well as at test time, which is practical in real world tasks. To handle OOD samples, this paper proposes a new framework based on OE and self trainning. Extensive experiments are conducted to verify the superiority of the proposed method. This paper studies an important problem which is underexplored in literature;3. This paper provides analyses on the base classifier based on Bayesian decision theory. 4.The proposed method shows significant performance improvement against several baselines on multiple datasets. The novelty is limited. The proposed method is complex yet not well understood. More ablation studies are needed to better understand the effectiveness of each component. For example, the last term of the loss function in Eq.(3); the choice of $k$ in sample selection; the classifier calibration; the size of in distribution validation set;3. In particular, it needs 1000 epochs of training. Therefore, I recommend rejection.<|endoftext|>The authors propose an out of distribution aware self learning (ODST) framework with a careful sample selection strategy that works well with unlabeled datasets containing a small portion of task relevant data. However, most of the recent works are hinged on a key assumption that the unlabeled data comes from the same distribution as the underlying task or from the same classes. The authors focus on SSL for image classification and consider a more realistic setting where the ratio of non related images in the unlabeled dataset is large. To this end, the authors propose a self training scheme with a careful sample selection strategy. The authors perform top K class based sample selection which has been shown to work well in self training settings in prior work like Uncertainty aware Self training (UST, NeurIPS 2020). The authors provide theoretical justification for this choice followed by ablation studies in the Appendix to evaluate of such regularization and other components of the framework on the overall model performance. Overall, the paper is well written and easy to follow with significant improvements over several baselines in the open world SSL setting. The authors propose an out of distribution aware self training method with significant improvements over several baselines in the open world SSL setting with extensive experiments and ablation studies.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper focuses on learning useful representations of histological patches for classification tasks. Acknowledging the large domain shifts typical of histological imaging (e.g., differences in examined organs/tissues, imaging protocols, staining intensities), the authors frame the classification problem as a few shot learning (FSL) one. In their experiments, they compare FSL pre training on a base dataset both using full supervision and contrastive learning. During low shot training, they make use of an unsupervised approach for data augmentation in the latent space (referred to as “latent augmentation”). 2) The proposed approach, which makes large use of some of the latest development in both FSL and data augmentation strategies, is interesting and convincing. Additionally, I’m not sure that the results shown by Chen & Li (2020) are directly comparable to these, which are relative to the same comparison but performed in an out of domain (i.e., previously unseen class) setting. I would appreciate a more detailed discussion of this comparison. Typos (page 5): “samples belong”, “we summarizes”2. Typos (page 9): “Later methods is attracted to transfer learning”, “This method is later extended by not relying given base samples”5. While the novelty might be relatively limited, the comparison between fully supervised and contrastive pre training in an FSL setting (with and without latent augmentations) for histopathological images is interesting.<|endoftext|>So far, contrastive learning has been studied for natural images, which differ from medical images. The paper shows that contrastive pre training provides a significant performance boost (~ +10% F1 score) compared to standard supervised pre training in few shot learning. Replace "golden labels" with "true labels"  Typo: "significantly lower than LA (image augmentations" should be DA instead of LA  Will the code be released?The paper shows very promising results for few shot learning in histology images and introduces a simple and effective method for doing data augmentation. Overall, the results are very promising, and the performance boost is significant. The paper would benefit from some additional experiments with competing augmentation methods and a few straightforward ablations. The proposed latent augmentation is simple to implement, and it consistently outperforms the standard input augmentation. Overall, the paper is very well written, feels complete, and has an extensive discussion. **Weaknesses**  The proposed method, "latent augmentation" isn t compared to other variation data augmentation methods. This makes the reader wonder whether "latent augmentation" is superior to other variation augmentation methods. For example, how well does adding white noise to the latent variables perform? Add a small investigation of the shape of the clusters (i.e., the covariance matrix). Did you notice any significant performance variation when running K means with different random seeds? Some paragraphs are too general and can be reduced in size. The few shot learning section from related works could be reduced in size by a third.<|endoftext|>They compare the generalization capabilities of self supervised contrastive learning and fully supervised classification as pretraining. They perform experiments to evaluate the impact of latent augmentation vs. data augmentation. Novelty:The authors say that few shot learning “is much unexplored in medical images, especially in histological ones.” and they “pioneer the study of low shot learning for histology images [...]”. However, there are a number of studies on this topic, some included in the paper’s related work section, but there are other recent works on this topic also, “Learning with Less Data Via Weakly Labeled Patch Classification in Digital Pathology“ by Teh et al.(ISBI 2020), “Supervision and Source Domain Impact on Representation Learning: A Histopathology Case Study” by Sikaroudi et al.(EMBC 2020) to name a few. The paper is well structured, but hard to follow. This topic is of significant interest to the biomedical community, as annotations are very expensive to obtain. Some vocabulary used is not conventional or precise. (sec.4.2)[...] DA can slightly polish LA [...]. I believe the topic is of significant interest to the biomedical imaging community, which is why I am suggesting to accept the revised submission. (sec 4.3)It should be addressed more clearly what is meant by “near domain”, “mixture domain”, “middle domain” and “out domain”. I also lack the information if at every iteration one augmentation was drawn at random, or a combination of the augmentations. If so, how does the additive color jittering affect the images?<|endoftext|>This paper studies the low shot learning problem with application to histology image classification. Specifically, contrastive learning pre training is used for representation learning in an unsupervised manner. For low shot classification, a new latent augmentation is proposed to augment samples by transferring variations from base classes to novel classes in the latent feature space. Three low shot classification tasks are set up with different levels of domain shift, i.e., near domain, mixture domain, and out domain, using public histology image datasets. This paper proposes latent augmentation (LA), a new augmentation method to diversify training samples in the latent space. 2.Extensive experiments show the proposed LA improves the baseline counterparts on the three low shot learning histology image classification tasks, although the improvements on the out domain task are much smaller. 3.Ablation experiments provide more details about the influence of prototype number and augmentation times in latent augmentation. The proposed latent augmentation relies on the random initialization of k means clustering. In other words, how should the seed for K means clustering be chosen? 2.The variation from base classes can also be calculated from the whole dataset without K means clustering. It seems to be a simple alternative to be compared. While the lack of dominant object in histology images is a major difference compared to ImageNet natural images, how does it result in the higher global local feature similarity in higher feature level? There are also some issues regarding the proposed method, i.e., the influence of K means clustering randomness, and the explanation of CLP s superiority over FSP. Overall, this paper s quality is good.<|endoftext|>The paper proposes to use contrastive learning (CL) with a novel data augmentation (latent augmentation, LA) strategy to build a few shot system for histology image classification. Two empirical findings are: i) models learned by CL generalize fairly better than supervised learning for histology images, and ii) LA brings consistent gains over baselines without data augmentation. The proposed LA is interesting and shows superior data augmentation including RandomResizedCrop, RandomHorizontalFlip, and ColorJitter. 2.The paper is rich in contents and provided a comprehensive appendix. This paper s result show that CL can be better than supervised pretraining, which is a encouraging finding. The technical novelty of this paper is limited. LA is simple and effective, but it has supervised counterparts (as stated in the related work part). 2.Lack of comparisons. The authors compared "CL vs. supervised learning" and "LA vs. stardard data augmentation", but did not compare with other few shot learning, semi supervised learning and data augmentation algorithms. These methods can also be used in problems with few labeled samples. Therefore, the finding of this paper is not so surprising. In this paper, the novel classes are improved more by SSL pretraining, while supervised pretraining still prevails in the base classes, which is consistent with existing papers which found that SSL are better when the task or dataset is different in pretraining and finetuning. It may be better to submit the full paper to a journal in this field such as Medical Image Analysis. Since each patch may contain one manually assigned label, does that mean labels have been somehow encorporated in the self supervised learning process? The paper has slight novelty and interesting findings but neither novelty nor finding is significant enough. It may be better to be submit it to medical image journals.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This paper proposes a new update rule for the continual learning setting to boost performance when novel classes are added to the data stream. Strengths:  The paper is clearly written and the main ideas are very well presented  The motivation is clear and well illustrated by the experiments run in Figure 1  Solid benchmarking effort. Weaknesses:  It appears that the advantage of this method may start to break down when the data stream is a combination of new classes and old classes, since the old class representations would also start to be updated; is this the case? There are some preliminary experiments in Section 5.5, but I would have liked to see a much more thorough analysis   what happens when the number of examples per new class in the incoming data batch is very low? What about at a range of "blurriness" levels? Overall, the paper presents a strong analysis of a potential failure mode in continual learning, proposes a simple method as a fix, and compares thoroughly against baselines to validate their results.<|endoftext|>This paper studied one problem of continual learning which causes the forgetting of old classes, i.e new class representations will often overlap significantly with the previous classes, leading to highly disruptive parameter updates during the incremental training phase. Two versions of asymmetric losses are proposed and compared: metric learning and cross entropy based  losses. Experiments are conducted over multiple datasets to verify the effectiveness of the proposed method. Figure 1 is good to illustrate motivation. The online continual learning scheme is valid and practical in real use cases. AML is designed to be slightly more complex (sampling of positive and negative samples) yet less effective than ACE. It uses features from live training examples instead of the class weights. Could elaborate more on their respective resultant prototype distributions with AML and ACE. The author can elaborate more on this. Also, the components of the proposed methods are not new, although it may be first used in this continual learning task. The motivation is well illustrated and the results are convincing.<|endoftext|>The paper proposed a new explanation of a performance drop phenomenon in a memory based continual learning method, Experience Replay. As the training goes on, the performance drop can be improved but not as much as satisfactory. The authors conducted experiments to illustrate their methods have a superior performance over baselines. The paper does good work explaining the phenomenon in the introduction section, which the reviewer found interesting. The paper also states the metric learning idea, which sounds plausible. Unfortunately, the writing somehow doesn’t connect this idea to the proposed method in section 4. The paper assumes the readers are familiar with the contrastive learning literature and doesn’t explain the loss functions quite well. Since the reviewer is not from the area of representation learning, I couldn’t justify the correctness of the method. It is hard to distinguish between gradients for tasks 1 and 2. The authors may want to explain how the gradient for task 1 is computed since the learner is in the task 2 phase. The paper analyzed an interesting representation drift problem of a popular continual learning method.<|endoftext|>In the online continual learning setting, representation of previous classes will change over time, especially at the task boundary. The authors present a new explanation for the abrupt representation change when applying experience replay (ER) algorithm. Pros:+ The explanation of catastrophic forgetting from the perspective of abrupt representation drift is somewhat novel. + Some new metrics are proposed, such as Averaged Anytime Accuracy, Total FLOPs, which make the comparison more rigorous. Cons:+ Some important related methods of online continual learning are not included in comparison, e.g.[1, 2, 3]. Moreover, [6] also finds that adopting metric learning on representations results in less forgetting. To make it clear that forgetting happens at the last fc layer or the whole network, I suggest using NCM classifier to calculate acc here, or another better metric. + The motivation of using metric loss is not clear. The proposed losses need more theoretical explanation for why/how they work. + Compared to SS IL, the improvement is not significant. arxiv.org/abs/2006.11234Overall, this paper proposes an interesting phenomenon in online continual learning. However, the relation between the solution and the insights are not strong. Besides, the empirical improvements are marginal.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; These collections of facts can be viewed as a graph with predicates as undirected edges between arguments. The approach is interesting boasts great results on two benchmarks compared to previous approaches. This makes it hard to place their approach in comparison with prior work thus making it hard to understand the novelty of the approach. Perhaps the authors should describe other methods that have been used to solve this task. This would help make it easier for the reader to understand the novel contribution of the paper and also makes the paper more complete by itself.<|endoftext|>Subject predicate object triples (dubbed "fact units") are extracted from the text using dependency parsing, extra coreference links are added using a coreference system, and the resulting graph is initialized with representations from a contextualizing encoder, passed through a GCN, and recombined with the contextual representations via multi headed attention. * I would suggest more caution when it comes to interpretability. ## Strengths* The model seems fairly intuitive. I am still not satisfied with the discussion of interpretability (which I think should be removed; see discussion below) and I think the paper could have done a better job with experiments demonstrating the relative strengths and weaknesses of the model, but the results that are present seem strong.<|endoftext|>The paper claims that a lot of previous work in QA based MRC focused only on entity aware common sense knowledge. I view the main contribution as using the Levi graph from dependency trees as  Beck et al.2018 and connecting them with coreference and entity linking. ### Strength  (From the previous review) All reviewers noted that the main strength of the work to be its good empirical results on the 3 benchmarks considered. The paper is well written overall. Some of the comments that other reviewers made from the last round of review (which resonated with me):  " **Significance**: This is where I am most conflicted about the paper.<|endoftext|>The authors define the fact triplets as subject predicate object relational path extracted from the dependency parser and use them as local knowledge, compared with the commonly used entity relation knowledge to enhance the task of logic driven question answering. Strengths:The paper is well written with clear motivations and structure. The proposed model is then experimentally verified on three logic driven datasets which demonstrates some performance gain. 2.According to the description, the fact units are constructed using the dependency parser. Indeed, the proposed graph model seems to only implicitly convey knowledge across facts in terms of local reasoning.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper proposes an original trick to tune the receptive field kernels for 1D ConvNets to tackle time series problems.<|endoftext|>The paper is on the receptive field of CNNs for 1D time series classification.<|endoftext|>This paper presents an Omni Scale block (OS block) to efficiently determine the kernel sizes for 1D CNNs. While the proposed method yields a simple and universal rule for kernel size tuning, and has shown to perform as best as the state of the art methods in the literature, there are some major points that are neglected in the current version of the manuscript:1) The motivation and justification of using the Goldbach’s conjecture is missing.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; The paper proposes an approach to generate stable crystal structures using variational autoencoders without having to train on out of equilibrium data, which is scarce for materials. Beyond that, benchmark datasets and tasks for materials generation are proposed. The proposed approach is a novel and relevant contribution to the domain of materials generation. CDVAE shows state of the art results in validity and diversity of the generated materials.<|endoftext|>The paper proposes a score based Diffusion Variational Autoencoder to generate the periodic structure of stable materials (crystal structure generation). *** The proposed model is SE(3) equivariant and periodic boundary condition sensitive, based on the recent advance on equivariant graph neural networks. It also considers the Periodicity influences on denoising target. The experimental results seems quite promising. **weaknesses*** All the datasets included are inorganic. * The lattice constant **L** is predicted from the latent vector and seems it is fixed during the decoding process. How is this condition modeled in CDVAE? * The crystal structures often exhibit some symmetrcity, e.g.space group. I do not see any discussion related to this concept.<|endoftext|>The paper proposed a new generative model for 3D periodic material molecular structure. Experiments demonstrate the model can successfully generate valid and realistic material, and optimize desired properties. Strengths:  The tackled problem is specific but new and interesting for the machine learning research community, and the paper nicely provided 3 benchmarks to standardize the material generation problem. The experiments are extensive to demonstrate the effectiveness across different datasets and metrics. In my understanding, $p_A +\sigma_A p_c$ is not a valid categorical distribution, so I m concerned with the sampling. Is there any softmax function imposed on it? However, I still hold some technical confusion (weakness 1), and I think some important metrics from the highly related topic are missed in the material generation task, which is the most important empirical part of this paper.<|endoftext|>This paper aims to address challenging stable crystal materials generation problems via diffusion variational autoencoder with graph representation learning. The authors demonstrate the proposed method by using several datasets and baselines. Although the performance can be improved and some metrics are not very reasonable, these efforts can be identified as benchmarks for further exploration in the AI/ML community. Multiple recent models, like SE(3) with PGNNs, diffusion models (NCSN), CrystalNN, DimeNet++, GemNet dQ, are simply used and combined without improvements. Does it depend on how do you define the stable structure, it is statistical stable or physical stable? I suppose that is the reason why you use the NCSN model for generation tasks but it might be easy to follow the intuition. It looks like G SchNet is a strong contender in this paper, however the authors implement it without considering periodicity. If so, could you add more discussion about the existing limitations of CDVAE? I also expect to see a better comparison with the current baselines.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper approaches the problem of supervised learning with missing data. The main contribution of this model is that they rely on deep generative models which seems to be an improvement from previous methods based on simpler generative models like mixture of Gaussians, for example. With the proposed model, the authors derive an optimization problem that consists in optimizing the discriminative model (classifier) and the generative model simultaneously. Strenghts:  Strong theoretical foundations  Clearly written with nice illustrations of the used principles. There are recently published methods cited by the authors that provided implementation code for learning classifiers and missing data at the same time, i.e.avoiding the imputation followed by classification approach. A good paper including very clear and technically sound principles for solving the problem of supervised learning with missing data.<|endoftext|>The paper presents a method to more optimally learn deep learning classifiers in the presence of missing values. The paper is written clearly and there are no obvious errors I can see in the theory or the experiments presented in this paper. However, I have not checked some of the derivations in detail. The method presented in the paper is largely based on previous work on variational inference, but it is otherwise novel and noteworthy theoretical contribution. Moreover, the results presented by the authors indicate that their method outperforms other relevant approaches (single imputations methods) in a restricted number of scenarios: low capacity regimes, or when the discriminative model has a strong inductive bias. Overall a solid paper, I recommend accepting the paper in the conference.<|endoftext|>The paper handles the issue of missing values in supervised deep learning settings. They use importance sampling technique to get multiple samples from the generative model. Pros:1.The authors do a good job of explaining the challenges of training discriminative models with missing data as well as going over the related works. 3.The paper is well written and easy to follow. 2.The novelty of the work is marginal. The multiple imputation idea, viewing the imputation problem as joint distributions (eq.1,2) and also going through a few of the related works mentioned in this paper, I feel that most of the ideas proposed in this work are already out there.<|endoftext|>They claim that the new approach, a VAE combined with a discriminator, jointly trained, is more scalable, and performs better than existing data imputation methods, in particular MIWAE (Mattei & Frellsen, 2019) Strengths:+	The proposed method offers end to end training, which is useful for scalability and transferability. Image, language, or statistical datasets. The addition of supervision and joint training to MIWAE seems novel, and while the work improves upon the prior methods of using DLVM for missing data, it falls short in the claim of scalability and overall gains in performance across different types of datasets. In UCI datasets, where features are real numbers, and probably contain more information per feature, supMIWAE performs at par with MIWAE.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The proposed ideas are interesting. However, there are several main issues as follows:1. the motivation is not clear and strong. The authors state that " imbalance of pseudo labels harms the performance”. However, if the pseudo labels of OOD is not imbalanced, it may still force the ID data into the incorrect class. 3.The Υ Model may not work well in a large scale dataset since the performance of the proposed SEC technical relies on the deep clustering algorithm. 4.Can Υ Model extend to other SSL methods? If not, it could be a limation of the proposed method. It is better to show some empirical results.<|endoftext|>The authors tackle the pseudo label in class mismatched semi supervised learning when there are unlabeled out of scope data from other classes. Strength  This paper has an interesting observation on how does OOD data influence pseudo label and resulting model performance. WeaknessI have several concerns about the experiments. This paper has an interesting observation and the proposed method seems to be original.<|endoftext|>Authors conducted several experiments to show the effectiveness of the proposed method, such as CIFAR 10 and SVHN. (2) The insight that pseudo labeled data is often class imbalanced is also interesting and can guide future research in this direction. (3) The OOD SSL is in fact investigated by a con current work [2], which was released on arXiv about 10 months ago and was in submission to ICLR 2022 too. (6) Using the expensive and time consuming clustering method on large scale benchmark like ImageNet may be of a big challenge to computational efficiency. Harvard	I like this paper in general, but I have some concerns on the effectiveness of this method in large scale benchmarks.<|endoftext|>It is known that if there are OOD data included in the unlabeled data, it can degrade performance of semi supervised learning algorithms. This paper focuses on one of the semi supervised learning methods, which is the pseudo labeling method. Experiments show comparison with both traditional SSL methods and class mismatch SSL methods, and show how the proposed Upsilon model works better than these baselines. but is this correct? The answer to "It is interesting .. experimental result." So there may be some implicit transfer learning going on.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; Authors propose to use dropout and layer norm to make Randomized ensembled double Q learning more computationally efficient. What are the unique advantage of this combination over other methodologies? The proposed algorithm uses simple modifications to achieve comparable sample efficiency while being faster in computation. Strengths   Simplicity: the proposed method requires minimal modification to existing methods, making it easy to implement and use  Significance: REDQ has achieved very strong sample efficiency but the ensemble of Q network leads to slower computation. How to achieve the same sample efficiency while reducing computation seems to be a very important question to solve. Though the paper studies an important problem, and has some good results, due to this major concern, and considering other issues in the paper, I don t quite think the paper is ready for publication in its current state, thus I currently lean towards rejection. Post rebuttal: after reading the authors  response and the revision, I feel that a large number of my concerns are addressed and the new results ablation and analysis look quite interesting. And why layer norm will deal with the difficulties? What is so special about layer norm? Though the name of Dr.Q is technically different from DrQ, I m still a bit concerned whether this will lead to confusion I think the paper has potential to make some good contribution, the authors already obtained some good results showing that Dr.Q can achieve the same performance as REDQ while greatly reduce computation and memory consumption, which is very nice. However, what is lack is some more in depth investigation on why the proposed method works.<|endoftext|>This paper attempts to address computational efficiency in Ensemble based Q learning. Strengths: This paper is well written and easy to understand. I thinkt he obser The results and analysis support the hypothesis that dropout and layer normalization can have the same effect as a larger ensemble. Many of the individual components presented in this paper (dropout for Q functions, ensembles, etc) have been discussed in previous works. Such a set of experiments will really help make this paper s claim stronger. It would be good to see an analysis of SAC with the dropout architecture (essentially this method but without any of the improvements from REDQ).<|endoftext|>The new algorithm Dr. Q. uses a small ensemble of dropout Q functions along with layer normalization. The proposed algorithm achieves similar sample efficiency as REDQ but better computational efficiency. Contrary to the ensemble approach for bias reduction which is rapidly gaining popularity in recent years, the authors proposed an alternative which allows an SAC like algorithm to work in high UTD settings by adding dropout and layernorm, which in my opinion is the main contribution of this paper. The presentation of the paper is quite clear and it was very easy to read. Weakness:My main concern is that it is not entirely clear from the paper why the method presented by the authors work so well. More analysis on this front would do much to strengthen the authors  overall arguments.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The final action outputs are determined from a mixture of experts of all options. The approach learns options offline from demonstrations (behavioural cloning) and combines them online by learning a new high level controller via RL (PPO) while using frozen options. It is overall well written and easy to follow. "The option critic architecture."<|endoftext|>This model is used to learn temporal abstractions and the control policy in the space of options using demonstration data via imitation learning. The performance on discrete action (Craft) and continuous action (Dial) environments are promising compared to existing baselines such as OMPN, CompILE and MoE. The authors introduce It is unclear whether this is a new problem introduced by the authors or an interpretation of an existing problem.<|endoftext|>To address the issues, the authors propose an option control network (OCN), including a high level controller and a pool of low level options, which is an "imitation finetune" paradigm as well. Finally, the proposed method is evaluated on two domains, Craft (discrete action space) and Dial (continuous action space), with results demonstrating its effectiveness. The paper is well organized and its method is sound to me.<|endoftext|>During pre training (i.e.skill extraction), the actions from the framework are computed by the weighted sum over options, which can be end to end differentiable, and trained using behavioral cloning. Once all options are extracted from the dataset, a new controller can be trained to solve a new task with the learned and frozen options via RL. The experimental results on the discrete CRAFT environment are impressive, outperforming all baselines. This paper tackles an important problem of reusable skill (option) extraction from a large multi task dataset. The proposed method is simple and works well in the discrete task, CRAFT. The experimental results in CRAFT show significant improvement over prior works. It is unclear whether the proposed method can achieve better performance compared to these approaches. Most hierarchical approaches so far have failed to learn more than two level hierarchy, which implies challenges in extending to multi level hierarchy.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; rating score: 10; The paper provides an considerable improvement to the DP analysis of hyperparameter tuning of DP algorithms (such as DP SGD). Also, this is a very important problem. The problem is important and the paper improves the state of the art so its merits are clear.<|endoftext|>This paper studies the problem of hyper parameter tuning in the setting of differentially private training of machine learning models. The problem studied here is a really important one, since it is directly related to actual deployment of DP models in real life settings. I don t see any major issues with the paper and I find the problem addressed very relevant.<|endoftext|>This paper has made the following contributions. This paper considers an interesting and important problem: how hyperparameter tuning on private dataset can leak information, where it provides an intuitive SVM example. Then, at the end, we return the best of the $K$ outcomes. Note that this is not equivalent with assuming each $M_j$ is DP, where the latter is more realistic. Generally speaking, I recommend acceptance of this paper.<|endoftext|>Overall, this is a strong paper and I recommend it for publication. The authors propose a way to measure the utility of the algorithms by looking at the expected quantile of the output.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper describes a transformation to add a hard regular language constraint to the output space of a linear chain CRF. Finally, they provide experiments showing the technique’s application to synthetic data, as well as semantic role labeling, which has natural constraints such as uniqueness of core roles. I would argue that this is a step forward, and a worthwhile ML contribution. Strengths:  This is a useful addition to the CRF toolbox; it is a nice, clean formalism for adding regular language constraints, and by making the connection explicitly to regular languages and NFAs, it opens up the possibility of fruitful cross pollination with formal language research. The authors get out ahead of the inevitable question of the utility of their work in the face of more general work on learning weights for FSTs. The advice amounts to, “minimize manually, and apply NFA minimization where applicable.” I think an example of an organic application of NFA minimization (plus a citation to the algorithm the authors have in mind!) I think it would be informative to include the number constraint failures in the unconstrained model. This paper formalizes and systematizes how to incorporate regular language constraints into CRF training and inference. This simplifies the incorporation of constraints, and makes it clear when they will become computationally infeasible. It also provides exciting hooks into formal language theory for future contributions.<|endoftext|>This paper describes CRFs that are constrained to generate tag sequences that belong to a given regular language (RegCCRF). This is useful, for example, in BIO tagging where tag sequences must be of the form O*(BI*O*)*. First, the application to BIO tagging is a good one, and it would help your presentation enormously to mention this application in the introduction. Perhaps the explanation can be left where it is, but summarized in the introduction. In Section 6, the experiments are interesting, but couldn t you go further and prove formally that a CRF is incapable of generating these particular string relations? The method improves performance on semantic role labeling, but the improvements due to the proposed method (as opposed to using RoBERTa) are not dramatic. On page 8, I d like to see a clearer explanation of how the constraint language is constructed. Maybe you meant direct theoretical comparison, but I definitely would have liked to see an experimental comparison between the two. I like this paper and just think it needs some improved motivation in the introduction.<|endoftext|>The paper proposes a modification over linear chain CRF models such that the output space of the model is constrained to be in a regular language. Training in a constrained fashion will directly minimize the NLL against the data distribution and achieve a better error, given output y \in regular language L.  The authors conduct two synthetic experiments to showcase that their model with constrained training is able to better capture non local dependencies and data distributions compared to a model with constrained decoding. Additionally, they show slight improvements on a semantic role labeling task compared to baseline CRF models. The approach and constructions are simple and natural. They have an interesting discussion on the relationship between their approach and WFSTs. The paper is very well written and the arguments are very clearly presented. I think a more systematic exploration even within the hierarchy of regular languages to test the limits of the approach would have been more insightful. The improvement in the SRL task also seems to be incremental and the need to induce regular language for various practical structured prediction tasks could be difficult in certain cases. In summary, it seems like a good paper, with a simple and clever idea to improve a fundamental model in structured prediction.<|endoftext|>This paper claims to propose a generalization version of CRF, regular constrained CRF (RegCCRF). Compared with traditional CRF, it can not only model local interdependencies but also incorporate non local constraints for the model. Specifically, by specifying the space of possible output structures as a regular language, assigns zero probability to all label sequences not in language to achieve the goal. The author mentioned that RegCCRF can incorporate their constraints during training, while related models only enforce constraints during decoding, but no matter in theory, I can t see what gains can be brought to the model by using constraints during training (in the current version). Because maximum likelihood estimation is used on the golden data during training, the labels will strictly follow the constraints, the constraints cannot bring any redundant information, so I don t see any advantages for supervised training under general data situations. Because the SRL test set is not large, this result is far from significant. 3.For RegCCRF, what about the training efficiency and decoding efficiency, and how does it perform compared to traditional CRF? 5.Since the added tag wise CRF can be viewed as a particularly well behaved special case of FST weight learning for an appropriately chosen transducer architecture and parameterization, a baseline that needs to be compared is to directly use an RNN network to simulate FST, its speed is obviously faster and can be trained. I have read the response and it has addressed parts of my concerns, so I raised my score. Although the discussion is very interesting, the current version of the experiment did not meet the requirements for publication, and many parts of the method were not properly studied.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper presents a variant of stochastic sequence neural network, the family of VRNN and SRNN. This paper adopts the CW VAE framework and completes the optimization process under the stochastic sequence neural network framework. The authors test it on the speech domain. The experiments show that it outperforms VRNN and SRNN in the benchmark datasets. The novelty is limited. 2.The hierarchy latent variable idea has been proposed in Stochastic WaveNet (https://arxiv.org/pdf/1806.06116.pdf) and STCN (https://arxiv.org/pdf/1902.06568.pdf). This paper didn’t compare the proposed method with these two methods. 3.In the paper, https://arxiv.org/pdf/1902.01388.pdf, the authors point out the evaluation problem of the stochastic sequence neural network. 1.And with some tricks, the deterministic model can catch up the stochastic model s performance.<|endoftext|>This paper proposes to put various models under the same experimental setting and compare their rate at compressing speech. The paper is well motivated. Marginalization is difficult as the paper argued. The majority of the paper is spent reviewing the models. I have mixed feelings visualizing the models the way it is done in Figure 1. Figure 2 is a much better representation, laying out the conditional assumptions for both the encoders and decoders. I do understand it would take up a lot of space, but it might be worth putting a figure in the appendix. The experimental design is fine, but it s unclear how the evaluation metric, the likelihood, is computed without marginalizing the hidden vectors.<|endoftext|>This paper presents an exploration of the use of latent variable models as generative models of speech. The performance of the proposed speech LVM is good, albeit it comes with increased computational complexity (hopefully something to solve in the future). In addition, it is shown that the resulting latent representation is correlated with phonetic structure, which is a pleasant bonus that other speech generative models (e.g.WaveNet) lack. I find that sequence of steps to be instructive, and the overall target to be one worthy of exploration. Weaknesses: This paper seems quite detached from the community that would find it most interesting. Of course, WaveNet is one model that can serve as a well recognized benchmark, but there is a lot more to compare with here. On the downside, this paper doesn t feel like it addresses any deep scientific questions (although it touches on some near the end), and it mostly reads like a todo list to get clockwork VAE to work with speech.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; I am also surprised that the authors did not cite the following work though it is only on arXiv but strongly correlated with this manuscript. The authors present how to use this platform in detail. There is very limited methodology contribution from this work.<|endoftext|>The main contribution of the work is introducing a novel strategy for batch experiments acceleration. this paper is more engineering oriented, lacking convincing theoretical works.<|endoftext|>### Empirical Novelty And SignificanceThe empirical results are the weak point of the paper. So, from an engineering point of view the system is well justified. In the experiments, the authors show that their package achieves often a fairly good performance.<|endoftext|>The problem studied in this paper is well motivated. I like it very much that the authors would open source such a powerful and convenient framework to complement the current state of the art packages.
Reject; rating score: 5; rating score: 5; rating score: 8; The proposed idea was experimented with and compared with respect to the convolutional LSTM baseline and several other temporal modeling methods (i.e., RIM or Conv TT LSTM). **Strengths:** * The paper presents a novel temporal model to capture spatiotemporal structures in the data and perform better video prediction. One relevant work using a similar idea was not cited. Lin et al., "Self Attention ConvLSTM for Spatiotemporal Prediction," AAAI 2020, https://doi.org/10.1609/aaai.v34i07.6819* In generative models, mode collapse means the generated samples being identical or very similar to each other. It may be caused by the imbalanced distribution of the training set or dependent structures and bias in the data. The motivation of spatiotemporal mode collapse is valid, however, the examples and used datasets are not enough to represent. * Even though it was argued on the contrary in the related work, the proposed work is highly related to feature disentanglement. Spatiotemporal slots idea reminds the attention modules on the temporal data and leads to a weighted fusion of temporal weights. Mainly the introduced problem is not clear and also approach is limited in novelty.<|endoftext|>This paper proposes a mechanism to reduce spatiotemporal mode collapse in unsupervised predictive learning. To achieve this goal, the proposed method is built upon the idea that different latent modes in the same data domain should share a set of hidden representation subspaces, which can be represented with various compositional structures based on the features in each subspace. Experimental results show improvement over simpler baselines such as PredRNN or ConvLSTM. While this may be true for very simple scenarios, e.g., moving mnist or KTH, for more realistic scenarios, which may not hold. The baselines authors compared their approach to don t represent the SOTA or even rather old but strong approaches, such as (Kalchbrenner et al., 2017). Regarding the method, I d see it as a good engineering effort of combining existing methods rather than proposing a novel one, with new insights. Moreover, comparison to SOTA and evaluation on larger, complicated datasets are missing.<|endoftext|>The manuscript proposes a novel architecture with a slot based decoupling aggregation framework for unsupervised sequence prediction. The model is motivated by preventing spatio temporal mode collapse, which affects many existing methods. The experiments clearly show that the model addresses this issue and performance comparisons across three commonly used datasets are presented. The proposed decoupling aggregation framework is demonstrated to learn a highly diverse set of mode factors which are combined to cover a diverse set of output modes with high accuracy. The analytical experiments show that the performance of ModeRNN consistently (across datasets) benefits from increased diversity of spatio temporal modes, whereas the performance of other methods often diminishes when trained on more diverse data. Did you also calculate the A distance using the cell outputs instead of the memory states? 9.The t SNE visualization of Figures 3 and 5 for KTH and the Radar Echo dataset would be interesting. Are the modes as clearly separated? I might have overlooked it, but I didn’t see a comparison of ModeRNNs with different numbers of ModeCells. 14.No discussion of limitations15. Do you have initial results on datasets with a larger diversity of spatio temporal modes (e.g.Human3.6M or KITTI)?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes an ensemble model between a full fine tuning model and a parameter efficient fine tuning model to improve the out of distribution (OOD) performance of a full fine tuning model. There are two ensembling methods presented in the paper: linear interpolation between the predictions of the two models; and distill from the predictions of a parameter efficient model with ID training data. Third, the experiments do not reflect how and why the hyperparameters (length of prefix vectors, bottleneck dimension) of parameter efficient tuning are selected, which is important for the discussions and conclusions. The proposed method also makes parameter efficient tuning method not attractive any more.<|endoftext|>The present paper first discusses the trade off between performance for out of domain data and in domain data with respect to whether the model is fully fine tuned or lightweight fine tuned on NLG tasks. This is because of two major concerns. For example, for ID data of the Data2Text task, full fine tuning achieves a score of 63.25 while lightweight fine tuning achieve 63.18. I generally like the idea of this paper and the paper is perfect up to the place where the experiments are introduced. Discussions of this paper overlook major phenomena in the results, making the discussions and, probably, the conclusions are, in part, wrong.<|endoftext|>This paper proposes a simple yet effective method, cocktail fine tuning, for the natural language generation tasks. This paper combines adapter finetuning and full finetuning to handle both in domain data and out of domain data and the idea is interesting. The authors should talk about this limitation in the paper.<|endoftext|>This paper presents interesting an idea of combining lightweight fine tuning and full fine tuning to achieve the best of both approaches, i.e.perform best on out of domain and in domain data. The authors also provided good analyses for more insights. in the experimental setup and results that make this paper a borderline paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; In addition, the authors introduce a CelebGlow dataset, which is more complex. This is a very well written and clear paper with sufficient implementation details for reproducibility. However, I have concerns with the following issues, which I believe, limit the contribution of this paper. 3.The conclusions drawn from the paper are interesting and informative. It is unclear that models without learning via backprop would be able to generalize. The study poignantly points out failures of current models in generalization through a large number of empirical studies.<|endoftext|>The ratio of distances idea makes sense, but I am not sure what the authors are trying to conclude with that statement. Positives:  The paper is extremely well motivated and addresses a very important problem in the field of generalization in NNs  The dataset also looks very promising  The experiments are extremely well done and thorough  The idea of having "conclusion" section in each subsection in the experiments is a very nice idea. It is a well written paper with great analysis and a very useful study to have. I am slightly concerned about what "new" information this adds to the field   a lot of the conclusions in this paper were well known in the community, to my knowledge.<|endoftext|>This paper presents an empirical study of generalization in visual representation learning. The paper compares in distribution (ID) generalization to out of distribution (OOD) generalization of three types   interpolation, extrapolation, and composition. I think this is a very good paper. The experiments are well designed and thorough, and the findings are valuable contributions to an important area. Because of this, I think the most valuable contribution of the paper may be the benchmark it provides, on top of which future studies may find something really new. Aside from this, I think the paper is very solid. It s probably all fine but some commentary on this could be useful.<|endoftext|>This paper studies the generalization ability of neural networks in out of distribution (OOD) settings on simple datasets (such as Shapes3D) and shows that generalization performance drops when real world datasets are used in comparison to artificial datasets. The direction of this work is nice and the problem tackled by this paper is indeed important. However, I have a few concerns with the paper:i) To test generalizability of neural networks, test datasets are constructed using four methods: interpolation, extrapolation, random, and composition. However, the datasets studied in this paper are simple and limited, and far from real world datasets. iii) Previous works have shown that larger models generalize better than smaller models. Can authors comment on that? Post rebuttal: I am convinced with author s response and I am leaning towards acceptance.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; In this proposal, the condition defined by the user is added to the main objectives and solves a new multiobjective optimization problem. Moreover, the claim that they are proposing an algorithm for large scale models seems to be inaccurate again. Especially since they have cited this paper in the related work and are aware of this work. In [B], similar to the current proposal the goal of finding preference based solutions on the Pareto set has been added as a KL divergence to the main objective. The difference of the current work with the one in [B] is generalizing the side objectives to other objectives such as diversity in addition to the preference based. arXiv preprint arXiv:2104.01634 (2021). Also, similarities with the current work and existing methods not referenced appropriately, which seems to reduce the novelty of this work.<|endoftext|>This paper proposes an algorithm to optimize in a Pareto set in order to reach a solution or solutions that can minimize an extra criterion. It has the flexibility to switch between two cases depending on the magnitude of the gradients w.r.t.the original tasks. The authors provide strong theoretical analysis for the proposed algorithm along with some empirical results to show its effectiveness. Weaknesses1) Many important details are left out, esp. For the 3rd experiment, it was mentioned in the appendix, but even there, it s not very clear. Moreover, no comparison of computational cost between different methods can be found. The algorithm proposed in this paper is intuitive and highly relevant in the context of multitask learning, but I find the experiment section didn t show the effectiveness of the proposed method clearly. Many missing important details result in many question marks while I read the paper. The experiment section of this paper definitely has room for improvement.<|endoftext|>This paper deals with multi objective optimization. The main contribution is an algorithm that attempts to approximate this optimization problem. The objective studied is quite general and it is conceptually nice that you can plug in any loss $F$ to optimize over the pareto set (it seems that this framework is quite versatile). The authors mention that the problem with the scalarization approaches (where the losses are combined into a single objective by taking their convex combination) is that they only find solutions on the convex envelope of the Pareto frontier, and thus can miss some solutions. In the figures that I saw in the paper the Pareto frontier is convex. The authors mention that one drawback of JiGen is that it requires a careful grid search for the weighting of the two objectives. I am not sure I agree with this, since in the JiGen paper this parameter doesn t seem to be extremely critical for performance.<|endoftext|>The paper summarizes a new gradient descent procedure for finding Pareto optimal points, while simultaneously optimizing with a "non informative" function F that depends on the whole set of points. Examples of these functions are given; however, it seems like such a problem has already been considered. Furthermore, it is unclear if the proposed gradient descent approach to solve the main problem is novel or if it has good convergence guarantees in finite time (not in the limit). Under certain assumptions, the algorithm have descent guarantees, but it is unclear if these assumptions are discussed or are realistic. The paper presents an interesting setting for multi objective optimization, but suffers from a lack of discussion of scalarizations, assumptions, why non convex Pareto frontiers can be discarded, and the novelty/convergence of the optimization.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes a new method for data augmentation, named Noisy Feature Mixup (NFM). Theoretically, it is shown that NFM enables smoother decision boundary, and amplifies the regularization effects of manifold mixup and noise injection. Overall this paper is well written and easy to follow. Overall I find the theorems offer a good insight for NFM. The problem this paper focuses on is important, i.e., improve the generalization performance of deep models.<|endoftext|>This paper studies data augmentation methods for improving the robustness of supervised learning. The main contribution is presenting Noisy Feature Mixup, extending input mixup and manifold mixup to all layers of a neural net. The experimental results show that the proposed approach improves the robustness of supervised learning under several noise attacks on the input data set.<|endoftext|>This paper proposes noise added version of the manifold mixup. The authors verify the theoretical properties of the method in terms of regularization and robustness. It will be informative to add a more intuitive explanation for the theorem in the paper. Overall, I believe this is a clear paper and I maintain my score. This paper is well written and provides thorough theoretical properties.<|endoftext|>The paper proposes a new and inexpensive mixup method named Noisy Feature Mixup (NFM) to mitigate over fitting and improve generalization. Then they identify the regularizing effects of the proposed NFM. It is difficult to rate this paper. Theorem 1 has theoretical value. But I would vote positively for theorem 1.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposed a sharpener module for sequence to sequence learning. The motivation is to address the construction of clear alignment in attention mechanism. The authors conducted experiments on handwritting text recognition and scene text recognition text datasets. The experimental results show that the attention mechanism with a sharpener module is better than the soft attention and hard attention. Weaknesses:The experimental results are not convincing enough. The results on the scene text recognition benchmark are much lower than the current mainstream results [1,2,3]. I suggest the authors choose a SOTA method as baselines.<|endoftext|>Equipped with the proposed Sharpener module, a new Seq2Seq model is proposed for text recognition. Strengths of this paper:(1). A new sharp attention mechanism is proposed, by introducing a latent vector into traditional attention framework, so that let the target output token only depends on one element of input sequence. I don t think to pursue the alignment such that each target token only depends on one element of the input X is a better way comparing with traditional soft attention mechanism. The reason is two fold. Therefore, it is doubtful that the performance gain achieved by the proposed Sharpener actually comes from the STN rectification rather than the so called sharpen attention. (3) The experiments are not solid and convincing enough:      The first experiment using synthetic handwritten digital string is quite simple and toy. To show the proposed method can be really useful, I recommend the authors should use more challenging dataset, such as IAM dataset which is widely used in the community of OCR. Otherwise, I am wondering if the proposed method only performs better under small training sample setting. In general, this problem addressed in this paper is not a new one.<|endoftext|>This paper presents a sharp attention module for sequence to sequence learning. It introduces a sharpener to compute context vectors for clearer alignment of features. The motivation is clear. The key innovation is the sharpener module for better feature alignment, which theoretically sounds according to the equations in the manuscript. Otherwise, I’m thinking it’s better to claim that this method is designed for scene text recognition task. Why not conduct such experiments for experiments in Tables 2 and 4? Currently, the performances shown in Table 4 are very far from state of the art. Extensive results on the scene text recognition task show the effectiveness of the proposed method as compared with the traditional attention mechanism. However, the experiments are not thorough and enough to support the authors’ claim well. Additionally, some details of the experiments are not very clear to me.<|endoftext|>In this paper, the authors propose a sharpener module to seek clear alignment in the attention mechanism. The experimental results show the effectiveness of the proposed method. Strengths: The alignment is one issue in the normal attention mechanism, especially for the long sequence. The motivation is strong, and the idea performs well on the two handwritten recognition tasks. If the paper size is limited, maybe the section 2.4 can be shorten because I think this part is very similar with the previous work. For example, it is not clear to know how the three ways to compute context vector c given Z. Maybe some kind of improvement on SOFT attention can be compared in the experiments, for example some works introduced in Related work. (4)The experiments are not clear.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper introduces the idea of “learnability lock” for data authorization. Compared to the “unlearnable examples” work of Huang et al.2021, this work proposes to use inevitable linear/convolution transformations to formulate the noise. This was demonstrated to have two benefits that seem to address two important limitations of previous works: 1) making the unlearnable noise more controllable so “learnability” can be flexibly locked and unlocked; 2) the noise seem can survive (to some extent) adversarial training now. The idea of “learnability lock” appears fairly interesting, although it is motivated by the “unlearnable examples” idea. 2.The proposed invertible noise generation method looks simple and effective, should be easily applied in real world scenarios. 4.The paper is well written, pleasant to read. The experiments are comprehensive. I suggest the authors provide a paragraph of analysis in the main text as well. Any thoughts on this?<|endoftext|>The authors propose two invertible transformations to craft adversarial perturbations: linear pixel wise transformation and convolutional functional transformation based on invertible ResNet. Overall, I believe that the idea of the paper is interesting and novel, the results are appealing. I have two questions for the authors: 1. Numerous experiments demonstrate the effectiveness of the proposed transformations in both securing the data (making the data unlearnable when transformation is applied) and unlocking the transformation (making the data learnable when the transformation is inverted).<|endoftext|>The system builds on top of recent work on data learnability, where, in essence, noise is applied to individual datapoints in a way to disrupt learning generalisable features. Strengths:+ Interesting setting and a challenging threat model+ Thorough evaluation Weaknesses:+ Narrative being very far from reality + Unclear contributions over and above Huang et al.First, I want to mention that the paper is very well written, thoroughly evaluated and presents a convincing technical story for an ML audience. Yet, for people more familiar with Computer Security, and “do not roll your own crypto” principle, it presents a rather weak argument as to why any protection is provided with such an encryption scheme at all. In fact, the results from adversarial training tell that the system design is broken and the paper contradicts its own story. I am very confused about the setting of the paper. ```Existing methods include training ML models on encrypted data, where the sensitive information could be hidden through cryptographic approaches in order to prevent malicious manipulation (Hesamifard et al., 2017; Ding et al., 2021). However, those kinds of data processing methods normally do not preserve visual properties from the raw data and thus affect normal use. Currently, there are international standards, competitions, and good understanding of privacy guarantees of different encryption mechanisms. If the authors are able to clarify these concerns, I would be happy to revise my review.<|endoftext|>  The paper tackles the problem of  unlearnable examples : to perturb the images of a labeled dataset $D_c$ to obtain $D_p$ with the desiderata (a) training models on $D_p$ leads to models with significantly lower performance (b) image perturbations are constrained to some $\epsilon$ ball and (c) with the correct  secret key  (learnable parameters in this case), one should approximate recover $D_c$. What is the number in the heatmap of Fig.2?Overall, the paper tackles a reasonably well motivated idea of introducing perturbations to prevent unauthorized training over a labeled dataset. [1] "Unlearnable Examples: Making Personal Data Unexploitable" Huang et al., ICLR  212\. My sub concerns:   (a) it seems that the improvement of the proposed approach is primarily addressing a specific failure mode   being more robust to adversarial training on the perturbed dataset.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; While this is true, I would have expected some empirical analysis, to justify the theoretical claims of the off policy critic error in the overall algorithms. For example, how does the critic evaluation error in algorithms like SAC/DDPG lead to the performance in the policy improvement step? I think a careful analysis of this is what is expected from a paper trying to analysue bias, variance and policy evaluation error of algorithms like SAC/DDPG etc which has strong empirical performance. For example, it d have been better to see how this error leads to the convergence/divergence of off policy algorithms? 3.Theorem 1 and the Corollary are useful to see how the overall off policy learning error can be characterized in terms of the bias and variance. 4.The experimental analysis of this paper is rather weak. However, from the context of the paper, it is not clear how this contribution is justified   I do not see any clear statements/results backing this claim? I think this paper is interesting and careful studies of existing algorithms that work well in practice is required. In practice, I expected a lot more ablation studies characterizing the bias variance trade off. I would encourage the authors to pick some simpler tasks and demonstrate the bias/variance analyais on some simple mdps too   instead of only showing performance curves for different sampling schemes on some standard mujoco tasks (which often is hard to interpret and not clear how these results are supporting the advertised claim)<|endoftext|>Literature reviews  As mentioned by the authors, off policy evaluation learning has been categorized into importance sampling based and regression based. Overall, I think the paper is lacking in a few aspects. I think the theory results are not very convincing, in that it does not reconcile with certain intuitions. In other words, there can be an average discounted visitation dist which cannot be realized by any Markovian / non stationary policy. This observation is based on the last term of the RHS in Thm 1. Though I appreciate that the authors try to connect theory with practice, I find the jump from Sec 4 to Sec 5 to be rather abrupt. A rather important factor here is that, intuitively, it is kind of clear why putting emphasis on recent experiences makes sense   this is because overall RL algorithms prefer on policy data over off policy data to be more stable.<|endoftext|>The paper proposes to decompose the critic error into several terms to better understand off policy actor critic algorithms. Empirical results are provided to show how their theory can be used to explain two experience replay sampling strategies. However, all the theories discuss only the critic error. Second, Algorithm 1 does not make sense to me. This confusion makes it hard to interpret Theorem 1. What do we get from the proposed decomposition? It looks it s just an understanding of ERE, which I don t think is enough for publication. Further, it would be good to explain how many seeds are used to generate the plots. For now it is hard to interpret the results. The presentation and motivation of the work is not clear<|endoftext|>This is a nice paper expanding our undertanding of actor critic algorithms. Focusing on the policy evaluation error is a simple approach yielding meaningful results. I think it would be a nice addition to the literature. The experiments are a good addition to validate the theoretical findings and make the case more convincing. This seems to be a less popular choice in RL. It would have been nice to have an experiment which directly measures the policy evaluation error. Is it not as important to assess the behaviour of the actor critic algorithms?
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper presents an approach to model the joint distribution over data from different modalities such as vision and language. It seeks to address a limitation that is common to prior work on multimodal VAEs, which have typically combined information across modalities by using explicit methods including products, mixtures and their combinations. Specifically, this work proposes to exploit mutual supervision between different modalities to circumvent the need for the above mentioned explicit combinations. Additionally, it provides a natural and intuitive extension to learning from partially observed data. The authors empirically prove the effectiveness of their proposed approach over prior work under both partial and complete data observation settings. The proposed approach also allows the joint distribution to be modeled under the partially observed setting, where a percentage of observations have missing modalities. The approach is also intuitive and well detailed such that it is easily understandable. 2) The equations are easy to understand and constructing a mirrored version of the VAE by swapping the modalities as well as their corresponding parameters is a clever idea that is premised on  the insight of finite conditional entropies. However, given the fact that it works well under the partially observed setting, it will be insightful to expand the discussion on generalizing beyond two modalities. With the recent focus on representation learning from multiple (more than 2) modalities, it might be helpful to discuss more if it s possible to extend the implicit combination to multiple modalities instead of using explicit combinations. For example, table 3 could be included in the main paper to highlight the effectiveness of the approach, despite the space constraints. In summary, this paper presents an interesting and intuitive approach for modeling the joint distributions across modalities. Hence, I recommend acceptance.<|endoftext|>Even though the novelty is somewhat limited in VAE literature, it s still a contribution to multi modal generative modeling. Unlike prior work that typically handles idiosyncratic modalities with the explicit combination (concatenation or factorization), this proposed method implicitly introduces a dependency between two modalities via a prior regularization. The method extends CCVAE (Joy et al.2021) by replacing the label as another modality. To compensate for the information disparity from the label to a data modality, the authors present a "mutual supervision" that uses a bi directional information flow. The method can also train with partially observed data where some modalities are missing. The presented framework (MEME) is an interesting and technically sound method for multi modal data modeling. The paper is well structured and easy to follow. Even though the idea behind is not novel (like the partial label of semi supervised VAEs), I still consider it as a contribution, since prior work usually handles the partially observed modalities in a hard way (zero padding or discarding). The method extends the CCVAE which models the dependency between label and data. I agree that mutual supervision can mitigate this issue as it establishes a symmetric information flow. Can the authors provide more details on the architecture and parameters related to each modality? 2.I m a bit concerned about the scalability of the method. Unlike other multi modal VAEs that simply concatenates a new modality, MEME is based on a bi modal setting. The authors need to double check the manuscripts   some typos I can find may confuse the audience: 1) In  Fig.6 MEME_Caption should be MEME_Image?<|endoftext|>This paper proposes MEME as a new method of multimodal VAE, which is an extension of semi supervised VAEs and can handle partial train settings. Strengths:  The paper is very well organized and easy to read. The proposed MEME is a straightforward extension of an existing semi supervised VAE to a multimodal setting, which is easy to understand and can be easily adapted to partial train settings. In other words, the authors discuss the merits of MEME only from a semi supervised perspective, and therefore should explicitly discuss why it is good in the complete setting. I understand that the authors are specifically focusing on the bimodal case in this paper. However, this approach is a combination of implicit and explicit methods, so it does not mean that implicit itself can be extended to more than two modalities. The MEME proposed by the authors in this study achieves high performance in both partial and complete cases. It also shows interesting results in terms of relatedness. However, there is a lack of discussion on why MEME is better than existing methods and an explanation of its limitations.<|endoftext|>The paper is well written and this reviewer enjoyed reading it. In “Generalisation beyond two modalities” (on page 5), the authors say it is quite straightforward to extend MEME to be used for more than two modalities. Can you explain more details about the extension? This reviewer is unsure why the MEME models that show a high performance in the cross coherence metric shows lower performance in the classification task. However, this result is not coincident with the results from Figure 5, which shows a lot higher scores than MMVAEs’3. However, the technical novelty is limited, except for extending the work of a semi supervised VAE [Joy et al.(2021)].Also, the problem itself is not new as the multimodal generative models were introduced in prior work [Wu et al.(2018)] and [Shi et al.(2019)].The paper is about the new approach to solve the missing unpaired data problem in the multimodal setting. Although the approach is solid and interesting, it is incremental to the prior work. Some of the experiments demonstrating the superiority of the proposed approach are also unclear.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The submission is not anonymized, the author s name chaoyanghe appears frequently in the attached SSFL_Sumpplementary/SSFL Source Code. Therefore, it should be directly desk rejected. See Summary Of The Paper for detail.<|endoftext|>The authors merge self supervised learning in personalized federated learning to solve the limited label and data heterogeneity problems in the local clients. The analysis and discussion after the experiment are sufficient and convincing. However, I am not sure the novelty of this work is good for this conference. Weakness:  I am a little bit concerned about the technical novelty in this paper.<|endoftext|>This work is concerned with a very practical scenario of Federated Learning where the participating agents may not have access to labelled data. SimSiam architecture to learn useful feature representations with extensions that incorporate personalisation for local client models. The learnt representation are evaluated against a KNN classifier for analysing their usefulness. Strengths  Deals with an important and practical scenario of FL  The analysis of personalised representations is interestingWeakness  Insufficient quantitative assessment of SimSiam approach against other self supervised approaches. It is unclear the feature representation can be used in practice, especially in the personalised setting.<|endoftext|>This paper introduces the self supervised learning (SSL) framework for FL. The performance comparison indicatesthat representation regularization based personalization method is able to outperform other variants. Although it mentioned in the implementation setting that the client number selected per round is 10, it is not clear how many total clients are used in the FL setting. Can you provide an explanation for that? What about the label distribution? I appreciate the emerging topic of SSL in FL. The proposed personalized federated SSL is interesting. However, there are quite a few unclear details regarding the method and experimental settings.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposes a bottom up multi person pose estimation method based on transformer model. * The ablation study on Tab.1 shows the effectiveness of the proposed method. * The performance is not promising when compared with some classic methods. * As for instance segmentation, the authors argue that the worse performance is due to lower resolution. The idea is clear and reasonable, but lack of novelty and promising performance.<|endoftext|>The paper proposes to incorporate the instance segmentation task into the human pose estimation task (serving as attention layer loss for transformers). This makes the method more like a top down method instead of bottom up, making this methods less compelling. Please refer to the Main Review of the paper. (3) the final results are not compelling, even worse than PersonLab (2018)(4) what is the speed and complexity increment  after adding this intermediate loss / for inference (compared with original transformers)(5) the paragraph above Fig 5 seems not reasonable to me.<|endoftext|>This paper presents a bottom up 2D multi person pose estimation system. The main contribution of this paper is that they introduced a supervised self attention (SSA), which supervises self attention maps with human segmentation masks. W2.Weak experimental results compared to recent state of the art methods. CVPR.2021.W3.Unfair comparison using the refinement. The refinement introduced in Section 3.1 seems used only for the proposed method and not used for other bottom up methods. No qualitative 2D human pose results are provided.<|endoftext|>Bottom up methods tackle the problem of multi person detection, pose estimation, and segmentation by localizing human keypoints and then grouping them into person instances, which inevitably bring up the question of what features to use for grouping and how to efficiently group keypoints and break down corner cases. In this work, the authors propose to use Transformer to exploit associative information between keypoints. The efficacy of this approach is also verified by the experimental results. While the proposed model can elegantly unify the task of keypoint detection and instance segmentation, the advantage of such approach is not supported by the experiments. Overall, I think this paper presents a novel and interesting idea. However, the current experimental results are relatively weak and cannot show the strength of the method, which limits the contribution. There are also some concerns with the self attention supervision that are not explained well and not supported by the ablation study.<|endoftext|>This work proposes explicit supervision of transformer attention for the purposes of bottom up multi person pose estimation. Beating the state of the art is certainly not a requirement here, but it is important to include other recent methods in comparisons. Overall, the paper is written clearly and sufficient information is provided to reimplement the method.
Reject; rating score: 1; rating score: 3; rating score: 3; However, this looks like a hyperparameter tuning for the dimension numbers unless the paper proposes a novel way to compress the dimension, which is missing in this paper. This paper does not address this issue. Table 2 shows that the complexity is still O(N^2). The proposed method lacks novelty, and the technical details of the proposed method are missing. Therefore, I recommend rejecting this paper.<|endoftext|>The writing is good and the organization is easy to follow. Cons:1.Novelty & Significance are limited. So I did not think the novelty/significance of this work meets the accepted criteria2. Therefore, I really wonder if the proposed method could accelerate the inference speed, and it is not reported in the paper. This work lacks novelty and significance.<|endoftext|>I think this paper is not good enough mainly due to the very limited novelty and results. Using fewer channels somewhere in a neural network can hardly be a contribution to the community.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper formulates the learning the schedule of learning rate as a reinforcement learning (RL) problem and finds the optimal schedule which outperforms the baseline scheduling algorithms such as constant learning rate, polynomial or cosine decay, warmup and restart, gradient based method like Hypergradient, and another learning based method SRLS. The paper proposes Graph Network based Scheduler (GNS). Rather than looking at the last layer or concatenating different layers of the network, GNS uses graph convolutional networks (GCN) to do message passings over the computational graphs of the model, and as a results, are more efficient, informative, and architecture agnostic which can easily generalize to other network architectures. Finding the optimal schedule of learning rate is an interesting and important practical problem which can help improve the performance of the models in all machine learning community. Using graph neural networks for computing the features of the nodes is an innovative idea, because it offers two benefits that the other methods lack: 1  it can use all the information in the middle layers as opposed to the last layer, 2  the learned graph convolutional network (GCN) can be reused and generalized to new networks, so this makes it architecture agnostic. The experiments are convincing and complete. It is clear to see the benefits of GNS over all other methods. The authors has shown the performance improvement in different tasks for image classification and language understanding, analyzed the generalization of GNS between different networks, compared the run times, and done an ablation study to show the importance of different parts of the model. Weaknesses:  The authors has used GCN as the model of choice for GNNs. But there is no explanation on why they choose this. I wonder if the authors have also considered other models such as graph attention networks (GAT)? It would be great if the authors check to see if other GNNs can even outperform GCNs in this specific problem. The authors has formulated the problem as an RL problem, which is the most correct way to do it, and has also thought how to make it architecture agnostic, generalizable, and versatile in using the hidden layers  information by message passing through the computational graphs. Overall, it seems that this formulation and solution is the best one can do to learn the optimal learning rate schedule, which is validated by the experimental results and the ablation study. Therefore, I believe this paper is very interesting and I recommend it for acceptance.<|endoftext|>In this paper, the authors study the learning rate scheduling for optimization in training deep neural networks. It proposes graph network based scheduler (GNS) to learn a scheduling mechanism for learning rate. GNS encodes the neural network by GNN and trains an agent to control the learning rate. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method. Pros: The proposed approach for the learning rate scheduling is novel. It can achieve better performance than baselines. 2.For a graph constructed based on a network model, what are the features of nodes? 3.The training loss curve should be displayed for better comparison. The author should compare some baseline methods based on reinforcement learning, such as [1,2]. Learning an adaptive learning rate schedule. However, there exist aforementioned concerns that the authors need to address.<|endoftext|>The paper proposes a Graph based learning rate scheduler for neural network optimization. The graph is constructed based on the network structures and trained with reinforcement learning. The method is evaluated based on Image Classification and Natural Language Understanding and achieves superior performance than baselines. 1.Finding a good learning rate scheduler is an interesting topic. This paper proposes a new way to search for a good learning rate by exploiting the neural network structure with a graph. 2.In practice, the mean and variance of the weights, biases, and absolute gradient values, etc. are used as the node features. Why choose them as the features? 3.What is the learning rate for the training of GNS? How to set this learning rate? Those questions should be investigated. 4.Why not use some other forms of action, for example, adding a residual? 5.I recommend reporting the performance on ImageNet to make it more convincing. 6.The full form of some abbreviations is not declared, which makes it hard to understand for some readers, for example, MRPC, RTE, and STS B. However, some of the investigations such as the node feature and the meta learning rate are missing. post rebuttalMy concerns are well addressed in the rebuttal. I will keep my score.<|endoftext|>This paper proposes a Graph Network based Scheduler (GNS) method that learns a learning rate scheduling policy for problems with moderate time horizons for optimization. The novelty of the paper consist of representing the optimized neural network as a graph and using a policy that takes GCN representations of the architecture and its weights in a PPO algorithm. The paper also introduces strategies for validation that allow for more efficient searching of the policy. There are extensive experiments on training ResNets on CIFAR 10 and fine tuning RoBERTA on GLUE. Additional benefits are demonstrated with transferring the learned policies to novel architectures or datasets. Ablation studies underscore the components in the GNS method. # Strengths:* The improvements in the paper are intuitive and consistent across the benchmarks considered. The paper makes an effective presentation of the idea and the experiments are largely sound. * It is great that the paper improves realistic large models in settings such as RoBERTa fine tuning. The authors address the problem in Section 5.5, however they do not say how much time it took for the CIFAR experiments. In Section 6 the authors also acknowledge that limitation, but they do not discuss how the potential future direction might look like. It is not clear to me how this method could be useful for long time horizons. * Focusing on the current experiments, it seems that the empirical argument the paper makes is that using GNS is more optimal than doing a grid search. What if we expand the space and use a more efficient search methods in the hyperparameter space? For example, a natural baseline seems to be CMA ES (https://arxiv.org/abs/1604.00772). It would be interesting if there is something to say/ show about comparing your method to more efficient alternatives of grid search. * Is there an ablation on the choices of the raw features $x_v$ used in the GCN? It would be useful to take a look at such a study for future work with GNS. * At the end of page 4, please use different enumeration for the steps in the hierarchical GNNs, because (a,b,c) is used in Figure 1 and it becomes difficult to read the text. I believe that this paper has great potential to become useful to the community. I have listed my comments/ thoughts and suggestions in the Main Review.<|endoftext|>The paper claims that existing RL based adaptive learning rate schedule methods are insufficient to work well for large models such as RoBERTa because they only use model states from the last layer. The paper then proposes to use all layers  hidden states as feature vectors and uses graph neural network GCN to encode those features. The paper claims that such a feature encoding scheme allows reinforcement learning (e.g., PPO) to better learn a learning rate schedule, especially for large models. The paper evaluates its proposed method on both image classification (e.g., Fashion MNIST and CIFAR 10) and language understanding tasks (finetuning RoBERTa on GLUE) and shows that the proposed method is able to achieve better final accuracy than some heuristic based learning rate schedules. Weaknesses:  The novelty of the paper is limited. The paper fails to provide convincing evidence that the suboptimal accuracy of existing methods is truly a result of using the final layer state, e.g., is it possible that the RL based optimization scheme itself is suboptimal? To be more convincing, the paper should include an evaluation of larger datasets such as ImageNet. The paper explores an interesting idea of using graph neural networks to better encode model states, with a hope to improve the existing RL based meta learning method. Despite showing some promising results, there are quite a few concerns about this work. First, the RL based scheme (e.g., PPO) is very similar to Xu et al.2019.Therefore, the remaining novelty is really on changing FFN to GCN to encode more model hidden states. Second, it seems there could be multiple factors that impact the final accuracy, such as the encoding scheme (e.g., GCN vs. other encoders), the features (last layers vs. all intermediate states), the optimization mechanism (RL vs. alternative methods), the choice of hyperparameters. However, the paper jumps to the conclusion that the encoding scheme and features are suboptimal without giving enough evidence to show that they are indeed the culprit. The paper also changes multiple factors at once, including tuning additional hyperparameters, making it difficult to tell where the improvements actually come from. To be more convincing, it would be better if the paper could provide more insights on how encoding affects the final accuracy. For example, does the quality of the GCN matter when performing the encoding? If so, how do you measure the quality quantitatively? Could the existing method from Xu et al.2019 still work by slightly adjusting its model architecture, e.g., by increasing the model depth/width or changing the hyperparameters? If we take the publicly reported results as the baseline, the proposed method actually performs worse than the RoBERTa large baseline. However, it does not seem to be the case that the paper includes this basic baseline in the comparison. It raises concerns on whether the improvements in Table 2 are merely an effect of comparing with a set of suboptimal learning rate schedules. To be more convincing, it would be better to compare the proposed method with the best practice of fine tuning RoBERTa large. Fourth, given that the proposed method requires training an additional policy network and GCN to learn the learning rate schedule, which adds additional difficulties to the already complex training of large models,  it raises concerns about the usefulness of the proposed method in practice.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper presents HCM, an approach for chunking a sequence of data into a hierarchical representation. The paper is clearly written, and the method is simple. It is not clear to me from the presented experiments how to compare this method to alternatives. I am concerned that the method as currently defined cannot generalize to real world data. If it s the latter, I believe much more empirical evidence of the learned representations  usage needs to be shown. Although I believe the motivating idea is very compelling, I don t believe this paper is ready for publication. I thank the authors in advance for their response, and am also interested in seeing other reviewers  thoughts.<|endoftext|>They also provide some examples of how this algorithm can be applied to temporal image data or video. The paper is clear. I enjoyed reading about the relative sample efficiency of the classical algorithm vs the RNN, though I would have rather seen a fair comparison with a TreeRNN or some other system that involves latent tree structure, as well as a comparison to other classical dependency parsers. The overall problem I had with this paper was the fact that it is presenting a classic parsing algorithm but contains no citations to any work from the age of classic parsing algorithms. The problems they have with efficiency of their own algorithm are resolved by many statistical parsing algorithms. The general area of structured prediction is one that has a long history, and the authors seem not have a particular background in the problem space. There was also no discussion of non neural hierarchical algorithms for structured prediction on video (e.g., structured prediction cascades), which seems necessary in a paper with experiments on a non neural hierarchical algorithm for video. This paper is missing significant background on classic hierarchical structured prediction.<|endoftext|>This paper proposes a method for learning representations of non  i.i.d.data in terms of hierarchical sets of chunks, inspired by cognitive theories of grouping by proximity. These sets are assembled over time from the initial set of primitive data points by finding correlations between temporally/spatially sequential primitives/chunks and appending to the set. Strengths:  This paper is particularly well written and understandable. The learned hierarchy itself, as the authors note in the conclusion, could be applied to down the line endpoints such as  causal learning. My main concern with all of this is the lack of actual baselines. It was not clear to me how the chunks were generated until I read the independence tests section in the appendix, and I think that this is too important to push out of the body of the paper. I like this algorithm and think it has potential.<|endoftext|>The paper proposes a graph learning model (HCM) for learning hierarchical chunks from sequential data. Strengths:The paper is very well written, with very clear, intuitive explanations for how their method works, and justifications for the authors’ design choices. The paper provides several well considered experiments to demonstrate the HCM method quantitatively and qualitatively. Secondly, the paper verifies that the learned model shows positive and negative transfer to similarly or differently structured heirarchical environments, as might be expected from a chunk learning algorithm. The connections to animal chunk learning are well thought through. Quantification of results was a bit lacking.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The experiments show that an agent trained using only input images performs worse than an agent trained also using clean abstract information about the properties of each object. The paper s central claim is ambiguous due to a missing operative definition of explanation and not adequately supported by experiments. The proposed odd one out tasks do not cast a significant new challenge with respect to existing tasks. In this regard, there are two major concerns:   What are explanations? This definition is missing. Many aspects should be expanded, such as the description of the RL environment and the training datasets for the NN.<|endoftext|>Thisarchitecture is showcased by solving different instantiations of the samerelational task ("one odd out", identifying the one object that stands outin a set of objects). Empirical results showcase the usefulness of learningfrom explanations and the ability of the proposed approach to do so. I really enjoyed reading this paper. Explores an important research direction but neglects existing literature. This is however false. 2021.which specifically studies learning from explanations in the context of relationalprediction tasks. "Right for the right reasons: training differentiable models by constraining their explanations." The literature on debugging NLP models using explanations has been surveyedin:  Lertvittayakumjorn, Piyawat, and Francesca Toni.<|endoftext|>This is a paper about sort one out identification. The goal is to emphasise that standard reinforcement learning is not optimal for this task, while adding (language) explanations to the learning phase might be highly beneficial. Each experiment is used to advocate a specific point such as the effect of the explanations on accuracy, robustness wrt correlated features, and other more detailed analyses. The paper is mostly a position paper supported by dedicated experiments in specific setups intended to support the various claims. Similarly the discussion about the related works seems to be quite detailed. The arguments to support the claims are mostly empirical, but restricted to a specific simulation architecture.<|endoftext|>This paper investigates the use of explanations during the training of an agent for downstream tasks. The investigated tasks are variants of the odd one out setting, and the experiments demonstrate improved performance on this task (relative to training without explanation targets) for static (observational) and interactive (interventional) settings when the agent is trained with description based and/or reward based explanations. Clearly, the paper investigates many empirical setups, which in the end show that explanation targets are useful in various settings.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; ## Strength:The paper is well motivated, clearly written, and seems technically sound. The paper does a good job in analyzing the limitations of ratio matching in expensive computation and excessive memory requirements, and in describing how it can leverage the gradient of the energy w.r.t.the discrete data space to address these limitations. The idea is simple, but can potentially be very useful in making ratio matching more practical for learning discrete energy based models. ## Weaknesses:  While the paper demonstrates improvements over vanilla ratio matching, it lacks comparison with other types of methods for learning discrete energy based models. For example, maximum likelihood training with MCMC sampling (e.g.with Gibbs sampling or the more advanced Gibbs with gradients, which the authors acknowledge is a big inspiration for RMwGGIS), can be easily applied to the same problems, and should be included as additional baselines. While the improvements over ratio matching are nice, if RMwGGIS cannot outperform other readily available methods, then it picked the wrong method to improve upon, and its improvements over ratio matching cannot be justified. I am not convinced by the current arguments in the section **Better optimization? ** in Sec.3.3, and I don t think this point is verified empirically as the authors claim. As the authors mention at the end of page 2, the estimator given by Eq.(3)/(4) is consistent, and if Eq.(3)/(4) is minimized perfectly, the obtained model distribution will capture the data distribution exactly. The authors claim that RMwGGIS, with a biased estimator, performs better than ratio matching due to better optimization. Does that mean that the theta learned with the biased objective function (12) actually results in lower value for the objective function in Eq.(3)/(4)?I.e.somehow optimizing a different objective function (Eq.(12)) leads to better optimization of Eq.(3)/(4) than directly optimizing Eq.(3)/(4)?I would like to see this confirmed in experiments (which is a very simple evaluation). If it s really the case, I would like to see some analysis on why this is the case, since Eq.(12) is picked somewhat randomly, without theoretical justification as Eq.(7).If this is not the case, then there is no better optimization, which means Eq.(3)/(4) is not a good objective function, and the paper should rethink its foundation, or explain where the better performance comes from. While the paper is promising, given the two major concerns above, I don t think the paper can be accepted in its current form, and recommend weak reject. While the authors clarified certain points, after the discussions I still have concerns over the overall presentation. The authors have repeatedly tried to force their conclusions on me in their response. I want to emphasize that I still don t find the presentation to be convincing, and I believe the paper in its current form hasn t sufficiently analyzed an important point it touched upon.<|endoftext|>This paper proposes a more efficient version of ratio matching (RM) for training discrete energy based models. The proposed method subsamples the dimensions to use in the original RM objective and then uses importance sampling to reduce variance. The importance sampling distribution is based on a Taylor series approximation to the target energy function which approximates the minimal variance importance sampling distribution. The authors show that this estimator is considerably more efficient than standard RM and enables RM to be applied to training EBMs in higher dimensions. The method is evaluated using MMD. Strengths:Training EBMs on discrete data is a difficult task and few methods currently exist to do so in high dimensions. RM is an appealing option because it avoids the MCMC sampling typically used to train EBMs at scale. Beyond this, the method appears correct and valid. Weaknesses:The main weakness of this work is in its empirical evaluation. Evaluation of EBMs is difficult, but in small scale settings likelihood can be estimated with AIS. Likelihood based evaluation would allow comparison to autoregressive models or VAEs, for example. I am concerned with the comparison to Gibbs With Gradients training. How was this done? If so, what were your buffer size, refresh rate, and the number of steps per training iteration? I am not convinced by the scale of the experiments as well. Prior work [1,2] has successfully trained discrete EBMs on datasets with thousands of dimensions. The results in this work are much smaller scale. If so then that is certainly a weakness of the method and it should be mentioned. You could compare results to the MNIST/Potts/Ising training experiments in [1] (no need to do all, but at least one experiment demonstrating scaling ability would be good to have). This would give a better context for the performance of this method relative to other recent approaches. Post Author Response I thank the authors for their response to my feedback. I think they give additional clarity on how the method performs and scales to higher dimensional data. The paper proposes an interesting and novel variant of RM that enables it to get around the memory/compute constraints of the original method. The method appears valid and appears to work but the empirical evaluation is lacking likelihood evaluation, experimental details, comparison to recent approaches and does not demonstrate that the method can scale to relevant problems. Unless these issues can be rectified, I cannot argue for the paper s acceptance.<|endoftext|>The paper propose a Monte Carlo approximation for the expensive ratio matching method for learning discrete energy based models. The key idea is to first write the ratio matching objective as an expectation wrt a uniform distribution, then use importance sampling for a more efficient estimation, where the optimal proposal distribution can be approximated by the gradient of the energy wrt the data space. Empirically, such an approximation shows even better results than the original expensive objective, possibly due to regularization from stochasticity and more focus on neighbors with low energies. Strengths:   The paper is well written and contains detailed method derivation and discussions. Overall, it is easy to follow. The method derivation is sound and the proposed solution is a reasonable way to alleviate the expensive computation/memory requirement of the original ratio matching approach. Although motivated from computational perspective for fast approximation of an expensive quantity, the proposed method even achieves a better performance than original method in most tasks, which is a surprising result and achieves both computation and performance improvement. Weakness/Questions:  The proposed approach seems to heavily rely on the assumption that the data is binary, which is not applicable to more realistic data such as natural language modeling (or even graph with different types of nodes and edges?). Indeed, an efficient and effective generalization of ratio matching to non binary discrete data seems more challenging, interesting and useful. The taylor approximation in eq (8) may not be accurate enough since the difference between x and x_ i is not close to zero. So the approximation error might be larger depending on the structure of the energy function (e.g.non smooth) and the number of dimension (for d 2, x and x_ i may be very different in the sample space). In table 1, there is no standard deviation calculated from multiple random seeds. Also why biased version is better than unbiased version in most experiments? (I suppose using Gibbs sampling?) Potential issue with Table 2: I think you are using the same s (number of proposal samples) for various data dimension from 32 to 2048. However, we typically need larger s for larger dimension d to ensure good performance, and in this case, the speedup/memory saving may not be that large? All experiments are "toy" data, possibly because the method is limited to model binary data. Please see the detailed comments section.<|endoftext|>The authors propose a new gradient based importance sampling scheme, applied to learn parameter in energy based models (EBMs). Some point must be clarified. How to draw samples from your proposal n(x)? this is the main issue in your method. Please clarify the relationship with the papers:V. Elvira, L. Martino, D. Luengo, J. Corander, "A Grandient Adaptive Population Importance Sampler", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2015. The paper seems to contain interesting material, however the degree  of novelty seems to be a bit incremental.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper proposed a transformer based routing module to improve the routing network for multitask learning. The main contributions of this paper can be summarized as follows. 4.Although the proposed approach outperforms the routing network baseline, the paper lacks the performance comparison with the SOTA MTL work. How to apply the routing module to all the layers is not introduced in the implementation.<|endoftext|>The results for routing networks for MIN MTL in table 1 (35.9%) seems to be inconsistent with the original paper (which presents an accuracy of ~57%   original paper figure 7.) This paper does not compare with any other models, other than routing networks. Multi task learning is a widely studied field, and thus many different approaches have been proposed to address this problem. Although the paper identifies 3 constraints with the original routing network, it is not clear why these constraints are limiting.<|endoftext|>The paper proposes a limitless routing network to route the instance in a certain task through a couple of modules in the multi task learning scheme. And modules are not limited to a certain depth and can be connected in different orders, which is called as limitless routing network. The idea is novel and interesting. Second, they do not compare to many SOTA MTL methods, such as residual adapter [A], cross stitch [B], ASTMT [C], MTAN [D] and etc to show their performance. [A] Learning multiple visual domains with residual adapters[B] Cross stitch Networks for Multi task Learning[C] Attentive Single Tasking of Multiple Tasks[D] End to End Multi Task Learning with AttentionThe proposed method looks novel and reasonable but the experiments cannot well support the effectiveness of the proposed method.<|endoftext|>This paper presents an improved routing network for multi task learning by removing constraints that are applied in the existing routing network. More thorough experiments should be conducted to make the paper stronger. I have some concerns on this paper; the novelty of the proposed method and a limited set of experiments. However, it is incremental because the main structure is based on the original routing network, and on top of it, they add more embeddings not to use the previous module with a fixed output size.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The authors also introduce two new fairness metrics BFRR and BFAR, which are more security demanding and  interpretable. Under BFRR and BFAR, we do not compare across multiple sensitive attributes, only the worst performing class and the best performing class. It would be good to give such figures if you are claiming it is a fast post processing method. Also, can you provide an intuitive explanation for the role of $\kappa_0$ and $\kappa_1$ with respect to the ratios? Can the BFAR and BFRR discussion be applied to Ethical Module other than a grid search (which suggests a weak link if this is the only use)? How so?I also find it surprising that the authors did not compare with other post processing methods in the experiments. Without them, the paper in the current state is difficult to accept even though the idea is technically interesting.<|endoftext|>This paper proposes introducing a shallow network used as apost processing stage for face recognition models. The new module is referred to as "the ethical module". The ethical model is usedto mitigate gender bias. The performance of the model is also verified on several image datasets. I m curious to understand, if the evaluationwould involve a "deployment scenario", in which a dev/eval split setis used to choose a particular combination for K0, K1, how would theresults look for the test set. The reason I m asking this is that theresults are not improved overall for all combinations of K0,K1 and itis not clear what would happen in a realistic setting. The adaptation of fairness metrics to the face recognition setting isinteresting. The evaluation of the method proposed has some gaps thatlowers the quality of the paper.<|endoftext|>However, the fairness metrics that the paper suggested are only empirically connected to the loss design. Strengths:  The paper proposes an effective post processing ethical module, which helps to improve the fairness of the pre trained face recognition models. Thus, I believe that this paper is currently not good enough to accept. Weaknesses:<Algorithm>     Several design choices are unclear. It seems that the current algorithm just adjusts the influence of each group in the loss function through a grid search on hyperparameters. Moreover, the effects of the hyperparameters $\kappa_0$ and $\kappa_1$ are not clear. <Experiments>  The comparison with existing fairness algorithms is missing. Thus, it would be better if the proposed algorithm is tested on other scenarios as well. After reading the authors  response: Thanks for your answers and more works on the experiments. I understood that the vMF loss is an appropriate loss function for face recognition, and there is some link between the groups in the data and the vMF loss.<|endoftext|>This paper proposes a post processing mitigation method, termed as the Ethical Module, to mitigate gender bias of Face Recognition models. The ethical module is a three layer shallow MLP built on top of a frozen pretrained model, with the goal to correct the biases that could exist in the embedding space of the pretrained model. 2.The proposed post processing module is simple, yet effective. 3.Two new metrics are proposed to help evaluate the fairness of Face Recognition systems. It is thus difficult to evaluate the relative effectiveness of the proposed model. It would be interesting to know the mitigation performance for other pretrained models. The proposed ethical module and the way to train it is novel. Besides the paper is well written.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper studies the sample efficiency of the Q learning algorithm in both tabular and linear setting. In this paper, the authors study the sample efficiency of the Q learning algorithm, which is a popular approach with broad applications in RL. Assumption 3 ensures that the Markov chain induced by the policy $\pi$ can quickly converge to a stationary distribution after $O(\tau_{mix})$ steps, and Assumption 4 indicates that the stationary distribution has a good coverage in the feature space. Besides, I remember that the lower bound proposed in Azar et al.(2013) is proved in the generative model setting without such assumptions. More comparison between the tabular results in this work and that in [1] is necessary in the related work section. The parameter $\kappa$ may implicitly depends on the feature dimension in the linear setting. 2.Several techniques in this work have also been used in the algorithmic design and complexity analysis of previous related results. It would be better if these relevant references are added in the paper. Besides, more discussion on the related works needs to be added.<|endoftext|>The paper provides sample complexity bounds for a Q learning based algorithm with target Q and experience replay. The analysis relies on common heuristics of experience replay buffer and using a target network given these some theoretical grounding. Much of the paper is in the appendix (all proofs, the experiments). 4.I m missing some understanding regarding the reverse experience replay. But why do we need the buffers to be independent? Is this just as a technicality to make the analysis easier? I think the paper provides interesting theoretical results and analysis. The work is not groundbreaking but is a solid advancement in its niche.<|endoftext|>This paper studies the convergence of Q learning with two popular empirical heuristics (a) online target learning and (b) experience replay. However, I am not very convinced with the significance of the results or techniques. The paper does not study Q learning as it is, but uses a fixed policy to remove correlations between Q estimate and actions. I think that the formulation of (reverse) experience replay or online target Q learning should be more explicitly described before Algorithm 1. Assumption 3 with Assumption 4 is quite strong and crucial for the analysis. With this assumption, I think the analysis should be similar to previous analysis on Q learning or SARSA (e.g., Zou et al., 2019). However, considering the theoretical nature of the paper, I think the contribution of this paper is marginal given a long line of work in the convergence analysis of practical RL algorithms.<|endoftext|>This paper connects Q learning with online target learning and reverse experience replay and also obtain the result for linear MDPs. Those results are the first of its kind and are shown to be near optimal in their respective regime. In particular, for asynchronous setting the Q REX has order $\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1 \gamma)^{4}}$ and Q REXDARE has order $\frac{\max \left(\bar{d}, \frac{1}{\epsilon^{2}}\right)}{\mu_{\min }(1 \gamma)^{3}}$. Especially, the new algorithm Q REX achieves the sample complexity $\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1 \gamma)^{4}}$ and Q REXDARE achieves complexity $\frac{\max \left(\bar{d}, \frac{1}{\epsilon^{2}}\right)}{\mu_{\min }(1 \gamma)^{3}}$. Although Q learning is known to be sub optimal, the variant of Q learning are optimal in a couple of setting. I do notice the there is a theorem 1 which holds for linear MDPs, but I cannot see whether this is optimal. (By the way, the paper did not include the related literature [1],[2])[1] Zhang et al., Almost Optimal Model Free Reinforcement Learning via Reference Advantage Decomposition, NeurIPS 2020,[2] Yin et al.Near optimal offline reinforcement learning via double variance reduction, NeurIPS 2021. 2.Another major weakness is the current algorithm can only guarantee Q function learning, which cannot guarantee $\epsilon$ optimal policy with the same complexity since it is known $\epsilon$ optimal Q function learning can only imply $(1 \gamma)^{ 1}\epsilon$ optimal policy. Since the theorems are correct from my point of view, I will choose weak reject.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper studies two problems plaguing graph neural networks (GCNs/GNNs): 1) oversmoothing of GCNs/GNNs; and 2) GCNs/GNNs often do not yield good performance on heterophilous networks. Based on these theoretical results, the authors develop a generalized GCN (GGCN) with negative message passing. As such, I have NOT confidence on the theoretical results obtained in the paper. However, despite making many simplifying assumptions, the "theoretical" developments are sloppy and hard to follow. The attempt to develop a theory to simultaneously address heterophily and oversmoothing issues in graph neural networks is important. For example, in the proof of Lemma 1, you go from eq.(13) to eq(14), but never explain what $\mathbf{x}$ is? Or does it refer to the input (node features)? I assume it is the latter, as in Lemma 2, you seem to use $\mathbf{x}$ to denote the input. This is in general NOT true. What is the relation between eq.(15) and eq.(16)?<|endoftext|>The contributions of this paper are as follows: 1) it attempts to establish a connection between heterophily and over smoothing by identifying common causes; 2) it establishes that the relative degree and homophily level have an effect on the misclassification rate; and 3) it also designs and presents a model that allows for negative interactions between nodes, which is shown to be robust to over smoothing and capable of achieving state of the art performance on heterophily datasets. It is a good starting point for tackling these two problems from a theoretical perspective. However, based on this theorem, they will not flip the labels because the parameters will never be negative. When we discuss the heterophily problem faced by the GCNs, we are mostly talking about the heterophily level of the graph. Unluckily, the homophily level defined in this paper cannot be translated into the homophily level of the graph. At the same time, the empirical experiments are conducted on a collective level (performance on the whole graph on average) instead of on each node.<|endoftext|>In this paper, the authors attempt to theoretically unify the cause of oversmoothing and heterophily. Specifically, the authors prove that under their proposed framework with certain assumptions, the level of homophily and relative degree may affect model performance. However, there are several issues the authors need to address. Both signed messages and degree corrections have been adopted to improve GNN performance and to solve a similar or the same problem (e.g., heterophily). I also have two questions for the evaluation: 1) It is good to see that GGCN performs well on heterophilous datasets. 2) The ablation study results in Table B.1 seem to be not aligned with the results in Table 2. I suggest the authors to further explain the difference between these results.<|endoftext|>The paper analyses the GNN over smoothing problem in terms of node degree and homophily level, suggests two correction mechanisms, and empirically shows that they help in various experiments. ( )  The paper is there to present the work and should be self contained. Although the theoretical setting is slightly different in this paper, the novelty of the proposed solution is limited. A closely related recent work is not mentioned/discussed in the paper:Lim et al.Large Scale Learning on Non Homophilous Graphs: New Benchmarks and Strong Simple Methods, NeurIPS 2021(other)Oversmoothing is particularly a problem in dense graphs, e.g., DDI data. I wonder if the authors have any insights (from other experiments or similar) that the theory about the nodes with varying degree holds also for such data sets.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper study the long time behavior of heavy tailed SGD with gradient clipping. Beautiful theoretical analysis and insightful numerical experiments are provided. Very insightful synthetical experiments are provided to justify the intuitions and theory. Inspired by the above analysis, the authors proposed two variants of SGD with injected heavy tailed noise + gradient clipping. These modified SGDs are expected to converge to flatter minima, thereby generalizing better. It is possible that SGD with structured noise can avoid those sharp minima (completely). The authors should disentangle the effect of noise magnitudes and noise structures, and state clearly what is concerned in this paper. For example, the stability driven escaping can tell us that, in Figure 1, GD with a relatively large learning rate never converge to the sharp minima: m1, m2. ### Other comments  In Figure 1, please make it clear in the caption that where the SGD starts from. This claim seems wrong to me. In Section 4, "Contrary to the report in (S ̧ims ̧ekli et al., 2019a), heavy tailed noise may not be ubiquitous in image classification tasks.". Why is there this contradiction? This also contradicts the synthetical experiments, where the heavy tailed SGD converges to flat minima more likely than light tailed SGD.<|endoftext|>Their analysis in the univariate case shows that gradient clipping in the heavy tailed gradient noise (almost) eliminates the algorithm s tendency to stay at sharp minima. The authors support their analysis with synthetic experiments. The authors then conduct experiments on real data where they add heavy tailed noise to the gradient, and clip it afterwards. I find the authors  analyses and results interesting and well presented. I think it has potential to improve our understanding of the generalization characteristics of SGD trained networks. However, I have a hard time understanding the authors  characterization of recent results in the literature (or lack thereof), which also informs their experiment design. My point can be most dramatically made by drawing attention to the choice of baseline that the authors present. While the recent theoretical and empirical findings in literature emphasize the relationship between learning rate, and tail index and/or generalization, the authors somehow base their methodology and experiments on the supposed absence of heavy tails in gradient noise in image classification tasks. This is not necessarily true, and this fact is well documented. Given this fact, the fact that the authors present the baselines as thin tailed noise algorithms (without any detailed analysis of learning rate / batch size and their effects on generalization), as well as not estimating the tail index of these noises is surprising. Their characterization of the recent literature, and the experimental design based thereupon seems to need more attention.<|endoftext|>The paper studies gradient descent with injected power law tail noise. The work shows that in the infinitesimal learning rate regime, the heavy tail noise can cause GD to not to converge to sharp minima. 1.The title seems inappropriate. This work, however, only studies GD with injected power law noise. One of my main objections is that the paper assumes finitely many minima, yet, neural networks, both underparametrized and overparametrized, should have infinitely many minima, and the fact that the minima of neural networks are degenerate makes it inappropriate to apply a transition graph analysis. This then makes me think that the theoretical analysis is not relevant for deep learning3. This amounts to a continuous time approximation, and I think is a crucial limitation of the theoretical results. The theorems reveal nothing about the behavior of SGD at a finite learning rate, which is the actual regime that SGD is run in practice. I am unconvinced by the experiment section. I would ask for evaluation on at least a modern ResNet to demonstrate the effectiveness of the proposed method  Even if evaluated on ResNet, I still think it would not suffice because it is quite easy to improve a vanilla model; I would really want to see being able to improve some state of the art results for the paper to be experimentally convincingOther important questions (but may not constitute reasons for rejection):1. It has been found that the power law index of SGD noise crucially depends on the learning rate of SGD (see https://arxiv.org/abs/2106.02588, or https://arxiv.org/abs/2105.09557). How does this fact affect the theoretical results of this work? The experimental evaluation is weak because it uses outdated architecture and because the result only improves on badly performing vanilla training strategiesTherefore, taken as a theoretical paper, I find the theory limited and irrelevant.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper focuses on understanding the tail behavior of normalizing flows through a mathematical and statistical way. 3.Well written and easy to followThe paper is well structured and easy to follow. Although it is argued by the authors in the potential work, I still believe the comparison is necessary. 3.Experiments need to be improved with large scale and high dimensional datasetsCurrently, only synthetic toy examples are provided to demonstrate the performance. The paper provides a strong theoretical insight but the experiments and baselines are weak.<|endoftext|>The authors propose to learn the tail behavior by learning flows that match the tail properties of the marginal distributions. They achieve this by using a source distribution consisting of marginal distributions with tail properties matching the target distribution. It can be shown that the error in modelling a probability density can be bounded arbitrarily well by learning the density properly on a bounded subset of the support of the density. Thus, what are the drawbacks in the model if it is unable to capture the tail phenomena present in the problem? However, I believe a more thorough discussion about the implications of these results (both in general and particularly for normalizing flows) and some recipe or ideas to alleviate these problems will help the paper tremendously. Overall, the paper studies a pertinent and difficult problem.<|endoftext|>In particular, they propose a new type of normalizing flow (NF) that can learn marginals with mixed tail behavior. The paper provides a more general definition of heavy tailedness that extends existing work and uses it to construct their mTAF method. The experiments were probably the weakest aspect of the paper. Also, the evaluations conducted in the experiments did not clearly demonstrate the advantage of mTAF over existing methods. **Questions:**  I’m also curious if the method performs worse relative to conventional flows (e.g.MAF) when the distribution in question is only light tailed or heavy tailed. Additionally, the vanilla baseline has pretty high variance and sometimes seems to perform on par with TAF   would the authors elaborate upon this point?<|endoftext|>Both the theoretical and the algorithmic parts are a straightforward extension of the analysis and methods introduced in the paper "Tail adaptive flows" (Jaini, 2020). The paper is very well written and it serves as a good introduction to both normalizing flows and heavy tailed distributions. The present paper does not contain major new ideas.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 3; The paper presents a novel image texture model that is a hybrid of the traditional wavelet constructions, and the more modern machine learning constructions (incorporating only ReLU nonlinearities on the wavelet basis), proves that this class of models is actually akin to phase harmonic functions and that these are sufficient for capturing texture statistics, and claims to show competitive performance with state of the art with significantly fewer parameters. Proving the relationship between the different sets of texture models, with the additional proof that ties ReLU s and phase harmonics together is nice work and a strong contribution. This paper is not suitable for ICLR as nothing within the paper is learned from data, these are hand constructed (and well though through) representations that are similar to those found in learned representations but are not here learned. This is not a problem per se, it simply doesn t fit with the conference themes and as such may not be found useful by most of the conference attendees. This is especially apparent in the color images which have more of examples of the name badge in the corner. 4.The authors state parameterizations of comparison models on page 8 that are at least partially incorrect (here stated that portilla simoncelli has ~7000 parameters when in reality it has 701 parameters, an order of magnitude difference). For these reasons above I am inclined to reject this paper for ICLR, and suggest the authors find a more suitable conference for this work, as well as reanalyzing some of the results and rethinking the overall presentation of the work.<|endoftext|>This paper presents a new image representation model based on wavelets and non linear rectifiers that allows to synthesize complex geometric textures with a better visual quality than previous wavelet based models. They also show that the PS model (Portilla & Simoncelli 2000) is a particular case of the wavelet phase harmonics based (WPH) model. Both methods underline the importance of statistical dependencies between wavelet coefficients across scales. **Positive points**: The results presented in the paper are very interesting because they shed light on what exactly are CNNs learning on images, since the presented model can produce similar results to state of the art CNNs with a much smaller number of parameters and a fixed filter structure (wavelets). In this sense, this paper shows that the rectifier wavelet operator is **Negative points**: The relationship with patch based methods is not discussed at all. Even thought the proposed algorithm lays within the "statistics based methods", in the numerical results it should be compared to other family of methods such as exemplary based models (see Raad  et al 2018 for examples) The paper presents the results on texture synthesis using a new image representation model based on wavelet phase harmonics.<|endoftext|>This paper provides an attempt to characterize wavelet based texture synthesis models contingent on the number of parameters and the nature of the parametrization. * Not enough highlight on the number of parameters of the wavelet based model. This should be emphasized more in Table 1, or in an extra table as it’s superficially mentioned at the end of section 4.1. The exploration of how the number of parameters affects the quality of the synthesized stimuli is a pivotal part of this paper that has not been highlighted enough (the results are there, but the presentation needs to improve). What texture constraint has varied, such that the model all of a sudden breaks down or succeeds for some of these other images that are different from the canonical textures shown in the texture synthesis literature. Overall, the paper is well written, it provides a good mathematical overview of wavelets and also cites the relevant literature on this subject. [I apologize to authors and/or reviewers if they have seen me toggle back and forth between 6 and 8. A solid first step, but more qualitative insights are needed given to make this paper shine as much as it should. Weak Reject with a possibility of increasing my score during rebuttal to accept.<|endoftext|>This work proposed a texture synthesis framework using the rectified wavelet coefficients. The paper claims that the proposed method cand achieve similar quality with the VGG feature based method (Gatys et al.22015) and random filter based method (RF, Ustyuzhaninov et al 2017) and gets better quality than PS (Portilla & Simoncelli 2000, while requiring less number of statistics than RF.This paper provides a detailed review and explanation of wavelet and how to use the rectified wavelet coefficients for covariance statistics matching based texture synthesis. However, I have several main concerns regarding to the comparisons with recent papers and the types of texture images this method can handle. 1. as the main motivation is trying to achieve good quality with as few number of statistics as possible, from figure 3 and figure 4, the quality of “VGG” seems to be better than the proposed method and also uses less number of statistics (much less for color textures with 177k vs. 320 and similar for gray texture 177k vs. 142k)2. After Gatys et al.2015 and Ustyuzhaninov et al.2017, there are more recent methods on texture synthesis which have achieved many impressive results. More examples of structure images and non stationary images are needed to validate the proposed method. The results are quite limited. It also missed the citation and comparisons with many recent works.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 6; This paper proposes a modification to the popular dropout scheme which enables networks to be resized without having to retrain them. Unfortunately, as it stands it fails to provide the level of experimental validation that would be required to make the case for the usefulness of the approach. MuJoCo baselines are also notoriously insensitive to network capacity above a threshold, and collapse rapidly below it. Any practical user of those baselines would hence either never use a model that s too small to solve the task, and be completely insensitive to parametric complexity above that threshold. Since the proposed scheme doesn t speed up training over using a large network, there is no practical interest in this scheme here.<|endoftext|>Leaving Triangular Dropout for convolutions or transformers to future work makes this paper less informative than expected. This paper proposes a method, called Triangular Dropout, to allow a fully connected layer in a network to have variable width at deployment, which is achieved by applying a lower triangular mask to the output of the fully connected layer. The research topic is of value and the proposed method is simple and very effective on toy examples.<|endoftext|>However, the paper falls short at explaining what gap it fills or what bottleneck it addresses that is missing in the existing literature. This paper is written in a comprehensive way it is easy to follow, and the description of the experiments and presentation of the results is clear. The main appeal of the proposal, in my opinion, is the simplicity of the technique and I do see potential uses, some of which are described in the paper, such as flexible selection of the amount of resources during inference time. Why is Triangular Dropout not analysed in convolutional layers?<|endoftext|>This paper proposes Triangular Dropout, a mechanism for training fully connected layers whose width can be decreased at test time, by using dropout masks with a particular structure at train time. The proposed approach compares favorably to previous methods when training autoencoders of varying bottleneck size on MNIST. Have you visualized the training curves of the autoencoder with Triangular Dropout? Have you trained the same network without Triangular Dropout to rule this out? Finally, the proposed method is used to measure task complexity in RL by using the minimum layer width required to solve the task as a proxy. The method is simple and should be easy to adopt by practitioners.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; On a technical level, the paper proposes two methods for Genetic Programming (GP) tree pruning, as well as a method for a population _restart_ that relies on both pruning methods. The aim of such approach is that in the second, or subsequent, GP runs, the GP algorithm starts from with an already somewhat good initial population of individuals that would allow GP to reach better solutions. Setting, related work and background. Table I, for example, it is not clear what exactly those values are... the average of the MSE of the 50 runs... ?<|endoftext|>Winning tress are random sub trees from GP solutions after simplification and a novel pruning strategy. The link with the lottery ticket hypothesis (LTH) is intriguing. Apart from the results in Table 1, the evidence seems to suggest the winning tree idea is not good? In particular, although it s interesting to link to the LTH, I feel the link is very weak. Presentation can be improved. I found the motivation of this paper is weak as its link to the LTH is not clear.<|endoftext|>This paper discusses the influence of simplification and pruning in GP for regression problems. By doing so, although the obtained expression can be further simplified, the fitness of the expression might be dramatically deteriorated. Especially, it seems that the influence of winning trees on the performance of GP relies on the tree depths. 7.What kind of results in Tables are? Although the idea seems interesting, this paper is not technically sound.<|endoftext|>This paper presents two optimizations to standard GP: a tree pruning with introduction of operation neutral nodes, and a restart strategy reusing a population of best trees from past runs. How would this be fair? That is right, there is not a single figure in the paper. This is exacerbated by the experimental results being presented as 6 (six!)
Reject; rating score: 5; rating score: 5; rating score: 5; The paper presents the language complete ARC to study how human language can affect abstraction and reasoning capability. The paper is well motivated with the goal of studying the gap between communicating natural programs to humans and machines. It s a critical problem to be addressed. 2.The LARC dataset construction is by collecting verifiable instructions from a describer and builder, which ensures the task finish rate. 3.It uses existing program synthesis techniques to study how to execute the natural programs as humans. I would like to see a new learning method to tackle this challenge.<|endoftext|>The paper extends the ARC dataset by adding human written instructions that they term as "natural programs". The extended dataset is called LARC. The linguistic analysis is interesting but I would have liked to see the results of more program synthesis systems especially the large models like Codex [1], GPT Neo [2], GPT J [3]. I felt that the clarity of the paper needs to be improved ( see concrete suggestions below). Just describing the results with these tags used directly in the sentences seemed a bit abrupt to me. 6.I found the brief section on suggestions quite interesting. Personally, I would like if the authors could expand this section with more observations drawn from other program synthesis systems and evaluation settings. However, I feel the paper needs some work in terms of improving clarity as well as more extensive evaluation of large language models.<|endoftext|>Authors construct a dataset of natural language programs for solving ARC tasks by using a population of Mechanical Turk workers to validate that a turker can correctly communicate how to solve an ARC task to another turk purely via the natural language used. Fortunately the authors show that in 88% of the cases human descriptions of a solution can correctly communicate a solution to another human. The dataset appears to be adequately constructed, however a lack of downstream applications and proof of utility make the effort harder to contextualise, and the dataset can be seen as incremental over the original ARC effort.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This result is still novel and does generalize to the underdetermined case, but I don t feel great about the paper not giving enough credit to [1] in the context of this result. In particular, this is nothing but the least norm solution for underdetermined equations. It is more that all but the last layer are jointly learning a basis function given which it is very easy to fit the value function using linear function approximation. I believe authors will address my concerns, so I like to see this paper accepted.<|endoftext|>This paper studies the convergence properties of three classical value estimation algorithms (TD, FVI and RM) under over parameterized linear case. It also proposes an generalization bound for FVI. It is not clear whether this dependency will break the results in this paper. Is that possible to check whether these two regularizers are also effective in more complicated environments? My concerns have been well addressed and thus I increased my score. This paper proposes a novel convergence analysis of three classical value estimation algorithms and reveals their implicit bias.<|endoftext|>In this paper, the authors consider the overparameterized linear representations of TD, FVI, and RM. 1.This paper is concerned with the overparameterized linear representations of TD, FVI, and RM. On the downside, the linear case is restricted, and technical analysis is not very novel. I have some high level comments as follows. This projection operator is similar as the forcing \theta to lie in the row span of M. Is my understanding correct? In the theorem statement, it is mentioned that the algorithm will not converge if the norm is large. This is a theoretical paper about the overparameterized linear representations of TD, FVI, and RM. This is an interesting question.<|endoftext|>To my opinion, the contribution of this work is not solid enough, even some results might be wrong. This paper is claimed to understand over parameterized model in OPE. Thus, I would be much appreciated if the authors could have a clear explanation of the motivation. Thus, the recursive expression (43) is also wrong. 2.Appendix A.7 presents two lemmas to control the expectation term in Corollary 2. There are a number of grammatical problems and the language is not precise enough.
Reject; rating score: 1; rating score: 1; rating score: 3; This paper presents Multi Distribution Learning, exploring the effectiveness of Data Augmentation to prepare for Distribution Shift. Pros:The expression of this paper is okay. Cons:The novelty and contribution of this work are marginal.<|endoftext|>In this work, the authors present a data augmentation method to address the data distribution shift problem in a multi task learning framework. For example, there is no literature study, no evidence to show the limitation of the literature, and no comparison with state of the art. Many important comparisons and studies are missing. No technical novelty.<|endoftext|>2.The introduction and related work contextualize the authors’ method with the most recent work in this domain. Distributional Generalization Metrics  > distributional generalization metricsFinally, there are several latex/formatting issues:1. I think the concept of choosing dataset augmentations from a predefined set to create a model which is robust to distributional shifts is very interesting and promising.
Reject; rating score: 5; rating score: 6; rating score: 6; This paper investigates self paced reinforcement learning (SPRL) which is a type of curriculum based RL. Hence, I believe the paper needs some more work to warrant acceptance at ICLR. The authors also compare their approach with other automatic curriculum algorithms. 4) Even if the proposed method is a natural extension of SPRL using the Wasserstein metric, this hasn t been done before (as far as I know) and it is well motivated, making it a reasonable contribution at ICLR (in terms of novelty). 4) The paper seems to be missing experiments with ACL GMM on Maze and Point Mass.<|endoftext|>The authors try to alleviate the need for parametric distributions (namely a Gaussian) from the self paced RL framework (SPRL). They compare SPRL using a KL divergence metric between a discretized proposal distribution and the target as well as a Wasserstein distance metric between particle based approximations. The goal spaces in this paper all seem rather low dimensional and simple. It would be more promising to me if there were experiments with e.g.harder mazes (more invalid regions). Is it due to the choice of starting distribution? It s a bit disappointing that the experiments with this environment are with a recorded trajectory. I enjoyed the paper; however, because I m not familiar with the math I will defer to other reviewers on their assessment on this front.<|endoftext|>I recommend that this section should be extended, even though not all work on CL in RL may have exactly the same focus of solving specific hard instances or generating curricula through generating instances as SPRL. Previously, the target distribution was limited to a Gaussian, which the authors now extend using Wasserstein barycenters. Or why a comparison against other baselines made more sense? The change in metric itself makes sense, as demonstrated in the paper, to increase the range of problems to which self paced reinforcement learning (SPRL) can be applied. Additionally, the nature of the work is rather incremental with limited impact and novelty. Would it be fair to say that for unimodal cases, there is no large performance gap between the two?
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This work introduces the multi label box model (MBM), a method for multi label classification that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (trainable Venn diagrams based on hyper rectangles). the paper is relatively well structured. The method is well framed in the existing literature. The method is very well motivated in theory. The baseline models for comparison seem reasonable. It would be nice to see which results are statistically significant in Table 2.<|endoftext|>In this work the authors propose a new model for multi label classification problems. In their model, the authors represent the labels as boxes and they use neural networks to embed the input datapoints in the same space. The model has been tested on 7 publicly available datasets, and evaluated using three different metrics, namely, CMAP, CV and and MAP. 1.The authors should change the term “consistent” with “coherent” throughout the paper. Indeed, the authors use the word “consistent” to express the fact that the model is coherent with the taxonomy. 7.Would it be possible to also do MVM T? 12.Why aren’t the results in terms of AU(PRC) of MHM reported?<|endoftext|>The paper has mostly been written clearly. Weaknesses:The proposed method is a relatively straight forward application of box embeddings as developed in earlier literature (including e.g.Dasgupta et al., 2020a) into hierarchical multi label classification. While it is true that  HMCN does not try to enforce consistency strongly and focuses solely on predictive performance , it does not mean that HMCN could not be a strong competitor in the experiments. The experiments would benefit from more baselines. The proposed method is quite a simple step from existing literature, and there are several shortcomings about the text highlighted above.<|endoftext|>The problem tackled in this paper is that of multi label classification, where class labels form a taxonomy (i.e., a hierarchy), and the goal is to enforce the predictions to comply with the label taxonomy. The approach taken in this paper is based on box embeddings, where the idea is to view classes as boxes in the space, and apply a certain probabilistic semantics of box embeddings to eventually model taxonomic relations between classes. Weaknesses:  The novelty/originality is limited: box embeddings, probabilistic semantics of their intersections, gumbel boxes, bessel volume, etc are all adopted from existing literature. The overall setup is not well motivated. The paper is well written and interesting but limited in originality and novelty given works of Vilnis et al., Dasgupta el al. It is an interesting application of earlier findings to the domain of multi label classification, but it is lacking in motivation for the precise model choices.
Reject; rating score: 3; rating score: 5; rating score: 5; The focus of this paper is on membership inference attacks. More specifically, a framework for understanding the relationship between, success of membership inference attacks and information leakage is introduced. Therefore, it appears difficult to identify which are the main contributions in this paper. The authors do not provide any valide proof or formal justification for this assumption. To conclude, I believe that the work contains some valuable ideas which deserve to be further investigated (both theoretically and practically)  but in its current state, the contributions and the presentation of the results are too short in terms of exceptions to be accepted to be published in ICLR conference.<|endoftext|>I agree with the general sentiment of the paper, that having a framework for auditing privacy could be helpful. This goal is pursued by proposing multiple different attacks for membership inference. I also think that the paper needs to be much more clear in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the “report” on privacy. Then, many different factors, beyond just the success rate of the attacks are compared and reported with experiments. The first attack writes probabilities P(theta|D) as if we are aware of the learning algorithm.<|endoftext|>This paper proposed a hypothesis testing framework for membership inference attacks (MIAs). The notation of membership privacy loss is the same as in differential privacy and the proposed algorithms improves the utility of the attacks compared to prior works on benchmark datasets. The attacks based on distillation also seem noval and not discussed before. The authors also showed that the newly proposed attacks are better than shadow model MIAs. The hypothesis testing framework in this work is very similar to the threshold adversary in [1]. Minor: related works should be discussed in the main text instead of supplementary materials. I therefore recommend a weak reject.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The numerical results indicate that unsupervised score matching methods are well suited for inverse problems in imaging and yield superior generalization performance. Strengths:The overall idea   combine diffusion based generative models with inverse problems   is well presented and the paper is easy to read. The Dirac delta measurements distribution prevents any incorporation of real world noise typically observed in inverse problems in medical imaging. Clearly, the proximal map minimizes an energy that employs a quadratic penalization of the data fidelity, which implicitly leads to a Gaussian noise assumption of the measurement process. More details w.r.t.this regard would clearly improve the quality of the paper since the SDE perspective is an integral part of the entire work. Please, correct this. The paper misses many implementation details.<|endoftext|>The paper introduces a method to use score based generative model as a powerful prior when solving linear inverse problems in medical imaging. Compared to previous work that applied score models to inverse problems, this paper proposes a new conditional sampling approach. Instead, the authors propose to utlize the linear relationship between $x$ and $y$, and introduce a coupled stohcastic process $y_t$. The paper is well written and the idea is concise and novel. I think the biggest issue of the paper is lacking an motivation for the method. https://arxiv.org/abs/1905.11672The paper is well written, with a clean idea and strong empirical results.<|endoftext|>This paper provides an unsupervised approach to solve the inverse problem for reconstructing medical CT and MRI scans using score based generative models. Strengths: 1) The paper targets an interesting, important, but challenging problem in medical image reconstruction, which could be attractive to ICLR participants. 3) The experimental results demonstrate the effectiveness of the proposed method, which shows comparable or even better results compared to supervised methods. In the SDE model, is there a time step size that should be predefined? Although the paper has some limitations, it presents good technical contributions. Therefore, I recommend the weak acceptance for this paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper proposes DNN quantization with Attention (DQA), which uses a learnable linear combination of high, medium, and low bit quantization at the beginning. There are grammar errors and typos in the paper. There are many quantization works, both quantization aware training and post training quantization. For example, the work LQ Nets reports an accuracy of 68% with 2 bit weights and 32 bits activation with ResNet 18 on ImageNet, but DOA proposed in this paper only achieves an accuracy of 66.9%. Minor comments or questions:1. It might be better to report averaged results across several runs even if the convergence of the networks is not noisy. 4.In Table 1, it seems that DQA using SWAB consistently gives better results than the FP version.<|endoftext|>The paper states that the method can be used with several types of quantization methods. Eqn (2 3) and Figure 1. 2) The paper does not consier in the literature review techniques for quantizing neural networks [A,B,C] that, to my knowledge, are state of the art for quantizing popular neural networks. The paper states that it could be combined with any compression method, therefore in this context it would be worth using the same or similar quantization as in [A,B,C] and show how the method compares to these methods. In the case of this paper, I found that the introduction is actually more a related work than a formal introduction providing the motivation and rationale of the paper content at the core of the initial discussion. 5) The paper  mentions that the method could be used for quantization activation, but only addresses the case of weight quantization.<|endoftext|>This paper attempt to address a challenging quantization problem, i.e., low bit quantization. This work utilizes a learnable linear combination of high, medium, and low bit quantization at the beginning while converging to a single low bit quantization at the end of the training. In the quantization procedure, multiple quantizers and the corresponding attention matrices are adopted to fuse the quantized weights or activations. Cons:  More parameters are introduced in the training stage, such as α. More theoretical and experimental analysis should be given to study that. Multiple quantizers with different bit width are conducted in the proposed method, which will increase the storage and computation cost for quantization. Therefore, this comparison seems unfair.<|endoftext|>This work presents a training method for low bit network quantization. Almost all papers I know doing low bit quantization has better results than this one. On weight only 2 bit quantization with MobilenetV2:SAT: 66.8 (Neural Network Quantization with Scale Adjusted Training BMVC 2020)DeepComp 58.1 (Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.ICLR 2016)This work: 52.2On both weight and activation 2 bit quantization with ResNet18:PACT 64.4 (Pact: Parameterized clipping activation for quantized neural networks.arXiv 18)LQNet 64.9 (Lq nets: Learned quantization for highly accurate and compact deep neural networks.ECCV 18)SAT 65.5 (Neural Network Quantization with Scale Adjusted Training BMVC 2020)This work: 60.4 	Some technical details are not clear. The authors use a penalty term (equation 6) to regularize the attention weights of different bitwdith. However, it is not known whether all bitwidths in the network will be converged to the lowest bit which is the target.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; In order to eliminate the need for large training sets, one can consider a transition from (1) fully supervised to (2) self supervised methods, and then from (2) self supervised methods to (3) single instance reconstruction methods. In the context of accelerated MRI reconstruction which is considered in this work, the above categories translate into models that are trained based on (1) having access to a fully sampled dataset, (2) having access to a dataset of under sampled measurements, and (3) having access to only one under sampled measurement. The paper targets (3), that is proposing a zero shot learning approach for accelerated MRI reconstruction. The algorithm is based on the idea proposed in paper [1] combined with building a dataset for the given sample in order to eventually perform self supervised training on the synthesized dataset. [1] Yaman, Burhaneddin, et al."Self supervised physics based deep learning MRI reconstruction without fully sampled data." IEEE, 2020. ####################################################Strengths+ Overall, this is a very interesting paper with impressive experimental results. The proposed algorithm yields high quality reconstructions and the rationale behind designing such an algorithm is very well motivated by the authors. + The idea of self validation is also interesting since it mimics the presence of a validation set for training and prevents the network from overfitting. + The robustness investigations are appealing and very related to the problem considered. Moreover, from the robustness perspective, any effort toward single instance reconstruction is of great value provided that it results in more robustness. Regarding robustness evaluations, please see the reviewer’s comment below. Yet what is missing is a computational comparison among single instance reconstruction methods such as traditional sparsity based, DIP based, ZS SSL, ZS SSL TL. This is important since one of the critical challenges of single instance reconstruction algorithms is their inefficiency at the inference. Adding such an experiment would also make the paper even stronger. Table 2 in the appendix contains very interesting results. For instance, how come for ``trained on brain & tested on knee`` ZS SSL TL achieves 40.4 dBs, yet according to Table 1, ``trained on knee & tested on knee`` achieves 40.1 dBs? This is counter intuitive in that it suggests pre training a model on ``brains`` is marginally better than pre training it on ``knees`` if one wants to get a good PSNR on ``knees``! Table 1 suggests that DIP TL performs strangely poorly compared to PG DLR and ZS SSL TL. Does this mean that the pretrained network used for DIP TL performs poorly and DIP has not been able to improve its performance, and thus the low score has nothing to do with the DIP itself? However, (1) several concerns/comments mentioned above, and (2) the fact that the major difference between the proposed algorithm and the prior work [1] is the self validation step combined with dataset synthesis prevent the reviewer from giving a higher score.<|endoftext|>This submission deals with MR images reconstruction in a context where the raw data is under sampled. This data (which is in the Fourier domain) can be under sampled to accelerate the imaging exam and thus improve clinical workflow and it is thus a very relevant research topic. The training examples do not need to be (under sampled / fully sampled) pairs. While the paper is pleasant to read and rigorous it does not offer a significant improvement over ref [Yaman et al., 2020] which by the way is regrettably not included in the set of benchmark methods to which the submission is compared in the result tables of the experimental section. I am not sure to understand the argument w.r.t.transfer learning and single dataset reconstruction. The proposed approach also retrains the model on a new dataset with patient specific properties. I think that what the authors mean is that this dataset does not need to be supervised. But this is a built in property of the model from [Yaman et al., 2020] from which the authors are building upon. This might be a problem for diagnosis or for other trained ML based models that rely on texture features such as radiomics and take MR images as inputs. Is this a known issue in MRI reconstruction in general or in DL based reconstruction ? Pros :   the paper is well written and ideas are clearly explained and stated. Cons :   the contribution is too incremental compared to [Yaman et al., 2020] as it consists in re using the Fourier domain partitioning idea in order to have a validation loss allowing to detect overfitting and stop learning. patient specific training demands more computation time and resources than usual clinical workflow.<|endoftext|>This article propose a "zero shot" method for MRI reconstruction, which is a well studied inverse problem. Overall the paper is pretty good, but I think it does not clear the bar for acceptance at ICLR due to the lack of technical novelty. This self supervised learned network can then be used in a plug and play architecture to solve inverse problem with a variational approach, i.e.as if the learned denoiser were a Total Variation (TV) minimiser. Their denoiser can be improved in a transfer learning approach to benefit from a more complex network trained on similar images than those at hand, and fined tuned on the image to be reconstructedThe authors go on to apply their plug and play architecture to solve the MRI reconstruction problem. The method is based on the ideas of deep image prior, i.e.the ability of correctly sized neural networks to learn about the structure of a single image, sufficiently well for denoising tasks. The paper is well written and clear with great illustrations and only a few typos. The idea of zero shot learning is not new, in the particular context of imaging, it has long been known that the denoising problem could be solved by training on the very same image one would seek to denoise [3,4]. The range of solutions provided to solving inverse problems in medical imaging is too large to mention [5]. There are many aspects to using zero shot learned approaches for denoising, in particular the fact that their regularity is not established and so they can only be applied for a limited number of iterations, and the need to use early stopping. All of these elements are known and have been applied before to this problem. They do achieve better results than Senouf et al [5], thanks to a better TL regimen.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The work presents an interesting equivalency between a generative probabilistic mixture model and a winner take all Hebbian local learning rule based learning. The approach seems promising, especially the adversarial robustness results compared to a multi layer perceptron. **UPDATED SCORE**I would like to thank the authors for their revisions and modification based on my feedback. I would also like to mention that there have been works based on Hebbian/Non Hebbian Learning that outperform backdrop based  approaches in challenging scenarios such as continual supervised/reinforcement learning. There are other strategies for backprop networks such as implicit formulation (treating NN as a dynamical system) that improves adversarial robustness without adversarial training. * The equivalency between mixture model and WTA has been proposed in a previous work (Moraitis et al.2020) so the novelty on that end is quite limited. * The comparisons are only made with 1 or 2 layer MLP and the accuracy differences are not significant. It is not clear what the advantage of softHebb is. The performance seems to be poor on the F MNIST dataset. Discussion and experiments on learning with larger/more complex datasets need to be addressed. The approach is promising, but is limited in terms of novelty, experiments and the comparison with existing approaches that makes it difficult to assess the true potential of this approach.<|endoftext|>In this paper, the authors develop a theoretical framework to incorporate winner takes all (WTA) connectivity with Hebbian like plasticity which translates into a Bayesian generative model that can be used with generic artificial neural network (ANN) elements. Furthermore, it also trains marginally faster than a standard backprop multilayer perceptron (MLP), though its performance saturates at a lower accuracy. While these are interesting toy datasets, they are very simple to solve and solutions to them not always generalize to more challenging problems closer to real world applications. For example, the authors claim that the number of true causes for the data, which in their single layer implementation translates to number of neurons, can be chosen using common heuristics from cluster analysis. Can the authors indicate what this number would be for the CIFAR 10, TinyImageNet or even ImageNet datasets? While the robustness to Gaussian noise is undisputed (though it would be interesting to evaluate robustness to other types of random noise), the same cannot be said about the PGD adversarial attacks. When claiming improvements in robustness to gradient based attacks, the burden of proving that the attacks have been properly performed is on the proponents of the defense. PGD has multiple hyperparameters, such as the number of the attack steps and the attack step size, that need to be optimized for different models separately. The authors make no reference to the choice of these hyperparameters and whether they tested different combinations to ensure that the attack was optimal. Also, it has been shown that the activation function greatly affects adversarial robustness (Xie et al 2021), does the standard MLP use the same activation function of the SoftHebb? In the paper, I think a few visualizations are missing. This way, it gives the impression that the authors are cherry picking the baselines to compare based on the benchmark. How much better is the SoftHebb model over the hard WTA in terms of robustness? Also, the model description should be expanded either in the main text or supplementary. While the authors focus their analyses on the number of epochs and training examples, it is not clear whether the training times and inference times of the different models is the same or deviate considerably, as well as the memory requirements to train/run each model. ## Minor points* The authors point the dependence on feedback by standard ANNs as a criticism. However, there is a high degree of feedback in the brain. This study is definitely an interesting take on incorporating biologically plausible components in an ANN design. However, the proposed model contains several limitations in terms of its expansibility to more challenging problems and some controls are missing to ensure the validity of the main claims, particularly those related to the robustness of the model.<|endoftext|>The authors pursue a formalization of WTA networks as optimization. In the paper, they show that WTA networks that compete via soft max (approximating possibly plausible biological lateral inhibition) and are updated by Hebbian learning are implicitly minimizing the cross entropy between inputs and layer activation. Strengths:  The formalization of (a type of) competitive Hebbian learning from an optimization perspective is interesting and useful. The experimental investigation on the effects of noise and adversarial attacks in the proposed method vs standard gradient based methods is very nice. Weaknesses:  Evaluation is a bit weak (more below). Evaluation is performed only on two simple datasets, MNIST and Fashion MNIST. The models evaluated only have 1 or 2 layers, which are extremely simplistic and limited. It is also not clear whether and how well the method would scale when a deep architecture is used. The paper has both valid strengths and weaknesses. The paper could be a clear  accept  for me if the authors would at least present results with more standard MNIST architectures, like conv+pool layers and/or 3 4+ layers. Additionally, it would be nice to see results of the method at least on CIFAR 10.<|endoftext|>The paper wants to provide an optimization theory for WTA networks that it claims to be missing. More precisely they consider soft WTA networks that are implemented as ANNs.They apply their model to MNIST and Fashion MNIST. In addition some robustness to adversarial attacks is demonstrated. The paper mentions prior work in the same direction, such as (Nessler et al., 2009), but claims that a Bayesian theory for WTA learning has been missing. This claim is not correct. The incomplete discussion of the state of the art makes it hard to identify the innovations of this paper. Although the paper states that is aims at biological plausibility, its synaptic plasticity rule (8) is not local: the rule for adapting the weight from neuron i to neuron k requires knowledge of the current weights of all other synapses to the same postsynaptic neuron k (via the term u_k, see (4)). A nice aspect of this paper is that it includes empirical studies of the noise robustness of the model. But it only compares the robustness of their model with that of MLPs, rather than with other unsupervised learning approaches. Some of these points have been improved in the update. The paper addresses an interesting topic. But it is not clear what its innovations are, and whether it can be implemented with a local learning rule.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper proposed a residual style architecture namely STRIC for multivariate time series (TS) forecasting and anomaly detection, by first decomposing TS to trend, seasonality, and irregular components as residual, then extracting features from the residual using TCN, and finally introducing a likelihood ratio estimation method. Experimental results show that STRIC can provide interpretability for TS forecasting and outperform some existing anomaly detection models. For example, as pointed out in [2], Yahoo dataset is not preferred for comparing AD solutions. 2.TS decomposition itself is a well studied research problem and it is essential to compare the proposed architecture with existing techniques (e.g., [3]) to obtain the residual. Secondly, the model is not described clearly. Vol.33.No.01.2019.While some of the proposed techniques seem to be interesting, the paper is very difficult to follow and the experimental results are not convincing.<|endoftext|>·	An interpretable stacked residual framework is proposed for time series anomaly detection with interpretation on seasonality and trend of the target time series. o	The selected datasets has been shown as flawed benchmark [1] and can be easily solved with few lines of rules. However, it is very likely that the three components are not linearly dependent to the resulting time series. Providing more justifications on using addition to model aggregate the information may be helpful to eliminate the concern. o	There is no empirical evaluation of the interpretable components. However, the selecting datasets are solely in univariate setting and are shown to be flawed in previous study, which makes the empirical evaluation less credible. I believe some more modifications will certainly improve the quality of paper to meet the ICLR standard.<|endoftext|>This paper aims to boost the performance of deep neural networks (DNNs) for time series applications by focusing on the characteristics of interpretable forecasting and anomaly detection which are important for real world time series data. Although previous studies showed that conventional simple linear models often outperformed DNN models on typical time series benchmarks that require robustness and interpretability, this paper shows that the proposed DNN model outperforms state of the art robust statistical methods in some datasets, kind of demonstrating the best of both worlds. Strengths:* The network architecture (Figure 1) is carefully designed to incorporate useful ideas from both conventional statistical time series models and DNN models to allow the proposed model to get the best of both worlds, including its interpretability and flexibility. At least it should be shown as “Figure 1” explicitly. This is a good work which tries to integrate ideas from both conventional time series forecasting models and more recently DNN models.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; By combining generative models and discriminative models in the framework of an energy based model, this paper provides several theoretical insights about adversarial training, including that the softmax normalization term already provides a hint on adversarial training, and the importance sampling (with a justification) approach to unsupervised contrastive learning. Overall, I like the insights provided in the paper and their experiments also proved their points well. The results are also very competitive, I think the unsupervised contrastive results are especially quite strong. I think the insight in contrastive learning is quite interesting.<|endoftext|>This paper justifies why models trained with adversarial training are good generative models by proposing a probabilistic framework. Then the authors use this probabilistic framework to analyze the adversarial training. In the last part of the paper, they use their framework to propose an unsupervised adversarial training method followed by sampling algorithms from the produced models. The paper is technically sound and brings a novel perspective to adversarial training. The technical analysis has some valuable novel aspects that justify well why adversarially trained models are good generative models.<|endoftext|>This paper proposed a unified probabilistic framework, dubbed as Contrastive Energy based Models, to understand the robustness and generative capability. 1.The proposed Contrastive Energy based Models (CEM) gives a probabilistic interpretation for adversarial training, which explains the generative ability of adversarial trained models. The proposed unified probabilistic framework CEM is fundamentally novel and interesting. It gives explanation on the generative capability of adversarial trained model. The derived adversarial sampling method show a better sample quality.<|endoftext|>It also offers a unified perspective of adversarial training and sampling from both a supervised learning setting and an unsupervised learning setting. The contribution is a unified probabilistic framework to explain adversarial learning, however, it is not clear whether this probabilistic framework can be beneficial for improving the training of adversarial models. The sampling strategy is not claimed as a contribution of the proposed work, however, the experiment part spends a lot space to demonstrate that the sampling in the proposed framework is better than existing works. The authors need to more clearly elaborate the contributions. The results presented are not ideal to explain the effectiveness of the proposed framework.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The authors propose a novel model that focuses on improving cross domain few shot classification. I have some concerns regarding the actual importance of dependencies within the latent variables. This probably leads me to believe that ensembling is providing the major improvements and the proposed hierarchical formulation isn t actually that important. The paper is well written and easy to follow.<|endoftext|>This paper proposes a hierarchical memory to store features at different semantic levels for few shot learning across domains. The proposed method is evaluated on 4 various datasets which have a domain gap between the training data. The authors also show that this method is competitive on the commonly used few shot image classification benchmarks. Pros  The paper is well written and easy to follow. The overall idea is interesting but there are concerns regarding the experiments as well as the introduction of external memory. Post rebuttal Comments The authors addressed most of my concerns in the feedback.<|endoftext|>This paper presents a hierarchical version of the variational memory approach of [Zhen et al., 2020] for few shot learning. Since the proposed system is an extension of Zhen et al., it is quite detrimental to actually perform worse than the baseline. The paper presents results on the same few shot tasks as Zhen et al.(mini Imagenet and tiered Imagenet), as well as comparison to other meta learning methods on few shot cross domain tasks.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The paper considers the natural class of algorithms, namely *Aggregators with Gaussian noise* for distributed SGD with differential privacy (DP) and Byzantine resilience (BR). Previous results shows VN $\Rightarrow$ BR $\Rightarrow$ convergence of SGD. The authors first show that aggregators with Gaussian noise algorithms satisfy DP but violates VN necessarily, so approximate VN is proposed. Theorem 2 shows approximate VN $\Rightarrow$ convergence. Proposition 2 shows the above algorithms satisfies approximate VN with certain parameters. With the combined bound Corollary 1, the authors observe (and then verify by experiments) that larger batch size is beneficial and in particular more beneficial than when DP or BR is enforced alone. There s actually a big line of work.<|endoftext|>The paper considers the problem of distributed learning via SGD when a fraction of workers are Byzantine and the rest want their data to be kept private. The authors consider a generic/naiive combination of Byzantine resilience (BR) and differentially privacy (DP) in Algorithm 1. They show that the VN condition is incompatible with Gaussian noised SGD, but propose an approximate VN conidtion that can be realized by noisy SGD. They then establish a convergence guarantee for the algorithm and conduct numerical experiments. The paper makes some good progress towards an understanding of DP and BR in SGD via Proposition 1 and 2. There are many other more relevant works from 2014 to present. When you loop through "honest" and "Byzantine" workers, it seems as if the analyst/curator who is implementing the algorithm knows which workers are honest and not, which is clearly not the case Contextualize Theorem 1. the privacy properties of Gaussian mechanism and subsampling are well known, so this theorem is not at all novel; this should be stated.<|endoftext|>This paper combined differential privacy and byzantine resilience in the distributed SGD algorithm. The authors provide a simultaneous theoretical guarantee of DP and BR by re tuning the algorithm. Both theoretical results and numerical experiments are conducted to show the effectiveness. The targeted problem is interesting and important. Weakness:  1. Is that possible to extend current results for non smooth loss? Is the proposed method robust with a certain range of learning rates?<|endoftext|>The paper studies a federated learning setting combining differential privacy (DP) and byzantine robustness (BR). The paper shows that the "variance norm" condition for byzantine resilience needs to be relaxed as we can expect when DP is added, and then shows an adapted theorem which allows for both DP and BR. The paper is well written and understandable. I didn t check every detail but at a high level the results seem correct. Detail:In Algorithm 1, please explain the meaning (what?) of "without replacement" in the sampling? Depending on the distribution D this concept may be easier or more complicated to interpret.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper considers the problem of calibrating quantiles outputted by a regression model. The paper proposes a method that adjusts the output quantiles to be better calibrated for a less restricted definition of b calibrated. This paper investigates the problem of calibrating regression outputs with a distribution drift setting. As the algorithms only work on each quantile, the authors later provide an adjustment method to ensure all the quantiles are monotonic and another PID control based approach to improve the stability of quantiles. 3.Improve model under distribution shift is a well established domain, I wonder if other reviewers with a stronger background in out of distribution prediction could enlighten if there are any good baseline to compare.<|endoftext|>For real world applications where ML models are used, quantifying predictive uncertainty is an essential task as, for example, in safety critical applications misclassification might have disastrous consequences. Lowering the score is mainly motivated by the issues that are discussed in the main review. I was wondering whether the authors could comment on the following questions:  The paper heavily refers to conformal calibrators. In the current form, it seems to be a bit hand wavy, so a slightly more detailed description of the procedure (either in the main paper or in the Appendix) would be helpful to the reader.<|endoftext|>This paper proposes a online regression calibration method using conformal and control theories. This paper considers four types of drifts but the the drift data are synthetic. Although the current calibration/Bayesian methods are not directly available for distribution drifts, there should be some prompt apply and variants of these existing methods that need to be added as baselines. This paper considers an new but important branch of truthworthy regression/forecasting, online learning with distribution drifts. The draft is clearly written but some concerns lie in the novelty and experimental setting.<|endoftext|>This paper consider calibrated online regression; to attack this problem, this paper proposes a new calibration algorithm (i.e., adjusting conformal calibration for distribution drift by ensuring feasibility and improving stability) and proves the proposed algorithm is "b calibrated" (as defined in the paper). In particular, (3) adjusts calibration values made by the iid algorithm (e.g., conformal calibration) with exponential models; this implies that the distribution shift may not be significant. **Strengths**1. calibration for online regression is an important problem and well motivated.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The paper proposes three reductions/meta algorithms to speed up existing distribution compression algorithms while maintaining the error guarantee of the original algorithms. As mentioned earlier, the paper makes a good contribution and thus I recommend acceptance.<|endoftext|>This paper gives a meta algorithm for speeding up coreset constructing algorithms for the distribution compression problem. The main new idea is very simple.<|endoftext|>The paper introduces meta algorithms "Compress" and "Compress++" that take existing thinning algorithm as a subroutine and improves on their runtime while incurring marginally more error. Eventhough the framework is simple, it helps improve the runtime of existing thinning algorithms (from magnitude of days to hours).
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Are they per attribute? THis is then part of a larger recurrent architecture that produces words to describe or discrimate objects. The method is evaluated for success but also in terms of its ability to create compositional and symmetric language. The paper unfortunately suffers from some unclear writing as well as some lengthy unimportant descriptions and some important aspects that are missingPRO* interesting domain with lots of problems* main claim of language emergence from simple to complex is illustrated by one example. Some of this may stem from me not being able to understand the paper. But this is such a general claim   it would require a lot more experiments* the main claim of the paper is not really a technical claim CONS* some key explanations are unclear. The authors move around between architectures, tasks, training regimes   which made it very hard to follow for me. The paper is trying to make too many different points including a "new" architecture, a "new" game, a "new" curriculum approach. Some of these seem trivial or known in prior work. I don t think the paper really adds anything interesting to that discussion. Unfortunately, I did not understand well how the agent architecture works. But from what I understand   the symbolic mapping is basically an MLP that looks up words for objects. It s these words then that are fed to the recurrent sentence generator. This seems like a prior that works in the particular setting here. Some major details are missing. What s m?It seems to be the number of values per attribute. What is a reconstrution network?<|endoftext|>This paper investigates methods for emergent communication. Specifically, it looks at inductive biases that can contribute to the emergence of compositional and symmetric languages. 2) It proposes a new agent architecture,  symbolic mapping , which can help agent performance and increase compositionality. There has been quite a bit of previous work on looking at what environmental pressures and inductive biases lead to the emergence of compositional communication. I would say this is not a bad paper. The paper uses an environment that, while simple, is in fact a bit more complicated than environments in previous emergent communication papers. The paper also proposes a fairly specific LSTM based architecture (SM) that further improves performance. One of the claimed benefits of SM from section 3.3 is that the architecture allows us to do  vocabulary expansion , but the results indicate that vocabulary expansion seems to help the base LSTM policy just as much as the SM policy. Ultimately, if we take the main contribution of the paper to be  forms of curriculum learning help compositional languages to emerge , I don t think this paper presents experiments that are quite extensive enough for the reader to really understand *how* curriculum learning helps. I d want to see more ablations, e.g.looking at multi step curricula (the paper only considers 2 steps), effect of initial task difficulty, etc..<|endoftext|>Although, the authors show promising results on the small  toyish  dataset, there is no evidence or discussion on whether the findings would scale to more complex domains. The authors state that symbolic mapping provides knowledge about the learned language implicitly. It would be better to demonstrate this with a small example on what exactly transfers from learning the simple solution to solving a more complex task. Would it require fewer game interactions or samples than training an agent from scratch? More empirical evidence is needed to support the claim that the proposed module would scale to any complex task, or state how such a complex task should be designed. The authors state that training agents in the discrimination game is harder than expected and the effect of symbolic mapping is not really pronounced in this game. A qualitative example would be helpful to highlight the possible issues with this game as compared to the description game. The authors propose a novel module that helps in training better compositional and symmetric language using a curriculum from simple to complex tasks. The complexity of the tasks can be varied along different axis but only a few options are explored to extend this.<|endoftext|>This not only facilitates the transfer of these object symbol mappings to different tasks but also expanded vocabularies. The current version of the paper greatly overstates the novelty of its technical contributions (SM, refdis, task transfer). I recommend editing the verbiage to be more task specific instead of general (see point 1). For example, in grounded language games, symbolic association is closely linked to the task. It is important to specify that this claim applies to *this specific setting* rather than in general tasks as implied. Finally, the authors conduct a vocabulary expansion experiment. 2.Missing significant literature. 3.General issues with clarity **Strengths:**1. 2.Results make a strong case for further work in intermediate symbolic representations and support prior research! **Recommendation:**Based on the above reasons, especially due to lack of citations/originality, I vote to reject the paper in its current form. Why does detecting the difference between objects imply the agents may not be clear what their partner s symbols mean? However, the experiments showing that SM is beneficial in task transfer are important in that they support prior findings, and the experiments on vocabulary expansion are novel, to my knowledge. (1) The main idea of symbolic mapping, which is mapping raw input to an intermediate symbolic representation, is not new and should be discussed in relevant work. What is intended by "language properties"?<|endoftext|>What are the inputs and outputs to the word bank? Is the word bank generating multiple $s$ s? Why is that? ## Strengths  It presents an intuitive yet novel approach to emergent language: transfer from a simpler to more complex task. The experiments use a small number of trials (3) in very small environments which raises concerns about the robustness of the observed effects. Please be clear with what the symbols and the word bank are because they are critical to understanding symbolic mapping. ## RecommendationI recommend "reject" in the current form of the paper because even with careful reading, I am not able to clearly understand what symbolic mapping is, a key component of the paper. Additionally, the empirical significance is lessened by the fact that only the only environments that work are of the scale $(3,3,3)$ and $(4,4)$. `s3.4 Compositionality`: I see how positional disentanglement is inappropriate for this task, but BoW disentanglement from the same paper might work just fine   it is very similar to referential disentanglement as given here. Is there a better alternative that could be used for this paper? `s4.1 p1`: "unfixed"  > "not fixed"  `s4.2.1 p2`: If the experiments are using a shared listener in the dialog setting, wouldn t this undermine the concept of refdis measuring _symmetry_? What is the symbolic mapping box? This undermines the usefulness of the metric. The paper presents a novel and motivated setting of task transfer and has the right start for experiments.
Reject; rating score: 3; rating score: 3; rating score: 5; In this work, the authors design a causally focused CNN framework through human guidance. Extensive experiments on several datasets have indicated the effectiveness of the proposed method. For each dataset, there lacks an introduction on the specific classification task, and the data split details. Given the unstable performance gain in Table 3, it is questionable whether the proposed framework is worthwhile to use in real practice. In addition, the overall manuscript is not clearly written. Therefore, I recommend rejecting the paper.<|endoftext|>Although the reviewer finds the problem interesting and relevant for the ICLR community, the reviewer thinks that the paper in its current state is below the acceptance threshold. It is also a bit unclear to which principle the authors are referring to. The paper tackles an interesting problem of making the model more robust to spurios correlations. From the current description, it is unclear how the model works at inference time.<|endoftext|>What are the technical improvement over GAIN and how are they working? Also, there is no comparison with intrinsic XAI methods. 3> It is not clear from the paper that if all the classes (at least a few images per class) in the dataset have binary masks for training or some classes have zeros masked training. However, this is rare in medical imaging which authors claim to be the main objective of the proposed method. Any comments on this?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper proposes a called Foveater which uses a foveated module to extract the information from the feature map with different levels of details and different locations. However, I found it is difficult to find significant technical novelty in this paper. The proposed method achieves better performance than baselines in the adversarial robustness experiments. I agree with the author that the foveation module could be helpful for the computer vision task. Essentially, there are three components of attention mechanisms: foveation module, sequential fixation, and transformer. The author provides detailed information about the proposed network architecture that is possible for re implementation. If the author wants to claim novelty over the family of dilated convolutions, they should provide more discussions, analysis and experiments to show the advantage of the proposed method. If the author would like to claim this combination itself is the contribution, there should be more ablation experiments on the contribution of each component. The task itself is not easy to train.<|endoftext|>The authors present an approach to computer vision combining Transformer networks and foveated vision. Can you show that more fixations were used by the model for images that were more complex by some entropy or other complexity metric? Or compare to human gaze experiments? While recent success in convolutional processing has dominated the field, certain aspects of vision common to many biological vision systems have had limited treatment. The introduction provides motivation for foveated vision and review of prior art in this area. It is unclear from the introduction why adversarial attacks are a focus of this paper. The motivation of the paper starts on the basis of biologically inspired vision systems, yet such systems do not have to address adversarial attack other than overcoming natural camouflage (which is not at all similar to the types of adversarial attacks compared in the paper involving gradient informed adversarial examples), and in any case there is not explanation of why adversarial robustness should be a feature of foveated vision, or why to evaluate foveated vision systems on this basis. Explanation of the foveation pooling and aggregation approach was a little confusing. Given the biological motivation, I would be interesting to know how this compares to biological systems. It would be informative to compare eye movements predicted by the model to biological movements in gaze tracking datasets.<|endoftext|>The experimental validation is performed by the object recognition task (ImageNet) and adversarial attack tasks and performed better than the Deit Tiny model that uses a similar number of parameters. The method presents a unique idea of computer vision tasks based on the human perceptional system. Since humans archive vision processings with limited computing resources, employing the idea may work for computing tasks with the limited resource, thus, the idea proposed in this paper may give an impact on the community. Performance of object recognition:The performance of ImageNet task is far from SOTA. I understand the main claim of the paper is introducing a new scheme that increases the performance of the existing algorithm, but if so, the question is why the paper used Deit Tiny for only the reference. dilated convolution since they have similar computational models. The method presents an interesting and unique approach that is motivated by human vision perception system including gaze and attention control as well as the foveated vision. However, experimental validation is limited, and thus difficult to understand the pure effectiveness.<|endoftext|>The paper introduces a fixation based method based on a hybrid transformer CNN architecture, which demonstrates many of the strengths of sequential processing, namely computational efficiency and adversarial robustness. Strengths:  Overall, I found this paper both an interesting read and quite a novel approach. It is great to see practical sequential models being developed, and the inclusion of robustness evaluation via adversarial attacks was very helpful in bolstering the paper s claims. The components of the model, training steps, and experiments were clearly described. I am assuming from context it is performance. In Section 2: Computational models of categorization and eye movements, it is mentioned that saliency based models do not incorporate foveal vision, but Wloka et al., Active Fixation Control to Predict Saccade Sequences, CVPR 2018 provides a foveated model of eye movements based on an underlying saliency map. This is an overall strong paper. It proposes an interesting new approach, and does a good job of exploring the behaviour of this approach with respect to alternative architectures.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The authors design a new statistic called expected transferability (ET) for detection. The advantage of ET is that the detection threshold is a constant (1/2) and is independent of classifier domain and attack algorithms, so the proposed detection algorithm can be applied to a wide range of backdoor attacks on different databases. With a constant threshold, ET can be used to detect different backdoor attacks in different domains.<|endoftext|>This paper proposed a post training BA detection approach for two class classification problems, which can also be extended to scenarios with multiple attacks. The approach is derived based on a proposed notion of  expected transferability , which works with a principled threshold that is irrespective of the domain nor the network or attack configurations. Empirical experiments verified the effectiveness of their approach. This paper proposed a novel and working solution to a tough problem of backdoor attacks detection with binary or multiple labels.<|endoftext|>The threat model assumes no access to the model s training data. The author reason theoretical what values of ET indicates the presence (or absence) of backdoor attacks. Then they empirically demonstrate how this value can be estimated and leveraged. What I liked:  The paper is well organized. The latter would give a better idea of how the ET based approach performs on average (as opposed to in the best case, that too if values furthest from 1/2 imply best).<|endoftext|>To detect backdoors in such scenarios, it leverages a new statisticalmetric known as the expected transferability, which can derive a fixed thresholdfor backdoored models detection. Itskey selling point is that the proposed method works under three conditions: noaccess to training data, reference model, and binary classifier.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper provides a multi mode framework for the deep learning based tensor decomposition, which could be useful for dealing with nonlinear high dimensional data sets. Based on this, it also develops a multi mode nonlinear deep tensor factorization method with convergence guarantee. Numerical experiments on synthetic and real data sets of the matrix/tensor form have shown that the proposed methods outperform other state of the arts. This paper provides comprehensive experiments, including both synthetic and real matrix/tensor data sets, to show the effectiveness of the proposed framework. The results based on various optimization algorithms make the performance convincing and the comparison fair. 4.The Assumption 2 in Sec 3 is not well organized. The multi mode generalization of deep matrix factorization and its extension to tensors are insightful with detailed theoretical discussions. Hopefully, the authors can address my concern in the rebuttal period.<|endoftext|>Specifically, it first presents the theoretical results showing why nonlinear deep matrix factorization is better than the ordinary matrix factorization model. The authors also extend this method to tensor factorization by further factorizing the factor matrices in the Tucker decomposition using the deep factorization method. Theoretical analysis and synthetic experiments show that when data is generated according to the assumption, it outperforms baselines and achieves better generalization. 2) The sizes of the tensors used for experiments are quite small, how would the proposed model scale to a large tensor? It would be great to see relevant discussion and comparison with those recent methods. 2019.This well written paper presents some interesting and important theoretical results and proposes a promising multi mode deep matrix and tensor factorization model.<|endoftext|>This paper summarizes the theoretical guarantee for the existing LRMC and LRTC algorithms, and provides the theoretical analysis for a new proposed Multi Mode Nonlinear deep tensor factorization. The analytical results show that when n2 is larger than n1, the nonlinear DMF provides a tighter generalization bound than MF. Similar analysis has been extended to two mode matrix factorization and multi mode  tensor factorization. Experimental results in synthetic data and real data show better results of the proposed algorithm as compared to other algorithms in completion tasks. [2] There are some concerns about the synthetic data simulation used in the paper. While, it would be better if the synthetic data can are generated the same way as the assumption (tanh activation function as used in the experiment) to verify how efficient of the algorithm in recovering a tensor with known factorization.<|endoftext|>To better motivate the proposed models, the authors provide theoretical analysis for why and when nonlinear deep matrix factorization outperforms linear deep matrix factorization in matrix completion. Besides, only a few baselines are included in the paper. More recent methods can be included as baselines for experimental evaluation. The proposed models built on top of meaningful assumptions for matrix and tensor factorization tasks which aim to address the gap between deep learning and factorization. (1 3)Please include more baselines for experimental evaluation(2) Please include more details on the optimization procedures in the paper or appendix.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The proposed method is specially useful when classification at fine grained level is performed where in addition to the class to which an instance belongs, predicting the subclass is desirable as well. Experimental results are provided in the paper that validates the merits of the method compared to a set of variations of the proposed method and a state of the art approach as well. The method is positioned well in the existing body of works and the contributions of the work is clear. Authors have also done a good job discussing the experimental results and sharing the interesting observations. It would have been good if the authors could have added one or more ablated versions to the experiments to check this. Minor comments:  P3: "Training stops when accuracy using nearest neighbor classification on a validation set is minimized"If I understand correctly, training stops when classification error is minimized not the accuracy. Some hyper parameters seem to be set arbitrarily (e.g.# prototypes   30) and it s not clear how assigning a different value could have affected the accuracy of the proposed method or how authors have come up with the used values. My recommendation is to accept the paper.<|endoftext|>The paper proposes a novel combination of prototype learning and deep nearest neighbor learning in order to achieve an embedding of the input data that is more friendly for prototype learning while maintaining the computational efficiency and intepretability of a prototype approach. In more detail, the approach first trains a Multi scale deep nearest neighbor network to embed the input data. /edit The authors have resolved the most crucial points in their response. Then, it jointly learns a prototype layer and a fully connected layer which outputs class logits based on distances to prototypes. In experiments on MNIST, FashionMNIST, CIFAR 10, and ImageNet, the paper shows that the proposed approach outperforms state of the art prototype learning. As far as I know, this result is a novelty in the literature and motivates the use of prototype/nearest neighbor approaches. However, more baselines would have been possible, taking more recent work into account, such as Chen et al.(2019) or [Hase et al.(2019)](https://ojs.aaai.org/index.php/HCOMP/article/view/5265).<|endoftext|>This paper presents a method to learn neural nets that preserves the subclasssimilarity in the embedding space. The proposed method adds a prototype layer,which stores the representative prototypes of some subclasses, then a fullyconnected MLP is used to output the class label based on examples  distance tothe prototypes. I am not expert in deep learning, but I know that prototype method was verypopular in statistical learning. Many commonly used algorithms, such as KNN andK means, are exploiting this idea. My main concern is the novelty of this work, and its lack of comparison toprevious work in similar direction. Moreover, the main contribution of this work isit extends MsDNN with a prototype layer. The authors have state that it should be applied onclassification problems that each class consists of many subclasses. The novelty of this work could be limited, and the experiments seem not enoughfor validating the effectiveness of the proposed approach, so I recommend to reject.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 3; A novel general approach for weakly supervised learning. The main idea is to learn embeddings over the labels tractably into two embedding spaces (Boolean/Euclidean). The paper is well written and has a very general framework for weak supervision. The paper seems well written, has both good experiments and theoretical guarantees. Is snorkel the only possible comparison baseline (particularly since a lot of the tasks are novel and cannot be done by snorkel).<|endoftext|>This work studies a weakly supervised learning setup of aggregating multiple weak sources of labels into high quality pseudo labels for learning. Results show that learning the graphical model achieved better performance than majority vote and can achieve parity reach a certain level of fully supervision. Strength* The proposed embedding based approximate estimation approach is novel. * Theoretical bounds are derived to monitor the closeness of this approximate estimation. Specifically* Needs a bit more background introduction. "LF" (label function) in section 1 is defined in section 2. Readers have to go to the appendix for a more comprehensive picture of the proposed approach. 2.Discussions and experiments on the universality aspect is lacking. This work claims to be a universal approach for WS, which boils down to an embedding based approximation of distance function d(,). Update: My concerns on writing and universality have been sufficiently addressed with the latest revision. and 2) the experiments included in the main paper seem shallow and repetitive.<|endoftext|>The paper proposes a framework for weak supervision which works for both discrete and continuous labels. Weaknesses:  I find the theoretical contributions on the more trivial side. The criteria metrics might not be fair since they are directly related to this paper s distance choices. Since the paper claims to be a universal framework, I would expect using different types of labels in the same task (like if we have binary labels for some data and probabilistic outputs for others), or coexisting more than one task type is doable. I believe Related Work section is significiant and should be in the main paper, not in the supplementary. Why snorkel works worse than majority voting in your experiments? How would you adapt your method for the problems that use different label types at the same time? I am willing to chance my vote towards accept if the authors could clarify my concerns and convince me their work has sufficient contributions to be published.<|endoftext|>The authors present a framework that intends to extend the label types considered traditionally in learning from weak labels. I commend the authors on their thoroughness and the aims of their approach   which are much needed in the field of weak supervision. There are many approaches in the past that also move past classification to consider ranking and regression. Also, the structure of the paper does not help in understanding this, with the related work being in the supplementary material. Finally, the title seems a bit to general for what it is shown in the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes an adaptive tree search algorithm for NMT models. One advantage of this algorithm is that it does not make any assumptions about search objectives, and this enables the proposed algorithm to be applied on top of more general search objectives. 2.The baselines are strong and the proposed approach also yields BLEU improvements when the model score correlates well with BLEU. The paper does not intensively analyze the running efficiency of the proposed BATS algorithm. 2.In the paper, max rank is claimed to be a contribution but it does not conduct an ablation study to highlight its individual contribution. 3.It shows the BLEU comparison for standard beam search and BATS, but it does not show the comparison in terms of model score.<|endoftext|>This paper presents a novel algorithm called beam adaptive tree search (BATS) to enable the incorporation of search objectives that cannot be easily factorized. Unlike the regular Monte Carlo tree search algorithms that relies on a large number of playouts to update the value function, BATS relies on "informed playout", which is guided by the greedy decoding of the autoregressive models. Strengths:+ The paper is well structured and easy to follow even for someone who does not regularly follow latest research on MCTS. + Since the author mentioned that the search budgets of BATS and beam search is not point wise comparable, it ll be interesting to see a discussion on the computation time of BATS in the experiments, which is missing for now. + Some missing details about evaluation: (1) I don t think the paper mentioned what non autoregressive models were used for the experiments. (2) It took me a while to understand what "iteration" means in section 3.3. I would try to connect this with the notion of "traverse" in section 3.2.<|endoftext|>**Summary**This paper proposes an adaptive tree search algorithm BATS, an MCTS variant, for NMT that could optimize any desired objectives/metrics (e.g.BLEU).The algorithm values each internal node by scoring a greedy searched rollout (instead of that of authentic MCTS) with an autoregressive model, which avoids search biases caused by other heuristics (e.g., beam search would be biased towards shorter translations or incomplete partial generations). 2.Empirical results verify the efficacy of the method. The findings of the limits of beam search are inspiring. Page 6, first equation of r. "where we count the number of other actions with lower log probability than y_i" conflicts with the equation of r where you count the actions that are more probable than y_i. However, I did not find the results of non autoregressive models or I might be wrong. I thought it was that from [Gu+2017] generating a whole sentence in parallel. This paper inspects the biases of commonly used decoding heuristics and provides a variant of MCTS algorithm for better decoding in NMT with empirical verification. Overall, I feel like the submission is a good one and would give a concrete contribution to the community. However, the authors should address some of the issues I stated above.<|endoftext|>The paper proposes an adaptive tree search algorithm for text generation. The comparison with beam search is unfair. The authors claim that they address the issue that the translation quality drops when increasing the beam size in beam search, but in Table 6 BATS also suffers from this issue as the best performance is achieved when iter 2. In my view, the paper has two main contributions: utilizing an autoregressive model as the value network to enable MCTS, and proposing a new metric that is not limited to autoregressive decoding. I have several concerns:  The writing of the paper can be largely improved. In equ (3), what is d() and how is it computed? Some metrics are missed in experiments. In section 4.1, what are "other actions"? And do you use knowledge distillation to train the NAR model? In NAR models, how the token probability is computed? I m still a bit confused regarding the term "non autoregressive" in the paper. If I understand correctly, in the rebuttal, the authors claim that the "AR/NAR" in Table 2 refers to different objectives such as NC and MR, which means that the backbone models of AR/NAR are the same, and different objectives result in different models.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper proposes a minimum margin (MM) attack to evaluate defenses. Thus, the technical contribution and novelty of MM are quite limited. Namely, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper (Croce & Hein, 2020).<|endoftext|>The paper proposes an attack for testing adversarial robustness that is reportedly faster than the state of the art attacks but still produces reliable results. This is not widely used, as for now it seems only used in [croce2020]. Should be "improves the adversarial examples" or "improves the adversarial evaluation". 3 should be clarified. ### Incorrect statements and unsupported claims**Abstract**  there is no definition of the "most adversarial example", even though there are several references of this in the paper. Depending on the objective, a stong adversarial example can be seen in different ways. International conference on machine learning. "Unfortunately, PGD fails to reliably evaluate adversarial robustness of a robust DNN". PGD was succesfully used against many defenses, just by making it adaptive to the defense [tramer2020].<|endoftext|>Although, I believe the proposed method has the potential for a good publication, I do not recommend the acceptance of the paper in the current form. The authors propose minimum margin (MM) attack to provide comparable performance with AutoAttack while significantly decreasing the computational cost. Strengths:  The paper is well written and the preliminaries are described clearly. The authors have mentioned that: "For reliability, we evaluate the quality of adversarial examples using the margin between two targets for precisely identifying the most adversarial example.".<|endoftext|>The paper proposed a strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. Some points should be clarified and stregnthened in the revision. This is quite critical for the significance of the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; I recommend publication, as the paper is well witten and clear, and brings about a new scheme and neural net model to compute exaclty one of the most fundamental quantity in their training: the mutual information. In this case mutual informations can be computed exactly and the data quantifies the true information flow along training. I find the paper very well written and interesting.<|endoftext|>I know this is a long shot   but to me this would be the most significant improvement of the current work. I am in favor of accepting the paper and will argue so during the reviewers  discussion. Additionally, it is currently unclear how the training dynamics of (heavily) quantized networks relate to the training dynamics of “non quantized” networks; after all; the activation quantization could potentially have a regularizing effect that might interact with the compression induced by SGD alone.<|endoftext|>The paper has merits and deserves to be published, even though I have still reservations regarding a few aspects. The paper is interesting, well written, and covers a relevant controversy (albeit in a niche). Does this not also affect the quantization aware training that was used here? Finally, it is not fully clear what practical or theoretical implications the results of this paper have. The controversy about the existence of a compression phase is known.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper enhances the efficient BPTT training of SNNs by proposing an initialization method to match the response of spiking neurons in initial training. The authors have presented interesting results. to overcome the vanishing gradient problem. While the authors have compared with recent works, I would like to bring the attention of authors to recent works from a group that authors have cited in their work, that use batch norm and threshold initialization as a way to mitigate the gradient training issue. Revisiting batch normalization for training low latency deep spiking neural networks from scratch. Based on this, my next question is, can authors comment how weight initialization impacts the overall spike activity, latency of processing of the network. I feel the authors should make a more thorough investigation of recent work that highlight similar ideas and yield better results on more complex and large scale datasets.<|endoftext|>The authors tackle the problem of improving spiking neural net training with better initialization. The improvement is to a large extent due to early onset of convergence. The authors demonstrate the superiority of their approach on MNIST, N MNIST, DVS MNIST and CIFAT datasets and present extensive experiments using different optimization algorithms and response functions. The paper addresses an important problem of inefficiencies in trainign SNN models and presents a compelling argument for the inefficient SNN initialization and demonstrate that improving it so that neurons are responding from training onset can increase both convergence and test accuracy.<|endoftext|>The paper investigates the effects of weight initialization for deep spiking neural networks (SNNs) on training speed and final accuracy. The paper derives a theoretical first order approximation on the response curve, and from it a weight initialization scheme that initializes the weights in a region where neurons fire from the first epoch on, but also avoid explosion or reduction of activities throughout the network. For CIFAR10 the results are significantly better than for other initialization schemes. This is achieved from theoretical analysis of SNN neuron models and network dynamics, and the results seem to indicate that the method is generally superior to non SNN specific methods under a variety of parameters and datasets. If this method generalizes to larger tasks and networks, this could become a new standard for SNN training, and therefore a significant contribution to the SNN literature. It remains to be shown that the method scales to more difficult tasks and larger networks.<|endoftext|> This paper focuses on the parameter initialization problem of training SNNs. The authors derive the theoretical response of spiking neurons, and propose an initialization method based on slant asymptote, which can overcome the gradient vanishing problem. The results show that the proposed method can effectively improve training speed and accuracy. The derivation and validation of the weight initialization method appears to be sound. It is interesting to see the authors relate the spiking neuron response to the traditional deep learning training. (3) I suggest to demonstrate the effect of the initialization method on more complex dataset.. Overall, it is a nice paper with solid theoretical basis.
Reject; rating score: 3; rating score: 5; rating score: 5; The structure of Section 5 (questions, description about the experimental setup, answers) makes it somewhat hard to read. These two policies control compete to maximize/minimize surprise, respectively, by taking turns to control a shared body. In ICLR, 2020. I have some concerns (and doubts) about the evaluation of unsupervised agents. I would advise authors to unify the evaluation protocol for each domain (i.e.same protocol for all Atari games, same protocol for all VizDoom experiments, etc). The paper mentions that $k^E$ and $k^C$ are independent hyperparameters that can be set to different values. It would be interesting to include an analysis of the role played by these values in the discovered behaviors.<|endoftext|>The algorithm uses a single agent with two policies, an Explorer and a Controller, which switch during an episode with opposite rewards: to maximize and minimize "surprise". I may have missed it but couldn t find it anywhere. One of the greatest strengths of this work is that is it well motivated, and thorough. The flip side of this, is that the specific setting considered, stochastic BMDPs, seems very specific. 3.Why are the methods in Fig.2b) and 2c) different? E.g.AGAC is in one but not the other.<|endoftext|>AS employs a two player, adversarial, sequential procedure in which an Explore player tries to maximize the approximate entropy of the observations, whereas a Control player tries to minimize this same entropy. AFTER REBUTTALAfter having read the authors response, as well as other reviews and discussions, I am keeping my original, slightly negative, score. While the proposed methodology is certainly interesting, my main concerns about the current version of this work regard (i) the applicability of observation density estimation and (ii) the robustness and completeness of the experimental evaluation. EMPIRICAL EVALUATION2) In Appendix C.4, the paper reports that all the experiments are obtained with six independent runs. Moreover, an observation coverage metric is only reported for the MiniGrid experiment.
Accept (Oral); rating score: 8; rating score: 8; rating score: 10; rating score: 10; The authors identify two limitations to this approach, and propose BMG to address them. (i) MG is myopic, in the sense that it does not account for future learning dynamics beyond these K steps, therefore BMG proposes to bootstrap a target from the K step parameters (in practice by continuing to optimise w.r.t.parameterised update rule for L 1 steps, and then taking a final step w.r.t.a fixed objective to ground the signal). (ii) MG updates are necessarily restricted to be within the geometry of the parameterised learning process. **Strengths**   The paper is generally clear and well written. The empirical results are strong and thorough. Empirically, however, grounding the bootstrapped target with a single final step on the meta objective is sufficient for good performance.<|endoftext|>The proposed algorithm addresses these two issues by minimizing the distance to a bootstrapped target under a chosen metric. Theoretically, some guarantees on performance improvements are provided. I think the paper would benefit from a running example and a dedicated section and pseudo code describing the algorithm and how it can be instantiated in different experimental settings. **Significance**The work is significant and will benefit the reinforcement and meta learning community by addressing some of the limitation of the current meta learning algorithms. **Limitations**  The theoretical analysis is limited to noiseless 1 step target updates. **Questions to Authors**  Some engineering / handcrafting is still required by the machine learning practitioner to select what "target" the meta learner is going to optimize, as well as the proper "metric" for the meta learner to optimize for. **Minor Typos**  Abstract: "show that metric"  > "show that a metric"‌‌<|endoftext|>This paper broadly considers meta learning, a.k.a. bilevel optimization, across single task, multi task, supervised learning, and reinforcement learning settings. The main algorithmic contribution consists of a family of meta learning objectives called bootstrapped meta learning, in which meta parameters are optimized to bring post inner loop learner parameters $x^{(K)}$ closer to a bootstrap target (which are also learner parameters) computed from $x^{(K)}$. Strengths  The proposed idea is simple. The authors give the precise form to recover MG from BMG. The authors run informative ablation studies that support the intuition behind the benefit of BMG: resolving curvature and mitigating myopia. Weaknesses  Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta parameters), it would be nice to have some experiments on learned sequence model update rules (e.g.RNN).All current experiments use update rules with a fixed functional form. I believe that the meta learning community will find this paper interesting.<|endoftext|>The paper proposes *Bootstrapped Meta Learning,* a new meta learning algorithm for hyperparameter optimization. Drawing inspiration from temporal difference learning techniques in reinforcement learning, the meta learner is asked to predict the result of additional unrolled steps of the optimization process, by minimizing a carefully selected distance to a target generated during training. The related work section adequately connects the algorithm with existing work in similar directions. **Strengths**  The method is based on a conceptually compelling and inspiring idea. The empirical results are remarkable. The ablations and additional experiments (also from the Appendix) help in understanding what matters in the practical algorithm, as well as highlighting the important parts of the contribution. I enjoyed the theoretical results, but it is a pity that they only deal with targets of specific forms and, especially, with $L 1$ only.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; Paper focuses on pruning 1x1 convolutions in FBNet with existing technique (magnitude pruning). Authors tend to answer 2 questions: a) how a larger pruned model compare to smaller model trained from scratch; b) is pruning an efficient way to get NAS model derivatives. Applied technique, magnitude pruning, is a well known techniques. Two research questions that paper tries to answer are interesting, but have been answered multiple times. It is known that pruning a larger model to the level of smaller will give a more accurate model. Researchers have been showing this by pruning Resnets from 101 to 50 to 34 to 18 etc. MLSys 2020Paper presents an empirical study on a well studied problem. The setup, method and results are not significant and are well known to the community.<|endoftext|>Strengths  This paper is the among the first if not the first to demonstrate that FBNetv3 (a state of the art compact network architecture searched with a sophisticated NAS method) can be pruned to achieve better computation accuracy performance, than searching for the architecture at the same complexity. It also shows that pruning the larger FBNetv3 model gets to a higher accuracy more quickly than running architecture search. The experimental results are good and they show that at different FLOP points, pruned FBNetv3 models always perform better than their original searched counterparts. It merely uses some existing pruning methods on the FBNetv3 architecture in a straightforward manner. It seems like the authors tried with other architectures but find that they do not work well with pruning. It could be that the claims in the paper are only narrowly applicable to FBNetv3. This paper has some good empirical contributions but those are all it has to offer.<|endoftext|>This paper applies conventional pruning and finetuning techniques to further compress the networks searched by NAS. The experiments and evaluations are based on the family of FBNetV3. Also, the authors show that pruning is more training efficient than searching by NAS to achieve a compact FBNetV3 model. Pros: 1) The paper is generally well written and easy to follow. 2) The authors provide valuable evaluations to show that pruning can be useful even for optimized network architectures, i.e.those searched by NAS. This paper only applied previous pruning methods to the network obtained by NAS and made a series of evaluations and comparisons. Technically, this paper didn t make novel contributions. 2) There are many typos in Table 1. It is strange that the shown numbers are not equal to (accuracy of pruned models   accuracy of baselines). For Table 2, what are the selected seed networks? Are the selected seed networks the same in Table 2? 4) The evaluation is only based on FBNetV3 architecture.<|endoftext|>The paper presents an experimental evaluation of simple pruning techniques applied to modern architectures that are designed to be inherently resource efficient. It is also shown that pruning large models is faster than performing NAS to find the smaller models directly. The paper is very well written and clear in most aspects. The two main contributions as stated in the paper summary above are novel. Consequently, I do not see a real benefit of the achieved sparsity levels in Table 1 which are often below 40%. In this view, I believe that comparing a small dense matrix with larger sparse matrices having the same number of non zero entries is unfair. The paper only shows experiments where pruning works reasonably well. I am missing experiments where we can see that the pruning breaks down, i.e., how far can we go until the accuracy degrades drastically on some specific model. This is important to study the impact on pruning for architectures that are inherently resource efficient and to understand how "overparameterized" these models really are. The second main contribution states that pruning a larger model is faster than finding a smaller model directly by means of NAS. Please emphasize the "all".
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; Figures and tables are presented in a nice and easily readable way, and help grasping the contribution of the study. The paper is well motivated, clearly written and coherently structured.<|endoftext|>The paper is a good case study. The ideas presented in the paper are not something that s unknown but this paper does a good investigation on the choice of perspectives.<|endoftext|>Specifically it compares the performance of a disembodied third person camera vs. place the camera on the robot s hand/gripper. What about recurrent policies instead of the third person camera? I continue to think this is a good paper that should be accepted for publication.
Reject; rating score: 3; rating score: 6; rating score: 6; The paper studies the problem of controlling the dynamics of a networked dynamical system,under partial observations. Here it is about coverage of the coarse communities. The authors consider a reduced order system from coarse data,and derive bounds on the convergence and approximation error for the original dynamical systems model.<|endoftext|>This paper studies network controllability with partial knowledge of the network structure. The scope might be narrow for the computer science group. The scope is a bit limited for the computer science community. 2.Since the network controllability has its application in practice, it would be better if the authors can provide illustrative examples on when such kind of approximation is needed.<|endoftext|>The paper studies the problem of quantifying certain controllability metrics, based on coarse scale observations of a network, which is assume to have community structure. The main contributions are some theoretical characterizations on when and how well this is possible, given the chosen setup. "Blind identification of stochastic block models from dynamical observations." Yes, there is a large literature now that uses linear control theory to study brain networks; but the brain is clearly a nonlinear system.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; It would be great to investigate more on this. Though the topic is interesting, given the current status of the paper, I would recommend a reject and I believe it would be a valuable paper if the authors study more deeply on the underlying causes of reward shifting. The paper further studies the idea in two settings: (1). Reward shifting techniques have been studied in the bandit literature, this paper studies how it affects the value based RL algorithms with function approximations.<|endoftext|>6.In the experiments, I would particularly like to see how the authors choose the value of bias. Clarity: The paper is well written and clear in the flow.<|endoftext|>This paper proposes a linear combination of reward shifting to improve the performance of value based reinforcement learning. 3.It is claimed in Section 2.1 that this paper considers off policy cases? The intuition of using pessimistic Q value for offline RL and optimistic Q value to encourage exploration is easy to understand.<|endoftext|>By leveraging these insights, the paper proposes to modify existing RL algorithms by simply adding reward shifting to improve their performance. Therefore, I vote for a weak reject for the current submission. To be more rigorous, I encourage the authors to state this as an assumption in the paper or specify that this part of analysis only applies to tabular Q learning settings.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; In this paper, the authors propose a conjunction of Bayesian learning and regularization via information gain as a means of adversarial defense. Unfortunately, there are many claims made in this paper which are either overly strong, incorrect, or fail to appreciate prior work.<|endoftext|>This paper proposes leveraging "information gain" with Bayesian Neural Networks to improve the robustness of Bayesian Neural Networks.<|endoftext|>I recommend to not accept this paper. * The authors implement an appropriate adaptive attack to account for the randomness of Bayesian Neural Networks. It might be helpful to refer to the attack with the widely used terminology of Expectation Over Transformation (EOT) attack, as in Athalye et. * The work lacks novelty, and its novel elements are not properly evaluated.<|endoftext|>In this paper, the authors propose to learn a multi modal posterior of the Bayesian neural network to defense the adversarial attacks, which can prevent the collapse and encourage the diversity accordingly. The authors are expected to clarify to how to estimate the information gain reliably.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This work proposes a natural actor critic (NAC) algorithm for two player zero sum Markov game in the tabular case. The paper develops a new NAC algorithm for solving zero sum Markov games with finite time convergence guarantees. In Algorithm 2, $\phi_n$ is confusing. 3.The authors presented the order of the complexity results that only emphasize the dependence on $\epsilon$. While there are many other important problem related parameters, I suggest the authors also explicitly mention the dependence of the complexities on, e.g., $|S|, |A|, (1 \gamma)$. For example, the complexity result established in Theorem 3.2 in the appendix seems to has dependence on $|S|^2, |A|^2, (1 \gamma)^8, \lambda_{\min}^2$. The introduction mentions that the value based methods offer near optimal guarantees which are lacking in policy based methods. Is it sample complexity?<|endoftext|>This paper considers algorithms based on natural actor critic for solving two player zero sum Markov games in the tabular case. In particular,  the authors focus on the analysis of the sample complexity for a two stage algorithm that solves a matrix game and a single agent problem, alternatively and iteratively. As claimed by the authors, the results are novel for policy gradient methods in this game setup. The assumptions and results are explained, which is helpful for understanding the context without referring too much to the technical details. I didn t check the proofs, but the theoretical results seem solid.<|endoftext|>This paper studies the sample complexity of learning algorithms in two player zero sum tabular Markov games. Based on two assumptions on stationary state distribution and action exploration that are different from previous works, the authors show two finite time convergence bounds, which improve much compared with existing results. The authors also present some numerical results that can verify the performance of the proposed algorithm. Strengths: The theoretical RL community will find the results presented in this paper interesting. Although Assumption 2 is less common than 1, I think it is reasonable, and one of the results in this paper does not rely on Assumption 2. It is good to see the order of sample complexity can be improved much based on these assumptions.<|endoftext|>This paper considers the problem of solving zero sum Markov games using natural actor critic algorithms. The authors derive $O(\epsilon^{ 2})$ overall sample complexity of the proposed algorithm, and conduct numerical experiments to verify the convergence of the algorithm. (Xu et al., 2020b) has $O(\epsilon^{ 3})$ sample complexity (see their updated arXiv version). Therefore, the overall sample complexity of NAC in  (Lan, 2021) is $O(\epsilon^{ 2})$, which is essentially the sample complexity of the critic. As for the critic, it seems that the authors derive $O(\epsilon^{ 1})$ sample complexity. Unless I miss something important, I do not think that is possible. Consider an MDP with a single state and a single action, and the discount factor is zero.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 6; The paper studies the role and implications of weighted empirical risk minimization, where one can also choose the weight of each sample instead of fixing it to be equal. In the first section, the paper motivates the moments penalization by showing that variance penalization leads to tighter confidence intervals (Maurer and Pontil, 2009). It is possible that Algorithm 1, by detaching weights from the computation graph, is approximately optimizing the desired objective of (1), but I don t see any immediate connection at this moment. I would be happy to increase my scores if my understanding of the results in the paper is flawed, and authors can clarify their contributions. In my current understanding, the paper seems to study a simplistic setting (and hence the theoretical results of the paper are immediate) and it is not clear to me how the results/framework extends to the general setting. Hence, I do not think the paper is fit for ICLR in its current form.<|endoftext|>This paper studies weighted Empirical Risk Minimization (ERM), where the weight at data point $(x_i, y_i)$ is a (polynomial) function of the loss value $\ell(f(x_i), y_i)$. For affine functions, the authors show that it is equivalent to perform variance penalization (Theorem 1 and Lemma 2). The authors also propose an iterative algorithm to perform the weighted ERM (Algorithm 1). The problem studied is interesting, but the provided answer seems very limited to me, especially at the theoretical level. For instance, the initial motivation is provided by eq.1 (on another note, the link between eq.1 and minimizing eq.3 could be better explained, as it is not immediate to me). How can you answer this? The weights used are actually completely different from the family studied in Section 2, breaking the coherence of the paper  overall, my feeling is that we have a motivation for variance penalization (eq.1), but the approach proposed is the complete opposite as a negative $\lambda$ is used. Although it is specified that the results stated still hold with the empirical distribution, then one has to relate the minimized empirical quantity to the true risk, which seems nontrivial. How would this approach compare to the one developed in this paper? why are other points present?<|endoftext|>In the paper, the authors proposed minimizing a simple weighted mean, which leads to an optimization problem of the high order moments of the loss distribution. Experiment results show the weighted mean trick has similar performance as other robust loss functions. Here are my comments on the paper:(1) Theorem 1 assumes that the first moment of $\ell$ exists. What do the authors mean by "convex objective $\ell$"? (3) " When using the weighted mean trick to penalize the variance and clip the negative weights to 0 then the objective remains convex for any positive": Why is it true? The authors need to explain this point further. (8) I do not understand the writing in Page 7. The authors may consider rewriting the writing in Page 7 to improve the readability of the paper.<|endoftext|>In this paper, the authors demonstrate that minimizing the weighted mean results in higher oder moments of the loss distribution. They do this by explicitly demonstrating specific choices for the weights such that the expected weighted loss actually corresponds to minimizing the original loss regularized by a linear combination of the higher moments. The authors state their work in the context of two recent papers [1], [2] which attempt. But there are several other works that also do such things. [1] "Robust Sparse Estimation Tasks in High Dimensions" Li 17 I think this is an interesting paper and vote to accept it.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; In this paper, the authors propose a new optimization method for continual learning. The authors propose a task aware optimizer to adapt the learning rate for each task. However, I found the results of A GEM in this paper are significantly worse than the original paper. More analysis about why the proposed method can outperform other approaches would make the paper more convincing. I think it might be valuable to compare and discuss with it. Please kindly revise it in the next version. I am willing to adjust my score if the authors can address my concerns. "Improved schemes for episodic memory based lifelong learning." After rebuttal   The authors  response does partially address my concerns. After reading the authors  rebuttal and other reviewers  comments, I still think the contributions are not enough for publication. The idea is clear and the results seem promising.<|endoftext|>This paper proposes TAG, a method for continual learning in the task incremental setting. Given the nature of TAG, I think is is fundamental to measure and report memory/storage requirements in comparison to other methods that also require additional memory/storage such as replay based methods and network expansion methods. Furthermore, claims around avoidance of catastrophic forgetting in the main results reported in Table 1 and explained in page 8 are not well supported since it is clear that the method is strong at learning new tasks, while not necessarily much better than counter parts regarding catrastrophic forgetting. The authors report performance in terms of overall accuracy, forward transfer and learning accuracy (LA). Strenghts of this paper are:   The method provides a simple yet effective way of dealing with catastrophic forgetting, which is to some extent original since it proposes to use the relatedness among tasks and use this information to control gradient updates while learning new tasks. Based on this, I would expect to see comparisons of the proposed method vs. replay based methods and network expansion methods, in terms of memory usage. I think that these concerns need to be fully addressed in this paper, to really demonstrate the advantages of the proposed approach. As I can infer from Table 1, most of the gain in final accuracy is due to a gain in LA. I would strongly suggest to clarify this point. In the experiments, I do not see the reason for not comparing with network expansion methods.<|endoftext|>This paper proposes a  task aware adaptive learning rate method, TAG, for continual learning. The optimizer TAG is to promote the learning rate if they are similar to previous tasks, while decreasing the learning rates if they are dissimilar to previous tasks without storing previous examples. The authors combined the proposed method with existing optimizers, such as Adam, SGD, etc to demonstrate the effectiveness of the proposed method. Weakness:*  The technical novelty of the proposed method is limited. From this aspect, TAG can be seen as a memory free version of LA MAML. But it is based on ameta learning setting and hence beyond the scope of our paper”. They are not in meta learning setting, but instead in the continual learning setting. *  The proposed method seems to only work on the task aware setting. *  The experiments on more other architectures, such as MLP may demonstrate the effectiveness of the proposed method.<|endoftext|>The authors present an approach for lifelong learning where each task is processed in a single pass. The learning rate is decreased when there are many dissimilar tasks to avoid catastrophic forgetting. The paper presents an adapted RMSProp algorithm, but the procedure can be adjusted to Adagrad and Adam as well. Pros:* General approach that can be adapted to multiple optimization algorithms and used in other meta algorithms* Strong performance in a single pass scenario* Can achieve better learning accuracy than naive methodsCons:* The effect of the proposed procedure on optimization algorithms is not studied enough. Additional insights, like visualizations of trajectories, will help to build an intuition and boost the method adoption in practice. * Effect of task order is not studied/highlighted: the method seems to be very dependent on the task order and it is unclear how the order is chosen in experiments. Minor comments/questions:* what is the point of having \lambda(t,t) in eq.3, i.e.why do you need to multiply the currently accumulated gradients by task similarity to itself? What is the reason for this? The presented method is a general procedure that helps to improve performance of many methods in lifelong learning scenario. The presented experimental results are convicing. However, the paper would benefit from more research and intuition on why the approach works. I recommend to accept the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper presents an interesting approach to clustering of graphs sampled from graphons. The papers presents a number of guarantees for the quality of obtained clusterings and a few preliminary experiments. On the other hand, in practical applications we don t know the order of the vertices, hence there is a lot of work on graph isomorphism, graphon cut norms, and other tools to deal with that issue.<|endoftext|>This paper studied the clustering problem exactly as encountered in practice. Overall, the proposed idea combines existing approaches on graphon estimation and spectral clustering to lead to the proposed clustering and testing approach. Strengths:  Deals with the issue of vertex correspondence for clustering of a population of graphs.<|endoftext|>The paper proposes a graph distance based on graphons that can operate on the small sample size regime; this is possible by exploiting a large number of nodes. The paper also shows theoretical guarantees associated with two existing clustering methods and a two sample statistical tests when operating with the proposed distance. The developed method is applicable and effective to small samples of graphs and it appears novel to me. Overall the paper is well written. 2017.A new graph based two sample test for multivariate and object data.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; ## SummaryThis paper explores the use of a non Gaussian diffusion process for Diffusion Probabilistic Models. The main motivation why Gamma distribution is used seems to be that Gamma distribution is more expressive than Gaussian due to having an extra parameter. ## Strengths* Interesting extension to diffusion models, a new, relatively underexplored class of deep generative model. ## Main concerns and questions* Lack of motivation:  I think it s great that the paper shows sample quality improvements on both image and audio dataset. However, I have a few concerns. * (Minor comment) The current version of the manuscript I believe has an unnecessary amount of mathematical detail in the main text. POST REBUTTAL: Based on the authors  response, I raised the score to 5. If the authors can better address these issues, I am willing to adjust my review accordingly.<|endoftext|>This paper proposes to use Gamma distribution noise to replace Gaussian distribution noise in the denoising diffusion probabilistic models (DDPM). It includes experimental results conducted on speech generation and image generation. Comments:    Figure 2: Which model is used as the baseline, DDPM or DDIM? However, all the audio samples from the proposed model with any number of iterations contain similar white noise like static noise, which does not present in the baseline. It can be helpful to include discussion on both the pros and the cons of the proposed method.<|endoftext|>But the sum of two Gaussian distributions is an unnormalized mixture model. There is some discussion of this issue but it is relatively superficial. The experiments are good, but not extensive relative to other recent papers on generative models, including diffusion and score based models. I also thank for the reviewers for the added quantitative results, etc in Table 3. 2) Experiments:  I have several very minor comments on the experiments. This paper describes a novel diffusion model based on Gamma noise instead of Gaussian noise. The approach is shown to produce good empirical results compared to the Gaussian case. But there are questions about the quality of the writing, the motivation for the use of Gamma diffusion, and the formulation, all of which the authors may want to address. 3) Minor issues of writing:a) The paper states in multiple places that the sum of two Gaussian distributions is Gaussian and the sum of two Gamma distributions with the same scale parameter is also a Gamma distribution. This is not strictly true.<|endoftext|>Tho goal of this work is to replace the original Gaussian noise distribution in DDPM with Gamma distribution, since the Gamma distribution is with more degrees of freedom. To achieve this goal, it reformulates the diffusion and the corresponding reverse process, and also deduces the variational lower bound. The derivation of the proposed DDGM model is sound, but the experiments are not sufficient enough. 1.Some obvious mistakes are needed to be corrected. According to my understanding, all the terms of $\bar{g}_t$ in Eq.(13) should be replaced with $g_t$, please have a check. Even though the idea is interesting, this paper should be further improved from the mathematical formulations and the experiments. If so, I tend to increase my score.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This work tackles a latency constrained structural pruning method by formulating the global resource allocation optimization problem and addressing them via an augmented knapsack solver. During the pruning process, to estimate latency on a target device and accuracy drop, this work uses look up table and global saliency score, respectively. The proposed approach is validated on classification and detection tasks with desktop GPU (Titan V) as the target device. Strengths  This paper is well written, well motivated, and clear presentation. Formulating the hardware aware structural pruning as a knapsack problem is new. Weaknesses  The reviewer s main concern is that this work is one of the lookup table(LUT) dependent methods that is not new and has critical limitations. The reviewer guesses that this method also would not guarantee high performance on the target tasks when the performance of LUT is poor. While this method is  hardware aware , the target devices used in the experiments are just two (GPU and Jetson). Recent hardware aware methods for the efficiency of neural networks under latency constraints have been handled many hardware platforms and device entities. [3] Brp nas: Prediction based nas using gcns, NeurIPS2020. This paper has a clear motivation and introduces a knapsack algorithm for NAS, yet, validates the proposed method on limited devices and has weak novelty since the latency estimation is dependent on LUT.<|endoftext|>This paper is an effort to perform structured pruning with regard to inference latency on hardware. Evaluation shows that this method effectively reduces the latency with regard to hardware via pruning. Importantly, the paper works on a very important topic. I believe the main benefits of the work are (1) formulating hardware aware pruning while retaining good performance and (2) doing this efficiently. The formulation of the overall pruning procedure into a Knapsack problem seems to be where the main benefit comes from. Neuron grouping proposed in the paper also seems to be another source of speedup. Questions I have are:* While the paper claims that this can be applied to other platforms too. Could you provide how long this would take for a new platform? Can this be done efficiently? Hardware awareness of pruning, while touched upon by some works, still opens up large potential for inference latency reduction. This paper proposes an interesting direction in optimization where each layer is assigned importance score and measured latency of each layer. Then, the overall pruning problem is formulated as a knapsack problem.<|endoftext|> +  A.Paper summaryThis paper proposes a method to do channel pruning. By building up latency and important score for each group (or channel), the paper use an ILP solver to select the channels that can achieve the best trade off between latency v.s. Accuracy.The evaluation shows that the method achieves better speedup compared to traditional channel pruning with almost no accuracy loss. The Pareto curves show the effectiveness of the algorithm. +  C. Weaknesses  In my opinion, the novelty is not very exciting. There are a lot of design choices (e.g., computing important score) and the alternatives are not discussed. Also, my intuition is that using dynamic pruning during evaluation can push the Pareto curves further (fig.3); so it should be an important baseline. There is also a lot of design options in each part of your design. For example: 1) Computing the importance score you can use the method here (https://arxiv.org/abs/1810.02340) by computing the gradient of each neuron. As far as I can tell, you did not include the extra GPU training time. Minor:Your title is so vague and it looks like a survey paper. You can consider changing the name of your paper. The quality of this paper is good. However, I think the novelty of this paper is limited.<|endoftext|>The authors propose to formulate structural filter pruning as a global resource allocation optimization problem with latency constraints. To solve the resulting knapsack problem, the authors devise an augmented knapsack solver and further propose neuron grouping to reduce the pruning space and computational cost. However, some important details of the proposed method are missing, which makes it hard to follow. Extensive experiments on image classification and object detection tasks show the promising performance of the proposed method. The performance is promising. 2.The authors devise an augmented knapsack to solve the resulting combinatorial optimization problem. For example, what does dp_array denote? Moreover, the authors do not provide any explanations regarding Algorithm 1, which makes it hard to follow. 2.In Section 3.3, the authors state that the proposed method groups the neurons sharing the same channel index from the connected layers in a network with skip connections, which have been proposed in [1 2]. Does the performance improvement of the proposed method come from a better metric of computation cost or a better solver? More discussion and experiments are required. It would be more convincing for the authors to provide more results on lightweight networks with residual connections (e.g., MobileNetV2 [3]). **References**:[1] Centripetal sgd for pruning very deep convolutional networks with complicated structure. CVPR 2020.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This would be good to mention. Rather it is possible to find data points which have bother low loss and are "invisible" in some metric. This is interesting, but due to the high dimensionality of the investigated neural networks not too suprising.<|endoftext|>In general, work is at a preliminary stage, and many of the ideas discussed in the paper have been explored in the past, albeit in different forms. I would also encourage the author to support the claims by a strong set of experiments.<|endoftext|>The paper investigates the problem of the susceptibility of neural networks to adversarial attacks. 2. the Author posed an interesting hypothesis (if I understood it correctly) and I would encourage them to investigate it further. 3.If we abandon the adversarial example point of view, the work presented in the paper can be understood as analysing what the neural network has learnt for each class.<|endoftext|>3.The work in its current version looks more like a report instead of a conference paper. I would suggest the author improve the paper organization by moving most of their implementing details to the appendix or a single section. 4.The authors have found trackable patterns existing in adversarial examples. The authors observe some interesting and trackable patterns in adversarial examples. However, I believe the current version still has a lot of room for improvement.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper aims to study properties of deep convolutional models via the surrogate of simple hierarchical kernels with convolution and pooling layers. The authors characterise the underlying RKHS and their norms, and provide generalization bounds. The paper addresses an important question, is well motivated, and carried out very thoroughly.<|endoftext|>How were these design choices made? What are the axes in the illustration? The paper makes a valuable contribution to our understanding of convolutional networks for image classification, via expanding the theoretical characterization and generalization guarantees of their associated GP kernels, and offering experiments with simple architectures that match the state of the art for a kernel method on CIFAR10. In the main review I offer some suggestions for improving the clarity of the presentation.<|endoftext|>In this paper, the authors study some simple convolutional kernels. The question of what accuracy can be achieved by kernel methods is important in its own right, and can potentially inform us on the performance of other methods (CNNs). The paper is very well written, with detailed and interesting discussions, which makes it a very pleasant read.<|endoftext|>First, it is hard to find in the paper where h1 and h2 are defined, and what are their dimensions. ", there is a Phi which should be Psi? Theoretical results on the approximation and generalization of convolutional kernels are limited in the literature.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper proposes the idea of a "disguised subnetwork", which are hidden random subnetworks that can be transformed into a well performing subnetwork. The paper introduces PaB as a way to uncover these subnetworks, by first searching for a mask over the random weights using pruning at initialization techniques, and then learning a transformation on the subnetwork. The paper further shows that this PaB process can be efficiently implemented, offering significant advantage over prior work. The paper tackles an important question, which is to rethink the optimization strategies of sparse neural networks, especially when using the idea of masking as training. The results are significant as it reduces training cost compared to fully trained networks while maintaining competitive performance. The authors mention that this is due to the fact that the random initialization can be stored using a single seed. While this may be true in some cases, I would be hesitant to rely on this attribute as the seed may not result in the same parameters in different machines. Most of the work in supermasks consider the "signed constant" variant, where the weights are converted to a single signed value for each layer. This parameterization has often shown improved performance over the parameterization using actual weight values, and leads to significant reductions in model complexity and size. The method is novel and achieves better efficiency performance tradeoff than the various baselines. I think the contributions of this paper are significant and would support its acceptance.<|endoftext|>Optimizing sparse neural networks is an important topic due to theircomputational and space savings. Building on the work of the lottery tickethypothesis, others have shown there exist hidden subnetworks within randomlyinitialized NN that have good performance. The authors extend this definitionto disguised subnetworks, which contain hidden subnetworks as a subclass. The presentation of the PaB is clear and well motivated with good context. The results are quite convincing that PaB scaled well to larger networks,something other methods lack, while providing good accuracy for much smallerand easier to train models. I would have liked to see some more theoretical discussion, though the authorsadmit this work is primarily empirical. 2.It would be nice to discussion of other weight transformations U in the unmasking phase,    that may be useful. 3.Though the definition of disguised subnetworks is new, and the generalizedapproach is novel, and the results speak to the validity of the approach, thePaB algorithm is a relatively straightforward application of two existingmethods. The paper is well written, and does a good job covering all the related material. The definition of disguised subnetworks is novel and will be useful for future researchers.<|endoftext|>Malach et al.(2020), on the work(Ramanujan et al., 2019) comment: “within a sufficiently overparameterized neural network with random weights (e.g.at initialization), there exists a subnetwork that achieves competitive accuracy”. This paper extends the definition of the hidden subnetworks in randomly initiated neural networks. Here the key standpoint is, rather than the generality or the novelty of solutions, to demonstrate that instead of solving a dense network and “transforming the weights”, or to mask for a sparse network, one can combine these methods to have a sparse network that also performs better than the alternatives. If we think of the above solution process as a two phase problem, the author(s) use the literature on sparse neural networks for the first phase and use the literature on binary neural networks for the sign flipping phase. Maybe it is better to connect with before “, that is, if a sparse NN …”? ** Problem (2) formally describes the “optimal hidden subnetwork” problem, and it immediately follows from the definition of problem (1) that what the authors suggest will be an ‘improvement’ over the training data. I am not very sure about whether it is common to say the problem is NP hard. However, in my view, this work is more like a case study where heuristic methods are being used iteratively. Namely, the contribution is actually: “taking a recent paper that finds a masking variable, and just flipping the signs w.r.t.the training loss”. This is still an interesting result, but I also would like to highlight the distinction from my perspective. So this algorithm does not find a disguised subnetwork (as claimed in the introduction), rather finds a subnetwork and then re assigns the weights, which is *a* disguised subnetwork, but not a new architecture. I believe the problem definition of disguised networks is very general but PaB first finds the hidden architecture so this is not a new way of finding subnetworks. The fact that they are training independent (or sometimes called “without training methods”) is the main focus in the literature around them.<|endoftext|>This paper presents an algorithm named peek a boo (PaB) to optimize network pruning (at initialization) and optimization (limited within flipping the sign of weights). This setting has not been studied by prior works. A two step algorithm was designed   pruning first, optimization second. Though the setting is new and interesting, I shall say that its value to the community has not been perfectly revealed based on the existing experiments. In the studied cases, the neural networks and datasets are mostly small. The value of this work is supposed to be accelerating large scale network training, but I am not sure that it scales up well. This can align both parts and hopefully improve the final performance. Since ImageNet results are missing, I am not sure if the approach can actually scale up. In particular, according to Tables 1 and 2, PaB does not show a significant advantage over SGD, which further limits the application of the proposed approach. I think ImageNet experiments might be a good add on to this paper. Overall, a bit below the acceptance threshold.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The motivation for the two cost functions and how they represent different learning "principles" could have been clearer. strengths:  novel idea to use curricula to find signatures of different loss functions  curricula inspired by those used in neuroscience experiments   directly applicable. minor comments:  Figure 4 is very useful.<|endoftext|>To this aim, the authors show that the learning principles governing two systems (RNNs) trained according to different objective functions can be inferred by examining the curriculum learning dynamics, despite the final “state” (effective dimensionality) of the networks is indistinguishable. This research work is interesting and recent related work is adequately referenced. The introduction and discussion sections are well written and clearly motivate the research work and its potential impact. I understand the idea (and the importance) of adjudicating between different “learning principles”. Discrepancy is of course important for the task being simulated. However, the approach proposed by the authors might also fit well with other recent frameworks that have been used to simulate numerical discrimination, where we could differentiate between target based learning (i.e., the goal is to classify input numerosity [2]) or representation based learning (i.e., the goal is to first build a generative model of the environment, and then eventually learn discrimination tasks [3]).<|endoftext|>In this paper, the authors propose an approach using curricula to identify how a system has learned. Using two commonly used tasks in neuroscience: evidence accumulation and delayed decision recurrent neural networks (RNNs) are trained using two different loss functions (target based and representation based). On the other hand, the learned state space trajectories of RNNs are indistinguishable thus unable to disambiguate which loss function was used for training. 2.The idea of using the learning time is new 3. The idea of using the curriculum learning time as a criterion to disambiguate learning processes is novel and the authors have done a good job in clearly demonstrating that using a wise choice of tasks and curricula.<|endoftext|>They suggest a similar approach can be used to identify learning rules in animal neuroscience experiments. Strengths:The general idea of using learning behaviour under different training curricula to decipher something about the underlying learning process of a system is really interesting. Learning rules vs loss functions was a regular miscommunication. The learning rule is the same for both RNNs, which is batched gradient descent with an Adam optimizer. A related paper that analysing biological learning dynamics to understand different potential underlying learning rules is Cao et al 2020.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The paper addresses the problem of very large scale deep neural network training through model parallelism. Data parallelism cannot help when the models are so big that they cannot fit in a single accelerator. The paper proposes a new pipeline called WPipe, that builds on the existing ones, but addresses the previously mentioned two issues. A careful construction is proposed, based on two groups of weights. The experiments show that compared to state of the art PipeDream 2BW, WPipe is more memory efficient (by36%) and has higher throughput (1.4x). Although the final accuracy is similar for the two schemes, WPipe is shown to have weight update semantics closer to data parallelism. The paper proposes a new model parallel pipeline called WPipe, for DNN training. Theoretical analysis and an extensive experimental evaluation on text and image data show that WPipe outperforms state of the art schemes, being 1.4 times faster while using 36% less memory.<|endoftext|>This paper introduces a novel pipeline training strategy WPipe. WPipe divides model partitions into two groups and updates each group alternatively, which eliminates half of the delayed gradients and memory redundancy compared to Pipedream 2BW. The experimental results show that WPipe can achieve higher throughputs and reduce memory footprint with similar final model accuracy. The proposed pipeline strategy is novel and effective. The term "model parallelism" should be used more precisely. In your paper, it seems you totally ignore the third type operator partition. I think all numbers in the last column of Table. Take GPipe for example, activation recomputation only reduces the memory footprint of a micro batch on a stage by a constant factor. The same analysis applies to other pipeline schemes. 5.How do you split the models into pipeline stages? Although the writing can be improved, I recommend accepting this paper.<|endoftext|>The paper presents a novel pipeline parallelism technique for training large DNN models named as WPipe. The method aims to improve upon existing pipeline methods such as Pipedream 2BW by having a better memory efficiency and more fresh weight updates. There is some novelty in the new pipeline scheme, but I feel it is incremental. The presentation and clarity in the paper could be improved   the authors should re use terminology used earlier in the pipeline parallelism literature to make the presentation consistent and less confusing. Pros:  Simple idea that seems to work very well empirically. "model partitions"  > do the authors mean pipeline stages? What is the maximum number of pipeline stages the authors tested the method with? The novelty factor in the proposed work is incremental and I also feel the paper can benefit from some more polishing of the draft due to the presentation issues I outlined.<|endoftext|>The paper proposes, WPipe, a technique for reducing the memory overheads of weight versioning and improving freshness of weight updates in pipeline parallelism through a novel scheduling of the the pipeline stages. 2.It proposes an interesting approach to address key issues surrounding memory efficiency and weight update sematics in existing pipeline parallelism approachThe weakness include:1. The overall appears narrow and incremental in the sense that WPipe is fixing a memory efficiency bug of PipeDream 2BW relative to PipeDream flush. In other words, WPipe is trying to deliver the best of both worlds for these two existing approaches. As a result, it is not obvious how the results applicable beyond the PipeDream* line of pipeline parallelism. 3.I feel the writing could be greatly improved as I found multiple portions difficult to follow. Although I am not hands on with pipeline parallelism, I feel I have a pretty solid understanding at the high level, but yet this draft was less accessible than expected.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors build on SARAH and propose AI SARAH which aims to estimate the local Lipschitz smoothness parameters on the fly. The authors provide theoretical analysis for a modified version of AI SARAH and provide extensive practical experiments for AI SARAH. What about adaptivity to strong convexity? Probably,it should be explained that this "full adaptivity" is not to the strong convexity. Does the algorithm use this information in any way? Moreover, if the paper wants to make a claim more on the practical side, then I am curious what happens with "a rough and systematic tuning" for other algorithms.<|endoftext|>This paper studies the stochastic recursive gradient method for finite sum problems. Strengths:1: This paper proposes a practical variant of SARAH with faster convergence by exploring the local geometry of the stochastic functions. 2: Extensive experiments are conducted to verify the effectiveness and efficiency of the proposed algorithm. Weaknesses:1: The contribution of SARAH over SVRG is mainly on convex functions rather than strongly convex functions. After the discussion, some of the concerns are solved.<|endoftext|>In this paper, the authors proposed a SARAH type variance reduced gradient method that adaptively and automatically selects the stepsize. Extensive experiments are done in solving convex problems, and the comparison with other state of the art first order methods is also carefully presented. The authors presented a parameter free and hence tunning free variant of the SARAH variance reduction method. This is the strength of the result. The reviewer does appreciate the efforts in the empirical study. Therefore, the global Lipschitz constant may be much larger than the local Lipschitz constant, resulting in a more conservative stepsize, i.e., the stepsize $1/L_{global}$ may be much smaller than the stepsize $1/L_{local}$. Therefore, the theory does not explain the strength of the algorithm.<|endoftext|>(2) The experiments show that AI SARAH outperforms some main stream stochastic optimizers. Concerns:(1) My major concern is that both the theorem and the experiments only involve convex optimization, while nonconvex optimization is more popular in machine learning, and SARAH achieves near optimal sample complexity for nonconvex optimization. For the convex optimization, the complexity result can be directly obtained from Theorem 1. **I reject this paper mainly due to the above three concerns. (3) For nonconvex optimization, you might use STORM algorithm [1] instead of SARAH, since both of them achieve the near optimal sample complexity for nonconvex finite sum optimization, but STORM does not require full gradient.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper considers generalized label smoothing (GLS) for binary classification tasks where the smoothing coefficient is allowed to be negative (negative label smoothing, NLS). The authors show the connection between GLS and several existing loss functions through elementary algebraic manipulations of the expected loss under (generalized) label smoothing. They also analyze GLS in the noisy label setting where the labels are randomly flipped and show that using negative label smoothing is desirable in the high label noise setting. While that paper is cited, the results of this paper are not put in context with their results. This is a fundamental conceptual problem with the paper.<|endoftext|>This paper studies label smoothing when learning with noisy label. It proposed generalized label smoothing (GLS) containing positive label smoothing (PLS) and negative label smoothing (NLS), where NLS allows the smoothing parameter to be negative. The authors proposed a very simple yet interesting idea NLS. Then they demonstrated the theoretical findings by conducting experiments on UCI and CIFAR datasets. However, I have several questions/comments that need the authors to address. It is not obvious to me. However, its practicality is limited.<|endoftext|>A recent work shows the benefits of using label smoothing. The above observation can imply that label smoothing is considered unfavorable with a high label noise rate. The proposed generalized label smoothing scheme ties closely to several existing learning with noisy labels solutions. This result quantifies the scenarios of using positive and negative label smoothing. Could the authors clarify its use? What will happen when only an imperfect r^* is known? I tend to accept this paper. It does explain some interesting things, and the experiment is also sufficient.<|endoftext|>This paper provides understandings for a generalized notion of label smoothing (GLS) when learning with noisy labels and theoretically show that negative label smoothing improves the expected model confidence over the data distribution. My question is why the authors choose these datasets. The empirical experiments on multiple benchmark datasets demonstrate that with the presence of label noise, negative label smoothing (NLS) becomes competitively robust to label noise. 6)	With regard to the comparison results, statistical tests are needed in the comparison results.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The authors propose and explore  a method based on changing (mutating) the labels of training data to do model selection (selecting among learningalgorithms, or hyper parameters, etc). The paper is well written and well motivated, and proposes a simplemethod for model validation, mutation validation (MV), based onchanging the labels of training instances, and measuring how well thelearner fits to the new mutated problem, and combining the original and newaccuracies. However, the major shortcoming of the paper is performing the modelselection experiments on only toy problems with 2 features. I double checked my implementation andmatched the scores and trends with what the authors have published(more details below). It seems to me model selection is the main potential application ofMV, specially because unlike cross validation, MV doesn t give you asense of performance on test (beyond training performance). (of course, both the learningalgorithms to choose from and problems should have some diversity) More details of my experiments:The proposed MV measure is simple and elegant, and it was simple enoughthat I implemented it and compared it to say 10 fold cross validation,head to head on the breast cancer and Bank (binary class datasets, as I didn t implementthe authors  label shifting method for multiclass ). Isaw a similar result on another dataset (text classification). I first matched scores from my implementation (MV andcross validation) and validations with the author s results on thebreast cancer dataset (Figure 3, eg 3rd plot is for Wisconsin breast cancer). So a single decision tree gets a MV score of around0.93 (eta of 0.2 as in paper) and that s the peak of MV for breast cancer. And it may be that a simple combination of these accuracyscores is not sufficient for successfully picking a model in practice. MV, as developed in this work, isappropriate when 01 error is useful. (My review before reviewer discussion and experiments) I enjoyed reading the paper! How about other theoretical/conceptual potentialdrawbacks? Probably it s better to mention it here.). It appears this label swap generates systematic error, not random noise, but I am not sure what the implications are (eg could be easier, than random label noise, to fit the training data, since there is a pattern to the injected label errors, but generalization degradation can be impacted further by this, vs random noise.. ) Later I saw in the appendix that the theoretical result from the appendix motivates this choice vs simpler random noise. it would be good to mention it when you state you are using label swapping, vs uniform random .. Also, it would be good to empirically see whether label swapping (or "label shifting") vs random uniform selection of labels, makes a difference. pg 9.insert  more  in "We show that MV is effective and stable than thecurrently adopted CV, validation accuracy, and test accuracy"pg 9: (in contrasting with noise injection) perhaps better to replace inputs  with  features , eg. Minor: m is used as training set size at one point (while in the rest of the paper, it s the mv score). The paper is clearly written and well structured, with goodmotivation, examples/illustrations, and the techniques areunderstandable and novel to the best of my knowledge. The workis mainly empirical, but the experiments need to be substantiallyextended, and the basic approach may need further development, to show the benefits of mutation validation.<|endoftext|>The paper explains the larger the score the better the learner fits the training data and hence can be used for model validation. In this study they explore 8 different learning algorithms, 18 datasets and 5 types of hyper parameter tuning tasks. There are several things to like about the paper  Novelty  (1) I think the idea presented in the paper is novel and interesting. Clarity (1) I found that the paper was easy to read (2) The synthetic experiments in Figure 2 provide an intuitive explanation for the algorithm at known distributions. However the claim of overfitting mentioned here is not justified empirically by showing the generalization error of the model. What does overfitting mean in this context ? Is the set used for CV very small that it is no longer determining overfitting? Can you show that MV finds the best model compared to CV when the generalization error is evaluated on sufficiently large dataset? Moreover the paper describes in Figure 8 that with large scale datasets MV no longer shows similar trends as shown in Figure 3. This makes it difficult to evaluate the significance of this work. (3) In Table 4 what is the variance in the test accuracy across the different recommended depths? What does the stability here translate to empirically in terms of model performance? Metamorphic testing is a more definitive test to find bugs in a program (if the relation is violated implies there is a bug) whereas the relation/method proposed by the paper is used in a more relative context (comparing it with nearest hyper parameters) rather than being a definitive violation of a relation. (5) How do the two terms   (a) difference in training accuracy of models trained on original data and training accuracy of models trained on mutated data and (b) Accuracy of model trained on mutated data based on the original training data change   change as MV changes empirically? (6) How is the hit rate in section 4.1 calculated? The idea presented in the paper novel and interesting to me. My major concerns are with the experimental sections   I did not find it convincing enough and I am unsure of its use case given the current results.<|endoftext|>This paper proposes a new form of model validation called Mutation Validation (MV), which analyzes how well a model fits the training data by first training two models: one using the original training labels, and another using training labels in which a fraction of them (<  0.5) have been mutated (i.e., the next available class label is used); then, a goodness of fit measure is computed via the accuracy of these models on the mutated and non mutated subsets of the training data, with the idea that the model trained using mutated labels should have similar predictive performance to the original model if less affected by the mutated labels. The paper provides a thorough empirical comparison to other model validation strategies such as cross validation (CV) and using a validation/test set; the results suggest that MV provides a more sensitive response to changing hyperparameter values and can better detect the unnecessary complexity of the model than any of the other methods. They also find the value of MV tends to increase with addition of more data, signifying a resilience to mutated labels with additional examples; and they show that MV is relatively robust when choosing what fraction of training examples to mutate. As far as I know, this work is original, and applies techniques of Mutation Testing and Metamorphic Testing from software engineering to develop MV. The proposed measurement itself is simple, easy to understand, seems to be technically sound, and has a justifiable theoretical foundation. WeaknessesI think my main concern is how to choose the fraction of training examples to mutate. Why did the authors choose the mutation protocol to be choosing the next label in line, instead of selecting a different label uniformly at random; would the results be significantly different if using the latter? MV only works for classification tasks; can MV be adapted for regression tasks? 3: consider using A(S) to represent a model trained on the data subset S; or, rephrase to say f(S) is the OUTPUT of the model for the subset S.Is AUC used as the performance metric for the binary classification tasks? AUC is a more sensitive measure of model performance and may lead to different model selections especially for methods such as CV. Can the mutation of both lead to a better measurement of model validation/selection? How does MV perform in the context of model generalization, especially when tuning multiple hyperparameters? My main concerns are how to choose an appropriate fraction of the training examples to mutate, and how this method can be adapted to regression tasks.<|endoftext|>This paper introduces new approach for validation of ML models which is based on top of ideas imported from software development & QA (so called mutation and metamorphic techniques). The core claimed advantage consist in the ability to avoid cross validation and validation and test sets (the famous approach to avoid overfitting while learning a ML model). The work contributes also a huge empirical study with experiments for dozens of datasets and several models and hyperparameter tuning. Strengths: 	A novel approach in the area of validation techniques, which are very conservative 	Huge empirical validation supported by theoretical basis (see Appendix) 	Large potential for future studiesWeaknesses: 	The method looks highly dependent on a particular evaluation metric (performance of a model): for instance, the paper address one case about accuracy (the formula (1), arguments in Sec.2.2, theory in Appendix). It is not clear how to develop a measurement m (analogue of (1)) for other performance techniques 	Overfitting is a very important problem in the industry. Minors:1)	Fig.1 “A better learner is less”: Which ones are better learners? 2)	In Sec.1, 4th paragraph: “The model recommendation hit rate for MV is 92%” model recommendation hit rate is not a clear term for introduction. 3)	In Sec.1, 4th paragraph: “set, the average variance is 0.004 and 0.008” Is it reasonable to talk about var among 5(?) runs?4)	Sec.3, RQ1: how do we define the effectiveness? I vote for acceptance due to the “fresh blood” in the so conservative area of validation techniques. So, a lot of things would be nice to check and study to get comprehensive overview of the novel approach, but this first step is enough to be presented at ICLR 2022.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; At a high level, this work considers the problem of designing machine learning systems that generalize well even when the "domain" (i.e., the data generating process) under which the system is tested (the "target") does not match that under which it was trained (the "source"). More specifically, the authors look to provide an answer to the question of what makes a "good" representation or encoding (a stochastic transformation of the original features), from the perspective of achieving a small risk (expected loss) on the target domain, assuming the learner only has data (and thus representations) from the source domain. To this question, they provide one answer by showing that in a generalized covariance shift scenario, a natural notion of representation optimality (their "IDG optimality") is only possible if a representation minimizes the (best) risk on the average target distribution, while ensuring that the support of the representations is constant across domains. This is obviously a strong requirement, and they show that one cannot expect to satisfy it without domain knowledge going beyond the source. They also show that there is some hope for special cases of domains in which we have augmentations that preserve label information and "cover" the support of the input distribution across all domains. In such a case, the authors show that their strong optimality requirement can be satisfied by designing the encoder to maximize the mutual information with the augmented data. Relaxing this objective leads to objectives that are more practical, and the authors complement their theoretical analysis with a rather in depth set of empirical tests that evaluate the efficacy of their methodology for representation learning. High level theoretical points are quite clear in the main paper (details in the appendix), the relation to existing work is described, and the experimental results are distilled into new questions/insights which are easily parsed by the reader. *Weak point(s) of the paper*  The theoretical results are stimulating, but they are quite idealized and leave a substantial gap between what we can know and check in practice, and what we want to know formally. For example, while the notion of a domain covering augmentation is very simple mathematically, but one expects that evaluating whether we have something close to such an augmentation in practice becomes quite challenging when tasks are difficult to describe in words or in simple visual terms. *Questions*  Most of the notation is immaculate, but I would like to confirm about  $\\mathrm{R}_{h}^{d}[Y \\,|\\, Z]$.<|endoftext|>The paper studies representation learning under covariate shift. Under the IDG setting and the proposed assumptions, the paper gives a so called variational characterization of the optimal representation. This characterization shows that the optimal representation should remain discriminative while has the same support across domains. It is argued that without any target information, no representation can do uniformly well over constant representation, thus supporting the necessity of target knowledge. The paper provides practical objectives of the proposed variational characterization by self supervised learning using domain covering augmentations. The authors emphasize that the condition they proposed is also necessary, but how non trivial is the necessity? Intuitively, if the representation of the source and target could have different support, then it is hard to guarantee good performance in the target. So there appears a gap in the theory: The theory requires a strict domain covering augmenter, while in practice this can only be nearly satisfied. Numerical results: It seems that the benefit of domain bottlenecks is not very significant. In Table 1, sometimes the improvement of CLIP S/L + CAD over CLIP S/L is not significant enough, e.g., 94.7\pm 0.4 of CLIP L+CAD vs 93.7\pm 0.8 given in the PACS column. The only significant improvement is seen in the DomainNet column where the improvement is about 1%. Why couldn’t there exist other objectives not given in this paper? The paper provides insights on the optimal representation under covariate shifts, which is a potentially important problem.<|endoftext|>This work focuses on learning representations for domain generalization. Under several assumptions (importantly, domain covering augmentations), the IDG risk objective can be altered (Thm.1  > Prop.2  > Eq.(6)) to a more practical one. Experiments on standard benchmarks show that the proposed method can out perform SOTA alternatives. The main idea is to learn a representation that matches the support across domains while maintaining discriminative information (Thm.1). Starting from idealized domain generalization risk (page 2), the paper goes through a long path of derivations and assumptions to derive practical objectives. 1.Technicalities (and required clarifications for some parts)  It is surprising to see that the current version does not discuss hypothesis class in question. For example, after (2), what s the candidate hypothesis set for h (argmin_h) when defining the set of source risk minimizers? However, both R_{IDG} and R involve expectations over domains or pairs of domains. It is not clear how "the key requirement" can be satisfied by the given augmentation. It would be much better if the paper can provide a running example about A, X and Z. It is not clear why p_{A|x} p_{A|x } implies they have the same Bayes predictions. It is unclear where this "key requirement" comes from. If so, the augmentations in Fig.2b look arbitrary and do not make sense. "Enforcing the support constraint for augmented data (X, A)", but the constraint of Eq.(5) does not involve the augmentation A.  Sec.4.2      What is the definition of the domain bottleneck B[Z, D]? And why it is related to the support constraint in Eq.(5)? The improvement in Table 1 seems to be mainly due to the fact that CLIP is pre trained using a much larger dataset. Thus the proposed methods are only marginally better than vanilla CLIP if I understand it correctly. The results on TerraIncognita can justify this. To add some discussion, how can we ensure domain covering in practice when even CLIP may not have sufficient coverage? In summary, the paper theoretically analyzes the conditions under which domain generalization would be possible. However, the writing is not clear at places and the empirical results are not very convincing.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The authors provide guarantees (in term of the probability of correctly labelling any given vertex) regarding the learning of communities in the stochastic block model. The proof technique relies on the analysis of three types of neurons : the so called "good" neurons which contribute positively to the correct labelling, the so called "harmless" neurons, whose effect on the classification can be neglected and the "bad" neurons which contribute negatively to the classification. The contribution is interesting although some of the aspects of the paper could be improved (see my comments below). Detailed comments  Abstract:  I would be careful (remove or expand) with the sentence “This state of affairs is in contrast with the steady progress on the theoretical underpinnings of traditional dense and convolutional neural networks ”. You should state that clearly.<|endoftext|>This paper presents a theoretical result on performing label propagation in a stochastic block model (SBM) using a trained graph convolutional neural network (GCN). The result is quite interesting and leveraging well established graph models to provide guarantees for graph neural networks is certainly a promising direction. I have the following comments for the authors:1) The notation and presentation can be improved. The function $g$ is used in page 3 but introduced in page 4. The main text has corollaries of results in the appendix, there are several references to equations in the appendix, in page 6 there is even a reference to the fact that some of the results are stated in the "full version of the paper", and is unclear to me what the authors are referring to here. 4) In the experiments, the choices for $a$ and $b$ seem to be quite specific, is there any reason for these?<|endoftext|>The authors demonstrate that with high probability over the initialization and training data used, a GCN will efficiently learn to detect communities on graphs drawn from a stochastic block model (SBM). This paper can accelerate much needed theoretical work in the graph neural networks. Weaknesses : 1) The presentation of the paper is bad and really hard to follow. The current version of the draft needs some work to make it more digestible and easier to follow. 3) The experiments lack any discussion or analysis with regards to the results and how they are tied to the theory introduced in the paper.<|endoftext|>The paper presents a theoretical analysis of two layer graph convolutional networks (GCN). The main goal is to study the behaviors of GCNs when the inputs are random graphs generated by stochastic block models. The paper presents an interesting technical approach studying the concentration or separation of GCN outputs by understanding the dynamics of three parameter settings (good, bad, harmless). However, currently, the paper faces two major issues:The main result is not informative enough and may contain erroneous claims. The main theorem seems interesting but can be problematic in the way it is interpreted. The experiments are designed in a way that does not support the theoretical claims.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper presents a learning + adaptive control approach to synthesize control policies that fulfills STL specifications. In my opinion, here are the strenght and weakness of the paper. The authors should demonstrate the benefit of learning. 2) Theorem 1 does not mean much as the transient tracking can be really bad, thus violating the STL formula.<|endoftext|>In this reference the authors also learn a component to augment control inputs. This paper describes an approach to adaptive control incorporating a learned component. The paper is interesting to the community and combines theory with experiment in a coherent manner. 4.In general the paper oversells some of the claims. In addition more discussion could be added about the controllability requirement and how limiting this is; I acknowledge that the paper has made some efforts to be precise here.<|endoftext|>Moreover, the authors provide no convincing evidence (rigorous, intuitive, or otherwise) that their learned neural network feedback law should satisfy their Assumption 2. However, the adaptive controller design contribution is not adequately contextualized against the adaptive control literature. Overall, the neural network learning content of the paper is not substantive and not convincingly conducive to the adaptive control theory presented later on. *** STRENGTHS ***(1) The paper is generally well written, and is timely given a recent surge in works at the intersection of parametric/non parametric learning and adaptive control theory.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; The paper attempts to unify all CL research with a single formalism. Next, they present a software implementation of their framework. Finally, experiments are ran which demonstrate that Sequoia can be used to evaluate CL methods. Cons:C1) CL hierarchy   I think there are missing nodes in the hierarchy. It is not clear to me why someone else would not just use the other frameworks. This way, it would be easier to distinguish your contributions are. C3) Experiments: The text does not give us information on which methods are implemented by the authors. It gave me the impression that the evaluated methods have all been implemented by another software package. The provided hierarchy of CL methods might contain an interesting insight, which relates RL and supervised learning approaches to CL. However, due to the lack of comparison to related work, I am not certain of this. Overall, I don’t see a significant technological or conceptual contribution.<|endoftext|>It is unclear what advantages sequoia is bringing compared to using its dependencies directly. The paper proposes a theoretical framework to organize research problems in the continual learning (CL) domain according to a hierarchy. FRAMEWORK:According to the paper "each setting is described as a set of assumptions. Defining settings as a set of assumptions does not result in a tree shaped hierarchy (it would be a lattice). This a strong limitation of the framework that is not discussed. This is not a big problem by itself, but it is not clear what Sequoia is adding compared to the original libraries. Most methods are not implemented by Sequoia and are inherited from avalanche for SL and stable baselines and continual world for RL. At this point, what is the advantage that Sequoia brings compared to its dependencies (continuum, gym, avalanche, stable baselines, continual world)? it would be interesting to see an example. Sequoia (the software) seems to be still at a very alpha stage in its development cycle. I see very little unification in the methods, which is the main scope of the paper.<|endoftext|>In this paper, the authors try to establish a unified framework for different continual learning settings. They also provide a Python library, which includes different related methods. ### Strengths &nbsp;  The continual learning research tree (Figure 2) is well organized and makes a lot of sense to me. &nbsp;  I think the motivation of building a unified framework for continual learning is meaningful. Many different continual learning papers evaluate their methods in different benchmark protocols. It is difficult for the following researchers to compare these related methods. I think it is better to use class incremental learning (or domain incremental learning) directly. The reasons are as follows. It is not reasonable. &nbsp;  The authors include too many details about the code implementation in the paper. &nbsp;###   Post rebuttal Comments  I thought the authors aimed to establish a unified software framework that makes running continual learning experiments easy. However, after reading the rebuttal, I think Sequoia has the following major issues and the authors failed to address them in the rebuttal:  ***Sequoia heavily relies on the previous libraries, such as Avalanche and Continuum. I think your framework should be designed for a researcher instead of a software engineer. ***Sequoia hasn t been evaluated on large scale datasets (e.g., ImageNet 1k).<|endoftext|>This paper introduces a new continual learning framework that aims to boost the research in the field. This framework is based on a taxonomy of all possible assumptions that are common to CL methods. Moreover, this taxonomy helps in putting supervised and reinforcement methods in a unified framework. The work is mainly an engineering effort that s appreciated but might not fit this conference.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper focuses on time series (TS) segmentation. As noted above, there are many missing pieces from different parts of the literature which could be used to improve this work and better situate it with other progress in this area. They also claim that these approaches ignore long term dependences. Re: the stepwise segmentation module. However, this seems antithetical to the premise of the paper, where the goal is to have very precise timestamps. While the authors clearly put a lot of time an energy into this work, I think it would benefit from workshopping with others in the field.<|endoftext|>The main problem is that there is no explanation of the role/the idea behind each module. Pros: * The paper addresses an important problem of time series, i.e.accurate segmentation of a time series * The philosophy of the presented approach (stepwise classification) seems promising* Using both LSTM and CNN to treat the multi scale challenge of time series is interesting, especially how it is done in the paper with two separate modules. Neutral:* While the architecture is novel (to the best of my knowledge), most of the used modules are adapted from existing literature.<|endoftext|>The paper presents a stepwise segmentation for time series data, namely SegTime. Moreover, technical novelty is a bit limited. However, this paper needs further experiments and evidence to properly support the author’s claims. If all the issues mentioned are fully addressed, I may reconsider my assessment of the paper. This can increase the possibility of capturing long term dependencies. The authors argue that the model achieves computational efficiency by reducing parameters and computations.<|endoftext|>It also seems to be insensitive to the label changing frequency and this constitutes a major advantage over other approaches. There are several concerns regarding the manuscript:1) The idea of LSTM with skip connections is interesting and is borrowed from image segmentation models like DeepLabv3. However, it requires the proper adjustment of the skipping factors. 5) I strongly suggest to include the term ‘supervised’ in the title. Alternatively, the term ‘stepwise classification’ better illustrates the problem that is solved. The paper presents a rather complex model for stepwise time series classification.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper provides a framework which attempts to find a weighted subset of the unlabeled pool that matches the loss on all the points. Although the algorithm features favorable empirical results and a nice conceptual framework, I have some concerns about the details of the algorithm algorithm and evaluation. Are they optimized for each dataset? I m concerned about the scenario that heuristics were added to the algorithm until the algorithm worked well, rather than being theoretically motivated. Why are the dataset for the Bayesian setting (Fashion MNIST, CIFAR 10, CIFAR 100) different from the non Bayesian setting (MNIST, SVHN, CIFAR 10)? Is it possible that all methods could be run on the 5 datasets presented? Without the above concerns addressed, this paper could have major drawbacks.<|endoftext|>This work formulated batch active learning as sparse approximation problem and provided bayesian & non bayesian versions of their framework. Experimental results shows their effectiveness of reducing the acquisition time, especially when compare with other hybrid AL sampling strategies, such as BADGE. By transforming the sparse approximation problem to finite dim optimization problem, and solve w to implement subset selection of AL. The idea is interesting and novel. Weaknesses: my questions focused on the experimental part. 2) For the chosen of baseline, why not employ BatchBALD[1] instead of BALD? SABAL should compare with hybrid AL sampling strategies, BatchBALD would be more suitable for comparison. This paper provides a novel perspective of combining uncertainty and representativeness criteria together in deep active learning.<|endoftext|>This paper formulate the batched active learning problem as a sparse approximation problem. The authors also provide algorithms to solve the sparse approximation problem. The re formulation of the active learning problem into a sparse approximation problem is interesting (which I haven t seen elsewhere before). The paper is generally well written. I summarize my concerns as follows. 1.Is it true that the authors implicitly assume that they are studying the problem in a realizable case where there is no labeling noise in the Y domain? Based on the main review above, I vote for a weak rejection of the current version.<|endoftext|>The paper proposes SABAL as a framework to formulate batch active learning as a sparse approximation problem. The paper considers the SABAL framework as a finite dimensional optimization problem, efficiently solvable by the proposed greedy or proximal IHT algorithms. * The paper considers two choices for the norm for Bayesian and non Bayesian settings. * In short, there are multiple approximation techniques used in this paper while the empirical gain is minimal. Random approach performs extremely well for batch active learning task without relying on complex computation. * Missing a key comparison with Batch BALD. Note that the performance of BALD with $b>1$ is different from BatchBALD with $b>1$ as showed in [1]. To solve this approximation, the paper utilizes two approximation algorithms including the greedy and the iterative hard thresholding to optimize the objective function. * where $\alpha$ in Eq.(15) comes from? Reproducibility:* The results appear to be reproducible. Formulating the batch active learning as sparse approximation problem is interesting.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; However, in this case, because this paper has less novelties in  the theoretical part, the experiments are important. In summary, the method in this paper is very novel and interesting, but the experiments to support it are lacking. On the other hand, it is difficult to evaluate the novelty of the paper because many of the results rely on existing studies, such as Chernozhukov et al.(2021).This paper may be more of a summary of the existing studies. This is the reason why I vote for weak rejection. I think that $min$ operator should be removed. I think that we can derive the convergence rate from Theorem 1, which states the convergence rate of $\|\hat{\alpha}   \alpha_0\|^2_2$, but it is not clear, at least for me. I find it difficult to evaluate the contribution of this paper because it is not clear what the contribution of this paper is.<|endoftext|>The authors address the problem of estimating the average value of a moment function that depends on an unknown regression function, which is commonly used in causal inference. The experimental evaluation seems to be very strong and can achieve state of the art performance in estimating the average treatment effect. The contributions of this paper is unclear. For ForestRiesz, if readers are not familiar with Athey et al.(2019), it would be very hard for readers to follow the idea of ForestRiesz. Therefore, I would recommend acceptance of this paper if the paper was well written.<|endoftext|>The main innovation is the "automatic" nature of the procedures, one based on deep learning and one on random forests. I have some suggestions for improving the paper:1) Compared to other papers in this line of work, it appears to me that the scope of the paper is relevant to a broader audience and especially so for applied data scientists. Yet, the paper feels rushed (more on this below), e.g., with a lot of notation remaining undefined. 3) There are many ingredients to the proposed procedures (beyond what I described above) and from the experiments it is not clear which ingredient is relevant (or if all ingredients are relevant). It would be very helpful to at least define potential outcome notation early. However the paper could be substantially improved by improving exposition and making the paper accessible to a wider audience. I think it would also be helpful to make a remark of how Lemma 1 is used. 5) In Section 3, I think that having a figure on the architecture (similar to Fig.1 of Shi et al.)<|endoftext|>This paper considers the problem of estimating an expectation over covariates of some functional of an unknown regression function. The authors propose two estimators, one based on neural networks and one based on random forests. Explaining this might also make the paragraph below more understandable. previous estimators which were derived for specific functionals, these estimators are applicable to general functionals. Why $a \in \mathcal{A}$ and not $\alpha \in \mathcal{A}$? 10.Figure 1: It seems (at least by text search) that the abbreviation IPS is never explained. Therefore, I would currently classify the paper as borderline, but I am open to adjust my score based on the authors  response. Also, it is not clear that $m$ even depends on $Z$. In general, the steps here are not cited and the formulation does not make it clear whether they are novel. Also, making the best values bold might aid interpretation of the results.
Accept (Poster); rating score: 5; rating score: 3; rating score: 3; rating score: 10; The paper s main claim is that recurrence aids to enhance visual acuity in settings with limited resolution, such as the one imposed by limited photoreceptors in the retina. The authors therefore build a convolutional network with recurrent connectivity in its early layers (termed DRC) that receives a time series of low resolution frames and learns representations   for classification in CIFAR   from a teacher network receiving full resolution inputs. pros:* as far as I can tell, this is a novel setting and I have not seen much work investigating the impact of low retinal resolution on object recognition models* the results on CIFAR 10 and CIFAR 100 are clearly described and show that the recurrent DRC model aided by a full resolution teacher can regain most of the performance of a standard resolution model* the paper provides good background on the biological motivation for modeling low resolution photoreceptorscons:* lack of connection to biology: the proposed model is motivated from biological observations, but model predictions are never tested against any experimental results. Does it exhibit the same hyperacuity as observed in biology? * requirement of a teacher: the DRC is only tested when learning representations from a teacher which both has a non obvious connection to biology and is an unfair comparison to the non recurrent baselines which do not use a teacher. Would any of the baselines perform better when trained with a full resolution teacher in the same way as the DRC? Some connection is made in the very last paragraph to always on cameras such as body worn cameras but it is not made clear if those are really in the regime of low resolution and high temporal sampling. The use of a full resolution teacher network is also not well motivated especially in connection to biology, and the second half of the paper is a bit hard to follow (i.e.what to take from the feature visualizations). REBUTTAL UPDATE: I have increased my score following the authors  attempts at connecting to biology more directly, but I still believe key comparisons are missing: either a stronger link to biology and concretely relating model predictions to experimental results, and/or explicit comparisons to alternative models in ML tasks.<|endoftext|>Here, the authors attempt to leverage spatio temporal computations for object recognition on the standard CIFAR 10 and CIFAR 100 datasets. Figure 3 I do not understand why the authors are plotting an average of *2* datapoints. In short, they use a network with a front end of recurrent units (ConvGRU) to recognize objects given spatially jittered downsampled images – effectively approximating an active sensor. The authors demonstrate that their network is almost as performant as ResNet50 with 4x downsampled images, especially when the down sampled images are jittered in a spiral formation. If the DRC network does perform better than all other control networks, then additional analysis is required to understand how the network is able to improve its performance. They also present analysis demonstrating that the network is in fact performing spatio temporal calculations. The use of an "active sensor" (i.e.jittering the input image) is an interesting idea that capitalizes on recent developments in neuroscience and psychology. A central claim made by the authors is that spatio temporal computations *in the front end of the network* are important. This needs to be evaluated much more systematically, since it is not an apples to apples comparison. So really, the comparison is spatiotemporal computation for the DRC network and temporal only computation for ResNet+RNN. It is also unclear how the ResNet+RNN network was trained. As k becomes more negative and the trajectories more curved, trajectories are likelier to remain closer to the center and have more overlap, yet I could not find any analysis of this. If indeed curvature matters, then the authors must show that curved trajectories are better performing than other trajectories with less curvature but similar aggregate statistics (e.g.the trajectories are a similar distance from the center point and have similar degrees of overlap). A very simple control here is to shuffle the trajectories over time, in this case the statistics should be the same, but the degree of curvature from point to point will be destroyed. This is essential to understanding what allows the DRC network to perform well.<|endoftext|>The authors train a neural network to do object recognition on downsampled, moving images of objects. They show that by using a recurrent neural network in the early layers, it can learn to produce representations that result in recognition performance nearly as good as with static, full resolution images. However none of the baseline models considered the most important control:  what if you just feed in a series of static images without motion to the DRC FE? In fact, if the motion did help, it would beg even more questions. The paper seems motivated by neuroscience and psychophysics, but there is very little attempt to tie anything about the neural architecture of the model to substrates in the brain. This seems like run of the mill deep convnet engineering as opposed to neuroscience. There is no overall theory presented as to how the brain could benefit from motion of the sensor in building a higher acuity representation enabling tasks such as hyperacuity. There is much verbal reasoning in the introduction, however there is now much engineering and mathematical know how about how such problems can be solved   e.g., super resolution. These works are mentioned at the end in the discussion, but then almost immediately dismissed because they reconstruct the image rather than doing recognition. Instead, all of the requisite established theory is tossed aside and the authors resort to training a neural network to solve the problem, yielding a non transparent solution providing little insight into how the brain might actually solve this problem. Burak s (2010) important earlier work is cited but misattributed as providing an account for how how drift motion could improve acuity, which is wrong. Also missing in the intro is any mention of Ratnam et al.(2017) and Anderson et al.(2020).Those works are brought up in discussion at the end, but given the high degree of relevance of these prior works to the authors  thesis it is baffling why they are not brought up earlier, especially with regard to what the authors hope to do here that goes beyond or improves upon this prior work. An interesting idea but implementation is problematic.<|endoftext|>They find that more curvature in the drift dynamics, the better the accuracy. In fact, an enforced "spiral" dynamics gives the best results. They demonstrate that this "Dynamical Recurrent Classifier" (DRC) is capable of restoring performance on 8X8 images to nearly the performance on "high" resolution 32X32 CIFAR images (actually, no one would call 32X32 high resolution!). Finally, they show that using curved trajectories improves performance over more random walks, which can potentially explain recent results in humans. Same as what? This paper is well written, proposes a highly innovative model that is consistent with behavioral and neural data, and obtains excellent results. This result suggests that it can be used in engineering applications where the stimuli are low resolution. It has some confusing parts, but these can be fixed by the authors. Finally, they demonstrate that a recently discovered phenomenon, curved paths in the fixational drift, promotes higher classification accuracy. The front end of the model is a two layer, recurrent convolutional network. You have an unused half page in the main text, so that should be enough room to elucidate how this is done. Minor comments, wording, etc. The network was trained to reproduce the activations of the teacher network after 5 or 10 inputs. Positional information also improves performance. They then go on to analyze the features. They use the idea of the generative network (Nguyen, et al., 2016), modified for their setting. These resemble the corresponding ResNet features they were trained on, but obviously have dynamics. Wording suggestion for Discussion: This setting is novel and has been hardly addressed in the... >This setting is novel and has been mostly neglected in the...middle of page 8: stack up  > architecturelast word in third paragraph from the bottom of page 8 is not the one you want!
Reject; rating score: 5; rating score: 5; rating score: 5; Specifically, the author leverage the theories from constrained policy optimization and multi agent trust region learning to propose two algorithms: MACPO and MAPPO Lagrangian. Strength:This paper proposes two interesting algorithms to solve the safe RL problem in the multi agent setting based on CPO, PPO and primal dual approches. The author provides theoretical guarantee for their proposed algorithm (Thm 1). More discussions are need to highlight the novelty and challenging compare with CPO (Achiam 2017). (2) It is also not clear how MACPO updates the policy at the beginning of the algorithm, when the initialization point is infeasible. From the theoretical results, it seems that the constraint satisfications can only be guaranteed after the algorithm enter into the feasible region. 2.The update rule of MACPO and MAPPO LAGRANGIAN seems to be not efficient when the algorithm is not feasible. Because in the infeasible region we hope the algorithm can enter into the feasible region as soon as possible. I can consider raise my score if the author can address my question in the "main review".<|endoftext|>This paper considers the multi agent reinforcement learning (MARL) problem with safety constraints. 2.Their method was shown to be valid and promising both theoretically and empirically. The main idea of this work is to combine and extend [1] and [2], while there is no sufficient discussion on the methodological and theoretical contribution of this work beyond these two cited works. Based on MAPPO Lagrangian where the authors use a hard constraint on satisfying the safe condition, it seems that the cost itself is more important than reward. However, in the real application, there may exist a trade off between cost and reward. I felt some human preference or domain knowledge as required for the constraint optimization literature.<|endoftext|>This paper proposes two algorithms for multi agent reinforcement learning with constraints using policy optimization based approaches. The evaluations of the proposed approaches are provided. The problem is important. Missing literature: For single agent, authors are suggested to see CRPO Xu et al.(2021), PDSC Chen et al.(2021), Triple Q Wei et al.(2021), CSPDA Bai et al.(2021).2.For multi agent, see ECML paper https://2021.ecmlpkdd.org/wp content/uploads/2021/07/sub_181.pdf which also gives a model free approach. The combination of approaches does not seem to have enough novelty. 4.Although the constraints mentioned in Section 3 has the form of  "average constraints". I think what actually used in the experiments can be seen as a "peak constraint". It would be good to compare with the ECML paper approach mentioned above for scalability and performance.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces a set of architectural components and design principles for constructing neural networks for embodied control. The proposed method is biologically inspired, and can be used to build neural networks specifically designed for the considered agent, e.g., the swimmer agent studied in the paper. I appreciate how the authors accurately explain the meaning of each component of their method. I also like the choice of the Swimmer agent as a case study, because it is clear how the method can be used for this use case, and the explanation provided by the authors is clear and informative. Despite the good quality of writing and the clarity of the explanation, I have serious doubts about the contribution provided by this paper. As also stated by the authors, the proposed method has the limit of being strongly dependent on the knowledge of neuroscience and biology; however, most RL and AI researchers are not experts in these fields. I consider this a major drawback of this method, because it strongly limits the impact of this work. Another drawback of the paper is that the claims about sample efficiency and sparsity are not supported by any theoretical guarantee. The experiments are fair results, far from impressive. I understand the importance of the evidence of sample efficiency due to the big jump start, but I am concerned about the weaker performance compared to some baselines. I think this paper has significant drawbacks that outweigh its strengths. The proposed method is very difficult to use, and although the authors do a good job explaining the single use case, the paper would greatly benefit from the study of more use cases.<|endoftext|>This paper proposes a set of bio inspired Neural Circuit Architectural Priors (NCAP) as ingredients for building trainable networks for continuous control. Pros1.The paper is well written, especially the abstract, introduction, and related work sections. 2021.Overall, the reviewer recommends not accepting the paper. 2.The paper proposes a bio inspired network trained by DRL methods while using much fewer parameters than an MLP. Although the work is correctly motivated, the authors failed to prove that the proposed method is general enough to be applied to a broad spectrum of robot tasks. 3.The ablation studies performed by the authors demonstrated the effectiveness of the learning on the proposed network for the swimmer agent. More detailed issues with the paper are listed in the cons in main review. Cons1.The set of NCAP and the network proposed by the paper share many similarities with existing CPG works. It is better to present the proposed approach as a fine tuning method for CPG networks than as a general solution for robotics. Though the fully differentiable network proposed in this work is interesting, its novelty and contribution are limited compared to previous works in the field. 2.Since the proposed approach is similar to CPG based works, they share the same problems as well. First, these approaches are more suitable for producing stable rhythmic patterns in controlled environments, limiting their use in general real world robot tasks. 2) How can the trained model adapt to more realistic environments compared with MLP based approaches? 4.The NCAP approach introduces strong heuristics to the network design, limiting the network s maximum performance. The authors may need to think about relaxing the heuristics used in the network to improve the performance and generalizability. For example, the second ablation done by the authors in the ablation studies section might be a good direction by relaxing the excitation/inhibition restriction. 5.Other than fewer parameters, the paper failed to explore other advantages of the NCAP based network compared with the MLP. The MLP cannot achieve this since the fully connected network needs to be trained from scratch whenever input/output dimension changes.<|endoftext|>This paper proposes both a set of artificial neural components and interesting principles for producing biologically inspired neural networks for embodied control. This work aims to be at the intersection between neuroscience and machine learning for improving the design of artificial neural networks as well as improving our understanding of observed biological networks. Various components of biological networks are replicated in their framework, e.g., the balance between excitation and inhibition, intrinsic oscillators, or sparsity. ### Clarity: This paper is well written and the authors clearly explain the motivations, and framework in which they plan to operate. We would suggest the authors reorganize the figures for more clarity, which would help with the explanation of the results as well. It uses an interesting example that is C.elegans for which a lot is known about it s neural architecture and principles of locomotion. This paper is interesting but does not proposes a significant improvement to the literature as the gap between the promises made in the motivation and actually delivered work is too wide. From a neuroscience point of view, this work does not provide substantial evidence of the importance of the model at either modeling or simulating biological neural systems. From a "theoretical" point of view, the model does not provide much advancement to the machine learning community either.<|endoftext|>The paper addresses a really important challenge: To understand innate contributions to neural circuits for motor control. Its goal is to present a set of reusable architectural components and design principles for embodied control. They show that a resulting model can learn to swim more efficiently and requires fewer parameters while achieving similar accuracy as an MLP. Unfortunately they only consider this one swimmer task, and the architecture was apparently designed with this task in mind. One interesting aspect of biological circuits is that synaptic weights cannot flip sign during training. I do like the direction of research, as it tries to take inspiration from biology to improve ML However, the way I see it, what they really have is a nice model for a really specific task but not a lot beyond that. The paper did not convince me that their design principles would be beneficial for other tasks, It is also not clear what exactly the principles are that make this single task work well.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors conduct experiments on source code as well as natural language articles. They analyze the results to point out reasons for improvement, and also highlight differences between the domains. + The paper is well written and easy to read+ What the paper is suggesting is simple, yet useful. The example depicted in Figure 1 describes their motivation as well as what they are doing effectively+The concept of structural locality as used in their paper is defined clearly+Results are compared with other state of the art models (Table 2)  The only weakness that I can identify is that the authors have used one dataset of each domain.<|endoftext|>Authors demonstrate that the structural locality information improves results for two domains they experiment with. Strengths:  Authors propose a way to include structural locality into kNN LM models  Authors demonstrate improvement on retrieval tasks for Wikipedia and Java source code domainsWeaknesses:  It seems that the structural locality and its value are known and not new. The paper s contribution then is adding the structural locality to the non parametric language models. There is no comparison or discussion of other ways to add structural locality to LMs. It seems authors tried just a single approach that they present in the paper. Using locality info in authors  experiments leads to minor improvement. This is not a significant weakness since there might be other tasks where locality will contribute more. For tested tasks, structural locality information improves results but not significantly.<|endoftext|>The paper is about modelling structural locality in non parametric language models. The key hypothesis is in modelling not only the co occurrence characteristics but also structural characteristics such as locality. The model paradigm is based on non parametric language models. The authors then conduct experiments to demonstrate that the method improves upon existing works. It is unclear how does the method generalise across different tasks and datasets, i.e., beyond two datasets. The key advantages are clear from the paper, this seems to be the weakness that is hard to defend. Overall, the paper indeed has some merits.<|endoftext|>It makes a claim that a) structural locality is not implicitly fully captured by the distance metric used in non parametric language models and further that b) explicitly plugging in structural locality into non parametric language models can improve their effectiveness. It validates this claim first by doing analysis of two datasets with the help of custom locality functions and then by plugging in the locality functions into a non parametric language model with the help of learnable parameters. Incorporation of locality features in non parametric language models using simple learnable functions of the distance metric	4. So it would be interesting to set w to 1 for these features and learn only b and for l_0 learn w. In Eqn 4. y should be w_t? The paper makes an interesting hypothesis and goes about validating the hypothesis. However, the improvements due to the proposed method are marginal.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 5; This paper modifies from a differential tree architecture (NODE) for tabular data that has lower theoretical computational complexity to be suited for lower end CPU devices. I have several major concerns. 3.The claimed contribution is also unclear when no experiment of speed up is compared to traditional tree based packages. They claim that S HTE enjoys the deep learning representation and thus only require hundreds of trees while traditional trees require thousands of trees, but this claim needs to be backed up in the experiment. This makes readers suspicious of the accuracy of the method. The accuracy results are somewhat lower than CatBoost in 4 out of 6 datasets. So I recommend rejection for this paper.<|endoftext|>The training and inference algorithms are provided, and the computational complexity has been analyzed. The selling point is that the approach is computationally efficient on CPUs and can learn sparse representation. The prediction performance is on par with state of the art methods. First of all, I am not an expert of this specific problem (tabular data prediction and differentiable decision tree learning), although I published papers on decision trees before. Weakness:  The performance is not so impressive (esp.in comparison to CatBoost which is also quite fast). However, its performance is not so strong and some experiment results are not solidly justified.<|endoftext|>The paper constructs Sparse Hierarchical Table Ensemble (S HTE) based on oblivious decision trees for tabular data processing. The complexity analysis is helpful for understanding the computation efficiency of the proposed method.<|endoftext|>Finally, annealing mechanism is used to obtain sparse connections. Therefore, I think that the methodological contribution is not significant. Probably, the proposed method can improve significantly for non tabular data (e.g.images and texts) where correlation of features matter where representation learning matters (and traditional boosting machines do not perform well). "the computational component in XGBoost are Random Decision Trees (RDTs)". As far as I know, XGBoost use traditional CART style decision trees which are not random. Based on major concerns regarding technical novelty and experiments, I am leaning towards rejection of this paper.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper studies sample complexity bound for finding approximate Nash equilibrium in two player Markov cooperative games using decentralized algorithms. There are the following problems of this paper. Such a correlated policy is a CCE, but not Nash equilibrium. However, it is O(1/eps^4) for Matrix team and O(1/eps^8) for Markov team. The main theorem is wrong.<|endoftext|>This is an interesting subclass of problem, I found the paper was well written and explained the approach clearly. The agents then don t rely on any further exchanges of information to achieve a local optimum that achieves a global approximate Nash equilibrium policy.<|endoftext|>Authors tackles the cooperative multi agent reinforcement learning problem with stochastic dynamics. Room for Improvement  The paper is difficult to follow. After all that is the main catch of your bound. I think the theoretical contributions of the paper are great and would benefit the community. The writing is currently very dense.<|endoftext|>The authors establish a variety of novel theoretical results for Markov team problems. The paper is extraordinarily well written; I did not find any issues with spelling, grammar nor presentation. The authors provide a variety of interesting theoretical results. To further improve the paper, I believe the authors should compare their empirical results against other methods that aim to tackle nonstationarity in multi agent learning. The paper s writing and presentation are very clear and polished, and the results are both sound and significant. I expect this paper to stimulate the cooperative MARL community.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper presents a theoretical perspective on deployment efficiency in linear MDPs. The paper formalizes the notion of deployment complexity and presents an information theoretic lower bound for worst case deployment complexity of any algorithm, identifying horizon as the main bottleneck for deployment efficiency (in addition to feature dimension when restricting to deterministic policies). The derivations and algorithms are novel, and provide insights that may be useful to future theoretical or empirical works. I did not look over the proofs carefully, but I found no issue with what I skimmed through. The work is wholly theoretical in nature. Overall, this paper provides a significant contribution to the field. I found the theoretical results insightful and, although the present submission contains no empirical results, I believe the derivations may inspire future more practical algorithms.<|endoftext|>The deployment complexity is near optimal. It shows when MDP is linear, the lower bound of deployment complexity for deterministic policy and arbitrary policy is $\tilde{\Omega}(dH)$ and $\Omega(H)$ respectively, and Layer by Layer Batch Exploration Strategy for linear MDPs and Deployment Efficient RL with Covariance Matrix Estimation can achieve this deployment complexity. Those results are new and answer the questions that are left by [Gao et al.] say the total sample complexity $N\cdot K$ to be small? This paper provides a solid study in deployment efficient RL from the theoretical perspective. Therefore, I choose acceptance.<|endoftext|>The paper focuses on the theoretical properties of Deployment Efficient Reinforcement Learning, DE RL, focusing on linear MDPs. Deployment complexity is defined as the number of times that a different policy is selected to collect data, and an algorithm is said to be deployment efficient if the number of deployments is as small as possible and the number of trajectories per iteration is polynomial. In section 3, the authors provide a lower bound for the task, under both deterministic and stochastic policies. Assumptions for proving the upperbounds are reasonable and does not appear to be stronger than existing literature. The paper introduces a new framework, DE RL, and provides lower bounds under different settings. The setting is interesting and motivated by real world concerns.The results are comprehensive and reasonable, with clearly explained proof in the appendix.<|endoftext|>In the context of a linear MDP, the authors prove an information theoretic lower bound. + The authors prove a novel lower bound on the deployment complexity in the linear MDP setting and show that this is achievable. Weaknesses:+ No practical realizations of the theoretical results are demonstrated. The paper makes novel theoretical contributions to the challenge of deployment costs in practical RL algorithms.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The results appear to be interesting, the problem of characterizing learning curves for Gaussian processes under mis specification is important. ## Weaknesses  Parts of the main results feel like a bit of list of results. I think the exposition in these sections could be improved by providing more context for each result as well as how the results differ. For Mercer s theorem to apply you need some assumptions on the kernel/input distribution. The connection to neural nets seems a bit orthogonal to the results in the paper, and I am not convinced it should be emphasized so heavily. I have only minor concerns about the correctness, but I think these can be resolved (or it is possible I have misunderstood). Overall, I think the paper is above the threshold for acceptance. If I understand correctly, the suggested conclusion is that a sample from the GP prior (almost surely?) I don t see why this would be the case.<|endoftext|>For example, discussion about the rates  optimality and modeling choices such as knowing $\sigma$ would significantly improve this paper. They assumed that the kernel eigenvalues and the target basis coefficients decay according to the power law, i.e., polynomial decay. I found it helpful that the authors gave proof sketches and summary of the main steps detailed in the rather long appendix. Here are some questions /comments:1. The error variance $\sigma^2$ is assumed to be known which is rarely the case in practice. Is it possible to get better rates by using different kernel functions or eigenfunctions? In particular, where did $2.0795$ and $0.7875$ come from in $2.0795n^{ 0.7875}$ for the excess mean square error of $f_1$? The line after (2), $K_{x\mathbf{X}}$ should be $(k(x,x_1),\dotsc,k(x,x_n))$ a row vector. See also the proofs. I feel that some parts of the paper need more explanation and exposition.<|endoftext|>*Spectrum dependent learning curves in kernel regression and wide neural networks*. In particular the abstract and introduction. Finally, the authors provide one numerical experiment with the arc cosine kernel and data uniformly distributed in the unit circle. **Update after the discussion period**During the discussion period the authors have addressed all my questions and have welcomed my suggestions. The existence of a region with exponent $n^{\frac{1}{\alpha} 1}$ for other choices of regularisation and the cross over between these two regimes is a particular case of the discussion in [4] (cross over between blue and orange regions in Fig.1, with purple point corresponding to [1]). For these reasons, I am updating my score towards an accept. Top of page 5: *As mentioned in the introduction, this assumption is adopted in many recent works (Bahri et al., 2021;Canatar et al., 2021; Nitanda and Suzuki, 2021). This is a classic assumption on the KRR literature, and to my knowledge were introduced by Caponnetto and De Vito in the mid 2000s, see [1]. Can this be related to hypercontractivity property in the concentration results from [5]? 4.Figure 1 is very hard to read, as one needs to zoom +300% to read the axis and caption. 5.Remark 14: As mentioned before, even though [4] derives the rates for Gaussian designs, they provide numerical evidence that this result works more broadly.<|endoftext|>The paper summarizes the generalization error of kernel ridge regression and Gaussian processes under a spectral decay assumption. Given that the paper is largely theoretical, I believe there should be more emphasis on the technical novelty and the difficulty of extracting an asymptotic expression for the loglikelihood formula given the strong assumptions made on the kernel spectrum. For example, what is the novel theoretical contribution for showing that ? If it s an nontrivial application of matrix concentration, can you point out how it is nontrivial. I also saw that you applied a spectral truncation argument. Is this argument necessary and/or novel? Perhaps emphasizing such points in the main sections of the paper would significantly strengthen the paper since it seems like the reader would need to infer the non triviality from the appendix. **Update: The authors have significantly strengthened the theoretical understanding of the problem and the intuition/novelty of their techniques.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The results prove that the method is outperforming baselines with comparisons on both quantitative and qualitative results. This ensures that the model can be trained using low resolution patches (e.g.101 x 101) and, at inference time, it can be used to generate high resolution renderings: images up to 4096 x 4096 are showed. There are a couple, mostly minor weak points, that I hope could help the authors to make the paper even stronger:  Evaluation on temporal consistency: it would be great to assess how the method performs on temporal sequences and assess how stable/consistent it is. This would also help to establish a baseline for future work in this area.<|endoftext|>Therefore I recommend accepting this submission. I think this is a very solid paper. The authors proposed a modification to StyleGAN to eliminate zero padding. A lot of details are provided in the supplementary material. 2 I found the visual result on the Flickr Landscape data is better than that on the LSUN bridge or tower datasets.<|endoftext|>1) It is better to add the symbols in Sec 3.2 & 3.3 to Figure 2, e.g., $G_s$, $Z_s$, $G_T$, $p_c$. I recommend accepting this paper.<|endoftext|>5) What will happen when a non landscape dataset is used? ** Update after rebuttal period** The authors have adressed many of my concerns during the rebuttal period, including new extensive comparisons with other methods, ablation studies and discussion of limitations. I am overall more positive about this submission and thereby recommend it for acceptance. 3) The user study needs further explanation. The proposed applications could be impactful in the image generation literature. 4) Why is the inception score (IS) used on Table 2 but not on Table 1? First, several of the claims of the paper should be toned down. In terms of results, on the Supplementary Material, it is clear that InfinityGAN struggles to add variation to the uppermost parts of the images (eg Figures 20 and 21). The responses to concerns by other reviewers were also convincing.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper is about improving conditional GANs. To be specifically, it also aims to resolve the bias issue of ACGAN by proposing a discriminative classifier. * I have the concern of the analysis. Preliminary analysis of the proposed method are provided. The discriminative classifier is a hybrid model of discriminator and classifier, where it has to not tell real or fake, but also the class. at the very beginning. The authors should provide a better overview for the progress of this direction. Any insights why the proposed ADC GAN, which optimizes an asymmeric loss, should be better? Suggestion:* There are some very recent works in NeurIPS and also highly relayed,     Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training, NeurIPS 2021    A Unified View of cGANs with and without Classifiers, NeurIPS 2021  Although they are posted online after ICLR deadline, I would strongly encourage the authors comment on the similarities and differences between the proposed work and these two, because they will be on public for a when the paper decision of ICLR is out anyway. A common practical implementation of AC GAN is also using generated data to train classifier, which I think it s a straightforward idea. It s fine even though the ideas are overlapping. Under this, whether it can simply resolve the generator agnostic issue? Second issue is the empirical results, which seems not consistent with other works. * Some descriptions are not accurate.<|endoftext|>This paper proposes the Auxiliary Discriminative Classifier GAN (ADC GAN) to eliminate a contractionary objective and conditional entropy in ACGAN generator training. Specifically, the authors mathematically demonstrate that training ACGAN without a discriminative label classifier causes minimizing an undesirable divergence (KL(q(x)||p(x))) which conflicts with the joint distribution matching (KL(q(x,y)||p(x,y))). (+) The proposed auxiliary discriminative classifier is reasonable, and easy to implement. I think it is essential to clarify differences between two arguments. It would be better to conduct experiments several times since GANs have been known to have a large performance variance. I accept that ADC GAN can learn the joint distribution which consists of the one dimensional conditional gaussians better than PD GAN, AC GAN, and TAC GAN. Can ADC GAN learn the joint distribution better than other cGANs? They apply adversarial training not only for the discriminator but also for the auxiliary classifier to eliminate a contradictory divergence and conditional entropy in ACGAN training. However, I think Theorem 1 has already been addressed in TAC GAN paper and experimental results do not fully demonstrate the effectiveness of the proposed method.<|endoftext|>This paper proposes a new conditional GAN model that employs a discriminative classifier that predicts in the joint space of label and real/fake domain. The theoretical analysis shows the proposed ADC GAN can minimize the reverse KL between joint $Q_{X,Y}$ and $P_{X,Y}$. I am willing to raise my score if my concerns are resolved. 2.The experimental results on synthetic and real datasets demonstrate the superiority of ADC GAN on conditional generative modeling tasks. I don t fully agree with some claims made by the authors:   1. Page 5, footnote 2, it might be true that term (a) is "ignored" or set to zero, but it is not sufficient to say this inductive bias is a mistake. 2.Equation 8, this is not the original form of TAC GAN. 4.It would be helpful if the author could provide results of ADC GAN on ImageNet at 256 resolution. 6.It is nice to see ADC GAN worked even without GAN loss, I am curious how the model performs (w/o GAN loss) on challenging datasets such as ImageNet? Vol.34.No.04.2020.I think the paper presents an interesting analysis of TAC GAN, AC GAN, and other cGAN methods. The paper also lacks results on high resolution generation.<|endoftext|>As far as I know, this is an important issue that limits classifier based cGANs (the counterpart is the projection based cGAN, i.e, PD GAN). The authors point out that the reason is that the classifier of AC GAN is generator agnostic and minimization of conditional entropy decreases the intra class diversity. The authors propose ADC GAN (auxiliary discriminative classifier) to solve this problem, and theoretical analysis is also presented. Reasons: The projection based cGAN, PD GAN, does not suffer from the low intra class diversity problem, but it converges more slowly than classifier based cGANs (see Omni GAN, arXiv:2011.13074). The proposed ADC GAN is very simple to implement without additional computational overhead. Weaknesses:  Please detail in the paper how the FID in Table 2 is calculated (for example, how many generated images are used, whether the training set or the validation set is used, and whether the inception model is from PyTorch or tensorflow). In addition, I also recommend including IS in Table 2. The author can refer to the notation of equation 8, which is clear. It would be better if the author could discuss the relationship between ImAC GAN and ADC GAN. The author proposes a simple but seemingly promising classifier based cGAN. I will improve the score based on the author s answer.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The authors propose a biologically constrained latent linear dynamical model of the C. elegans nervous system. They fit the model to calcium imaging data from whole C. elegans using a variational approach. Why is this and does it impact the conclusions? The paper highlights that known biological constraints can be included tractably within latent variable models of biological neural networks (in this case, the C. elegans).<|endoftext|>The paper develops a latent variable model with biologically meaningful parameters for the worm *C. elegans* whole brain neural traces. The question of whether anatomical constraints help with improving model predictions and constraining the space of generative models of neural activity is an important and significant question in computational neuroscience. * How does the regularization work? At least intuitively shouldn t we expect to have better performance when a single neuron is missing compared to all the neurons from a worm missing? If a neuron is missing in the training dataset, then how do we have a latent variable corresponding to it? This would be important for biology experts and can provide further insights into the proposed model. * How should we ensure that the model is learning time dynamics?<|endoftext|>The current manuscript presented a connectome constrained latent variable model for whole brain calcium imaging of C.elegans nervous system. One limitation of the current work is the model generalization. Specifically, the activity of each neuron of the whole brain imaging was modeled by latent variable analogous to the voltage of one unit in the model network. I like that the authors tested on different variants of the model, which are all relevant, and provided insights on model selection.<|endoftext|>This paper uses VAE (variational autoencoder) to model neural activities of C.elegans observed in calcium imaging. Weakness — not clearly written at all:  About the prior p(v|h) generation: it only mentions “Pθ(v|h) is a biophysically realistic connectome constrained network with passive point neuron dynamics” without pointing out what the network is: are the only trainable parameters in the network W^c_ji and W^e_ji, and a MLP  layer for encoding h_i(t), or are there any other layers? If so, how can the decoder be trained since it needs the prior to calculate KL divergence for the loss? Apart from clarity, the technical novelty in computer science is a bit insignificant. and re submitting to a neuroscience journal.
Reject; rating score: 1; rating score: 5; rating score: 6; This paper study the local intrinsic dimension of graphs in terms of feature, structure, and representation. Apparently, CiteSeer is easier to learn, but the authors give the opposite conclusion. This is an interesting topic and can help us understand many graph learning techniques. The SLID takes the distribution function as $G(y) a^y$.<|endoftext|>At least some preliminaries results on that direction would hugely increase the innovation of the paper. In addition, estimators for Feature LID (FLID), Structure LID (SLID), and Representation LID (RLID) were introduced.<|endoftext|>This work investigates the LID of graphs, and interpreted the GNN models from the perspective of FLID, SLID, and RLID, which is interesting. WeaknessThe major concern is how to understand and utilize these LID metrics in graph learning.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; The authors introduce an approach based on using graph neural networks to parametrize wavefunctions represented by an equivariant neural network. The experiments demonstrate the utility of such an approach in terms of accuracy and computational speed up.<|endoftext|>The paper develops a neural network based variational ansatz for modeling wave functions. While it is an interesting observation that GNN is capable of conditioning the model parameters on the geometry to adjust the modeled ground state wave function, other methods could use optimization results from one configuration as a starting point for others. In addition, authors use a GNN “hypernetwork” that predicts the parameters of the variational wave function for a given configuration of the system. (e.g.show the results of PESNet for Fig.3 and Fig.4 with single parameters w; w_m predicted by GNN intermediate regime).<|endoftext|>The resulting method can solve the Schrodingers equation for multiple geometries while training significantly faster. This method combines an existing neural wave function model called FermiNet together with a GNN (MetaGNN) to solve the Schrodingers equation for multiple geometries simultaneously. The MetaGNN takes the atomic graph as input and outputs a set of parameters that capture the 3D geometry of the system, which are then input to the FermiNet. This paper, in contrast, presents an ab initio method that does not require a training dataset.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper focuses on domain generalization (DG). It proposes a method based on adversarial training (AT): the main idea is learning universal adversarial perturbations at domain level (domain adversarial perturbations, DAT), and rely on those within a standard AT routine. Since this benchmark is embraced in this submission, I do not understand why Table 1 i) provides worse performing baselines and ii) does not include results related to all the benchmarks. * The numbers in Table 1 are not from (Gulrajani and Lopez Paz, ICLR 2021). These are the two main references that address AT for DG, and given the nature of the proposed method they should be included in the experimental analysis. I cannot understand how the two derivatives are compared in Eq.11 and 13. On a different and more subjective note, I do not believe that an analysis based on a single sample domain assumption if informative for the paper (Remark 1). **Clarity/writing:**  The description of correlation shift and diversity shift in my opinion is not clear: a figure and/or a concrete example would help. A better reference for AT   when introduced   would be (Goodfellow et al.ICLR 2015), which actually propose it. I am assuming that also standard data augmentation is performed, as suggested in (Gulrajani and Lopez Paz, ICLR 2021).<|endoftext|>Experiments on four benchmark datasets suggest that the proposed approach might be beneficial for DG when target domains suffer correlation/diversity shifts. Strengths:  The paper is well written and explores the challenging Domain Generalization settings. The paper highlights a relationship between IRM and AT  Empirical results show that the method might be better than ERM for Domain Generalization settings. DAT is a trivial adaptation of UAT to DG settings. Furthermore, UAT is not compared to DAT in the experiments. The advantage of the proposed DAT algorithm on diversity shift tasks is minimal compared to ERM. The aggregated performance improvements are mostly due to improvements in the CMNIST task. Related to the above comment: there is no discussion on failure cases. Although the inquired relationship between IRM and AT is interesting, the proposed approach is only marginally novel, as it is a simple adaptation of UAT to Domain Generalization settings. Furthermore, I am not convinced by experimental results: there is only a limited insight on favorable scenarios, as domain split results are not presented, and improvements are mostly due do very good performance on a single dataset consisting of synthetic digits. Still I am looking forward for the authors  response, and I hope that they can address my concerns (see Weaknesses).<|endoftext|>The paper investigates whether adversarial training (AT) could be used for extracting domain invariant features, and whether AT can benefit OOD generalization. Why is it more "realistic" to have spurious correlations in the background? The proposed method (DAT) also relies on this assumption. While this may be a useful assumption for settings in which multiple environments are indeed available (eg.PACS), it does not hold for benchmarks such as "DIGITS" (train on MNIST, test on 4 other datasets) where only a single environment is available (aka single source domain generalization). On the other hand, standard adversarial training makes no such assumption. For example:    1. It is not clear whether DAT (and IRM) would also work for single source domain generalization. It would be better to show the heatmaps from DAT in Figure 1 (similar to Figure 2). I would recommend making this distinction clear. The theory and empirical results in this paper do not provide any insights into this. While the motivation and theory behind the paper is sound, empirical results are not conclusive and the paper is lacking in terms of analysis and interpretation of results. From Table 1, it appears that sample wise AT is worse than ERM for all 4 benchmarks      On CMNIST:  AT < ERM < DAT < IRM      On NICO:  IRM < AT < ERM < DAT      On PACS:  AT < IRM < ERM < DAT      On TerraInc:  IRM < ERM~ AT < DAT    This means that while the while the comparative relation between IRM and DAT is somewhat clear, that between IRM and AT is mixed. Why is IRM better than AT in some case, and worse in others?<|endoftext|>The paper tackles the problem of out of distribution (OOD) generalization of deep learning models with a novel adversarial training formulation (DAT) that introduces a shared adversarial perturbation per training domain. The authors posit that DAT inherits the advantages of IRM (which is effective for correlation shift while it performs more poorly on diversity shift) and AT, which improves upon Empirical Risk Minimization (ERM) for diversity shift. Experiments on several OOD benchmark datasets show that DAT outperforms prior works and ERM on average. The connection between IRM and the proposed method is interesting, and the derivation/exposition of the connection is elegant  The paper is well written, and the method is well presented  The approach outperforms ERM and prior methods on several OOD benchmarksWeaknesses:  The authors claim that adversarial training can handle diversity shift well, but experiments in Table 1 do not support this (AT is worse than ERM in most benchmarks). It would be good to know if the method performs less well on the other tasks or the rationale to limit the comparison to the chosen datasets. Though having some visualization is nice, a quantitative evaluation would be important there. For example, Proposition 1 suggests that IRM is an instance reweighed version of DAT. It is unclear what the implications of this are on the performance of DAT compared to IRM. The paper tackles a vital problem (OOD generalization) with a novel adversarial training approach. The paper also illustrates an interesting connection to Invariant Risk Minimization.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The submission suggests addressing this problem setting by first imputing the missing training data and what they call a mask based update. The submission presents experiments for multi agent particle environments. ### Problem SettingI don t find the problem setting suggested by the submission to be well motivated. Could the submission give an application setting  that is currently addressed by decentralized MARL algorithms in which centralized MARL algorithms are inapplicable because of missing data? The submission states that it does not perform experiments on SMAC because MADDPG does not perform well in SMAC.<|endoftext|>This paper proposes an imputation assisted multi agent reinforcement learning (IA MARL) method under the problem setting of MARL, where the training data of each agent can be randomly missed with a certain probability. Specifically, IA MARL uses a generative adversarial imputation network to impute the missing data, and train the MARL method on the imputed data. 2.The problem of MARL under missing data is interesting. This paper has abundant experiments, from different percentages of missing data to different environmental settings. The authors should add this experiment, also with different missing rates, to convince the readers that imputation does work. The main concerns are located in the experimental results and the complexity of the imputation problem itself.<|endoftext|>This paper develops an imputation method for missing data in multi agent settings. 2) No large scale experiments: The authors argue that SMAC can t be used as a testbed since MAPPO does poorly there. It s also unclear why the method should only be restricted actor critic variants. It clearly illustrates that the imputation of the data does not actually recover the correct values. In principle, under centralized training, having wrong values for _the other_ agents can also be detrimental to learning, which would render the current masking ineffective. This is both due to the limited /noisy experimental evaluation and the missing discussion regarding the assumptions that went into the method and limitations.<|endoftext|>Authors present a novel and effective method leveraging a generative adversarial approach to a specific MARL problem with regard to missing training data. The approach presented treats the missing data as targets of imputation, loosely similar to that of inpainting problems in computer vision, where missing pixels are "filled in". I would appreciate if the authors could include some of these discussion points in the paper, where they see fit. Q1.What would you say is the strongest motivation for employing imputation techniques in the multi agent case? Q2.In the single agent case, if the imputation could be applied across the time axis (and not rely on the observations of other agents because there would be none), what benefits would you foresee?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper presents a Bayesian optimization method based on meta BO. However, the proposed approach is not interesting to the Bayesian optimization community and is trivial to some degree. The reviewer believes that the targeting problem presented in this work is a very important one and an effective method could be of great practical value. In the abstract, authors claim that "data from similar functions" could lead to a better prior for GP. Such a formulation is not only trivial to the GP community , but also to the empirical Bayes community. Additional (minor) issues:1. the graphical model for GP in Figure 1 is wrong2. there exist a lot of inconsistencies in this paper. 3. lots of claims and statements are superfluous. The proposed method is trivial.<|endoftext|>This surrogate model can be learned from past data. The problem of "warmstarting" HPO by making use of data from previous experiments is an obvious idea, and it has seen a large amount of past work, much of which the authors of this submission do not seem to be aware of, neither apparently was (Wang, 2018b) which seems more of a theoretical paper. Two of the most interesting ones are maybe [1], [2]. In fact, they even seem to invent on their own methods to compare against, such as "MIMO", in a way which has never been used for transfer HPO. Apart from that, I also do not get much out of the experimental setup. Unfortunately, a lot of relevant prior work is ignored here and not compared against. Instead, the proposed approach is compared against simple baselines, as well as methods that mostly seem to have been made up (such as "MIMO").<|endoftext|>This allows for an efficient Kronecker decomposition of the kernel and thus linear, rather than cubic scaling, across tasks. I have a few key concerns about this paper. HyperBO, in the experiments, uses the PI acquisition function. Is there a particular reason why this is? The error bars are also all over the place. In particular, the fixing of GP hypers seems to largely remove the need for scaling, which is the primary strength of HyperBO. Furthermore, though the experimental set up uses a large amount of data to achieve somewhat unconvincing results in my mind, and only one optimization problem is presented (though worth noting, is thoroughly analyzed). Thus, I can t recommend acceptance at the time.<|endoftext|>This paper suggests a meta Bayesian optimization strategy that optimizes free parameters of GP including a prior function and noise variance, where multiple sets of historical observations are given. In particular, the proposed method chooses a free parameters using one of three approaches: (i) optimizing a marginal likelihood, (ii) measuring KL divergence, (iii) considering both marginal likelihood and KL divergence. + It solves a very interesting problem, which transfers a history to the current task in Bayesian optimization setup. + Compared the work by Wang et al.(2018b), it solves more realistic setups. Following the above point, is there any specific reason why the authors use four dimensional search space? I do not think this algorithm is not scalable. Moreover, for example, batch size can be one of the meta parameters to be optimized.
Reject; rating score: 3; rating score: 3; rating score: 8; This work studied the research problem of graph data augmentation for supervised graph classification. The authors proposed G mixup that performs mixup on graph data via graphon generators. Pros:1.The graph level data augmentation methods have been mainly focusing on augmentation for unsupervised graph learning, and the graph data augmentation for supervised graph classification is rather underexplored. 2.I like idea of using mixup on graph generators instead of directly on graphs, which is simple yet reasonable. For example, what if we concate $W_{\mathcal{G}}$ with $\mathbf{h}_G$ for all $G \in \mathcal{G}$ and directly use this new graph representation for node classification? 4.I would recommend the authors to use the term "graph data augmentation" instead of "graph augmentation" to avoid potential confusion with a different problem in graph theory.<|endoftext|>The paper introduces a novel attempt of applying Mixup from vision and text to graph data. On the other hand, I have the following concerns regarding the paper. That is, each class has a graphon and this graphon can be accurately estimated. I wonder how this guarantees the proposed g Mixup would work. a.A critical baseline is missed in the experiment. c.	There is a parameter n, i.e., the number of nodes to be generate by the graphon.<|endoftext|>Specifically, G mixup first estimates the graphon for each class of graphs with the same label then interpolates the graphons of different classes of graphs. Therefore, the contribution of this work is clear since this work makes the mixup be applicable to the graph structured data by introducing the concept of the graphon. Especially, it is easy to understand the concept of this paper through the figures and examples. ## WeaknessesThe empirical results are not sufficient to show the effectiveness of G mixup. I wonder that why the authors did not include the experiments against Manifold Mixup [2] although the authors refer to it in the discussion. * Experimental setting is too limited. This is an impressive paper but there is a lack of experiments.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; At each step, the joint distribution $p(x_k, x_{<k}$ is optimized by not only autoregressive cross entropy loss, but also the EBM KL divergence.Moreover, the model uses importance sampling to bypass the MCMC as conventional EBM models do. Experiments on neural machine translation, language modeling and image samling all demonstrate that the proposed integration outperforms the base model. Strength:+ This paper is well written and well organized. Issues on conventional ARGMs are clearly pointed out, which lead to the motivation of this work. + Three experiments show the overall improvement of E ARM compared with based ARGM. Weakness:  Training ARGM with EBM in a cooperative manner has early been proposed byLearning Latent Space Energy Based Prior Model, NeurIPS 2020Cooperative Training of Descriptor and Generator Networks, TPAMI 2018Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Conditional Learning, TPAMI 2021These works, however, are essentially missed from related works and model comparisons. The experiments of this work are relatively insufficient. 1.All experiments only demonstrate improvements compared with base model, while comparisons with other baseline models are missing. This work proposes to alleviate ARGM existing problems by leveraging the EBM objective.<|endoftext|>This paper designs a new globally normalized probability function to approximate the sequence data distribution, which is defined as the product of traditional autoregressive model distribution and energy based model distribution. The authors also propose a new training objective that is different from maximizing log likelihood. To tackle the optimization issue due to the energy based loss term, the authors propose to use importance sampling to sample from the autoregressive model to approximate the negative phase loss, so that MCMC is not required. Experiments on machine translation, language modeling, and image generation demonstrate the effectiveness of the proposed method. The proposed method is interesting and novel. It is also well motivated as a new energy based model that does not require MCMC during training. 2.The experimental results are generally good. In addition to efficiency, performance comparison with energy based model baselines is also important, otherwise, I am not sure about the advantage of E ARM compared to other energy based methods. (3) do (1) and (2) share the same samples for approximation or need sampling twice? 2.The end of page 4 says “xx means improvements in this direction will be automatically taken care of as a result of xxx”. The proposed method is novel and interesting, but it only compares to traditional autoregressive models in experiments, and other energy based model baselines are missing.<|endoftext|>This paper considers the problem of exposure bias and lack of long range coherence in auto regressive models. The authors instead jointly minimize an energy based learning objective and the vanilla MLE. Extensive experiments on NLP and vision tasks show that the proposed learning objective reduces the discrepancy between training and inference. # Strong points  The considered problem is important, and the authors  novel framing of it is well motivated: using an EBM model that takes the global information into account to mitigate the gap between training and inference. They use the model distribution as the importance distribution, and It s faster to estimate the weights in the importance sampling than MCMC procedures in traditional EBM. Experiments are designed appropriately to showcase the resulting behavior of the model. The proposed method outperforms the base autoregressive models, and is compatible with other techniques and architectures. # Weak points  The design of the joint distribution seems a bit arbitrary. It seems to me that this choice is a more intuitive one. Lack of comparison/discussion of relevant works. The paper also does not compare to those ``two stages" methods in intro.<|endoftext|>This paper proposes to improve the training of AR generative models by introducing a "regularizer" based on a decently designed energy based model. The EBM is seamlessly integrated with the AR model and the sampling from the EBM can be transformed as an importance sampling from the AR model. Empirical results on text and image datasets verify the effectiveness of the proposed model. The results look promising and the idea of using EBM guidance in AR model training is interesting. Do you observe training instability during training? Would be better to provide more analysis on this point. * In the paper, it is claimed that the joint training improves the long range coherence. Overall speaking the paper is well written and well motivated. However, I m sort of concerned with the computational cost and training instability introduced by the extra EBM module, compared to the performance gain it brings.<|endoftext|>The authors propose an approach to integrate energy based models (EBM) into into general auto regressive generative models ARGM. A learning algorithm based on Hinton s wake sleep algorithm is derived that approximates the EBM part of the loss with importance sampling thus avoiding expensive MCMC approximations. The approach is interesting and new (to the best of my knowledge). It is motivated by the exposure bias or in other terms *"a discrepancy of the input context distributions between the training and inference stages"*, which ARGM are reported to suffer from. Accordingly, the experiments show improved performance when combining several ARGM with the proposed E ARM method. That alone, however, does not yet make strong paper (rating 8 or higher) in my opinion, because the authors don t study the effect of their method in detail. That should be made explicit for better understanding. An interesting idea that can lead to improved performance on certain benchmarks. However, deeper insights about the effect that the model addresses (exposure bias mitigation) as well as estimations of the increased overhead are missing in the experiments, thus limiting the scope of the paper.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper proposes a method to make generative replay for continual more effective even if generated data quality is not perfect. The method uses replayed data only as negative samples for current tasks and not as training samples to remember. The paper presents an interesting method that could improve how to use replay in continual learning. It would make more sense to run the different baseline approach with and without the replay strategy to estimate the interest of the proposed method. Other Remarks:  the approach is designed to make low quality replayed data useful. However, no assessment of the replay quality is presented. the approach for Core50 and Imagenet is not the same (one pretrained feature extractor and one feature extractor trained only on task 1), making it difficult to assess the results. the paper should be clear from the beginning that the replay process is realized in the latent space. Why the backward pass of new samples goes to past classes? The related works section presents a bibliography of generative replay but do not provide perspective on the link between the presented approach and the bibliography. However, the experiments are not designed in a convincing setup to evaluate negative replay.<|endoftext|>However, privacy issues or storage overhead makes replay methods impractical. Aware of this problem, generative models have been proposed in previous work to generate data that represent previous experiences. Still, these methods suffer from low performance because of the continual training of the generating model and/or the generation of high dimensionality data. Instead of using the generated data as a positive example of previous classes, they used it as a negative example for the classes present in the current experience. The authors do not present a new idea but a very ingenious solution for a recurring problem. By generating data that cannot be used directly as positive examples (because of the limitations of generation methods), they use this data as negative examples for other tasks. I think the authors present surprising results. To train each classifier, this method uses only 1 positive example and the others as negative examples. I liked how the paper is structured, introducing the current problem of methods that use generated inputs. Then, how this can be alleviated by using negative replay as negative examples. Since you only train the columns of the classifier of current experience, there is no supervision needed. This is since there would not be enough data to correctly train the columns of the classifier of the classes that are not present in the experience. Could this technique be used in other scenarios?<|endoftext|>This paper proposes a novel learning scheme for generative replay methods in continual learning. Different from the original generative replay method, the generated examples are treated as negative examples for new classes. The main method is based on AR1, is there a particular reason of using this AR1? Could the method still work well under the case that only generative replay is allowed (no additional components as in AR1)? 2.To my understanding, the main purpose of the paper is to learn new data better, instead of reducing forgetting on the old data. However, according to Figure 2 (c), the gradient will also flows back to the feature extractor (when it is not frozen). It would be idea to compare with those methods. The idea is interesting, but the novelty is limited, and some details and comparison experiments are missing.<|endoftext|>The key idea proposed in this paper is to use generative replay as negative samples to improve performance in class incremental scenarios of continual learning. But, these imperfect samples can still be relied on as negative samples for classes being trained in the current experience to prevent "learning in isolation" problems. ### **Strengths**  The key idea of using generative patterns/samples only as negative samples is simple, yet effective. But these samples are still important for the new classes being learnt in the current experience due to the "learning in isolation problem". Empirically, this approach is found to significantly outperform both positive generative replay and no replay paradigms. Maybe for CORe50, the accuracy is calculated across all classes even though they haven t been trained yet, while this is not the case for ImageNet 1000. Hence, instead of PR OD cumulative scores would probably be a better upper bound. Another question that could be raised here is the choice of pretrained model as the feature extractor. For example, if an unsupervised/self supervised approach is used that uses negative examples and learns linearly separable embeddings, will it reduce the need for generative negative replay for continual learning? ### Typos  Table 1: PR OD instead of PR OGOverall the key idea proposed in the paper is sound and reasonably well justified with experiments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The foundation of the work is the Neural Network Gaussian Process (NNGP) model, which is taken as the infinite width limit of a deep net under certain (mild) assumptions; that is the so called Master Theorem. The here proposed approach treats the variances, σ^2, of the penultimate layer weights as random variables. They show that, under this setting, the infinite width limit of the network is a scale mixture of NNGPs. In the special case of an imposed Gamma prior, the so obtained scale mixture reduces to a Student s t process; this is a well known result. The derivations pertaining to posterior inference are correct and result in efficient algorithms. The results are broad enough, with many comparisons and many datasets considered; these vouch for the usefulness of the method.<|endoftext|>Building on the recent results on the correspondence between infinite width neural networks and Gaussian processes (NNGPs), this paper proposes and studies a simple extension, looking at a scale mixture of such processes. The key idea is to introduce a scale prior on the parameters of the last layer, allowing construction of a richer classes of stochastic processes, particularly those with heavy tails. Empirical results are also provided, showing the promise of the approach and robustness to out of distribution data. As for the empirical results, the scale mixture of NNGPs seems to give mixed performance (both accuracy and robustness to various perturbations) for different datasets. How should one decide which model to use for a given task to achieve desirable accuracy and robustness? Rigorous results are provided and they seem to be technically correct (although I have not checked all the details).<|endoftext|>In this work, the authors propose an extension to the NNGP model that allows for the formulation of more general stochastic processes that may follow alternative distributions such as Student $t$. This is achieved by way of introducing a prior on the scale of the parameters in the last layer of the neural network. The experimental evaluation features a mixture of regression and classification tasks, with a particular emphasis on how the use of models with heavier tailed distributions can be better suited to datasets with challenging properties such as corrupted data and label imbalance. Although the core contribution feels fairly incremental in nature, both the execution and the corresponding analysis are non trivial. This should at least be possible for the image classification experiments right? The contributions featured in this paper provide a valuable and non trivial extension to the literature on NNGPs. Given that this is a fairly new area of research, such work is especially timely.<|endoftext|>This paper proposes to extend the typical GP formulation as the limiting case of a NN with Gaussian weights when the number of hidden units tends to infinity. In particular, it considers a scale of mixture of Gaussians at the last layer for the prior. Gamma distribution it is well known that the result is a Student t process. The resulting processes are heavy tailed and more robust as shown in the experiments. So there are little new practical applications of this work.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper focuses on the problem of identifying bad training data when the underlying cause is unknown in advance. Authors develop an algorithmic framework, DATASIFTER, for general robustness to bad training data. The authors present no empirical evidence their algorithm solves this task in a reasonable manner. Models considered in the work are small in size as compared to the state of the art models. However, the method primarily depends on a set selection phase for which no empirical/theoretical evidence is provided.<|endoftext|>The paper provides worst case analysis of data selection using linear heuristics, revealing potentially hazards using valuation based approaches, which is interesting and novel. The experiment in E.5.1 uses SVM as utility metric which as a special instance can be much better than the worst case performance bound (and the same is also true for a wider range of utility functions used in practice). 2.The paper shows good empirical performance of the DataSifter algorithm in certain identification tasks. The authors first show that the valuation based approaches can have bad performances, and propose the DataSifter algorithm to approximately solve the combinatorial optimization problem.<|endoftext|>This paper proposes, *DataSifter*, an optimization based, general purpose framework for filtering "bad data" from a training set. * There is a good number of typographical errors in the main text. I have concerns in particular around the evaluation as detailed above. Edit: The authors provided thoughtful detailed reviews and addressed many of my concerns. 3) Your empirical evaluation includes an impressively large number of baselines. * Reviewing your supplemental materials, most of the models you consider are small in size (please correct me if I am wrong). General robustness is a clear strength of your method so being explicit on this definition earlier may be more impactful to a reader.<|endoftext|>In this work, the authors:1. shows that linear heuristic like Shapley value based methods do no perform well w.r.t worst case performance in selecting the best subset of data. This paper suggests an algorithmic framework based on utility optimization approach to achieve robustness against general bad data. The method performs well empirically. 3.The authors criticize linear heuristic methods showing that worst case domination ratio of the methods are small. But there is no theoretical analysis of DataSifter. It performs only empirically better, and since the experiments are done for a specific choice of utility function, nothing can be said about its worst case performance, and it could as well be as small as linear heuristics. 4.The DeepSets approach to utility optimization has been introduced by Wang et.al.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper presents a genetic algorithm (GA)  based method to generate adversarial images, i.e., images perturbed in a way such that it leads to misclassifications in a number of image classification models. It s however difficult to see the connection of this with Instagram filters. How is a particular state represented (each row indicating filter parameters)? Details need to be included on what happens during the cross over and the mutation operations? The paper mentions about "gradient towards a better solution". From a novelty perspective, the paper is quite limited because algorithm is also based on the AGV paper (Baia et.al.2021).The only difference is that multiple target networks are used. It s not clear why (Baia et.al.2021) wasn t used as a baseline. Moreover, there is no rational explanation provided on why zooming in on an AGV generated image wouldn t produce the visible artefacts   what characteristics of the proposed approach is likely to ensure this?<|endoftext|>The paper proposes a black box attack method that uses an evolutionary algorithm to find the best image filter parameters that can achieve untargeted attacks. Model ensembling is also used in the AE generation to improve attack transferability. The proposed method is compared to other similar image filtering based AE generation methods. As also identified by the authors, the idea of using image filters is not new many existing works (SemanticAdv, Colorfool, Edgefool, Filterfool, ACE, etc) have already adopted such a concept to generate benign looking AEs. The authors address this by using evolutionary algorithms to find the best filter parameters, which is also a straightforward solution and the results are not surprising. However, I cannot find any stealthiness evaluation in the paper. One solution is to calculate image perceptual metrics (such as those used in GANs [1]) of the AE. However, the current evaluation in the paper is still lacking.<|endoftext|>This paper proposed a nested evolutionary algorithm to generate adversarial perturbations under the black box settings. In addition, the authors compared a lot of other "unrestricted attacking" methods and demonstrated AGV can achieve higher transferability with less queries. Weakness:Experiments are not complete. (1) The paper claimed, "our goal is to find one adversarial perturbation per image that can fool many deep learning models". (Table 3) Readers can learn what model structures are harder for unconstrained adversarial attacks. But I am not sure if other methods are also trained for attacking 5 different models. (4) I am not sure how to replicate this paper. Visual examples are limited. The authors claimed, "our filter composition cannot be distinguished from any other filter composition used extensively every day to enhance images". In addition, ten image filters used in the paper should also be visualized. I think the paper can be largely improved if previous issues are solved.<|endoftext|>This paper introduces a new family of black box adversarial attacks. These attacks are constructed by composing Instagram filters based transformation in the input space. The input is transformed with 10 different filters. To get the optimal values of these parameters, an evolutionary approach is used. 2.The proposed family of attacks is a black box and more applicable in the real world. However, it would be interesting to see some fail cases. 2.A comparison of the number of queries used by these attacks vs. other black box attacks should be added. It would be interesting to see if Instagram filters can also be used to improve privacy, something similar to [1]. However, I am not very familiar with recent work on black box attacks.<|endoftext|>The resulting adversarial attacks do not have recognizable artificial patterns as in other methods and show competitive performance including a difficult black box scenario. Strengths:1) Interesting evolutionary strategy to craft strong transferable attacks using image filters. 2) Adversarial examples look only as filtered clean images with no artificial patterns as in other gradient based methods. Judging by the examples shown in the paper, the adversarial images are mislabeled to a class which is very close to the ground truth (for instance, from “standard schnauzer” to “mini schnauzer” or from “Labrador” to “Dobermann” ) indicating that the attack strength is low. Is there any way to fix this? Unfortunately, it undermines the whole purpose of the method, its practicality and effectiveness. Therefore, I tend to be on the rejection side.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Under some distribution assumption on the images based on their patches, the authors claim that the algorithm provably learns CNN. Experiments are conducted on MNIST and FashinMNIST aiming to verify the importance of covering number of the patches. ** Weakness **  The proved result seems to be "underwhelming" compared to the author s claim. However, the learned model is not necessarily a "standard CNN". The authors stressed the importance of covering number in the paper.<|endoftext|>In this paper, the authors consider learning the function class of one layer convolutional networks. However, the author falls short of convincing that this covering number could a feature of real dataset that ensure tractability. Hence, in order to close the gap between theory and practice, and guarantee tractability, distributional assumptions should be introduced. This paper  give a simple example of tractability under the natural assumption of covering number (somehow necessary for PAC learnability).<|endoftext|>Some experimentsare provided to demonstrate the algorithm on MNIST and Fashion MNIST. ### Strengths  The paper is well referenced, well written, and well motivated. This description may not be appropriate, given that the parameter $N$ is not a true input parameter, but rather a derived one  from $r$ and the properties of the set of all patches $P$ (and without  further assumptions on $P$, one cannot guarantee a priori that this is not  exponential in $d_P$).<|endoftext|>These are two major flaws of this work: I believe that the idea that if some data are low dimensional then it s easier to learn is not really new and specific to this method   it needs to be shown that this is actually the good underlying model of the data. This paper tries to address the difficult question of understanding CNNs through a simplified model.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper describes a relatively simple modification of the DQN algorithm using a proximal term. The approach improves the performance of DQN, but it does not improve on the current state of the art. They argue that applying the idea of proximal iteration to DQN can improve the stability of the learning and the performance of the algorithm.<|endoftext|>This paper applies proximal iteration, via L2 regularization from the weights of the target network, to deep Q learning. Similarly, much of the paper is devoted to discussing proximal methods in the context of strongly convex objectives, e.g., with linear functions, which is not the setting used in the paper. One can conclude from this analysis that the proposed method generally improves performance in these settings. These results need to be tied more specifically to the issues with deep Q learning. While this paper provides a set of promising empirical results, the relative lack of explanation and analysis around why proximal iteration helps in deep Q learning significantly detracts from the impact of this paper. The proposed method could also be applied for the purposes of state value estimation.<|endoftext|>This simple *trick* improves the performance of DQN and DDQN on a set of Atari 2600 environments. Although the regularization term in Equation 4 is effective for DQN type algorithms, the authors did not give a convincing explanation to explain why proximal iteration works in this setting. 4.The author just verified through experiments that DQNPro can be combined with DDQN. Since there are some contradictions between the functions of DQNPro and DDQN, is there a deeper explanation for the performance improvement? The authors use a simple trick to improve the performance of DQN type algorithms on some Atari environments.<|endoftext|>This paper is innovative and well written, the experiment parts are very solid and the authors give a clear analysis. The authors theoretically analyze the advantage of proximal iteration. (2) Analyze the source of improvement of other DQN variants, in terms of the magnitude of the updates to the target network similar to Figure 4, I think this will give a new perspective into the understanding of the critic network.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; In contrast to previous works on lifelong machine learning (LML) that put their focus on the supervised learning settings, this paper concentrates on the scenario that only a limited amount of data is available. Considering that the performance of the proposed method is not that satisfying either, my current rate of this paper is: "3: reject, not good enough". [2] Most of the components of MAKO are based on the prior arts proposed in previous literatures, for example, the method for labeling training data with weak supervision is based on Snuba, therefore, the technical contribution of this paper may be limited. Labeling new data can be realized by using the data programming method which is supervised by the labeled data.<|endoftext|>This paper proposes  data programming method, named Mako,  for semi supervised continual learning. Mako automatically generates labels for unlabeled data with a set of weak labeling functions, each of the functions is trained on subset of training set with bootstrapping. The analysis and experiments also need to be significantly improved. Also, seems that there are many hyperparameters in the proposed method.<|endoftext|>The scenario that the author want to solve where labeled training data is expensive to obtain in some lifelong machine learning tasks is also interesting. I hope the author can give some description about this. Even in Sec.4, I found that it seemed to be integrating some of the previous working methods to form a system framework, such as data programming, confidence calibration and so on. For example, the most important problem in semi supervised learning is how to obtain accurate pseudo labels, but I don’t see how the author solves this problem, only by using a previous method data programming? This paper implemented a semi supervised continual learning framework that can be mounted on top of any existing LML tool. The problems and application scenarios proposed in this paper are of practical significance.<|endoftext|>This paper presents an interesting idea of using data programming techniques to enable continual semi supervised learning with limited labeled data. Experiments show that the proposed framework achieves similar performance to fully supervised methods. #### ProsContinual learning with fewer annotated data is an important research task. c. It seems a sophisticated process to annotate unlabeled data. #### Minor points“purposed” in the 1st line of the second paragraph in Sec.2 should be “proposed”? In conclusion, despite the successful improvement of the LML task, I still have lots of concerns about the labeling results, which is the key to improvements.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; * The paper reads a set of subjective observations about the meaningfulness of explanation and relationship with data manifold + tangential theory. The paper does not have a coherent story.<|endoftext|>This paper studies the following hypothesis: "Gradient based explanations are more meaningful the more they are aligned with the tangent space of the data manifold".<|endoftext|>This paper makes the hypothesis that gradient based explanations are "meaningful" if they are aligned with the tangent space of the underlying data manifold. For example, is an explanation more meaningful if is more structured?<|endoftext|>The paper argues that the components of image gradients that lie in the tangent space of the data manifold are semantically meaningful, whereas the part orthogonal to the image manifold is nonsensical. The experiments in the paper support this hypothesis to an extent. Squaring the formula would correspond to the length of projection of v1 onto grad relative to length of grad, and would seem like a perhaps more appropriate choice.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 3; Any thoughts/comments on this? 2) For Table 3, the experiments are performed using independent 4,6 and 8 layer generators to establish the point that the pretraining signals provided are diverse. Strengths:1) The paper is very well written and easy to follow. Was some form of cross validation performed?<|endoftext|>This paper presents an adversarial learning framework for pre training text encoders following ELECTRA style architecture. Authors spent great deal of effort on the ablation study to show that the proposed framework and design choices perform better than existing state of art methods and other alternatives on several downstream NLP tasks. I recommend this paper for publication given a satisfactory explanation of the raised weakness.<|endoftext|>With the strengths being said, I hope to also point out that the paper s application of adversarial training is one attempt in many possibilities, and in many cases it is not clear where the improvements come from. Good experimental results of the proposed model.<|endoftext|>This work proposed a new framework AMOS to enhance ELECTRA style pretraining. The paper is clear written and relatively easy to follow. The results show on GLUE dev and SQUAD 2.0 also looks significant. First, the author states that the model is based on mix of training signal generator, however, the work is actually using different layer mixture of signals. Besides this, as this is a pretraining work.<|endoftext|>The proposed idea, therefore, is to have the discriminator operator on top of a “mixture” of discriminators of different capacities. This is a meaningful contribution to Electra like pretraining approaches and provides fairly significant downstream improvements.<|endoftext|>This paper present a method for pretraining language model using generator discriminator training. 4  the author proposed using gumble softmax to enable gradient backpropagation from discriminator. what is the performance on downstream task? This is confusing, because generator is trained jointly with discriminator, and MLM embeddings are trained via two gradients, one from discriminator, and the other from MLM pretraining of generator! what is the explanation on this?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; I believe the paper still needs significant edits to reflect the points in the reviewer responses. The paper adapts the known concepts for image GAN literature to time series, without substantial extra inventions. It poses a concern for novelty. Whereas for the experiments, the Author use the RNN based DeepAR. The loss function choice seems heuristic and not very well explained. What happens if the signals have high vs. low frequency components? I don t think the FID score would have significant value for time series as the perceptual quality of time series to humans is different than images. Overall, rather than focusing on a lot of results, I suggest emphasizing on the key capabilities the data synthesis brings, with significant improvements. But the novelty is not very high, and also several aspects of the paper need improvements. I am willing to increase my score if the authors address the points above. I am updating my score as the reviewers have addressed some of my concerns with extra experiments.<|endoftext|>This paper proposes a GAN based approach (PSA) for generating realistic time series data that can be used to improve several downstream tasks such as imputation of missing data and forecasting. The proposed approach is novel for generating time series data. 3.This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. 4.The paper is well written and easy to follow. Could the authors clarify how the loss function in Equation 7 is used to train the generator and the discriminator? 2.What are the time features (referred as X in the paper)? How does the summation work with varying lengths of sequences? 2.Remove the additional  of  in section 4 para 1. It also introduces a metric for evaluating the quality of synthetic time series data which has been lacking in this domain.<|endoftext|>The authors use the data generated by their proposed model to train forecasting networks, which improves the baselines. They also show that the proposed GAN can itself be used as  a forecasting model and that it has competitive performance to baseline models. Finally, the paper proposes an adaptation of the FID score for time series. 2.Adapting the FID score to time series seems like a good idea. This could be an interesting measure of performance for future work. * “The training procedure to address those issues are presented in the appendix.”  > *is* presented in the appendixThe proposed model outperforms the baselines and the proposed FID score for time series looks interesting.<|endoftext|>The authors propose a new GAN based algorithm for time series synthesis. The idea of progressive growing of GANs is convincing for synthesizing time series. The time series data is difficult to fit due to high volatility. So, the idea is convincing. EBGAN is not usually applied  in this scenario. But the novelty and experiments are not good enough for ICLR.
Reject; rating score: 1; rating score: 5; rating score: 5; The paper does not provide a theoretical analysis of the proposed method. The authors oversell their contribution: Generalizing the Generalized Random Forest (Athey et al.2019) from a partially lienar model to a full nonparametric one is not very creative. It is unclear to me how the estimator $\hat{\mu}(t, X_i)$ is constructed. However, they do not provide any evidence to back up this claim. Additional comments:  Poor notation: For example, on page 3 $\Omega$ is defined as a single vector, on page 4 the authors draw samples from $\Omega$, in Algorithm 1 on page 5, $\Omega$ denotes the dat set. The language is unnecessarily pompous and riddled with grammatical mistakes to an extend that makes it difficult to follow the authors ideas. The simulation study and real data analysis are weak.<|endoftext|>The paper is concerned with an interesting and important problem in causal inference, which is heterogeneous treatment effect estimation with continuous treatment. Modeling the nonlinearity of the dose response function is also an important and challenging aspect of this problem. The presentation of the proposed method is also difficult to follow. For example, it is not clear to me what are the components of CF, and how GCF is derived and differs from CF in section 4. It is not clear to me whether such a contribution is significant enough to motivate acceptance. So, it is not clear what might be the additional value brought compared to Kenndy. However, the empirical results on synthetic data and real world experiments help to assuage this concern. Correctness: I did not identify major issues in terms of correctness. I appreciate the authors running real world experiments with a/b test to demonstrate the performance of the proposed method.<|endoftext|>However, I think the paper is hardly understandable to most readers, and the theories are not deep enough. Here are some details. I suggest conducting a more extensive literature review to provide more clear motivation. 2.What is the meaning of "local splitting" in the 3rd paragraph in Section 1? Please don t assume that the authors are familiar with the details of the causal forest algorithm. 3.This paper doesn t answer or motivate why we have to use the proposed GCF instead of the Kernel DML (Colangelo and Lee, 2020) or the doubly robust estimator provided by Kennedy et al.(2017).It would be great if authors can provide more extensive literature review on the HTE estimators for the continuous treatment case. 6.It s not clear what the contribution of the paper is. 8.In the experiment, why don t you compare with the Kernel DML (Colangelo and Lee, 2020)? Without this prerequisite, it s impossible to understand the paper.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper lacks necessary evaluation on real life datasets. The authors report numerical results of the proposed algorithm on synthetic data.<|endoftext|>## Weaknesses  There is a claim concerning the time complexity in the abstract but no further discussion. It would strengthen the paper if the authors presented their analysis. The proposed method is based on the FFQR algorithm of Feng et al, but includes a tolerance based stopping criteria.<|endoftext|>The main theoretical results are essentially the same as in Feng et al., (2019).
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; It samples a K sets of edges based on learnable node affinity matrixes. Strengths:1) The proposed methods perform favorably in the experiments. 2) The presentation of the paper is clear and easy to follow. Weakness:1) The motivation of designing such partition based GNN is not clear. The presentation of paper is well organized and experiments results are generally favorable.<|endoftext|>The suspect is that this is a choice driven by necessity of convergence than to make the edge partition "more facilitative to the classification task". Also, it is not clear why modeling latent node communities should be important to solve the main supervised task (point i) above). For these reasons, the empirical results in this paper should not be considered of sufficient quality, and the authors should set up a proper empirical evaluation. Q: what are "observed (target) labels"? In light of the above considerations, the paper needs substantial rewriting and the empirical evaluations must be done following a proper protocol.<|endoftext|>This paper investigates how an edge is formed by different latent inter node relations and extends the community based edge formulation mechanism to graph neural networks. A variational inference framework has been proposed to jointly learn how to partition the edges into different communities and combine relation specific GCNs for the end classification tasks. Experiments on several real world graph datasets demonstrate the effectiveness of the proposed method in both node level and graph level representation learning problems. It is interesting to investigate how an edge is formed to make use of the inter node relation to enhance GNNs for node representation learning. Experimental results on several graphs demonstrate the effectiveness of the proposed method in both node level and graph level tasks. However, several claims are not well explained.<|endoftext|>The key idea is that nodes may be organized as overlapping "communities", where each community represents a particular type of relation. Overall this paper makes an interesting contribution to the field. This stage is entirely unsupervised, because it uses only the observed edges and not the edge labels. My concerns are limited to the problem motivation as presented in the paper, and the extent of the ablation studies.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper tries to study the effect of dropout by studying the trajectory of optimization via dropout. It is claimed that dropout can help find flatter minimizer. I am not quite familiar with this area. The main concerns are:1. 2.It is observed that regularization also helps and find flatter minimizer and there is work to bridge the dropout and reguarliation: Dropout Training as Adaptive Regularization.<|endoftext|>Most of tools are from the existing work, and neither insights nor empirical results are strong enough to make this paper stands out. The paper shows that the noise induced by the dropout has a similar structured introduced by SGD that leads the training to find flatter minima. It would be great if the authors could polish the figures and/or explain the current observation. The computation method for H on neural networks has never been introduced in the main text, making the current empirical results questionable.<|endoftext|>3.Experiments can be strengthened. To make a more convincing conclusion that dropout can lead to flatter minima, I believe experiments on modern datasets and models (e.g., ResNet on ImageNet) need to be run. In addition, the novelty of this paper is not that high based on my understanding.<|endoftext|>To my best knowledge, this is a first work that shows the relationship between Dropout and the flatness of minima. However, my main concern about this manuscript is the complete missing of theoretical analysis, which makes the current version of the paper not strong enough for publication.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 5; In that case, has this not been done before? (Accuracy for SAE large and Baseline in Table 3)* Third, I think there are a lot of other issues which make the paper overall hard to understand. The paper gives a "simplified example" but never elaborates on it to describe how the full model works.<|endoftext|>The paper proposes to use the predicate argument structure of sentences inorder to improve the results of a question answering system on the HotpotQAdataset, where the answer depends on a number of supporting sentences. In addition, the authors should study and review the latest related work,clearly write the problem setting, model description, experimental setup andresults, as well as give an analysis of the results to provide anunderstanding of why it is working.<|endoftext|>11) "Who" does not appear in the figure due to it is regarded as a stop word? What is this? 2) The paper claims that using SRL can improve interpretability of a model.<|endoftext|>Questions to the Authors. Extra: please double check for typos and the title of the paper. The authors perform an evaluation on  Question Answering and an ablation study.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Additionally, it is said that the “…shaping rewards supplement the environment reward and promote effective learning, our framework therefore addresses the key challenges in RS”. Clarity: The paper’s clarity can be improved by being more precise in writing and avoiding redundant arguments. This paper aims to tackle an important problem in RL and does a good job of conducting some experiments to show what the method learns. However, there are some significant concerns with the paper.<|endoftext|>The authors propose ROSA, a reward shaping method that trains a separate “Shaper” policy to learn how to generate reward bonuses. As the paper notes, this multi agent approach could lead to interesting new solutions for RL. “these methods provide no performance guarantees nor do they ensure the optimal policy (of the underlying MDP) is preserved”Some curiosity based reward shaping methods provide guarantees (though “curiosity” is a bit ambiguous of a category).<|endoftext|>The paper proposes to frame the reward shaping method as a Markov game between two players. Yet the method that the authors present ties the two concepts together. The authors provide some theoretical guarantees under a large set of assumptions. This is simply false.<|endoftext|>In my mind, the primary insight is to connect reward shaping to Markov games. Can the authors provide additional theoretical or experimental justification for this claim? This seems at odds with the later explanations about the switching behavior. It is well known that RL performance can vary significantly between random initializations. The paper makes interesting contributions, but the clarity could be improved.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 10; The paper provides scaling laws for machine translation models. They find that the scaling behavior of cross entropy loss can be described as a bivariate function of encoder and decoder size and make recommendations for optimal allocation of encoder/decoder capacity. They also find that “translationese” can affect the model scaling behavior. 2.It is interesting that contrary to previous findings, this paper suggests that it is better to scale the decoder than the encoder, providing guidance for future research on deep NMT.<|endoftext|>Quantify the evolution of model quality as a function of model capacity for encoder decoder NMT models. The research questions explored in the study are important and interesting, e.g., the scaling properties of composition bias and the influence of source/target naturalness on the scaling behaviors. 2.This paper provides experiments to explore how the composition of training/test data affects the test/training loss for scaling NMT models.<|endoftext|>This paper studies scaling laws for NMT. It confirms and extends some of the existing work on scaling laws, and as such it is a valuable contribution to the field. This is a strong empirical paper that extends our understanding of scaling laws in deep learning. The research questions are clear and the results are convincing.<|endoftext|>This paper presents a study on the scaling power of NMT Transformer (encoder decoder). It shows:   how the scaling of encoder decoder compares to language model  that NMT behaves very differently on translationese texts   propose to model the scaling of NMT encoder decoderStrengths:  the research questions answered in this paper are interesting and help to better understand the mechanics of NMT. evaluation is not only performed with BLEU (BLEURT is also used)Weaknesses:  almost the entire evaluation is not reproducible: undisclosed datasets, use of a non standard BLEU implementation, pre processing of datasets not detailed, etc.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper proposes an MCMC sampling algorithm for discrete distributions. Do the authors have any proposals for how to choose the path length proposal? Ideally, by applying the proposal distribution multiple times before the accept/reject step, the sampler is more likely to escape local modes than the original Locally Balanced algorithm. There has been much work on this in the context of HMC (to which this method has some spiritual similarities). As well, theorem 3 of this work is remarkably similar to theorem 1 of that work. I liked this work initially and I am glad now that other reviewers see it as I did. Methods like this are likely to find quick impact since they can be easily incorporated into existing systems. Further, there *appears* (see comments in Weaknesses section) to also be a notable improvement when using PAFS to train EBMs. This work presents a simple extension over prior state of the art discrete MCMC methods which the authors demonstrate leads to improved performance on a wide variety of tasks. The method is simple, easy to implement, and appears correct.<|endoftext|>The paper provides a generalisation of the work of pointwise informed proposals (Zanella 2020), where the acceptance criterion is applied after L sampling steps, instead of each single one. This comes with the advantage that the overall acceptance criterion  is guaranteed to be larger than the corresponding one in (Zanella 2020). The analysis is corroborated by experiments on several energy based models both for inference and learning tasks, thus showing the superiority of the proposed algorithm and its extension over existing MCMC strategies. **Contribution and Potential:**The work tackles an important open problem for MCMC in discrete spaces, as it enables making large sampling steps while retaining the local benefits of pointwise informed proposals. **Related Work**Some recent work must be discussed. However, I have some questions for the authors, which might affect the final result.<|endoftext|>Additionally, following previous work, they propose an efficient version using a linear Taylor expansion, thus saving a significant number of likelihood evaluations. (In this setup, we can only expect that using the approximation will hurt performance.This would show how much.) Also, for the real models, PAS and PAFS are compared only for the Ising model, which the authors state is close to linear (and thus possibly favors the Taylor expansion approximation). Is there a typo in equation 10? Is it the DAG that defines the factorization of the distribution as in a directed graphical model? On the positive side, I think the paper provides a nice and natural extension of MCMC methods in discrete spaces based on locally informed proposals. The theorem lower bounds the efficiency lost by using the approximation. Finally, I have some comments on the empirical evaluation. Section 3.2 shows that the method with a path of length L is more efficient than running LB 1 for L steps.<|endoftext|>[GZ] proposed a Kernel which evaluates the energy function over a Hamming ball surrounding the current state and [GWG] improved the efficiency of the latter by using a Taylor series expansion of the energy function. The authors of the current paper improve the acceptance ratio (Eq 2) and mixing (Thm3) by sampling a multiple transitions (a path) instead of a single neighborhood sample. Informed proposals for local mcmc in discrete spaces. ## Weakness1   The primary weakness in the paper is that the proof of Theorem 1 is incorrect. 2   The second listed contribution (linearization) is a rehash of [GWG] and is not novel. 3   There are multiple grammatical errors in the paper examples include:    The first phrase of the abstract should ve been `EBMs offer`    Second sentence, second para, page 3 should be `Instead of directly sampling from a large neghborhood path auxiliary sampler sequentially samples new states from a ...`. [RB]   On Markov chain Monte Carlo methods for tall dataR Bardenet, A Doucet, C Holmes   The Journal of Machine Learning Research, 2017 https://www.jmlr.org/papers/volume18/15 205/15 205.pdfThe proof of the main theorem is incorrect and hence I am rejecting the paper. I am updating my scores based on author discussion. Please refer to comments for details.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The contributions in the paper are limited and not well focused. Confined to using binary neural networks  It is not clear how the quantization of probability values for soft voting are set (i.e., predefined thresholds) and the effects on performance.<|endoftext|>[Bernstein’18] https://arxiv.org/abs/1802.04434[Chen’18] https://arxiv.org/abs/1803.09877[Karimireddy’19] http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf[Rajput’19] https://proceedings.neurips.cc/paper/2019/hash/415185ea244ea2b2bedeb0449b926802 Abstract.html[Sohn’20] https://proceedings.neurips.cc/paper/2020/hash/a7f0d2b95c60161b3f3c82f764b1d1c9 Abstract.html[Xu’21] https://arxiv.org/pdf/2109.05872.pdf[Konstantinidis’21] https://arxiv.org/abs/2010.04902This is an interesting paper, but the idea is quite similar to SignSGD paper, and has no thorough comparison with existing works. I like the topic of this paper, but I have some concerns as below. 1.It seems like the basic concept of FedVote is from [Bernstein’18]. I’m not sure whether the authors compared their work with this scheme.<|endoftext|>In Remark 2, the author(s) mention that the variance of quantization error will impede the convergence, which also holds for FedPAQ. **Pros**:  The paper is well written and related works are well address to my knowledge. AISTATS 2020.<|endoftext|>5) In the introduction the authors claim that "However, directly quantizing the gradient vector does not provide the optimal trade off between communication efficiency and model accuracy." Is there any prior work that supports this blanket statement? That said, there are some questions that remain about the claims made in the paper (see weaknesses above). The paper employs the existing approach of quantizing normalized versions of latent weights to achieve this objective.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The authors adopt ideas from multigrid methods in solving PDEs to Physics Informed Neural Networks (PINNs). They decompose the PDE solution as a sum of a coarse and a fine term, and train independent PINNs to learn each term. It would be helpful to see more difficult problems, as well as multilevel problems with n > 2. Training proceeds by alternating training between the two PINNs: $\nu_1$ epochs on the fine PINN, followed by $\nu_2$ epochs on the coarse PINN, and repeat. They compare a standard PINN vs a larger PINN vs their method (MPINN). That being said, the results in this paper are too preliminary to be accepted. The equation is chosen so that you get two different modes in the solution. How were the learning rates for the different problems tuned?<|endoftext|>This paper proposes a new physics informed neural network (PINN) that is motivated by multigrid methods. The idea of the proposed multilevel PINN is to write the solution for a given problem as sum of two terms, where the first term models fine scale structures and the second term models coarse scale structures. The performance is demonstrated on both 1D and 2D problems. * The term  robustness  typically means something different in the ML community. One shortcoming of this paper is that the experiments appear to be too preliminary, i.e., no benchmarks are provided and the models are not carefully fine tuned.<|endoftext|>This paper is not well written in English. My comments are as follows:1. Some explanations about how to choose this number should be added. 4.On page 7, on the last two lines the authors say “In 2D problems the effect of the size of the training set on the cost is far more important than in the 1D case, therefore we train the fine network on a grid of 484 points and the coarse one on a coarser grid of 400 points”. This paper proposed the multilevel physics informed neural networks (MPINNs), which is inspired by classical multigrid methods for the solution of linear systems arising from the discretization of PDEs. The authors are mostly just stating the observations, which is not enough to meet the standards of a good paper.<|endoftext|>The following claim is also not supported by any evidence in this paper and it is an opinion of the authors instead of a finding in a scientific paper: “It is clear that structuring the parameters in a clever way is more beneficial than just augmenting their number, to gain both computational time and expressivity.”In table 2, F is not used and the following sentence should be deleted in its caption: ”F” means a failure in all the runs. This paper proposes multilevel physics informed neural networks (MPINNs). Because the solution of PDEs can contain fast/fine components as well as coarse/slow components, the proposed method makes sense and I would expect that this works well for such PDEs. For the experimental setting, the authors mention that the learning rates are experimentally tuned, and the best decreasing strategy is adopted for each network. I would recommend reporting the results with various learning rates or fixing the learning rates a priori by directly using the learning rates of previous work of PINNs. In the current form, I cannot tell if we are overfitting the data and problems with the tuning of learning rates of MPINNs. The paper makes several random claims without any evidence or support. I recommend deleting those claims from the paper.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The submission proposes to evaluate covariate shift and "concept shift" through two scoring functions, one based on feature norms (for covariate shift) and the other based on angles (for concepts). To help the network decouple norms and angles, (a modified version of) a very recent method is used to decompose the feature norm and the cosine distances and also calibrate the network. I’d recommend rewording some of the framings of novelty such as “existing works do not distinguish between different types of OOD datasets”: Hsu et al.[1] discussed the distinction between “non semantic” and “semantic” shift, which as far as I can tell, is pretty much synonymous with the paper’s “covariate” and “concept” shift terminology, but without the notational difficulties discussed above. Ahmed et al.[2] discussed the need for detecting “semantic” anomalies, since we typically want to be robust to non semantic shift rather than flag it down (which is also part of the reason benchmarks such as corrupted CIFAR are typically used to test for robustness rather than as OOD detection). The novel evaluation perspective in the submission, which is quite interesting, is the breakdown of CIFAR 100 into sets of classes with increasing semantic shift. If the difference is due to differences in architectural/training choices, these choices need to be motivated since most of the reported baseline numbers in the submission seem significantly worse than in existing literature.<|endoftext|>This paper proposes a method for detecting two types of distributional shifts: covariate shifts in the input space $\mathcal{X}$ (due to input corruption) and semantic shifts (due to test data falling outside the support set of ID classes, $ y_\text{test} \notin \mathcal{Y}$). The idea is based on the decomposition of KL divergence between softmax prediction and a uniform vector. Furthermore, the authors propose Geometric ODIN to improve OOD detection and calibration, outperforming strong baseline on CIFAR10, CIFAR 100, and SVHN datasets.<|endoftext|>The paper starts from the KL divergence between a uniform distribution and the predicted distribution, based on which two score functions are derived for covariate shift and concept shift, respectively. The covariate shift score measures the feature norm while the concept shift score is essentially the difference between the cosine distance of the predicted class and the average cosine distance of all classes. Furthermore, the paper integrates a variant of the geometric sensitivity decomposition method introduced in a recent work into the proposed score functions, aiming to make them more sensitive to distribution shift and to help calibrate the model as well. I give borderline accept for now and will reassess the paper after the discussion period. The fields of OOD detection and confidence calibration are often studied separately though they are closely related to each other. CIFAR 10C and 100C have been widely used to test model robustness. What is the motivation for such a modification?<|endoftext|>This paper studies the problem of Out Of Distribution detection, which a focus on analyzing the covariate and concept shift. Leveraging these analysis, it further investigate score functions that capture sensitivity to those shifts and then theoretically derive new score functions, to improve OOD detection. This proposed method leads to a calibration function that obtain the state of the art calibration performance on both in distribution and out of distribution data. Experiments are conducted on small image recognition benchmarks, such as CIFAR100 and SVHN. 3.Leveraging geometric sensitivity decomposition and model the learnable instance dependent scalar alpha and beta is a neat idea. Overall, I like the explanation of Figure 1, but CIFAR 10 vs CIFAR 100 is a quite extreme example, because they are using the same source of data but categorized differently (I believe that CIFAR100 is a more fine grained label set). I believe the paper is very clearly written, well justified, and provides strong theoretical result and sufficient empirical result.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; Authors propose a way to perform multi task learning with gradient boosted decision trees. It is hard to perform multitask learning in decision trees. The main contributions from the paper are as followsAuthors propose a multitasking gradient boosted decision treeAuthors further propose a way to dynamically adjust the learning rate for each task so the learner does not overfit any specific task. They evaluate their approach on two public datasets. The authors evaluate their approach on just 2 datasets. More evaluation is needed to make a convincing argument for their approach. But the results are limited and do not appear very convincing.<|endoftext|>This paper proposes a new multi task variant of Gradient Boosted Machines (GBMs), a corresponding learning algorithm that adaptively adjusts the per task learning rate, and implements a version on the LightGBM framework to show empirical results.<|endoftext|>This paper proposes two novel techniques to the learning of GBDTs   the multi output tree and the mutli task splitting   that can be easily incorporated to the existing implementations of GBDTs without significant computational overhead. In the current form, it is not clear why the existing schemes for multi task boosted trees are not considered as baselines. It is very hard to follow the precise problem being addressed in the paper and the precise choices made in the proposed scheme. Finally, the empirical evaluation is quite limited, with just two data sets and an artificially created multi task setup. 2016."_I recommend a rejection based on the weaknesses I list above.<|endoftext|>Algorithm 2 looks ambiguous and it is not clear why it is correct. Could authors refer to any work to support this statement? The idea on developing algorithm with the shared tree construction based on tasks losses makes sense and in some sense parallel to the things done in deep learning. There is no any empirical evaluation in the paper why scaling with Algorithm 3 is needed or helpful. Thus, I think, the stated contribution "We propose a new learning algorithm that adaptively adjusts the learning rate for each task by taking the scale of residual errors into consideration. This should be considered as one of the multi task baselines  What is the performance of all models on the sub tasks?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; This is an exciting manuscript. An introduced point process based technique seems to be adequately fitted to model brain external/reactive and induced/active/imagery experiments. The proposed parameterization procedures work thoroughly. A similarity of the proposed method to old school ICA is somehow disappointing. The authors shall explain this part in more detail since the limitations of ICA applications are somewhat known in the community. To summarize, the authors propose an interesting DriPP technique to decompose complex M/EEG signals with a bit disappointing twist.<|endoftext|>This paper presents a novel model for revealing underlying stimulus response patterns in MEG/EEG data. How is the number of atoms and the duration of atoms determined for each dataset? A comparison with other related work (e.g., epoch averaging) is needed to support authors claims that their work address limitations of these prior work.<|endoftext|>The paper presents a model of the occurrence timing of waveforms in neural potential recordings in terms of a baseline rate and a set of driver processes. Is this feasible? Specifically the paper proses a inhomogeneous Poisson process with a baseline rate and a truncated Gaussian distribution for each driver process. 2.The approach seems to work well in practice. It would be useful to mention whether a joint model would make sense.<|endoftext|>This paper introduces a novel technique for the analysis of electro or magneto encephalogram based on an innovative point process approach. The point process model is estimated using an expectation maximization algorithm and is aimed at characterising events in the signals representing the brain activity. The paper introduces an interesting technique for the characterization of neuronal events, but I do feel that the technique should be evaluated more thoroughly and compared with other approaches. The authors suggest a smart initialization procedure, which is used for all experiments but the effect or benefit of this procedure has not been evaluated.
Reject; rating score: 5; rating score: 5; rating score: 6; I find however that the authors overemphasize the significance of the improvement they obtain with their approach. The idea is to first cluster the samples using a decision tree and then to tune the size of the ensemble independently for each cluster, instead of doing it globally for all instances. It is not new however, unlike what is claimed in the paper. Why not compare the optimum found by this protocol with the "theoretical" optimum found on the test set? I think the related work discussion should nevertheless includes these approaches. The proposed method is very straightforward. It would be interesting to report the size of these clusters. But I agree that there is a negligible overhead with respect to tuning globally the ensemble size.<|endoftext|>In particular, the authors propose an adaptive strategy that sets distinct ensemble sizes for different regions of the input space. For clustering data, they propose using a decision tree induced from the entire training set, and the leaves of the trees are the clusters that comprise the data partition. For addressing that I would recommend:      You should try it on dozens of datasets, given that the method is allegedly very fast. It is not clear to me which procedure is that. (3) several times with distinct number of clusters, and using the one with the best results? I think that is a vital part of the method whose discussion is not done at all in the paper. Paper with a nice, elegant, and very simple idea for adaptively generating different number of ensemble sizes throughout the input space, in an attempt to generating better results.<|endoftext|>An enhancement of the popular gradient boosting method is presented wherein the input space is subdivided into regions and the cross validated optimal number of trees to include in the ensemble is chosen on a per region basis rather than using a single global number of trees for the entire input space. Nonetheless, it does make sense to me to partition with a decision tree, so I am not bothered by this misstatement much. The method is intuitive and elegant but the number of benchmark datasets is not as large as it could be, and for that reason, the paper gets a 6 (marginal accept). My main concern is that the number of benchmark datasets is smaller than I would have liked.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; What’s the theoretical benefit of using that? I have no idea on any of these questions. * As the authors don’t provide the bound for $\kappa$, and results of Theorem 3(3) is not satisfactory, I don’t get any information the authors want to argue. It’s never a ``neural’’algorithm.<|endoftext|>Overall, the paper can feel like some disconnected pieces glued together. However, that does not automatically mean that the in between cases (when epsilon is a positive number) will inherit the advantages of the two methods.<|endoftext|>I feel the paper is not yet ready for publication at the current form, and I vote for rejection.<|endoftext|>Generally speaking, the paper is poorly written; many parts were hard to follow, but more importantly, the different sections feel very disconnected.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The paper proposes a flow based causal discovery method that is supposed to be scalable (in the experiments d 16,32,64) and able to impute missing values. One of the issues I have is that it is supposed to provide a more scalable alternative to better understood methods, e.g.constraint based methods like PC or FCI.<|endoftext|>The paper proposes a flow based causal discovery method to learn nonlinear causal DAGs with potential missing values. The proposed approach is based on variational Bayes and continuous optimization approach. •	Not clear why Carefl is relevant to this paper. I might have missed something here. This choice loses some uncertainty in addition to that lost by using Variational Bayes. In summary, while the paper has some interesting idea of combining variational Bayes algorithm with NOTEARS (continuous optimization) for nonlinear DAG learning, the significance of the contribution is not yet strongly supported theoretically or empirically.<|endoftext|>This  paper  focuses on causal discovery,  especially  on learning DAG using continuous optimization methods. \In the nonlinear additive noise setting, the authors propose a flow based method, called FCause, to optimize the log likelihood of the observed variables. Their method can handle high dimensional and missing data. However, there are some concerns / suggestions:1. I notice that it does not restrict the distribution of noise z. minor typos:Equation 3: no symbol description for tr,e. The method is a novel and interesting contribution to the continuous optimization literature<|endoftext|>This paper proposes a general flow based approach to learn DAGs from data which provides a unified view of existing continuous optimization methods for structure learning. As a side benefit, the authors demonstrate that the proposed method could naturally be modified to handle missing data. I would encourage the authors to include explanation about the difference between their work and [1]. However, it is surprising that this function works well with the Gaussian process dataset considered in the experiments, although the functional assumption is not met. Could the authors provide explanation on this? As the authors know, previous works [5, 7, 9] focus on learning DAGs instead of claiming about causal discovery.<|endoftext|>This paper proposes to combine the continuous optimization based causal discovery approach from notears with flow based function learning. An extension is given to data missing (completely) at random. Strengths:  The method presented in the paper is scalable and empirically gives good results in various settings. All aspects of the method are founded on theoretical developments in many earlier papers. But the empirical results are very strong, so that I recommend acceptance.
Accept (Poster); rating score: 6; rating score: 6; rating score: 10; I can understand the message that the authors want to deliver. All the other components remain the same and all the drawbacks are inherited (sample inefficiency and exploration). For policy optimization, you should study sample complexity), I am wondering if there exist other corner cases that C DQN could also suffer. When the paper mentioned efficiency or inefficiency, I think it is worth pointing out it is sample efficiency or computational efficiency. Current Section 3.2 seems to be not very in principle. But in this paper, the authors still discussed and compared the original version of RG algorithm. "In this work, we only consider the deterministic case and drop the notation Est+1 [·] where appropriate." If so, I think this should be highlighted in the abstract.<|endoftext|>The paper argues that the DQN and its algorithm and its variants do not guarantee convergence and that they can diverge in realistic settings. However, this is not the first algorithm that is doing so, and the experimental results show improvement only in specific selected, somewhat unusual circumstances. The authors are proposing an approach where the loss of their new algorithm C DQN is the expectation of the maximum between the MSBE loss and a DQN loss which is derived from interpreting DQN as a form of fitted iteration. While this guarantee is interesting, the experimental results do not show a clear motivation for the use of this approach.<|endoftext|>It is assumed that the problem can be overcome by a provably convergent NFQ like method. The paper is excellently written. There is a one dimensional variant and a two dimensional variant that is somewhat more difficult. Perhaps an investigation with this, or a similar benchmark, would eliminate the concerns that in this case the Bellman residual is always dominant and CDQN performs worse than DQN ... or confirm it. In Table 1 in the Appendix is written "DQN (ours)". The method presented could be groundbreaking. However, it is not explored whether the proposed method is generally applicable (see "Main Review" for details).
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Such a strong assumption might require experimental / theoretical justification, which is not provided in the paper. In addition, while the authors understandably had insufficient time to revise the paper during the rebuttal period to address the writing issues, I believe the claims of the paper should be more precise and well supported by the actual experiments. That being said, I do think that the paper can become much stronger with a clearer statement of the actual contribution. However, much of the theoretical results focus on how to obtain tighter bounds on the f MI.<|endoftext|>The results in Table 9 in the Appendix seem not to demonstrate a clear advantage of the new framework compared with existing baselines. Strength: The theoretical results show that the generalization from mutual information to f mutual information is not merely a mathematical game but also has its own merits. The paper is well motivated and clearly written.<|endoftext|>For example, SimCLR reports 95.3% accuracy on CIFAR10 with a linear classifier but in Table 3, the authors only show 89.7% accuracy. I think the authors should check their experimental settings again to make their results more comparable. 1) Novelty:  The main limitation of the paper is that the proposed idea is not novel.<|endoftext|>I assume aug is equal to either aug1 or aug2. Finally, the authors carried out extensive experimental results to validate the effectiveness of the method. It is novel to substitute the cosine similarity with $f$ divergence in contrastive learning, and the experiments validate that this is a very effective modification. The notation in Figure 1 is not easy to understand.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The paper proposes a new method to train ensembles of classifiers which are robust to adversarial attacks, in particular black box transfer based ones. This is achieved by enforcing the output of early layers of different members of the ensemble to have, on average, gradients with low cosine similarity, which should in turn create different decision boundaries. For this, the authors design a specific loss function, PARL, to be minimized at training time.<|endoftext|>This paper addresses the problem of L_inf transfer attacks. The proposed method is an ensemble that is trained with an extra loss term that measures the pairwise and layerwise similarity of components. This loss term is the average cosine similarity between gradients with respect to the input. At inference time, the overall prediction is by voting. The main idea of training diverse components makes sense. And why choose early layers? Although the proposed method has a higher standard accuracy on clean inputs than (Madry et al.2018), it s not good to have a weaker robustness under transfer attacks than a competitor s white box robustness. There are other non ensemble methods against transfer attacks that should be included in comparison, for examplehttps://arxiv.org/pdf/1705.09064The idea makes sense, but results are weak.<|endoftext|>This paper propose a regularizer of layer wise gradient to enhance the robustness against adversarial examples of an ensemble of deep neural network classifiers. PRO:  The paper is overall well written and easy to follow. It is also nice authors attach the codes and even pre trained models as supplementary materials. CON:However, I have to say the idea needs more refinement as the ablation study section does not provide too much information. Although the gradient of each layer of each classifier can be reused for regularization, will this slow down the computation? What if layers or features used for calculating similarity is sampled for computation efficiency? As explained in the main review, I admit the idea is good but it is at an early stage and needs more refinement.<|endoftext|>This work develops an ensemble based adversarial defense which diversifies the sub models to obtain robustness. I mainly have the following comments/questions for the authors. The high level idea of the proposed PARL is "to train an ensemble of neural networks such that the gradients of loss with respect to input in all the networks will be in different directions". Given that this is the same as the idea of GAL, one of the considered baselines, I think it is necessary to clearly identify the differences between PARL and GAL in Sec.4.2.2.To my understanding, there are two differences between PARL and GAL as to the formulation of the loss function: 1) GAL takes the gradients of the loss while PARL takes the gradients of intermediate layers  outputs, and 2) GAL minimizes the (smoothed) maximum cosine similarity between pairs, while PARL minimizes the sum of the cosine similarity between pairs. Or at least, can the authors comment on the intuition/insight on these differences made by PARL? However, there is a weird trend in the results that raises my concern.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper deals with tasks where we need to do pairwise comparison between two sequences. Moreover, training stability (of self distillation) and aggregate training time (for mutual distillation) deserve more discussion in my opinion. The paper proposes integrating two models together: bi encoders and cross encoders. A bi encoder encodes each of the two sequences separately and maps to a common embedding space. A cross encoder first concatenates two sequences, and the concatenation is sent to the model. Figure 1 illustrates the authors’ proposal well. The author proposes an extension called mutual distillation where the authors do self distillation on multiple pretrained language models in parallel (but the paper only experiments with two BERT and RoBERTa). Strengths  The author experimented on stronger models (RoBERTa large) too, and the results still hold. The figures are great. Reproducibility is likely strong. Concerns/comments  The paper will be stronger if the authors can include more hypotheses or justification on why, in some cases (see Table 1 and Table 8), TNec mutual works much better than TEnc (cross), especially because the bi encoders do not perform as well. How do the training time compare among different approaches, for GLUE tasks (especially larger ones)? How difficult is it to train the mutual distillation? The mutual distillation approach requires doing self distillation on multiple pretrained language models. Minor  I’m interested in seeing the performance on more difficult tasks that might require more world knowledge (e.g., MNLI), but this is not necessary. The improvements compared to SimCSE is significant. The writing is clear and the reproducibility is good. However, I would expect more theoretical justification on why TEnc mutual could be much higher than TEnc cross.<|endoftext|># Summary  This paper highlights the fact that bi encoders (sentence encoders) and cross encoders (sentence pair encoders) have been considered somewhat separately in the literature of sentence pair modeling, and that cyclic knowledge distillation from one to another (and vice versa) can be effective for improving the performance of both the encoder stereotypes. And then, the labeled sentence pairs are utilized to improve the quality of cross encoders. Further, the fine tuned cross encoders can also be exploited to label other sentence pairs which are again able to be used as a dataset for tuning the bi encoders (sentence encoders). The framework repeats this process until both the bi  and cross encoders become satisfactory in terms of their performance. The results demonstrate that the method improves both the bi  and cross encoders to be more suitable for sentence pair modeling tasks. # Strengths (Reasons to Accept)  Solid work. From this, it proposes an intuitive combination of the two encoder paradigms. # Weaknesses (Reasons to reject)  It is hard to say that the proposed method is novel, especially considering that all of the technical components used in this paper (e.g., pre trained language models, contrastive learning for sentence modeling, knowledge distillation between two similar models, pseudo labeling for data augmentation/generation) are quite common in the literature. I believe that the main novelty of this paper only comes from the fact that it proposes for the first time (to the best of my knowledge) to combine bi  and cross encoders. However, the specific way introduced to merge the encoders is not that special. Considering this, it is questionable whether the proposed experiments are comprehensive enough to back up the paper s claim. Even though there exist so many other concurrent and similar works for sentence modeling other than SimCSE, this paper only focuses on incrementally refining SimCSE. After all, it is still unclear why the iterative and coupled fine tuning of bi  and cross encoders results in performance improvement. How about using two separate bi encoders or cross encoders? This paper clearly points out that bi  and cross encoders have been evaluated and dealt with separately in the literature. From this, it proposes to combine the strengths of the two different paradigms by relying on cyclic knowledge distillation (or data augmentation). The paper is generally well written and easy to understand. Furthermore, I feel that this paper does not contain a thorough analysis on "why" the proposed method works, which is what I want to know the most. Therefore, my suggstion is a weak accept (or on the borderline to be accepted).<|endoftext|>This paper proposes TRANS ENCODER, an unsupervised approach to training bi encoders and cross encoders for sentence similarity tasks such as information retrieval, natural language inference, semantic textual similarity and clustering. The core idea of TRANS ENCODER is self distillation that alternatively trains a bi encoder and a cross encoder with pseudo labels created from the other. The authors also propose a mutual distillation extension to mutually bootstrap two self distillation models trained in parallel. The effectiveness of TRANS ENCODER is verified by empirical evidence. 2.TRANS ENCODER outperforms state of the art unsupervised sentence encoders on the sentence similarity benchmarks. The paper lacks the theoretical analysis and empirical investigation of why exactly these happened to TRANS ENCODER. 3.What is the rationale for using different training losses for "bi  to cross encoder" and "cross  to bi encoder" (soft BCE vs. MSE)? Overall, this paper introduces an unsupervised method that seems to effective in sentence similarity tasks, but it lacks the insights and empirical investigation of why this method works well. The work is not well supported.<|endoftext|>This paper focuses on the unsupervised training of bi encoder and cross encoder for sentences. A self distillation framework is proposed, in which the bi encoder and cross encoder may iteratively distill knowledge from each other. The experiments are performed with typical sentence pair datasets like STS, QQP, QNLI, MRPC, where improvements are achieved over SimCSE and mirror BERT. The joint training of bi encoder and cross encoder goes beyond the existing self distillation framework, which I believe is the major novelty of this paper. S2.Comprehensive experiments are performed, which clarify most of the critical aspects about the proposed methods. I m afraid that the proposed framework is not really unsupervised (I would call it pseudo unsupervised instead). In fact, it relies on labeled data: although labels are discarded, the pairwise relationships of the sentences are preserved, in which a large portion of the sentence pairs are positively correlated. The paper has to demonstrate that it may still work on a plain corpus where no such paired data is available; otherwise, it will be unrealistic as an unsupervised method. The paper is interesting and technically sound.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; Cons:  First, the existing FPN based detectors (RetinaNet, etc.) Second, it is a common sense that the FPN module is specifically designed for multiscale representations which is extremely suitable for object detection, and thus it s not surprising that enhancing the representation of FPN outperforms heavier backbone with the same computation budget. Finally, what’s the performance if the proposed TPN added in large backbone, e.g, X 101 DCNv2? To validate the generalization ability of the detector, experiments on large backbones are required. The limited novelty is my main concern to accept this paper, and the module design is too heuristic. I recommend "Reject" as the pre rebuttal score.<|endoftext|>The technical novelty of this paper is limited. So the major difference between TPN and existing works is that TPN replaces the plain skip connections within each FPN with residual modules. It is interesting to see that replacing skip connections with residual module is a more effective way to improve performance than stacking FPNs. But this change is incremental, and I do not consider this to be a novel design. The authors can consider an efficiency and accuracy tradeoff curve, which would give a better understanding on the importance of the residual modules.<|endoftext|>The proposed trident pyramid network, or TPN for short, is a core network structure. In this paper, the authors point out the importance of the core network (the feature aggregation part between the backbone network and the task specific head) and make a clear point that more computations should be allocated to the core network. The authors also provide a concrete implementation of the core network, called TPN, which performs "self processing" along with "communication based processing" during the aggregation of feature pyramids. The backbone core(neck) head structure can also be used for semantic segmentation, which is one of the three important tasks in image domain (the classification task is mainly about backbone, and not quite relevant to this architecture). This reviewer does not find any ablation study about whether (and to what extent) this complicated fusion structure outperforms the original simple TD and BU modules, given that "self processing" modules have already been inserted between TD s and BU s. This paper provides some insights to the design of a three part network for fine grained vision tasks. However, the experiments are not comprehensive, lacking the evaluation on a relevant task and necessary ablation.<|endoftext|>The proposed TPN has two tunable parameters B and L, that control the amount of self processing (B) and the amount of communication based processing (L). The quality of the paper may be enhanced if some concerns (see cons below) are addressed during the rebuttal period. Or are there desirable values of L>B for which the performance is optimal and further increasing L does not help? However, this result is not sufficient to verify the claim above. Experimental studies compare the proposed TPN to existing baselines like PANet and FPN, among others, on the COCO dataset. 3.The top down and bottom up blocks as shown in Figure 6 have a specific and distinct structure.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; 7 In section Denoising score matching it is written  empircal  instead of empiricalThe paper proposes an original work by offering a way out of the problem of approximating very complex densities by passing through a smoother variety. Even if some discussions are not well motivated, I recommend acceptance for the paper. The authors make a connection between the proposed method and some methods of the literature constituting new intuitions, different views of algorithms proposed in the literature. The experiments conducted suggest the efficiency of the algorithm to generate the most diverse samples${\bf \text{Strengths}}$1 The problem addressed in this paper (generating new independent samples of a given selection of samples drawn from an unknown distribution) is of great interest to the machine learning community. 2 The approach used (going through a variety where the density estimation is easier) is very original. But more importantly what are the disadvantages of Poisson MNM to discard it? It seems clear that with a low level of noise we do not need to have too many channels measurement (independent samples). This is not proved, right?<|endoftext|>This paper proposed a multimeasurement noise model to learn and sample from some unknown distribution and demonstrated its effectiveness on datasets like MNIST, CIAR 10 and FFHQ 256. It studies the convolution of the distribution with different levels of noise and uses the Bayes estimator in each noise level to recover the original sample. Are there any further results on Poisson MNM? How does it compare with the Gaussian MNM? It is also mentioned in the appendix that in your experiments, UL mixes faster. The new generative method it proposed can be of interest to the ICLR community.<|endoftext|>This paper introduced an alternative sampling method, with an application on generative models, by convolving an unknown distribution $p_x$ with a factorial kernel called multi measurement noise model (MNM). The resulting M density $p_y$ is smoother (easier to sample from), and is permutation invariant. Two factorial kernels, Poisson and Gaussian MNMs, are introduced for the convolution, and can be connected to Bayes estimator, and the learning of parametric energy and score functions. Two parameterization schemes are proposed for modeling the energy and score functions of the Gaussian M density respectively. .However, in Section~6, the discussion on the permutation invariant M densities is missing.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; In this paper, the authors studied improving the efficiency of adversarial training. The authors first analyzed the difference of perturbation generated by FGSM RS and PGD and proposed an efficient single step adversarial training method I PGD AT by adopting I PGD attack for training. Extensive empirical evaluations on CIFAR 10 and Tiny ImageNet demonstrate that the proposed I PGD AT can improve the robustness compared with the baselines and significantly delay catastrophic overfitting. 1 .I wonder if the approximation in (6) makes sense? The intuition of the proposed method is not really making sense and the empirical results are not quite convincing, therefore, I recommend reject for this paper<|endoftext|>The paper notices that the existing methods of adversarial training has the problem of catastrophic overfitting, and proposes an efficient single step adversarial training method I PGD AT by adopting I PGD attack for training, in which I PGD imitates the perturbation of PGD based on the magnitude of gradient. Extensive empirical evaluations on CIFAR 10 and Tiny ImageNet demonstrate that I PGD AT can improve the robustness compared with the baselines and delay catastrophic overfitting. ##########################################################################Pros: 1. The paper focuses on the catastrophic overfitting problem caused by the limited number of adversarial perturbation patterns generated by the single step attack method during the adversarial training process, and proposes I PGD AT to alleviate the problem. 3.This paper provides comprehensive experiments on CIFAR10 and Tiny ImageNet to demonstrate the effectiveness of I PGD AT. And further analysis on catastrophic overfitting problem is provided. However, since both FGSM RS and PGDk utilizes the random start in the beginning of the attack, I think both of the methods can explores the whole search space in \Deta. My major concern is about the margin improvement and the limited compared methods in experiments (see cons below). UPDATEAfter reading all the response and comments from other reviewers, I will keep my scores.<|endoftext|>Summary:The paper introduces a new adversarial training method to improve the performance of Fast adversarial training (FAT), an adversarial training method that depends on only one gradient query. My major concern is that the proposed method is not evaluated on large image dataset, like ImageNet. To improve the performance of FAT and maintain the efficiency, the authors study the characteristics of PGD perturbation and proposes a method to imitate PGD perturbation based on only one gradient query. Some methods perform well on small datasets like CIFAR10 but do not perform well on large datasets. The problem the paper is trying to solve is an important one. Part of my concerns are addressed by the author. However, the efficiency of multi step adversarial training is a problem. ##########################################################################Pros: 1. The difference between PGD generated adversarial examples and FGSM generated ones comes from the perturbations. The proposed method introduces a heuristic way to imitate PGD adversarial perturbation based on one gradient query, which is efficient and novel. 3.Experimental results on CIFAR10 and TinyImageNet show that I PGD AT outperforms several fast adversarial training methods against state of the art attacks. Besides, it does increase the training time much compared to FAT. The concern about lack experiments on ImageNet is hard to address during rebuttal period. It would be better if some details of this process is provided in the appendix as well. Please address and clarify the cons and detailed comments above. I like the idea of imitating the PGD perturbation to improve the performance of one step adversarial training.<|endoftext|>**Few sentences summary**: the paper proposes a single step adversarial training method called I PGD AT to improve over the Fast Adversarial Training method. Based on the fact that not all the perturbations lie on the boundary of the perturbation set when using multiple steps PGD, this method aims at replicating this behaviour while still using only one single adversarial training step. I am vouching for acceptance due to the great experimental results but currently I am not yet satisfied by the theoretical justification of the method as a few statements are not well supported. For the **experiments**:* Evaluation on CIFAR 10 and TinyImageNet against other single step methods: Fast AT, GradAlign and Kim et al.(2021). * Evaluation on the extension of the proposed method to the PGD case. * The paper is well written. I might be wrong for both points but it would require some clarifications. Page 5 and in the appendix: {0 : 1 − p, 2 : 1}  >  {0 : 1 − p, 2 : p}.
Reject; rating score: 3; rating score: 6; rating score: 8; rating score: 8; Unfortunately, the paper lacks some connection to the wider literature and experimental evidence that prevents me from recommending acceptance at this time. In particular:(1) At this point there is a fairly robust literature on balancing weights beyond entropy balancing. The data generating mechanisms are very simple and are unlikely to match what practitioners are likely to see in practice. It would be helpful if the authors addressed this literature and include at least some of them in experimental comparisons. to (a) have a sense of what accounts for performance and (b) have a sense of the limitations of the proposed approach. Kernel Balancing: A flexible non parametric weighting procedure forestimating causal effects. As I detail below, I do not believe that the current paper sufficiently engages with prior art (especially in the experiments, but also in the main text), and as a result it is difficult to properly discern the relative contribution of this work. Kernel based covariate functional balancing forobservational studies.<|endoftext|>The authors propose a way to intelligently choose the base weights in an entropy balancing framework for estimating treatment effects from observational data. Typically, the weights are chosen arbitrarily (e.g.to be uniform) or based off of domain expertise, which is not always available. The theory seems to suggest that I could choose e.g.constant functions and be alright. It would be nice to see the effect of some sort of misspecification. The approach is nice in that it is compatible with other advances in the general entropy balancing technique, such as better choice of basis functions. I think this is an interesting, novel idea with potential for empirical benefits. and theoretically (why is the theory independent of this aspect of the procedure?). 3.Related to the above, can you say anything about your estimand of interest? I believe this would have more to do with entropy balancing than your specific procedure, but it would still be good to mention whether you re typically interested in conditional or average treatment effects. Is there any notion of misspecification of the weights? 5.“We note that the choice of uniform distribution for qis, leads to minimization of the entropy of weights”. Does the variation even mean anything, if all the quantities on the axes are just estimated? 8.In step 5 of the algorithm, do you simply generate error B different times or do you also choose new response functions each time so that a new random response function is chosen for each b   1, ..., B and also for each iteration?<|endoftext|>More specifically, the authors proposed to an end to end approach to learn the “base weight” to be used in EB in contrast to the original, uniform distribution using randomly samples. Thanks for the authors for an interesting work showing that one can improve causal inference by randomly generated outcomes. This is a simple and effective way to improve balancing of weights. Can I just think of it as any proper mapping used before applying \ell? Is w_GSW irrelevant to the random sampling of response function? Meaning that, the analysis is rather about Section 3.1 (existing methods) but not about 3.2 (new algorithmic contribution)? Would you be able to add an experiment EB with base weight as the convex combination of SW and Uniform (alpha SW+ (1 alpha) Unif) for some 0<alpha<1 so that we can better make sense of the learned base weight is better than base weights based on some, simple convex combination of SW and Uniform. I would like to see the result with or without sample splitting. “We can also use generative adversarial networks to generate data that is more similar to our sample.“  Did you try the idea and didn’t it work well or the idea is under development? When I first read through the paper, I thought about GAN could be a better option than random ones. Then, Voila, there is the statement. Overall liked the idea but the novelty can be limited.<|endoftext|>The authors identify that propensity weighting can be unstable in specific instances (it only balances in expectation, and can lead to unstable weights), and that entropy balancing weights do not directly optimize for treatment effect estimation. Addressing those problems, they suggest End to End Balancing (E2B), which does optimize for the subsequent regression step and which provides asymptotically consistent estimators of population weights. Strong points: The paper is technically correct and thorough. The experimental setup is reasonable, including both simulation studies and real data experiments. The ideas are novel and overall well presented. My only suggestion would be to increase discussions of propensity weights and entropy balancing for context; many readers will be familiar with one more than the other, since this paper lies at the intersection of causal inference coming from more economics/mathematical and computer science backgrounds. Methods to learn continuous treatment effects will be of broad interest across causal inference researchers and practitioners, and the authors back up a clever idea with asymptotic theory and empirical validations. I enjoyed reading it and I think many others would too!
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors evaluate the proposal on two algorithms, DQN and R2D2. In the case of multi task, the authors show that incorporating multiple tasks into the retrievable dataset is beneficial. Strenght:* The approach is well written and the paper is pleasant to read. * The experiment s motivations are clearly justified. The paper proposed a retrieval based state augmentation approach for online and offline reinforcement learning. The approach is evaluated in online RL using Atari benchmark and offline RL.<|endoftext|>It is not crystal clear in the paper what is the overall loss term for the retrieval process. The internal state in the retrieval network is partitioned into slots. Each slot independently retrieves information from the trajectory dataset. The retrieved information is used to update the internal state of the retrieval network and used as an additional input of the value function. The retrieval network work is a recurrent model. Also, this paper presents results on grid environments to verify that the retrieval network can be beneficial in offline RL settings. what s the importance of each component in the retrieval process? why not just one slot?<|endoftext|>The retrieval process is trained to retrieve from the dataset based on the current context of the agent and thus make a better prediction of the Q values. The results are strong to demonstrate the utility of the proposed approach. I was not able to find any details related to how these modules were trained and it is an important detail relevant to the approach. 2.Because the main contribution of the paper is an empirical improvement across domains, would the authors release the code or a very detailed pseudocode (similar to the one presented in Schrittwieser et al.2020), which seems necessary for a reimplementation? Intuitively it makes sense that the retrieval process is considering prior experiences to make an accurate estimate of the Q values.<|endoftext|>In this paper, the authors introduce R2A, an agent that is augmented with an attention based retrieval mechanism that augments its state with information directly from an experience replay. Is it not the case that we could be prefilterting out useful state information in the first round of top 10 trajectory information? There should be an ablation with and without this. I would therefore recommend at this point a weak reject, but believe if the authors could address the points made concerning experimental methodology and design choice explanations, I d be willing to increase this score.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper introduces Average Thresholded Confidence (ATC), a method that aims to predict a model s accuracy on an unlabeled test distribution that varies from the original training distribution by some form of distribution shift. Their method ATC selects a threshold t on a scoring function (max confidence, or negative entropy) based on the error in the original source domain. The estimated error is then equal to the number of points in the test set that have a score lower than t. Importantly, this is done without using any label information from the test set. In their experiments, they demonstrate improvements over various baseline methods on a large selection of datasets and distribution shifts. Finally, they have a toy example with very strong assumptions on which ATC MC is a consistent accuracy estimator. The proposed method is simple and easy to understand. I also think that their setting where no labeled data from the test distribution is available is highly relevant. The only thing that is not quite clear is which scoring function should be used in practice, as there exist certain configurations where ATC MC and ATC NE perform the best. They also compare to a regression model that is trained with labeled data and are able to achieve similar performance without this extra information. While temperature rescaling preserves the classification of the model, it can vary the ordering of the scores on different samples, however I would not have expected such large changes, especially in the order of ATC NE pre and post T. Are there specific logit configurations that clearly show how temperature rescaling helps? I liked the initial theoretical discussion of the problem. The paper offers some good theoretical insight into the general problem and introduces a simple method that clearly outperforms the baselines. The problem statement is interesting and very relevant to the community. The evaluation is extensive and clearly demonstrates the strengths of this paper. While the theory does not cover all aspects (eg temperature rescaling), I liked the paper overall.<|endoftext|>The paper proposes a new scoring mechanism, Average Thresholded Confidence (ATC), to estimate out of distribution performance (i.e.accuracy) of a trained classifier using only labelled data from the source distribution (train distribution) and unlabelled data from the target distribution (test distribution). The threshold is based on the logit output of a classifier and can thus be used on unlabelled data. Overall, I find the paper quite interesting and intuitive. The mechanism is simple and seems to be effective compared to prior work. ## Strengths1. an appropriate amount of experiments2. interesting discussion of related work3. relevant theoretical results to motivate the problem## Weaknesses and other Remarks1. However, I don t think they can be considered actual strong contributions of the paper. The main contributions are a heuristic based score and accompanying experimental results showing the effectiveness. So please make sure that this is reflected in the writing. Why are there so many more sampled points around higher accuracies? b.Could you add more discussion around temperature scaling (cf._For all methods, we implement post hoc calibration on validation source data with Temperature Scaling_?) If such an approach improves your performance, could also try improving your performance using more sophisticated calibration methods? That would be interesting to see. I would be happy to engage in discussions during the rebuttal period. The paper is well written and I enjoyed reading it. I cannot 100% verify the novelty of the paper since I am not so familiar with the related work but given what the authors wrote in their related work section and their results it seems novel.<|endoftext|>This paper proposes a simple yet effective technique for estimating a classifier s accuracy on test data which exhibits distribution shift from the training data. Given a score function for the model and a data point, the technique finds a threshold such that such that the relative proportions of validation data above and below the threshold are the same as correctly vs incorrectly classified points, respectively. This threshold is then used to predict the model s accuracy on test data, and the paper demonstrates that this predictor is significantly more accurate than prior methods on a range of distribution shift benchmarks. The paper is of relatively high quality. An extensive number of experiments are carried out on different distribution shift benchmarks, and it seems like the proper prior methods are compared to, though again, I am not an expert. In my view, the paper is primarily an empirical contribution. Section 3 ostensibly says nothing specific about the proposed technique, it just presents some general results that seem relatively obvious (though I am unsure whether they have been explicitly stated in prior work). What are the key insights we can take away from the toy problem? Most concretely, evaluations on the other WILDS datasets could be quite interesting. WILDS also contains language tasks, which would be separately interesting to evaluate on, though I acknowledge this may take more time to set up. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.<|endoftext|>This paper studies how to predict the target domain accuracy using only labeled source data and unlabeled target data. They propose Average Thresholded Confidence (ATC) to learn a threshold on the model s confidence. In particular, ATC predicts accuracy as the fraction of unlabeled examples for which model confidence exceeds the threshold. Extensive experiments have been conducted to show that ATC outperforms prior work across multiple model architectures, types of distribution shifts, and datasets. The problem of predicting accuracy of the models is interesting and novel to me. How to identify the accuracy of a model under distribution shift is a practical challenge. The proposed method of ATC is extremely simple and makes intuitive sense. The claims in the contribution are well supported by theoretical analyses and empirically results. The experimental details are also very specific, such that reproducing the results should be possible. The claims are well supported by theoretical analyses and extensive experimental results.<|endoftext|>This work proposes a simple method, Average Thresholded Confidence (ATC), to predict the OOD accuracy of a classifier based on a labeled source distribution and an unlabeled target distribution. First, the authors show a theoretical lower bound that such task is impossible without assumption on the nature of the shift. Experiments show that ATC empirically performs better than baselines on different datasets and models. Theoretically, they show that ATC is a consistent estimator for a toy setting with spurious features. The experiments are thorough, conducted on a variety of types of shifts and models. The proposed method lacks theoretical rigor. The threshold for source and target would differ based on different types of shifts, and there s no reason why the source threshold should be used to predict target accuracy. The fact that it works empirically is not a good justification. 3.Section 3 doesn t fit with the rest of the paper. Since there s no general purpose estimator, why does this paper still propose such an estimator? The impossible results are not surprising. It is perhaps more meaningful to study principled estimators under specific settings, just like the covariate shift or label shift settings. 5.I wish the authors compare with other calibration methods beyond temperature scaling. The paper is strong empirically but lacks theoretical rigor.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper suggests a framework for designing orthogonal convolutions using paraunitary systems. The proposed methods were examined on several datasets in comparison with state of the art orthogonalization methods. The paper proposes a nice framework which covers different orthonormalization methods for convolutional kernels. In the analyses, the proposed framework performs on par with the state of the art. Second, experimental analyses should be improved with additional analyses on additional datasets and tasks in comparison with the other state of the art methods. 11.In the experiments, the proposed methods perform similar to the state of the art. The paper states that “However, many previous approaches are heuristic, and the orthogonality of convolutional layers is not systematically studied.” However, there is a nice literature on orthogonal CNNs, which explores these CNNs from various aspects including generalization and convergence properties of models, in various tasks including image recognition, speech processing and NLP, including adversarial robustness studied in this paper. However, there are various unclear parts in the paper. Second, the orthogonal convolution proposed in this paper is associated with paraunitary property of the transfer matrix. 4.In the experimental analyses, in most of the results, state of the art outperforms the proposed method, while in some results, the proposed method outperforms them. However, it is not clear how orth.<|endoftext|>This paper proposed a theoretical framework for orthogonal convolutional layers based on the equivalency of orthogonal convolution in the spatial domain and the paraunitary systems in the spectral domain. The layers are also more memory and computationally efficient than most previous methods. The analysis of strided, dilated convolution layers is inspiring. Numerical evidence on orthogonality evaluation of different designs for standard convolution shows that exact orthogonality is achieved. The authors can compare their core idea with related work that is more heuristic, such as [1] which also considers achieving orthogonality in the spectral domain, as well as [2],[3]. Even though the method is more computationally efficient, it is only compared with methods such as Cayley which is known to be computationally heavy.<|endoftext|>This work proposes a new method for orthogonalizing the convolutional layers by exploring the equivalence between spatial orthogonal and spectral paraunitary. The work then empirically demonstrates the effectiveness of the proposed methods by comparing (1) the Lipschitzness (2) the results of adversarial robustness and (3) the time and memory cost among different methods. The major concerns are (1) the work seems to have made some overstatement of the contributions, claiming that all the previous work are heuristic, and the proposed approach is systematic with theoretical justification. The reviewer does not quite buy this point, and better explanation on this is needed; (2) the experimental results are not consistently showing the advantages of the proposed method, also the improvement in terms of computational efficiency seems to be marginal. 2.In the paper, the Q matrix is defined as an orthogonal matrix that is randomly initialized and fixed during training. The reviewer is curious about the results of  $\ell_\infty$ based attacks. (An algorithm for handling different cases would be nice). However, better positioning of the work, and more convincing experimental results are needed.<|endoftext|>This paper presents a method for enforcing strict orthogonality of convolutional layers, by means of a factorization in the spectral domain. Orthogonality is an important problem in the design of neural network architecture that relates to many fundamental properties of the network such as trainability, generalizability and robustness. This paper provides a solid study in this area by providing a method of enforcing orthogonality in convolutions, revealing its technical connections with previous methods, designing a deep residual Lipschitz network architectures and conducting solid experiments. The paper claims that it offers a complete parameterization of orthogonal convolution, but this is not really the case. As stated in Sec.2, it only offers complete design for *separable* orthogonal 2D convolutions. Solid work on orthogonality of convolution, though there seem to be some overclaiming / imprecise statements in the intro/abstract that may be misleading.
Reject; rating score: 6; rating score: 8; rating score: 8; rating score: 8; In this paper, the authors developed an upper bound of adversarial Rademacher complexity, which includes the product of weight norms. This implies that the large weight norm hinders from achieving good generalization performance in adversarial. The authors also empirically show that the product of weight norm in adversarial training is indeed much larger than that in standard training. Strengths: 1. This paper provides new bounds for adversarial Rademacher complexity. The reason is there existing some constants in the bounds in adversarial training and standard training, which may be different. Therefore, simply comparing the product of the weight norm is not rigorous. I would like to see more discussion on this. 2.There exists a product of the weight norm in the proposed bound, which implies that the trained model can generalize better if this product is smaller. The authors are recommended to give such kind of experiments to support their theoretical results.<|endoftext|>The main result of the paper is an upper bound on the Rademacher Complexity of neural networks in the case of adversarial examples (meaning, analyzing it with respect to the robust loss class). The main idea in this paper is to analyze the covering numbers directly (as opposed to calculating them by induction on the layers). The experiments suggest that the change in the margin and the product of the weight norm may explain the larger generalization gap  (compared to the standard generalization). Weakness: showing a depth/width dependent lower bound could have been nice. The analysis goes through the Rademacher complexity as well. In section 3: d^2 should be 2^d? I recommend accepting the paper.<|endoftext|>This paper proposes new generalization bounds for adversarial training in neural networks based on the Rademacher complexity. These are more general than previous results in that they apply to neural networks of any depth. Lacking this, it is harder to judge the improvement the current paper makes over prior results in terms of tightness of the bound. Moreover, it might be worth comparing the contribution to other types of theoretical approaches in the field, e.g., the provable methods that the paper cites. Questions and other comments:  It would be good to underline that the bounds provided also hold for convolutional neural networks earlier than the experiments section (or, more generally, what layers are covered). Is it reasonable to consider a loss in the range 0 1 for neural networks? Are Rademacher complexity bounds tight enough for neural networks to be informative or applicable in practice? Good theoretical result supported by experimental evaluation on relevant topic.<|endoftext|>(1) For the first contribution, is there any interesting findings in the bound? This paper provides both the lower bound and the upper bound. Besides, this paper conduct numerical experiments to combine with the theoretical bound to justify why adversarial training has a worse generalization than standard training. I think the main contribution of this paper is interesting. It provides both the upper bound and lower bound, both of which are important. But there are many issues towards the experiments, proofs, and the writing of this paper. However, there are still a lot of things that can be improved in this paper. [1] My understanding towards the adversarial training and the adversarial Rademacher complexity is that, both adversarial trained and standard trained neural networks have their own standard Rademacher complexity and adversarial Rademacher complexity. So "the generalization gap of adversarial training loss and adversarial testing loss" is larger than "the generalization gap of standard training loss and standard testing loss" for both adversarial trained (denote as A and B) and standard trained neural networks (denote as C and D), i.e.A>B and C>D. On the other hand, through experiments, it is observed that the adversarially trained neural networks obtains larger norms, so its generalization is worse, i.e.A>D.Is my understanding correct? In addition, could you provide some insights on why the norm of adversarially trained model is larger? [1](as important as the above) The current proof of Theorem 3 only says "By the results of the lower bounds of ..., we obtain that ...". Please provide more details on the existing results and how to obtain the final result. [4] The main idea of this paper is not hard to follow, and the authors make a lot of comparison to existing literature. Which steps do they skip?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 3; The advantages of this paper are as followings:1. It seems not very clear to me that how is genetic algorithm integrated into the process of molecular optimization and synthesis planning. 2.It might be better to compare the recovery rate(shown in table 1) of the proposed method with other synthesis planning methods(i.e., retrosynthe  sis). 3.It might be valuable to analyse more about why the proposed method is more computationally efficient. The font size of table 6 is too large. Overall, the paper is well written, and the method appears to be promising.<|endoftext|>[EDIT: as reviewer "vmGs" pointed out, this idea is very closely related to the NeurIPS2020 paper by Bradshaw et al, which I didn t realize at the time of writing the rest of this review. I will reassessthis work after any revisions, and have temporarily reducedmy previously enthusiastic score. The results of the baseline models in Figure 5 and Figure 11 are atbest embarrassing for those other codes, and at worst somemisconfiguration of those codes. I imagine that some ofthese problems are easy to fix with a couple additional instructionsand it is possible that the rest could be resolved with a couple extrascripts and some minor modifications. It is a strong, foundationalpaper that is well written. Until the authors address this concern and clarify their additional important contributions,I do not recommend publication of this paper.]<|endoftext|>## Strengths:  interesting extension of the work by Bradshaw, Gottipatti and Horwood, which addresses some of the limitations of these models. In the way the contributions of this paper here are presented, it leads to the impression that they were actually invented by the authors, which is not the case. It is a significant step forward for the field, with a modeling approach interesting from the ML side as well as non trivial, domain relevant empirical validation (which many other papers on the topic at ML conferences do not have). Building on other people s work is an essential part of science, but it needs to be acknowledged accordingly, and there is no harm in doing so.<|endoftext|>The present paper is concerned about a forward synthesis approach to molecular optimization. The second one examines the ability of molecular optimization. ## Weaknesses###   The relationship between the proposed method and existing forward synthesis based methods is not clearI am not very satisfied with the discussion on the relationship to the existing methods. I suggest to reject this paper and encourage the authors to revise it for next opportunities. While I consider the method proposed in this paper is reasonable, the relationship to the existing work is not discussed enough, and therefore, I consider the paper is not very mature to be published as an academic paper. I would suggest the authors to revise the paper so as to clarify the difference from the existing work and how much the difference contributes to improve the performance.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This work proposes a quasi Newton method APOLLO for nonconvex stochastic optimization. Based on a parameter wise weak secant condition, a diagonal approximation of the Hessian is constructed, and rectified absolute value is adopted on the approximation. How strong are these assumptions? 3.In the experiments, it is claimed that the proposed method outperforms the compared first order methods on convergence speed. Though there is certain novelty in this work, the theoretical contributions seem to be flawed, and the numerical results are not convincing, as explained in the main review.<|endoftext|>The paper presents a diagonal quasi Newton method, by approximating the Hessian with a diagonal matrix. The paper proves a regret bound for the method in the convex case, and shows that in the non convex setting the expected norm of the gradients goes to 0. The paper presents a diagonal approximation to the Hessian. The experiments show improvements on several datasets. Furthermore, given that the current method is estimating the Hessian, it should be compared with the second order methods such as  Shampoo (Anil et al), as these methods are able to outperform SGD and Adam on large problems. The paper presents an optimizer by combining several ideas from previous papers, and shows small improvements on some datasets.<|endoftext|>The paper could be a good contribution to the literature (as well as for practice), however I have several concerns currently holding me back, which I hope the authors can address in the rebuttal (they are all related to the parameter wise update regime enforced by the algorithm; please see my comments in the main review section) The paper developed a new quasi Newton algorithm for stochastic non convex optimization. I m not able to judge the technical correctness of the statements in Theorems 1,2,3 from the main paper; One question I have is about the parameter wise update of the gradient/Hessian; this results in a different type of algorithm than SGD variants; While I m not a hundred percent sure, I believe this is also not the case for Adam/Ada grad type algorithms. In contrast to existing works, it uses a (rectified/capped) diagonal matrix to approximate the Hessian, and incorporated techniques to reduce the stochastic variance.<|endoftext|>This paper presents a first order quasi Newton method, named APOLLO,for solving stochastic nonconvex finite sum problems with a largenumber of data points. Each iteration of the method consists of computinga sparse and positive definite (diagonal) approximation of the objectivefunction s Hessian, followed a quasi Newton step. It should also be expected that the produced convergence ratebe similar to other quasi Newton convergence rates for the convex setting. (2018).Accelerated methods for nonconvex optimization. A few gaps in the presentationof the theoretical contributions, however, prevent it from receivinga stronger score.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper presents a model tuning strategy for steering conditional music generation given the pre trained Music Transformer model. Steering the generation results to the direction intended by the user has been emphasized as a lesson of recent AI song contest events. The authors address the issue focusing on continued generation given a prime. In other words, the listening test does not reflect the control by the musician subjects. However, the presentation and results are not clear to me in term of validating steerability of the model. While the demo examples contrasts the superiority of the proposed approach well, I have a bit doubt about the results. But, it seems that the non contrastive loss is not clearly defined in the paper. Is it the loss that does not include the negative case (this is my guessing)?<|endoftext|>This work considers how to control sampling from transformer based autoregressive generative models of music. But there does not seem to be an empirical analysis of the additive approximation proposed in Section 3. In particular, it considers constraint satisfaction problems (CSP) given a collection of binary constraints. The contrastive loss (Section 3.1) and additive model for feature composition (Section 3.2) are clearly described and interesting. I am a little confused by Figure 3: it appears from this figure that prefix tuning and bias tuning are used together, but later in the paper it seems that these two techniques are analyzed separately? I found the discussion of the empirical evaluation (Sections 4, 5, and 6) very confusing. Below are some specific questions and concerns. Which one? What is the distinction made between the "maestro validation dataset" and the "actual validation dataset"? How many samples are used to calculate efficacy? (6) How should I interpret the results of the listening test in Section 6. Overall, regarding the experiments.<|endoftext|>The authors propose to investigate two methods (prefix tuning and bias tuning) to steer a pretrained music transformer using a set of manually chosen musical features. There are interesting aspects in this paper (especially the comparison between bias tuning and prefix tuning in the compositional setting) but the overall contribution is not very substantial (only Eq.5) and its effects not extensively discussed. This paper introduces a novel objective to finetune a model on several features (the contrastive loss of Eq.5) and compares two existing methods (prefix tuning and bias tuning) to adapt such a pretrained model so that it generates sequences featuring some user defined musical features. A may be relevant for composers.<|endoftext|>This paper describes a method for adapting the parameters of a symbolic music generation model such that generated continuations of a prefix are more likely to satisfy a set of user specified constraints. (A prefix tuning method is also investigated, but consistently outperformed by the bias tuning method.) A human subject listening test demonstrates that the proposed method is more likely to generate "more musical" continuations than the unconditional model. The results seem good, although I m not sure that I fully understand what is being presented. Putting a bit more explanatory text into the captions and surrounding text would significantly improve this paper. Specifically:  Figure 4 is described in a one sentence subsection (5.1), which does not at all explain what is being measured in the figure or how to interpret the plots. Table 1 lists what appears to be mean efficacy scores, as well as some kind of bounds in parentheses?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposed a new client reweighting algorithm for selfish federated learning where the goal is to optimize the performance of a few internal clients not all clients. For strengths: this paper studied a new selfish federated learning setting (i.e., the objective is the loss on a small (internal) subset clients instead of on all clients) and propose a VaRSeL algorithm for solving them. For weaknesses:   Regarding the selfish FL model in Table 1. ** Note that in Section 3 (Problem formulation), the authors even assume that **$M\ll N$**.<|endoftext|>There are several issues and inconsistencies in theoretical arguments. The paper provides some theory for methods where the optimal weights are known and proposes a heuristic method VaRSeL to estimate the optimal weights on the fly. **Internal clients  distribution. ** The motivation of the proposed new setting *Selfish Federated Learning* is not well substantiated.<|endoftext|>This work studies a variant of the federated learning (FL) problem where the stakeholder (i.e.central planner) only cares about the performance of a trained model over a small proportion of local (internal) clients. The proof techniques in the paper for convergence results are rather standard and, to the best of my knowledge, have been introduced in previous works. The main weakness, from my opinion, is that the methodological and technical contributions seem insignificant.<|endoftext|>This paper proposes selfish federated learning, an FL framework that optimizes the performance of a small subset of clients. Weaknesses and questions:   The idea of only optimizing the performance of a subset of clients is not really new.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This is a solid paper that proposed a promising approach to MARL. So I cannot comment on the correctness and I wonder whether such an extensive material is really essential, although as part of the support material does not affect the fruition of the main paper. The paper claims that the proposed algorithm helps with exploration as well as preservation of known policies. Many details in the paper require extensive examination, care, and previous knowledge.<|endoftext|>The exploration reward is factorized into an on/off gate (dubbed ‘switching control’) and a scale function. Have you experimented with a continuous gate, or no gate at all? More generally, what are the assumptions that make the switching work? If the presentation is improved significantly, I would be open to recommending a weak accept. I would strongly recommend to the authors to condense the presentation of the method in the main text and spend more time on evaluating the method. The paper s contribution could be more significant if a bit more time/space was spent on the results (this should not be a problem if the presentation of the method is condensed).<|endoftext|>The paper focuses on learning intrinsic rewards for multi agent reinforcement learning, which is an important problem. For example, the authors hold that "CT DE methods are however, prone to convergence to suboptimal joint policies (Mahajan et al., 2019). The reviewer recommend rewriting the third paragraph of the Introduction section to better motivate the proposed methodDiscussion of related works should be improved. ***Evaluation***The reviewer was expecting an ablation study regarding the exploration bonus. However, the empirical evaluation, the discussion of related works, and the presentation of the paper can be further improved.<|endoftext|>Experimental results on several domains show its advantages over several MARL methods. From the perspective of learning intrinsic rewards and preserving the optimality, instead of hand craft intrinsic rewards, it is a worthwhile investigation direction. If all these concerns are addressed well by the authors, I am willing to increase the score. The first two points can be concluded into one point. Second, in common, each agent should have its own reward function R_i(o_i,a_i), not from the global reward function. Especially, this paper assumes some setting that each agent’s reward is non monotonic to the team reward. Therefore, it is not suitable to obtain an individual reward from the global reward function.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This paper attempts to explain the behaviour of multi headed self attention in vision transformers using a series of empirical observation. I overall think that this paper shows some interesting phenomena, even though many of the observations are post hoc and don t seem to lead to immediate actionable design rules. The paper states "... does not improve performance sufficiently ...": on what dataset is this measured? Would some of the insights change with larger models?<|endoftext|>The paper shows experimentally that contrary to popular belief ViTs do not directly benefit from their more expressive underlying representation, but instead smooth the loss landscape which aids training. The paper then presents a hybrid CNN ViT architecture that combines some of the benefits of ViTs with advantages of CNNs. There is currently a bit of repetition within section 1 when talking about points (1) (3) multiple times. I like this paper, and the insights it provides in terms of optimization landscapes of ViT vs CNN architectures. It also provides a pretty clear route towards better ViT or CNN architectures by combining the strengths of both.<|endoftext|>Paper presents empirical analysis Multi headed Self Attention (MSA) as part of Vision Transformer (ViT) and its variants. Based on these observations, paper proposes a new architecture   AlterNet, where CNN block is combined with MSA block and this "base" structure is replicated to produce deeper models. The paper is just very dense. Overall, the paper is empirically addressing a series of important questions about the (relatively) new class of architectures (ViT). Empirical evaluations point to some interesting insights that are ultimately leveraged to build a new form of the model which is a (relatively simple) combination of the MSA and CNN. Despite these issues with exposition, which I think would benefit from fixing, I think the paper does address an important topic and drawn insights can be broadly useful. For these reasons I am in favor of acceptance.<|endoftext|>The authors show that the robustness and better performance of ViTs is attributed to their property of flattening the loss surface. The paper shows that the properties of spatial smoothing are exhibited by MSA as well. The paper interprets deep models as a series of independent blocks, and then proposes a new architecture, in which the last conversation block is replaced with multi head self attention. While the paper makes a good effort in explaining how ViTs work. I guess, the paper draws analogy between MSA and spatial smoothing, and then builds most of its argument around that. Most have multi scale feature hierarchies.
Reject; rating score: 5; rating score: 5; rating score: 6; Authors proposed to use soft Q learning (SQL) to formulate the RL problem in sequence generation. The stability can be increased when techniques of path consistency learning is corporated. 1.I think when mentioning text generation, people will recall language modeling or machine translation. This paper only did experiments on some tasks which I don t think it s central to the topic. It reads to me that the major contribution is trying some existing technical methods in these sort of problems. However, in SQL the major concept to me is the entropy term. I think if SQL is the reason, this should also be verified. Overall, I have the experience in using RL for machine translation so I agree with the authors that those problems mentioned in the paper are indeed challenging. Since most if not all technical stuff are existing work, I think the novelty is limited and thus justification of each component in the method is important to me.<|endoftext|>This paper proposes a new text generation framework based on the existing soft Q learning of reinforcement learning. Weak Points:  The novelty of this paper seems limited as the techniques used in this paper are directly from the reinforcement learning area (soft Q learning and path consistency learning). However, this problem had been clearly pointed by the previous text generation methods, e.g.LeakGAN [3] and IRLGAN [4]. However, the paper does not compare with them and even does not mention them. This is limiting. The paper shows the generation results of the prompt generation which is an interesting application of text generation. In addition to the prompts results, It would be better to show generated examples of other tasks.<|endoftext|>To address these problems, this paper adapts the path consistency learning approach to the text generation setting. Strength: This paper establishes a connection between the text generation problem and path consistent learning. Weakness: The work is almost a direct application of PCL to text generation, and there is no newly developed RL algorithm for text generation. The experiments on standard MLE based tasks are relatively small scale. This paper establishes a connection between text generation and path consistent learning, which leads to an elegant RL based text generation algorithm that inherits many advantages from PCL algorithm. The performance of the algorithm is encouraging for RL based text generation, although having more large scale experiments would be more convincing.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; While trying to obtain a prior for which tasks of a given winning ticket can be transferrable using the theory from statistical physics seems interesting, the paper doesn t provide enough experimental results to show how to use such an explanation to improve iterative magnitude pruning or determine the best architecture that can be transferred for different tasks. The work is more like working in the progress report, and more results can help strengthen the work. The authors provide some experimental results/conclusions and try to explain them using the renormalization group. The paper attempts to build a theoretical basis for understanding the iterative magnitude pruning, which is widely used in works related to the lottery ticket hypothesis.<|endoftext|>This paper seeks to explain empirical observations in the literature on the Lottery Ticket Hypothesis (LTH) based on ideas from renormalization group (RG) theory in physics. The authors focus on the particular case of iterative magnitude pruning (IMP) as the pruning scheme and view the flow on the space of model parameters during IMP as analogous to RG flow in the space of couplings of a Hamiltonian. (2) Transferability of lottery tickets. Strengths:The paper tries to tackle an interesting and important question on the understanding of LTH ticket properties. Weaknesses:While I appreciate the high level conceptual connection between RG and IMP, I am not fully convinced that the connection is really a precise one, or that it provides strong quantitative "explainability" for LTH phenomenon (that is, RG can be used to explain things that a simpler hypothesis can not). On the other hand, I appreciate that there is some preliminary evidence for this in the paper. In Sec.3.2, in discussion on how the smallest ResNet has the weakest transferability: a simpler explanation could be that a smaller model is not expressive enough to learn a good representation useful for more expressive architectures. Where it says "...which does not match the source tickets  structure", should it read "target" tickets ?<|endoftext|>This paper tries to find a theoretical explanation of the transferability of lottery ticket used in similar tasks. Observing the similarity between the universality in renormalization group and the lottery ticket hypothesis, the author proposes that the iterative magnitude pruning, which is used to find the winning tickets, could be a renormalization group scheme. The authors also provide some evidence on their theory on vision model of ResNet families.<|endoftext|>In this paper, authors make use of renormalization group theory to perform detailed understanding of this hypothesis. Authors find that the principle method iterative magnitude pruning (IMP) is directly related to the renormalization group (RG). Such insight is build upon the experimental support of the theory in large scale lottery ticket experiments. + It provides a theoretical basis for understanding the success of IMP and the universality of winning tickets. Concerns and questions:  Authors claim that RG theory can provide a way to predict which combinations of task, optimizer, activation function, and architecture can have winning tickets transferred between them.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper studies the induced bias of gradient descent on L layer linear group equivariant convolutional neural networks. Their main result is: gradient descent implicitly controls the 2/L Schatten norm of the Fourier transform of the (linear) predictor. More precisely, they show that gradient descent trained on a L layer group equivariant CNN with exponential loss on a linearly separable dataset converges in direction to a first order stationary point of an optimization problem minimizing the 2/L Schatten norm of the Fourier transform of the linear predictor subject to the margin being at least 1. From a technical perspective, the results for abelian groups (as the authors note) follow from appropriately applying the result of Yun et al.(2020).(This is because convolutions in Fourier space can be expressed as pointwise multiplication for abelian groups.) The more interesting technical contribution is for non abelian groups. In this case, convolutions corresponds to matrix multiplications of matrices which need not be diagonal, and thus do not correspond to point wise multiplication. An appealing aspect of the paper is to highlight that the norm that is implicitly controlled by gradient descent on group equivariant networks takes a simple form, even for general (finite) groups. This technical point is made clearly in the paper, and the paper is generally well written. One weakness of the paper is that the it does not explicitly connect the theoretical results with practical settings. First, since group equivariant convolutional neural networks are not a standard architecture, it could be helpful if the authors added a discussion of what group structures have been used / are meaningful in practice, and provided insight (e.g.in the empirical section) into the implicit regularization of gradient descent in these special cases. In light of previous work (i.e.Yun et al.(2020, Gunasekar et al.(2018)), the results are somewhat incremental. However, given the results in previous work (e.g.Gunasekar et al.18, etc) for the trivial group, I still think that the results in this work are fairly restrictive since they only hold for single channel linear networks with full dimensional filters.<|endoftext|>Because of the explicit inductive bias of G CNN, other inductive biases have not been discussed much. Technically, they present the non explicit bias in terms of Fourier matrices by using a group Fourier transform, which depends on the group structure. The results show that learning a linear G CNN (with linearized β) by gradient descent implicitly biases the singular values of the Fourier matrix coefficients of β to be sparse. Weaknesses: I do not see anything special about it. Questions:From a theoretical point of view, we would like this kind of result to include the case where the group does not do group convolution as a case where the group is trivial. What does Theorem 1 imply when the group is trivial?\It would be great if this kind of result could provide some insight in analyzing over parametrized FNN. When we regress the G equivariant function on an over parametrized FNN, can your results give any insight?\This result was for finite groups, but there are also convolutions for geometric groups, such as Lie conv. Please tell us about the part of this result where the finite group assumption works and give us some insight into the generalization to Lie groups. After the revision, the paper was improved, for example by generalization to the Lie group, but in the main the assessment did not change significantly, resulting in the following scoresBasically, this paper is theoretically well done. Overall, I think this is a worthwhile paper.<|endoftext|>The results show that for linear G CNNs trained on linearly separable data using GD converge to sparse solutions in the Fourier domain (equivalently dense solutions in the real domain). The theory is theoretically confirmed and shows that the implicit bias also occurs to some extend for non linear G CNNs. Strengths:As far as I can assess, the paper is technically sound, novel and timely. The appendix is quite nice actually and is rather complete in mathematical background. Weaknesses:My main criticism is that the paper is very theoretical and feels a bit like a nice math exercise to prove something, however, without being clear why the problem is addressed in the first place (apart from the fact that no one did it yet). To me, as someone working with G CNNs and having a good understanding of (irreducible) representation theory, but not in analysis, it is unclear what I should take home from the paper. I.e., why should we expect different results for non Abelian compared to Abelian groups in the first place? Comments:It would be helpful to have explicit statements about the main contributions/novelty of the paper. The experiments in the figure make sense to me in the comparison G CNN vs FC. However, I do not understand the comparison CNN to G CNN. If the feature maps are functions on G, is a CNN not automatically a G CNN? Some essential details are missing regarding the data. I cannot find it in the appendix either. The closest I get to a description is in caption of Figure 3: “trained … with six isotropic Gaussian data points”. What is an isotropic Gaussian data point? (w.r.t.indexing to l should be l+1?, it says something like Fw_l Fw_l \geq \lambda)Although I am comfortable with group theory and basic harmonic analysis on groups through irreps, I find it hard to follow the derivations and make sense out of the statements. The paper could improve a lot from more layman s explanations throughout the paper. For example, it already starts early in the paper with talk about Schatten norms, where to me it is not immediately obvious why these are considered.<|endoftext|>This paper generalizes these results from linear CNNs to linear G CNNs. The authors show that the implicit bias is to minimize the p norm of the singular values (the Schatten norm) of the Fourier transform of the resulting linear transformation. In the case where G is Abelian, by a tensor factorization of the linear G CNN, the authors can straightforwardly apply a result from prior work on the implicit bias of such tensor factorizations to prove their result. In the case where G is not Abelian, the authors prove their results in three steps. (1) They use prior work on gradient descent on linear predictors that are polynomials of the parameters, which their linear G CNN falls under, to show that the stationary points minimize norm the joint norm on all weights. (2) They define a linear regression optimization that minimizes the Schatten norm of the linear predictor. (3) They show that all optima of the polynomial linear predictor are also optima of the Schatten normed linear regression. To do so, they find the subdifferential of the p Schatten norm and show that the polynomial optimum is in the subdifferential. For the Abelian case, this required an incremental generalization of prior work, but for the non Abelian case required an interesting proof method. The paper is very clearly written and was a pleasure to read. My main problem with this work is that it considers a model class that I’ve never seen used (besides the lack of non linearities): it uses group equivariant CNNs and then takes an inner product with a G feature weight, resulting in a non equivariant linear predictor. As G CNNs are chosen for their symmetry properties, studying them in a non equivariant context makes little sense to me. Whether the implicit bias found in this paper generalizes to equivariant non linear G CNN networks has not been convincingly shown. Other weaknesses:* The non Abelian case states two assumptions (gradients converge in direction, iterates converge in direction to a classifier with positive margin), but these are not clearly defined, nor is it stated when these are satisfied. The paper would be improved if there would be some discussion of these assumptions. * The key point of the paper seems to be that Fourier Schatten norm, not spatial norm is the implicit bias of the learned networks. The paper would be stronger if it would give some more intuition about this point. Perhaps visualize some of the learned weights in both Fourier and spatial domain? For the paper to be recommendable for acceptance, the authors should make a stronger argument why their results also apply to models actually used.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; POETREE aims to construct an interpretable model for a policy over a time series using decision trees. **Strong points:**  Rather well written and clear paper. The interpretability of the proposed approached is quantified by clinicians. Interpretability of models is important, especially in healthcare. Nevertheless, it is impressive you could learn such a tree. I liked the illustration of the model based on real data/examples. This is really nice. Experiments are well done. I think the paper is worth publishing.<|endoftext|>This paper proposes a novel approach for learning and representing human decision making policies from observed behavioral data. This is all fine, reviewers can have disagreements. I am admittedly not an expert in imitation learning/behavioral cloning, policy learning, or soft/probabilistic decision tree models. The central premise of the paper   distilling inherent clinician policies down to an easily interpretable (and easily followable) decision tree model   seems like a compelling and important task, and the authors motivate this well in their introduction, highlighting the unnecessary costs of medical practice variability. I was impressed by the experimental results.<|endoftext|>This paper proposes a new method to learn (stationary) interpretable policies using soft decision trees in partially observed settings. An algorithm is presented to optimize the parameters of the soft decision tree as well as the structure/topology of the tree. Overall I believe this paper provides an interesting contribution for interpretable policy learning particularly for clinical decision making. The paper is well written and the problem well motivated.<|endoftext|>Thus, this paper proposed a (soft) tree based method for synthetic clinical datasets in the matter of interpretability. The authors model the clinical decision process as a partially observable Markov Decision Process (POMDP), which naturally fits the assumption of medical diagnosis. The policy learning methods(IL, AL, IRL) are not related to me as the reward function is not important in clinical settings.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; Recent successful model based offline RL techniques have relied on heuristics to penalize rewards according to the uncertainty of the estimated MDP. This paper reviews the different penalties that have been designed in the literature. The impact and importance of associated hyperparameters such as the planning horizon and the number of models in the ensemble are also evaluated. I like the paper and the work done by the authors. I think it is valuable for the community (researchers and practitioners). I would just like to have some clarifications about the points I raised in my review. (see detailed comments below). for such an empirical work error bars are needed in Table 1, 2, 3, 4 to judge the statistical significance of the results. What about existing calibration metrids such as the expected calibration error (ECE)? What s the environment? Is the error and uncertainty the ones for the reward or another dynamics variable (or the mean error over all the dynamics state variables)    * I would especially like the authors to clarify whether the errors are computed step by step comparing the outputs f(s,a) (obtained with the true dynamics) and g(s,a) (obtained with the model) assuming same inputs (s,a) or for multi step predictions i.e.considering a fixed sequence of actions and a first initial state s_0 look at the error between each state of the sequence (s_1, ..., s_t) predicted by a rollout with the model and its corresponding state in the sequence (s _1, ..., s _t) obtained for the same action sequence with the true dynamics when starting from the same initial state s_0. What is the difference between figure 26 and figure 27: Figure 26 is just the median overall several rollouts whereas figure 26 shows individual rollout? * I think this is a bit more clear in appendix D, but I would still like the authors to make this clear in their replies and to make the effort of making this more clear in the main paper as well. Can the authors confirm that the hyperparameter optimization is done by evaluating the performance of the different hyperparameters on the real environment? I would not let this influence my review because I think this is the current practice for most papers in RL. equation (1)  > I think a capital "R" should be used for the reward function. I would expect better results by considering epistemic uncertainty which is what is attempted when using the ensembles.<|endoftext|>The paper provides an evaluation of many of the design choices and hyperparameter decisions made in offline model based reinforcement learning methods which have emerged recently. Particularly, the empirical study looks at uncertainty penalties used in these methods, as well as hyperparameters such as ensemble size, penalty weighting, and rollout horizon. The authors find that offline MBRL methods are quite sensitive to each of these parameters. They compare the “optimized” version of MOPO with hyperparameters tuned using Bayesian optimization, and find that it leads to statistically significant performance improvements over the version of MOPO presented in the original paper. I feel that this is a valuable contribution to research in offline RL, since it’s generally quite difficult to understand where performance improvements between various methods come from due to so many differences in hyperparameters. The comparison metrics for the uncertainty penalty are quite insightful and reasonable. It seems important to this evaluation to also determine how stable these hyperparameter choices are across random seeds. Additionally, in Figure 1, “Optimized” has not yet been introduced, so while in hindsight it is clear that it’s an optimized version of MOPO, it may not be immediately obvious. Figure 1(b) and the methodology used in this claim is not clearly described in the textI recommend accepting this paper.<|endoftext|>Model based offline reinforcement learning algorithms typically involve constructing a pessimistic MDP, which is implemented based on an uncertainty estimation of the learned model. This paper conducts empirical analysis to compare different design choices of the uncertainty estimation in practice. In more details, the authors compare different approaches in terms of the correlation between the estimated uncertainty and ground truth model error. They also use bayesian optimization to search the best hyperparameter configuration that achieves strong empirical performance. To this end, this paper fills an important gap in the literature. The paper is also well written. The experiment setups are clearly defined. However, my major concern is that the technical contribution of this paper is not strong enough. Other than designing several interesting experiments to compare model uncertainty estimation methods, the paper does not contribute enough new insights that can potentially be interesting for a broader research community. This paper also compare several uncertainty estimation methods in model based RL, including the ensemble based approaches (Lakshminarayanan et al., 2017). The main conclusion is very similar: a combination of epistemic and aleatoric uncertainty is the best choice. I recommend rejecting this paper because the technical contribution is not strong enough.<|endoftext|>The authors present an empirical study of several uncertainty quantification heuristics applied to model learning in offline model based reinforcement learning. Specifically, they consider the basic architecture of MOPO, in which an uncertainty based state action penalty function is applied on top of the standard reward to construct a pessimistic MDP. They present these results showing that the optimal choice of penalty and penalty weight can vary significantly, not only between environments, but also within an environment as a function of offline dataset. The wide scope of these experiments, evaluating the uncertainty penalties in relation to both model prediction error and downstream MBRL performance is a strength of the approach. My main concern with this work is that does not make any actionable claims. To me, this is the equivalent of "training on the test set," in that it uses interaction with the true system to select hyperparameters for the same system. In that sense, the result can no longer be called offline RL. Of course, hyperparameter selection is an under addressed topic in the community, and there are many papers which manually select hyperparameters that yield good performance. The work would be significantly improved if the authors used the results of their empirical experimentation to propose a general recommendation for penalty selection and hyperparameter selection. In stochastic environments, I don t see why aleatoric uncertainty should factor into a reward penalty, so the ensemble variance and standard deviation penalties proposed may not work well in such environments. The claim that the ensemble standard deviation or variance is "arguably the most principled" choice is not supported. What makes it "more closely aligned with the theory"? The AUC / AP are hard to compare across percentile choices because the relative proportion of positive and negative samples is also changing, so the AUC and AP of a random classifier changes across this task. As such, I m hesitant to recommend acceptance. If the manuscript is updated to have clearer empirical evidence for the main claims in the paper, including a fair evaluation of the recommended strategy across a variety of domains without optimizing hyperparameters separately for each environment, I would increase my score.<|endoftext|>The paper provides detailed analysis of different uncertainty quantifications in Model based offline RL, from both statistical and empirical perspectives. I think the research is well motivated and interesting, while I have some concerns about the experiments. The gap between the true model and the learned model is estimated by uncertainty measurements. Nevertheless, in the MOPO and Morel papers, different uncertainties are not rigorously studied. I think this problem is important and should be studied in depth. 2.The use of Spearman rank and Pearson bivariate to identify the correlation between uncertainty and true MSE is novel. The experiments verify such measurements are closely related to the performance. The trajectories generated by such policies may lie in a small range of state action space. 2.The aim of section 5.3 is to study the effect of rollout horizon. It is unclear how the results presented in table 2 relate back to the rollout horizon, since I do not find the rollout horizon is used as a variable in Table 2. The dynamics discrepancy should be related to the horizon length. Can you explain more? 5.The author should explain why “Ensemble Standard Deviation” measures both the epistemic uncertainty and aleatory uncertainty. Can we draw the conclusion that both the aleatoric and epistemic are important to offline RL? Although it has been discussed later in the paper. 2.At the end of section 5.1, the paper writes “This again suggests that the number of models not only affects the quality of the estimation, but also its distributional shape. (1) The dataset used in evaluation should be unified. (3) Additional ablation or explanation of Table 3 is suggested to be added. (4) Some discussion about the epistemic and aleatoric uncertainty.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; For this reason, I will give a marginal acceptance. Moreover, I would love to see some sort of empirical evidence that taking these approximations is okay i.e.comparing their KF to an extended KF. The approach is elegant and the empirical results are compelling. # WeaknessesI think there are very important details that are swept under the rug.<|endoftext|>The authors demonstrate the performance of the proposed method with a series of experiments on robot learning. Applications on different robot learning problems and settings are evaluated. It is unclear what the concept of latent task refers to (could the authors provide some examples?) and why multi task models are suitable to compare as baselines. Similarly, it would be suggested to elaborate more model details about, e.g., locally linear transition. However, some necessary clarifications and important baselines are missing.<|endoftext|>The paper describes the approach reasonably well (up to the points below). Please see below for details. As an aside, I suggest that B.5 be brought forward into the main text since it is a core part of the method. Can the authors better motivate the need to introduce the latent variable in this manner? It would be helpful if the authors clarify differences to existing models, especially KVAE and switching state space models.<|endoftext|>The paper proposes a method to learn a probabilistic recurrent state space model for time varying dynamics. I do not think this is true. As such it contradicts the main claim. Notice that this is not a fair comparison with the baseline methods. For example, even the basic GRU and LSTM models are more general as they can consider the latent variable to be a part of state space that is evolving with time.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Based on the proposed causal graph, the authors identify the origin of adversarial vulnerability as the spurious correlation between style variable and class label. In this light, the authors propose a method to align the adversarial distribution and the natural distribution to prevent a model from learning spurious correlation. The proposed method is empirically validated on prevailing datasets under several attacks. 2.The proposed causal graph well models the generation process of adversarial attacks and sheds new light on how to understand and defend against attacks from a causal perspective. 4.If $\hat{s}(X)$ is the integrated representation of both $s$ and $x$, as mentioned in the paper, are $\hat{c}(X)$ and $\hat{s}(X)$ supposed to be independent when $x$ is given?<|endoftext|>It first constructs a causal graph, which then inspires the design of the distribution alignment method for reducing the gap between adversarial and natural data. Extensive experiments on CIFAR10, CIFAR100, and MNIST demonstrate the robustness of the proposed method against various attack methods. * The distribution alignment method inspired by theory shows good robustnessWeakness:* I noticed the Natural accuracy on CIFAR 10 and CIFAR 100 can sometimes be worse than TRADES and Madry. Are they consistent among different datasets? This new perspective is novel to me and I would tend to accept this paper.<|endoftext|>The paper shows a causal perspective to the adversarial robustness problem. It identifies the spurious correlation between style and label as the main reason for adversarial examples, and then proposes a method to remove it from the trained model. Experiments on three datasets show that the proposed method is better than two baselines. I like the simple separation of content and style variables that allows this characterization.<|endoftext|>The work presents a causal perspective of adversarial attacks on image based machine learning models by studying a causal graph of the adversarial data creation process and highlighting how such a process makes the learned models vulnerable. It argues that the main reason for adversarial vulnerability is the reliance of models on spurious correlations between labels and style. Accordingly, it proposes a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Empirically, the method is shown to be more robust than two baselines on three datasets. It shows improvement on MNIST, CIFAR 10, and CIFAR 100 datasets. Although non causal in motivation, other robust learning work has also identified spurious correlation between labels and style features as reason for lack of generalizability. The paper undertakes an original approach to studying the important problem of adversarial vulnerability.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; It proposes to extend Mixap data augmentation from calssification to metrics learning. The authors provie extensive experiments that show the effectiveness of their techqniue (Table 2)  on 4 different dataset (Table 4) Pros:1) This is a good paper which has techincal novelty    generalized loss for metrics learning, which allows to incorporate mixap augmentation. 2) The paper introduces new evaluation metric, utilization, validating that a representation more appropriate for test classes is implicitly learned during exploration of the embedding space in the presence of mixup. What is not clear to me is if this metrics is measured on the classes that particiapte in training (After eq.12    Utilization measures the average, over the test set Q, of the minimum distance of a query q to a training example x ∈ X in the embedding space of the trained model f (lower is better)). I recommend to accept this paper because I think that the paper is relevant to the ICLR community and has some innovation supported by experiments (and some minor theory). It also serves as a good survey for the community with many useful details for AI practicionaries in the field of metric learning, which are outlined in appendix.<|endoftext|>It considers metric learning loss and data augmentation technique (e.g., mixup) together when handling two or more examples at a time. A generalized formulation of loss function was modified to accommodate for the mixup technique. Also, a new metric called utilization is introduced for evaluating representation improvements. A number of experiments were conducted on four benchmarks to show the superiority of the proposed method. 2) It proposes a new metric named utilization to examine the improvements of representation in the embedding space. Cons.1) Previous works have shown that the effectiveness of the mixup for interpolating embeddings and labels (Zhang et al., 2018; Verma et al., 2019). This paper is generally well written and easy to read. However, the novelty is of a bit limited.<|endoftext|>The paper presents a technique for using mixup augmentation in deep metric learning training. Specifically, the dml loss function is represented in a general form so that mixup loss can be easily computed for different pairs. 2) Good experimental validation showing the effectiveness of the proposed approach compared to the baseline and the SOTA Weaknesses:  1) Limited contribution/significance: The two main contributions of the paper are representing the loss function in a general form and incorporating mixup into this loss function (using continuous labels instead of discrete labels as used in the existing dml). Hence, its scope beyond dml is limitted.<|endoftext|>The paper mentions the missing of studying both metric loss function and data augmentation techniques for the metric learning problem and proposes to use a mixup strategy for the improvement. The authors claim the better results over the state of the art using the mixup and use a new metric (utilization) to claim their method is exploring new space. From my point of view, the lower of the utilization score does not necessarily mean the model is exploring new and meaningful space. So could the author please elaborate on what is the key reason of using L2 distance in this case? The experiments on popular datasets mostly support the claims in the paper with the needs of explaining some of ideas mentioned in the paper.<|endoftext|>The paper proposes a generalized loss function for the purpose of using mixup in deep metric learning, which extends the some existing loss functions for deep metric learning without mixup. Extensive experiments are conducted to show the superior performance of the proposed method. I really like the part on building the loss function in (9) step by step. 2.I really appreciate the extensive experiments that have been done in the paper. My main concern is in terms of the error function in (10). What if only the mixed loss term is used as an objective function. Moreover, the objective function also has the clean loss term and the analysis is only in terms of the mixed loss. In Section 4.1 in mixup settings, M(a) only uses positive negative and anchor negative pairs, while in section 3.5 in the description of M(a) positive positive pairs are also defined as a subset of M(a), would it hurt the experiment results if positive positive pairs are also used? I am not an expert in this area and may have missed some related work in this area.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper investigate incorporating entity abstraction to transformer language models for text reasoning tasks. However, experiments on two realistic datasets show that the proposed method fail to effectively improve performance. The paper is generally well written and easy to follow. Therefore the effectiveness of the method is not fully supported by the experiments. This paper proposed a simple method which incorporating entity abstraction to transformer language models. The experimental results on synthetic dataset is good but not as well on realistic datasets.<|endoftext|>Empirical result shows that the proposed method significantly improves the synthetic compositional language understanding task while only marginally improves the other two tasks in natural languages. Strengths  The paper is clearly written and easy to understand. Weakness  The paper only shows end to end performance on the three tasks without deeper analysis on why entity abstraction does not always improve the performance. As the related work section discussed, there are a lot of other related works also incorporating knowledge from named entities. For empirical results, the only significant improvement is on a synthetic dataset while the gain on real world datasets is also marginal. Therefore, the significance of the paper s contribution is limited.<|endoftext|>To achieve that, the authors have tried five different architectures to build the abstraction aware model. The proposed model is tested on three NLP datasets for reasoning. It is well presented, and the experimental results are solid. The goal of the proposed model should be improving real world data, instead of synthetic data (CLUTTR in this paper). 3.The method of incorporating abstraction is not universal. We’ve seen different methods work for different datasets. If there is a new reasoning dataset, which architecture in Table 1 should people use? Testing all five is very inefficient.<|endoftext|>Experiments on CLUTRR, HotpotQA, and CoQA show that the models with abstract entity knowledge perform slightly better than without it. This paper is in a good shape for reading, easy following. Overall, this is a solid empirical paper with convincing experimental results. More analytic experiments will bring more interesting findings of the proposed model.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposes a federated learning algorithm that discretizes the client updates in order to achieve better robustness against various attacks. However, some of the claims made in the paper are not grounded. Is the finding generalizable to other datasets and model architectures? This paper is the first to consider its effect on robustness, which is an interesting idea.<|endoftext|>The paper proposes to use stochastic quantization to increase robustness of federated learning. The work lacks novelty. The remark seems to say that if poisoning rate and $|u l|$ is small, the approach is robust.<|endoftext|>However, I am worried about the theoretical analysis on the robustness, as well as the empirical robustness against adaptive attacks. 2.Using discretization seems to be an interesting idea. This does not make sense to me. 3.It seems that the utility loss is large in some cases. For instance, signSGD [A] sends the sign of the gradients. Also, stronger attacks and defenses are not compared.<|endoftext|>This paper proposes a secure federated learning framework against weight poisoning. There are only general discussions about the security claim, but no supported theorems (like differential privacy guarantee) nor supported empirical evidence (client inference attack experiments). Lastly, numerical analysis is provided to verify the performance of the proposed method. It is interesting that the mechanism improves robustness and privacy while sacrificing little utility when the ratio of adversaries is small.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; rating score: 5; The inheritance? Solution: A novel architecture that can encode prior knowledge (known terms, PDE structure, boundary conditions), and sparse regression procedure that is hypothesized to be more robust to scarce and noisy data scenarios. All in all the other reviewers have shaken my confidence in the empirical results and rigor is methodology descriptions, but not as much in the novelty. I am not extremely familiar with the field but I believe, especially the latter technique, is quite novel.<|endoftext|>The focus of this paper is to develop a learning framework to discover spatiotemporal PDEs from scarce and noisy data.<|endoftext|>### Updates after discussionsMy concerns are partly resolved and I raise my score to 5. Pde net: Learning pdes from data. The authors should compare more models in the experiments, including the several studies mentioned at the end of the section 2 of this paper, and some other studies dedicated to sparse and noisy data.<|endoftext|>I raise my confidence from 4 to 5 based on the discussion with the authors. Physics informed learning of governing equations from scarce data. It is not sure how and what physics is encoded into your system as claimed in the introduction. Deep learning of parametric partial differential equations from sparse and noisy data. Learning continuous time PDEs from sparse data with graph neural networks. The paper is technically correct but lacks novelty and clarity.<|endoftext|>b.The discussion in "Data reconstruction" is very abrupt and does not tie in with the discussion so far. This is exacerbated by the changing dimensions of u. The paper is well written in parts with some notational issues and incomplete description. Why did the authors choose this architecture? Consider adding a discussion about this. Signal reconstruction from noisy random projections.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; In my opinion, the paper introduces a very simple model for continual learning and showcases extremely encouraging results in class incremental learning. In authors  proposal, the matrix alpha is trained on the first task only. PROs:  The paper is for most parts well written. The method proposed is technically sound. Even in its absence, the model can outperform a number of competing methods that store examples from past tasks, at a fraction of the memory overhead. To my knowledge, weights for the previous tasks are fixed and therefore it should have exactly 0 forgetting and BWT (same as PNN and the model proposed by authors.) A substantial part of Sec.3 describes several ensembling techniques that are used in different settings within experiments. It seems the authors included that as their model is structurally more prone to ensembling (which I acknowledge it is) that regular CNNs. For this reason, it is a bit difficult to assess this contribution.<|endoftext|>The contribution of this paper includes the low rank filter scheme and the designed intra task and inter task model ensemble performing on the filters. The proposed method also achieves SOTA performance on several datasets with tiny size of model memory. This paper is well written and easy to follow. 2.Learning new tasks on low rank filter subspace seems interesting and novel for solving continual learning problems. For the experimental results, we can see that the SOTA performance of the proposed method is based on the ensemble scheme. The authors may need to compare the inference time among different methods. If we use a lightweight network architecture, the advantage of the proposed method compared with other expansion based methods would narrow down.<|endoftext|>The paper proposes a continual learning algorithm that enforces the convolutional filter in each layer to a low rank filter subspace defined by a small set of filter atoms. For each task, each convolutional layer is defined by a new filter subspace but subspace coefficients are shared among the tasks. The algorithm is validated on multiple benchmark datasets. the computational required to compute the distance between filter subspaces requires calculating an SVD which is computationally heavy. ( )The performance of the proposed algorithm can vary heavily on the diversity of tasks. ( ) (Introduction) I do not agree with the first sentence of the introduction. The paper is written and organized adequately. The paper is based on extensive experimental results. The idea is relatively simple and performs well.<|endoftext|>The paper, motivated by the task subspace modeling literature, enforced a low rank filter structure to each CNN layer across time in continual learning. It not only ensures that the knowledge of the past tasks is not lost but also saves a lot of computing memory. In Sec 5.2.2, there are some recent methods that show better performance on Cifar100(20 tasks) in Paper with code. ii.I think only the results of the two datasets are not enough to support the author s conclusion strongly. The author would better add a few more experiments, and there are plenty of datasets shown in other papers for your choices, such as CUBS, Stanford Cars and Flowers, etc.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; Motivated by the gap between training environments and testing environments, this paper proposes to use offline data to calibrate the simulator then use online RL methods to train generalizable agents. \  The proposed method is relatively heuristic and may have limitations on the novelty. As the authors mentioned, using the policy gradient method to optimize simulator parameters could have a large variance. \  There are lots of components in multiple stages in the proposed method. What if the BC model is not well trained and makes the learning of the simulator fail? How do the authors deal with this problem? How large is $i$ enough to reduce the high variance? Are there any theoretical or empirical analyses about it?<|endoftext|>The authors also introduce methods for selecting learned simulators and policies across simulators, and compare their method against a range of offline RL methods and online RL methods with variations of domain randomization. ### Strengths  The proposed method for learning simulator parameters seems well motivated and supported by compelling empirical evaluations. The paper would be strengthened by examining how well the method works in adapting  realistic  simulators from real world offline data, where there is more likely to be interactions between multiple simulator parameters and the domain gap is larger.<|endoftext|>Evaluations assess how well the training procedure can recover known simulator parameters and the performance of policies trained on a learned simulator to policies trained with offline RL algorithms on the offline data directly. # Strengths1) Novel approach	  Offline RL methods have generally focused exclusively on the policy learning process. I could not find where these results were reported. The empirical results are weak in the current form, leaving me less certain the work is mature enough for publication. My score would improve if the authors can provide clear statistical evidence of the superiority of the new method.<|endoftext|>This paper proposes a new approach, which combines offline reinforcement learning with learning in simulation, without the need of training on the target environment. It looks like you are missing the results on MiniGrid. 5.The paper is also missing a discussion of the limitations of the proposed approach and future directions stemming from these ideas. More specifically, they propose to use offline data to learn a distribution over the simulator s parameters and then use the inferred simulator to train an RL policy online.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors establishes a connection between GPCA and graph convolution (graph convolution as a first order approximation of GPCA), and uses that connection to 1. propose a GPCANet model and 2. propose a new initialization strategy for GNNs. Nice experimental results. If the learned parameters W in the NN are not the appropriate eigenvectors, then there is no meaningful first order approximation going on. The authors did not provide any justification for why the weights of the NN will be the eigenvectors of said matrix (though this is an interesting conjecture given that GPCA provides good initialization).<|endoftext|>This paper attempts to establish the relationship between Graph PCA and Graph Convolutional Layer, so as to define a new Graph Neural Network based on graph PCA. The paper shows us that regularization of weights may be necessary for GNNs. 2.A new mixhop network architecture has been proposed that seems to be more promising than mixup performance. 3.This paper is well written and easy to follow. 2.The definition of supervised formulation can hardly be considered as an original definition, and such methods are widely used in semi supervised manifold learning (so much relevant literature), but this paper does not seem to be a suitable reference for the relevant methods.<|endoftext|>The authors make a number of clear contributions as listed in the paper: 1) they build the connection between GPCA and GCN, 2) Based on this connection, they propose novel way of using such GPCA as a graph layer or as an initialization process for training GCN etc. and 3) thus present a new architecture of GPCANet that performs well on their tests. Basically the paper is well presented and written with great readability. In addition, the paper provides sufficient experimental evidence to back the main claims introduced by the authors. This observation is very interesting indeed. Similarly the idea of the semi supervised version is not absolutely new, this has been long in application in supervised dimensionality reduction framework. However the novelty is limited as the proposed model is a parameter constrained NNPN or GCN.<|endoftext|>This paper relates GCN to PCA from the perspective of optimization. The authors propose Graph PCA that is a general form of GCN. The proposed method is technically sound. The relation between PCA and GCN is well established in this paper. My concerns are as follows,1. The experimental results of GPCANET INIT is insignificant. The paper is well written and easy to follow.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper suggests a new strategy for pseudo labelingbased on cluster structures in FedLearn. In your case you consider local cluster assignments as labels and use   subsequently a supervised learning scheme to generate predictions. This is not really unsupervised because in general SL methods focus    on margin maximization of the class boundaries ... which is not the same   as approximating a cluster distribution (so I would say your title is a bit   wrong). Maybe better: Federated learning with surrogate labeling?<|endoftext|>The unlabeled data are transformed at each client to make them compatible with supervised federated learning; consequently, the learned models are transformed accordingly. It is also nice that the method can be easily implemented by adding a transition layer to the existing models, neither changes on the optimization process nor introduces additional hyperparameters to be tuned. Although they assume some labeled data are available, which is not exactly the unsupervised federated learning setting, adding discussions with them in the related work may help position the paper well. In this paper, the authors do assume certain conditions on the unlabeled data, e.g.the assumptions on the class prior knowledge on each unlabeled dataset. Overall, this paper is novel and interesting. However, some points should be improved in the revision.<|endoftext|>The proposed method allows different clients to have different unlabeled data distributions. Then, with the additional information about the class ratios, each client could align with the supervised learning counterpart itself, and the server would not be affected by the fact that the training data are all unlabeled. This means that it goes along different lines from semi supervised federated learning based on semi supervised regularizations (the loss function is the same as supervised federated learning and the regularization is mandatory). 3.The idea is motivated. The assumption that "all unlabeled data distributions must share the same set of class conditional distributions" sounds strong to me. In tables 1 and 2, the proposed method worked much better than the baseline methods, but might the proposed method be much slower than the baseline methods too? This is an overall well executed paper. Besides the above comments, the authors need clarify on a few raised questions.<|endoftext|>The authors assign labels to each of the class and uses a prior for the classes so that each of the client can learn without any labels. The authors have also shown convergences of the algorithm by deriving the upper bound. I felt that Section 2 is not really needed in the paper as explaining the context of supervised federated learning is not relevent to the contributions in this paper. Could this be explained. But I guess this is up to the authors. The paper has provided some novel contribution in terms of the theoretical properties for unsupervised learning.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper focuses on graph injection attacks (GIA) in which an adversary introduces new nodes and links such that the predicted label by a trained classifier for a victim node is changed. They further address the vulnerability of GIA against homophily based defense and propose a GIA attack that preserves the homophily and is robust against such defense mechanism. I liked the approach and thoroughness of this paper.<|endoftext|>In summary, this paper studies an important problem, provides theoretically analysis to understand GIA and further propose a new regualrizer to improve GIA. Extensive experimental results also show the effectiveness of the proposed method.<|endoftext|>This paper studies the advantages and drawbacks of node injection attacks to graph neural networks. Based on the observation, the authors propose to add add homophily preservation as an additional regularizer for the attack to remain stealthy against defenses. Extensive experiments demonstrate that the proposed homophily indeed improve attack effectiveness against defenses. Overall, this paper is very well written and the experiments are also comprehensive.<|endoftext|>This paper studies the problem of adversarial attack in graph neural networks. It aims to improve the unnoticeability of graph injection attack which injects carefully crafted nodes into the graph data. The experimental results are impressive while there are still some concerns regarding some analysis. Extensive experiments have demonstrated the effectiveness of the proposed method under various defenders.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors propose a pre training strategy for Table (structured data) grounded tasks (question answering and fact verification). the performance in the 4 evaluated benchmarks are significantly above the state of the art. the paper provides many insights on the results (Figure 4&5&6, Table 6) and the paper is clearly written. This is not a weakness for this particular model but more for the general methodology of providing the entire table as input to the model. Anyhow, the proposed pre training is very effective in this setting, so overall, this is not a major weakness in my view. A simple and effective pre training strategy for Table QA.<|endoftext|>The paper is in general very easy to follow with a quite clear structure. I believe simple is a good feature to have. As the huge amount of data can be very noisy and could waste large amount of time filtering and preprocessing. The pre training objective function is also irrelevant to the table structure. To sum up, the paper has strong empirical evidence to support its simple yet effective approach. Though the wording and claims are sometimes not quite accurate, it s still a very good paper.<|endoftext|>With the attention mechanism, the attention weights are very commonly calculated and shown as in Figure 4. While TableQA is a challenging problem, some analysis on the impact of difficulty of queries on the final performance would be appreciated. I recommend this paper to be accepted. The experiments further confirmed the effectiveness of the method.<|endoftext|>Weakness:  Compared with naming it as  pre training , the proposed approach should be named as data augmentation from constructed synthetic table corpus. It is intuitive to see the improvement of performance since the input and output format between synthetic corpus and the downstream task (QA, fact verification) are the same. This paper proposes a new table pre training approach.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper is concerned with the non asymptotic analysis of SDE based sampling algorithms and has two main contributions. * "$W_2$" instead of "W2" (page 8)* "error. The authors support this theory with numerical examples, where they demonstrate that a lower bound on the 2 Wasserstein error scales as $\mathcal{O}(\sqrt{d})$ and $\mathcal{O}(h)$.<|endoftext|>The authors provide new non asymptotic bounds for the Langevin Mont Carlo algorithm in the strongly convex settings with additional growth conditions for third order derivatives. The main strength of the paper is to get sqrt(d) dependence on a dimension with not very restrictive assumptions.<|endoftext|>This paper derives an error bound for the LMC that s improved in dimension compared to earlier bounds, at the cost of an extra assumption. This closes the gap between second order methods like underdamped Langevin Monte Carlo which are claimed to be better in this sense   this paper effectively shows that this claim is not correct (at least in terms of dimension). It must be also clarified in this sense why the mean square analysis is precisely useful for $W_2$ (as the mean square error upper bounds $W_2$). I think Section 2 must be devoted to main results and especially the non standard assumption made in this paper to attain this result. 2) The authors should clarify in a remark how precisely A2 helps to get the result. It has to be shown that the analysis is valid for these simple examples.<|endoftext|>The manuscript considers the unadjusted Langevin Monte Carlo (LMC) algorithm, and performs non asymptotic analysis of its convergence with respect to the 2 Wasserstein distance. Originality: The authors improve upon existing work to obtain a 2 Wasserstein mixing time bound of O(d^0.5/\epilson) for LMC, and show this is optimal. This is an original and significant contribution.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; However, as far as I can tell, it seems to reduce space for content, so I did not flag it. The core evaluation metric used is KL divergence (cross entropy loss) between the predictive distribution and the true likelihood of the data generating process, and this work proposes an algorithm to compute this metric with joint distributions. Would there be discrepancies in the ranking of methods based on other $\tau$ s? The experiments section does a good job in demonstrating the discrepancies that can arise between marginal and joint predictions during evaluation, and these findings are interesting.<|endoftext|>The paper proposes a new benchmark for approximate inference methods, with emphasis on comparing the quality of joint predictions instead of marginals. ~                                                                                                                                   I believe the scope of the benchmark is quite limited, and the paper does not show any strong benefits provided by this benchmark over other already existing benchmarks. Interestingly, Wang et al find that directly evaluating joint likelihoods gave little more information than simply marginals, while this paper concludes that different approximate inference methods can produce very different joint likelihoods with similar marginal performance. In addition to just measuring likelihoods, I think the benchmark would be a much stronger contribution if it also emphasized comparisons on downstream tasks, which can be synthetically generated similarly to the existing settings. "Evaluating Approximate Inference in Bayesian Deep Learning."<|endoftext|>The paper proposes a simulation based framework to evaluate different techniques proposed for uncertainty estimation of predictive models. The paper also go beyond evaluating marginal posterior predictive distributions and extend their benchmarking work to joint distributions capturing sequential decisions that can be made with such models. Some of the highlights from the results: 1) Their results show that Bayesian deep learning is impactful for capturing the joint predictive distributions. Cons:Even though in the abstract the authors mention that the proposed method provides insights into aleatory and epistemic uncertainty, the manuscript does not elaborate on this point.<|endoftext|>The authors discuss whether it is sufficient to consider the marginal posterior predictive vs considering a joint posterior predictive when evaluating Bayesian deep learning approaches. Additionally, extensive code is provided for efficient implementation and evaluation of new models. (To be more precise in the rest of the paper, the joint posterior predictive.) This prior work is missing completely in the discussion. ## Specific additional questions to the authors  Can the authors clarify the differences/similarities with Lu et al., 2021? While I do not know the original story behind Munroe s comic, the discussion of different types of sources of uncertainty is a lot older than our current deep learning popularization. I would claim that this finding is not too surprising.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors of this paper proposed a theoretical explanation of Neural Collapse (NC) for the DNNs with MSE loss. In particular, the authors showed that MSE loss can be decomposed into a sum of terms that corresponds to NC conditions and proposed the theoretical model (central path) that explains why NC emerges in DNNs with MSE under some mild assumptions. While MSE delivers similar performance to cross entropy loss (CE), it is less used in classification settings than CE. The paper further extends our theoretical understanding of deep learning. The result is new and supported by experiments.<|endoftext|>Recently, Papyan, Han and Donoho (2020) have observed that training neural networks beyond zero training error leads to simplex arrangements of the features. In particular, it is shown that the square loss can be split, such that one summand corresponds to the quality of the features (quantified by the loss of the MSE classifier on them) and that the other summand corresponds to the quality of the classification layer (quantified by the its deviationfrom the least squares classifier). Could you explain the terminology central path (Equation 4)? In fact, the updated definition of NC1, is precisely the quantity measured in the experiments by Papyan, Han and Donoho.<|endoftext|>This paper studies the phenomenon of Neural Collapse (NC) and empirically shows that it occurs during the training of deep networks with the MSE loss. Then it theoretically analyzes NC with MSE loss by decomposing it and introducing the notion of central path. It shows a closed form dynamics predicts NC in this setting. The decomposition of the loss function and how it is helpful in understanding NC is interesting. In the legend of Figure 2, the term "Lperp" should be referred to as "L^\perp" in the caption. The strength and weakness of the paper are very clear, as described above.<|endoftext|>This paper extends the recent work on Neural Collapse, using Mean Squared Error (MSE) instead of CE, as MSE is easier for analysis. With this, the paper shows that the least square loss can be decomposed into one that corresponds to a so called  central  path (namely a set of optimal tuples (W, b, H) given H for which there is an optimal loss), and a perpendicular loss. The paper shows that the perpendicular loss is much smaller than the optimal least square loss and thus Neural Collapse appears due to the fact that the optimizer focuses on the central path. The strengths of the paper:  The phenomenon of Neural Collapse is certainly an interesting and thought provoking and analyzing it further to understand its premises is certainly worthwhile of publication.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; rating score: 6; I found this paper straightforward to read although I have several large concerns. From a technical novelty/innovation perspective, there is very little that is conceptually new. As is, the experimental results presented make it seem that DART is not a clear winner over top performing baselines. Some explanation of these discrepancies would be helpful. Statistics in Medicine 1995.<|endoftext|>The authors introduce a deep learning based extension to accelerated failure time modeling for survival analysis and introduce a rank regression type loss function based on Gehan s rank statistic. Note that DRAFT in Figure 1 is not introduced or discussed in the paper.<|endoftext|>The paper extends the previously proposed linear semiparametric AFT model based on Gehan’s rank statistic to a nonlinear setup. However, the technical contributions are lacking, there are some misleading statements, and the writing needs improvement. Introduction: "The accelerated failure time model (AFT) or accelerated life model relates the logarithm of the failure time linearly to the features." This statement is not necessarily true for some parametric AFT models, *e.g.*, Weibull, Exponential, *, etc. The semiparametric AFT model based on Gehan’s rank statistic objective function has been previously proposed.<|endoftext|>This paper proposes to combine the idea of Gehan’s rank statistic idea on fitting the AFT model, as well as using the deep learning model as a non linear method for replacing the linear method in the original AFT model. 2.The results do not support the conclusion. The intuition of the approach is not quite clear.<|endoftext|>It is not entirely clear how much influence either of these have on the benchmark results. While there is a large number of machine learning methods for survival, by considering a (for the machine learning literature) novel loss function in the AFT setting, this paper can be seen as a meaningful contribution to the machine learning for survival literature. The theoretical foundation of the loss function is however not perfect and the influence of loss function and architecture are hard to disentangle. I would recommend a weak accept.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper presents a new metric learning method on the RL formulation of query object localization. This paper presents a novel method for solving query object localization as an RL task. The writing and presentation of the paper is clear. However, it is not easy to understand the similarities and differences between the proposed method and the baselines it compared with in the transferring task.<|endoftext|>This paper extends works performing RL based object localization by:1. Conditioning the localization on a exemplary set of images, instead of more classical hardcoded finite set of classes2. For now, I’d qualify this as borderline and tend to accept given it was an interesting read, but I am not extremely familiar with the literature and related work. It is quite hard to find details of decisions and specific model choices however, so perhaps the authors can improve on that and clarify some of these points:1. The early sections provide a good overview of the problem setting and of the particular way this will be addressed. 2.What is trained and frozen in the different tasks?<|endoftext|>The paper proposes a reinforcement learning approach for localization in images. The policy is learnt with REINFORCE with entropy regularisation. The results on COCO are a bit limited but I would be willing to overlook that. strengths:* reinforcement learning approaches are interesting in this domain* results seem promising* experiments thorough and give some depth to the methodologyweaknesses:* the paper is overly complicated and lacks clarity on many details. I find that very unclear. The actions are unclear, the role of the reward should be clearly stated, the environment is not an image it s a set of states and transitions etc.<|endoftext|>The paper found that the proposed approach compares favorably to DDT on object location on CUB and FTA on few shot object detection on COCO. As a result of the simplified setup, the paper uses the CorLoc metric to measure the percentage of images with IoU > 0.5, and it is not the same metric used in few shot object detection (which uses mAP over different splits). The paper presents an interesting idea for object localization using RL. Therefore my initial rating is “weak reject”. After seeing the authors  response, I decided to increase my rating from 5 to 6 since the added experiments addressed my concerns.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper presents a benchmark for experimental design in drug discovery based on previously public data sets. Although the  data sets are already publicly available, it s nevertheless nice if someone else has done the required work and one can start modeling immediately. Cons/questions:  The paper is straightforward, does not have any particular shortcomings, and is potentially useful to the community. However, I feel it lacks the kind of machine learning novelty and significance that would be expected from an ICLR publication, and hence some other forum could be more suitable (though I m happy to discuss this with the other reviewers and the area chair). I still find the benchmark potentially useful and something I might use in my own research. I will update my score to borderline positive (6) to reflect these thoughts.<|endoftext|>The paper has the following contributions:  A new experimental design benchmark suite of genetic experiments for CRISPR screens with unique challenges not found in other benchmarks. The strengths of this paper include :   The paper focuses on an important aspect of experimental design for drug discovery, which has not previously been studied. In practice, this information can be very useful for active learning algorithms. The authors study only two candidate algorithms so far; however it is still an important step forward because the novel dataset and methodology enable other groups to conduct extensive evaluations on their own machine learning algorithms. It is unclear if biological replicates are included in the experiments. If replicates were not used, they should be added to the text. All in all, this paper proposes a benchmarking suite that can help developers of active learning algorithms improve their programs.<|endoftext|>The authors introduce a benchmark suite for evaluating active learning algorithms for experimental design in drug discovery. The work focuses on using counterfactual estimators of experimental outcomes to propose experimental hypotheses for validation in in vitro experiments with genetic interventions (CRISPR) in order to discover potential causal associations between biological entities that could be relevant for the development of novel therapeutics. Two model types are used (BNN and Random Forest Regression) and nine different acquisition functions. The paper is much more about gene knockdown than drug discovery. Overall, the authors provide a decent dataset and benchmark for genetic knockdown experimentation in the setting of active learning. However, I believe the paper would be better suited for datasets/benchmarks venue.
Reject; rating score: 5; rating score: 5; rating score: 6; The paper presents a LaTeX based language and compiler called kokoyi to write math based models and compile them to actual code (such as PyTorch). The authors present an approach to support optimizations such as auto batching during this compilation process which significantly reduces user burden. The authors presented kokoyi implementations of several popular models such MLP, CNNs, LSTMs and transformers and showed that the kokoyi compiler does not introduce much performance drop. **Weaknesses**While I understand the appeal to have a LaTeX based coding language to enable executable papers, I do not feel that it is easier to code in LaTeX rather than code. and (iii) inability to partially evaluate the model (for debugging, again the authors identify this issue). These issues both make it hard for a user to program in kokoyi and might result in inefficient code. For example, since there is no state in the loops (summation),  it is not possible to merge multiple data processing steps into one loop. The evaluation is great, but it is missing several details. —What were these models trained on? — How does memory usage look like for the two implementations? Is there extra memory overhead because of the translation? What is the standard deviation? — Table 1 does not report the numbers for all the models. — A user study evaluating the ease of use of kokoyi would answer whether it is easy to code in python or LaTeX? I lean towards rejecting this paper. The authors didn’t make a compelling case for why they need a new LaTeX based language to enable auto batching. The evaluation also lacks several details.<|endoftext|>This paper proposes Kokoyi, which can automatically translate mathematics into Python implementations. To measure the flexibility of Kokoyi, the authors have implemented a variety of popular DL models and performed evaluation. Such a tool could reduce the gap between developing models in math language and implementing them in programming languages. The developers still need to write other modules, such as data preparation and result visualization, in Python. It is not clear how many DL programs actually contain many math equations and can be benefited from the proposed tool. Many DL programs can directly reuse existing operators/packages without going to the details of math equations (and they are not written from scratch)? Although the size of latex code is generally smaller than that of Python code, the efficiency/usability of such a tool still needs to be evaluated. Furthermore, like Python code,  Latex code could contain bugs too. The correctness of latex implementation cannot be guaranteed, so there is still a gap between model and its implementation. The paper is generally well written.<|endoftext|>The paper presents a language similar to LaTex with a compiler able to translate mathematical formulas of network layers into executable source code. The article considered the PyTorch framework and presented several usability and performance studies. The paper is well written and clear. It hides many aspects of translation and focuses more on the user side. The idea is extremely fascinating, especially about  "executable papers" which would be very interesting. Unfortunately, the approach is not testable. I would have appreciated the possibility to test the system to understand how much the language helps to implement my network. If I have to find a second weakness, probably it is that the paper does not provide formal definitions or details about the implementation. But the message the authors wanted to give is different, thus I cannot consider this as a malus. Honestly, as said above, I do not see any problem with the paper itself. It is discursive, it tells a story, which in my opinion is a good story, worth discussing. However, it is a story that I have to trust but that cannot play with, which limits the quality of the article.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The problem is very well motivated, and the paper explains the core ideas in a very precise way. However, in many real physical systems, the underlying ODEs can exhibit stiffness, i.e., numerical instability, over some time intervals or for certain values of initial conditions and/or parameter choice. As a result, the existing realizations of Hamiltonian preserving neural networks underperform in these scenarios. This paper proposes a solution to overcome this issue. In particular, it introduces an easy to compute stiffness aware index to split the training data points into stiff and nonstiff groups, which are then treated with different integration schemes (with different values for integration time interval and time steps). The paper is well written, and it conveys the key ideas in a very precise manner. The authors have done an excellent job in highlighting the need for a stiffness aware approach. The idea is straightforward but quite effective, as one can see from the experimental results. Therefore, if $M$ is indeed assumed to be a diagonal matrix, the scope of this work is very restricted unless the authors can show with additional experiments that the proposed approach holds true even when the mass matrix is non diagonal and position dependent.<|endoftext|>Hence, without SAI, it is difficult for a neural network to learn a stiff dynamics. The contribution of SAI is confirmed using a three body problem. ### NegativesSection 5 demonstrates that SAI is a good approximation to stiffness index (SI), but this might not hold for different coordinate systems. As shown in Table 1, the results are sensitive to the hyperparameter tuning. ### After discussionAll my concerns were addressed by the additional experiments and explanations. I update the score from 5 to 6. This study is based on an insightful suggestion, and the proposed method is simple but effective.<|endoftext|>This paper proposes to improve the learning of a Hamiltonian system, by characterizing the stiffness of the time series data. A stiffness aware index is first used to classify the time interval into stiff and nonstiff. Separating the stiff and non stiff parts of the data when training a Hamiltonian network is a novel idea. However, only two examples are shown in the experiments, it would be more convincing if experiments of more Hamiltonian systems can be conducted. There are certain contributions of this paper on proposing characterizing the stiffness of the time series data, which shows its advantage of improving the performance of the Hamiltonian network.<|endoftext|>This paper presents a new method, stiffness aware neural network (SANN), for learning Hamiltonian systems from data. The authors define a stiffness aware index (SAI) for classifying the training data into stiff and nonstiff samples. The effectiveness of SANN is demonstrated using chaotic Hamiltonian systems, i.e., a three body problem and billiard model. This paper is well written. There are several concerns about the experimental setting. The proposed method is simple but effective; however, there are the hyper parameters $\gamma, S$ to be manually determined. 3.The authors assume the separable Hamiltonian $H(p,q) T(p)+V(q)$. In the experiments, did the authors use this assumption in the HNN learning? Also, the proposed approach is novel, and the experimental results are insightful.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; To sum up, this paper seems to have several technical flaws for the claims to be well supported. It also presents a method of mapping the proposed CSQ to BNN hardware through bit wise operations. This paper claims that no specific analysis of CSQ has been made in previous studies and that low precision quantization results in poor performance due to the allocation of asymmetric quantization levels in the positive and negative sides of the distribution. The idea of using a zero centered quantizer is ok, but it might need more novelty because it has already been used in several previous works. That s why the authors claim that this paper has other contributions in that they propose an explicit quantizer function for CSQ and provide some analysis on the effectiveness of such a scheme at very low precision. However, it is not clear to the reviewer if their claim is technically sound because theoretical analysis or empirical evidence does not support their main contributions strongly. The learned weight distribution as described in Figure 2 of DFQ [1] may not be symmetrical. It seems that applying CSQ is not always better than the previous approaches in the above situation. The author should explain first the necessity of allocating symmetric quantization levels by comparing these methods to claim the motivation of this study. Is there any particular reason why the authors excluded non uniform quantization methods or other QAT approaches to solve the same problem from the discussion? 2.In the experiment, it is explained that CSQ is applied only to weight quantization. 3.Overall, the improvement due to CSQ seems to be minor and insignificant for both CIFAR 10 and ImageNet because there is little performance gap when looking at the results presented in Table 5 and Table 6 and results of applying PTQ covered in the appendix. Therefore, it seems not convincing to validate the effectiveness of the proposed CSQ scheme. 3.There are a few typos.<|endoftext|>The authors also propose a binary coding method to run efficiently on hardware. The authors show that CSQ can be efficiently expressed by binary representations, and then be calculated using XNOR operations. CSQ has quantization results for mobilenets, which are compact models that generally are harder for quantization. Experiments on large datasets (such as ImageNet) are also helpful to validate the effectiveness of CSQ. In addition, some ablation study are meaningful, such as the BRECQ CSQ experiments in the supplementary material. From the paper we can see that CSQ can consistently outperform CLQ on 2 bit, but is not guaranteed to be better on 3 bit or 4 bit. Plus, there are other orthogonal methods that can alleviate the quantization degradation on 2 bit. 2.In Table 5, the ESQ results mentioned on the caption are missing. 3.The experimental results of CSQ didn t show great improvement over CLQ. I wonder if the accuracy gain will remain if advanced quantization aware training methods (with or without distillation) or mixed precision methods are applied together with CLQ/CSQ. I suggest using ResNet50 which is a more common and standard choice. I think CSQ is a simple but effective method that can be helpful for ultra low bit quantization. However, the novelty and application scope are limited and the experiments have room for improvement. Consequently, I think the paper is marginally below the acceptance threshold.<|endoftext|>This paper presents a centred symmetric quantizer(CSQ), a new symmetrical quantization scheme. Strength: A new idea of applying centered symmetric quantizer for model quantization. Speedup setting: The authors only compare the single matrix multiplication runtime, which does not necessarily represent the speedup and the accuracy of the whole network [1]. 2.Due to the new quantization level, the authors are encouraged to conduct experiments on other tasks (e.g., detection and NLP tasks.), which can present the generalization and robustness of CSQ. 3.The accuracy improvement is marginal than CLQ on the ImageNet dataset. 4.If people optimize sparse mand quantized models jointly [2, 3], is the CLQ with sparsity equal to CSQ? A unified framework of dnn weight pruning and weight clustering/quantization using admmThe analysis and experiments have some issues (see weakness). I will give a borderline due to those concerns and change it accordingly based on rebuttal.<|endoftext|>This paper analyzed the difference in quantization methods between the conventional linear quantizer (CLQ) and centered symmetric quantization (CSQ). The authors further proposed a bitwise implementation of CLQ and CSQ. The evaluation in terms of the quantization accuracy and GPU implementation speed is given for their analysis. This paper seems to be the first to deep dive into the difference between CLQ and CSQ, although these quantization techniques are not new. However, the resulting analysis seems to be straightforward for the following reasons:  The conclusion that CSQ could be (marginally) better seems to be as expected, since CLQ might not be efficient for representing the signed (or centered) distribution. Although this paper explicitly revealed this fact, it does not seem to provide new insights. The analysis based on the representation capacity seems to be weak. Table 3 reveals that the number of distinct output states varies for different combinations of quantizers. Furthermore, the importance of the representation capacity is not clear; although CSQ CLQs has a 50% larger representation capacity for the signed activation, its accuracy gain seems to be marginal (~0.4% from Table 5,6). 2) In the case of MobileNetV2, the reported accuracy results CSQ seem to be significantly lower than the state of the art (e.g., PROFIT(ECCV2020) 4 bit MobileNet V2   69.06% compared to CSQ 66.98%). Therefore, I am inclined toward rejection.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 3; The authors explain that autoregressive (AR) models suffer less from the same issues because of a combination of multiple factors, such as causal attention, auto regressive modelling, and introducing positional embedding (the last point is more specific for the Transformer architecture). In general, the paper is mainly about tackling a well scoped problem (token repetition) using an existing architecture (CMLM), which is well known in the domain of non autoregressive MT. The analysis by the authors shows that the proposed ideas help mitigate the multi modality problem, although the problem is not fully eliminated. Eventually, non autoregressive models will become useful by removing this constraint. This paper shows interesting observation points to understand why the multi modality problem occurs in non autoregressive MT systems, and proposes a few ways to mitigate the issue. I also find that it is nice to remove the restriction of non autoregressive models that rely heavily on autoregressive models.<|endoftext|>Authors describe two important issues behind performance gap between semi autoregressive CMLM (conditional masked language models) and autoregressive models in Machine Translation   (i) indistinguishability of tokens, and (ii) training and inference mismatch. **Strengths*** The paper is very clearly written and easy to follow. * The issues behind performance gap between NAR and AR models have been clearly explained and methods to circumvent those issues have been well motivated and justified. * The resulting model CMLMC achieves state of the art scores for semi autoregressive machine translation, and more importantly achieves better performance on non distilled datasets. **Questions/Concerns*** *More Ablation Experiments*: In case it is easy enough to execute, it will be informative to see results for `SMART + RevPos`  in Table 1 as well. This is slightly wrong, as in SMART, the masking length can be set to sequence length which would then result in predictions from a fully masked sequence (1st inference step). if the authors agree with this, it would be great to clarify this in the paper. While the arguments presented by the authors make intuitive sense, it will be nice to perform some empirical assessment of this claim. The paper is well written, presents well motivated improvements for improving NAR models. I recommend acceptance for the paper.<|endoftext|>This paper presents two techniques that improve iterative non autoregressive machine translation (NAR). This is designed to encourage the model to distinguish between target positions and avoid repetitions. **Strengths**  The proposed method is very simple, and can be applied to many iterative NAR models (and perhaps more, e.g., when we use BERT like models for generation in general). Removing the necessity of distillation will have a practical impact for NAR models because distillation is, as authors point out, time and resource consuming. How does this increase compare to the training time increase you would get from knowledge distillation? If the proposed method s increase is much smaller than applying knowledge distillation, it would mean that the method can save training overhead compared to approaches that require knowledge distillation. I would strongly recommend that the authors add the discussion on increased training time to the final version, as one major benefit of removing KD is training time savings. Overall, this paper presents s simple, effective approach to improve NAR machine translation.<|endoftext|>This paper argues that the indistinguishability of tokens and the mismatch between training and inference leads to inferior translation quality in the previous state of art non autoregressive~(NAR) model   CMLM. To address the above issues, the authors propose the CMLMC model and achieve better performances without the help of the autoregressive model. 2.The performance improvements are impressive, which remarkably narrows the performance gap to the autoregressive model without the knowledge distillation. The novelty is somewhat limited. 3.Although this paper focuses on the performances trained with the raw datasets, several comparisons may not be fair for distilled datasets. Some results of baselines in Table 2 use the Transformer base as a teacher in WMT14 instead of Transformer large. Figure 1 only analyzes the first inference step but misses the 4  or 10 iterations results. I know this issue is important to fully non autoregressive models and discussed by Wang et al.2019[1].Need to say, their method can be incorporated directly into your setting. 2.I notice that the CMLMC model achieves significant improvements in the first inference step than the CMLM model (Figure 3, it is also very curious to report a Table in the Figure environment). One step results can be directly compared with most NAR models, which will provide a stronger baseline for subsequent work.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposed a fine grained automated data augmentation approach, Patch AutoAugment (PAA), which tries to increase diversity in local regions by divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. The experiments show a good result and visualization results provide some insights that the PAA  help the target network to localize more class related cues. The most important contribution of this paper in my mind is the new search space focus on image patches for data augmentation. 2.The proposed PPA formulate the task as a multi agent reinforcement learning problem, and the results show it can find better policy than random, but [1] demonstrate that only use reinforcement learning can not get better policy than random baseline. The two experimental results are contrary. 4.Another question is that recent work [2] with careful designed augmentation policy and advanced training strategy can get much better results, It is better to compare with it to give more insights.<|endoftext|>This paper target the task of automatically determining the best augmentation method to obtain improved accuracy. Strengths+ It seems effective to utilize the MARL algorithm to reduce the computational complexity for the multiple patches. + As a result, the computation cost of the proposed algorithm becomes much reduced when it is compared to the previous auto augmentation algorithms. When the image is the original image before the determined augmentation, the state and the observation are not affected by the selected action. Then, the training loss becomes also similar, which results in no selection of translation augmentation. In the experiments, the proposed algorithm is only compared with the out of date algorithms, so its performance cannot be validated well. I recommend the authors add the state of the art studies related to auto augmentation and the learning based augmentation algorithms, which include Saliency mix[1] and Co mixup[2].<|endoftext|>The problem of selecting a transform for each cell  is cast as a multi agent RL (MARL) task, and the agents  learn as the main network trains within a (multi agent)  Advantage Actor Critic framework. S2: The efficiency of the proposed method is    investigated very clearly in Section 4.3. It seems    considerably more efficient than many other    learning based DA methods. S3: The patch wise data augmentation is a powerful idea    which can spark a large body of interesting follow up    research. ## Limitations    L1: While this is not the main focus of the paper, the    empirical improvement over RL baselines like Fast    AutoAugment and non RL baselines like RandAugment is    relatively small. The paper proposes a new way of looking at the classic  task of image augmentation for classification. The  idea to apply image augmentations at a local level is the  core contribution of this paper, in my opinion. While it would be interesting to see how this technique  works on tasks which, unlike image level classification,  are themselves more "local", such as semantic  segmentation or detection, overall the proposed approach  is evaluated thoroughly. As  such, I recommend accepting the paper to ICLR.<|endoftext|>Different from existing works, they proposed to augment patches in the image rather than the whole image. The approach is formulated as a multi agent reinforcement learning problem. ### Strengths:  1. The paper conducted a comprehensive evaluation of their approach over several datasets. 2.The approach considers patched based augmentation, which is sufficiently different/novel from existing work. ### Weaknesses:  3. The motivation of the approach is somewhat lacking. For example, it is unclear why MARL on patches is much faster. Based on its relevance, the paper should discuss and contrast with this work, as they also have minimal search time. I saw the hyperparameters for the target model but not MADDPG. It would further motivate the use of MADDPG and the multi agent setting. Currently, I recommend a weak reject.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper claims that it is possible to compute the Newton method s update exactly for deep neural networks (multi layer perceptrons). The motivation is that Newton s method, as a second order optimizer that includes loss curvature information, should improve upon first order optimizers, such as gradient descent, that use only first derivatives (gradient) of the loss. Second order methods tipically converge in a smaller number of iterations, but each update is expensive to compute and that s why they are not widely used in practice. However, the authors claim they have some tricks to fix that. I think approximating the inverse of H by knowing just its product with a single vector is unreasonable. Basically, we know the curvature of the loss along one direction only (which we can t even control)! It looked like the authors struggled to finish the paper in time for the submission deadline. That is very intuitive, introducing auxiluary variables and lagrange multipliers does not change the final result. However, when moving to section 2, it is completely unclear where the Newton s update comes from. 3) Finally, the "Results" section: by just looking at the plots, it seems that the proposed method is worse than most of competitors.<|endoftext|>This paper proposes a stochastic second order method to train neural network under some specific regularisation criterion. The method is based on the Sifrian, an extension of the Lagrangian that splits the definition of the gradient of each layer s parameter as different constraints with their own multiplier. This allows for a stochastic algorithm to train the neural network. p.2 _"$p$ designs one single pattern"_  > $p$ designs a single sample. Overall assessment Overall, I find the paper hard to read and follow, in particular due to the notations (see section bellow). Eq.(4): it is not clear immediatly that $x$ is the activation of all layers. It seems to derive from a splitted problem with auxillary variables but this is never explicited. p.4 _"Our approach to extend the Lagrangian consists of"_  > "consits in". p.7: _"which by construction overfits to every pattern"_  > overfits every sample. **Q2:** The proposed approach looks very similar to ADMM training for neural network (see for instance [A]). This paper should be mentionned and the main difference with the proposed method shoudl be highlighted. This makes it hard to know when this method can be applied. Moreover, the limitation they introduce are not really discuss. The proposed method is not performing better than the other methods, neither in iteration or in time. While this probably helps with variance reduction, discussing this in the main part of the paper seems necessary.<|endoftext|>When the regularizer added to the optimization problem satisfies certain additional assumptions, the algorithm can be implemented efficiently with additional assumptions on the activation function, including monotonicity and piecewise affinity. This manuscript is among the many proposals of applying a second order method to train neural network models. The major issues of the manuscript include:1. Usually we expect a paper with either a strong theoretical result or outstanding numerical performance. Unfortunately, the numerical results show that the proposed method performs significantly inferior to existing methods, even in terms of the number of epochs and in terms of the training objective, for which usually second order methods stand out. Even in the preliminary part of introducing backpropagation, the path the authors took is abstruse, while back propagation itself should be a simple concept by viewing from the simple chain rules. The modification of the update step to make it a descent one also looks like quite arbitrary, and if the original update direction is "Newton", this change for sure destroys the meaning of the derivation. I have read the authors  responses, and some misunderstandings of the regularizer are clarified.<|endoftext|>The paper considers feed forward networks as a base model, build a second order Lagrangian which it calls Sifrian,and provides a closed form formula for the exact stochastic Newton direction under some monotonicity and regularization conditions. The work extends the 1st order results in (LeCun et al., 1988) to include 2nd order information for feed forward neural nets, and experimentally compares the new 2nd order methods with other neural net training techniques. There is a potential issue for the Sifrian to be hard to solve. However, the paper requires the existence of a peculiar regularizer, but does not discuss any statistical properties of this regularizer.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; With the exception of unidirectionality, which may be considered a contribution? The method description well describes and justifies the main contributions of the paper. I think this is an interesting paper that has technical novelty, however there are issues with the experimental results that need to be addressed.<|endoftext|>LIME is notorious for being very sensitive to its hyperparameters, and the paper posits that a robust variant of LIME is one which is invariant to these hyperparameters. How is this different from these efforts? The novelty of the work seems limited, as the algorithm and theoretical results in the paper are largely based on [1],[2].<|endoftext|>Creating “environments” around a data unit further exacerbates this concern. On the other hand, one other related concern is the answer to the following question: Hoe exactly to create environments? I am not clear about the theoretical properties of the Nash equilibrium in this problem. The authors have shown their results on more than one environment (<  5) in the Appendix. How would the authors recommend choosing the number of environments? I have an understanding that this is, at the moment, determined only empirically, and am not clear if there is any theoretical analysis over the number of environments.<|endoftext|>Update:In our back and forth, the authors have addressed or committed to address the concerns I had with this work. Section 4.2.1: "optimized subject two constraints"  > "optimized subject to two constraints"  Section 4.2.1: "In other words, for features where there is massive disagreement in even the direction of their impact are eliminated by our method" There is an agreement error here. In addition, I have a few points I d like some clarity on. I do have some minor comments that I have listed below.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; In an almost homogeneous setting, this approach can improve rates. Is it not too strict to have a uniform bound for this difference? Upper and lower bounds explain methods  behaviors in detail. In this paper, local methods with shuffling are also considered. I understand that this work is theoretical, but simple and small experiments on toy models are desirable. This paper introduces a deep and wide theoretical study of permutation based variants of local SGD and minibatch SGD. The novelty and tightness of results are significant, so the paper should be accepted.<|endoftext|>This work analyzes the convergence rate of local and mini batch Random Reshuffling. For \mu PL and smooth objectives, It provides high probability upper bounds, and matching expected lower bounds, and a special variant of the random reshuffling that outperforms the previous tight bounds with \sqrt{M} speed up in some regimes. 3.The provided bounds are tight in some sense and variants with better performance (linear speedup) are analyzed. 4.The lower bound for mini batch RR in small epoch regime is informative in that it concludes the weakness of mini batch RR for K \lesssim \kappa. 5.Although this is a theory paper, giving some experiments to validate the analysis especially for the discussions on different regimes would be appreciated.<|endoftext|>Moreover, the authors showed that their provided convergence analysis is tight for the case with not large number of epochs. This paper is solely theoretical with promising guarantees. The authors showed that if $K \geq c_2\kappa$ where $c_2>0$ and $\kappa   \dfrac{L}{\mu}$, then their convergence rates are faster that the with replacement counterpart. However, for the ill conditioned case, the number of epochs (theoretically) will be huge to satisfy the mentioned assumptions.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This work provides an analysis of the sharpness aware minimization (SAM) by Foret et al.2021.This work provides an analysis of the sharpness aware minimization (SAM) by Foret et al.2021.While the paper has some experiments, they are minimal and serve to reinforce the points made by the theoretical analyses. Therefore, I view this work to be a primarily theoretical work. Although the model considered is very limited, and although the analysis technique is not novel, I still think this is a good first step. The authors summarize the results of their analysis in Proposition 1, but I am not sure if I agree with the interpretation of the anlayses. The second result is a convergence analysis of SAM. This is a result that was missing in the original SAM paper, so it s nice that the authors establish that SAM converges to a stationary point, just as SGD does. However, the analytical techniques are not quite novel, and the conclusion says nothing about why SAM is better than SGD. Overall, the paper presents some interesting and useful analyses, but the analytical techniques are not very novel and they do not (at least to me) provide significant clarity on why SAM outperforms SGD.<|endoftext|>I think that the paper has to make more clear what are the main contributions. The proof technique used in this paper is quite standard, and the results are based on the square loss. So the impact of the results may be limited. I also have some concerns about some details, as I have mentioned in the main review. In the SAM algorithm, the gradient is normalized in the inner step. However, those examples are far from the practice because they are strongly convex with global minima at point zero. The training loss also has many local minima. In this case, a small perturbation will still guarantee the convergence and help find the flat minima. [1] Foret, Pierre, et al."Sharpness aware Minimization for Efficiently Improving Generalization."<|endoftext|>Finally, the authors discuss how to further improve SAM by leveraging a gradient reweighting interpretation and combining with a robust loss. The first major result is Proposition 2 in which the authors use analytical techniques from SGD to control the expected gradient of SAM with stochastic gradients. The next contribution has to do with SAM s robustness to noisy labels. This is good aim to study. However, the explanation falls short of being convincing. The authors merely use the fact that SAM changes the direction of the gradient to be less aligned to the noisy direction. They substantiated this fact using Figs. The third contribution is the observation that SAM even improves generalization for linear models. This is a surprising phenomenon, but the explanation is not clear. Again, I was hoping for a more theoretical explanation, since this is an "Understanding" paper. The aims of this paper are laudable, but the explanations for short of being convincing.<|endoftext|>The aim of the paper is to provide theoretical explanations for the recent successes of adversarial weight perturbation and sharpness aware minimization (SAM) methods. The paper aims to fill this gap by getting a better understanding of SAM. It contributes a collection of miscellaneous results:* A proof that SAM provides better generalization for linear neural networks than SGD* The convergence of SAM is analyzed and generalization behavior is discussed* A new interpretation of SAM is presented in terms of gradient reweighing * Finally, a connection of SAM to the noisy label literature is made### StrengthsThe main strength of the paper is the theoretically sound and insightful analysis of SAM. To me, this is actually a weakness of SAM and not a strength, since ideally one wants a robust method to converge to points in the landscape which are non stationary but in an overall "flat" low loss region. Is this conclusion based on some empirical evidence or a theoretical insight? If the issues mentioned in my main review can be addressed, I am inclined to increase my score and recommend acceptance of the paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; In this paper, the authors aim to find an offline solution to the imitation learning problem with corrupted demonstration data. In general, the authors propose a novel and interesting MOM objective for robust imitation learning from corrupted data. The contributions of this work are as follows:1.<|endoftext|>(This relatively simple but hyper instructive mathematical analysis is also absent from the paper). B   The corrupted samples are selected at random from the demonstrations, and, the ratio of corrupted to non corrupted is quite low (<10%). A lot of imitation learning papers do assume that you can query the environment or an expert policy, which is not always feasible.<|endoftext|>This paper proposes the definition of corrupted demonstrations and a new robust algorithm for offline imitation learning from corrupted demonstrations. Strength:The authors provide theoretical and empirical analysis that the proposed algorithm is simple yet effective.<|endoftext|>This paper considers an offline imitation learning task wherein a constant $\varepsilon$ fraction of demonstrations have been potentially arbitrarily corrupted. Strengths:+ The paper considers a problem of relevance to the ICLR community, and proposes what appears to be a novel algorithm with both empirical and theoretical support.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; This paper explore multi target domain adaptation by graph. To remedy this problem, In this paper authors propose the graph based method to learn the adjacency between domains. In this paper, authors propose adversarial learning method, which refers the encoder and the discriminator. What is more,  authors show the theoretical analysis for this problem. The reported results shows that the proposed method successfully generalizes uniform alignment. This paper propose novel method multi target domain adaptation. Authors introduce the adversarial learning method for this issue, which looks interesting. 2.The paper gives clear description about this problem, and the proposed method, which is easy to follow. 4.I like the shown figure in this paper, which is great for me to depict how the proposed method works. What I mean is to learn the graph for data instead of domain. 2.The reported results looks has little advantage compared to baselines. After rebuttal Thanks for authors  response. Authors have addresses my concerns excepting for the further experiment for two domain, which does not influence my score. After reading other reviewers  comments (e.g., on one mention the less novelty), it think the proposed method still keep advantages compared existing methods. I would like  keep my score. It looks the proposed method is new, since it built an adversarial learning for this question.<|endoftext|>This paper proposes a new model called GRDA to relax the uniform alignment which ignores topological structures among different domains. The authors also generalize the existing adversarial learning framework with a novel graph discriminator using encoding conditioned graph embeddings. Empirical results on both synthetic and real world datasets demonstrate the superiority of this method. Some specific comments are as follows:Pros:  Paper presents interesting ideas on how to utilize the original topological structures among different domains to benefit the domain adaptation results. The idea of using a discriminator to reconstruct rather than classify is novel. Very detailed theoretical analysis is provided which makes the result be persuasive. Experiments are well thought out and highlight the parameter study which makes the results of the work reproducible. Cons:  Lemma 4.1 “Optimal Discriminator for GRDA”. Proposition 4.1 “Chain of Three Nodes”. The authors didn’t analyze why the change of the task of discriminator from classification to generation would help the model’s performance in domain adaptation.<|endoftext|>The proposed method is simple and easy to follow, and the overall idea is clearly described. Some experiments show the effectiveness of the proposed method on both synthetic and real world datasets. The authors generalize the adversarial learning framework by replacing the domain discriminator with a graph discriminator. 3.Experimental results validate the usefulness of the proposed method on domain adaption. Some concerns are listed as follows. 1.Domain alignment has been intensively studied and graph based domain adaptation also has been well discussed in the existing works. There are lots of variants on the adversarial discriminator or entropy based or distribution alignment theories. The authors fail to convince the reviewer the proposed graph discriminator is better than the original adversarial discriminator or its variants, although they present a series of theorems and corollaries in the support section. 4.If the proposed method can replace the original adversarial works, this paper could be rated a high score for acceptance. The authors should pay more attention to the advantages of the proposed methods, and how could you make your work outperform these popular works, which is also the goal of ICLR. This paper proposes a new graph discriminator to replace the conventional adversarial discriminator in adversarial domain adaptation. Some theoretical and experimental analyses validate their claims, while some concerns should be clarified and improved.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; The submission considers a reward learning problem when the reward function is not uniquely recoverable from the data, even in the infinite data regime. It is not even clear what the goal of the paper is. I think the authors should have given more effort to setup the problem and goal more precisely and concisely, while using easier to understand notation and terminologies. The paper is very hard to follow and I may not be able to assess the paper properly.<|endoftext|>This paper characterizes the partial identifiability of data sources and the reward function. This is an unconventional paper. The problem it s attacking is very fundamental and interesting: given reward functions that are "close" to each other under some measure (doesn t have to be rigorous and literal "measures"), what will they affect (in both the optimum of the MDP and the behavior of the downstream algorithms). What the paper does is to provide a set of measures and a set of claims that partially answer the questions, for the cases that are more or less low hanging fruits. I would personally be more interestied in a particular, presumably not that general, setting (say, just tabular and linear program), with a clear and complete answer to that question. To this end I would wonder if the manuscript better fits a journal publication or a book chapter etc. But I m more or less open for discussion in case this manuscript could provide some support for future studies.<|endoftext|>At the end of page 3, you mentioned the maximum entropy policy as the fixed point to $\pi_\beta   \pi_{\beta}^{\pi_\beta}$. In my understanding, a maximum entropy policy is the optimal policy result from solving policy optimization with an entropy regularization. 4.5 transformations are defined in section 2.1. So instead of characterizing the information flow through the reward function, why not directly analyze whether the information from a data source is sufficient for a downstream task? **Example application of the framework to improve upon/subsume prior results**: While the introduced reward function centered information theoretical framework provide a novel and unified view on the information theoretical relationship between different objects, it alone is in my opinion not sufficient for publication. I would suggest the authors throw a majority of the theorems and transformations in section 2 and section 3 into the appendix, and use the space to set up one or two concrete examples on which the proposed framework can be useful in deriving new/matching upper and lower bounds. 1.Discussion of related works can be significantly improved. In summary, I think this paper provides some novel insight in the problem of reward learning, but there can be substantial improvements to be made to make the paper significantly stronger. I would highly suggest the authors make the additional effort, and I know it s gonna be a lot of work. But it will potentially make it a spotlight/oral paper instead of a borderline.<|endoftext|>In particular, by considering the infinite data limit of the data source, they study the level of reward ambiguity that can be obtained for a given downstream task. For example, for the expert behavior data source, they characterize the reward transformations that are determined by the optimal Q function. However, this work makes substantial contributions by conducting this study in a unified and rigorous way for variety data sources and downstream tasks. The paper is overall well written. The related work is clearly discussed, and this work is well positioned in the literature. The paper is a bit heavy in terminologies; however, it is inevitable due to the theoretical nature and rigor purpose of the paper. 4/ All the results are derived for the finite MDP setting. The reward functions are atomic to RL   understanding the theoretical limits on how much information can be extracted from various data sources used for reward learning is important.
Reject; rating score: 3; rating score: 3; rating score: 5; This is an experimental paper that seeks out to investigate information processing in multi path networks i.e.networks such as ResNet, EfficientNet, Inception style. The authors used logistic regression probes (LRP) and representation saturation as metrics to power their analysis. Through the analysis of a network composed of many multi path networks with differing number of layers or different receptive fields, the authors demonstrate that shorter pathways often dominate longer pathways. However, it seems as if the results are very specific to this network. However, there are no real recommendations to take these results onboard. I feel that the observations do not lead to significant insights that may help advance the field. However, in the current format, it is hard to see how the results of the experiments generalise to networks used in the literature, and which aren t directly consistent with the network used for experiments. Furthermore, the analysis is very similar to Richter et al.and the main novelty is its application to multi path network.<|endoftext|>optimizing convolutional neural network architectures without training by receptive field analysis. The study focuses on problems like: (a) does learning depend on receptive field and/or depth of various branches, (b) how features differ in representation at different stages of the networkThe paper has following strengths:1. Studying multi path networks with respect to receptive field and depth is an interesting problem. The paper has following weaknesses:1. For instance, their first contribution in introduction: input size vs. receptive field size (as cited by the authors themselves) was already published in Richter et al.2021a [R1] and 2021b [R2]. The second contribution in introduction on preferring shorter pathway over longer pathway due to vanishing gradients: At first, this is also obvious since many papers (starting with the ResNet paper [R3] itself) have established that consecutive convolution layers without skip connections will result in diminishing gradients. In such more complex cases, the conclusions of this study will not hold. If both branches in the multipath network are identical, then of course they will move the solution at a similar pace and will have high redundancy. The authors have conducted all experiments on CIFAR 10 and ImageNette datasets. Note that, most statements made in the paper themselves are correct and well supported. The reviewer has concerns about the claims being obvious. In the current form, the study seems inconclusive. Two very important papers in this direction have not been discussed. They derive theory on DenseNet type models but show empirical results for MobileNets/ResNets as well.<|endoftext|>This paper analyzes the distribution of information processing in multi path networks (including skip connection models such as ResNet and DenseNet). They apply logistic regression on the hidden layers, namely logistic regression probes, to track the progress of the intermediate solution quality, and they analyze in which condition (depth of the path and receptive field size) the neural network prefers to skip the paths. They also measure the CKA similarity of the hidden representations learned by the multi path model when the paths are homogeneous. They claim that with their analysis, later layers in ResNet and DenseNet can be skipped as pruning due to the unproductive layers. In addition, they find that for multi path model, the shorter path is dominant when the depth of the paths are very different, and when the pathways are homogeneous, the behavior of the pathways changes to a coexisting behavior. Strengths:1) The motivation is interesting that we can design better neural networks without training. It is not necessary to discuss the heterogeneous and homogeneous cases since the paper mainly cares about the receptive field size and the depth of paths. 3) There re multiple typos and errors in the paper. Deep  drummer : Generating drum loops using deep learning and a human in the loop. Although the motivation of the paper is interesting, the analysis results of the paper is weak and not that surprising. In addition, part of the experiments are redundant since it is mentioned in the previous paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; They provide a definition for episode hardness, and investigate the performance on easy and hard episodes. I believe that another round of review would be required to address these issues, so I do not recommend acceptance at this stage. They then propose two strategies to mitigate this, based on adversarial and curriculum training. To summarize, albeit some clarity concerns resolved during the rebuttal, I still find the narrative of the paper unclear and the proposed approach not sufficiently well motivated (please refer to my latest response to the authors for details). I also agree with reviewer uYEr that OOD tasks are a natural candidate for ‘hard’ tasks. Detailed comments below:  There is a disconnect between how the work is motivated, and the proposed approach(es). ‘We first order the test episodes in decreasing order of hardness’   how is this done? Answering these questions is important for interpreting the results, and unfortunately this information is missing from the paper. This should be clarified.<|endoftext|>In this paper the authors present an investigation of the relations between meta learning methods and hard episodes. They found that (i) there is a large gap in performance between easy and hard episodes, (ii) hard episodes are forgotten more easily than easy episodes, (iii) a comparison between adversarial training and curriculum learning strategies show that the former are more effective in improving the performance gap between hard and easy episodes. The authors do not dive into this link at all, neither in the related work section nor in the experiments. (iii) The authors compare adversarial training and curriculum learning strategies, but other strategies could also be tested. This strategy would be similar in spirit to the Focal Loss (Lin et al.2017).Nature of hard tasks. OOD MAML: Meta learning for few shot out of distribution detection and classification.<|endoftext|>The experimental protocol and the ablation studies also complement the narrative well. In this paper, the authors analyze the hardness of different episodes for an episodic training regime in the context of a meta learning training (for few shot classification tasks). Strengths In this paper, the authors tackle a problem in meta learning setup that has not received enough recognition   performance of the meta learner on easy vs hard tasks in a collection of few shot tasks (used to train the meta learner).<|endoftext|>The investigated topic is interesting and relevant to the few shot classification community, and the writing quality is good. Some experimental details were not clear from the paper s description:  **[major]** In Section 5, were easy and hard episodes sampled from training or test classes? This is a crucial distinction, and it determines whether I agree or not with the use of the term "forgetting". I had to scroll down to Appendix B.1 to determine what "ways" value was used for Figures 1–5 (5 way), and I could not find what "shot" was used. Can the authors clarify what numbers are used in drawing this conclusion?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This makes it hard to make a good comparison between the methods. This paper introduces some nice ideas for uncertainty estimation with distillation, but it fails to make comparison to many more related existing works and it has some clarity issues. In the Introduction section, there is this statement "inputs it is expected that the trained model parameters can return reliable predictions." The good story flow in the paper makes it easy to read and follow.<|endoftext|>The multiple teacher predictions can be generated through by adding multiplicative Gaussian noise. * Significance: The distillation itself is very similar to [1] and the authors could make a better job in detailing the difference of it. comparisons to other distillation approaches or state of the art (efficient) ensembling methods. Hyperparameter ensembles for robustness and uncertainty quantification. However, at this stage, I found the paper lacks in clarity, significance and thorough evaluation (see weaknesses above for details).<|endoftext|>I would like to see more of a theoretical investigation of what kinds of uncertainty are actually represented by the estimates. This makes me wonder if there is a problem with tuning the baselines, which might make the rest of the results less reliable? I think the paper could use a more careful grounding in prior work which investigates the contributions and their theoretical motivations more carefully. Are you able to construct experiments that show that this is the source of the benefit?<|endoftext|>This paper contributes to neural network classifier training and uncertainty prediction. These components exist in the literature. However, since the proposed method is like a combination of existing techniques.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The experiments showed that AlphaZeroHER is better than AlphaZero on BitFlip, 2D Navigation Task, 2D Maze and Quantum Compiler environments. This paper proposes to incorporate HER into AlphaZero to solve the problem of goal directed planning under a deterministic transition model. Their method is particularly eﬀective for those sparse rewards problems as demonstrated in the experiments. While the method is interesting, the following comments need to be addressed. * In this paper, M subgoals are sampled for training.<|endoftext|>In the paper: "The basic idea is to neglect its on policy nature generating additional training samples at the end of each episode by sampling additional subgoals from the visited states." How exactly are subgoals selected? The authors have done well to address and rectify  the potential issues around the on policy feature of AlphaZero with HER. I m somewhat on the fence as I do believe this a worthy contribution showing clear results, however I think that this paper would benefit from a bit more clarity around the algorithmic details and would strongly recommend that something be included to this effect at least in the appendix, but more preferably in the main paper.<|endoftext|>The authors demonstrate performance on a handful of goal directed planning tasks—including BitFlip, navigation, and a quantum compiling task—on which they show improved performance over AlphaZero without the addition of HER. The number of states that the agent has visited? This seems like the simplest thing one might do in this situation (and seems quite reasonable given the challenge of the alternatives), but it would help me to be certain of my understanding. In particular, most (if not all) of the experimental domains studied here are studied in other domains. More results comparing AlphaZeroHER to existing baselines would greatly strengthen the paper. Making the language more precise in the abstract in particular will help guide the reader. The network definition at the end of 5.2 is unclear.<|endoftext|>My main comment is that the technical and empirical novelty of the paper is relatively low. The paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017). My main comments remain the same.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper presents a video prediction method that combines a transformer based auto regressive model with a pre trained image generator for high resolution frame generation. The auto regressive model is trained to predict the discrete VQGAN s latent codes which is then used by the image generator to generate patches in the video. Each technical component is not new and the way of combining them has also been explored before, which limit the novelty value of the paper. Strengths:   + The paper is well written and easy to follow. Weaknesses:   + While I appreciate the good results achieved by the proposed method, my major concern is that the novelty of it is somewhat limited. At the high level, the idea of the proposed method is similar to [1], only with VQ VAE replaced by VQ GAN.<|endoftext|>This experimentally results in an efficient training procedure, with a model that is demonstrated to be able to generate high resolution videos and match or outperform the state of the art in the domain on multiple datasets and tasks. The proposed method is an interesting contribution for the community, since it reduces the complexity of learning video prediction models with appealing results. Therefore, I choose to maintain my score. ### Post Rebuttal UpdateI acknowledge the authors  response and would like to thank them for their answers. Overall, the paper is well written and easy to read, and experiments seem to be reproducible. However, I find the contribution of this paper to be insufficient for two main reasons. ### NoveltyThe first limitation deals with the novelty of the method.<|endoftext|>Is it the number of layers in VQ GAN or the 1 D Transformer? The technical novelty of the paper is arguably only the VQ GAN encoder. **Strengths*** The paper is well written and easy to read and the proposed framework is conceptually simple. The authors should show more empirical evidence that this component is crucial for their results on video prediction.<|endoftext|>Specifically, they leverage a pre trained image generator (VQ GAN) and fix it in the video prediction stage. The predicted codes are generated by a progressive causal transformer. Strengths:* The paper is well written and easy to follow. * The idea is simple and clear. Also in the paper, the predicted videos are only shown with some selected categories. (e.g.examples on more categories and/or FVD scores on the whole dataset) This paper introduces a new method for high resolution video prediction, however, the experiments are not sufficient, I believe more experiments are needed to justify the effectiveness of the proposed method.<|endoftext|>All in all, I m leaning towards a rejection because I m not convinced that the results of the paper warrant a full conference publication and might be better suited for a workshop, but I m curious to see what the other reviewers think. However, the novelty value compared to previous approaches is very limited. The demonstration that such a simple approach can give (near) top results is definitely interesting. Is it possible to train only the policy network while fixing the (pre trained) transformer model?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper studies an interesting problem of estimating preference shifts induced by recommender systems and proposes a distance metric for the shifted preferences based on the so called "safe shifts". It is well motivated. ## Weaknesses  Why don t the authors deploy some wide used simulators, such as Virtual Taobao [1]. The literature survey about the simulator in recommender systems should be considered. It is always noisy if the experiments are based on the simulation. If there is no solid evaluation manner, a proper solution is to plug this method into some commonly accepted recommendation pipelines. For example, you can first use the estimated preferences shifts to improve the backbone recommendation models and then evaluate based on traditional evaluation manners of recommender systems. In the current version of this work, it is still unclear whether the method work or not. There is no competitive baseline method in the results (Table 1 and Table 2). The complexity of the proposed approach is unclear. I agree with the reply to the 1st, 2nd, and 4th, while I partly agree or disagree with the 3rd and 5th. Overall speaking, this paper studies an interesting problem of estimating preference shifts induced by recommender systems. Therefore, I recommend rejection.<|endoftext|>In this paper, the authors argue that it is important to 1) estimate the impact of recommendation systems of user preferences, 2) evaluate if the shifts would be undesirable, and 3) optimize to avoid undesirable shifts. The authors propose a method to do this and rely on simulations to evaluate it. I do not wish to reiterate the strengths and weaknesses as described by the other reviewers. Instead, I will pose a set of points for the authors to consider as they revise the paper. More subtly, there is a difference between consumer preferences and consumer behavior [2]. This paper does not distinguish between the two. Even when consumer preferences shift, do RSs really influence users? Prior work with Amazon data suggests that at least 75% of observed activity would likely occur in the absence of recommendations [3]. Additionally, showing similar results under different simulation assumptions would go a long way to dispel concerns about the use of simulation methods. "Implicit consumer preferences and their influence on product choice." "Estimating the causal impact of recommendation systems from observational data." Otherwise, it addresses an interesting problem in a new way.<|endoftext|>This paper discusses an interesting but important problem in recommender systems, i.e., the users’ preference shift under the influence of recommender systems. Instead, authors provide a simulation framework that enables system designers to estimate the induced preference shifts an RS; evaluate its influence before deployment; and actively optimize to avoid such shifts. 2.The simulation framework, though with simplification and assumptions, is elaborately designed and seems promising for handling the preference shift problem. In Sec.4, when introducing estimating users’ preferences, the inference process is demonstrated in detail. 2.Experiment details are also not clear. It seems to me that this framework can improve engagement under the initial or naturally shifted user preferences according to Table 1/2. I would say that the current version of the paper is marginally below the acceptance threshold, but I am looking forward to the authors addressing my concerns above in their rebuttal. Also, I expect the authors to format their released experiment code, such that followers can better understand the model and simulation.<|endoftext|>The authors address the challenge of preference shifts, a potentially undesired effect of recommendation systems (RSs). This can happen, e.g., by positive feedback loops. The main challenge of tacking this undesired effect is that we lack the “natural preferences” or a robust way to predict preference shift given a recommendation policy. Reasons to reject:*The results rely on modeling assumptions, for instance, their choice modeling. *The work has many limitations (despite that the authors address most of them). For instance, I am not sure that random recommendation is a good benchmark to test for preference shifts; the authors did not motivate this selection well enough. I think the topic of this paper should be a core one at the ethical ML research direction, and yet it does not receive enough attention (especially not from industry). Overall, I think the community can benefit from the ideas presented in the paper, so I vote for (weakly) acceptance. There are several typos in the paper; here are some of them: page 2: the \tilde sign is presented incorrectly with the percentages.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper studies signal propagation questions in wide fully connected networks with layer norm. This analysis is then used to comment on the connection between the exploding gradient problem, choices of parameter initializations, and a phenomenon that the authors called "representation shrinkage." This paper studies fully connected networks in the infinite width limit with layer norm applied at each layer before activations. Further to my point, as the authors discuss in their introduction, the infinite width analysis will break down anyway for large enough depth. If possible, it would be nice if the claims about the importance of preventing this were further supported   either theoretically or empirically   by an analysis of its effect on training.<|endoftext|>The paper studies the operator norm of the input output jacobian fully connected networks with layernorm. Using this result, they argue that for very depth neural network, either this norm is very large or the network s representation power shrinks. In sum, there seems to be a large gap in the proof of the main theorem. Inspired by the "mean field" approach (e.g., Poole), the authors study the lower bound of the operator norm of the input output Jacobian map in the large width limit and discover a trade off between representation power and trainability (exploding gradients). I think these two limits are not unchangeable in general. The third/fourth cases seem to be a finer analysis of the critical regime.<|endoftext|>A similar lower bound is derived for residual networks. This bound can be used to deduce some implications for training of neural networks. Main concerns:  The authors only have included layer wise normalizations in the definition of neural network architectures which seems to simplify the proofs, but it is not clear whether the results are applicable to commonly used networks that do not include such normalizations. I believe since it is used so often, there should be a mention of it or at least a clear pointer to the equation corresponding to this upper bound in the appendix. As the authors have mentioned in the literature review, there are many works that consider signal propagation through wide neural networks of different architectures in the past such as Lechao Xiao et al.[Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000 Layer Vanilla Convolutional Neural Networks]which consider convolutional networks and previous works that consider fully connected ones. The authors mention that at least one of these effects will necessarily occur. But the main issue I have is that the neural network considered with normalization seems not to correspond to networks that are commonly used.<|endoftext|>1.This paper studied the signal propagation in a deep fully connected neural network using the mean field formalism, which assumes that the network width goes to infinity and the parameters are i.i.d.Gaussian.Theorem 1 proves that in a non linear network, increasing the depth leads to either gradient explosion or representation shrinkage. 3.On MNIST, it s shown that one can replace several non linear middle layers of a deep network with a single linear transformation, and still gets a similar training trajectory. 8.At the end of Section 5.2, what do you mean by "training should be started from a linear network"? But this paper is not well written and many parts are very confusing to me (as pointed out in the main review). I will consider raising my score if my questions are well addressed. Thanks for the response.<|endoftext|>A mean field analysis of deep feedforward networks is given, which demonstrates a tradeoff between gradient stability, expressivity, and the choice of activation functions. This tradeoff is experimentally explored and verified for several network architectures, and high level insights are given for designing better deep networks. ### Main Comments* Beyond the mean field assumption, the bulk of the analysis in the paper is made without any additional assumptions, and applies to networks of any depth or activation function. * Although the main result is somewhat technical, the summarization of the proof method and subsequent takeaways of the result are done in a very reader friendly manner. ### Minor Comments* In Figures 2a and 3a, gradient shrinkage (as measured by 1   \rho) relative to depth is bounded by a power law relationship (blue curves), but in both cases the empirical behavior converges to a different power law with faster decay (black curves). The former prediction comes from Theorem 1, but could the authors elaborate on where this latter power law might come from?
Reject; rating score: 3; rating score: 5; rating score: 6; The paper proposes an adversarial inverse reinforcement learning algorithm that learns purely from expert demonstrations, and does not require any online interaction with the environment or a dataset of unlabeled interactions with the environment. I would consider raising my score if the paper included experiments with different tasks and compared to at least one of the prior methods listed above. Update Thank you to the authors for adding the experimental comparisons to BC and RED on the Hopper and Walker2D simulated locomotion tasks. Unfortunately, it seems that the BC and RED baselines substantially outperform the proposed method on those two tasks, so I will keep my original score.<|endoftext|>### Strengths ###   The problem that the paper tends to address is challenging and is very relevant to the ICLR community. The paper is overall clearly written and easy to follow. ### Concerns ###  The approach that the authors propose is very interesting. The analysis of LogReg IRL seems to only apply when learning classifiers that distinguish between trajectories generated from uncontrolled and expert controlled transition probabilities. The experiments need more work. LogReg IRL is affected by baseline trajectories in theory. ### Post rebuttal comments ###Thank the authors very much for responding to my questions and adding more experiments in a short amount of time. However, the results do not seem very promising and I am still concerned with the theoretical grounding of the proposed algorithm. Therefore, I would like to keep my score.<|endoftext|>### WeaknessesThe main weakness of this work is the sparsity of the experimental section. A baseline that should be compared to is behavioral cloning (BC). The setting of this work is under an offline setting using only expert trajectories which is the same for BC; thus, I am curious if BC performs on par with SOLO IRL? The authors demonstrate that the reward function is better shaped than LogReg IRL, but they do not showcase how it can be used. It would be interesting to see if the learned reward can be used to transfer to like tasks? One additional concern is the sparsity in tasks and baselines in the experimental section. It would be nice to see an ablations section to understand how sensitive/hard to tune that is, as compared to selecting baseline trajectories?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper studies the problem of out of distribution generalization by modeling uncertainty in feature statistics. Instead of being deterministic values, the feature statistics are hypothesized to follow a multivariate Gaussian distribution. 1.Overall, the paper is well written and nicely organized. 4.Some parts of the paper feel repetitive. In summary, the method is simple yet effective, showing strong performance on various benchmarks.<|endoftext|>Strengths:(1) The proposed method can capture the uncertainty of the feature domain, which is very important to address the domain shift issue. Weaknesses:For the method:(1) The proposed method is simple and is similar to batch normalization. Can traditional batch normalization method also improve model generalization ability? Is the proposed method significantly better than traditional batch normalization? (3) We may expect more discussion on why the domain generalization can be improved by modeling the probabilistic distribution of features. Is there a specific theory or reason? (3) For the image classification problem, the performance of DSU drops on Photo style compared with the baseline. Are there some explanations? Overall, this paper is well written and well organized for both theories and experiments.<|endoftext|>In this paper, the authors proposed to model uncertainty with multivariate Gaussian distribution for better network generalization. 2.Instead of sampling from a Gaussian distribution, can we just normalize it to be an uniform destruction? Several visualizations and analysis also illustrate the effectiveness of the proposed approach. The proposed approach seems to be a small change to AdaIN.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; Standard entity linking first does mention detection and then the actual linking of mentions to entities in the knowledge base, and the authors propose to flip this by using open domain QA. This paper is clearly written, has a working original idea, and provides a great new option for entity linking that has empirical promise. It s in good shape, and I don t have a lot of criticisms. It could be "The". Solid paper about an original approach to entity linking that performs pretty well.<|endoftext|>This paper proposes a new paradigm for entity linking. The proposed method EntQA retrieves candidate entities first, and then uses candidate entities as queries to find mention spans. EntQA addresses the issue of finding mentions without knowing their entities and offers many practical advantages. Strength:* The novel entity linking paradigm proposed by this paper is insightful and the performance gain is significant. I suggest the authors clarify the definition. The insights and novel paradigm for entity linking proposed by this paper are attractive. I recommend accepting this paper.<|endoftext|>Entity linking involves connecting entities to their mentions in test passages. The authors address this challenge by rearranging the standard QA pipeline by learning questions first (finding entities) and searching for answers (mentions) at a later stage. This approach appears to be well researched. This paper addresses a problem of entity linking. Instead it finds entities from text passages and then finds lined mentions in test passages.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The models allows to decode the network output by varying this parameter. **Weak points:**The results section is a bit underwhelming. The results demonstrate the bare minimum that a) the approach works, matches the performance of other models b) how the performance changes when we vary p_max. Should it be p_min? The paper has an interesting idea to condition the Neural ODE on the parameter p that can be interpreted as the “depth” of the network. The Results section does not explore the new architecture enough and provides only the basic results to demonstrate that the model works. I suggest the acceptance of the paper, but recommend the authors to expand the results section.<|endoftext|>The paper aims to tackle the problem of modeling a continuous time dynamic system, using a category of neural networks. Instead of solving a two point boundary conditions problem (as in regular Neural ODEs), the authors solve two initial value problems for the same task. Equations 21 and 22 describe an alternative scheme which is not actually used, and I believe they may be omitted without hampering the flow of that section. The experimental section is not as clear as the rest of the paper. Overall, this is a good paper which provides an exciting novel architecture, with good theory and some simple experiments to justify the quality of this architecture, with comparisons to related work.<|endoftext|>In recent years, continuous depth neural networks, such as Neural ODEs have helped understanding of ResNet in terms of non linear vector valued optimal control problems. The strength of the paper is that it introduces an alternative view to the neural ODE, by reducing the non linear, vector valued optimal control problem to a system of forward facing initial value problems. Second, I think another weakness of the paper is that it does not seem to provide a rigorous treatment. There are two weaknesses of the paper.<|endoftext|>The authors also give a practical implementation for training the InImNets architecture. This paper studies a very important and interesting topic, continuous depth neural networks. According to the current experiment results, I am not sure what is the superiority of the proposed InImNet compared to existing approaches. For example, in Table 1, both the InImNet and other existing models can achieve similar performance. 3.According to the implementation of InImNet, it seems would take more calculation consumption for training the InImNet.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; It is not entirely clear to me how the network can generalize to multiple spatial resolutions: more specifically how the encoder and the message passing mechanism handle a varying number of nodes (i.e.cells) in the graph? The paper proposes a framework to solve generic PDEs written in conservation form, in an autoregressive fashion. The network is trained with an additional loss which promotes zero stability of the solver, by constraining the network to be invariant to small perturbations in the input solutions of previous time steps.<|endoftext|>This paper proposes a message passing neural network to solve PDEs. The proposed method also adopts notable training methods to reduce error propagation. The paper is very well written with: (1) thorough and detailed literature review; (2) intuitive explanation of motivation; (3) clear methodology and approach used in designing the architecture; (4) extensive details in empirical study and design of experiment. Overall, it is a good paper with a complete analysis of the design of architecture, training/testing approach and extensive experiments.<|endoftext|>The paper addresses the stability of training neural PDE solvers. This helps to be closer to classical numerical solvers. I.e., the paper proposes two separate innovations, puts one into the title, but experiments confirm the effectiveness of only one. Does the proposed model help with this respect? Comments:  There is no ablation study (the loss or the architecture?) The dependence of the properties of the method on the number of parameters in the model is not studied (at least, comments are welcome). Reasonable idea, which proposes a loss that improves stability; however, it has two disconnected contributions and is not so easy to read.<|endoftext|>The paper utilizes neural message passing to propose another neural solver for PDEs. In particular, the METHOD section is very vague and not clearly explained. Also, it is not clear what is the difference between using GNN and other regular networks in solving PDEs the authors did not mention that in the paper. The paper has a clarity issues and many notations are not introduced or defined. Edit  The authors either explained or addressed the issues in the paper with other reviewers.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper presents a new architecture for image classification neural networks based on various forms of image tessellation. They show that this architecture results in better  black box  adversarial robustness. The paper presents a novel architecture for image classification using image tessellation. This is interesting but warrants more work to understand why tesselation reduces black box transferability. This work presents an interesting architecture, but does not support the central claim of adversarial robustness because they do not present white box results, and do not specify on which architecture the black box attacks were derived.<|endoftext|>This paper proposes to split the input image into tiles that are fed into separate subnetworks for feature extraction, and to form an implicit ensemble by combining the results from the subnetworks for a final prediction. The paper claims that such ensemble can improve adversarial robustness of the model. * The choices of the adversarial attacks are also problematic as recent strong attacks are excluded from the empirical results. ### Minor issues* It is not clear if the models are obtained after adversarial training or not. The reviewer was unable to find explanation for this in the paper.<|endoftext|>The first concern is whether this method is still effective for a high resolution image. This paper presents a tessellated convolution network that is more robust to adversarial attacks. The idea of this paper is simple and intuitive. The overhead will also be an issue if the number of tiles needs to be significantly increased in that situation.<|endoftext|>This paper studies defense against adversarial attacks. Weaknesses:  My #1 concern is that the method is not validated on larger datasets such as ImageNet. Are these typos of "MNIST" and "CIFAR 10"? The proposed method is quite simple, straightforward, and looks to be effective.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper proposes a general framework for so called "out of distribution generalization" that can handle non linear associations and causal links between variables. Therefore I recommend acceptance. This would be an invariant causal association across whether the characters were printed, hand written, displayed on a screen, blue, purple, etc.<|endoftext|>This paper proposes Invariant Causal Representation Learning (iCaRL), which learns causal representation for OOD generalization and can be viewd as a generalization of IRM to the settings with both nonlinear representations and nonlinear classidiers. It would be much better to include one more real world data set (e.g.PACS) There seems to be some technical flaws in the proposed algorithm and the experimental results could be improved by adding some popular benchmark  real world data set. The problem is important but part of the algorithm seems not technically correct.<|endoftext|>This paper proposes invariant Causal Representation Learning (iCaRL) for OOD generalization in the nonlinear setting. The work extends iVAE to a somewhat more general setting and shows the direct cause of the target can be discovered. I look forward to authors  response. The algorithm part seems less justified. Or do you mean conditional on E? However, it is not clear how practical the assumptions in Theorem 1 and 2 are.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The paper (claims to) prove that the proposed method can correctly identify within finite steps the underlying structure. ### Other comments and questions  Beginning of Section 3: "In stark contrast to our results, it is actually well known in convex optimization that those algorithms based on proximal stochastic gradient are *unable to identify the manifold within finite iterations*". The simulations then show that the method outperforms existing packages in identifying structured sparsity without compromising prediction accuracy. The new method does not incur computation additional to SGD with momentum and could be of practical use. Update after author s responses and revision: Most of my concerns have been addressed in the revision, with (1) a more rigorous reframing if the significance and importance of the work, and (2) experiments to validate the efficiency of the approach. I think this is a good paper in its current state. In the experiments, they conclude that "RMDA is the only algorithm that steadily identifies the correct structured sparsity". I think those are not correct characterizations of the results obtained in this work:      Roughly speaking, Theorem 2 for structured sparsity of neural networks just proves that:         *If the estimator converges to some stationary point W, then its sparsity converges to the sparsity of W within finite step*      Similarly, the experiments just show that RMDA has a stable structured sparsity    While these results are non trivial, they concern the **stability** of the algorithm, and have little to do with **correctness**: a bad algorithm that got stuck at a non optimal stationary point also satisfies those properties. My main question is: Is W* the same for all realizations of randomness?<|endoftext|>In this paper, the authors propose Regularized Modernized Dual Averaging (RDMA) algorithm to train structured neural networks. The algorithm does not require any extra hyperparameters than what stochastic gradient descent requires. Authors theoretically prove that structure identification is guaranteed after a finite number of iterations. The proposed algorithm is well motivated. 2.The proposed algorithm is a unique and different take on structured learning. Experiments are limited. It is understandable that the main goal of the paper is to share a new proximal based method for structured learning but it will be good to have a few more experiments. 2.It will also be good if comparisons with other structure learning based methods are done.<|endoftext|>The proposed RMDA can be viewed as a non trivial extension of the dial averaging algorithm to use momentum. can the authors explain what they meant by variance reduction and where in the paper they showed that? Also, despite claiming that RDMA s cost is the same as SGD, keeping track of momentum and dual averaging makes it slightly more complex than SGD (although still better than other variance reduction techniques)  **Guaranteed strucutre identification**: I find this slightly exaggeration of the results as there is no theoretical result on the convergence of the RDMA at all. Theorem 2 assumes that the algorithm converges, and under some assumptions, the converged parameter belongs to the active manifold. **Superior empirical performance**: "RDMA identifies the optimum structure" is hard to prove, as there is no way to show that the structure found by any algorithm is the best and optimum, even though it is better than baseline. 2.The theoretical analysis is not complete, and is based on the restrictive assumptions and expecting that with the given values of $\\beta_t$, $\\eta_t$ the algorithm converges. 4.Section 3, It is stated that in convex optimization, "algorithms based on proximal stochastic gradient are unable to identify the manifold within finite iterations". or the way that momentum is incorporated in developing the algorithm? Using tools from non linear optimization and manifold identification, they theoretically showed that if RDMA converges, then it will find the active manifold after a finite number of iterations. Experimentally, they showed that RDMA outperforms proxSGD and ProxSSI in terms of both accuracy (sometimes) and group sparsity (always).
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The paper does this by introducing the Winogradversarial dataset and documenting performance. Strengths:The paper seems to introduce a new dataset that could be very useful to the community. The remainder of the paper continues the same way. This is a pity, since it seems like the authors have some interesting results to present. They are too unclear for me to be secure in restating them.<|endoftext|>(Maybe in the experiments with the 20 sentence pairs?But I wouldn t consider those enough evidence.) However, the authors also seem to argue against finetuning as a general approach for commonsense reasoning, though I don t see how this is backed up by the paper (except for maybe the small experiment with 20 sentence pairs?). Additionally, some of the statements made by the authors seem wrong or at least misleading:  The authors argue against finetuning by proving a quote from Bender and Koller (2020) on page 3, but I don t see how the quote is related to finetuning as opposed to general language models. The authors could design more experiments backing up their main claims (e.g., about compositionality).<|endoftext|>The paper investigates the performance of the Albert pretrained language model on Winograd style tasks. They find that it outperforms other models on some datasets and conclude that this is due to the Albert model reusing parameters. The paper appears to be more of an opinion piece and lacks a clear novel contribution. Each of these could be responsible for one model outperforming another on a particular dataset. The experiments in 2.2, using only 20 sentence pairs, are not adding value to the paper.<|endoftext|>I encourage the authors to pick just one or two of these ideas, and flesh them out in a more thorough, convincing manner. In its present form, I believe this paper should be rejected. Highlights the shortcomings of fine tuned language models on this adversarial common sense reasoning dataset, and the relative strength of pretrained models. It is assumed that researchers are following the established principle of not peeking at the evaluation set, this does not need to be explicitly called out.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper makes some analysis on the convergence behaviors of SGD with label smoothing in deep learning. Further, a two stage label smoothing algorithm is proposed to improve the convergence. Weaknesses:For the theoretical part, this paper does not make analysis from the overfitting perspective and builds on some assumption that “It seems that training with smoothed label in the late epochs makes the learning progress more difficult”. The baseline SGD without LSR also should be added into Figure 1. mixup: Beyond empirical risk minimization. Further, much more experiments should be added to prove its effectiveness.<|endoftext|>This work theoretically analyzes the convergence behaviors of stochastic gradient descent with label smoothing in deep learning, which implies that LSR may slow down the convergence at the end of optimization. The authors propose the two stage label smoothing strategy to further improve the convergence. (2) The theoretical analysis for label smoothing understanding is interesting. (3) The proposed TSLA is simple yet effective on several small datasets. Otherwise, the presented results are not convincing. Is there any theoretical analysis? (3) How about the generalization of label smoothing? It is a combination of two commonly used strategies. But the analysis from the optimization aspect is interesting. For the label smoothing strategy, this paper theoretically analyzes the convergence behaviors and proposes the TSLA. More experiments are needed to verify its generalization.<|endoftext|>This paper proved that when performing stochastic gradient descent for a non convex objective function, an appropriate smooth parameter might speed up the convergence. The authors then proceed to introduce a two stage TSLA algorithm which firstly learns with LSR. * **Quality** Overall, this is a novel work that provides insights on understanding the convergence behavior of label smoothing regularization. When optimizing with SGD, it is theoretically depicted that an appropriate LSR which reduces the variance can speed up convergence and require less sample complexity. It would be better if the authors could provide me with more explanations or empirical validations regarding this threshold. Clearly, if we adopt a smaller value of $T_1$, the performance of TSLA is very likely to perform worse than learning without LSR. This paper is overall an interesting paper. Although I still have some additional concerns regarding the threshold of $\delta$ in Theorem 3, and empirical aspects of $T_1$ and $\theta$ in the proposed TSLA.<|endoftext|>The convergence results show that an appropriate LSR with reduced label variance can help speed up the convergence. Integrating TSLA with SGD can produce the improved convergence result that TSLA benefits from LSR in the first stage and essentially converges faster in the second stage. Extensive experiments show that TSLA improves the generalization accuracy of deep models on several benchmark datasets. According to the proposed theoretical analysis, this paper further proposes a two stage label smooth method to enhance the generation ability of LSR. However, the authors need to deal with some issues as follows:4. In the experiment with different theta, the case with theta 0.1 should be provided in the experimental results, which is a normal setting in the LSR.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; The paper offers a simple method for estimating the performance of deep models without test data using GANs. The empirical results are strong and the analysis is interesting. This paper proposes a simple method for estimating the generalization gap via training a GAN. The central idea of the paper is straightforward and the presentation of the idea is very clear. I believe that many of these can be merged. Along the same line, how does the estimated change as the function of different samples from the GAN?<|endoftext|>A further empirical finding is made on the greater similarity between test and synthetic samples than test and train samples, as measured by FID. A variety of GAN architectures are explored. ##########################################################################Pros:  * Predicting generalization is an timely and relevant problem. * No theoretical justifications are made for the observations. This is often the case for new empirical findings, but the paper would be stronger with some theory. Interesting and relevant empirical results on predicting generalization with a simple technique based on synthetic data generated by GANs.<|endoftext|>The paper considers the problem of predicting the test performance of neural networks, given only the training set and model hyperparameters. While there have been many methods proposed for this, the paper considers the simple baseline of generating synthetic test samples from a GAN and computing error using these samples. This baseline appears to outperform all or almost all existing methods on a variety of tasks, including a recent competition on predicting generalization. I want to make it clear that this is a minor weakness I don t think the papers strong results should be held against it.<|endoftext|>This paper proposes to use synthesized data generated by GANs to estimate the generalization of image classification models. The authors argue that the GAN samples are closer to the test set than the training set. The proposed outperforms existing methods on both PGDL and DEMOGEN benchmarks. 2.As far as I know, the proposed method is novel. 2.The authors do not provide any statistical analysis of the proposed method.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper looks at the problem of improving the generative quality of GANs. + The paper is generally well written and quite easy to understand. It does however mix preliminary work (I FGSM) with the proposed contribution in 3.2. Does the presented method work on larger GAN architectures such as StyleGAN2 or BigGAN? Does the presented method actually address the issues highlighted in Fig 3? The paper explores an interesting idea of adding adversarial robustness into GAN training to improve latent distribution sampling and diversification. Unfortunately, the paper falls a bit short in the experimental validation of the approach, and comparison to prior approaches. However, I still do not see what the paper adds on top of baselines, or how the problem setup in Figure 3 (interpolation artifacts) is actually addressed. The rebuttal mentions some experimental evidence that seems to indicate latent space sampling can helps. However, I would need to see these results in an actual paper submission for review to feel comfortable about accepting it. As is the paper seems interesting, but not ready for publication.<|endoftext|>This paper applies two existing techniques derived from adversarial example literature and MSGAN to improve the quality and diversity of generated samples by GAN. This paper introduces their motivation convincingly with some easily comprehensible figures. However, the experiments to show the effectiveness of the proposed method are basically performed only with small scale datasets, so that it makes difficult to figure out how complex manifold in the target space can be handled by this approach, in particular, it is unclear how it can scale to the ImageNet dataset. The paper is well organized and easy to follow the authors  claims but the qualitative comparison results seems difficult to be interpret as the captions state. And from the perspective of transformation of the latent vector z, there can be more discussion in the comparison with other methods such as StyleGAN that transform z before putting it into the generator.<|endoftext|>This work proposed a sample shifting method in GAN, formulated as adding intermediate latent space to generated pixel space. Such method is based on observation of continuous mapping limit: image quality in pixel space is not as continuous as latent space; limited latent space will incur mode collapse thus poor image diversity. The main contributions are: a new optimization problem as sampling method to improve image generation by quality and diversity, propose to use I FGSM optimization method to achieve this sampling optimization problem. Ablation study on various baseline GAN architecture (DCGAN, WGAN, WGAN GP, SNGAN) is conducted to show generalizability of such sampling method. Cons:1.Compare with baseline method MSGAN: (1) why it only compare the div+ with MSGAN; (2) improvement over baseline method MSGAN is very limited. 2.If generator trained with better quality regularization, the latent space after mapping should have better continuity? Comparison like Fig 3 after training would be needed to prove that. Fig 1.It’s not obvious what the latent space did in this figureThe paper is overall well written, and the idea is very clear and elegant. Meanwhile, I have some doubt on the sufficiency of experiment to show the improvement from quality. Also the improvement over baseline method MSGAN seems minor for me. Expect the rebuttal to clear my doubt.<|endoftext|>The authors demonstrate that the generator in a GAN is a continuous function two latent codes that are close in the latent space are mapped to two images that are close in the pixel space. However, the quality of the generated images is not preserved as quality is not a continuous function in pixel space. To address this issue, the authors propose to transform the original latent codes and demonstrate that it results in better generation quality and diversity. (https://arxiv.org/pdf/1912.00953.pdf)b. https://arxiv.org/abs/2005.02435c. https://arxiv.org/abs/1809.03627Please do a thorough literature review to incorporate any such missing work. Reliable fidelity and diversity metrics for generative models. 5.It would be good have experiments on large scale datasets such as FFHQ. 6.I wonder if FGSM is the only method this can be applied with or any other can be used as well? If so, how does the method depend on the adversarial training method employed? 8.There is no theoretical justification on why should the proposed method work?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; This work proposes using normalising flows to obtain a latent representation of the data that is trained to minimise a surrogate of an MMD [3] based statistical distance used to measure adversarial accuracy when classifying protected features. All this whilst preserving the desired properties of the original features. However that is far from the case, instead this work tries to prove that said distance can be estimated “very well” empirically. Thus all experiments in this work should be estimating this quantity as any success statements regarding fairness rely on such empirical estimates and there are really no theoretical guarantees on the fairness aspect of the method itself, it s more like there are theoretical guarantees on the evaluation of the method. It s important to emphasize that this has nothing to do with the “provable guarantees” . there s no explanation for this when it is introduced.<|endoftext|>This paper proposes a representation learning technique for fairness called Fair Normalization Flows (FNF) with theoretical guarantees on the maximum possible unfairness. FNF minimizes the KL divergence between the converted distributions of the groups, which is an upper bound of statistic distance. Strengths* Fair representation learning with theoretical guarantees for fairness is a significant contribution. * Even if the probability distribution of the input data is not known, density estimation can be used. * Experiments are extensive and show how FNF outperforms adversarial training baselines and suggests upper bound performances of any adversary. * In Page 6, how tight is the bound of the square root of KL divergence for statistical distance? * In Algorithm 1, there should be some discussion on when to stop the learning. The paper makes significant contributions by proposing a fair representation learning technique with theoretical bounds.<|endoftext|>In this paper, the authors propose fair representation learning using normalizing flows. The authors argue that improved fairness guarantees can be achieved compared to adversarially fair learning. [Strengths]  Normalizing flows are a flexible tool to disentangle the predictive information and sensitive attributes from the data. The idea of using normalizing flows for fair representation learning is interesting. This led me wonder if some of the densities do not perform well on certain datasets.<|endoftext|>This paper aims to design a fair representation learning method. In the experiments, the authors conducted extensive experiments to demonstrate the effectiveness of the proposed methods. If more tailored theoretical analysis can be provided, I think the paper can be more interesting. While I have the above concerns, the technique part in this paper is sound, and I cannot find large mistakes, thus I vote for a weak acceptance.<|endoftext|>This is justified because controlling the statistical distance amounts to controlling the maximum adversarial accuracy and additionally gives guarantees on demographic parity and equalised odds for any downstream classifier $h$ that uses $z$. Strengths of the paper+ The paper considers the relevant task of learning fair representations+ The issues with learning fair representation via adversarial training are explained wellWeaknesses of the paper+ Algorithm 1 is incomplete. Shouldn t we be updating the parameters of the classifier $h$ in Algorithm 1? Much fuss is made about controlling the statistical distance between the latent representations $z_0$ and $z_1$.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposes a communication efficient parameterization methodology for federated learning tasks through the Hadamard product of low rank weights. Such parameterization has better model expressiveness in terms of minimal parameters to achieve a maximal rank, compared with conventional low rank approaches. The work shows extensive empirical studies of the proposed method on multiple FL datasets. Questions and comments:1. Or what is the difference compared to other literature involving Hadamard product parameterization? The work proposes an effective low rank parameterization that shows the superiority of communication efficiency in federated learning tasks. The idea is relatively interesting to the reviewer.<|endoftext|>I am not very familiar with reparametrization, and I am not able to comment on the significance of this paper. I think that the idea of this paper is interesting, using the Hadamard product of two low rank matrices may only increase the model size by a factor of 2, but may drastically increase the expressiveness of the parametrization. As for the experiments, I think the experiment design is generally enough. In the experiment, the authors compare FedPara with original low rank reparametrization and without reparametrization, and the optimization algorithms are chosen to be the same, e.g.FedAvg, SCAFFOLD, etc. It may also be good to add some experiment results with compression. Some questions and small suggestions are listed in the main review part.<|endoftext|>This paper proposes a communication efficient parameterization for federated learning by using low rank weights. Several numerical experiments are conducted to demonstrate the effectiveness of the proposed FedPara and pFedPara. After Rebuttal  I have read the authors  responses. Part of my concerns is alleviated. I would like to increase my overall evaluation to 6. The authors provided comprehensive experiments. It does not necessarily mean that $rank(W) r_1r_2$. 5.It would be better to move the details of proof to the appendix and add more discussions in the main manuscript.<|endoftext|>Different from the traditional approaches to finding low rank factorized networks, FedPara introduces a novel low rank factorization strategy, which attains a higher maximal rank for the weight matrices. Part of my concerns are addressed. Cons: 1.The main concern I have for the current version of the draft is that the proposed FedPara method is only compared against the traditional low rank based model factorization method, e.g., vanilla low rank model or Pufferfish. It is not clear if FedPara is also better compared against other communication efficient federated learning methods, e.g., [1, 2] or recently proposed low rank based communication efficient FL method [3]. 2.By looking at Algorithms 1 and 2, it is not clear which weights are kept on the client side and which weights are kept on the server side.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; rating score: 3; This paper proposed an approach for solving TSP, using GLS guided by global regret, and designed a neural network to predict the regret. In recent years, methods for the TSP have paid little attention to the fairness of the evaluation criterion, and this paper achieved a good performance under the improved evaluation method. But I suppose the experiment should be more comprehensive. The method proposed by the author is based on GLS, but uses LS as the baseline, and the experimental do not show which part of the method contributes more. The contribution of the paper is significant and somewhat new with insights into the TSP problem by GNN.<|endoftext|>The paper presents a hybrid data driven approach for solving the TSP based on Graph Neural Networks and Guided Local Search. The model predicts the regret of including each edge of the problem graph in the solution; GLS uses these predictions in conjunction with the original problem graph to find solutions. The paper is well written and presents an interesting approach to solving TSP that can outperform some other existing approaches. The introduced method is novel, might be significant, and the quality of this article seems to be on par with other papers applying ML techniques to solve TSP published at top tier conferences (which are also cited in this paper).<|endoftext|>This paper proposes a novel deep learning + guided local search heuristic technique for approximately solving the Travelling Salesperson Problem (TSP). To the best of my knowledge, this is the first work on **combining deep learning with a metaheuristic** (GLS). I believe this paper is in the second camp: similar to Joshi etal [4], the neural network outputs predictions over edges, which are then used to build a solution via a classical search algorithm. The proposed idea of using a GNN to learn the global regret to guide the LS + GLS search procedure  is novel and interesting. However, I am still lukewarm because the motivation of the work is to tackle real time routing problems with non standard constraints, but the **empirical results are only shown for TSP**. It is not convincing to simply say that the method can be applied.<|endoftext|>Pros:1.The paper is well written and easy to follow. 2.Hybridizing machine learning with local search is an interesting approach. 3.The experimental results show that the proposed algorithm outperforms other recently published learning based algorithms, which is nice. Cons: 1.The motivation of this work is not strong enough. 3.Only learning based methods and a weak baseline GLS are compared in this paper. The exact method Concorde should also be compared since it is generally very fast for solving TSP instances with a few hundred nodes.<|endoftext|>The submission is concerned with developing a heuristic for the traveling salesperson problem (TSP) using graph neural networks an dguided local search. I am not convinced that the randomly generated instances in this submission are difficult and would like to see solution time/quality of a simple greedy nearest neighbor heuristic. As opposed to previous work, the regret is computed against the global optimum (which is approximated by learning). In that regard, it could be regarded as a sophisticated local search rather than an ML approach.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper aims to improve the conventional contrastive learning (CL) by introducing a CACR loss, which takes the distance based weights into the consideration for CL loss computation. Theoretical analysis is provided on the robustness property of the representation learned by CACR. Experiments are also presented to evaluate the performance of CACR compared with MoCo V2, SimCLR and other recent CL methods. 2.The experiments demonstrate the proposed method works better than SimCLR and MoCo V2. 3.From the experiments, the performance advantage of the proposed method over existing CL methods (like BYOL) is not very significant. The compared baseline is not always consistent. The proposed method is compared against SimCLR in some experiments and MoCo in other experiments. Overall, I feel this paper makes the presentation of the proposed method over complicated and the resulted approach has limited novelty. The experiment is not very solid and convincing.<|endoftext|>Extensive experimental, as well as theoretical analyses, show the effectiveness and robustness of the proposed method. ### Strengths\+ The authors provide a theoretical analysis of the proposed CACR. \+ Extensive experimental analysis validate the effectiveness of the proposed method. \  The proposed method is motivated by the conventional CL s robustness on uncurated datasets, as claimed by the authors. But the experimental analysis does not validate this well. The "wild" uncurated data is not only data imbalance, it is also related to the quality and noise level of the data. As a result, suggest the authors either tone down the claim or provide more convincing justifications, either empirical or theoretical. \  The proposed method is also related to (though not exactly the same scenario) the hard negative and hard positive sampling used in video audio representation learning (e.g.[*1, *2, *3]) and it would be better to discuss the relationships. On the other hand, there are some unclear statements I hope the authors could address in rebuttal. As a result, I recommend a marginal acceptance at the current stage and may change my rating after rebuttal.<|endoftext|>This paper introduces Contrastive Attraction and Contrastive Repulsion (CACR) loss for self supervised learning with a doubly contrastive strategy. The importance, which is reflected by weights, guides the query to not only more strongly pull more distant positive samples, but also more strongly push away closer negative samples. Theoretical analysis and empirical results show that CACR generalizes the conventional CL loss and is more robust in more general cases. 2.CACR is more robust than conventional contrastive loss, according to the performance on label imbalanced datasets. Though the proposed framework is closely related to previous CL methods on the ideas of multi crop and hard negatives, it still shows novelty on modeling the intra relation among positives/negatives. The paper provides detailed theoretical analysis for understanding the method. Experiments and ablation studies are also sufficient to demonstrate its effectiveness.<|endoftext|>Experimental results show superior performance of CACR against traditional contrastive loss and state of the art methods. For example, why the probability of “moving it to a positive sample” is high when query is far from its positive pair? 2.It shows that increasing number of positive samples (number of augmentations, controlled by hyperparameter “K”) benefits the performance. It would be better to know the performance curve in a large range of K. Currently only K 1,4 are evaluated. The experiments are sufficient and the proposed CACR shows impressive performance.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; More specifically, the method uses a Knothe Rosenblatt transport approach which applies a one dimensional optimal transport to all conditional marginals of one distribution into another. The method uses an autoregressive density estimation from the RNADE algorithm based on a Gaussian Mixture strategy. StrengthsAs far as I can check, one of the main originality of the paper is to use an auto regressive model to estimate the conditional marginals allowing a relatively easy application of the Knothe Rosenblatt transport. Another point is the application to a dataset in high energy physics with interesting results. WeaknessesThe idea of using optimal tranport based approaches is not really novel and many approaches exist. This paper proposes a novel strategy but there is no real strong point that makes this contribution significantly different from others, in particular with respect to other optimal transport strategies. The expressiveness with respect to other OT methods should be developped and better compared. There is no benchmarks in vision, which limits the comparison with existing results in the State of the Art. This is new and interesting.<|endoftext|>The authors propose to estimate the density of both source and target data via autoregressive modeling (i.e., decompose the d dimensional density into d one dimensional conditional densities), then apply Knothe Rosenblatt transport to obtain the transfer map from the source to target for UDA. What are the motivations for the proposed method (e.g., using density estimation by autoregressive model for Knothe Rosenblatt transport) compared with other optimal transport based approaches for domain adaptation? The choice of autoregressive modeling for density estimation is not well motivated. I think a discussion about the choice of autogressive modeling for density estimation will help readers. I think it is a good baseline for the proposed approach (simple + fast computation)   autogressive modeling density estimation is also based on 1d density estimation. 4.For experiments, why does the proposed approach obtain better results than the OT approach? The authors should discuss the experimental results in detail. It seems that the contribution is quite incremental. The usage of optimal transport based approaches for domain adaptation is not new.<|endoftext|>In this paper, the authors propose a Knothe Rosenblatt Domain Adaptation (KRDA) method. Specifically, it exploits autoregressive model using a mixture of Gaussians to accurately model the different sources, and then takes advantage of the triangularity of the autoregressive models to build an explicit mapping of the source samples into the target domain. Experiments on both synthetic and real world datasets are done to verify KRDA. Strength:(1)	A new KRDA method is proposed. (2)	Both synthetic and real world experiments are done. The overall idea is estimating the source and target distributions and then learn the matching through transport. The idea is simple and not new. See comments above.<|endoftext|>The paper proposes Knothe Rosenblatt Domain Adaptation (KRDA) for unsupervised domain adaptation with covariate shift. The key idea of the paper is to use an autoregressive model (RNADE) to estimate the density of source distribution and target distribution, then transport the estimated source density to the estimated target density by Knothe Rosenblatt transportation. The authors evaluated the proposed model on synthetic data (mixture of Gaussians, Inter twinning moons), and real data (Hepmass, Amazon dataset). So, originality is an advantage of the paper. The author cleverly exploits the nature of Knothe Rosenblatt that is well fitted to the density estimation tool like an autoregressive model. **Weaknesses**:I think that the paper can become a much stronger paper if the author can address the following weaknesses:  The limitation of autoregressive models is also the limitation of KRDA as mentioned by the authors,  so the KRDA is not scalable in terms of the number of dimensions. Also, the authors should motivate the usage of mixtures of Gaussians here. I encourage the author to publish the code in the discussion phase. Could Knothe Rosenblatt transportation be transformed into a notation of divergence between the source measure and the target measure?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; While the theoretical results seem new, they are not well motivated (at least practically), and the paper does not seem to provide sufficient justification of the success of the proposed methods in practical applications. These are constructed by forward Euler discretizing the $q$ rescaled gradient flow ($q$ RGF) [Wibisono et al., 2016] and $q$ signed GF ($q$ SGF) [Romero Benosman, 2020], respectively. These gradient flows are shown to converge in finite time in [Romero Benosman, 2020], under the gradient dominant condition of order $q$. The authors show that their forward Euler discretized versions have linear rates, under an additional Lipschitz smoothness of order $q$. The paper also considers their stochastic variants. Numerical experiments on toy examples and practical example illustrate that the proposed method might have practical advantage over existing methods such as GD and ADAM. **Strength**  Fundamental methods: This paper studies two (discrete) fundamental methods, which are gradient methods (with or without sign function) with gradient norm dependent step sizes. This paper studies the convergence rates of the two first order methods, named $q$ RGF and $q$ SGF. For example, it would have been nice if the authors provided references to the sentence "Concerning that many ~" in page 1. I think matching the convergence rates as other papers did would have better convinced the readers on the theoretical justification of acceleration (in early iterations). Since the authors claim that their methods are faster than other methods in experiment, it would be nice to have some theoretical comparison. Therefore, I am not optimistic that this trend will appear in other experiments. **Minor**  Page 3 below (6b): $F_d: Z_+ \times R^m \to R^m$  Page 3 Examples 1 and 2: A feedback based step size, component wise step size and a backward Euler discretization are not used in the paper hereafter, so I think it is better to make this page concise, and state something more related to the main part. (e.g., after certain number of iterations?)<|endoftext|>In this paper, the author study two first order optimization methods, which are the Euler discretizations of rescaled gradient flow and signed gradient flow. They prove the convergence rates for both methods under both deterministic and stochastic settings. Convergence behavior of the discretization of gradient flows is an important question in optimization. 2.The paper is well written and very good to follow. The convergence rate of Euler discretizations of rescaled gradient flow and signed gradient flow under the deterministic setting exists in the literature. The author should illustrate why such generalization is important and what is the main technical difficulty in extending the existing theoretical result. 2.Under the stochastic setting, the result is not convincing in the sense that although the epsilon can be arbitrary small, the batch size will go to infinity, which is not good especially under the stochastic setting. I am, rather unfortunately, inclined to recommend that the manuscript be rejected on the grounds that the work may not have sufficient merit / novelty to warrant publication.<|endoftext|>Motivated from the finite time convergence of two first order gradient flows, the submitted paper studied the convergence properties of discrete iteration schemes in both deterministic and stochastic settings. The submitted paper studies the convergence properties of discretized gradient flows, and provides some convergence results which is the main strength of the paper. Suppose $\epsilon$ is some chosen accuracy and $\eta$ is some sufficiently small time step size. 2.The results for stochastic gradient descent are $\epsilon$ dependent, such as the batch size which is $O(\frac{1}{\epsilon})$ for stochastic $q$ RGF and $q$ closed to $2$. And the result is not very practical in the sense that if sufficiently high accuracy solution is desired, then the batch size would be too large such that it will be close or equal to the full gradient descent. As a result, is it possible to establish result for accuracy independent batch size or for the vanishing step size? Also I have two side questions1. Since the discrete algorithms fails the finite time convergence, is it possible to have other type of discretization which can provide finite convergence? Or at least numerically. Motivated from continuous dynamical system, the authors studied the convergence behaviors of discrete first order methods. edit: the responses look good to me, and i raised my score by 1.<|endoftext|>This paper considers the analysis of two discrete time schemes derived from gradient flows named q RGF and q SGF. This is an interesting direction with many interesting promises. My comments are as follows. 1) The paper starts well but in Eq.(6a) (6b), when authors describe the state space form of the optimization algorithms, I think there might be a typo in (6b): Is it supposed to be $G(X_{k+1})$? Authors should put an example with a clear case where $G$ is not identity (e.g.a proximal gradient scheme?) 3) The authors then introduce the fundamental assumption of the cost functions considered, that is, gradient dominance of order $p$. What kind of non convex functions does satisfy this inequality? Is it similar to, e.g., weak convexity or dissipativity conditions usually used in non convex optimization literature? 4) The toy examples in the paper are helpful but their connection to theory is not exploited: In particular, in this section, I’d strongly suggest authors to verify their assumptions for these toy examples precisely by working out relevant constants.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper investigates the effectiveness of the language model for training the policy in embodied environments. The authors use a pre trained GPT 2 to initialize the policy, then show the generalization effect in policy learning. In the experiments, the authors demonstrated the language model shows a better generalization effect with simple baseline and ablation studies. The detailed concerns and questions are as follows:1. Based on these results, it does not sufficiently support the claim that using a pre trained language model is helpful for generalization.<|endoftext|>The paper studies the effect of using pre trained components in a neural policy network for an embodied agent in a 3D environment called VirtualHome (VH). While the empirical results in Fig 4 do show benefits of initializing the model components from a pre trained language model, the following are quite confusing/unclear from reading the paper:(1) The main motivation of the paper is to explore the effect of LM pretraining on a non linguistic task. However, the task chosen in the work is inherently linguistic   so much so that the visual aspects of the task (i.e., observations from the environment) are represented as English tokens. (2) It is unclear why the goal and history are encoded as English sentences before feeding as input to the embedding layer while the observations are input as graph entities (similar to original formulaic representation)?<|endoftext|>I also found the architecture used pretty strange. ALFRED should always be in caps[1] Keep CALM and Explore: Language Models for Action Generation in Text based Games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan. EMNLP 2020. The paper has some interesting results, but the paper lacks novelty. Pre trained LMs have already been shown to be effective in similar text/symbolic RL environments (text adventure games) and the paper is not as impressive even as some earlier work (see [1]). In VirtualHome it s just a fit set of objects and actions and the goals are just to satisfy a fixed set of predicates.<|endoftext|>The paper s claims would also be strengthened with reasonable explanations for the observed phenomena. **Before rebuttal**:The paper proposes and studies the effectiveness of using pre trained LMs to solve a sequential decision making problem where the observations, goals, and actions are not originally represented in language. Convert the problem to language modeling and measure the effectiveness of pre training the LM. The paper concludes that (1) language modeling improves generalization in policy learning, (2) language based environment encodings are not needed to benefit from LM pretraining, and (3) the results point the possible effectiveness of language modeling as a general purpose pre training scheme. It is largely empirical and the experiments are carefully setup and deliver insightful results. The claims in the introduction are supported by the experiments (but needs be further restricted to reflect the limited scope of the paper). I would like to ask whether the visual observations are used for training the models (it seems like they are not), and if not, why? I suggest the authors revise the claims, emphasizing that they apply to a specific task/environment and a non visual input representation.
Reject; rating score: 5; rating score: 5; rating score: 5; The authors have put decent effort to bring concept based representation learning and disentanglement learning together under one umbrella in terms of the quality of generated concepts in presence as well as absence of ground truth concept labels. This is an interesting work that tried to unify concept based representation learning and disentanglement learning and proposed corresponding metrics that helped to find some important conclusions. This paper introduced metrics to evaluate the concept quality generated by both methods i.e.concept based representation learning and disentanglement learning in the scinerio of concept supervision and correlation of concepts. The property is definitely a desirable property for the concepts, but how the proposed metrics capture whether a set of generated concepts follow this property, specially when the ground truth concepts are not available.<|endoftext|>The authors consider the question of whether recent concept based learning algorithms, as well disentangled representation learning algorithms, result in high quality representations. In particular, they consider what high quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts: 1) a score that captures how well the learned representation preserves the relationships between concepts (which may be correlated), and 2) a score that captures how well concepts can be split into groups that are useful/useless for predicting particular label dimensions.<|endoftext|>The authors propose new metrics for methods in disentanglement learning and concept learning,  which have some nice properties, including robustness to correlations in the underlying factors, and experimentally probe the properties of these metrics on a number of different proposed methods in the disentanglemnt/concept learning literatures. I don t fully understand what the authors see as the main differences or similarities, or what they want to communicate to the reader on this front. can you clarify the role of the downstream task here? just one?do we expect to know it at training time? you focus on weakly supervised disentanglement, why not weakly supervised CL as well? 1: what does it mean for two representations to be "aligned element wise"? why should we prefer niching based metrics? this point about efficiency goes over my head, would be good to have a quick discussion in the paperThe authors propose metrics which address a very important problem in the relevant literatures (in particular, underlying factors which may be correlated), but the communication of the ideas and experimental results are not currently clear enough for me to accept the paper as is.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper connects the architecture of a resnet and a gaussian injected resnet to that of a transport equation and a diffusion equation respectively. They show that under this framework they provide robustness guarantees of their PDE framework scale with the $\sigma$ parameter of the diffusion equation. They introduce a learning algorithm, to attain the network from their PDE framework starting from an initial NN (also a resnet). The framework introduced is certainly not novel, as shown in this paper "ResNets Ensemble via the Feynman Kac Formalism to Improve Natural and Robust Accuracies" by Wang et. al, 2018.In fact, they too use it as a framework for adversarial robustness, and authors have not compared their methodology with this paper. Secondly, the algorithm used to retrieve the final network uses a resnet as an initialization, so I suspect that all this famework is doing is approximating a noise injected version of the original resnet, in a rather complicated way. In general the authors should improve the exposition of the paper as it has a lot of grammatical mistakes and misused words. The paper needs to be revised carefully. Further, the paper in the current state is not very well written.<|endoftext|>This paper considers DNNs as transformations of the input data which can be seen as discrete approximations to the solutions of PDEs. More precisely, this paper represents the network as an operator, which is actually solution to a PDE, which acts on a "base classifier". Using facts from PDE theory, they then propose improvements for standard neural architectures. While I like the idea behind this work, I must say that I was disappointed by the way it was presented here. There is a formula for its discrete counterpart but the continuous one, while mentioned in the introduction, only formally appears in Theorem 1 which meaning thus becomes unclear, and one has to guess that the operator which is being characterized is defined through the properties above. This is only one example, albeit the most striking one. The contributions of the paper are not clear and neither are its assumptions. Moreover, while I found the experiments interesting, the toy dataset experiments were not convincing: is the claim that the proposed method can be used to ensure adversarial robustness? In this case, there should be other baseline methods to compare to. Update after rebuttal:I thank the authors for the improvements made in the updated version of the paper.<|endoftext|>The authors cast DNN classifiers as the push forward of a base classifier under aflow map at some fixed final time. Under some reasonable assumptions on the flow,they show that, given any base classifier, the flow map can be obtained as the solutionto a convection diffusion equation. They show that ResNets and Gaussian noise injectioncan be viewed as special cases of their model and give a robustness guarantee for any classifier defined as a solution to their PDE. Experiments on the 2 d half moon data setas well CIFAR 10 and 100 show better robustness  of their model to adversarial attacks when compared to standard ResNet(s). Viewing a DNN model as the flow of a regular Markovian operator seems quite natural and generalizes the neural ODE idea. I also appreciate that the authors derive an explicit PDE based on very reasonable assumptions on the desired properties of a generic classifier. Nevertheless,I think exploring the more general form should certainly be done given the first part of the paper places a lot of emphasis on this result. While the PDE is very high dimensional, it is also very simple so applying some Using classical techniques to solve it will, I think, make for a very interesting comparison. Very interesting connection between DNN(s) and PDE(s), generalizing neural ODE(s) but the practical algorithms presented don t make a strong case for the method.<|endoftext|>This paper studied a ResNet like DNN model that can be expressed as a discretization of PDEs. First, this paper showed that, under some assumptions, any adjust operator is a solution of a second order convection diffusion PDE (Theorem 1). ResNets and ResNets with Gaussian noise injection are special cases of this theorem. [8] P.4: We then require [...] that no enhancement is made. The updated version improved the readability. # Initial Comments【Strength】  [1] This paper provided an architecture independent sufficient condition that a convection diffusion PDE models a DNN. I would suggest writing their details in more detail. [2] P.15: I could not confirm the correctness of the proof of Theorem 2. It derived the condition under which a model is can be described by a PDE (Theorem 1). [3] I think the proof of Theorem 2 is based on the idea of [Wang et al.2020].On the other hand, it is a new idea to derive the bound for Rademacher complexity of $\mathcal{G}_\sigma$ from this idea. This paper is technically significant as it gave a model agnostic PDE theory that encompasses several DNN models. [2] In addition, according to Table 2, the proposed method can increase the robustness against adversarial attacks.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes a technique for domain adaptation in the semi supervised setting (we have access to some labels of the target domain). I encourage the authors to polish their paper and improve their experimental results to eventually resubmit their work. “anti” Adversarial examples are computed on the labeled target data in order to have them classified correctly by the source classifier. (I call them anti adversarial examples since the goal is to change the labeled target example to improve the accuracy of the source classifier). 2.The explanation of why this method works could be developed more. For instance, these reasons should have two consequences: A. 3.The writing of the paper could be significantly improved. I also found the flow of this paper confusing:  Why does cycle monotonicity suggests using optimal transport? Is the OT map between $\Sigma_f$ and $\Sigma_t$ or $\Sigma_{semi}$ and $\Sigma_f$? The conclusion contains unfinished sentences “The main limitation of our approach is that necessary to have access to the labels in the target domain.“4. Algorithm 1 $X_f$ should be $\Omega_f$  Steps $n,...,N$  $x’_{n+1}$  “*The* source domain classifier”  “Such *an* an attack”As a summary the ideas from this paper are interesting but it is clearly an unfinished work.<|endoftext|>This paper proposes using the method of adversarial attacks for the task of semi supervised domain adaptation. I appreciate the authors  effort to conduct experiments on various vision datasets and several OT algorithms. The new approach seems to perform better than unsupervised domain adaptation. While cyclical monotonicity is a good evaluation metric to check optimal transport. 2.The definition of cyclical monotonicity (CM) is not clear in this paper. The authors compared the "standard setting" with their semi supervised settings. This "standard setting" is not detailed in the paper. 4.From Figure 2 it seems that with larger perturbation, the source classifier can perform better on the source fiction domain. I think we could see it more clearly if we train a new classifier on both the source domain and the labeled part of the target domain, and compare with Algorithm 1. Moreover, adversarial training methods should be used so as to achieve better generalization property [2]. Using adversarial attacks for domain adaptation is an interesting idea and has been explored before (https://arxiv.org/pdf/1810.00740.pdf).<|endoftext|>This paper studies the use of adversarial attacks and optimal transport (OT) in the context of semi supervised domain adaptation. The authors show that adversarial attacks satisfy the cycle monotonicity property. They then propose an algorithm that generates adversarial examples for the labeled target samples using the source classifier, and maps the vanilla target samples to this new domain via OT. Strengths:  Studying the use of adversarial attacks for domain adaptation is an interesting topic, which has not been extensively covered yet. The results using discrete OT methods suggests that the proposed approach can bring some improvement. Let me describe Algorithm 1 in words: Given a classifier trained on the source data, one generates adversarial examples for the labeled target examples. As such, these adversarial examples will be misclassified by the source classifier. In short, there is no clear benefit of the method in this case. 2.3) As there are no references, is not entirely clear to me if the discrete OT and neural OT methods used in the experiments correspond to the state of the art methods for OT based domain adaptation. 3.2) In the second paragraph, the authors state that the conventional OT approach for domain adaptation is to transform the target domain to the source one.<|endoftext|>The enhanced method does require at least a few labeled samples in the target domain. Then, all the target data is mapped to this "source fiction" dataset and the classifier is applied to this mapped target data. The paper shows that this idea can possibly boost the performance of existing OT based domain adaptation methods. For clarity, it should be more strongly emphasized that you are applying OT methods in the latent space of the classifier, NOT the raw pixel space. Empirical results demonstrate that the proposed algorithm can boost the performance of prior OT based domain adaptation algorithms. **Strengths:**  The paper develops an interesting connection between "slightly perturbed samples" (possibly misnamed adversarial examples) that gives a cycle monotone map with respect to the original samples. Thus, this mapping step must be done whenever a new sample comes in i.e., it cannot be done once during training and then work for any new test sample. However, the paper lacks clarity and intuition about why this is good or useful. Are there some baseline semi supervised domain adaptation methods that could be compared against (or even unsupervised domain adaptation methods) to compare against? If they are not, then what is the reason to use OT based domain adaptation over the state of the art methods (i.e., what additional benefit is gained by using OT if performance is the main concern)?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a multi armed bandit (MAB) framework with three realistic considerations: incentivized sampling, delayed feedback and self reinforcing preferences. The paper proposes a ‘UCB Filtering with Delayed Feedback’ (UCB FDF) policy for the new MAB framework. For general feedback delays with bounded expectations, the authors showed that the delayed sampling feedback has additive penalty on regret and incentive costs, then utilized this key fact to derive that the UCB FDF policy achieves logarithmic regret and incentive cost in the new MAB framework. The theoretical bounds are verified by experiments on instances with 3 arms using Amazon Review Data. Second, the technical novelty and significance should be highlighted in a more clear way. 3.The theoretical results, to my understanding, are derived under fairly general assumptions. Additionally, I would expect the experiments to provide some empirical evidence for the benefits of considering these additional factors. To my understanding, the main contribution from this paper is the additional factor of delayed feedback in the MAB framework. Questions during rebuttal period: Please address and clarify the cons above.<|endoftext|>This paper combines three aspects of MAB: delayed reward, incentivized exploration and self reinforcing user preference. They motivate this problem from the perspective of online recommender systems. For this model, they propose a new UCB based algorithm that achieves the optimal upper bounds. They also setup an online experiment based on Amazon review data and show how the regret evolves for both arm independent delays and arm dependent delays. Strengths of the paper are as follows. The main weakness of this paper is as follows  My main complain on the paper is primarily around positioning. I would like to better understand the challenges which will help me appreciate the results better. Thus, I would like the authors to address that.<|endoftext|>This paper studies a stochastic multi armed bandit setting with delayed feedback, where additionally, the bandit policy must incentivize an external agent to pull the desired arm, and the arm preferences of this external agent are self reinforcing. They design a policy, UCB FDF, for this setting, and prove expected regret (and incentive payment) upper bounds under various delay distribution assumptions. Finally, they evaluate their policy on bandit environments constructed using Amazon review data. I will outline these concerns below:**Discussion of related work**:It seems to me that there are a number of typos in the related work discussing Bandits with Delayed Feedback. I think that most of my main concerns have now been addressed. Since I do not think these concerns can be sufficiently addressed during the rebuttal, I do not think that this paper is ready for publication. The authors have given a satisfactory response, and I have increased my score accordingly. Does this mean that the algorithm is also allowed to make unbounded payments and “no cost” to performance? It seems this algorithm is more of an Elimination style algorithm?<|endoftext|>This paper is largely motivated by Zhou et al.2021, which incorporated self reinforcing user preferences into the incentivized bandit learning framework. Could you elaborate more on the technical difficulty when you consider the delay effect, compared to the work Zhou et al.2021?2.In bandits with delayed feedback [Pike Burke et al.2018], their Theorem 2 illustrates that the regret bound has an additional term log(1/\Delta_j) E[\tau], which does not depend on T. However, in your Theorem 3, it has an additional term \sqrt{4 E[\tau_1] ln T}, which has the dependence on T. Why is that? Can the result be generalized to the setting where \tau_a,t follow different distributions when t varies? 4.Why only the term g(b,1) shows up in the regret bound but not g(b,t) for t>1? 5.Regarding the numerical experiments, how do you choose the self reinforcing preference function f(x) and the incentive impact function g(b,t)? The problem this paper studied is defined in a clear way, but authors may need to emphasize the technical contribution that is built upon the existing work and demonstrate the tightness of the regret bound.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 6; rating score: 8; rating score: 8; For instance, imagine a grid world in which the policy for the offline dataset is to go right with 97% probability, and the other directions with 1% probability, whereas the optimal policy on the downstream environment is to always go up. The priors can absolutely hurt when the downstream task is unrelated to the offline data, and a few empirical results are not sufficient to support the broad claim. The main claim of the paper is not supported by the evidence. It s not hard to imagine scenarios for which the main claim can be demonstratively false. In my opinion, the paper must clearly mention when the proposed method would help and hurt, and tone down the main claim.<|endoftext|>This paper proposes the concept of temporal priors, extending previously proposed behavioral priors. Given the high standard deviations for some of the performance curves, I believe the authors should increase the number of utilized random seeds from the current 3/5 per experimental setting (Peter et al.2018).2.It is unclear to me how the policy s entropy quantifies the agent s confidence to reach the goal, as stated in Section 4.2 (if multiple actions lead to the same outcome, wouldn t the entropy still be high regardless of the agent s confidence?). The paper is well written and easy to follow. For instance, there is a large gap between the results in the original hand picked meta world tasks and the other tasks in MT 10. Thus, I think this paper is not yet ready for publication.<|endoftext|>2.The proposed learned priors showed good transfer properties on selected environments, some even with a wide gap. The baselines being compared to are rather weak both in terms of their exploration capabilities, and that they were not designed with transfer tasks in mind. There seem to be many other baselines that could be considered in this space, such as intrinsic motivation based agents (e.g.Agent57), randomized value function / policy based exploration, or model based exploration agents (e.g.Plan2Explore) etc. The paper presents a set of temporal priors for exploration, empirically showing good results for certain transfer tasks. I m concerned with the generality of the approach beyond the selected environments and the lack of competitive baselines that the paper considered.<|endoftext|>The proposed solution is benchmarked against a rather large set of related approaches and it is shown to perform very well, for tasks that differ significantly from the original tasks in particular. However, the most important concern when it comes to the proposed approach is the inclusion of the mixing weights in (4). The notation a_t ~ pi(s_t) in (2) and (3) is a bit confusing since it gives the impression that the policy could be deterministic. Further, since policy parameters are used in (5) and (6), it would be good to include also the parameters for the Q function. It would be good to test with and without the weights, assuming there is no good motivation why the weights need to be in the objective. Given the concerns raised in the core part of the paper that relates to the policy objective, the paper cannot be directly accepted, but not completely rejected either.<|endoftext|>This paper considers the exploration problem in RL and proposes a new type of the behavioral prior, one only conditioned by recent actions. But it needs to be clarified. The empirical results of the proposed method are impressive. Therefore, I lowered the score to weak accept. I strongly encourage the authors to resubmit the paper with discussion on the limitation of the method and correction of some overselling wordings in the main text. The paper proposes an interesting idea, and current experimental results are impressive. W3.The objective function for the proposed SAC like algorithm contains a mistake. There are many other claims which I feel is not well supported, and this is confusing to me.<|endoftext|>This approach is shown to encourage state coverage, to be capable of accelerating learning in unseen and different tasks. The main contributions are novel, interesting, and well motivated. The experimental results seem informative and in support of the claims made throughout the paper, but the low choice of seeds leaves room for doubt in some cases. Overall, I think the main idea is good, and the work is interesting. If it weren t for the aforementioned issues, this would be a strong paper. p. 6, "We finally note that the modified learning objective remains aligned with the original formulation. What is the motivation for dropping this term? # Minor comments and nitpicks:  p. 3, "the issue of temporal correlation is merely relocated in the hierarchy, as the high level planner is not encouraged to produce correlated sequences of skills", I m not sure this is a fair assessment.<|endoftext|>This paper suggests a task agnostic exploration priors that can be trained on policies that solve random tasks in a given environment (task agnostic data). Given a new task, the induced exploration quickly finds how to solve it, compared to other methods that disposes of the same pre training data. They use a good choice for the baselines and show that their methods outperforms them on environment from the meta world suite. Regarding the experiment, I was expecting more type of environment, to show that this method does not only work on the meta world suite. +) novel and simple approach and nice results on meta world. ) Needs more environments for experiments. Since the results are nice and the method is corrects (they finally used the original SAC), I would be more favorable for a weak accept, but I strongly suggest to remove the paragraph about the entropic regularisation in section 4.2 as well as the appendix A and re write Algo1 to explicite that the used RL algorithm to learn the exploitation policy is SAC.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Improving training effectiveness through better masking is an important research direction for speech representation learning. Strengths:The paper presents a robust experimental setup with three SOTA models (W2V, HuBERT, W2V BERT) evaluated in two sizes L/XL on multiple datasets (LS, AMI, CHIME 6, Tedlium, and Commonvoice.Weaknesses:There are three major points here:1) The paper presents an incremental improvement over the baseline systems with minor novelty.<|endoftext|>This study investigates the use of an external frame synchronous CTC based ASR system to get confidence scores for frames in masked speech modeling which is called AskToMask (ATM). Would a better scorer help with ATM? The paper uses ASR based scores to mask high confident regions in masked speech modeling. The main strength of the paper is that it provides extensive comparison for different aspects of the approach.<|endoftext|>This paper explores the use of weighted masking for learning speech representation. Furthermore it offers a limited insight about the quality of confidence scores used and sensitivity of the proposed scheme to this important in practical applications factor (e.g.limited resource languages where WERs of typical external ASR systems range wildly).<|endoftext|>In this paper, authors proposed to use an external scorer to weight the frames to be masked for the MSM loss. In ASR, you can take non speech region vs vowel and you get a big difference in importance. And finally, if you look at the Table 6, the difference in WER betwene proposed, that uses extra data, vs baseline is not too great. Only in CHIME we seesome real improvement. This sentence is very suspicious: "The scorer’s training data is chosen such that it is matches the target data condition". Could you elaborate on this? Please replace string "%WER" with "WER (%)".
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper structure and writing is good. To solve this problem, author introduce the Thompson sampling to it. However, since they do not assume the existence of a more powerful offline oracle, so cannot achieve the optimal complexity upper bound.<|endoftext|>This paper designs the first TS based algorithm to solve the pure exploration problem for both multi armed bandit (MAB) and combinatorial MAB (CMAB) problems. b) The verification framework achieves lower complexity. Second, the authors should provide the complexity of this full framework. d) It is a bit strange that the algorithm can go on forever in some cases. So I would rather keep my score.<|endoftext|>Overall I found the problem and proposed method interesting. Overall I think the work is good. I don’t think there was any formal comparison about assumptions of the oracle that makes the differences clear, just statements that other papers considered more ‘powerful’ oracles. That would make sense to me.<|endoftext|>The techniques used are well known and a similar analysis works for verification in both vanilla multi armed bandit as well as the combinatorial bandit problem. However, the problem being solved using TS in existing literature is regret minimization. If so, it could be mentioned that the Gaussian was chosen for convenience. While the analysis is correct, more discussion about why the simpler (in comparison to [Agrawal and Goyal 2013] or [Kaufmann et al 2012]) analysis works in this problem would be helpful.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The main contribution of the paper is adapting an  existing line of nonparametric belief propagation methods  to this setting, which allows (partial) end to end  learning. However, such baselines    are not evaluated. That paper also seems to learn the deep    potentials end to end. I did not read that paper in    detail, but is the main idea that they use a    variational method rather than belief propagation? That being said, I am likely to be the  least experienced of this paper s reviewers when it comes  to graphical models, so please take this into  consideration when evaluating my reviews (this is  addressed to both the authors, as well as to the ACs).<|endoftext|>BP is known to be exact on trees but is not tractable for infinite state spaces. **Strengths:** The paper aims at a differentiable approach for supervised learning and prediction in MRFs with infinite state spaces. The authors do not clearly state that in such cases there is no need for loopy BP. Further empirical approximations of the BP approach (which is itself an approximation) are explained in the supplement even though they are part of the proposed approach. This approach has the potential to learn MRFs on graphs with cycles.<|endoftext|>Weaknesses:The main issue I have with the paper is that I find the results of the paper weak:1) The chosen baseline is LSTM, and it is trained with an extremely small batch size (6). Also that paper has a good methodology to check how informative the uncertainties are. 2) LSTM is not the natural choice for graphical models. In fact, GNNs are shown to beat BP techniques significantly in this uncited paper: Yoon et al."Inference in Probabilistic Graphical Models by Graph Neural Networks".<|endoftext|>It allows them to learn the parameters of these networks using labeled data which is the main contribution of the paper. In each application, DNBP is not able to outperform the considered baseline but it is able to provide measures of uncertainty associated with its predictions. The idea of replacing the handcrafted potential functions with feed forward neural networks is interesting. 3.The main benefit of DNBP over classical neural networks, is its ability to associate a measure of uncertainty to its predictions but the three applications that are considered do not accentuate this benefit.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper introduces a new problem, called federated inference, and proposed a framework of solutions including local representation alignment and learning a consensus graph. The paper made a list of interesting contributions, including formulating a new research problem, developing a complete solution with a few technical innovation. However, I have the following concerns of the paper: 1. I hope there are some discussion on the privacy aspects. And is it a novel contribution to use GCN this way?<|endoftext|>From the local models, an opportune data representation is extracted and shared in a centralized setting, which is then used to perform global inference. My feeling is that a benchmark with simple pre processing and data imputation approaches is needed to fully appreciate the value of the pipeline proposed in this work. The pipeline requires the further training of centralized models to provide the alignment and consensus graph. How is the homogenization performed in a FL setting? There are also  concerns on the privacy leaked during the sharing of the local representations.<|endoftext|>First, the client extracts representations using its pre trained model. Second, the server aligns client wise representation, and then trains a GCN based classifier to leverage the structural information across clients. 3.The experiments are based on datasets for power grid and traffic network with time series inputs that are unusual benchmarks in FL. The paper is a combination between dynamic graph neural networks and federated settings. However, the alignment of federated settings is unconvincing, and the contribution from graph neural networks is also very limited.<|endoftext|>Cons:(1)	The studied topic is interesting, but the technical ideas have the high risk of leaking privacy of local datasets, due to the upload of new representations of the local data. To enhance the proposed framework, the authors propose two alternative alignment methods and a consensus graph learning among the new representations of different clients. Pros:(1) The paper proposed an interesting framework to perform "federated inference" to address the scenario which vertical federated learning also studies. (4)	In essence, the technical idea is the first local representations, and then fusion of these representations for decision.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The authors have compared their methods with prior work and showed that the proposed method obtained better FIDs. (Update: after reading the reviews and the authors  reply, I would recommend the paper to be accepted to the conference.I m raising the score from 6 to 8.) 2.Strong results on X2I translation tasks are shown, indicating that the distillation method is effective for X2I tasks. 2.The clarity of writing can be improved. This work introduces a nice idea that improves image translation methods by distilling the knowledge of unconditional GANs.<|endoftext|>In this paper, the authors present a unifield learning method to investigate the knowledge transfer for X2I translation. Compared to existing methods, there are two advantages here: (1) this framework can be used for varying kinds of conditional image synthesis tasks; (2) it relieves the limitation for student generator to be the same as the pre trained GAN. I really like the figures which are clean and helpful in understanding the ideas of the paper and performance of the method. However, with the default settings, they have the best performances on their chosen dataset. Or can the authors present some citations to support it?<|endoftext|>The paper presents a framework for knowledge distillation in image to image translation and arbitrary domain to image (e.g., text to image) translation. Authors claim that the proposed transfer learning method outperforms modern state of the art. Strengths   Authors evaluated their approach on multiple datasets and multiple tasks. The provided comparison with the baselines is limited and doesn t present modern image to image translation models, e.g.[a,b].[a] Schönfeld, Edgar, et al."You only need adversarial supervision for semantic image synthesis." The semantic diversity loss is declared to be novel in the abstract.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors address the problem of selecting the bestforecasting model for a time series dataset. In experiments on two meta datasets(called performance tensors), one univariate, one multivariate,it is shown that esp. But none of the baselines isfollowing this simple setup. The notation of the paper is not easy to follow. forecasting. w2.baselines seem to be very weak.<|endoftext|>As a caveat, I m not an expert in meta learning, so I cannot comment expertly on the contributions of the paper in the meta learning space. For example: what actually is a model in the way that the authors use it? Instead, they represent a large selection of methods available for which I would laude the authors. If state of the art was the primary reason, a transformer based method (TFT for example) and NBEATS should also complement DeepAR. This selection by the authors should be justified further. Probably "number of time series" or "items" is more appropriate. Or am I missing something? I base this primarily on the exposition of the work which leaves too many too important details open. It s also unclear to me what a forecasting "algorithm" is.<|endoftext|>This paper proposes a meta learning approach for time series. Experimental results show an improved performance over other meta learning baselines. The corpus of time series datasets that was used for the experiments, is also provided as supplementary material. However, due the issues in the experimental evaluation/results, and the dataset collection process, I am leaning towards a reject. Extensive experimental evaluation shows improved performance of the proposed algorithm over other meta learning baselines. However, I do not see this as a major drawback of the paper. The presentation of the paper needs to be improved greatly.<|endoftext|>This paper formulates the problem of automatic and fast selection of the best time series forecasting model as a meta learning problem. A large benchmark and a collection of models are established to evaluate the problem. Model selection for time series forecasting is a new setting and the proposed AUTOFORECAST is reasonable to solve the problem in an efficient and effective way. It seems the baselines in Sec 5.2 are relatively old. The proposed method is reasonable to solve the problem. However, there are issues that need to be addressed.
Reject; rating score: 3; rating score: 6; rating score: 8; Strengths:   Proposed an interesting method with nice experimental results. Such connection, as well as the connection to other methods based on VRNN (e.g.Aksan et al., 2018), is not clearly discussed in this paper. 2.This paper claims "state of the art". VAE based approach, such as Hsu et al., 2019, are better baselines. For the Tacotron baseline, it s also worth to include the result conditioned on speaker ID. The idea is: to generate sample $x$, the model is conditioned on both a content $c$ and a style $z$, where $z$ is from a function $M(x, x )$. This is neither theoretically nor empirically answered in the paper. It would be nice to save some space from them, and use it for the connection to related works.<|endoftext|>In this paper, the authors argue that the typical training algorithms for controllable sequence generative models suffers from the  training inference mismatch . Song.NEURAL TTS STYLIZATION WITH ADVERSARIAL AND COLLABORATIVE GAMESThis paper is well motivated, the idea of applying  style equalization  is interesting. the models show good results. Also, the showed experimental results are impressive. It is a critical while very challenging research problem, because the  content  and  style  are entangled in the training samples, and ones must carefully design the training objective such that each of these factor can be learned in a controllable way. For example, on TTS, what is the training dataset and what is the evaluation dataset? One of the most fundamental problem is how to effectively capture the content information and style information, respectively. If the latter, are the evaluation performed on  seen  or  unseen  speakers? does not trained to encode pure style information as expected. 2.To deal with the  training inference mismatch  issue, there are another line of works hat encourage disentanglement of  content  and  style . 3.There lack of implementation and experimental details.<|endoftext|>  To enhance the quality of style controlled generation, especially in an unsupervised manner and non parallel setting, this paper proposes a "style equalization" mechanism to prevent the content leakage problem. The authors assumed that content information is time dependent whereas the style can be time independent so that the authors employ time average pooling to learn the global style. In particular, the unsupervised learning and non parallel setting are realistic. The experimental results are much improved in the automated evaluations and human evaluation. In particular, I was hard to distinguish the synthesized speech examples whether it is ground truth or generated. The implementation codes are not submitted.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper tackles learning a flexible distribution approximator to approximate the output of a high dimensional and computationally intensive stochastic simulator. 2.2   I would also like some comparison of runtimes. 2   Definition of a new acquisition function and using a neural process in an active learning setting. The contributions of this paper then reduce into three main components: 1   Definition of a spatiotemporal neural process that can succinctly model a more richly structured latent process. The method appears to work, validated on a toy ish SEIR model, and a more sophisticated pre existing epidemiological model. These are important quantities to know if the main selling point is “accelerating”. There are also a lot of separate ideas in this paper that are intermingled. Just as many models can be learned _using_ interactive methods, that does not make each of those models interactive. The training method has a degree of interaction, but that is not to do with the neural process. I feel the authors have just dropped this in to make the paper feel more legitimate, while adding very little to the paper. _Convolutional conditional neural processes for local climate downscaling_ [Vaughan et al., 2021]. The epidemiological traces (especially in SEIR) are quite simple, and so I am not convinced that the STNP has been sufficiently validated. Ultimately, it is the performance of the learned function approximator that is paramount. The raw quality of the manuscript is well below publication quality.<|endoftext|>This study proposes a new method for learning surrogate models of stochastic simulators. The new method, Interactive Neural Process (INP), builds on Neural processes and leverages the spatiotemporal structure of the problems at hand to reduce the complexity of the inference task. Some previous work is cited and discussed, and it is clear how this work differs from these previous contributions. However, I believe it would be important to revise the section on related work. I would urge the authors to clarify this statement. The paper is technically sound, and claims are for the most part well supported by empirical evaluation. However, I have a concern I would appreciate the authors to address: I believe the study would strengthen substantially if the method would be compared both in terms of inference quality and computational efficiency with other state of the art likelihood free inference methods, such as the ones based on neural density estimation (e.g.Papamakarios et al.2019, Lueckmann et al.2018).Indeed, if the simulators are not very expensive computationally, then maybe neural density approaches for learning the likelihood (Papamakarios et al.2019; Lueckmann et al.2018), which tend to need a large(r) number of simulations, will be competitive with the proposed approach in learning a surrogate of the simulator. There are a few other instances of "dimension" where this should also be corrected; in "which is scales linearly in computation", it should be "which scales linearly in computation"; "where it first randomly selects a set of groups and then use(s)"; one can guess the dimensionality of the problem for the SEIR problem (if I am not mistaken, 2 parameters and 100 dimensional output). Overall, the paper is technically sound and has a novel methodological contribution with convincing empirical results. Some of the discussion of previous work needs to be revised and augmented, and it would be good to provide empirical comparisons with state of the art simulation based inference methods (in particular, ones that learn the likelihood function).<|endoftext|>The authors proposed a novel active learning framework integrated with the neural process. The experimental results show that INP works better to accelerate stochastic simulation than GP, and the proposed LIG leads to faster convergence compared with alternatives. I think the model is novel with a spatial and temporal extension of NP, an NP enabled active learning mechanism, and a new latent based acquisition function. There needs some significant improvement in the illustration of the model:1. "Augmented parameter theta" is mentioned a couple of times. Is it target parameter? But SNP also has the latent dynamics z. So based on the analysis, the spatial part should be more significant. Minor:1.In the second paragraph of 4.1, R is not defined. But I think the paper (especially the methodology part) is poorly written.<|endoftext|>The manuscript entitled, "Accelerating Stochastic Simulation with Interactive Neural Processes", presents a novel approach to the problem of statistical emulation for mechanistic models of epidemic disease transmission. An active learning strategy is developed to train these neural processes to minimise the computational costs of generating training instances of disease simulator outputs. These choices are supported by appropriate theoretical analyses of bounds and convergence rates. In this case I believe the paper is presenting an argument for itself in the latter category, but I personally do not see its merits here as I will explain below. Mechanistic models of disease transmission can be incredibly computationally expensive to run with high overheads for both RAM and cpu time. In many cases an additional problem will be the lack of a tractable likelihood function and/or lack of control access to latent system random variables, such that the simulator needs to be treated as a black box. If the application of the surrogate model is to address one of these problems then the Bayesian optimisation methods focussed on that single objective will inevitably out perform a general surrogate model. Since the method proposed here falls into the latter category of producing a general surrogate I am not convinced that the applications to which it has been explored here are sufficient to demonstrate its utility.
Reject; rating score: 10; rating score: 3; rating score: 8; The paper proposes a modular and efficient framework along with its JAX implementation for the implicit differentiation of optimization problems. The proposed framework is labeled as efficient, since it doesn’t have to unroll the computational graph like in autodiff, and modular since it doesn’t require case by case mathematical derivation like in implicit differentiation. The authors show that existing implicit differentiation methods can be instantiated in their framework. They provide and empirically validate new bounds on the Jacobian error when the optimization problem is only solved approximately. Code and implementation in JAX are provided along with the paper. The main novelty of the paper resides in the efficiency and modularity of the framework proposed to solve bi level optimization problems. This framework allows to abstract away low level details and significantly lowers the barrier to use implicit differentiation. The paper is well written and touches upon very relevant topics to the community. The framework, its implementation, and the theoretical insights of the Jacobian error are novel and useful contributions. Therefore I strongly encourage accepting the paper (modulo minor comments below) and I think it should be highlighted at the conference. I suggest you add line comments or a paragraph with the code notation. 2.In section 2.1 General Principle, the inner working of the method is proposed; however, it is difficult for me to untangle the novelties presented with respect to what is already known in the literature. Is the procedure of differentiating a root as a linear system part of the novelty? Or the main novelty resides in how to solve the linear system? Please clarify your contribution in this section. I think it would be beneficial to the reader to have a brief explicit definition of Implicit Differentiation.<|endoftext|>This paper introduces a Jax package for implicitly differentiating various numerical solvers. ## Strengths* The proposed package is a nice unification of the core implicit differentiation procedures. I believe that this construction has the potential to speed up development and application of implicit differentiation methods. * The theorem is a good, nice to know theorem for the literature. ## Weaknesses* The paper can effectively be split into the "code" portion, the "theorem" portion, and the "examples" portion. The theorem also has a straightforward, routine proof, so not much theoretical insight is gained here. Finally, the examples are smaller scale and are more toy than previous work such as [2, 3]. * The core contribution, the software, is not presented in a digestible manner. In addition, I would hope that Section 2.2 could be rewritten with more code examples. However, I feel that the paper is not sufficiently novel on any front to warrant publication. Furthermore, I would encourage the authors to present the main contribution, the package, in a more digestible manner.<|endoftext|>This paper provides a unified tool for combining the implicit differentiation technique and the  automatic differentiation method widely used in existing deep learning packages such as PyTorch and TensorFlow. In the experiments, the authors illustrate how their tool can be useful to simply the implementation. My detailed comments are given as below. The presentation of this paper is good and most parts are easy to follow. The motivation of this paper is clear, i.e., to provide a unified and easy to use implementation for implicit differentiation by leveraging the tool from automatic differentiation. The tool developed here seems to be useful and cover many existing designs of interest. I feel this is good. One of my concerns is the novelty of this paper. 2.The analysis is not new as well, since there are already many works on studying the iteration complexity of the response Jacobian and the hypergradient, which I find are missing in this paper. I list some of them and highly encourage the authors to add them (and some related works therein) and provide a short discussion. I am wondering whether this package is generalizable enough for such more practical settings. Although I feel the novelty of this work is not that high, I am still slightly positive about it. I am open to increase my score if the authors can address my concerns and add missing related works.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 10; The paper under review studies the universal approximation theory with constraints. Furthermore, a chart free universal approximation is established on the Riemannian manifold with geodesically convex constraints. The general theory shows that the universal approximation can use the probabilistic transformer to project the outputs in the set $K$. 2.The geodesically convex constraint seems to be an interesting and promising idea to treat non convex constraints in the Euclidian space. The paper seems to address a well known important class of problems.<|endoftext|>The paper intends to provide a stronger type of universal approximation result of transformer models. Strength: + Provides rigorous theory to support the utility of transformers for deep neural network based predictive purposes under constrained settings. + Soft constraint set that is defined using available data points is an interesting aspect that the paper considers. No experiments to verify any of the quantitative results. Given that the paper shows quantitative results in the infinite sample setting, it is not clear whether the results hold in the finite sample setting where transformers are being used in practice.<|endoftext|>**  Throughout the paper, the authors give numerous examples to illustrate why and how one might have constraints when training neural networks. It s clear that the problem is well motivated and there are few results in the area. The idea of providing a universal approximation property under constraints seems quite natural, and the authors motivate it extremely well. Indeed, it is true that there are many works that argue for constraints in deep learning without considering the question of whether satisfying such constraints is even possible. For this reason, I believe that this paper has a significant claim to novelty. **  I like the experiments at the end of the appendix as well. This helps to cover another natural question regarding how one can use these results in practice.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; Taken together, I don t believe this paper is ready for publication at this time. Post Rebuttal  I thank the authors for their response but the justifications for the utility of representing epistemic uncertainty and lack of baselines are shallow;  it s clear that substantial revisions are needed before the submission is ready for publication. # Final Remarks* The authors summarize their contributions in Section 6.<|endoftext|>**After rebuttal**: Thanks to the authors for responding to some of the points raised in the review, and for running additional experiments. **Clarity**: The paper is mostly clear. * I found the discussion of prioritization hard to follow.<|endoftext|>This paper aims to improve learning efficiency for Hindsight Experience Replay(HER). Although this idea is shown work well compared to the naive HER, I think that further more principled solution may be needed. some questions and typos:1. the caption of the last figure in Appendix should be  Figure 8  instead os  Figure 7 ;2.<|endoftext|>The above reason is why I am unconvinced about the experimental validation. I understand that this paper is very recent, but comparison to some other exploration enhancing mechanisms would be good. Could authors clarify the difference, and why this method was not compared to in baselines?
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; In [2, 3], g is not null and hence the authors should provide a better argument. The authors show that the limit points of the algorithm are stationary points. It will be easier to follow the theoretical results if the authors group assumptions separately. Can the authors use an existing analysis for a deterministic method to compare, such as [1] or other references therein. Of course, the assumption in [1] is different from the one in this paper and therefore the authors should make it clear difference between their assumption and the assumption of [1]. My main problem with the authors  current comparison in Remark 4.1 is the following.<|endoftext|>This paper proposes a randomized primal dual coordinate method for solving linearly constrained nonsmooth nonconvex optimization problems. It is not clear whether the new local uniform metric subregularity assumption is weaker than the KL inequality assumption. W2.In the sufficient decrease properties in lemma 4, the parameters kappa and delta are unknown, it is not known how to choose these parameters to guarantee the global convergence of the algorithm.<|endoftext|>4.The paper could present more about the experiments in terms of comparison with existing methods to show the effectiveness of the proposed method, particularly when double the dimension of primal variable. 1.For convex case (including nonsmooth and bregman divergence), this paper proposed randomized coordinate primal dual coordinate method for augmented Lagrangian. Overall, the paper looks good and the results are aligned with randomized coordinate methods, i.e.O(1/N).<|endoftext|>I would support its publication in ICLR but my support is not strong. The paper proposes a randomized primal dual method for solving the nonconvex nonsmooth optimization problem with linear constraints. Although some new notations/relaxation of assumptions were proposed, the analysis and techniques of the paper rely on a combination of the techniques used in Zhang & Luo (2020b) and standard techniques of randomized algorithms in the literature.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper proposes a method to train adversarially robust few shot classifiers. (+) The proposed approach is simple and technically sound. A single metric of accuracy is used for all the experiments which failed to justify the proposed method from a different perspective. 4.The performance increase is either negative or very subtle in general. To sum up, this is a mediocre submission and I would rank it below the acceptance bar of ICLR.<|endoftext|>This paper 1) proposes a simple transfer learning approach to enable train adversarially robust few shot classifiers for few shot image classification, and 2) present a method for novel classification task based on calibrating the centroid of the few shot category towards the base classes. My main concern of this work is that the improved performance can be mainly resulted from the pretrained stage (base training) using the base dataset $X_b$. Normally, pretraining can improve the performance on small tasks.<|endoftext|>However, the core contribution of this paper is weakened due to the lack of theoretical analysis. This paper demonstrates a simple approach for learning a robust few shot classifier. Despite its simplicity, the contribution of this paper remains weak and vague. As a result, the technical contribution of this work remains unclear. While this paper demonstrates good results with the proposed simple framework, there is no theoretical analysis of why such simple method is working.
Accept (Spotlight); rating score: 6; rating score: 6; rating score: 5; This paper presents a novel method for rule induction. The main weakness of relational pathfinding is the grounding problem, i.e., foreach training example, the algorithm needs to search for ground paths beforetraining. ### Some minor issues:  "unknown relations" should be called "invented predicates/relations", following the  convention in the area of program induction;  The algorithm implicitly assumes all rules are using the same template, which  is a dyadic chain meta rule [3], which could result in incompleteness [4]  (i.e., some hypotheses can never be learned), I think this should be made  clear in this paper. 4.Andrew Cropper and Stephen H. Muggleton. Logical minimisation of meta rules   within meta interpretive learning. Springer Verlag, 2015.<|endoftext|>This work explores a rule learning approach (R5) for 2 relation prediction tasks (CLUTRR and GraphLog). The proposed approach starts by finding connecting paths between the two query entities. The rules are induced whenever the output relation matches the gold relation   in a fashion similar to that of curriculum learning. The introduction of new relations, and application of RL are interesting. However, the task and model design seem to be very specific to a type of task where the relevant part of a graph (to the prediction) is restricted to a path, which can be recursively derived from the target relation. An interesting approach with good results for a limited task setup.<|endoftext|>It mainly contains two components: RL agent to model rule learning as a sequential decision making problem and a dynamic rule memory module to maintain and score candidate rules during the training of the reasoning agent. Strengths  S1 The dynamic rule memory module is interesting. Therefore, I don’t see why RL should be used in this scenario. It is possible for a two relation sequence to have multi heads. How can the proposed solution solve the multi head scenario? Although the idea is interesting, the model and the experiments are less convincing as pointed out in the weaknesses.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper addresses the problem of ad hoc teamwork, where an agent learns how to coordinate with a team of unknown teammates. I believe the paper is interesting and provides a novel contribution in terms of ad hoc teamwork (addressing partial observability). The proposed method, instead, builds a predictor of the ad hoc agent s marginal utility (which depends on the teammates) that relies on local information alone. "Associate latent encodings in learning from demonstrations."<|endoftext|>The core insight, which is learning a flexible and adaptable way of utilizing teammate observation in ad hoc teamwork via latent representations seems to be a very useful approach, and the overall architecture seems sound. Overall this is a reasonably solid paper which presents a novel approach for ad hoc teamwork policy learning via learning latent embeddings of partial teammate observations.<|endoftext|>The paper focuses on the important problem of building agents that can function as useful teammembers in a ad hoc team where agents can change behavior over an episode. The paper propose a new framework named ODITS that allows training an agent for ad hoc teaming applications without strong assumptions on observability or pre defined roles and categories. Removing some of limitations of required prior knowledge is quite important to allow full data driven learning of such behavior.<|endoftext|>This paper proposes a new method, ODITS, to enable effective ad hoc cooperation without requiring to predefine a static set of teammate types. This is done by training encoder networks to extract latent variables which capture the "situation type", which is trained to be a sufficient statistic for predicting the joint action value. I don t feel qualified to comment on the method s limitations in the broader context of previous work. Outside of the method s quality, the experiments seem well executed and the work is relatively clear.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The empirical studies also show that DAdaQuant can improve client sever compression. The strengths:1, The paper develops a new communication efficient quantized FL, Particularly, the client adaptive quantization is the first time to be considered for FL algorithms. 2,  The paper has done extensive experiments to show the improvement of proposed methods. The weaknesses:1,  Some math notations used in this paper miss definitions. For example, the definition of  $E_{p_1, ..., p_K}[Var(Q_q(p))]$ is missing. 2, The quantization $Q_q(p)$ used in this paper based on section 3.3 QUANTIZATION WITH FEDERATED QSGD is "$Q_q(p)$ then returns the sign of p and jpj rounded to one of the endpoints". This quantization is not an unbiased estimator of $p$, which is conflict with the claim on page 14. 3, The paper introduces the expected variance of an accumulation of quantized parameter $E[Var(\sum Q(p))]$ as a measure of the performance of quantized FL algorithm and tries to minimize it in Thereom 1. The authors should explain more why it is a good measure. Why not give the theoretical analysis in the following logic line: convergence guarantee  > get rounds T, given $epsilon$  > get computational complexity and communication cost? 4, In algorithm 1, RunClient($c_k$) is missing between line 8 and line 9. However, the theoretical analysis is not convincing and the commonly used analyses, like communication cost via communication rounds, are missing.<|endoftext|>In particular, the authors found (1) adaptively increasing the number of quantization levels (2) adaptively assigning different quantization levels on different clients, can effectively outperform previous static quantization schemes. The proposed two adaptive quantization strategies are simple and easy to implement in practice. In the experiments, they exhibit better performance than static quantization schemes. The idea itself makes sense and the authors proposed a theory grounded approach to make it work. ### WeaknessesI think the "time adaptivity" part in the paper is a bit improper. The contribution in this part is kind of trivial compared to previous works. In the introduction, the authors mentioned that "we observe that early training rounds can use a lower q without affecting convergence". But in fact, this observation was first made by previous literature, such as (Jhunjhunwala et al.ICASSP 2021). More generally, the intuition that one need more communication towards the end of training already appeared in two years ago in (Wang & Joshi, MLSys 2019, "Adaptive communication strategies to achieve the best error runtime trade off in local update SGD"). The way the authors wrote this part can make people think this paper find this idea, which is not. The authors are supposed to make it clear in introduction that "previous literature observed that.. and we further improve their algorithms by overcoming/addressing.."  The comparison of time adaptive quantization schemes with previous work (Jhunjhunwala et al.ICASSP 2021) is unfair. But the algorithm itself can be easily extended to the case where we only sample few clients at each round. Compared to this work, the authors should demonstrate (1) moving average of the loss help; (2) a step increase strategy is better than the strategy used in (Jhunjhunwala et al.ICASSP 2021); (3) the proposed strategy converges faster than any other static quantization schemes, as predicted in Figure 2. Related to the above point, I feel the authors failed to demonstrate the effectiveness of the time adaptive quantization methods. There are still many remaining questions. The authors do not compare the results with previous works AdaQuantFL. The authors fail to demonstrate the effectiveness of their proposed method.<|endoftext|>This paper studies the federated learning problem with a focus on communication efficiency. The major contribution of this paper can be summarized as:(1) Propose a time adaptive quantization algorithm that adjusts the quantization levelas training progresses(2) Propose a client adaptive quantization algorithm that assigns quantization level to individual clientsStrength:(1) A novel double quantization design(2) Communication overhead saving is promisingWeakness**(1)  Computational overhead in quantization**This paper proposes a double quantization strategy for efficient FL. While the saving in the communication overhead is promising, there is little discussion on the extra computational overhead introduced by the algorithm. For instance: i) what is the complexity of performing the time adaptive and  client adaptive quantization algorithm, ii) how does the overall training time being affected if we use the proposed algorithm, iii) what is the ratio between the communication time saving and the total training time**(2)  Comparison with other communication efficient algorithms**This paper compares with quantization baselines. How does the double quantization algorithm perform when we compare it with sketching based FL methods? This paper is well organized with a clear presentation. However,  there exist concerns regarding the efficiency of the proposed algorithm The authors are strongly encouraged to address these shortcomings by:(1) Benchmarking the total training time and the computational overhead in quantization(2) Comparison with other  communication efficient  FL strategies such as sketching<|endoftext|>The paper proposes a communication efficient federated learning framework named DAdaQuant. It is a quantization based FL compression algorithm, which chooses both time adaptive and client adaptive quantization levels to improve the communication efficiency of FL algorithms over previous quantization works. The authors also show solid empirical studies over FL datasets and validate the superiority of the proposed approach in communication efficiency. The paper provides clear intuition behind the double adaptive quantization approach. The work also presents solid empirical studies by comparing with a series of baseline algorithms over multiple classical datasets. For Theorem 1, what is the definition of $e^{p_1\cdots p_K}_q$? How is the agent specific quantization level $q_i$ s relates to the given quantization level $q$? 2.For Figure 4, the authors comment that for DAdaQuant, the communication cost does not scale with the number of clients. Can the author comments on this? The reviewer does not doubt the intuition. But is there any rigorous justification for this observation? 2.Have the authors tried other orthogonal techniques combining with quantization to further improve the communication cost empirically? The paper is lean towards empirical studies. So the reviewer does not judge from the theoretical contribution perspective. Overall, the paper well presents the intuition and the logic of the data adaptive quantization approach. The empirical study part is well rounded and the results are relatively solid.
Reject; rating score: 5; rating score: 6; rating score: 8; The authors propose a new task named NAS (natural attribute based shift), which is a sub task of OOD (out of distribution). Total three OOD methods are tested on NAS datasets and show an inconsistency in their performance. * The writing is easy to understand. * The acronym for the new task should be distinguished from the abbreviation for the Neural Architecture Search. Though the performance has not shown significant degrades, the time cost can not be ignored. Overall, I like the idea of this article, the proposed new task is valid.<|endoftext|>Also, for all the figures (a), (b) and (c), the AUROC is at 0.5 for D_I. The paper claims the following contributions:  A new task of NAS detection and datasets to study it. A novel analysis based on the location of shifted samples in the feature space and performance of existing OOD methods. The paper empirically demonstrates that they get comparable performance to the original model (even with the new terms) and are able to show good performance in the UTKFace dataset where the Mahanalobis model was not doing well earlier. I d like the authors to comment on if they consider such a problem to be a NAS problem and if their proposed solutions apply to this NAS setting?<|endoftext|>The paper defines a task called Natural Attribute based Shift detection (NAS detection) to identify shifted samples on some natural attributes, e.g.age, time and lighting. The authors create three benchmark dataset from existing datasets in three domains, i.e.vision, text and medical. The strengths of the paper are the following:1. 2.Extensive experiments to demonstrate the ability of OOD detection methods to detect NAS. The paper is well written with extensive experiments.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; In this paper, the authors show that applying the Swin Transformer to both neural and image video compression show significant rate distortion gains over Conv Transformer methods and provide an analysis of latents and ablasion studies to understand the difference in the two methods. This paper shows that Transformer based transforms can replace Conv based transforms in image and video compression, and simultaneously achieve better RD performance at much faster decode times. The results are strong across datasets for image and video compression and multiple entropy modeling techniques.<|endoftext|>This paper addresses the problem of improving rate distortion (RD) performance for learned image and video compression without ignoring runtime. The result is a much faster model that still achieves near SOTA rate distortion performance. In particular, the SwinT ChARM model described in this paper outperforms VVC (via VTM 12.1 a recent version of the reference implementation for H.266) with faster decode times. Although the authors don t mention it, VTM can actually be very slow to *encode* an image at a very high quality level (on the order of dozens of seconds). This conclusion is based on the strong empirical results and the thorough evaluation. The results are also somewhat surprising to me, which is an indicator of important research.<|endoftext|>The experiments show that Swin achieved better rate distortion performance with lower complexity than existing solutions. 3.For the detailed experiments, they show that the inference time of SwinT decoder is less than that of Conv decoder, also Swin has effective receptive field, incur less redundancy across different spatial latent locations, and progressive decodingThe authors did a novel exploration in using Swin Transformer for transform coding and proved that SwinT ChARM (the authors proposed) demonstrated better compression efficiency as opposed to CNN counterparts. It is a nice exploration and provides new perspective in image codecs.<|endoftext|>This paper proposed to replace the typical cnn based transform in image and video compression networks by Swin transform. Experiments show its better performance and lower computational complexity over cnn basd methods. The proposed transformer based framework shows good performance, proving that the transformer is a good replacement for typical convolution for compression tasks. The contribution of this paper is simple and direct, and the results also show good performance.<|endoftext|>This paper introduces the swin transformer for the learned image and video coding. The experimental results are impressive. Although introducing SwinT is not very new, the authors provide a neat and practical solution, which should be encouraged. Although some latest works like Guo et al.achieve slightly better compression performance, the proposed approach is much neater. If we can use this approach to replace the expensive entropy coding method, could we achieve a better performance complexity trade off even we use Conv based model? (3) There are some related works in video compression, it is suggested for discussion or comparison.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This behavior is analytically shown on (deep) linear networks by writing out the dynamics of the NTK. It provides a new perspective of the "rich regime" of neural network training. I vote for acceptance of the paper. The claims are backed by analysis on (deep) linear networks and align well with empirical evaluation. ## Strengths* From the claims and the cited references this seems to be the first work that figures out the so called "silent alignment" effect. ## Weaknesses* The analysis was done on (deep) linear networks.<|endoftext|>This paper identifies a phenomenon called the "silent alignment effect," which happens in linear networks as well as ReLU networks on whitened data, with sufficiently small initialization of the network weights. The paper also demonstrates that non whitened data can weaken this effect, for which the NTK needs to evolve later in training. Understanding the behavior of neural networks outside the NTK regime is a major question in the theory of deep learning. 4.Overall speaking, the results are quite restricted since they are only for linear networks and whitened data.<|endoftext|>al.A unifying view on implicit bias in training linear neural networks. The authors call it “silent alignment”, and show that this happens for deep linear networks with small initialization and whitened data. Minor: 	There is a typo in the definition of $K_{c,c }$. The authors demonstrate the effect empirically.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a novel graph diffusion model with a source term in order to solve the over smoothing issue of GRAND model and GNNs with diffusive property when the depth of the GNNs is large. Solid theoretical and experimental results are provided in the paper. The causes remain unexplained.<|endoftext|>In this paper, the authors propose a graph deep learning method called GRAND++. This framework based on GRAND and can work with a limited number of labeled nodes. The authors also provide the source code to facilitate experimental replication. The overall quality of the paper is good, with a clear narrative. So I recommend it for acceptance.<|endoftext|>The paper addresses an important issue which affects neural ODEs and deep NN on graphs. In particular, concerning the GRAND differential equation, I think that adding a source term is an important idea essential to obtain a non trivial long term behaviour of the dynamical system. The theoretical analysis proposed in the paper deals mostly with the simplest case of a linear diffusion operator, which is certainly useful to gain intuition. So this seems to be the case of a $G$ that does not depend on $t$ NOR on $X$.<|endoftext|>This paper studies neural based diffusion models for graph data. It considers deep learning on graphs as a continuous diffusion process and treats GNNs as discretizations of an underlying PDE. They theoretically show the effectiveness of the source term in mitigation over smoothing problem. They theoretically show that the source term helps alleviate the over smoothing problem. The experiment regarding limited label data is interesting. Cons and concerns:One of my main concerns is the empirical results reported in the paper.<|endoftext|>The paper introduces GRAND++, GRAph Neural Diffusion with a source term. As its predecessor GRAND, this method is focused on the development of a new continuous depth GNN to tackle the over smoothing and the bottleneck issues which are typical of deep GNNs.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; A general loss function should get rid of constraints of learning tasks and datasets. Motivated by this insight, the paper expresses the loss function as a linear combination of polynomial functions and shows that cross entropy loss and focal loss are special cases. The final version $\text{Poly} 1$ is with first order. The paper verifies the proposed loss on ImageNet for classification, MS COCO for detection and instance segmentation, and Waymo Open dataset for 3D detection. The final PolyLoss is simple and effective. The idea is new, and the discussion is deep.<|endoftext|>In this paper, the authors introduce a new loss function for classification problems. To be specific, the authors introduce Taylor expansion of cross entropy loss + focal loss and show that various subsets of this expansion can improve the models on image classification, 2D object detection and 3D object detection problems. An application of the Taylor expansion of cross entropy loss and focal loss on different classification problems. Weaknesses:  The major problem that I had with the first version, which persists with the current version, is that some results are not significant. The authors obtain very minor improvements on several models and tasks. In general, I am positive about the paper, though I need to be convinced about the significance of the results and the importance of the work.<|endoftext|>The paper proposes a framework for creating loss functions based on the polynomial expansion of known loss functions. The authors show that, by fine tuning the polynomial coefficients of those expansions can bring improvements in multiple computer vision tasks. The authors experiment with image classification (ImageNet 21k), instance segmentation and object detection (COCO), 3D object detection (Waymo), showing small improvements over the chosen baselines. It provides an interesting observation showing how different losses could all be expressed in a common polynomial form where only coefficients differ. It is not entirely clear if the added complexity (in both having to fine tune the coefficients as well as determining where the expansions should be cut) would be justifiable. After rebuttal, most of my concerns have been addressed.<|endoftext|>This paper analyzes the cross entropy loss and focal loss through the polynomial expansion perspective. Further, this paper proposes a family of loss functions called PloyLoss. (3) The proposed loss function can improve various tasks: image classification, instance segmentation, and 3D object detection. ## Weaknesses(1) surprisingly, the final version of PloyLoss is Ploy 1. This paper starts from an interesting loss family as PloyLoss, but the final version is quite simple and trivial. After rebuttal, most of my concerns are addressed.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper proposes Federated Averaging Langevin Dynamic algorithm (FA LD). The paper has shown extensive theoretical analysis for the proposed FA LD method, such as convergence for independent noise and varying learning rates, with or without full device participation. However, given the existing theoretical analysis for SGLD based on 2 Wasserstein distance and theoretical analysis for FedAvg with or without full device participation, the results are straightforward. Could the authors show the difficulty of the theoretical analysis in this paper? Empirical studies are missing in the current version. The experiment section should be added to the paper to validate the theoretical findings and investigate the performance of FA LD. Particularly, the reviewer would like to see the performance of FA LD in terms of different local updates, learning rates, participating devices, etc, and its comparison with other distributed LD methods. Typo:section 3.2, "a energy function"The paper has given extensive theoretical convergence analysis for FA LD algorithm via 2 Wasserstein.<|endoftext|>This paper studies federated learning algorithms, where a collection of computing nodes seek to collectively optimize a global objective through parallelized updates executed at a server. Different from prior works which execute local only stochastic gradient updates on non convex losses, here a stochastic gradient Langevin update is developed, which additionally incorporates randomized Gaussian perturbations. Convergence theory is presented for the proposed scheme, which establishes point wise  convergence in mean, as well as convergence in distribution according to the Wasserstein metric. Assuming strong convexity (Assumption 5.2) seems completely ridiculous in this setting. One of the major motivations for considering stochastic gradient Langevin dynamics is to improve the performance of non convex stochastic optimization algorithms so as to escape spurious stationarity points and arrive at global extrema. Is there a practical phenomenon where standard federated learning or its variants that use Lagrange multipliers such as FedDyan and SCAFFOLD get stuck, whereas this version with randomized perturbations does not? I feel that this scenario is exactly in non convex settings, which is excluded from consideration in this work. It is difficult to make sense of what are the specific technical innovations in the analysis here, and how they are a departure from earlier results. This work is mainly of theoretical interest, as it considers randomized perturbations in a federated averaging setting. Therefore, it seems of limited usefulness in my view.<|endoftext|>2. develop theoretical guarantees for FA LD for strongly log concave distributions with non i.i.d data and study how the injected noise and the stochastic gradient noise, the heterogeneity of data, and the varying learning rates affect the convergence. 3. analyze the partial participation setting. The idea of sampling using Langevin diffusion in the federated learning setting is new. Weakness: Since this is a theoretical paper I am basing my decision on its theoretical contribution. Only part of the proof which is different from the traditional proof of LD is the divergence term. But showing that this term is small (Lemma B.3) follows very simply from typical strongly convex optimization techniques. All the results corresponding to partial participation and varying rates follow almost directly  from the proof of the main theorem and does not have much theoretical novelty. 2.I liked the idea of allowing for correlated noise but I could not find the proof of theorem B.8 (theorem 5.9) in the paper. So it s difficult to gauge the difficulty of this part or even the correctness of this result. If I have missed the proof, and the authors could point me towards it, that would be great. The paper lacks theoretical novelty but I liked the idea of introducing LD in federated setting. I may change my decision after discussion with other reviewers.<|endoftext|>The paper proposes a federated averaging Langevin algorithm (FA LD) and analyzes its convergence under non iid and partical participation settings. The main contribution is the theoretical guarantees for a federated averaging Langevin algorithm for strongly log concave distributions with non i.i.d data. The proof sketch is well written and clear. It seems that the proof is sound. I think the writing has room to improve, but overall it is clear and easy to follow. I have the following concerns:1. Besides, how $\beta_k^c$ involves the Local Updates step in one iterate of the FedAvg algorithm seems to be incorrect. I think it should add $\theta_k c   \beta_k^c$ for $2 \le k \le K$. 2.Algorithm 1 is easy to understand since it is an approximation of FedAvg. However, Algorithm 2 is not so intuitive. How does such a shared noise ensure privacy and what kind of definition of privacy is used ? To substantiate the point, I think more argument and discussion should be added. 3.The current presentation of main result is troublesome. The author could first introduce the main result and then provide a proof sketch. What’s more, the analysis for algorithm 2 is deferred to Section 5.3.3, but its introduction is Section 4. 4.The first paragraph of Section 5.2 seem to have a type. $\nabla f\left(\bar{\theta}_{t}\right)$ should be defined as $\sum_{c 1}^{N} p_{c} \nabla f^{c}\left(\bar{\theta}_{t}\right)$. 5.Though this paper is theoretical, the author could still provide some numerical experiments to show the convergence of FA LD. After all, FA LD is different from FedAvg. Besides, there are some presentation issues and ambiguity on motivation. I don t think this paper is ready for publication.<|endoftext|>This paper studies the federated learning problem. In this model, local clients are allowed to jointly train a model without sharing user data, and the central goal is to design communication efficient algorithms. The author gives a federated averaging Langevin algorithm and provide theoretical guarantees for strongly log concave distributions. These results provide guidance on the choice of learning rates and local updates to minimize communication cost. The authors also consider applying correlated noises and using only partial device updates, which are more applicable in practice. The authors present non trivial non asymptotic convergence analysis for FA LD for distributions with strongly smooth and strongly convex Hamiltonian and with bounded variance of noise in the stochastic gradient. More importantly, it shows that the number of local steps should be set roughly as the order of square root of the condition number. The highlights of this paper are novel theoretical analysis of the FA LD algorithm. It would be nice if the authors could also provide some lower bound or worst case analysis for the FA LD algorithm, to show that the current convergence guarantees are tight or close to tight. Detailed comments:Page 3, right before Section 4: “\tilde{f} is a unbiased estimate of f” Shouldn’t it be “\grad \tilde{f} is an unbiased estimate of \grad f”? Overall, I think this is a nice paper with strong theoretical results. The paper is also well written.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper proposes an alternative to the successor representation (SR)   the first occupancy representation (FR)   which represents the expected time to first visitation of a state. The paper motivates the representation to be applicable in environments with non Markovian rewards and show that the proposed FR can handle such cases. [Full Disclosure: I have not checked the accuracy of the theoretical claims thoroughly; they seem appropriate on a quick glance.] The theoretical results are also discussed elaborately in the appendix. 3.The suite of capabilities considered, and demonstrated, is very complete and makes a good case for "FR > SR"   exploration bonus, unsupervised pretraining and planning. The paper makes a great case for the _completeness_ of a first occupancy representation. The paper presents a simple idea which is supported by ample theoretical analysis and (some) experiments. I vote to accept the paper but strongly urge the authors to improve the empirical analysis for the overall impact of the paper.<|endoftext|>Unlike successor representations, which compute the likelihood of occupying a particular state at any time in the future, when starting from a given initial state, the proposed first occupancy representation encodes the likelihood of reaching a particular state for the first time. The benefits of the proposed representation are illustrated in several decision problems with non Markovian reward structure. The use of the first occupancy values as an exploration bonus results in much more efficient exploration. The authors also demonstrate that FR can be used effectively for unsupervised pre training RL with non Markovian rewards, where the successor representations tends to produce incorrect estimates of the value function. It is also apparent that the proposed approach has some limitations, and the authors readily acknowledge this.<|endoftext|>This paper proposes a new notion of state representation, the first occupancy representation (FR), inspired by the Successor Representation. It is motivated by situations where the rewards are non Markovian. The usefulness of the FR is demonstrated on exploration, unsupervised RL, planning and escape behaviour tasks. This paper is well written and well motivated. The idea is simple but is a nice extension of the concept of the SR. In particular, the case of non markovian rewards is studied in section 4.2. How does the FR relate to the dynamic distance function introduced in https://arxiv.org/pdf/1907.08225.pdf ? In Section 4.2, could you provide a reference for  “a bonus to maintain its effectiveness as time progresses in order to prevent the rate of policy improvement from exponentially decaying“? See https://arxiv.org/abs/2101.07123 https://arxiv.org/abs/2103.07945Minor points: Typos: take advantage *of*, theoeretical I think this paper provides a nice contribution to the topic of representation learning in RL. The authors provided theoretical results for their FR and demonstrated its benefits on 4 different tasks.<|endoftext|>This paper presents the first occupancy representation, a sort of non Markovian analogue of the successor representation that carries information about the time required to reach a state for the first time rather than the full discounted occupancy of a policy. The paper also shows demonstrations of its usefulness in unsupervised RL and exploration. The main issue I can find is that the proposed first occupancy representation might not be as new as the paper describes. For a more modern take on the same problem, [dynamical distance learning (DDL)](https://arxiv.org/abs/1907.08225) also estimates the expected number of timesteps required to reach a goal state, and [temporal difference models (TDMs)](https://arxiv.org/abs/1802.09081) estimate how close a policy will come to a goal state in a fixed number of timesteps. It is not necessarily an issue that there are papers from the last few years investigating a similar line of inquiry; the paper is still a thorough description of this idea and has enough in it that it is not subsumed by these prior works. Its main issue is that it frames itself as a sort of preliminary description of an idea, with a somewhat limited evaluation in any one setting, but very similar ideas have already been shown to scale to the types of problems commonly studied in contemporary deep RL.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper proposes a modification of Granger causality so that it is applicable in partially ordered observations. Moreover, the lack of explanation from the authors regarding their intuition in some key points of the paper contributes to lack of clarity. 2.I appreciate a lot the presentation of the related work and the comparison experiments.<|endoftext|>Could you comment on that? Overall the method is sound and the authors have put in a lot of work to showcase the merits of their approach. The paper is very carefully written.<|endoftext|>The Var Granger approach will perform poorly since it is just a simplification of GrID Net. The authors did a very good job addressing many of the concerns and comments raised by us and the other reviewers. UPDATE We read all of the authors detailed answers and reviewed the updated manuscript. We understand that the relation to eQTL based calls are problematic but with such poor correspondence it brings doubt to using this as a metric. The problem itself is interesting and important. What is y in this case?<|endoftext|>This paper developed an exciting method, GrID net to infer granger causality between multi modality on directed acyclic graph (DAG) in the framework of deep neural network (GNN), aiming to improve identifying causal interactions across modalities. •	Is it possible to compare DAGs by alignment based methods such as manifold alignment methods? I suggest that the authors also benchmark pseudotiming approaches. •	How cell type specific are those causal gene region interactions? on single cell data applications. The GrID net modeling was also presented clearly.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 5; All experiments are based on NAS Bench201. Comprehensive description on the problems of supernet based NAS;2. I would expect more experiments on more search spaces and algorithms.<|endoftext|>I suggest that the authors add a conclusion part to discuss the analysis results for Section 4 and 5.<|endoftext|>I strongly recommend the authors to comment on these results.<|endoftext|>All figures except Figure 2 are quite large, and can be shrunk to a smaller size. In other words, I suggest the authors consider using recent NASBench 301 surrogate benchmarks to perform solid test to see the findings can or cannot generalize.<|endoftext|>I recommend rejection. The authors run experiments on NAS Bench 201 and use their evaluation technique to identify the best combination of architecture training and selection.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; This work approaches the partial label learning problem where each training example is annotated with multiple candidate labels, in contrast to the conventional supervised learning setup that the ground truth label is provided. The experimental results look quite strong, where the performance of PiCO nearly approaches the fully supervised results. This paper is clear and technically solid.<|endoftext|>Moreover, its performance strongly approaches the one of supervised learning. ## Strengths The problem that the paper tackles, namely partial label learning, is important in the real life where labelling is difficult due to semantic ambiguity (e.g.husky vs malamute). The approach is backed by very impressive empirical results. This is a strong paper tackling an important problem. The approach is interesting and the technique is sound.<|endoftext|>The authors present a new technique for partial label learning (PLL). Strengths  Incredibly strong PLL performance close to supervised learning and substantially stronger than baselines.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; It is unclear whether the torus topology is as crucial as claimed. How were the tuning parameters (beta, etc.) Minor comments   The convention (as well as the rules of English) seems to have "autoencoder" as a single word, not two, but I could be mistaken. In eq.9, "N(0,1)" should be "N(0, sigma)"? The paper is not perfect, and could improve in being more critical of the approach proposed, as well as reproducibility in terms of describing the evaluation procedure.<|endoftext|>Strengths: I enjoyed this paper. The method of decomposing onto a set of circles that form a D dimensional torus makes sense and is clearly described. These are addressed by the authors in the conclusion. Since these were my main questions, I would like to see the heatmaps as in Figure 2 for the other experiments described in the “Dependence on beta and on D” section, but these could be added to an appendix. I would also like to see a deeper investigation of the proposed DC score metric, but, as is, it makes sense and is effective. If not, I think the notation could be cleaned up to improve clarity. I think the paper is well composed, interesting, and convincing. For these reasons, barring something major that I have overlooked, I recommend acceptance.<|endoftext|>A D dimensional torus is represented by the tensor product of D unit circles. They argue the torus induces disentanglement as the analogous of entropy entanglement in quantum physics. The main drawback of this paper is the lack of theoretical analysis. I m puzzled about the connection between this study and quantum physics. The concept of the torus representation is novel and its practical value is empirically convincing. The connection to quantum physics is not well explained.<|endoftext|>The motivation  entropy of entanglement  is not clearly explained. 3.I can see there are two steps of new techniques incorporated in the proposed method. The other one is the flattening operation defined by the tensor product $\otimes$ in Eq.(5).Can the authors explain why they are motivated individually? I think the authors should provide a solution to this obvious problem in this paper with experimental support. Based on my concerns, I think this paper requires some significant improvements and I rate it as rejected for now.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; 4.For  fair comparison, the improved generalization needs to be balanced by a more formal analysis of the privacy loss. The results reported here in this regard are experimental. On the other hand, the key downside to their approach is the privacy loss. In my view, this paper does not adequately analyze the privacy implications. 1.Assumption 1 feels a little ungainly, and circular.<|endoftext|>Another concern is the assumption of iid data which makes the applicability of the proposed algorithm limited. I wonder if the authors can elaborate more on this by providing relevant references and a more detailed technical discussion. In general, the technical and theoretical results of the paper are quite marginal.<|endoftext|>The main contribution is to discuss how to solve the small data research question in FL. Also, I would say the analysis of the privacy guarantee is convincing. To my best knowledge, there is limited work discussing this research question in this field;+ They cover the key points in this paper including privacy discussion, communication cost discussion, and comparison with selected baselines. The non iid setting is another point that most research papers would discuss in their work. The authors develop an algorithm to solve this question by exchanging model parameters across clients rather than exchanging data.<|endoftext|>I believe the authors should address this line of research as well. Code was also provided (for the synthetic data experiment only though). The paper presents PAC like ($\epsilon, \delta$) guarantees for their algorithm, and provides a discussion on privacy violation concerns of their algorithm. Most experimental details were given.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors demonstrate a deep and comprehensive understanding of the literature. * Though code was not provided, the experimental details look sufficient to reproduce the main results (hardware permitting…)## Weaknesses/Questions* I want to better understand what it is about Gumbel specifically that is important here. These subproblems are implicit in the contribution bullet points in the introduction, but they are also not exhaustive. This paper is likely to be of theoretical and practical interest to many in the ICLR community. But it seems that the Gumbel distribution itself is not really important   what’s important to achieve the inequality is that the same $g$ is used both for action selection during search, and for policy improvement after search. The improvements claimed in the introduction are:   1.<|endoftext|>This paper redesigns AlphaZero and MuZero with the principle of policy improvement and claims the following. This paper presents empirical results as well as a theoretical proof. Why would Gumbel distribution outperform the current popular UCT (or PUCT) for AlphaZero/MuZero in some cases? or rollouts (or MuZero) (as in Figure 1). That is, it seems that Gumbel MuZero already worked well without this additional improvement. The authors should give more discussion. In experiments, more information needs to be shown.<|endoftext|>The paper proposes a number of principled algorithmic modifications to state of the art planning algorithms (AlphaZero, MuZero) for improving performance in settings with many actions and a relatively small computation and / or sample budget. The experiments show that the Gumbel variants of AlphaZero and MuZero perform well in low search budget settings in the domains of Go, Chess and Atari. At the smaller simulation budgets, I d expect the impact of planning to performance is likely limited compared to the learned policy. Is this correct? Overall, the paper contains a number of intuitively clear, principled algorithmic improvements to AlphaZero (and MCTS in general) when used in low budget settings. After reading it along with the other reviews, I continue to remain positive about the paper.<|endoftext|>See detailed comments below. What would happen if the value of the policy were estimated solely based on the values backpropagated from previous visits to (other) children? I suppose in practice you would need the value network also to produce the "bandit rewards", but those could be replaced with rollouts... Overall a strong paper, with at this point unfortunately slightly too many places where technical details are not entirely clear or only become clear much later than they should be. The main paper summarises the proposed policy improvement operator as Eq.(11), and states that a proof for it being a policy improvement operator can be found in Appendix B. > I do not understand what this is trying to say.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; The paper focuses on the problem of graph sparsification   a problem of approximating an arbitrary graph with a sparse graph (specifically the one produced only by the  removal of edges in the context of this paper) while retaining the desired structural properties of the graph. Can the authors elaborate on why this is the case? Hence, this work is not ready for publication in its current form. Following are some specific questions related to that: 1.<|endoftext|>Should not this be an input quantity? The paper is not ready to be published. For evaluating the effectiveness of graph sparsification techniques, the authors investigate aspects of PageRank preservation, community structure preservation and shortest path distance preservation. I find the description of the edge sparsity control confusing in algorithm 1 on page 4.<|endoftext|>I‘d recommend making a stronger argument for why this particular graph sparsification problem is important and why one cannot (or would not be advised to) directly optimize for the downstream task of interest. In its current form, I am unsure about the significance of this work — especially since an RL based objective seems to be much more inefficient to train than the end to end differentiable graph sparsification technique (for a particular downstream task) introduced in NeuralSparse. While the paper is overall well written, it requires some further improvements in clarity in order to meet the bar for acceptance. The experimental evaluation does a great job at demonstrating superior performance to a wide range of baselines on a wide range of graph datasets.<|endoftext|>The paper provides some new insights to graph sparsification and takes an initial step towards learning based graph sparsification. The paper provides a novel RL framework for learning based graph sparsification. 1.The work is a novel application of deep reinforcement learning on graph sparsification and technically sound from the perspective of graph sparsification.
Reject; rating score: 6; rating score: 6; rating score: 6; The paper introduces Relative Molecule Attention Transformer, adding distance , bond  and a neighborhood embedding  in the attention matrix, and pretrain the model on 4M molecules via context pretraining and on 200 rdkit descriptors. Adding distance information directly to the learned "distance" before computing the softmax as well as additional information as in MAT is a very good idea (similar to ALiBi [1]). There are datasets the support a performace increase of the proposed method over baselines and established methods, however 1. In R MAT: is that it is first embedded, then a learn projection; rather than taking the distance matrix directly times an e.g.a learned scalar; is there a clear advantage of one over the other? 2.Table 1 reports the best performance for ESOL and FreeSolv, given that R MAT was pretrained on 200 rdkit descriptors and in one model variant uses those descriptors in the last layer, please make sure to also include a baseline variant of e.g.RF with the 200 rdkit descriptors (a few of whom describe solubility), (additional to using fingerprints) 3. One disadvantage is the costly calculation of the distance matrix, which should be mentioned at least once in the paper 6.<|endoftext|>The paper proposes a new transformer network architecture to pre train the molecule datasets. Based on the Molecule Attention Transformer, the proposed model, R MAT, incorporates a few handcrafted features into the self attention layer of a transformer architecture. The features can incorporate distances between atoms from multiple perspectives. The experimental results show that the pre trained model is useful to predict various properties of molecules. Theoretical justification on why it is impossible to learn these features with the vanilla Transformer or how difficult to learn such features. Difficult to reproduce the result due to the large scale experiments  Questions      How many overlaps between the task datasets and pre trained datasets? Increasing the maximum neighborhood order does not improve the performance. How do we interpret the result? What makes EGNN perform the best with the QM9 dataset (Figure 3) and why the proposed method cannot achieve a similar result?<|endoftext|>This paper proposes a relative self attention layer for the Transformer model. The proposed relative molecule attention Transformer can be first pretrained with a contextual property prediction task and then a graph level prediction task. Strengths:The proposed relative self attention is carefully designed to consider three factors: relative distance, shortest path distance in the molecular graph, and physiochemical. Inspired by the positional bias and content bias from NLP methods, a novel relative molecule self attention is proposed and helps the Transformer model achieve good experimental performance. Could the authors include results on more datasets like GROVER? Evaluating only on selected datasets makes the readers suspicious of the generalization of the proposed method.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces a Few shot OOD classification model, named FROB. While this paper offers an interesting angle into OOD detection with few shot examples, several flaws regarding writing/methodology/results must be addressed before this work is ready for publication. Two of the primary metrics, AAUROC and GAUROC, are not defined. Why is it that using 100 SVHN examples gives the exact same AUC as 70k examples from 80M? Additionally, the paper often uses 80M tiny images dataset, which has been retracted by the original creators. The focus on AAUROC and GAUROC is misplaced.<|endoftext|>The paper proposes to generate the support boundary of the normal class distribution and combine it with few shot Outlier Exposure (OE) to improve the OOD detection performance. 2.The results are not clearly presented. That’s strange because the current SOTA AUROC based on a wide resnet model is already around 0.95 [1]. I found the mean AUROC value in Table 4 for FROB was computed wrongly. However, the paper needs to be further polished such that the method and results can be clearly presented.<|endoftext|>The paper addresses an important issue of Out of Distribution detection in a few shot setting. I believe more work needs to be done in order for it to be accepted. It is possible that the proposed approach is significantly different, however lack of comparisons and description does not allow to assess that. Proceedings of the Asian Conference on Computer Vision.<|endoftext|>This paper focuses on classification and Out of Distribution (OoD) detection in the few shot setting. They propose the Few shot ROBust (FROB) model for classification and few shot OoD detection. The experimental results are sufficient. (2), which is the key component of the proposed method, are trivial.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper addresses the problem of doing unsupervised learning of visual representations, including clustering these representations to infer the existence of new categories. I have worked with many of the cited self supervised methods, and have published in self supervised learning as well. The authors compare their method with other popular self supervised learning techniques like SimCLR. First of all, I am very enthusiastic about the general topic that is being addressed: unsupervised (or self supervised) learning in more realistic environments specifically those in which we may see a large number of examples of a small set of objects, and perhaps a few examples of some other classes. Thus, the topic is highly relevant and important at this time. I found several aspects of the paper difficult to follow. One major issue was trying to understand the exact setting of the RoamingRooms data set task. Second, it is not clear if this procedure favors the current algorithm. If it is provided at test time, this makes the task much much easier. However, not enough details are given about the details of this problem. If such an approach is not reasonable, it is hard totell from the current text. In my assessment about "confidence", I have said that I am fairly confident of my review. If it is trained on the current data, could you give details about how this training is done? It is possible that if I had read all of the referenced papers I would understand these details, but a reviewer should not have to wade back through papers to understand these details. It should be easier to understand what was done. In particular, I am not someone coming from far outside the area; thus, the paper should have laid out the details more clearly so that someone with my background would understand it. That may be true, but it is not for lack of trying.<|endoftext|>This work studies an online version of self supervised representation learning. An little populated, isolated cluster may be more important than a highly populated cluster with many neighboring/overlapping clusters. Avrithis and Kalantidis 2012, "Approximate Gaussian Mixtures for Large Scale Vocabularies" develop an offline but dynamic (in terms of components) EM algorithm that removes mixture components based on pairwise interaction. The protocol of comparisons to non online methods is unclear. The choice of competitors is not well justified: there are simpler methods that are easier to adapt to online. Since $\sigma$ is a scalar (isotropic), this wouldn t add much complexity to the model and it would help in deciding when to add or remove prototypes, potentially getting rid of (or learning) hyperparameters $u_0$ and $\mu$. The networks used are too small. Softmax is mapping a vector to a vector, while here only a scalar is shown (the numerator of the LHS). As a result, the protocol of comparisons to offline methods it is unclear. Whouldn t such simple alternatives be much easier to adapt to online? One should provide a clear motivation for the model and all choices (which is missing), definitions all quantities (some of which are missing) and clear pointers to Appendices for all missing steps in the derivation. The following works should be discussed. One should at least separate the method specific hyperparameters (e.g.$K, \rho, \alpha$) from standard hyperparameters (like backbone and learning rate). The entire problem of incremental and online learning is to learn without forgetting. Online self supervised learning is a very interesting and realistic setting. Is this justified by the unsupervised setting? Or is there another reason?<|endoftext|>This paper claims that in real world the data distribution is nonstationary, which is different from the current standard machine learning formulation. To solve this problem, the authors design an online unsupervised learning algorithm with Gaussian mixture model and EM algorithm. Part of the successes of deep learning comes from the well collected dataset, e.g., ImageNet10K. To study some area, someone should first to collect enough high quality data. However, this is different from the cognitive process of human being. This paper questions the standard machine learning formulation of drawing i.i.d.samples and claims that the real world scenario should be nonstationary. You see, the tranditional learning formulation has its assumption that all samples are drawn in an i.i.d.way.But it doesn t violate the eventual performance. It works quite well. In this paper, the author assumes a new scenario of nonstationary distribution. To solve the unsupervised and online setting, they design a novel online unsupervised prototypical networks (OUPN). The idea is interesting and encouring, but the necessity is not well claimed. The difference between this method and life long learning / incremental learning is encouraged. Also, the hyper parameter of maximum number of K clusters should pay much more attention.<|endoftext|>The authors propose a method for unsupervised learning of instance based clustering which is more robust to data imbalance and non iid distributions than existing approaches, such as SwAW. The method is mainly evaluated on RoamingRooms   a dataset with an agent moving through indoor environments with instance labels assigned to objects. When compared to offline and online contrastive learning algorithms the proposed approach demonstrates stronger performance especially with low batch size and non iid distribution of examples. The experimental evaluation on the task of unsupervised, instance based clustering in the online, non iid setting clearly demonstrates the superiority of the proposed method compared to the baselines. However, I have two main concerns with this paper. Firstly, although it is well written, the presentation is quite dense and many details have to be inferred from the context. I understand that it s hard to pack so much content into 9 pages, but I would argue that the authors could make significantly more effort to better explain the results. In fact, the main issue with the presentation is that no ablation analysis is provided. There are several tables in the supplementary material which compare the values of the many hyper parameters in the method, but the accompanying text literally just takes 9 lines. This is not sufficient to understand what actually makes this relatively complex method work better than the baselines. After all, the goal of a research  paper in not to report strong numbers on a benchmark, but to provided some knowledge about the problem being studied. I would argue that it s not, and the authors agree with me, since all the claims are made in terms of semantic classification/categorization.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposes the scalable GAMLP architecture that can learn node adaptive features and can utilize label propagation to improve the model performance. The writing of this paper is very clear, the organization is easy to follow and the main ideas are delivered clearly. According to my understanding, both papers try to improve the SIGN model [2] by introducing a node wise attention mechanism and utilizing label propagation to further enhance the performance. [2] Frasca, Fabrizio, et al."Sign: Scalable inception graph neural networks." My major concern is the novelty.<|endoftext|>However, I’m still concerned about the novelty of the paper because it proposed several ideas but none of them is really novel. The results show that the proposed GAMLP(JK) and GAMLP(R) can outperform the baselines. However, I m hesitant to accept the paper because I haven t seen a contribution that is clearly novel. The proposed "JK attention" and "Recursive attention" are also not very novel and are largely borrowed from previous works like JK Net.<|endoftext|>The paper proposes an efficient method for inductive/transductive node labeling based on fast feature propagation, label propagation, and attention and yields good experimental results. 2  In terms of contribution, the paper seems to have incremental contributions with respect to previous work but not a significant one. Using a weighted average over the features from different layers seems very similar to attention graph pooling and also feature propagation is not different from previous work.<|endoftext|>The experiments show improvement on 12 graph classification datasets. The paper proposes an end to end model that first propagates separately node features and labels, combines them with attention weights, aggregates embeddings and trains MLP model. For node adaptive attention mechanism the authors propose two schemes: Recursive attention and JK attention. Experiments clearly show state of the art performance. 3.Cora/citeseer/pubmed datasets are quite small to motivate such an approach.
Reject; rating score: 3; rating score: 6; rating score: 6; The architecture is an extension of a previous architecture for the same task, known as the parasiamese network. The new architecture, called the repelling parasiamese network, essentially contains a copy of a siamese network and a parasiamese network tied together using a euclidean loss. While this paper proposes a new architecture for antonymy detection with provably strong results, I do not recommend this paper for acceptance due to several reasons, the chief among them being the lack of clarity in the paper. Finally, the paper does not contextualize itself very well with respect to prior work. I am also uncertain about the quality of the dictionary based dataset.<|endoftext|>The paper presents the new approach for distinguishing antonyms and synonyms. The authors present proposed the repelling parasiamese neural network. However, there are some moments that require clarification. Also, it would be better to show more details about datasets. Overall, the paper presents an interesting approach. However, more attention should be dedicated to the details in theoretical part and more thorough testing of the proposed approaches.<|endoftext|>The task is to say which pairs are synonyms and which are antonyms. It is always nice to find a new dataset such as Fallows. I think there are some nice contributions here. The main innovation of the paper is the repelling contribution to Etcheverry and Wonsever, though the Fallows dataset might have more impact on future papers by other researchers.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; This paper studies a splitting based gradient descent method for computing optimal transport objectives. It s based on splitting the row/column sum constraints into two objectives, and using a variant of the DR splitting algorithm. Some of the subtleties, such as numerical instability of sinkhorn2, is also something that I have observed before, so I consider the experiments quite thorough. The algorithms are theoretically well founded, and have convergence guarantees typical of this type of methods. My main worry about these theoretical guarantees is that they are quite difficult to compare vs. unconditional bounds such as those obtained via combinatorial optimization packages, or the recent (purely theoretical) n^2 type methods (https://arxiv.org/pdf/2101.05719.pdf). Overall, I believe this paper has useful ideas for improving the performance of OT methods, especially those currently in use, but it falls a bit short of proposing something that significantly advances the state of art for OT.<|endoftext|>This paper introduces a new splitting scheme for the original discrete OT problem. The main technical contribution is a new splitting scheme by a recent result on efficient range space projection, which is very interesting. The authors also parallelize the algorithm for GPU computing and made the procedure numerical stable and efficient. The theoretical results are somehow standard as DR splitting is a special case of ADMM, of which the standard convergence behavior is well understood now. My main questions are as follows:* The experiments are mostly conducted compared with the Sinkhorn method. As the global linear convergence (due to global Hoffman bound) is proved, it would be interesting to see the empirical performance for that. The new splitting Eq.(7) is interesting, and the authors have spent an extensive effort to make the implementation efficient and robust.<|endoftext|>In order to solve large scale optimal transport problems, the authors propose a Douglas Rachford (DR) splitting method that solves the original OT without entropy regularization. The proposed DROT algorithm has the same cost per iteration as the well known Sinkhorn method but avoids its numerical issues. Furthermore, the proposed method possesses the strong convergence guarantees for its linear convergence rate; and it can be implemented efficiently using parallel processing on GPUs. It s a novel idea to rewrite the original OT problem into the standard form for DR splitting and exploit the DR splitting algorithm to solve it. The organization of this paper is satisfactory, and the flow of algorithm derivation is very clear. Log domain stabilization of Sinkhorn proposed by [1] is also a popular strategy to deal with numerical issues introduced by entropy regularization, while there is a lack of comparison with it.<|endoftext|>This paper proposes a novel method for solving the discrete optimal transport problem based on the celebrated Douglas Rachford splitting (DR) method. Experiments on a synthetic problem show that the proposed method (using the authors  CUDA implementation) is more numerically stable than the standard Sinkhorn method, while being comparably fast. They also provide a rigorous theoretical convergence analysis as well as many insights on their CUDA implementation. The performance of the proposed method is competitive with Sinkhorn. The proposed algorithm and analysis/implementation are interesting but the experiments are not convincing. The paper also has some issues with the presentation (but these can be easily fixed). I would encourage the authors to add some learning experiments to make the paper even more solid. Finally, this paper also has some presentation issues.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The central hypothesis is that human uses program induction to generate prediction of possible spatial map. Based on human behavior in two experiments, the paper further compared different models of map prediction, and demonstrated that humans actually consider the distribution of possible maps instead of only consider the most likely map. The work is novel, as program induction has not been applied to map prediction and spatial planning before. One minor concern I have is about the experimental design and analysis: when participants visit mirrored environments, they may recognize it is an mirror image of a previously learned environment, and thus the predicted distribution of map may be different from when they first see that environment. Please clarify this. If not, where does stochasticity come from in the predicted distribution of visitation by the models? If yes, will D POMCP still win when the softmax temperature or epsilon greedy are free parameters? Perhaps because of the page limit, not enough details that would be expected for a human behavioral modeling are provided, and I hope they can be added during revision.<|endoftext|>This paper investigates the concept of map induction for exploration in novel environments. The article s main contribution is a new task and a set of experiments to evaluate the central hypothesis: "that humans use program induction to infer possible maps of unseen spaces, as made up of submaps encountered in the observed areas". This is evaluated by proposing a novel map induction task that is the main contribution of the paper, as well as a set of probabilistic models that implement the different hypotheses considered in the paper. I have on the other hand some concerns about the suitability of this paper for this conference and the methodology. There is nothing wrong with that of course, but the hypothesis, its relation to prior literature and methodology ought to be reviewed by an expert in the field. The second concern is methodoligical. Moreover, the experimental results do not seem to clearly provide the evidence to support or discard the hypothesis. This paper is mainly a Psychology study (as the computational model for AI part of the claims is not really demonstrated), and ought to be reviewed by an expert in experimental psychology.<|endoftext|>This work is motivated by the hypothesis that humans maintain a hierarchical spatial representation when exploring new environments, such that shared patterns (due to the hierarchy) between spatial regions can be used to predict how still unvisited places would look. A simple discrete 2D computational model based on probabilistic program induction is proposed for predicting the map at yet unseen places. 5 from the real life study, to judge whether the visitation patterns are similar / where they differ. of small extracted regions (submaps of previous observations), based on which a distribution over possible completions of empty map regions can be formulated. ), it is shown that model based planning under the map induction model results in more efficient exploration & reward collection behavior. ## Strengths  The experiments are tailored to the main hypothesis of the paper and manage to highlight the argued points well. The results in fig. I find the claim in the abstract that the proposed approach outperforms state of the art planning algorithms to be overstated (largely because of the previous point). Since everything is discrete and because of the combinatorial nature of map induction, the cardinality of the space over possible maps will quickly explode.<|endoftext|>The main hypothesis tested in the paper is that humans think of maps of unseen places as compositions of submaps in observed areas. Additionally, they try to model the exploration behavior exhibited by humans as a hierarchical bayesian generative framework which generates a distribution over possible maps given past observations, and use this distribution to plan. In this paper, the authors present an interesting take on this problem by treating it as program induction — inferring the program that will lead to a correct map of the environment based on past observations. The experimental setup and inferences made through the experiments were easy to grasp and made sense to me. 2) Do humans seek information about the task to score different possible map completion hypotheses?) **Weaknesses:**  A major concern for this paper is that the task was designed keeping the hypothesis is mind. On a related note, the environments are composed of simple units and are almost grid world like. However, I am not convinced that the experimental setup captures general human exploration. Thus, I am not convinced that these hypotheses will hold true when tested on realistic environments.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; On two GCSR datasets, the proposed method significantly outperform strong baselines and prior works. The paper is well written, the experiments are well designed. 4.In addition to the AMR tree, did the author try other semantic parsing methods, e.g., semantic role labeling? Comparing this number with Table 2 and Table 3 may make the motivation of having a neural imagination module stronger. For instance, the verb "want" will be something like "want 01" in the tree.<|endoftext|>This SKG is then input, along with the concepts and textual context, to a "verbalization module" which outputs a sentence (or sentences). The writing of the paper is on the whole clear, and the reasoning behind the use of the intermediate representations is logical and intuitive. First, I m not seeing a description of what is meant by the "vanilla T5 Large model", though the improvement over that baseline is used as key evidence in favor of the importance of the imagination module. Does it include the textual context input as well? Understanding these details more clearly will help in assessing what exactly has been demonstrated by the improvement over that baseline.<|endoftext|>Weaknesses  The novelty of the whole method may be limited. On the one hand, previous work has used SRL as an intermediate representation, and this work simply replaces it with AMR. On the other hand, pretrained language models are directly adopted in the two stages of the proposed framework. The scene imagination module constructs a structured representation of a plausible scene and formalizes the background knowledge required for the reasoning.<|endoftext|>The method is tested on the Concept2Sentence and Concept2Story tasks and compared to multiple baselines drawn from related works. Strengths:  The core task is well defined and motivated. Clarifications/Concerns/Weaknesses:  My reading of this is that two of the main contributions are in the iterative imagine and verbalize bits. I generally hate making novelty based arguments, but in this particular case   the method is very similar to many others that have come before, both in storytelling and other forms of generation. The plan with structured representations and then verabalize paradigm is very well know (as seen simply in the author s related work).
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper proposed DaSGD, an algorithm for large scale large batch training of deep neural networks. Additionally, I do not find the numerical experiments to provide convincing evidence that the proposed method is indeed better suited for large batch distributed deep learning than existing approaches. For example, local steps with delayed averaging has already been used in asynchronous decentralized methods. Not to mention that a numerical comparison is missed with a large body of related work. a case study for decentralized parallel stochastic gradient descent, NeurIPS, 2017.<|endoftext|>In this paper, authors propose the modification of distributed SGD algorithm that combines the ideas of Local SGD and Delayed averaging of updates. This assumption on delays allows algorithm to converge with the same rate as Local SGD algorithm. Second thing to mention is a novelty and a practical interest of the algorithm. Usually the communication is much more expensive than the gradient computation so the reason on delaying the gradients is not clear for me. In work (https://arxiv.org/pdf/1806.09429.pdf) authors use delayed gradients almost the same way as in this paper; however it allows to save the total amount of communications. All in all, I find this article in a very draft stage to be published.<|endoftext|>In this paper, the authors present DaSGD, which overlaps the computation and communication of distributed training. PipeSGD [1] proposed using SGD with 1 step of staleness to overlap communication and computation. However, [1] is not cited or discussed in this paper. 2.The combination of PipeSGD and local SGD is also not new. [2,3] both uses such a combination. Basically, I think [3] proposes the combination of PipeSGD and local SGD, and [2] adds communication compression to [3].<|endoftext|>The paper proposes to combine local sgd with overlapped communication. What s more, combining them is also not novel. As a matter of fact, the proposed method Figure 2 (c) is nearly the same to Fig 2 in [2]. 2.I did not find the improvement regarding the wall clock time by hiding the communication in the experiment. I believe the proposed method is not novel and [2] has proposed a very similar method.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; My rating is borderline leaning to accept. The proposed method relies on estimation of Shapley value, the authors then propose a kernel based approximator to make it more computationally efficient. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. The explanation framework is for retrieval/metric learning/similarity models, while this paper only conducts experiments on object detection datasets (VOC, COCO).<|endoftext|>The authors should also discuss and compare to this paper. ***Post Rebuttal***For the most part I was satisfied with the rebuttal. I would encourage the authors to at the very least discuss and (hopefully) compare to the paper I cited as it a published paper that addresses the same topic as this paper and currently is (still) uncited. My initial recommendation is to reject this paper.<|endoftext|>Under this framework, the authors discovered the current algorithms are approximating second order Shapley Taylor indices, where they also proposed a fast kernel based estimation method. The theoretic contribution is the unification of existing algorithms using Shapley Taylor indices. It is because, without good mIoU, faithfulness and inefficiency is not that meaningful metrics. 3.Variance for Table 1?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; There is a growing interest in vertical federated learning where each party only store a subset of features due to its various important applications. The experimental results show that the vector quantizer produces the same accuracy as the full precision counterpart while significantly reducing the communication costs. This paper studies an interesting and important problem. This a critical prerequisite for establishing the convergence in the paper. This paper studied an important problem, but the theoretical results are not convincing.<|endoftext|>Experimental validation is provided to show compression can reduce communication by over 90% without a significant decrease in accuracy over VFL without compression. The main contribution of this paper is to offer the theoretical analysis of the effect message compression has on distributed training over vertically partitioned data, and prove convergence of non convex objectives. This result is from a recent line of research that has been studied for a while. The dataset used in the paper is also limited and small. In short, the paper is technically sound and the developments are clear. ​However, the paper is still not novel enough based on several existing works and could be strengthened by demonstrating more significant results instead of incremental, such as weaker conditions.<|endoftext|>Finally, it provides some empirical results for validation. The paper also provides theoretical analysis of the algorithm and proves the algorithm can achieve the normal SGD style convergence rate. The experiments also show that the algorithm can reduce the communication cost compared to the vanilla VFL algorithm. Weakness:There are some questions to be answered:1. This is not realistic in practice. The paper studies an interesting problem.<|endoftext|>In particular, this paper proves that the proposed algorithm converges with a similar rate as the original algorithm without compression. In vertical federated learning, each party involved does not have all the features, but only a subset of them. The compression error has to be diminishing along the training process. ## NOVELTY & SIGNIFICANCEApplying compression on activation of some layers of a neural network while keeping the convergence rate has been studied in different scenarios for a while. I appreciate the authors  work to actually show the convergence and experimental results. An incremental work, by applying compression on vertical federated learning algorithm. The theoretical analysis follows existing compression stochastic algorithms  analysis (where the compression error eventually goes to zero).
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; Negative points: 1. The motivation and novelty of the proposed method are not convincing. However, the novelty of this paper may be over claimed. My detailed comments are as follows.<|endoftext|>The inspiration is interesting. But some parts need further clarification. 2.What is the difference of the objective between the sequential model and time pattern model?<|endoftext|>The paper claims the proposed solution is based on the entorhinal hippocampal system to process spatial and temporal information. 2.It s not clear to find a clear proof or discussion regarding the difference with other models in the use of spatial and temporal information.<|endoftext|>2.The paper claims that the proposed framework could be applied to other applications such as the wildlife preservation and urban traffic scheduling problems. This paper is technically sound, and the experiments are sufficient. Thus, my rating for this paper is weak accept. The concerns are list below:1.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper studies the embedding compression problem related to GNNs and graph representation. A two stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification task with GraphSage. Unfortunately, this claim is not true.<|endoftext|>The authors achieve this by first mapping each entity into a code vector using LSH, and then learning a neural decoder on top of the code vectors to reconstruct the original embeddings/adjacency matrix. The authors then apply their approach to node classification problems, where node features are unavailable (the authors ended up artificially removing node features from existing OGB datasets). Weakness  The main technical novelty seems the direct hashing of the adjacency matrix, which is graph specific. When pre trained embeddings are given, the learning based approach (Shu & Nakayama, 2017) seems sufficient. Many node classification benchmarks for GNNs are actually associated with features. In fact, in node classification, I would say that rich features are often available (so that the prediction can be based on the given features), and people often use graph information to regularize the prediction. There, the primary approach is to train shallow embeddings, and the key bottleneck is the storage of embedding for each node (especially for an industrial scale large graph). I d be much more impressed if you could demonstrate that your approach works well under the link prediction tasks.<|endoftext|>In this work, the authors propose a hashing based node embedding compression approach, which utilizes the random projection hashing method to generate a code vector for each node using auxiliary information such as the input graph adjacency matrix. Experiments also demonstrate that the proposed method outperforms other coding schemes in both the embedding reconstruction task and node classification task. It would be great if the authors could discuss and include more graph representation learning models and more graph related tasks in the experimental section. The first concern is whether the proposed method still works on other graph representation learning models. 2.Second, the experimental section only considers the node classification task, and I wonder if the compressed embedding generated from the proposed method still achieves good performance on other tasks, such as structural role prediction and link prediction? 3.Next, the authors use the adjacency matrix of the input graph as the auxiliary information in the paper. I wonder if other matrices related to graph structure can also be used as the input auxiliary information? 4.Finally, it would be great if the authors could discuss about how to select the code cardinality $c$ and the code length $m$.<|endoftext|>The paper presents a vertex embedding method with good scalability to large graphs. The proposed method is based on locally sensitive hashing and compositional coding: a memory efficient binary representation of each vertex is constructed from a hashing technique and then, when needed, a decoder creates real vector embeddings of the vertices via compositional coding from a pool of codebooks. A negative note is that some parts are unclear to me. 1.In my opinion, the proposed "full" method (which appears to be the main method proposed by the authors, because it is employed in all the experiments) seems to be more similar to the "learn" baseline by Shu and Nakayama, than to ALONE, both in terms of trainable components and performance. Is it associated with the training of the encoder in "learn"? 3.I think it would be great to also see whether or not there is any computational overhead associated with the three methods, and that the practitioner should be aware of. The test set of 5k words is the same for the three methods? 5.When the auxiliary information matrix is constructed from the adjacency matrix, it seems to me that the test set results in a set of the most connected vertices because words are selected based on their frequency. Could you comment on this? Although the methodological contribution is little, the method appears simple and effective.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper adapts transformer to multi agent motion forecasting. Extensive experiments are conducted on various dataset with good performance. + The latent variable is used to represent the discrete future for each agents. This shows the efficiency of the proposed model. In the provided code, the decoder is autoregressive instead of what is proposed in the main paper. It should be updated accordingly, and more ablation/discussion on these two type of decoder would be very helpful.<|endoftext|>The paper tackles the multi agent trajectory prediction problem, primarily for autonomous driving, but experiments also include TrajNet and predicting Omniglot strokes. The authors claim their contributions in a very general sense:  Novel method on modeling sequences of structured continuous variables and capture multi modal distributions. The key modeling novelty is the latent variable sequential set transformer which applies self attention across different agents and across time in the scene. I mostly agree with the authors  assessment in general. I have some concerns on the claimed novelty (see below). These are very extensive. I do recognize that the method generalizes well to multiple different tasks. POST REBUTTAL: The authors pointed out that SceneTransformer should be considered concurrent work.<|endoftext|>This paper proposes a transformer based VAE model for motion prediction that can output multi model and scene consistent predictions. The change to my score is mainly because I am satisfied with most of the authors’ comments to my concerns, which include: 1) the clarification on why the proposed method is much faster; 2) the expanded ablation experiments; 4) the clarification for experiments on Omnlglot. How many time/social blocks are needed? The ablation experiment can be expanded.<|endoftext|>This submission presents a Transformer based architecture for trajectory prediction tasks involving multiple agents. Strengths: The paper proposes a solid architecture by considering the requirements of the problem. Post rebuttal edit  I thank the authors for the clarifications. I read other reviews and the responses. I am glad to see that the authors addressed the questions and provided additional experimental evidence. The ablations improve the quality of the submission. I also would like to point out alternatives in case the authors are not aware of this line of work. 1?Finally, [3] seems to be relevant and should be included in the related work discussion if the authors also agree.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper developed a principled mathematical framework for a better understanding and design of loss functions. Limitations:(1) The class margin (Eq 2.4), sample margin (Eq 3.4), generalized margin softmax loss (Eq 3.3), and zero centroid regularization (Eq 3.7) are not novel, but the theoretical explanation is nice. We usually report TAR@FAR 1e 4/1e 5. post rebuttal After reading the reply from the author, I have understood the evaluation metric used on IJB C. I suggest the author change the AUC to TAR@FAR 1e 4/1e 5 in the camera ready version. This paper can enhance our understanding of margin based face recognition. The theoretical explanation in this paper is nice and the experiments on person Re Id and face verification confirm the effectiveness of the proposed method.<|endoftext|>This paper analyzes novel loss functions to promote large margins during the training of a supervised (softmax based) classification loss, under various conditions such as balanced and imbalanced number of samples per class. They propose a number of novel losses to promote large margins, and show analytical and empirical proof of the optimality of their proposed losses. 3.Empirical results are extensive, and show the benefits of their margin promoting losses. Do their losses work "reasonably" in this case? Such a scenario may arise in modern deep learning methods when we train on datasets with many fine grained classes e.g.ImageNet 21k. 2.It s slightly awkward that different losses are used for different datasets. Why not try all 3 together? The paper as it s well written, and shows novel analytical and empirical results. I think the losses are simple and easy to implement in practice.<|endoftext|>This paper develops a principled mathematical framework for better understanding and design of margin based loss functions, where the principled optimization objective is formulated as learning towards the largest margins. Experimental results show the effectiveness of the proposed method on imbalanced classification, person re identification, and face verification. The strengths of the paper:+ The paper is well written. + The paper develops a principled mathematical framework and formulates it as the problem of learning towards the largest margins for better understanding and design of margin based loss functions. The proposed method results in both more larger class margin and more larger sample margin than the compared methods, however, the accuracy of the proposed method is slightly better than accuracies of the compared methods, and the current version does not analysis this. However, the weaknesses of the paper should be addressed to improve the paper.<|endoftext|>This paper analyzes large margin based loss functions. While inspecting the existing margin based losses, they also formulate two practical methods of sample margin regularization and largest margin softmax loss to further enhance margins of classifiers. The paper presents some practical methods. They, however, are of limited novelty and are less contributive to theoretical understanding of margin based losses. It nicely addresses my concerns about Theorem 3.2 and the practical methods. Based on the rebuttal, I upgrade my score to  weak accept  and recommend the authors to put those materials into a revised paper for clarifying the theoretical and practical contributions. Zero center regularization is somewhat interestingly formulated for imbalanced learning. Besides, the regularization would not work in the framework of cRT [R4] which re trains classifiers on  balanced  dataset; balanced mini batch sampling would naturally render zero centered classifiers even by the softmax loss. Without comparison to the well calibrated softmax loss, it is hard to recognize the effectiveness of the proposed method, especially LM softmax.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors notably propose a generic lower bound that holds for all the studied class of algorithms. The lower bound is built on the regret decomposition between the regret of the conservative baseline during the time when the budget is not reached, and the regret of a non conservative algorithm that learns on the baseline. While the obtained lower in the bandit setting is not as tight as the one obtained in (Wu et al, 2016), this lower bound holds for a larger class of algorithms. While the reviewer tends to accept the paper, the reviewers has some concerns. AFTER REBUTTALThe authors have answered my concerns.<|endoftext|>This paper investigates bandits and reinforcement learning problems under the conservative setting where it is required that with high probability, the performance of the proposed policy is comparable to that of a baseline policy $\pi_0$. * The paper is written well and the ideas are easy to understand. A similar idea is already proposed in the BudgetFirst policy in Wu et al., 2016.<|endoftext|>This paper studies online sequential decision making under a conservative constraint, i.e., the agent needs to perform at least as well as some baseline policy. The authors give too much credit to their MAB lower bound in my opinion. Notice that this lower bound is less tight than the previous one, and that it can be deduced from the previous lower bound and this paper s new upper bound. That is, if I take the construction of Wu et al.and embed it in linear bandit or MDP what would I get? Embedding MAB in MDP or linear bandit is common in sequential decision making problems and should be addressed in my opinion. 7.The authors also mention budget in the context of the algorithms by Garcelon et al.and Kazerouni et al..<|endoftext|>This paper studies bandits and RL settings subject to a conservative constraint where the agent has to perform at least as well as a given baseline policy. Numerical experiments that corroborate the theoretical guarantees of the paper are missing in the current version and I highly recommend to add some in the revision since the theoretical guarantees and algorithms seem to be incremental compared to the existing works. Could the authors provide a convincing argument about the non incremental nature of the novelty. The problem studied in this paper is interesting and I think it would interest the ICLR community. While the theoretical derivations heavily reliy on Kazerouni et al 2017 and Wu et al.2016, the theoretical results are interesting.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; This paper presents a new approach for inference in a model that simultaneously provides latent dynamics, initial conditions, and   importantly   external inputs. This approach is enabled by using the outcome of an optimization algorithm (iLQR) in the recognition model, recently enabled by other work in the field. The paper demonstrates the use of this approach on simulated data and real neural data, and compares to other classic and contemporary models of nonlinear dynamical systems. Comaprison to state of the art methods is nice and clear.<|endoftext|>The paper proposes a control based variational inference approach that learns latent neural dynamics in input driven SSM. It utilizes iLQR in the recognition model that transforms it into an optimal control problem.<|endoftext|>This paper proposes ILQR VAE, a novel method that allows to simultaneously learn latent dynamics and infer unobserved control inputs. An important component  of the method is the prior on input distribution. The main novelty of the method lies in the utilization of iLQR with implicit differentiation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; The paper proposes a message passing scheme using spherical coordinates. It is tested on three datasets of 3D moleclular graphs. The paper offer an in depth analysis of different aspects, with an extensive experimentation of the method. I think the paper can be replicable, and the claims are motivated. While message passing methods on graphs exploit only the connectivity, this work shows an interesting method to include the embedding information in the case of geometrical graphs.<|endoftext|>This paper provides a novel geometric GNN: SMP. Besides, it is a unified framework that can cover some mainstream geometric GNN methods. Strengths:(1) The SMP introduces an interesting method to alleviate the computation cost issue in SCS from O(nk^3) to O(nk^2). This method is important and can be generalized to more broad types of tasks. (3) This is an empirical work, and the experimental results support the effectiveness of SMP. Weaknesses:(1) About motivation: Why SMP is better than Cartesian coordinate system (CCS) is not well explained.<|endoftext|>This work aims at the representation learning of 3D molecular graphs and proposes a principled message passing framework to unify existing works. In experiments, the performance of the proposed model is validated on three benchmark datasets. Strengths:  The idea of incorporating torsion information when representing 3D molecules is novel and helpful. The proposed MP approach can better distinguish certain structures than some existing models. Weaknesses:  The proposed SMP scheme in Eq.(1) lacks novelty since it basically enriches the GN framework in [1] with geometry features. I feel that the architecture of the proposed SphereNet is similar to DimeNet [2] except addressing torsion information.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper leverages the recent anisotropic certificates for randomized smoothing for certifying multi output classifiers. Experimental evaluation was conducted on semantic segmentation and node classification to demonstrate the effectiveness of the proposed method. The tasks on which the experiments are conducted are in line with the motivation. However, there several concerns in this work that need to be addressed:  While the major part of this work is dedicated to the theoretical analysis, it is not clear to me which parts belong to the contributions of this work and which parts are restatements from other work. In terms of the theoretical results in this work in sections 4, 5, and 6: what is exactly new (considered as a contribution)? The writing of the paper could be significantly improved.<|endoftext|>The authors consider tasks mapping a single input to multiple outputs and study robustness certificate against input perturbations. The proposed collective certificate is evaluated on both image segmentation and node classification tasks. Strengths: The studied problem is important and the paper is easy to follow (before evaluation) in general. The proposed method is somewhat novel. The paper lacks comparison with the existing method. Figures are confused for me. I suggest the authors to evaluate on more recent models such as deeplab v3, danet, hrnet, etc.<|endoftext|>The authors  are to be commended on it. Experiments are conducted on image segmentation tasks along with node classification. If so, this should be highlighted more in the paper as I believe this is significant. 3.Can the authors comment of the compleixty for solving (5) with the proposed splits in the inputs? Showing that localized RS has a better trade off between accuracy and robustness is not sufficiently novel for several reasons.<|endoftext|>The authors use the first and second order statistics (mean and variance) to provide robustness guarantees in this method. The variance smoothing idea in the paper is quite exciting and a natural next step for getting better certificates. The current results use a model trained with $\sigma_\min$ as the base model. Some of the local smoothing ideas should be reflected in training to make the objectives better aligned. The main ideas of the paper are pretty novel and would help future research. Taking all of this into account, I would recommend the paper be accepted.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The main result of this paper is a representation theorem for functions on a bounded set U by two layer neural networks with bounded width and bounded weights, via a semi norm ||_{R,U} based on the Radon transform. WEAKNESSES: None that I am aware of.<|endoftext|>The theoretical results in this paper are about two layer ReLU neural networks. By the introduction of R, U norms the authors show that their approximation bound is tighter than bounds in the previous papers, and meaningful in more instances such as finite width neural networks.<|endoftext|>The authors extend the analysis framework of Ongie et al.(2019) and show the approximation bounds for the infinite width network on a bounded open set as well as the bounds for sparse (i.e., finite width) network that refine the similar results in the literature.<|endoftext|>The paper provides some interesting and tighter spare approximation bounds for two layer ReLU neural networks. The authors generalize the results for $\mathbb{R}^d$ to a bounded open set $\mathcal{U}\subset\mathbb{R}^d$ by defining the Radon based $\mathcal{R},\mathcal{U}$ norms of functions. The presented tighter sparse approximation bounds for two layer ReLU neural networks are of interest.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper presented the auto encoding inverse reinforcement learning where the reward function is recovered from expert demonstrations in inverse RL problem. The reward was represented as a surrogate function which used the auto encoder with the metric of reconstruction error. Pros:1.Auto encoder based reward function was proposed to denoise the expert data through the encoding decoding process. 2.The experiments on the MoJoCo showed that the proposed method outperformed state of the art imitation learning on both clean and noisy expert demonstrations. 3.Ablation studies showed that different reward functions and loss functions also received good performance. 2.This method was to minimize the Wasserstein distance between the state action distribution of the policy and the expert demonstrations. But, there was not explanation on Lipschitz constraint or weight clipping. 5.Lack of suitable experimental results to support that auto encoder based reward function was more informative and denser than the discriminator based one. A new auto encoder based reward function was presented.<|endoftext|>The authors present an architecture that utilizes autoencoding as an approach to inverse reinforcement learning. The paper is an interesting approach to IRL, using minimax games and autoencoding to enhance training. While the approach appears novel (IRL is not my main area), the experimental results, while generally positive, are a little mixed in terms of which approach is preferred (especially VAE vs non VAE). The empirical results on noise free data are impressive (as are some of the noisy results), but I would have liked to have seen more explanation on why their VAE based approach outperforms non VAE on some tasks but not others. In what way are your approach s latent representations "more indistinguishable" from those of the others? In particular, I don t see any real difference on the non noisy data. Can you quantify this somehow? I read the authors  response, but it didn t compel me to change my score.<|endoftext|>This paper proposes to use auto encoder for inverse reinforcement learning. The main goal of using the auto encoder, is to use it as the reward function, which takes the auto encoder reconstruction error to provide reward signals for the agent. The authors claim that such approach provides more informative signal than existing works, especially adversarial imitation learning approaches. Their experiments demonstrate that this method has a better performance over other baselines, and more robust to demonstration noise. Pros:* The use of auto encoder and reconstruction error for formulating reward functions is novel. * The ablation study on distance function for inverse RL loss function is also impressive, which shows the importance of using auto encoder. * It s well known that the performance of learning a reward model (and also a police) is affected by how good the expert agent is, so I wonder if you have tried some more recent and better agents in the literature. I generally recommend the authors to provide more details when doing the t SNE plots. * *Question*: is it possible to provide theoretical analysis on why the proposed method can minimize the distance between the state action pair distributions of expert and agent samples, or other related similarity / distance metrics? The authors provide a good insight on how the adversarial imitation learning algorithms fail. The sensitivity to minor differences between expert and agent samples does impede the learning of agent because it may be pushed to learn how to imitate the details instead of other more important features, and thus less robust to the noise of data.<|endoftext|>(2) the auto encoder also provides a more informative reward signal whereas the binary classification objective in works like GAIL, can easily result in overfitting. * The method seems easy to implement only requiring changing the discriminator to an auto encoder. Paper Weaknesses: * Why does AEIRL perform relatively better on non noisy expert demonstrations compared to noisy expert demonstrations? The motivation of the method is that it can work well from noisy inputs, yet the method is outperformed in several of the noisy settings and has a large performance drop. This is the opposite of what I would expect. * While the reward is intuitive, can it recover a true reward function, or is the reward coupled with the policy like with GAIL? AEIRL must learn a decoder to reconstruct observations. The reconstruction error could be less reliable as a reward signal in these higher dimension tasks since learning the decoder is harder. The method is intuitive and performs strongly across in an extensive imitation learning evaluation.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper presents a study on different last output layers, under the continual learning problem. The paper proposed several design choices of the last layer, including a modified weight normalization layer, masking operations, and median vector. Different scenarios of incremental/lifelong and mixed learning have been experimented with. However, under the paper s assumption, it requires a fixed feature extractor. Since the study is based on a non projection drift setting, it requires pre trained models as the feature extraction as stated in the experiment part, which is not often practical in many settings. For Figure 2 (interference), what is the root cause for the interference? The paper provides some summarization of multiple tracks to deal with the weights normalization and preserving in the last layer but the insights brought from the work are limited. The analysis is strait forward on the observation of weights but there is not much deeper and richer analysis on the causes of such observation. The experiment results do not support most hypotheses made, but the cause of the failures remains unclear.<|endoftext|>This paper studies the output layer in a deep neural network in the continual learning setting. The paper studies multiple types of output layers under different continual learning configurations. Some suggestions are made based on observed empirical trends. The paper isolates the output layer for study yet in continual learning all layers are updated. No concrete motivation is provided as to why the output layer should be studied in isolation. Moreover, some of the observations highlighted in the paper may not hold if the neural network is allowed to be updated and the neural network is studied holistically. Furthermore, most observation (e.g., _masking is efficient in incremental settings to avoid weight modifications_) are well known from prior work. Most observations are not contrasted with prior work.<|endoftext|>the paper analysed behaviours of the last linear layer of pre trained convolutional neural networks in the settings of continual learning where either new instances are provided in new episodes or new classes are given in new episodes. 1.I am not clear on why using neural networks became essential in this study. Given that the feature extractor part of a given neural network is frozen, and the paper doesn t provide any insight on feature learning during continual learning, I d recommend the authors to start from simple linear models, including logistic regression, and SVMs. In that case, the impact of the neural network architectures can be ruled out, and the conclusion could be clearer. 2.As mentioned later in the paper, arguments made regarding the lengths and the biases of vectors in the weight matrix in the last layer don t seem consistent across various settings and datasets in continual learning. The secondary issue is that the feature vectors are all in very high dimensional space where, with high probability, vectors are equal distant from each other, which causes trouble and issues when analysing their relations.<|endoftext|>The authors analyzed the effect of changes to the last outputlayers of the neural network in the continual, incremental learning scenarioand effects of it two scenarios 1. weight modification 2.Inference. Authors also claim to propose simplified weight normalization layer, two masking strategies, and an alternative to NearestMean Classifier using median vectors to address catastrophic forgetting in the output layer in both continual, Incremental scenarios. [Much of the analysis is concentrated on Part B]Clarity:Overall writing quality of the paper can be improved a lot. Novelty:The methodological contributions do not seem to be novel and all the results are as expected w.r.t  the currentline of research of doing continual learning keeping the feature extractor part of the network fixed. Pros:Very well detailed experimental setup covering all the scenarios. Authors evaluated the proposed ideas on Incremental Scenarios(CIFAR10,Core50,CUB200), Lifelong Learning Scenarios(Core10LifeLong, CIFAR100LifeLong) and Mixed Scenario(Cire10Mix)Cons:As mentioned the Novelty is pretty much limited and results as as expected w.r.t current line of research.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper is the first try to train agent behave altruistically towards others without knowledge of their objective or any external supervision. The main idea is to train altruist agent giving the leader agents more choice and thereby allowing them to better achieve their goals. They  introduce three multi agent environments of increasing complexity to evaluate the proposed method. The results show that it can, in some cases, outperform the supervised baselines. 2.This paper, as far as I know, is the first to try to address this important problem. 3 This paper can serve as a baseline and also propose three testbeds for future research. A novelty work, worth accept.<|endoftext|>The paper posits a method for agents that learns altruistic behaviour from observations and interactions with other agents without knowledge of their objective fucntion by estimating the sub goals of the agent and actively maximizing their choice. For the model free estimation of choice from observations, it is unclear how the altruistic agent estimates the leader agent’s policy entropy from observations. Additionally, what was the loss function used. The authors are trying too many different(existing) methods to prove the central concept around how one agent can be altruistic to another by maximizing their reward. Although, the experiments are extensive.<|endoftext|>This paper provides an alternative to empowerment for goal agnostic assistance RL policy training   choice based optimization. It is an interesting and novel way to train goal agnostic helping policy by maximizing the choice of other agents. 3.The experiments and discussions are thorough and informative. The results are promising. If I understand this correctly, this means that empowerment only depends on the environment and is agent agnostic, while choice is agent dependent   the definition of choice depends on a specific agent s policy. If this understanding is correct, then I have two questions:Q1: Can you train a policy using a MADDPG like method that jointly learns the policy of other agents and the policy of its own? Crucially, this is closer to your approach than the vanilla single agent DRL is, since it also explicitly depends on the policies of others. I enjoyed reading this paper and I think the core idea is a valuable contribution to the multi agent RL research. My main concern is about its comparison with empowerment and other related multi agent RL work, which is detailed in my main review. I would appreciate some clarification from the authors and would be willing to increase my rating if my concern is addressed in the rebuttal and the revision.<|endoftext|>This paper proposes an unsupervised learning method for training an agent to assist another agent (called the leader) in solve its task (thus displaying a type of altruistic behavior) without access to the other agent s reward function or policy. I think the paper tackles an important and rather neglected problem, proposes a novel and quite general method for this setting, and demonstrate significant empirical gains on multiple tasks. 3) The method is new as far as I know, although some of its elements have been used in other contexts, but I think the authors do a good job at explaining the connections to related work. 4) I also found the analysis of the different choice formulations from Figure 1 to be quite insightful. Would it be possible to bias the set of states enabled by the altruistic agent towards the set of states which are desirable for the leader to reach?<|endoftext|>This paper introduces a method for developing altruistic agents in a multi agent RL (MARL) setting. The paper argues that this is a suitable proxy for the unknown true reward function in many environments, because optimal policies tend to choose actions which lead to greater choice (larger coverage of state visitation) in the future, using analysis of instrumental convergence. Strong points of the paper:The main idea is quite conceptually simple and an interesting approach to develop altruism for MARL. The analysis of the failure case is also quite insightful. So, these are environments where it’s unsurprising that the choice heuristic would work well. Because the contribution of this paper is quite dependent on the empirical performance of the method, I think that it needs to be evaluated on additional settings where it is less clear that “choice” is directly correlated with the reward, to be convincing as a generally useful metric when the true environment reward is unavailable. However, to me this seems to contradict the point that IC is a good proxy for the leader agent receiving high rewards, because the altruistic agent needs to help the leader to harvest the apple regardless, and the altruistic agent is closer to the apple at the top? The idea of this paper is conceptually simple and I quite like its application to this challenging setting of altruism without true rewards, as well as the analysis that the paper presents.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 5; rating score: 10; The goal of this paper is formalize and analyze the technique of domain randomization in robotics. The authors find algorithms that achieve a performance gap that beats the worst case performance gap of $O(H)$ in all three settings ($O(log^3H)$ and $O(\sqrt{H})$). I found this paper very well written and easy to follow. Each assumption that the authors make   for instance that the MDPs are communicating or that the value functions are smooth   is justified by showing that there exist MDPs where the best case transfer performance is $O(H)$ in which case not much can be said. I think it might be worth mentioning in the body of the paper that the way in which Algorithms 1/3/4 use memory is somewhat different from parameterizing a network by an RNN. In the conclusion, the authors say they hope their analysis will lead to the design of more efficient algorithms for sim to real transfer, but I think the paper could benefit from more discussion of which of their results the believe could be the most applicable to improving how domain randomization is used in practical settings. This paper formalizes the common practice of "domain randomization" and develops algorithms and bounds for the performance of domain randomization depending on if the set of randomized MDPs is finite (with or without separability) or infinite.<|endoftext|>The paper is a theoretical analysis of domain randomization in the context of latent MDPs. Bounds are provided with assumptions and most of these assumptions are justified by proving that removing these assumptions opens the possibility to counterexamples which do not meet the provided bounds. The paper is well organized and easy to read. Most of the mathematical concepts do come with an intuitive description of their meaning and this allows the reader to get a grasp on the importance of the theoretical results. The submitted paper not only provides this bounds but also express them in terms of quantities (e.g.the value function gap) which are relevant for downstream real world applications. The main limitation of the paper is in assuming no fine tuning with real world samples. Somehow this ability shares some commonalities with fine tuning and therefore it seems possible to extend the author s approach to the more relevant scenario of domain randomization with fine tuning. The submission is limited to 9 pages which the authors comply to. However, the proofs of all theorems are in the supplementary material which, as a reviewer, shouldn t be necessary to get most of the paper. This is in this case not really the case because I believe proofs are a necessary component to understand the quality and relevance of the paper. The paper doesn t have limiting assumptions besides the absence of analyzing how results extend to the fine tuning case.<|endoftext|>This paper presents a theoretical framework for reasoning and analyzing domain randomization techniques. In this designed setting, the work proves sharp bounds on the gap between an optimal policy s value in the domain randomized and the real world setting. The work also analyzes the conditions under which sim to real transfer can be successful in the considered theoretical setting. Overall I believe that the main strength of this work is providing a theoretical analysis of sim to real deployment via domain randomization. Theoretically analyzing and understanding this setting has the potential to provide safety guarantees and inform the design of new algorithms. While the analysis provided by this work is interesting, in the current stage I have three major concerns. However, the assumptions seem to be very restrictive:* While I believe that assumption 1 is common, it would be interesting to see for which simulator real world simulator it holds. * There is a similar issue with assumption 2. However, the authors should spell out which simulator & environment combinations currently satisfy such assumptions. And if there are none (which may be okay for an otherwise interesting theoretical work), there should be at least a discussion on what are the main limiting factors of the assumption compared to practical settings. Theoretical papers do not necessarily always need to have experiments (some theoretical contributions cannot be evaluated or may not require experimental evaluation). However, in the present case, the work aspires to address a practically relevant setting and seeing the performance evaluated seems quite relevant given that Domain Randomization already has an extensive empirical body of work. In summary, I believe that this work addresses an interesting problem. Its main shortcomings are the lack of discussion linking the particular contribution and assumptions to particular use cases as well as the absence of any experimental demonstrations.<|endoftext|>This paper presents a theoretical framework for analyzing the sim to real gap in the context of domain randomization. The paper defines formally the sim to real gap and the domain randomization method. The paper provides constructive arguments to prove the upper bounds. (50) the provided definitions of $\alpha$, $\delta_0$, and $n_0$. Still, I have a few comments, and it would be good if the authors could clarify them:1. While I agree Eqn. Could the authors comment on these two definitions? I think defining the problem formally and properly is a good contribution. 3.Comparing Sec.5.1 and Sec.5.2 seems to imply that narrowing the sim to real gap is more difficult for problems without separation condition, but I have a feeling that this is not always the case. Overall, I recommend strong acceptance assuming that all the equations in the main paper and in the appendix are correct or are relatively easy to fix.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 10; The algorithm is proved to improve upon random permutation when all the component functions are quadratic. Theoretical contributions include exponential convergence rate for 1 d smooth functions, lower bound for general strongly convex functions, and lower bound 1 d strongly convex function with nonconvex component. 2.The paper proposes an algorithm FlipFlop, which is easy to implement and proved to have better performance than random permutations for quadratic functions. Empirical results also show FlipFlop performs better for logistic like functions. 3.The paper is well organized and clearly organized. Convergence rates of FlipFlop are only proved for quadratic functions. A different but related problem is the permutation in coordinate wise SGD. A discussion on whether the proposed algorithm FlipFlop can be applied to coordinate descent would be interesting. Although the settings studied in the paper is limited, it provides a better understanding of permutation based SGD and can motivate further studies.<|endoftext|>They found that for general strongly convex functions with Lipschitz Hessian, random permutations are optimal in high dimension but not optimal in 1 dimension. For general convex quadratics, random permutations are also not optimal. Finally, the authors introduced a new technique termed FlipFlop that works by reversing the permutation of the previous epoch at every even epoch, and at the odd epochs the algorithm just follows its original permutation, whether it be random or cyclic. FlipFlop has been proven to improve the convergence of random reshuffling, single shuffle, and incremental gradient descent on quadratic functions. The accelerated rate obtained are also quite impressive considering how simple the method is. It d be nice if the authors have further insights for non convex functions as well. page 13: "Similarly, we can find show that"  > "Similarly, we can show that"Although the paper considers the optimality of permutation based SGD only for strongly convex functions, the theoretical results are interesting enough and the proposed trick FlipFlop seems to be a very promising approach to further accelerate random reshuffling.<|endoftext|>This paper is motivated by the observed phenomenon that, in stochastic gradient descent (SGD), without replacement sampling (random permutation) gives faster convergence than with replacement sampling. Focusing on optimizing convex functions at a constant step size, the paper shows that:1. there exist optimal permutations which converges exponentially faster than random permutations for 1 dimensional functions. 3. by reversing the permutation every other epoch (flipflopping), convergence on quadratic functions improves for three permutation based methods: Incremental Gradient Descent (deterministic), Random Reshuffle (random), and Single Shuffle (hybrid). The paper studies SGD, the workhorse of optimization in machine learning. 4.The analysis does not appear to be complicated. 5.The paper is well written and easy to read and understand. Regarding weaknesses, as admitted in the paper, the results are:1. Not applicable to all convex function (for flipflopping), or non convex functions.<|endoftext|>Theorem 3 refines these results to show that the improvement in Theorem 1 is specific to quadratics. I think that an intensive evaluation of FlipFlop is beyond the scope of this submission. However, they also prove that this phenomenon is specific to one dimensional functions using a new, dimension dependent lower bound for any permutation based SGD method. ## WritingThe submission is well written with a carefully thought out story. The lower bound showing that random permutations are optimal in the general setting answers an open question about permutation based variants of SGD, while the analysis of FlipFlop opens to the door to clever permutation schedules which improve convergence in specific settings. In addition to the novelty of the theoretical results, the paper is polished and the writing is well executed. **Theorem 2**: the lower bound appears to be invariant to initialization, but only as long as the initialization is deterministic so that it can be observed by a resisting oracle.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper built upon Pearce et al 2020 that shows regularizing parameters to prior distributions in ensemble of NNs results in the approximate Bayesian inference, aiming to improve computational efficiency of the previous method. Specifically, in the method in Pearce et al, sampling one parameter from posterior requires to find a MAP solution for some anchor point, which can be computationally demanding. Therefore, the authors propose to learn MAP function from an anchor point to MAP solution, which removes the required MAP optimization procedure. Theoretical results and mathematical states in the paper seem not rigorous. It would be better to write and develop their theoretical results in terms of convergence behavior. The paper is not self contained.<|endoftext|>GPN is a single network that approximately produces samples from posterior over neural network parameters; this is in contrast to the standard approach of training N individual neural networks. Moreover, as opposed to trying to get the posterior over parameter space the authors compute the posterior over function space; this drastically reduces the number of dimensions the GPN needs to predict, making the problem much easier to learn. # WeaknessesWhile I am a fan of the approach conceptually, I think there are a number of weaknesses. Next, I am confused by section 4.1 as a whole. This is also true for theorem 1 as well. While I like the idea, I think more work needs to be done on the structure of the paper and the experiments.<|endoftext|>The paper develops an uncertainty quantification method as an extension of the Bayesian ensemble method. Instead of training multiple ensemble models through MAP optimization, the proposed method tries to learn a mapping function between the prior distribution and the posterior distribution of model parameters, which avoids the complex training of ensemble models and achieves better efficiency. Strengths: (1)	The proposed method proposes a novel idea to build a mapping function from prior to posterior. Weakness:For the methods: the theory part is not well written. However, the authors did not explain their ideas well and there might be some mistakes in their formulation.<|endoftext|>NN ensembles are useful models to estimate epistemic uncertainty in the predictions. Since their computational cost is generally high, they introduce Generative Posterior Networks (GPNs) as a generative NN model to approximate the posterior distribution of the inference process. The contribution is novel and significative, although there are a couple of things that should be addressed in the revised version of the paper. This paper introduces a new proposal to make use of ensemble NN methods, GPNs. There seems to be a couple of tests that need to be done, but the results point in the right direction. * The results obtained support the proposed method in a solid manner when compared to previous methods. This could be fine in cases where data does not exhibit complex behaviour, but could be a problem for cases with multimodality, heteroscedasticity, etc. I would suggest rewriting this part of the introduction, pointing out more clearly why NN ensembles may be more interesting in some cases. * Have the authors conducted any tests in regression datasets?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 5; The paper proposes to decouple a model’s dual duties with two prediction tasks. ### Pros:  The paper is well written, easy to understand and follow along. The paper is well motivated and driven by a real problem in FL. ### Cons:Following are the concerns that can be addressed for change in the final scores. 5.In P FL, the local test sets used in the experiment are static because of the standard benchmarks used. This issue is not addressed at all. Any of the findings in this direction are also useful since you already proposed this and have the setup.<|endoftext|>This paper focuses on a challenging problem in federated learning setting where data across different local workers are imbalanced, and most of the current work in the community are optimizing either generic performance or personalized performance. This paper proposes a new framework, Fed RoD, which combines both global objective and local objective, to achieve that the trained model is suitable for both generic and personalized task. The main contributions are:1. Focuses on an interesting problem which aims to increase both local and global performance in imbalanced federated learning setting;2. The experiments are well designed and comprehensive;4.<|endoftext|>The paper proposes Federated Robust Decoupling (FED ROD) to excel on both generic FL and personalized FL. With extensive results, it illustrates that strong personalized models emerge from the local training step of generic FL algorithms, and show that class balanced objectives are effective for improving the generic FL performance when clients have different class distributions. For example in "Ablation Analysis",  2. conclusion is weak and would requires some work to summarize the contribution. The paper overall is a bit hard to follow.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The proposed method is a combination of two previously established methods, Dreamer and SwAV. Removing reconstruction is generally well motivated, however, the paper fails to support some of its key claims. In the results, however, the paper entirely ignores one and focuses on the second i.e.there is no talk or analysis on the computational aspect of the proposed method and it seems the authors  main focus was to get better results on environment with random video backgrounds. 2.Weak baselines, particularly for contrastive methods. 3.Lack of analysis. Intuitively, it can be assumed that it is mainly because the model learnt a more robust representation, however, there is no explicit theory or experiment to support this claim. Other issues 1. So the number of prototypes grow by the time and the model is approximating millions of prototypes at the end?<|endoftext|>Strengths  + The paper is well written and motivated, and the presentation is easy to follow. Weaknesses    I would like to see a more thorough experimental analysis. I would appreciate it if the authors can further clarify this in the response. While the paper proposes a valid combination of existing techniques (SwAV + Dreamer) with reasonable performance on the selected task of "RL with random visual distractions", to me this paper is not significant or novel enough to make a strong case for acceptance. However, it seems that the paper lacks novelty at the moment, and I would like to see a more thorough experimental section with more visualizations. Based on these, I do not recommend acceptance at this moment.<|endoftext|>This work proposes a reconstruction free method for pixel based MBRL, utilizing cluster assignment adapted from SWAV with an additional temporal term. The paper is well written and easy to follow. The major concern is the complexity of the environments. Further evaluation on Atari will make the results much more convincing. 2.The paper is not particularly strong in terms of novelty as swapping the reconstruction objective with prototypical objective is already shown to be effective in model free RL (Yarats et al.2021).This won t be a big problem if the paper can showcase effectiveness in more complex environments. I would be happy to increase my score if the above concern is addressed.<|endoftext|>Although the results show empirical improvements, the paper could do more to contribute to a more general understanding of these improvements and the approach itself. ###  Strengths  The approach is clear and well motivated, and a natural extension of representation learning for MBRL  Related work section is thorough and well written  Experimental results show an increased robustness to visual distractions compared to dreamer###  Areas to improveThe experiments are not as thorough as they could be. Several baselines are cited but not included in experiments (ie PSE, CURL, many of the model free temporal prediction models). I will maintain my score that this paper is above the acceptance threshold.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; There are several weak points of this paper. We have to admit that the convergence result in Allen Zhu et al.2019 has strict conditions: NTK regime, ReLU and square loss, which may not hold in the practical experiments. rather than different losses. The regularization penalty is quite similar to the function of the weight decay. Most importantly, CIFAR10 may not be a good dataset for benchmarking DP algorithms. Good intuitions about DP algorithm but the results and arguments are not convincing.<|endoftext|>This paper attempts to improve the accuracy of the DP SGD training from the perspective of loss function design. The authors propose a loss composed of SSE loss, focal loss, and L2 regularization penalty. This may be also valuable to the community. Weakness: The main disadvantage of this article is that it does not explain clearly the motivation for designing the loss and why the proposed loss can work. Another question: Are the parameters robust to different hyper parameters settings in the loss?<|endoftext|>However, I believe this paper should be viewed as an empirical paper and I am satisfied with the authors  efforts in exploring the impact of loss function on the performance of DP SGD. Overall I think this is an interesting paper that explores the impact of loss function on the performance of models trained by DP SGD. The loss function proposed by the paper consists of three components, all from existing literature. As pointed out by other reviewers, some intuitive arguments for motivating the losses are not well supported and misleading (e.g.using Allen Zhu et al.2019 to argue faster convergence in practice mentioned by Reviewer VgSd).<|endoftext|>I think this paper is clear and easy to follow. This empirical paper has analyzed a set of computer vision datasets and the improvement does exists. I appreciate the authors efforts to do ablation study and to give details of network architectures etc. in the appendix. First of all, the experiments are not comprehensive: only computer vision tasks are included and only on toy datasets. At most we can say on specific datasets and specific models, using the new loss is beneficial. The paper is clear and easy to follow, combining three existing losses to form a new one that empirically improves DP accuracy on some vision datasets.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; They apply their template to several existing deep learning models and perform experiments that show they can achieve comparable if not better results with less training time. The decrease in epochs needed for LSTMs is particularly impressive. Perhaps, some type of curriculum learning or more exploration on what the various shortcuts are doing could make this argument stronger. I think in some ways this work can be seen as encoder decoder with additional regularization, too? I would recommend this paper to be accepted. I think is a lot more to explore with the dynamic layer accumulation and gradient isolation, too, that would be interesting to other researchers.<|endoftext|>It attempts to propoose new learning approach associated learning as an alternative way to back propagation. The authors have resolved my concerns on the technical details of how AL is applied on RNNs and Transformers. The paper is well written. Experiments show that in text classification and image classification, the proposed method outperform BP in some basic architecture setting. Here are my concerns:  It is unclear how the AL is applied on RNNs and Transformers. In CNN, when flatting the hidden representation, it also lost the spatial information in feature maps. What is the impact of the increased parameters in computation cost? The experiments uses relatively simple network architecture for text classification. Right?In summary, I think though the paper proposes AL framework as an alternative to BP, it is actually a simple extension to a previous work, and does not proposes substantially new ideas.<|endoftext|>Different from back propagation (BP), AL decomposes BP’s global end to end training strategy into several small local optimization targets such that each sub networks has an isolated gradient flow. The idea is interesting. The experiments demonstrate the effectiveness on (IMDB Review, AG’s News corpus, DBpedia Ontology, the Stanford Sentiment Treebank, CIFAR10, and Fashion MNIST.First, as in Figure 2, the paper proposes to map input x and output y into a latent space for metric learning ($f(x) g(y)$) and auto encoder learning ($y h(g(y))$) are also investigated in multi label classification[r1,r2], which are not discussed in this paper. In my opinion, the main difference is the design of multiple latent spaces compared with these multi label classification methods. It is unclear why maps the target $y$ to the intermediate layers in this paper. It is suggested to analyze the differences. (1) The proposed method can be optimized locally, and achieve competitive results.<|endoftext|>This paper studies and benchmarks an alternative to back prop named associated learning. They analyze the pros and cons. The results and the methodology are definitely compelling. But it is not clear how you landed on this method specifically, as compared to many other attempts on finding more optimal neural network optimization methodologies. · Section 2 is very difficult to follow. There is a clear pattern that AL is faster, but then things changes radically for AG News? Would be nice with some further analysis into this.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper introduces VPR, an unsupervised hierarchical latent variable model. I lean towards accepting this paper, and would be willing to further increase my score if some of the concerns above (especially regarding the experiments) are addressed in revision. Do similar results to Fig.4 emerge from the other datasets as well? Though the paper does say in words what D_{st} and D_{ch} are (which is much appreciated), the fact that their comparison is a reasonable way to define a predictable event is not clear to me. That gives a bit more structure to (and motivation for) the comparisons made later on.<|endoftext|>This paper introduces Variational Predictive Routing (VPR), a spatiotemporal deep generative model for hierarchical video representation learning. I look forward to hearing from the authors during the rebuttal period on this matter. This paper has the potential to be of significant interest to that community as well. Figure 8 in the appendix was much more clear. It would be good to stick with one throughout the paper, if possible. A discussion on how $\gamma$ and $\tau_w$ are selected, and how sensitive the model is to these variables, is missing.<|endoftext|>Variational Predictive Routing (VPR) models continuous data as a hierarchical renewal process. Compared to a baseline method (VTA), VPR detects changes in synthetic datasets with high accuracy. I miss some more details on the following things:* What is the generative model for the change detection? I assume this becomes some sort of meta learning? * I think you should cite "Neural Sequnce Chunker" by J. Schmidhuber and Unsupervised Real Time Control through Variational Empowerment  by M. Karl et al.I believe the paper has a very nice core idea, and a well thought out  harness  around that idea.<|endoftext|>Although the authors claim in the appendix and the hyperparameters in Criterion U are robust across datasets, it is still not clear to me how to tune these hyperparameters. The model is compared with VTA only on the Moving Ball dataset, which in my view, is not enough to validate the superiority of the proposed model. However, my major concern is about the insufficient experimental results (see my comments above), and so I give this paper a Weak Reject at this moment.
Reject; rating score: 3; rating score: 5; rating score: 6; The main components of the approach are: 1) Disjoint MoE’s as ensemble members. A number of experiments compare predictive performance vs computational cost for the proposed approach (pBE), vision Sparse MoE’s (V MoE) and Vision Transformers (ViT). Weaknesses:The proposed approach is incremental. In the words of the authors, the method “introduces two changes to V MoEs: (a) the partitioning of the experts and (b) the tiling of the representations.”The paper focuses on the comparison of specific architectures and training procedures. While this paper’s results suggest perhaps not, I don’t think this is conclusive. While the paper presents experimental results likely not found elsewhere, in my perception, that appears to be the sole reason for the paper.<|endoftext|>This paper considers the combination of MoE and Ensembles for image recognition tasks to obtain both improvement of classification accuracy as well as stability of prediction. The proposed method is evaluated with extensive expereiments from large scale image recognition to OOD and the results indicate better performance for the proposed method. The paper is well written and the experiments are comprehensive. 1.The paper lacks novelty. It just combines the MoE with Ensemble in a trivial manner. The tiling technique is quite simple and it is just the classical way of doing ensemble across models. 2.The improvement is quite limited. I recommend rejecting it this time.<|endoftext|>Authors build on the Vision Transformer (ViT) for their experiments. To efficiently combine Sparse MOEs and Ensembles, the paper presents Partitioned Batch Ensembles (PBE), where the parameters of the self attention layers are shared, and an ensemble of Sparse MOEs are used for the MLP layers of the Transformer blocks. Other than that, the paper is novel and the experiments demonstrate the usefulness of the proposed Partitioned Batch Ensembles (PBE).
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The paper claims to present a new approach for multi task reasoning where reasoning operators are defined as neural operators and a planner is used to chain them based on tasks. The problems with the paper are that:a)  it pays cursory attention to important details about planning, the main idea, of the approach. The paper is only referring to the latter but makes no mention of it. b) glosses that planning is not just chaining. The authors do not talk about diversity in type of actions and their interaction. If there is no interaction among actions, any chain will give us the solution. d) does not discuss the implication of results. The paper is motivated on an important problem   MTR. Beyond improving explaining, I will suggest the authors to attempt more mainstream reasoning tasks and not on toy settings.<|endoftext|>Notably, this paper seeks to address a problem that plagues multi task learning more generally — the tradeoff between cross task generalizability, and single task effectiveness. In general, PRIMA seems like a strong approach for multi task logical reasoning. While the paper motivates PRIMA using the general framing of the generalization/single task performance trade off in multi task learning more generally, I find this a bit tenuous (not a reason to reject, but I recommend the authors maybe temper their claims a bit). Weaknesses:That being said, I have identified a few weaknesses, and I am hoping the authors can either point me to relevant information (I totally could have misunderstood certain results), or engage with me to discuss the importance of these few points:As it stands, we evaluate PRIMA in both multi task and single task settings, on a set of logical reasoning tasks on family trees/graphs. I am raising my score up to a 6. In general, PRIMA is a neat approach for multi task logical reasoning. However, while the method and optimization are sound, I’m slightly concerned with the results.<|endoftext|>I think the paper looks at an often overlooked problem and introduces an interesting mechanism to drive the reasoning process. Though I do have concerns about some experimental results that are missing and could provide more information about trade offs introduced by including a planner reasoner in the architecture. I think the use of RL methods to direct reasoning is an interesting solution. In terms of concerns regarding the work, I mostly feel the need to improve reasoning efficiency was not as well motivated and there is missing information on additional training overhead introduced by including an RL component to the overall learning problem. Reasoning Cost for Single Task Problem: The paper motivates the problem of improving reasoning efficiency by appealing to MTR problems. Unfortunately, this doesn’t present the complete picture.<|endoftext|>The paper presents PRIMA, a framework designed to solve the problem of multi task reasoning. The paper presents several tests that achieve performance usually stable on 100% succesful seeds. The experimental section is also divided equally between the two systems, making the purpose of the article unclear. The differences between the two systems are not many, since PRIMA is based on PRISA, but in my opinion, the organisation of the content and the title leave the reader a bit lost. For example, on page 1 "to solving multiple tasks". A pagina 6, "to integrating". On page 8 "we examine its the reasoning paths". The system is able to achieve really good results in the tests conducted. On the other hand, the article is not structured very well and not entirely consistent with the title.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The core idea is to decouple the feature extractor and the final prediction. This paper proposes decoupling based backdoor defense (DBD) on poisoning based backdoor attacks. Specifically, the authors first give a broad view of why backdoor works by visualizing the poisoned and benign data in the embedding space. The authors are welcomed to discuss this. Suppose the authors train the original model in a supervised learning manner.<|endoftext|>The paper uses self supervised learning to get benign representation and then uses a noisy label algorithm (SCE) to optimize the prediction model. The empirical performance shows the effectiveness of the proposed method. I understand that this paper mainly focused on empirical performance, but * it is quite surprising that the proposed methods perform well on label consistent attacks. This is because that proposed method decouples the label corruption and feature corruption. * The two step method makes the algorithm not in an end to end fashion.<|endoftext|>Summary: authors propose a modification to the training procedure to prevent backdoor attacks. Later they propose to remove low credible samples and fine tune the whole model on the remaining samples with labels. They claim that this procedure eliminates the backdoored inputs that have incorrect labels. The proposed method modifies an underlying training procedure multiplying training time, which I think is significant for practitioners. 2.The primary assumption of the paper is that learning in semi supervised learning is safe.<|endoftext|>With this observation, the authors propose a novel defense method based on contrastive learning and decouple end to end training to defend against backdoor attacks. Most of my concerns have been addressed. What is the necessary conditions that makes a feature extractor scatter poisoned data points in the feature space?
Reject; rating score: 5; rating score: 6; rating score: 6; The framework jointly leverages the original noisy labels and estimated pseudo labels of all samples for model training. When using pseudo labels, strong augmentations are also applied to the samples. This paper proposes an easy to implement method PARS for LNL task and it obtains certain improvements under the traditional LNL setting especially with a large noise ratio and a low resource semi supervised LNL setting. This paper mainly makes use of two existing techniques, i.e., negative learning and FixMatch (a semi supervised learning method) to build a better framework. I think some in depth analyses lack here. 2.In my opinion, when the noise ratio becomes large, from the perspective of PARS, the LNL task becomes more similar to a semi supervised learning task, thus PARS (part of which inherits from FixMatch) would achieve a high performance as shown in Table 1. However, when the noise ratio is small, i.e., 20% or 50%, PARS doesn’t perform the best on CIFAR 100 dataset. Are there any empirical results that would support this claim? However, some in depth analyses about the effectiveness of the pseudo labeling in PARS and the inferior performances under some settings still lack in this paper.<|endoftext|>The paper combines three branches of approaches (1. sample selection, 2. noise robust loss, 3. label correction) to address label noise in classification in a single framework. Specifically, the method includes (1) warm up phase, (2) a novel label free sample selection, (3) noise aware loss (as a standard technique) and (4) self training with pseudo labels along with the given labels. The proposed method outperforms prior arts in CIFAR 10/100, especially in high noise regimes (80 90%). In addition, it is not clear how the pseudo label generator is used to label the noisy labeled data. W2: Empirical gain in small noise regime (SYM 20% 50% in CIFAR 10/100) are small. W3: No empirical gain against SOTA in large dataset (Clothing1M)  W4: There are large gains in large noise regime (more than 80%). But this set up may not be very realistic. The proposed method is a novel combination of existing successful components for LNL. Although the proposed method is somewhat novel, the empirical gain does not assert the benefit of the proposed approach.<|endoftext|>This paper proposes a unified approach to handle noisy labels for training neural networks and utilize all the training data to learn effectively when noise is present. The results on semi supervised setting are also appreciated and can be useful for future research in this direction. The authors show how their approach improves over CIFAR 10 and CIFAR 100 with high noise ratio and demonstrate competitive results in Clothing1M dataset. They discuss about the shortcomings of sample selection algorithms, loss functions designed for noisy distribution and label correction/pseudo labelling algorithms. Compared to FixMatch which is dedicated to learn using semi supervised approach on low resource, the proposed approach is able to handle unlabelled data better and reduce impact from noise. Continuing on previous point, a large gain is seen with the use of pseudo labels in training which suggests the model’s prediction after warm up is better than the raw labels. The small performance gap by removing negative learning in both pseudo and raw loss shows its more dependent on the positive learning using perceived labels. This finding is also interesting as the pseudo label would also be assumed to have certain ambiguity to it that could benefit from the noise robust loss. Since the approach shows more impact in semi supervised setting, perhaps including more study on that similar to FixMatch would benefit the paper and the readers more.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper proposes a novel algorithm DIR that allows the learned GNN model to make predictions based on causal patterns. 4.The observations about how the variance changes during the training course are interesting and provide help for understanding other invariant learning methods. It has not been explained in the paper. It would be better to provide more observations in the experiments about the predictions of the learned spurious classifier. This paper is well written. The algorithm is based on an intuitive principle that is theoretically justified by the authors in this paper.<|endoftext|>Strengths:The tackled problem of identifying the environment invariant causal patterns for improving the interpretability of graph neural networks is interesting and important for many graph structured data representation tasks. The proposed DIR model is technically sound with the construction of interventional distributions and discover the salient features from different environments. In addition, the theoretical discussion is provided to analyze the proposed DIR method. In the evaluation section, the effectiveness of the new DIR model is analyzed from different aspects: model generalization ability, performance with different bias degrees, as well as the intrinsic interpretability. Weaknesses:It would be better to summarize the key notations used throughout this paper in a table, for easy reference when reviewer go through the solution details. More details about the experimental settings could be clarified in the evaluation section.<|endoftext|>The authors propose to improve generalization ability of GNN s for the task of graph classification via a causal theoretic analysis   under the assumption that only a subgraph of the graph is responsible for its label (and everything else are just spurious correlations). The idea to use a causal graph and the accompanying theory to explain the label associated with the graph is interesting. 2.The paper is well written and easy to understand. Via the top K approach, it appears like the authors make an implicit assumption about what fraction of edges is important in the graph (a fixed fraction, can t be lower or higher)   in my opinion, this certainly limits the wide applicability of the method. Minor:1.The separability of a graph into two subgraphs the causal and non causal might not always be possible?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposes to handle both continuous and discrete actions in reinforcement learning. The proposed method solves this problem by using the latent space and the latent action. Instead of using raw actions, which are continuous and discrete, actions are embedded into a latent space, in which all are continuous. Experiments show that the proposed method method combined with TD3 and DDPG performs significantly better. The concept of latent actions is very interesting. In the latent space, both discrete and continuous actions in the original space are represented by continuous vectors. This is based on the prior work on action representation learning for RL, but this work uses it to solve the difficulty of handling hybrid actions. This may be a small addition, but it looks reasonable to make the representations discriminative. This can be done in an unsupervised manner, so it doesn t introduce any additional cost of labeling. This is how to deal with the hybrid actions, and it can be used in any RL methods by replacing actions with the proposed latent actions. A concern is that among the four factors in Table 1, stationarity is not explicitly discussed in the paper.<|endoftext|>The authors proposed a novel framework for hybrid (discrete and continuous) action RL, called Hybrid Action Representation (HyAR). Then any RL algorithm for continuous action space can be used to train the agent. Experiments in various hybrid action tasks demonstrated that the proposed HyAR method contribute to significant performance gain to the baseline TD3 and DDPG models, as well as previous methods. Moreover, the t SNE visualization of learned actions representation shows an relatively interpretable latent encoding. Pros:  The paper is overall well written and easy to follow. In particular, two mechanisms are proposed (LSC, RSC) to alleviate the corresponding issues, which I feel important and should be described in more details in the main texts. The paper proposed a method to solve hybrid action control problems by encoding action to a continuous latent space via representation learning. Overall, I recommend acceptance. post rebuttal  Although I believe this work has some limitations that can be improved in the future, I feel the contributions of the current paper are already significant and I vote for acceptance.<|endoftext|>The paper presents an approach to handle action spaces with discrete selections and continuous parameters within each selection. The paper proposes an embedding space where the discrete component is mapped through a nearest neighbor selection, and the continuous component is mapped with a decoder conditioned on the embedding of the discrete component. The method to handle heterogeneous discrete continuous action spaces and the two extensions for RL training are evaluated on several simple interactive tasks that were used before in related work papers proposing methods for mixed discrete continuous action spaces. The results indicate an improvement over existing methods. I would include more about it in the main text  It would be interesting to apply it to more complex robotics problems (e.g.robot control) to see the limits of the solutionI would recommend accepting the paper for the conference. The text can be streamlined (there is a couple of typos) but the results are pretty solid, the problem is relevant and the idea is novel.<|endoftext|>  The authors are interested in the problem of Reinforcement Learning with hybrid action spaces. The absence of the “Related Work” section does help positioning the paper with respect to the literature. Experiments      The experiments, although convincing, are all made in pretty low dimensional state and action spaces. 3) Train a continuous action RL algorithm in the $d_1+d_2$ latent space. The problem at hand is very interesting and the proposed solution meaningful. Some works, like AlphaStar had already tackled hybrid action spaces (although little detail is given in the corresponding paper), but their solution was very ad hoc to the StarCraft problem, while the authors of this paper propose a very generic solution that could be applied to any (?I am not 100% sure about this, cf my question later) hybrid action space problem. Could you try to use them for the baselines too? In particular, I think it is important to understand how much effort was put into tuning the method as well as the baselines. The values of the most important hyperparameters should be explicit in the main text like the choice of $d_1$ and $d_2$. Could you think about such examples? If yes, I encourage the authors to simplify this paragraph. The work would be a clear 8 with an additional experiment on a more challenging and less toy ish environment. I argue this statement is too strong. 2) RSC requires more memory because you have to store both the original action and the embedded one in the replay buffer, and it needs more compute to calculate several times the embedding of a given action. Yet, I understand it can be left for future work.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; It is suggested that no single combination outperforms the others, but the proposed BEL approach has a potential to improve the prediction accuracy of continuous regression problems. ## Pros  It is interesting to consider such encoding/decoding approaches for regression problems in an application agnostic setting. The end to end training using L1/L2 loss with the proposed D^{GEN EX} seems reasonable, though experimentally "CE + GEN EX" works better in some cases (D2 vs D4 in Fig.5). ## Cons  I wonder how the best BEL results are selected in Table 2. I can understand the statement, however, it is not clear how such findings affect the prediction. In p.7, facial landmark detection seems better abbrebiated by FLD (instead of FDL). I like the motivation of the work and the authors provide a nice analysis of the problem from both theoretical and empirical perspectives. My major concern is about the clarity of the experiments in which the proposed BEL approach is compared with the existing regression models, as mentioned in the main review above. The newly added experimental results and manuscript modifications address all of my concerns. I have raised my score to recommend this paper to be accepted.<|endoftext|>The paper tackles regression problems using an error correcting code (ECOC) approach, i.e., reducing a *regression* problem into multiple binary *classification* problems. The paper examines several combinations of (encoding method $\times$ decoding method $\times$ loss function) on several (deep) regression tasks and reports significant improvements over existing methods. To the best of *my* knowledge, the authors are the first to propose using such codes in regression tasks. 1.Just before equation (1), should it be "for $k>Q_i$ instead of $k<Q_i$"? Moreover, I have some reservations regarding the comparison to the simplest baseline (the direct regression baseline) which I address below. **Empirical advantage over other methods should be more carefully demonstrated. I believe these findings require more careful description of the exact empirical process, and that as a reviewer I should make sure the comparison to other baselines is fair. Especially, I am bothered by the comparison to the "direct regression" baseline. I would even say that I would be slightly surprised if when the pretrained feature extractors will be also learned (rather than frozen), the direction regression approach will not be competitive with the proposed BEL method. **  Throughout my reading, several points were not very clear, and it made the reading slower and more difficult than actually needed. * Usage of the "decision boundaries" term is untraditional and can be confusing.<|endoftext|>The paper describes a general framework to solve a regression problem as a binary classification. The theory of the binary encoded label is well described also with clarifying examples. The analysis of Encoding/Decoding functions has been conducted for a simple Unary coding (BEL U) and the Johnson coding (BEL U) (in the Appendix). Even though the usage of classification for regression problem is not original, the work try to shed lights on some design choices. For the two detailed codes (Unary and Johnson), I suggest to describe clearly the Encoding and Decoding functions and then the computation of the error and other aspects present in sections 3.1, 3.2 and 3.3. For this reason I think that the Sections 3.1, 3.2, 3.2 should be revised in order to improve readability. According to Figure 4, the authors seems to have used only 1 Fully connected layer to connect feature vector to the output. As authors said this is left for the future works but the doubt is that with more layers the situation could change. Some typos:resepct  > respectdeocding  > decodingThe authors propose a general framework to approach regression problems using binary encoding and a set of classifiers. The paper is detailed, some interesting theoretical aspects are discussed and experiments seem to demonstrate the high accuracy of proposed approach even though only using shallow network after the feature extractor. I have raised my score.<|endoftext|>This study presents a label encoding method (binary encoded labels) to transform a regression problem into multiple binary classifiation problems. The authors designed encoding and decoding functions to encode a real value into multiple classes and decode the classes into the real value. as far as i understood the regression network and the decoding function are trained end to end. also, how does the scale of the original label and any normalization of the label on the proposed method? How they can be determined for a new dataset? for readers  convinence, please describe what each notation is in the main body of the manuscript. (e.g.theta in Table 1 is only described in appendix)The idea is interesting. The method is simple yet effective. I think the some issues needs to be clarified.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper studies the case of group robustness when only some of the group labels are known. They propose a generalized objective for this setting that considers the worst case possible group assignment subject to the known group labels, and an alternating optimization algorithm for minimizing this objective. The simple baselines of ERM and Group DRO (partial) Specifically, on Waterbirds and CelebA, many prior methods (such as JTT, GEORGE) that do not require *any* group labels substantially outperform this paper in terms of worst group performance. Similarly, on CMNIST, EIIL does not require group labels and outperforms the proposed approach (and I suspect JTT and GEORGE would as well if evaluated). Thus, L_{WDRO} is a fairly pessimistic loss upper bound whose loss landscape may be quite different than that of L_{GDRO}; indeed, this is borne out by the empirical results. Finding such a dataset would be helpful. Overall, the paper has some interesting ideas but unfortunately has limitations, and fairly poor performance in practice.<|endoftext|>The paper proposes a new variant of DRO, called Worst Off DRO, to address a ubiquitous real world setting where group labels are only partially available over the training set. Empirical results are reported to support the proposed methods. The paper is motivated by real world scenarios where only partial group information is available, for example when individuals may choose not to reveal this information due to privacy concerns. The paper is technically sound and well written. However, due to the two major concerns above (unrealistic MCAR assumption and missing important baselines), I can hardly recommend the paper as ready for publication.<|endoftext|>##########################################Pros:+ The theories in this paper are solid and are verified to be right. + This paper addresses a more realistic setting about fairness, and their solution seems to make sense. ##########################################Cons:  The research question of this paper is clear. They should add at least one paragraph about its motivation. They convert the feasible set to $\mathcal{C}_{\bar{p},\epsilon}$ by adding an extra condition, thus giving an error estimation will be better. However, related baselines are missing. "We optimize for the a worstoff soft group assignment"   "the" or "a" ? #########################################This paper works on a more practial fairness scenario and gives a solid solution in principle. However, there still exists some issues about writting and minor technical details.<|endoftext|>This paper tackles the problem of optimization for the Rawlsian criterion with only partial group labels. They propose to optimize according to the worst case of group assignment and also propose a method to constrain the set of group assignment. PROS:  This paper focus on an interesting problem that has both practical and theoretical importance. As stated in the paper, it naturally forms an upper bound of Group DRO. The paper is overall well written and easy to follow. I can get the point authors want to convey without ambiguity. CONS:  How the size of constraint set affects the convergence / efficiency is not stated theoretically / empirically. It is a crucial property for the proposed algorithm to be feasible.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper evaluates the training and generalization errors of the random feature (ridge) regression model, in the regime where the number of samples $m$, the input feature dimension $n_0$, and the hidden layer size $n_1$ tend to infinity at the same rate. As a consequence of this theoretical evaluation, the authors characterize sample wise multiple descents, steep cliffs, and how they relate to, e.g., the spectrum of the (random feature) kernel matrices and the structure of coefficient vector $\beta$. The major contribution of this work is to extend the high dimensional characterization of random feature (kernel) regression model to the case of anisotropic data. Many insights (multiple descents due to data and model structure, steep cliffs, etc.) offered by the proposed analysis are valuable in the future (and improved) design of large scale machine learning models more generally. Some detailed typos and remarks:* page 3 on the model: I do not understand why one needs to consider the setting of random coefficient vector $\beta \sim \mathcal{N}(0, \Sigma_\beta)$ instead of, e.g., considering deterministic $\beta$, as in previous efforts. * page 7 Figure 2 covariance model with $\alpha   10^3$: a typo here?<|endoftext|>Being more explicit about the linearisation assumption (c.f.*ot31* updated review). In particular, it shows that alignment between $\beta$ and the leading eigenvectors of $\Sigma$ reduces the test MSE, and that anisotropy can induce double descent like behaviour in the learning curves. My main concern with this work is the total disconnection with the relevant literature in the topic. This Gaussian equivalence was conjectured to hold also in the the anisotropic case of correlated inputs and target function weights, with extensive numerical evidence supporting it [3, 5]. This work provides some interesting insights on the effects of anisotropy in the RF ridge regression setting, but would strongly benefit from a major rewriting to account for a discussion of the relevant literature in this topic. However, I join reviewer *ot31* in the opinion that it could be further improved by:  Including the explicit connection with [5] in the case of the square loss with $\ell_2$ penalty (which the authors said they are already working on).<|endoftext|>This paper studies nonlinear random feature regression of a linear response in the *anisotropic* setting where both the covariates and parameter have arbitrary covariance matrix. This work does have some lack of novelty since versions of Theorem 3.1 were known and multiple descent phenomena and some effects of alignment had been recognized in anisotropic random feature regression. Are all of these equations necessary for the derivation? *Minor issues with (ii)*   pg 13: $K$ appears to be regularized in (S137), but later $\hat K$ is introduced that has the same formula. Should $K$ be non regularized in Section F? pg 13: $\theta$ is not defined and seems to sometimes be written as $\Theta$ or $\Theta_F$. To summarize, the manuscript would benefit from further revision to clarify its contributions and their place in the literature. Should it be 9x9 matrices? It would strengthen the paper to confirm these plots with simulations on synthetic data. **Proof of Theorem 3.1**My points below mostly concern Section F to the end of Section F.4.1 in the Supplement. pg 19: The details of R transform computations are omitted. My current recommendation to reject is because of the concerns mentioned above about the correctness, rigor, and clarity of the proof of the main theorem, Theorem 3.1.<|endoftext|>The paper studies random feature regression in high dimensional setting, when the data and model weights are assumed to be normal with anisotropic covariances. The results motivates the discussion on the multiple descent effect of the sample wise test error curve, and the authors also prove that the error against overparametirazation only exhibits double descent. Also, the error is improved when the data covariance aligns more with the model weight covariance. In general, the paper is written clearly with enough technical details to understand the arguments. Does the observations hold for more general data distributions? Seems that it is not described. Is that correct? In the sub field of high dimensional random feature regression, the paper present good contribution and results to consider the anisotropic data distribution, though the proof techniques mostly follow from previous works. In all, I feel the paper is above the borderline of acceptance.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposed an improved variant of the Stochastic Compositional Optimization (SCO) framework to train GNNs, replacing all nodes  moving averages with a sparse representation. The proposed algorithm only requires a fixed size buffer, regardless of the graph size, solving the memory issue of SCO algorithms and making it practically applicable to large graphs. 2.It seems that the hit rate (the ratio of sample nodes that can be found in the buffer) can be significantly affected by the sampling strategy. In general, given that there are many sampling strategies available now, considering only one sampling method (as baselines and used in Sparse_SCO) is insufficient. Most importantly, the proposed Sparse_SCO algorithm is not superior to Adam_Sample in terms of memory and time efficiency.<|endoftext|>This paper formulated the graph neural network as a stochastic compositional optimization problem and then developed a new optimization algorithm to train GNN. 2.The experimental results show improvement over existing methods. More explanation is needed. 2.The definition for the projection matrix is not very clear. No literature about the stochastic compositional optimization problem used this assumption. Thus, the variance could be large. I don t think this assumption is reasonable. But, based on Assumption 4, the variance could approach zero when $\beta$ approaches zero. The idea of reducing the memory cost is interesting and useful. However, there are so many flaws in the theoretical analysis.<|endoftext|>Specifically, it proposes to use sparse moving average for sampling based GNN training strategies. The convergence rate of the proposed approach has been proved under several assumptions. The resulting method can be applied to large scale graph training with sampling based methods. My main concerns of this paper are the novelty and the unconvincing experiments. This work considers the GNN training problem of large scale graphs. The main concern is that the novelty of this work is not enough. Hence, this point should be clarified.<|endoftext|>3.Under certain assumptions, SpSC has non trivial convergence guarantees, which is an advantage against many sampling based approaches. The difference between SCGD and SpSC is incremental. Also the analysis on convergence mainly follows the framework introduced in prior work. I think more comments on these assumptions should be helpful. 4.In the experiments, it seems that the time and space consumptions of SCO are comparable to those of SpSC, except for the product dataset. 5.There has been lots of recent work on scalable training of GNNs. Moreover, I think authors should compare more baselines in the experiments and the presentation could also be improved.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper provides an interesting direction to extract factual information from large language models accurately and consistently. Pros:+ This paper tackles one of the most important issues in the large language model: inconsistency results obtained from different prompts which have the same information needs. The problem itself is real and must be resolved. + This paper is well written and easy to follow. Instead of listing the result in the tables, it would be clearer if the authors provide more explanations about them although they are in the appendix.<|endoftext|>This paper addresses the problem of robustness for extracting factual information from large language models. It then proposes a few different methods for addressing this inconsistency that operate on the same portion of the language model, namely, between the input token embeddings and the first hidden layer of the language model. ### Strengths of the paper:  The problem of extracting factual and consistent information from large language models is of high interest to the NLP community. Overall, this work was well written throughout (easy to follow in most places except for a few rough parts detailed above). The experimentation work was also of high quality, with interesting results. Are they prepended/appended to the original inputs? I wonder if authors have tried using a weighted combination instead?<|endoftext|>There are two main reasons for this. The first direction, is the use of adapters to tackle the aforementioned problem. I did my best to identify these research questions in the paper and then try to link them to the experiment they conducted. Without it, it is hard to tell if the authors have conducted enough experiments to support their research question. Sometimes motivation given by the authors can be ambiguous. However, I had a hard time doing this. The second reason is the lack of experiments with the frozen LLM parameters of MoE. However, it can benefit from further citation of the literature in that domain. Such that it is likely to maintain all of factual knowledge learned during pretraining. I cannot recommend the acceptance of the paper in its present state.<|endoftext|>The paper presents an approach for extracting factual information from LLMs, where authors discuss a user oriented setting where they take in varied natural language prompts and return the objects with information needs. Authors proposed an approach, denoted P Adapters, which is a model that is between the embedding layer and first attention layer of LLMs. It takes LLM embeddings as input and output prompts used to query the LLM. The model and training procedure are well described and results are promising. It is also great that the data used is in the Github repositories. This said, the reader could benefit from better error analysis, as it is not clear how much hallucination and grounding the model is producing.<|endoftext|>This paper explores methods for improving the consistency of prompt based factual probing of pre trained language models. The main contributions are (1) several methods for mapping natural language prompts to continuous prompts that empirically improve accuracy and consistency on a factual probing benchmark and (2) analysis comparing reasonable alternatives in terms of accuracy and consistency. Strengths:* The problem setting mapping natural language prompts to more optimal continuous prompts is interesting. * The authors compare methods with different assumptions about the availability of annotations, and under different kinds of distribution shifts. * The paper is clearly written and the proposed methods are sensible and easy to understand. Drawing a connection to prior work on robust optimization, and providing a clearer formal justification for why this method will improve consistency. The current results are hard to interpret because models can be consistent but inaccurate, and because models can perform well by over fitting the entity distribution. * All of the models suffer a performance drop on the “OOD Objects” setting, which indicates that the models have over fit to the (imbalanced) distribution of entities in Wikidata.
Reject; rating score: 5; rating score: 5; rating score: 6; The main contribution of the paper is to show that using O(n polylog ) images one can recover all private images in the InstaHide challenge. This connection is simple but neat. The paper makes reasonable improvements to the sample complexity of InstaHide challenge. I feel that it is not, and paper has no experiments. I would like to know authors take on this.<|endoftext|>In this paper, the authors examines the sample complexity for recovery private images in the InstaHide setting. Comparing with existing works, the authors provided a tighter sample complexity bound when k_{priv}  2, which is nearly optimal up to logarithmic factors at the cost of an increased time complexity. It will be great if the authors can generalize the results for a larger impact. The authors provides a solid solution for a problem with limited scope.<|endoftext|>However, note that using such technology would result in an approximated solution to the regression problem. Secondly, the paper indeed sheds light on the weaknesses of InstaHide. The authors have in addition shown the hardness of the involved regression problem with hidden signs, which is responsible for a large amount of time (theoretically speaking) needed for the suggested attack on InstaHide. Is it possible to apply some data reduction techniques (e.g., coresets) of the data with respect to your variant of the $\ell_2$ regression?
Reject; rating score: 5; rating score: 5; rating score: 6; This paper proposes the Composable Perceptual Schemas recurrent architecture. The authors apply their architecture to three generalization scenarios:1. " The central claim of the necessity of feature attention and multiple schema modules seems to be refuted from Figure 3c. More details about these concerns are in the Main Review. I have raised my score to a 5, and I especially appreciated the clarifications and the qualitative analysis. It was difficult for me to take away from this paper a set of general principles that can be applied to future research. Regarding (2), this could be addressed with stronger empirical results. Given that the authors are motivated by modeling schemas that model independent aspects of the task, what measurement could the authors make to evaluate whether the schemas have specialized to learning something meaningful about different aspects of the task, in a way that would be distinguishable from the measurement take on randomly initialized schema modules?<|endoftext|>The paper proposes a modular state representation learning architecture called Composable Perceptual Schemas to discover regularly recurring patterns in its observations into “schemas” and process them in a modular and factorized manner. Experiments showing qualitative analysis of the subschema representations learned by the CPS model are interesting. [2] Goyal et. Another issue is with the lack of relevant baseline models being used to judge the benefits of the proposed CPS model architecture and its novel feature attention mechanism. Could the authors offer some explanation for this behavior? I’m willing to increase the score if my concerns are addressed.<|endoftext|>This paper proposes Composable Perceptual Schemas (CPS), a modular state representation learning architecture for reinforcement learning that combines modular RNNs as in RIMs (Goyal et al., 2020) with a dynamic feature attention mechanism. Finally, regarding the overall clarity and presentation of the work, I find that it could be improved substantially. yet these are the structures that are captured by the modules, which are referred to as subschemas. My question thus is, which are the schemas, the individual modules or their composition? This paper presents a method for learning modular representations for RL that allow for generalization in three regimes.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper presents a novel algorithm for identifying "provably robust adversarial examples": large regions in the input space that provably contain only adversarial examples. # Areas for Improvement## Originality  In the introduction, the paper states that "our regions are guaranteed to be adversarial while prior approaches are empirical and offer no such guarantees". mentions Liu et al., which "is also capable of generating provably robust hyperbox regions". Having said that, the paper has some issues with clarity. I d be happy to discuss this with the authors, but some preliminary questions:      What are the units of  robustness ?<|endoftext|>The term "provably robust" appears misleading; there is no theory showing that the examples must be adversarial. While authors highlight that there are massive amount of adversarial examples (say $10^{573}$) produced by the algorithms, such number seems really dependent on particular problems while lacking a theoretical justification. On the novelty of the algorithms, I feel it relies many black box components and their properties; which lowers the technical contribution if the work. **Updates after discussion**I agree that the paper brings out interesting ideas and the experimental results are convincing.<|endoftext|>In International conference on machine learning (pp.5231 5240). PMLR.Overall I incline to a weak rejection at this stage of the reviewing process but I am open to any discussions. ### StrengthThis paper has clearly stated its objective, the approach and the corresponding evaluations. The approaches are well documented in the paper and evaluations are conducted over several datasets and models. Motivation of the “provable” part of the adversarial examples is missing. To that end, can the authors explain more about the motivation for “provably” robust adversarial examples? 2.Important experimental setups and discussions are missing. This information should be helpful to understand the importance of the results.
Reject; rating score: 5; rating score: 6; rating score: 6; PrivHFL is a protocol for collaborative learning that does not reveal the private data to the server or answering parties $P_A$ if they operate as honest but curious entities. PrivHFL tackles the bottleneck of private inference by building a new protocol from scratch that can run on GPU. 7.**No assumption on a public dataset**. MixUp and Active Learning methods were already used in federated learning (FedMix, CaPC) so this is cannot be classified as a technical novelty. 10.Active learning was also used in CaPC. Figure 6: How many models have which architecture? 5.**No DP provided**. It seems that $P_Q$ can infer the model architecture that is used by $P_A$.<|endoftext|>The paper showed a new approach for heterogeneous federated learning, which uses augmented dataset instead of a public dataset for knowledge transfer between heterogeneous models. And shows superiority over the baseline methods, and also efficiency with regard to run time and communication cost. The authors proposed to use synthetic dataset generated using the Mixup method. I may have the following questions regarding this approach:1. How can we guarantee that no privacy information will be breached using this synthetic method? Will this still leak the privacy of local data? I feel not fully confident to assess the quality of the work as I am non expert in federated learning.<|endoftext|>This paper introduces a heterogeneous federated learning framework without requiring public datasets by designing a dataset expansion method and constructing cryptographic protocols for secure prediction. The authors have provided experiments on different datasets, heterogeneous local models, various degree of non IID ness across clients, and showed the runtime of three steps in their proposed secure querying protocol. Query data generation: It is not clear how $\lambda$ is selected? Given two private samples, the authors generate multiple query samples. Minor comment: On the RHS of Figure 4, $\delta _i$ should be $\delta $This paper is well written and provides extensive experiments which show the efficiency of PrivHFL and accuracy gains compared to a baseline, which uses models trained on the local datasets. Some aspects of the dataset expansion method and  the secure querying protocol are unclear, which require further clarifications.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; rating score: 5; This paper introduces a variational generative model based on Neural Cellular Automata (NCA). The model is called the Variational NCA (VNCA). The authors perform morphogenesis experiments on three datasets: MNIST, Noto Emoji, and CelebA. The results are good on MNIST, less so on the other two datasets, although there is clear evidence that the model can learn to generate meaningful images. The authors also perform an experiment to see if the VNCA is robust to perturbations (occlusions) and show that the model has a reasonable degree of robustness even without ever seeing any perturbations at training time. **Strengths**  I enjoyed the paper and, although I have some comments, I think the work can be interesting for the ICLR community. There is growing interest in (neural) cellular automata, and this work is a valuable addition to the topic. **After rebuttal**: The authors have addressed my comments. I have raised my score from "6: marginally above the acceptance threshold" to "8: accept, good paper". The authors propose a variational neural cellular automaton, which learns to generate images by iterating the transition rule. The paper is interesting, with good results, and a good fit for ICLR. The paper solves an interesting problem on the topic of neural cellular automata. There are some doubts/limitations that I have asked the authors to address (mainly concerning the architecture of the model). There are some missing references and details that would help the readers to get a better sense of the subject.<|endoftext|>The authors incorporate Neural Cellular Automata (NCA) into the decoding process of a variaitional autoencoder. They then train this model on a number of toy datasets, test out beta VAE to make better reconstructions, and show a few reconstruction/decoding tasks that highlight the contributions of the described approach. Pros:  Figure 2 is extremely clear, and I understand pretty much how the model works. I appreciate that the authors looked at a number of different datasets to compare their model against. I think the "damage" experiments in Figure 10 were very interesting and well done, and could have been more of a showcase of the paper. Cons:  I think this work could be improved by showing more tasks this approach would work very well for. This would be a great metric to compare against. For the decoding process figures (Figure 1 and 4), I don t see how this approach necessarily differs from a regular variational autoencoder with some sort of upsampling. With respect to inpainting of images, there are many other approaches that directly use the log likelihood that could be compared against. I don t think the claims of "super resolution" are well founded, especially if those claims are only using MNIST as the input dataset and the reconstructions have artifacts. Generally, I think this model is an incremental improvement over neural cellular automata. While I do think that this approach is interesting and the results are sound, I think there could be more careful analysis of where the VNCA shines compared to existing approaches.<|endoftext|>The submission proposes a VAE whose decoder is implemented via neural cellular automata (NCA). The authors show that this construction performs well when looking at the likelihood of the evidence (given samples) and show that the architecture has some robustness properties against damage during generation. As the authors rightly point out, cellular automata and self organizing systems are interesting directions of research. As I explain below, the main insight that can be derived from this paper (although not explicitly mentioned) is that a class of deep convolutional networks (networks that have repeated blocks and satisfy some other requirements) can be interpreted as neural cellular automata. With this alternative perspective, much of what the paper shows is not surprising and the questions that do arise are not addressed. From this perspective, the decoder architecture has repeating blocks (which allows it to be interpreted as an NCA). Furthermore, I believe that many deep convolutional networks with repeating blocks (say deep resents) can also be interpreted as NCAs. Therefore, what seems to be novel in this paper (in the eyes of this reviewer) is the NCA *interpretation* and not the architecture itself. As mentioned above, the NCA decoder can equivalently be viewed as a deep net of a specific shape. An important question for the paper is then: what does one gain by this NCA interpretation? Is there any way to train this system such that the updates to the weights of each automaton are also local?<|endoftext|>This paper proposes variational extension of Neural Cellular Automata for image generation. It performs experiments on MNIST, Noto Emoji and CelebA. The likelihood results are shown to be significantly behind SOTA on CelebA and also behind on other datasets. The paper provides qualitative analysis of the self organized generation process and shows robustness to early stage perturbations of latents. Pros:1.The paper is very well written, I could clearly follow the ideas and the experimental setting (modulo a few minor points mentioned at the end of this review). 2.It is interesting to see how good NCAs are in terms of likelihoods. Even the knowledge that they are not so good is valuable for the community (although I would prefer to see a few independent attempts like this to draw such a conclusion). The experimental results are underwhelming. There is little evidence that the method actually works, except on MNIST. 2.The motivation of this work is not fully clear to me. What problem of prior work is this paper solving? (2) "The Variational Auto Encoder (VAE) is a seminal probabilistic generative model"   I would suggest citing both Kingma & Welling and Rezende et al., both papers appeared roughly at the same time. (6) "state of the art, which is around  78.6"   assuming bits, it s not sota. (8) "This is far from state of the art which is around 2 bits per dimension"   what are the dimensions?
Reject; rating score: 3; rating score: 5; rating score: 6; The paper applies the recently proposed self supervised learning method Barlow Twins to graph structured data. Although the paper is clearly written and many experiments are presented, the paper does not meet the bar of the top conference such as ICLR because of it being a very direct application of a previous paper. I am willing to increase the score of the paper if in the rebuttal, the authors can clearly state the novelty of the paper w.r.t to the Barlow  Twin paper. Weakness:The paper is the straightforward extension of previous work Barlow Twins for graph structured data.<|endoftext|>The main contribution of this paper lies in that it adapted Barlow Twins from vision to graph representation learning field and evaluated the performance of this self supervised framework in multiple node classification tasks. This paper is a heuristic attempt to apply Barlow Twins in graph domain. I have some minor concerns below for the authors to address. 1.The novelty of this paper is limited. 3.The experimental results are on par with baseline methods on the most tasks.<|endoftext|>Disclosure: I have reviewed a previous version of this paper. The paper is clear, well written and easy to follow. My main concern would be with the reported BGRL results on ogbn products. The authors have presented a useful extension of Barlow Twins into the graph domain, and now have experiments in support of the industrial relevance of their method. The novelty is somewhat limited (as is the case for most of the recent graph SSL papers that adapt image domain techniques) but it is useful in and of itself that the gains observed in images transfer well to the irregular domains.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; In this paper, the authors focus on the problem of positive and unlabeled learning. Especially, the observed phenomenon and the key idea of correcting marginal pseudo negative examples with a heuristic mixup technique is interesting, and may potentially be useful for semi supervised learning. The phenomenon may imply there are a number of marginal pseudo negative examples that are more likely to be positive but labeled as negative. Based on this, the paper proposes a PU learning approach building on a novel heuristic mixup technique, which can achieve both data augmentation and supervision correction.<|endoftext|>This paper studies an interesting weakly supervised binary classification problem called positive and unlabeled (PU) learning. The authors propose a novel PU learning method inspired by the boundary deviation phenomenon observed in experiments. Specifically, a new mixup method is proposed, which selects the mixup partners for unlabeled examples heuristically to obtain more correct supervised signals. Extensive experiments and ablation study demonstrate the effectiveness of the proposed method.<|endoftext|>This paper proposes a variant of the mixup technique for positive unlabeled learning. The proposed P3MIX method and its variant improve the classification performance of PU learning. The ideas of marginal pseudo negative instances and candidate mixup pool are interesting. Since PU learning is different from ordinary supervised learning, it would be reasonable to develop a specific mixup approach for PU learning. The idea of marginal pseudo negative instance estimation is interesting.<|endoftext|>The authors proposed to reduce the bias of classifiers learned on PU data by a heuristic mixup technique that partially selects the unlabelled instances and mixes them up with the positive instances around the decision boundary learned with PU data. Strengths+ The motivation of this paper is strong, and the research problem is interesting. + To select the marginal pseudo negative instance, predictive scores can be different by employing different learning models. I like the idea of using a heuristic mixup technique for reducing bias in PU learning. I think the idea that adds heuristic to mixup could be extended to other weakly supervised machine learning tasks.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Despite the reported empirical improvement over the considered baselines, I found the motivation of the proposed approach unclear, and I found that the main claim is not well supported (i.e.is GIFT indeed capable of generating examples from intermediate distributions? Moreover, the manuscript lacks clarity in several aspects and I have concerns regarding the significance of the reported results since it seems only a single run was considered in each test case.<|endoftext|>2.The motivation and the overall idea of the paper is easy to follow. “What is being transferred in transfer learning?”. To prove that the proposed GIFT is better, the above methods should be compared after applying to the same problem. However, the proposed lambda scheduler is a linear function with a fixed step size. Why not keep the same training setup?<|endoftext|>The work presents results on synthetic and natural distributions to evaluate their claim. mismatch in lambda_scheduler step size notation \delta in Algorithm 1 and \sigma on page 3  figure 1 has "gradual self training" as a caption in one of the subplots but this method is not mentioned or cited. + The problem is well motivated and illustrated through different experiments and toy examples.<|endoftext|>On a few small synthetic as well as natural image datasets, GIFT outperforms a few baseline methods. I haven t seen papers that anneal the mixup weights. The authors also should compare with many more existing methods. The technical novelty of GIFT is limited. However, the experiments are highly insufficient, with missing ablations and baselines. Moreover, the evaluation datasets are not so popularly used.<|endoftext|>The effectiveness of the proposed method is evaluated and results in several interesting conclusions. Strength:This paper is well written and easy to understand.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 6; This paper proposes a reparameterization of non linear non convex optimization problems. This reparameterization amounts to a linear map (i.e., "optimization params   linear operation of a different set of parameters). These linear maps are interpreted as a graph convolution network. Weaknesses:* The experimental evaluation consists of two problems that are not of interest to the ICLR community. * The final reparameterization is not very interesting   although much ado is made about "using a neural network parameterization", it s just a linear map at the end of the day. As with the optimization comment above, I think this work needs to be grounded more in the literature. * GCNs are tangentially relevant, but don t seem to be used in any really meaningful way. This paper is clearly unready for publication. The main idea   using a structured linear reparameterization   is under developed, and the experimental results are on problems that the ICLR audience don t really care about.<|endoftext|>This work studies the question to what extend a reparametrization of an optimization problem, i.e.representing the original parameters w to optimize for as a function of some other parameters theta, can accelerate the convergences of the gradient flow / gradient descent for nonconvex optimization problems. In particular, I am wondering how specific the acceleration results are to the applications? Also, what amount of hyperparameter tuning is required for the proposed approach to work well? The appoach seems to reduce to a linear reparametrization, which seems to relate it to other (more classical) approaches. Along with the list of minor aspects that make the paper a little difficult to follow, I need some clarification on this aspect. Unless I misunderstood the main idea significantly, the "neural reparametrization" illustrated by a neural network in Fig.1(b), later turns out to be a linear parametrization only, i.e., considering the gradient flow for theta in $w(t)   \kappa(t) \theta(t)$ instead of in the original variable w.  Before considering this to be a graph neural network, I would have been interested in how this idea relates to other classical optimization methods: Has the idea of temporally changing but linear reparametrization not been considered in the optimization literature before? For me, the paper would have been easier to follow from this more classical optimization perspective. If now the change in $\kappa$ is negligible slow in comparison to the change in $\theta$, and if $\kappa$ represents the (scaled) square root of the inverse Hessian, isn t that the flow arising from Newton s method?<|endoftext|>This work proposed a neural reparametrization scheme to accelerate a large class of nonconvex nonlinear optimization problems. More specifically, by reparametrizing the optimization problem with a graph convolutional network (GNN), the proposed method can modify the condition number and obtain convergence speed up, the acceleration is demonstrated on optimizing synchronization problems and persistent homology of point clouds. Overall, the reviewer finds the paper is a bit hard to follow, and the presentation of the paper can be significantly improved. The experiments are interesting but the comparison is not quite comprehensive. First, the reviewer is not fully convinced by the benefits of reparametrizaiotn. Second, it is a bit unclear to the reviewer why in Section 2.2, the authors considered the NTK. Third, the speed up in Figure 2 does not seem impressive.<|endoftext|>The authors derive a neural reparameterization of non convex optimization problems in order to accelerate their convergence. They do this by deriving how the slowest components of the optimization variables can have their convergence rate improved by preconditioning with a NTK based matrix. However, one drawback of this approach is that it appears that it seems to only help the early stages of optimization, as this is how it is used in the experiments. I think the authors should take more care to make this point more clear. **Minor Points** The authors seem to pose the title and introduction to refer to any non convex optimization problem, but in some parts of the paper they seem only focused on neural network optimization (e.g.Fig 1).It would be good to smooth out these inconsistencies. The abstract on OpenReview and the abstract in the article do not match. In the experiments, why is the term "linear" used to refer to the gradient based baselines? I am not sure this is the best term to use and was confusing to me upon my first read. However, I think some more experimental verification is needed for ablating the different components of the proposed approach and for demonstrating its applicability to a broader range of problems.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; However, I have some concerns regarding the required width of the network, which might limit the overall practicality of the results. One suggestion could be to just focus on one of the bases (say polynomial or fourier) in the main paper, and defer others to the appendix. This would give the readers a more direct understanding of how the construction works.<|endoftext|>The key idea is to show that, by pruning a network, we are able to approximate a finite set of universal base functions. Overall, I think the paper is well written and studies an interesting direction. However, my main concern is the significance of the result. Such linear seems to make the result a bit trivial than expected. However, due to some concerns on the significance of the result, I am not sure whether the result is above the acceptance bar of ICLR.<|endoftext|>The developed proof could be of independent interest since it leverages the power of depth and improves the existing bounds. Could you also specifically show the case when the mother network has a smaller depth in the main section? Besides, my comments are as follows: By looking at Theorem 1, it seems that the major contribution compared to existing works (in terms of the theoretical results) is that this paper does not require the mother network to have depth $2L$ to approximate the target network with depth $L$, but could be of any depth, at the price of using more neurons per layer.<|endoftext|>However, I think the paper may have some significant limitations. This paper only has an informal theorem in the main paper, and has more detailed results in the appendix. Moreover, the authors set a sparsity level of 0.5 in the experiments. I feel that this may not be sparse enough. If this is the case, it seems pointless to introduce $\lambda$. In terms of the presentation of the results, the authors may consider presenting formal theorems in the main paper, and add discussions to justify the results. For the reasons listed above, I think the current paper needs some further improvement.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Prior works investigated the robustness of ViTs and CNNs under the adversarial attacks designed mainly for CNNs and conjectured that ViTs are more robust than CNNs. However, in this paper, the authors attack a specific single patch and develop an attention aware attack framework against which they find that ViTs are weaker learners than CNNs. This paper provides a new perspective for evaluating the robustness of ViTs which is novel and insightful. The experiments are extensive and solid enough. Some recent works have also proposed adversarial training for ViTs such as [1].<|endoftext|>Given recent finding shows that ViTs are more robust than CNN, this paper investigates an intriguing question:“Under what kinds of perturbations do ViTs become weaker learners compared to CNNs". They propose a Patch Fool to fool the attention mechanism. Their investigation leads to some interesting findings and might inspire more interesting future work. The strengths of this work include (a) extensive experiment to benchmark the robustness of different ViT varinst; (b) proposing a new attacj framework; (c) insightful findings. [1] also discusses universal attacks and I am curious whether the above attack method can be extended beyond Image dependent attack to universal ones.<|endoftext|>This paper propose a new attack on Vision Transformers (ViTs) called Patch Fool. The authors later go on to show that increasing the number of patches seems to make the attack more effective. Patch Fool does not have an \epsilon factor like traditional robust training seems to have, so, essentially, we can perturb the image by a large amount to make the attack more potent and potentially make large changes to the image. Maybe the authors could also quantify how far these adversarial examples are via L2 distance. 2.Related to the previous point, if we perturb all patches in the image, accuracy drops drastically for both CNNs and ViTs. I realized its accuracy on a later reading but the authors should mention this in the caption4.<|endoftext|>This paper studies the robustness of vision transformers (ViT) from the perspective of adversarial attacks on patches, where the attack algorithm is particularly designed to fool the attention mechanism. While some prior works show that ViT has better adversarial robustness compared to CNNs, this paper shows that ViT has worse robustness against patch attacks, when only a few patches are manipulated and the perturbations are dense within the patches. * This paper successfully identifies the vulnerability of ViT against dense patch attacks, while ViT was more robust under traditional Lp perturbations or natural perturbations in previous works. ViT was just relatively more robust than CNN. This paper makes an important contribution on identifying a particular vulnerability of ViT against patch attacks compared to CNNs and has comprehensive empirical results, though this paper does not consider robust training for the models.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5;  This paper proposes PP GNN, a novel graph neural network that learns multiple adaptive polynomial filters acting on different subsets of the eigenvalues. They show that the piece wise polynomial method can approximate a latent optimal filter better than a single polynomial in theory. Weaknesses:1) As the author discussed in the paper, PP GNN cannot be scaled to big graphs due to the complexity of the eigendecomposition, and therefore must make a trade off between performance and efficiency. I m concerned that a large number of hyperparameters will degrade the model s generalizability and result in considerable tuning work. 4) The implementation performs SVD on the node feature matrix, which is not discussed in the paper. In particular, the time cost for this operation should be discussed. Graph neural networks with convolutional arma filters. However, the theoretical depth and novelty may not be enough to meet ICLR s standards.<|endoftext|>This paper considers the problem of designing polynomial graph filters for use in graph neural networks. Based on these concerns, I do not recommend the acceptance of this paper. Of course, doing this requires a full eigendecomposition of the graph matrix, which has high complexity. In particular, evaluating the filter on the low pass and high pass components is still only defined in the spectral domain. I would like to draw attention to the claim following Corollary 4.2.1: "the dimension of the space [of filters] increases significantly by using just two adaptive filters." This is the reason why we like graph filters of low degree: because they are localized! However, in the proposed approach, you claim to also be using low degree polynomials, while achieving significantly higher expressivity. In this sense, it is almost obvious that your filters will be more expressive, since they are indirectly using much larger neighborhoods of the nodes, merely having the restriction of being implementable as low degree polynomials in a particular subset of the spectrum.<|endoftext|>In this paper, the authors aim to develop GNN that can better adapt to the given prediction task (both homophily and heterophiliy). The effectiveness of the proposed GNN architecture is demonstrated on diverse node classification tasks. Strength  The proposed architecture is well motivated. Weakness  There are a lot of hyper parameters to tune. ( The number of partitions, which can be interpreted as the number of filters, is swept in the range [2,3,4,5,10,20].The polynomial filter order is swept in the range [1,10] in steps of size 1. Please clarify the eventual computational complexity of the model that s used in the experiments. It is not clear why the reported performance of ogbn arxiv is much worse than that in the leaderboard.<|endoftext|>I think this idea is worth publishing in ICLR, albeit not in its current form. A) The numerical experiments are extensive, but unfortunately, the choice of baselines is misguided. Thus, a comparison with ARMA filters is fair in the context of graph convolutional neural networks. B) A discussion on the relationship between the proposed method and scattering transforms is necessary, due to the nature of the spectrum partitioning and wavelets. However, I find the premises of the paper not to be entirely adequate. In particular, there is quite a few references missing that should be more relevant for the discussion at hand. The next argument that the authors make is the choice of K. The authors correctly identify that for K   N 1 all possible filters can be learned (in theory, albeit probably not in practice due to optimization problems in a space that grows with the size of the data) from the FIR graph filter due to the Cayley Hamilton theorem (as long as the eigenvalues are distinct). The sixth concern that I have is about computational complexity. Going back to them, while interesting from a conceptual standpoint (and I do appreciate it), requires a much lengthier justification. The inclusion of a polynomial filter is slightly misleading.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; * The paper conducted experiments across several domains, involving a number of different datasets and tasks. In light of these works and lack of discussion and comparisons, the novelty and contributions of the work are unclear. Deep anomaly detection with outlier exposure. While the authors provide some experimental results that suggest their approach outperforms other methods, there was little justification for the different choices made and no analysis of how these decisions impact their model s performance. Because there are no strong theoretical justifications, ablations of some of the different components of the proposed model are necessary. It would be interesting to see this discussion. Fix the equation in 3.1: ‘th’ as threshold should be explained.<|endoftext|>This paper was fundamentally about out of distribution detection. The strengths of this paper are two fold. There were, however, some significant weakness in this paper. When you pick evaluation, baseline, and data, it is easy to show good performance. I think this paper has a lot of potential, but but has technical issues that should be addressed before acceptance. What they measure seems strong, but it is the connection between those measurements and the stated goals of the paper that are the limiting factor.<|endoftext|>In particular, it suggests a  hierarchical  evaluation method based on computation of existing distance/similarity scores on hidden embeddings of high dimensional data succeeded by a temporal component, that is also based on existing work, that treats chunks of  scores, computed by the first step, of input data as a stream. (2) the hypothesis stated in 3.2.1 (that in the latent space ood samples will lie in distance from  iid samples), especially for high dimensional inputs. Do the authors think that modifying the objective function to explicitly support this claim could further improve performance? I think it would be good if the authors could report an ablation study on the size of the coreset. However, I think some suggestions on the ablation study and some comments on the scalability on large datasets should be addressed to strengthen the paper.<|endoftext|>You could instead talk about time independent and time dependent distance measure. Moreover, you should refer to some work from the by now large literature on uncertainty estimation and calibration of classification models. Given this, I recommend to not accept this paper, as it is not yet in a state to be published in ICLR. However, given the encouraging results and the importance of the problem, I encourage the authors to address the weaknesses above, revise the paper, and submit it to a later venue. The strengths of this paper are that is addresses an important problem, it provides thorough experiments and it is well organized and easy to read. Unfortunately, the paper suffers from a number of major weaknesses, in that the title and intro paragraphs are misleading, there are issues with terminology and there are missing references to the literature on filtering and uncertainty estimation.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper aims to improve the quality of the parametric bootstrap when the number of particles is limited. It does so by constructing a small collection of "centroids" that well approximate the ideal bootstrap distribution on parameter space in a certain Wasserstein distance. Practically, the centroids are constructed by gradient descent with respect to a loss function involving the negative log likelihood. Further, these differences can only be ascertained by a careful read of the Appendix, which leads to a lack of clarity on what is being proposed.<|endoftext|>The paper proposes a way to optimise the choice of Bootstrap samples so as to obtain efficient inference with smaller number of such samples. The major points can be summarised as follows:1. While the concept and idea is nice, I think they are not entirely novel. Can you provide an experiment to verify that? How much do the results depend on the starting seeds? 2.Additionally, I cannot quite understand some of the intuition in the paper. 3.The primary usage of Bayesian methodology is to provide uncertainty quantification for small data sizes.<|endoftext|>Some discussion would be helpful about this. As a consequence, bootstrap can be quite expensive and even unaffordable for deep learning problems with huge models. The paper proposes to address the question of improving the accuracy of bootstrap inference when the number of particles is limited. The approach is to minimize the Wasserstein distance between the ideal bootstrap distribution (denoted by $\rho_{\pi}$ in the paper) and a probability mass mass function supported on a small number of particles (called "centroids" in the paper). Revised comments: I thank the authors for the updates on the paper and their detailed responses. It would take a major revision to address these concerns, so I am leaving my rating unchanged. If the authors are working in the fixed $d$ case, that should be clarified. 2) The algorithm lacks clarity. Is this optimized in any way?<|endoftext|>Here a bootstrap particle corresponds to a machine learning model and for a large number of bootstrap particles, a huge computational cost is incurred. The authors propose using a k means like centroid approximation which optimizes a set of centroids (each of which corresponds to a machine learning model) minimizing a data dependent Wasserstein distance. The paper addresses an interesting computational problem applicable to many models in machine learning. It is presumed that the iterative nature of the algorithm incurs many model updates and model loss evaluations in the inner loop. How much compression can be achieved in terms of memory? Please also comment on the dimensionality of theta in each experiment setting. See above for details.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The key idea is to change the hidden layer representation at which the concept based method is being applied in an adversarial manner. While the paper focuses on an interesting problem, it does not seem to add anything new to our understanding of robustness of interpretability methods. As the paper itself mentions, the adversarial attacks on representations of the networks, and adversarial attacks on feature based interpretability methods have already been out there. The only new thing that the paper seems to do is to apply the adversarial attacks on concept based interpretability methods. More in detail, are there any fundamental differences between the attacks on feature based interpretability methods (e.g., [Dombrowski et al.](https://proceedings.neurips.cc/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a Paper.pdf)) and the proposed attack on the hidden representations? Do we expect one attack to be harder than the other? An empirical or a theoretical analysis (see Dombrowski et al, or Adebayo et al) would have been greatly helpful. Without such insights, the contributions of the paper seems limited. Interesting idea, but lacks novelty.<|endoftext|>The authors derive a new adversarial attack, token pushing attack, that targets concept based explanations. **Clarity**: the paper is generally well written and clear. I think that the general concept of creating adversarial attacks for concept based methods is interesting and relatively novel. However, some aspects of the method are not evaluated in the experiments. However, the implementation of the idea requires assuming that the interpretability model will be re trained, which limits the applicability of the method. The attack is evaluated on one dataset and predominantly on one model, InceptionV1 (although there is one experiment of the adversarial examples from InceptionV1 to ResNet 18). I would ve expected the method to focus on trying to change the explanation, e.g.the TCAV score, for fixed concept vectors.<|endoftext|>This paper proposes a novel adversarial attack against concept based explanation methods. Technically, it utilizes a widely used attack method on DNN   PGD to generate perturbations. Adding these perturbations to concepts will mislead the target explanation methods to pinpoint meaningless parts in the input as important features. Most of the existing works on adversarial robustness focus on deep learning models themselves. Detailed comments and suggestions can be found above. As discussed above, this is not the first work that studies the robustness issue of concept based explanation methods. It has been studied in previous works. 3.The technical contribution of this paper is thin. 3.Despite its correctness, the novelty and depth of the proposed technique are limited. 10 has discussed the robustness of concept based against a type of attack. Regarding the target methods, the authors chose TCAV and another work that was recently published on distill. 5.I would suggest the authors discuss and even evaluate potential countermeasures of the proposed attack. 6.The writing quality of this paper is limited.<|endoftext|>It introduces Token Pushing (TP) attacks, which learn small perturbations for the token of concept leading to different output for the interpretability method. ### StrengthsThe paper tackles a novel problem of adversarial robustness of concept based explanations, and the proposed token pushing attacks could be of interest to the community. The proposed method is simple, and the paper is generally clear and well written. The paper perturbs the hidden representations to demonstrate the concept based explanations  vulnerability; however, the injected perturbation is not imperceptible. In light of the above concern, existing adversarial detectors [1,2] can defend against the proposed attack. Can the authors validate their proposed attack against these adversarial detectors? Further, it would have been more effective to inject imperceptible perturbation in the input space to show that the concept based explanations are vulnerable, as concept based explanations are comparatively more robust to adversarial attacks in the input space (see Appendix A [3])  The experimental evaluation is limited to two concept/class pairs on a single dataset, making it difficult to conclude the effectiveness of the proposed method.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The mentioned applications in the paper are practical and novel, it is interesting to see this work could promote the understanding of many empirical findings. Cons: I have some concerns as following list though. 3. in equation 54, the sign of $\Gamma w^\star$ should be positive?<|endoftext|>Strengths:  The paper is very well written and easy to read. The results are novel and close a few important gaps. To the extent I have checked the theorems are sound and derivations are accurate. 2: The following should be stated as an assumption that might not hold in practice. This work makes a clear contribution and is very well written.<|endoftext|>I think this finding would be of interest to the community. Strengths:  Clarity: The paper is well written, and the main technical results are well presented.<|endoftext|>Its not clear to me that this is a setting that is a limitation for the Hessian approximation. Consider discussing the recent empirical work "Stochastic Training is Not Necessary for Generalization"In summary, I think this is a good paper and leaning towards accept.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; Over the last few years, several machine learning based approaches have been proposed to solve common combinatorial optimization problems, such as the traveling salesman problem. However, it s never been entirely clear how well the previous work generalized to out of distribution instances. This paper proposes to improve generalization through dataset augmentation. Specifically, the paper proposes to train a model (aka the generator) that is used to drive the creation of training instances on which the neural network used to solve the combinatorial problem (aka the optimizer) does poorly. The authors then show that training the optimizer model on a set of examples taken from the original dataset as well as created under the guidance of the generator model results in better performance than training the optimizer model on the original dataset only. The paper is reasonably well written, though section 3.1 would be easier to follow if the architecture of the generator model was described using a figure instead of just text. Moreover, since the authors evaluate their approach on optimizer models trained through reinforcement learning (according to section 2), I would have liked to see a discussion of other approaches that have been taken to improve the robustness of RL techniques in the related work section. Furthermore, it evaluates the effectiveness of data augmentation in conjunction with 2 common optimizer models, AM and POMO, which indicates that data augmentation can be leveraged to improve the effectiveness of several prior work in this area. However, it is widely known that data augmentation helps the performance of neural networks, and the authors fail to demonstrate the effectiveness of their particular flavor of data augmentation. This paper attempted to tackle an important problem which hasn t received the attention it deserved, namely ensuring that neural networks trained to solve combinatorial optimization problems generalize well.<|endoftext|>This paper presents a generative adversarial training pipeline for general combinatorial optimization problems, aiming to improve the generalization ability of learned neural network solvers on unseen data distributions. In experiments, the authors select the attention model as the neural network solver, and REINFORCE, POMO as the RL algorithms. The experiment results are comprehensive on various routing problems, and on the knapsack problem. The idea of generative adversarial training for combinatorial optimization is new. 1.Figure 1 presents a clear and straightforward overview of the proposed framework. My first concern about this paper is the motivation and necessity for developing generative adversarial training methods for combinatorial optimization problems. However, the results of neural networks, whether aided with adversarial learning or not, are all inferior to the traditional solvers. It makes me doubt the capacity of the current neural network models. 2.Besides, is there any special design to prevent the neural network from "forgetting" the original distribution? The results on the "base" distribution entry show that the performance of the model will probably degenerate after generative adversarial learning, suggesting that the model seems incapable to learn the input to optimal mapping under the current pipeline and is likely to forget some information from the original distribution. 1.Concerning the experiment part, it will be very interesting to see the result of traditional algorithms that are configured to run comparatively faster with learning based methods. For example, the number of trials of LKH 3 is configurable to control the runtime of the heuristic and the current configuration (10000 trials as written in the appendix) seems too time consuming. On page 7 the authors mentioned that "Generally, their (the neural network s) run time is much shorter than the traditional algorithm as indicated in the table(s), which is a core strength of deep models." I doubt whether such a conclusion still holds if you replace your current solvers with a well configured LKH 3 solver. It will make much more sense if you do not set a large number of trials for LKH 3 to force it to run super slow. However, there is still room for further improvement if the authors can include more problems beyond routing problems because the title of this paper is claimed for general combinatorial optimization problems. See some examples in "detailed comments" below. 1.In Table 1, the inference times of AM and GANCO AM are regarded as the same, which is reasonable because they share the same model architecture. And in Table 2 and 3, the inference times of AM/GANCO AM and POMO/GANCO POMO are different, which is also reasonable due to random noises in experiments. The generative adversarial training for combinatorial optimization in this paper is novel, the experiment results are detailed and comprehensive. However, the motivation, writing, and the experiment part of this paper seem to have room for further improvement, and I am suggesting a rejection for this time and I am wishing to see an improved version in the future.<|endoftext|>This work presents GANCO, a framework that extends the training of RL heuristics for combinatorial optimisation problems (COPs) to include an adversarial agent that aims to generate hard to solve problem instances. Whilst the GANCO framework is novel as a whole, it is the combination of existing ideas and methodologies (e.g, learning construction heuristics for CO problem and adversarial training). **Strengths**  The paper is well written, the framework is presented clearly and (along with the supporting code promised to be released) with sufficient information to reproduce the key elements if the work. My feeling is that it is not clear cut, with the experimental results impressive, but to me only unequivocally demonstrating that training on a broader distribution of tasks results in a more general agent. I find the task itself, improving generalisation in RL COP solvers, to be well motivated and prescient. To the best of my knowledge, the proposed framework is novel. By considering multiple COP instances and using established methodologies, the empirical performance improvements are clear, however it there remains scope to further improve the analysis of GANCO itself as discussed below. **Weaknesses**  Whilst the broader distribution of training tasks provided by GANCO is clearly demonstrated to improve generalisation, the necessity of the adversarial entity (GANCO’s core contribution) is not demonstrated clearly enough to unequivocally endorse GANCO as an optimal (or very good) approach to generating additional instances. It would be extremely hard for the genetic operators to attain instances following a significantly different distribution.”. At the least, I would like to see comparisons to: 	1. The performance on an RL agent trained on multiple/all target distributions. 2.The performance of an agent trained on the curriculum of instances generated by GANCO *from a different training run*. I do not believe Figure 2 is suitable as it is a set of ‘plots’ denoting how the authors hope GANCO will change the performance across a distribution (e.g.performance is strictly improved for all instances and all stages of the pipeline), and does not present any data or additional insight. The authors claim that the challenge of generalising to out of distribution (OOD) test time instances is “particularly severe for neural combinatorial optimization (NCO) models because the solution quality intricately depends on the instance distributions”. Whilst I would agree that OOD generalisation is challenging and important, it is not clear to be that NCO models find it especially difficult compared to the multitude of other ML/RL applications.<|endoftext|>The paper proposes a way to generate training examples for neural combinatorial optimisation  (NCO) methods (specifically evaluated on POMO and AM) that leverages an adversarial RL policy trained in stages to generate instances that maximise the performance advantage of a baseline CO method over the NCO method. The method is tested on a variety of CO problems and shows improved OOD generalization especially for larger problem instances. Strengths:  paper is well written  good evaluation  intuitively understandable method with good results, both in terms of generalisation and speeding up training (which is expected due to other works establishing selecting tough training instances helping convergence)Weaknesses  the concept reminds me strongly of https://ai.googleblog.com/2021/03/paired new multi agent approach for.html which should be cited   I wonder how much of the improvement is due to simply training on more examples? I d still expect the adversarial method to be more universal, but if the distributions we might expect are somewhat known, domain randomisation can be computationally cheaper and simpler than this setup. Other comments:  instead of training in stages, did you attempt alternating the optimizer and the adversary on a per step basis? The paper presents an intuitive approach to generating training examples, makes it work and does a good evaluation. However, similar work has been done in the RL domain and other supervised training domains before (e.g.using GANs for data augmentations) so giving a marginal accept for now.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposes a structured pruning method based on kernel orthogonality analysis. The performance gain in ImageNet is incremental and not sufficient compared to the state of the art methods. 2.Experiments on ImageNet are insufficient and results are not comparable. 3.Moderate novelty.<|endoftext|>This paper proposes regularizing learnable parameters of NNs to maintain the dynamical isometry during pruning and improve their accuracy. The paper is well written in general, and the proposed methods are intuitive. This is achieved in the proposed method by user defined parameters that identify the indices of parameters to be pruned. Orthogonality is aimed to be achieved by regularization on gram matrices of kernels. In addition, regularization of batch normalization parameters has much more effect on the accuracy compared to orthogonalization. However, the proposed claims are not explored well.<|endoftext|>This paper studies how to maintain the “dynamic isometry” property during pruning. Specifically, after getting an initial assessment of filter importance, the algorithm will maintain the partially kernel orthogonality of the important filters. The method is quite intuitive and simple, which I think it’s good. Why this paper only use kernel orthogonality instead of convolutional orthogonality? In its current form, all I can see is the proposed method does not have a clear advantage over the previous method.<|endoftext|>The paper proposes a dynamical isometry preserving approach for structured pruning by introducing an orthogonality preserving regularization for weight matrices in each layer. 3.Overall the paper is well written. The role of BN regularization is not clear to me. It is not clear how the BN parameters of pruned filters would make a difference in dynamical isometry (DI) and how the proposed regularization alleviates this issue? 2.It is not clear that the method indeed improves DI.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The paper extended the use of prompt tuning to the scenario of life long language learning. The method achieved better results for these tasks using the proposed life long learning mechanisms, compared to fine tuning, prompt tuning, and their combinations with regularization based life long learning methods such as elastic weight consolidation (EWC) and memory aware synapses (MAS). The discussion on few shot learning also clearly demonstrated the difference of the proposed methods and a major collection of prior work using prompt tuning. In particular, the paper should include an ablation study on a range of \lambda_{KL}, not just a comparison between models with and without L^{KL}. 2.Some statistics that are useful for understanding the propose methodology are missing in the experimental section: 1) it is probably beneficial to include some numbers on the state of the art results on methods for the original tasks and datasets. They may not be fair statistics in a life long scenario, but they are helpful for the readers to understand where the methods stand in a larger picture of machine learning methods.<|endoftext|>This paper proposes to leverage prompt tuning on large pre trained language models to achieve Lifelong Few shot Language Learning (LFLL). Strengths:    Combining lifelong and few shot learning is a new setting. Weaknesses:    The actual method is simple combination of existing ideas. The number of tasks and domains is minimal setting. Perhaps adding more pre trained LMs such as GPT 2 and different sizes of T 5. Maybe adding another ablation on this would be a good idea. The proposed LFLL problem, in my opinion, is very important for the community to think about: how to adapt large scale pre trained LM to different tasks/domains with few samples? I think this work has potential, and if the authors can try to add more tasks/datasets/baselines, it may create more impact to the community.<|endoftext|>This paper explores lifelong few shot language learning. They use pseudo data for new domains, and additional prompt embeddings for new tasks. Strengths  Few shot setting and lifelong learning are important settings. I did not understand why the proposed method is restricted to the few shot setting. I am missing why this idea of retraining on previous domains each time is a good one. Let s say we re up to 1000 tasks. The negative transfer point needs some more evidence as well, as I believe the decaNLP challenge showed positive transfer, right? Minor comments  Is the finetuned BERT Large a few shot finetuned or with the full dataset? I also struggled to understand why the proposed method solves lifelong learning over regular prompt tuning. One sentence in the paper seems like an overclaim to me.<|endoftext|>The paper defines the problem of lifelong few shot language learning (LFLL) where the goal is to continuously learn a model with new few shot tasks without forgetting the previous ones. I am looking forward to the author s response in the rebuttal period and will consider updating my scores accordingly. With the rise of larger models, it is important to consider the scalability of these models for lifelong learning. Empirical results showcase that the proposed approach is superior to existing methods like EWC and MAS when evaluated on the sequence of 2 4 domains/tasks. Is there a consensus that prompt tuning based methods are better than adapters for few shot learning? However, the paper evaluates the proposed approach on the sequence of 2 4 tasks.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 10; In this paper, (1) The NeRF is used to produce a low resolution feature map and upsample it progressively to high resolution. Also, this method adopts rendering at interactive rates. (4) There is an interesting phenomenon where the pupil position in the human eyes of the synthesized images (Figure 6 and 15)  are different so that the human eyes seem to toward the direction of the camera observation, while the pupil position in the human eyes of the synthesized images (Figure 12 and 13)  are nearly same.<|endoftext|>With the great success that has been achieved by StyleGAN in 2D, it is natural to extend the key idea behind StyleGAN to 3D for a 3D aware generative model. 2. the proposed method produces SOTA results, the generated images are impressive. 3. the proposed method is fast and supports high resolution renderings. 3.3D aware generative models have a focus on multi view consistency, in contrast to 2D GANs. The proposed pipeline is a good workaround but this compromise is absolutely nonnegligible, it does not fundamentally solve the key issue existing in this 3D aware generative method. It seems to work very well in the paper.<|endoftext|>The paper presented StyleNeRF, a 3D aware generative model for high resolution image synthesis with high multi view consistency. It also presents a new upsampling module and a new regularization loss to enforce 3D consistency. Strengths:+ The proposed method is able to synthesize high resolution images at interactive rates while preserving 3D consistency at high quality. This is not consistent with the results in the original paper. It achieves very interesting results.<|endoftext|>In this manuscript, authors proposed a novel 3D aware generative model for photo realistic high resolution image synthesis. Weakness:(1) The proposed method remove viewing direction from the NeRF to avoid ambiguity learned from single images, which is fine, but strictly speaking it’s no longer a “radiance field”, but instead a “irradiance fields”. Based on the strength and weakness I mentioned above, I strongly recommend this paper to be accepted because its novelty on closing the gap between 3D GAN and 2D GAN problem and this is the first 3D generative view synthesis work that can render high resolution images with interactive speed while preserving very high rendering quality.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; For example, in the transition from (39) to (40), the authors justify the change of $\sin\theta_t$ to $\tan\theta_t$ by mentioning that $\theta_t$ is small. + The empirical findings are nice to see. Weaknesses  I perused some of the proofs of the theoretical results.<|endoftext|>The paper gives theoretical results to show the connection between the proposed surrogate gap and the sharpness. Convergence rates and generalization bounds are also presented. Then the convergence bound would be very crude since $L_p$ is very large, which may not apply well in practice. The proof based on these inequalities are therefore not rigorous. The proof of Thm 5.3 is also not rigorous since the authors use several inaccurate inequalities, e.g.($\approx$ in (70) and (71))  In Corollary 5.2.1, the authors tried to show that GSAM can find a smoother minimizer by involving the surrogate gap in the generalization bound. It does not show the benefit of a small surrogate gap in achieving a better generalization, which is a claim of the paper.<|endoftext|>* Overall, I am not sure whether the paper really needs to include the theoretical results presented in Section 5.2. Theoretical results (generalization bounds and convergence theorems) supporting the algorithm and new loss function3. Overall, the work proposes a simple and elegant method, which is well motivated and gives state of the art performance in an extensive numerical evaluation. The paper argues that minimizing the loss function used in SAM (which replaces the loss by its maximum in a neighbourhood) does not always lead to flat minima.<|endoftext|>It is a good submission that provides an interesting way to improve the updating rule of SAM. Although the authors alert the potential caveat for optimizing $f_p + \lambda h$, the proposed GSAM still aims at minimizing this objective.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; 4) The writing of the paper is quite bad and not polished. It is not clearly stated.<|endoftext|>Having said that I think the paper is still premature in terms of both technical and empirical novelty. The technical contributions are incremental and its performance is not validated against state of the art methods.<|endoftext|>Is this discussed in the paper? What is the novelty of the approach? The results are rather limited and do not seem to show a clear advantage over standard techniques.<|endoftext|>My main concern is the lack of relevant benchmarks. Considering the mentioned paper, I do not think the paper at the current shape is ready for acceptance at ICLR.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper studies a networked MARL problem based on the model in [Zhang et al 2018], where each agent can observe the global state, take local action and observe local rewards. The key difference in setting from [Zhang et al 2018] is that [Zhang et al 2018] assume the global action can be observed, but in this paper, only local action is known to each agent. Further, compared to [Zhang et al 2018], a finite time error bound is provided. Finite time convergence is provided which matches those bounds in single agent AC. For example, how does the batch algorithm compare with the “non batch” algorithm, where one update is made after each sample?<|endoftext|>This paper considers cooperative multi agent reinforcement learning (MARL) for average reward MDPs with fully decentralized actor critic methods. The authors then establish a finite sample bound for the proposed algorithm in terms of convergence to stationary points under linear value function approximation. However, there also are some issues in terms of the assumptions, comparisons with the global convergence of actor critic methods literature and numerical experiments that need to be addressed. 3.I have some questions for the numerical experiments.<|endoftext|>The paper studies the finite time convergence and sample complexity of MARL with average reward, focusing on actor critic algorithm. It seems that that log(N/\epsilon) is enough. It would be helpful to provide an example for it.. There is a typo in Assumption 5. Can the authors provide some intuition? Intuitively, they should also give reasonable performance.<|endoftext|>This paper establishes the first finite time convergence result of the actor critic algorithm for fully decentralized multi agent reinforcement learning (MARL) problems with average reward. The established finite sample complexity matches that of the state of the art single agent actor critic algorithms. The paper has made valid contribution to the area of multi agent RL, and is well written. The detailed comments are as follows:1.<|endoftext|>The theoretical results are important. Its theoretical contribution on the first finite time convergence result (to the stationary point) for fully decentralized MARL in the average reward setting is important. However, I have some comments and questions for the authors, which are listed below. This paper proposes a new actor critic algorithm to deal with a cooperative average reward fully decentralized MARL and provides the first finite time convergence result.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This work presented an adversarial collaborative learning named ADCOL for federated learning on non IID features. Strengths:(1) It proposed a novel adversarial collaborative learning for federated learning on non IID features. (2) It provided the convergence state of the training process. 2020.(2) One major concern of this method is to the privacy issue. It suffers much more seriously from the privacy leakage by sharing the hidden representations, compared to sharing only parameters in FedAvg. The communication cost also significantly increases due to the shared representations. The novelty of the proposed model is incremental, and the privacy issue of sharing features is not solved.<|endoftext|>This paper focuses on federated learning on non IID features. While the local parties train the local models and expect the representations not to be distinguished by the discriminator. 2.The experimental evaluation seems to be convincing. It may lead to more serious privacy breaches. It needs more work to protect the local data. 2.The advantage of communication size is not persuasive. The statements in this paper are not complete, especially in the experiment sections. There is a lot of confusion here, such as the statics of all the datasets,  the method for averaging results from different data sources (arithmetic average or weighted average according to data volume)  and methods for adjusting consistent label distribution but non IID feature distribution, and the final distribution of label and feature. While there are also some issues that need further refinement to be accepted, especially the statements in the experiment.<|endoftext|>To address the heterogeneity problem,  it proposes a federated learning scheme called ADCOL based on adversarial learning. In ADCOL, the devices transfer local representations to the server while sending the discriminator to the devices. The experimental results show that the proposed method has some advantages over several baselines. This paper studies an important problem   the non iid features problem in federated learning. The authors have some good ideas to tackle the problem. The authors might want to compare the proposed method with FedUFO. In addition, the number of the selected devices might also be different. While this paper has some nice ideas to address the non iid feature problems in federated learning, its contributions are limited due to novelty and practicability issues.<|endoftext|>The paper proposes an adversarial training method for learning from non iid data in federated learning. Both a theoretical and an experimental analysis of the method are provided. Similarly, privacy can be seen as an issue. Intuitively, it should be easier to infer information about the original data with so much information, rather than just with sharing (one) locally updated model at every round. However, expanding the discussion on that will be beneficial. The conclusion (and in some sense the comparison to standard FL throughout the paper) seems to suggest that the method is more computationally efficient than other FL methods. Related to the last point, perhaps the authors could comment on the possibility of communicating the representations of only a subset of the local data of each client at every round? Overall, this is a very well written paper that studies an important topic and proposes an interesting new idea.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; The idea of using RED for estimating the joint density for consecutive states for imitation learning from observation is interesting, and the overall structure of the paper is clear and easy to follow. I am not sure what are taking maximum over since $\pi_a$ should be fixed in this situation as a input of $J(\pi_a)$. Even in this situation the proposed method is outperformed or evenly matched by the baselines. State only imitation with transition dynamics mismatch. Some mathematical representations still need some improvement. I would recommend a weak reject for the paper.<|endoftext|>This paper tackles imitation learning from observations under model mismatch. If the authors can comment on and clarify this. ### Summary++:The setup for this paper is super interesting. However, I am unconvinced by the method itself. The method is quite involved, with lots of moving parts. Section one motivates the problem well, but doesn’t provide much detail on what content will be provided in the paper. The empirical validation presented is lacking. 4.2   I think the following “baselines” are missing:    SAC/RL on the learner (also to provide the true upper bound on performance of the learner). 2   The method itself is also _very_ complicated.<|endoftext|>This paper tackles the problem of imitation learning from observations only (no access to expert actions), in the setting where expert and agent are in different MDPs. More precisely, transition dynamics are different. The approach is quite innovative  The problem they tackle is very interesting, as in realistic scenarios, their will usually be such dynamics mismatch. It is still an interesting contributions, and empirical results are promising. Upon such clarifications to be made (along with potentially visual examples providing intuitions), I would be considering raising my score to a weak accept.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; * Sehwag 2021 is missing a venue entirely. The problem identified by the authors makes sense and solving it by finding useful unconcentrated representations is reasonable and could be of some general interest outside of deep AD as well. Small novel contribution with the vBM distribution approach* __Experimental Results:__ Results strongly indicate that the proposed method does improve the representation for AD. I think the non OE non transfer learning based methods are interesting in their own right, but I think these other approaches/results should at least be mentioned. * __Quite a few style and grammar errors:__ At end. __   Verdict   :__ As is the paper is just a bit worse than what I would expect out of ICLR, however there is a lot of easy room for improvement. Below are some examples  * There are several citations that reference arXiv version despite the existence of peer reviewed and/or archival versions.<|endoftext|>Specifically, the paper pointed out the issue of contrastive learning for OOD detection when trained longer as their embedding distributions become more uniform, and proposed an architectural modification by adding a L2 normalization layer after encoder. For models without normalization, I wonder whether authors use L2 normalized encoder output or unnormalized ones. I would be a bit careful when saying "Feature ensembling does not require a validation set". Feature ensembling could be effective in practice, but it makes comparison to other methods somewhat convoluted, especially when readers want to understand the source of improvement. The technical contribution of the paper is very simple (adding few L2 normalization layers).<|endoftext|>This work explores novel architectural modifications on SimSiam (Chen & He, 2020) to the self supervised feature learning step, that facilitates compact in distribution (ID) distributions to be learned on out of distribution (OOD) detection. In addition, the paper investigates how the geometrical compactness of the ID feature distribution makes isolating and detecting outliers easier. The authors claim that no uniformity and compactness of the learned ID distribution is the main reason to improve the performance on OOD. This confuses me a lot. Therefore, I vote this paper marginally below the acceptance threshold. Given that I am not very familiar with OOD, I would like to check other reviewers  opinions.<|endoftext|>The paper contributes to the field of self supervised anomaly detection by i) verifying the correlation between AD performance and in distirbution (ID) representation uniformity and how to improve it with a simple normalization; ii) introducing a simple solution to a gradient flow issue in previous work; and iii) proposing a multi level feature ensembling as a cheaper but quantitatively effective alternative to test time data augmentation and model ensambling. + (+) The paper introduces several insightful contributions: e.g.AD correlation with distribution uniformity measured by fitting a vMF distribution or by MMD; or solution to the gradient flow issue in SimSiam. MINOR+ typos: ood evaluation  > OOD evaluation (Sec 1, page 1); to actually to decrease  > to actually decrease (Sec.2.3 page 5); + Table B is references but it does not exists in the paper nor in the appendix. On one hand the paper has good contributions, provides insights into the problem that are supported by empirical evidence; on the other hand is poorly organized and written: most of the key results are supported in the appendix, which should be optional   not critical   to the reading of the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; 5.Eq.(6), there is no y(.) Therefore, the multiple control agents can be considered as one single agent. Due to causality,  I think it should only depend on partial control agents which are valid after time u. The authors propose  a new method for learning dynamics from continuous time data using neural Markov controlled SDE. It connects stochastic optimal control theory and deep learning which is pretty novel in my opinion. Also in many real applications, time series data come with covariates which may be very predictive for target variables. State of the art algorithms for time series prediction like DeepAR, NBeats, Prophet and Transformer, to name few, take some of the factors into account in the modeling process. It would be interesting to see discussions on how the proposed method handles these factors.<|endoftext|>This paper presents a framework for learning a stochastic dynamics from observed trajectories. The goal, as in works using the neural SDE framework, is to learn stochastic time series data using this model. Using a stochastic optimal control formulation, the authors use this notion of temporal privacy to decompose the optimization into sub problems which allows for separate optimization of each agent. The authors carry out several numerical experiments and achieve impressive results. In all cases the CSDE TP significantly outperforms alternative strategies. can this notion of incoherence be made more precise? Perhaps it is better to simply say "information" The author introduce an original and highly effective optimization scheme for controlled SDEs for modeling stochastic dynamics. While there is not a thorough analysis of the algorithm (aside from a discussion of the theoretical optimality in the appendix) the experiments amply demonstrate that the approach is productive.<|endoftext|>This paper proposes to model a time series directly with a neural controlled stochastic differential equation with multiple control agents. Then the authors introduce Markov dynamic programming to efficiently minimize a loss function defined in terms of trajectory of the stochastic differential equations. The paper seems to be well structured and the math seems solid. The proposed loss function MFcond is a principled way to incorporate the intermediate observations into the dynamics. I have a few comments and concerns:   The motivation behind modelling a time series such as stock price as controlled by multiple agent is not clear. I think scatter plot is more appropriate since the observations are at discrete time. This paper proposes some novel ideas that works well in practice.<|endoftext|>This paper presents a new approach to train stochastic dynamical systems using a set of tools from stochastic optimal control theory. Strengths:+ The paper concerns a timely topic. The stochastic view of the continuous time systems makes it even more interesting as ordinary differential equations based approaches dominate the recent literature for the time being. In turn, this would allow us to appreciate the methodology and different choices the authors have made. NCDE should be mentioned in A1 and the proposed method should be motivated in comparison with NCDE. The paper brings several interesting approaches to address the sub problems that appear due to the construction of the SDE controlled by multiple agents. Is not this an issue after all? Although the method is interesting and timely, I recommend a reject and re submission after the above mentioned points are addressed.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper showcases the effectiveness of a recently introduced reservoir of random features, i.e., randomized signatures. S3: The experimental results provide evidence of the effectiveness of the presented strategy. The methods of reservoir computing and random signature are mature components. W3: Although the signature is well theoretically guaranteed, it is not clarified how these guarantees can contribute to the problem of time series classification in comparison with the original Rocket where the randomized convolution kernels are used. As the main contribution of this paper is to showcase the effectiveness of the randomized signature layer, it’s expected that its performance on more representative tasks are evaluated and reported.<|endoftext|>Using Signature theory for time series analysis is a promising direction with some theoretical guarantees. The paper however is not self contained and some details are lacking. Experiments on time series classification also show its potential. S4: For time series classification, Randomized Signature when combined with Rocket (Dempster et al., 2020) gives better performance than existing related deep learning methods. There seems also no explanation about this in Section 4. This will help to better understand impact of $d$ on the performance.<|endoftext|>The authors demonstrate the utility of this randomised methodology on a variety of tasks including learning rough differential equations and standard time series classification benchmarks. The result of this is a number of methods that do not scale well to tasks with large numbers of feature channels. This paper would represent the best reference to the idea of "randomised signature" features if accepted.<|endoftext|>Section 3.2 paragraph 1: "problem, in particular kernelization techniques, see, e.g., Kidger and Lyons (2020)" This paper does not propose kernelization techniques. This is demonstrated on some toy problems as well as some real world datasetsThe manuscript builds on previous work that has showed that the signature is a reservoir by demonstrating the performance of the randomized signature. While the breadth of experiments is impressive, there is a lack of comparison with other signature based baselines such as neural CDE or RDE methods or low rank projections.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a scheme for partially sharing CNN parameter across layers. As the paper makes use of the same factorization as DCFNet [Qui et al., 2018], I would expect an experimental comparison between DCFNet and the proposed approached. The paper also cites [Li et al., 2019] as another previous work that uses a filter basis factorization. VGG vs Wide ResNet is a significant change in the base network architecture, which is a confound for the question of which filter parameterization is better; each method could be applied to either base architecture and the comparison should be made by choosing the same base architecture. This paper proposes a cross layer parameter sharing scheme for CNNs that demonstrates some benefits on image classification tasks. However, the proposed method is similar to filter factorization approaches in several previous published works, for which experimental comparison is missing or incomplete.<|endoftext|>Notably by sharing A across all layers, sharing A across groups of layers and by utilising the idea of filter groups. Further, they show that their method can alleviate issues of model degradation as deeper networks are used. This is especially true for the filter groups networks as the authors show a significant reduction in parameters and improvement in model performance (Table 1). This is an important section that needs to be reworked. * The idea to share filter basis  across layers is not entirely new some and _shares_ similarites to https://arxiv.org/pdf/2006.05066.pdf. (Deeply Shared Filter Bases for Parameter Efficient Convolutional Neural Networks). In that paper, they also learn a filter basis that can be shared across layers despite a different methodology. * The decomposition method that is based on Qiu et al.2018 is presented in Section 2.1. Why not just use Qiu et al? Despite this, it is to my feeling that the idea is not entirely novel as i) low rank decomposition of convolution kernels has already been performed and ii) the idea to share filter basis/weights across layers has also been proposed.<|endoftext|>This work first shows empirical observations that there exist obvious correlations in features across layers within a CNN after proper linear transformations. (3)	The paper is clear written. For example, the preceding layers or blocks extract the fundamental cues (e.g., color or boundaries), while the latter layers or blocks learn the high level semantic features. So, could the authors give the guideline or develop an adaptive method? Could the authors give more detailed discussion? Furthermore, the original backbone models with only groups of filters could be compared as baselines for verifying the effectiveness of ACDC g. How about the performance of ACDC g on ImageNet? (3)	The authors claim that the proposed method can provide better interpretation for CNN. However, it seems not very clear for me. Therefore, do the proposed method adopt to 3D convolutions (spatial and temporal information are coupled)? However, there exist some weaknesses in this work (see main comments above). Therefore, my current rating is borderline accept.<|endoftext|>Since the important features across the layers in deep CNN is maintained, the filters can be decomposed into shared coefficients and layer specific coefficients. This method is easily compatible to modern CNN architectures. By suggesting motivating experiment, the method of the paper could be verified theoretically as well as experimentally. 2.The method is simple but very strong. Since there are additional methods to be combined with the proposed such as block (section 2.3.) Cons1.There is a lack of reasons for using maximum value of C_in and C_out in the section 2.3. If you already have ablation study on that, it would be better to have in text, somewhere. If the authors provide additional description of the figure, it will be clearer. The description of third paragraph of section 3.1 has no referred table or figure. (The former is likely results about your methods and the latter seems to be an analysis about table 2.) However, the novelty of the proposed method is fully convinced with supported experiments and their explanations.<|endoftext|>Depending on the low rank structure observation, this paper proposes to jointly model filter subspace across different layers by enforcing cross layer shared subspace coefficients. The key idea is that all convolutional filters are decomposed into a shared coefficient submatrix and special filter atoms. Concerns:   The novelty of this paper is weak. However, this novelty is not enough for the standard of ICLR. I am sincerely hoping that the authors deeply think more important problems in the topic of speeding up deep networks. [ref 1] Speeding up Convolutional Neural Networks with Low Rank Expansions, BMVC 2014.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper presents a technique, spread spurious attribute, for inferring the group annotation for training samples in a dataset. The inferred group information is then used as part of a group DRO minimization scheme or some other worst case scheme. This new dataset is then used as part of a new DRO pipeline. ### Strengths  Reducing the amount of annotations required worst group loss minimization is an important problem, and this paper presents a simple procedure to help with that problem. Why can t the authors check the accuracy of their SSA scheme for a dataset where we have the ground truth spurious attributes like the waterbirds and the other datasets that they use?<|endoftext|>The proposed method leverages samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then uses the pseudo attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst group loss.<|endoftext|>This paper studies the group robustness problem in the setting of spurious correlations, when only a small number of samples have spurious attribute annotations. They present a pseudolabeling algorithm based on FixMatch to pseudolabel the remaining examples, and then use worst case loss minimization algorithms such as Group DRO to train a more robust model. The authors should instead clarify that some training examples have attribute annotations and some do not, and similarly for the validation set. Computational cost / runtime results are not provided.<|endoftext|>This work focuses on the worst group optimization (distributional robust optimization) problem with few spurious attributes are available as a validation set. They don t have spurious attributes anyway and the method only infers them on training data. "Environment inference for invariant learning." The performance of such models are very sensitive to these hyperparameters/validation set, and therefore, fail to perform comparably with methods that use the spurious attribute. SSA first does pseudo labeling and then does robust training. In experiments, it uses the datasets widely used in this field: Waterbirds, CelebA, MultiNLI and CivilComments WILDS. They show SSA improve the worst group accuracy significantly over existing methods without spurious attribute even with much less valiation data.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The paper proposes a Conceptual Counterfactual Explanation for explaining a model s mistakes. Then, it explains a model s mistake by finding a change in those concepts that changes the model s prediction, does not change too many concepts, and does not change the concepts in an unrealistic way. It makes the same assumption as the proposed method (ie, that we have concept labels for each image), provides its own algorithm for finding which concepts a model is relying on, and, further, provides methods for fixing the model when spurious concepts are identified. Running experiments that start by analyzing the entire set of misclassified images would be more meaningful. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Specifically, the analysis should not start with the assumption that a set of "OOD images" has been identified and the experiments should include more concepts that are relevant to the predictive task. At this point, I think that the setup, results, and analysis for the settings where the model is trained on a deliberately biased distribution are convincing. As a result, I ve increased my score to a 5. However, I am still concerned that using CCE in a less controlled setting will be difficult due to "false positives" for relevant concepts:   These appear in Figure 4 in the form of "water" and "ashcan".<|endoftext|>Is there a steep drop after 3, or is 3 motivated by the desire to provide a user with a more manageable number of possible concepts? The authors present a method combining two prior methods in explainability: counterfactual explanation and concept activation vectors, and meld them together as coneptual counterfactual explanations. While there have been several proposed lines of work in this area, none is yet accepted as standard practice. Strengths:  The paper is clearly written, and the figures are demonstrative and clear. Update: In response to the authors  extensive revisions to address my concerns (and those of the other reviewers), I have increased my score. In section 4.1, the experiments to identify spurious correlations by CCE a good beginning, but the interventions on the MetaDataset images are large drastic effects. Figure 1c is not a particularly good example of explaining a model mistake. The end of section 3.1 suggests concepts are validated by a hold out set, where those that do not meat a threshold on accuracy are discarded. It is defined as $d_i   \mathbf{c}_{i}b_{L}(\mathbf{x}_i)^T$. This seems to use $i$ to index both concepts and datapoints in the training set for concepts.<|endoftext|>In its current form, I think the paper is borderline accept*, however, if the authors can provide answers to some of the weaknesses I presented, I am more than willing to move the score up to an accept as I think the ideas and the problem are extremely interesting. The perturbation is the addition of a linear combination of concepts by different amounts. Lack of baselines in controlled evaluation (sections 4.1 to 4.3): consider the following baseline, take the test example, find the closest correctly classified example to it in a validation set (that has same distribution as training data), and subtract the concept vectors for the test image   closest image in validation. Then show the top 3 concepts of the difference. Lack of ability to visualize perturbed test examples. What happens if the mistake cannot be attributed to a concept? Does the method say that none of the concepts explain the mistake or does it falsely attribute it to something else? One can do this evaluation by removing e.g.“snow” from the concept bank and running the same experiment. After author response:  I have increased my score as the authors have raised some of my concerns, particularly on when the concept is not in the libraryThe paper provides a novel way to explain a classifier s mistakes on a test example when given it’s label in terms of a known set of concepts.<|endoftext|>The authors propose a method for computing concept based counterfactual explanations   i.e.using the presence or absence of concepts (human understandable) for explaining the models prediction. In the paper they focus on image classifiers. I understand that an automatic evaluation is challenging and I also appreciate that the authors are aware if this and suggest an user study   however, still I think that this manual inspection as an evaluation is a major limitation of this work. While the examples in the paper look convincing, I was wondering whether it is possible to come up with formal guarantees   i.e.are there any formal guarantees that the final explanation is valid? Overall an okay paper with some very interesting ideas, although I have some concerns regarding the evaluation of the method (see main review).
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Strengths:The paper is well written and high level ideas on the theory are explained. The paper mainly provides intuition for Algo 1 without solid theoretical foundations. The novelty is mainly to have parametrized the functions in the objective of the JKO by neural networks. Then, when the objective function is a f divergence, the objective inside the JKO admit a variational representation and can be expressed as a sup.<|endoftext|>This paper proposes a variational formulation of each JKO step for optimizing functionals on measures. The experiments are not convincing enough to demonstrate the practical advantages of the proposed method compared to the alternatives.<|endoftext|>The paper proposes a method to compute Wasserstein Gradient Flows (WGFs) via neural networks and the JKO scheme. ***Relation to prior work. **Scope.** If I correctly understand, the authors do not provide any high dimensional applications of WGFs. Adding an application would definitely benefit the paper. It is unfair to ignore this and compare in the experimental section only with [2] using and only by using the direct computation. Second, the variational approximation of the f divergence is not novel, see, for example, f GANs [5]. I encourage the authors to include a detailed discussion of this.<|endoftext|>The paper provides an interesting variation on recent JKO based methods for computational Wasserstein gradient flows, but its limited novelty and empirical evaluation diminish its contribution, and make it a borderline paper in my view. Relying on known reformulations of the JKO scheme as optimization over convex functions, the paper departs from recent related methods in expressing certain objectives as f divergences, and in turn using the dual formulation of these divergences to circumvent the need to do explicit density computation in these. * Limited experimental validation, especially with regards to high dimensional settings, which is arguably the main promise of this work. This paper proposes a method to solve Wasserstein gradient flows based on the JKO scheme using variational formulations of functional objectives, such as the KL divergence or the generalized entropy (non linear diffusion).
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper proposes a general purpose architecture(Perceiver IO) which can take any arbitrary type of inputs and produce arbitrary outputs. The architecture enables this while scaling linearly with input embedding size and output size. Perceiver IO performs comparably with SOTA results on a variety of tasks from language, vision and multimodal domains, which speaks about the generalizability of this architecture. This enables supporting different types of output and scales well to different tasks. Byte level performance of Perceiver IO is impressive compared to BERT baseline. Shows good results with multitask learning, using both single and task specific tokens. #### Weaknesses  Novelty over Perceiver : Although I find this work quite impressive as it scales linearly with output embedding sizes as well as arbitrary types of outputs, the overall architecture seems incremental compared to Perceiver. How does choosing values for N,D affect the performance on various tasks.<|endoftext|>This paper proposes a general purpose neural network architecture named Perceiver IO. I wonder what is the opinion by the authors what is the source of the improvements. (this is not a mandatory experiment for the rebuttal)## Minor comments  Many details are missing in the main text. The main strength of Perceiver IO is its high flexibility to various input domains. Although I have a small concerns that ImageNet results (a reader can mislead the results) I think this paper is a good paper and indeed recommend acceptance. The authors addressed my concerns very well in the rebuttal. I have minor concerns about ImageNet experiments.<|endoftext|>In this paper, the authors proposed a new general architecture called Perceiver IO for various tasks with different types of inputs and outputs. 3.Perceiver IO is proposed as a generic architecture for various tasks by modeling them as a read process write process. This paper does show some encouraging results on various tasks compared with the baselines. However, they are only compared with baseline methods and still underperforms established methods in specific domains. In this paper, the authors demonstrated the effectiveness of Perceiver IO across different settings, I am wondering whether such a generic architecture can be used for multi task training so that it can leverage the training data from different tasks. Overall, I like the idea of Perceiver IO considering it is a good way of modeling arbitrary numbers of input and output tokens. The experiments are solid enough to justify the main claim. [Post rebuttal comments]After reading the authors  feedbacks to all reviewers including myself, I think they had addressed most of the concerns and questions.<|endoftext|>Different from prior work, perceiver IO directly operates in the raw input space   UTF 8 bytes for language, xy coordinates in optical flow, raw audio, etc. #### Strength  The idea of perceiver IO is novel and solid   a general architecture capable of handling general purpose inputs and outputs across different tasks and modalities. This is very promising to simplify the construction of highly tuned task specific neural pipelines and improve the multimodal and multi task problems. The paper is well written, the idea is solid and novel, supported with massive and strong experimental results. Overall, the is a strong submission and I would recommend accepting the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; If this can be fixed, it would be a pretty good paper. I find the paper generally of high quality, but the lack of any timings is concerning. This was great to read.<|endoftext|>3.Why do the authors remove the component of the super net gradient that is conflict with the sub network gradient? This operation is not intuitive. What are the negative effects of this operation? It is highly suggested to give more intuitive explanations in Section 1. In general, the writing of this paper makes it difficult to follow the authors’ core thinking.<|endoftext|>The motivation of this work is clear, but the explanation of poor performance (gradient conflict issue) is somewhat  not convincible. The architecture in the search space consists of MBConv and transformer, it is better to compare with the existing works that also combine the CNN and transformers, like PiT, etc. It is a good work with clear motivation and  promising performance, the reason of poor performance by directly applying the supernet based NAS to optimize ViTs need to be discussed more.<|endoftext|>Does it mean the number of parameters? Though there are still some minor flaws in the paper, I think it is a good one and is nearly ready for publication. 3.Besides gradient conflict, the authors also improve the performance by changing data augmentation.<|endoftext|>This work presents the gradient conflict issue in ViT training, i.e., the gradients of sub networks conflict with that of the supernet, leads to inferior performance of ViT supernet training. The authors fix this issue by 1) a gradient project method to prioritize the sub network update; 2) Use switchable layers to increase the model capacities of sub networks; 3) Simplify the training recipe. I don’t see the relationship between it and the gradient conflict issue, also the improvement in Table 7 looks marginal. There is performance improvement shown in Table 7, but it is not clear whether this is caused by the conflict between supernet and sub networks. Detailed information should be presented. The motivation of this paper is not very convincing, need to provide more evidence to prove.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; 2.In the introduction, the authors claim that one shot object detection problem can be decomposed into three subproblems. The paper conducts many experiments to illustrate the impact of different factors on the performance of base classes and novel classes, including the number of training categories, the number of samples, the model capacity and the training time, etc. 3.The paper is well written. Moreover, a high diversity dataset has been proposed and corresponding experiments have been carried to verify the importance of the number of categories. Results show that the performance on VOC novel classes is worse than that when using base classes in PASCAL dataset, even COCO has more categories. 3.The contribution of the paper is limited.<|endoftext|>This paper studies one shot 2D object detections. Inspired by this observation, this paper improves the state of the art one shot detection performance on COCO from 22.0 to 27.5 AP50 by training on LIVS. Other two related conclusions in this paper:  PASCAL VOC is not suitable for evaluating one shot object detection algorithms. Only when the training data is challenging enough, increasing the model size and training time can help improve one shot performance. The paper designs a smart experiment to demonstrate that PASCAL VOC is too easy/biased for evaluating one shot object detection algorithms. Like in the introduction:  This generalization gap can be almost closed by increasing the number of categories used for training. Mainly because of the limited contribution.<|endoftext|>  The paper considers the problem of one shot object detection, meaning, the model is asked to detect unseen categories, based on only one provided a template. The main discovery made by the authors is that, to generalise better, the model should be trained with data from as many categories as possible, given the same budget on number of samples for training. Strength      The paper is generally well written and well motivated. Weakness:      The contribution of the paper is limited. The paper simply raises the discovery that training more categories can be beneficial for generalisation on one shot detection, which is probably not that surprising, given the architecture used is based on matching, and more importantly, while increasing the number of training categories, it becomes more likely that the training and held out categories are subcategories of one root. Overall, I think the discovery is well made, it shines light on one perspective, that is, using more diverse training data can be beneficial for generalisation,  however, it misses the other element on, how can we further boost the generalisation from an architecture or model design perspective.<|endoftext|>This paper studies the effect of category number in the one shot object detection task. It is claimed that this performance gap can be largely closed by increasing the number of categories used for training. Experiments are conducted on VOC, COCO, Object365, and LVIS using a Siamese style detector to verify the claims. # Strengths:  State of the art performance is achieved on COCO one shot detection benchmark by training the detector on LVIS. This paper is well written. The presentation is easy to follow. As the category number increases, the train categories have higher probability to contain more categories semantically related to the held out categories. With more semantically related categories to train on, it is reasonable that the detector can have better performance on held out.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes an approach to fixing synthetic python bugs by using the backtranslation model. However, the novelty in the learning based auto program repair is quite lacking. 2) Good Result on QuixBug and outperform baseline by significant results3) Interesting idea of using the skeleton around the function. Weakness 1) I would argue that the novelty in the paper is quite lacking.<|endoftext|>In the paper, the authors propose DeepDebug, a Transformer based approach for automated program debugging. Pros:+ The idea of data augmentation with back translation is interesting although not new. Fine tuning DeepDebug on that dataset seems straightforward to compare to those models. Do you have a rough estimate of the percentage of the Github code that is bug free?<|endoftext|>Authors also propose including context such as import statements, class declarations, and docstrings to improve performance of the debugging model. It is possible that the authors  approach for such bug generation is better than what others have done, but this is not demonstrated. Limited comparison to other bug fixing tools. Authors present a bug fixing approach using tuned transformer models. They additionally train the model on stack traces with errors caused by synthetic bugs. All this results in improved bug fixing model.<|endoftext|>This manuscript describes DeepDebug a transformer based mode that performs code repair on Python methods. They interact not only among themselves but also with other libraries (e.g., packages from PyPI) in complex ways. In summary, this paper presents some potentially state of the art performance on neural code repair. and the tests available in open source for examination and reproduction? However, to me the author didn t define false positives clearly. 4.2 that the data is "extremely noisy".
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; Unfortunately, I don’t think this paper fully realizes its potential. It could be clearer and more focused; it reads like it was written quickly. Many of the examples don’t feel sufficiently explained, and some don’t feel well integrated. I assume it should “Fig.3”.The paper is well motivated and attempts to fulfill a critical need in the field, but feels a bit rushed and incomplete in its current form. There is at least one company—[MosaicML](www.mosaicml.com)—founded with the goal of improving the efficiency of training deep learning models.<|endoftext|>There is little research insight beyond this, which, given the “survey like” nature of the submission, is acceptable. It lists a fairly broad set of cost indicators in Section 2, then focuses on demonstrating issues with the most common ones in the subsequent sections. While the insights provided in this submission may not be novel, this is the first paper to provide a comprehensive argumentation of advantages and disadvantages of cost indicators.<|endoftext|>It would be interesting if such examples were taken from current papers that draw wrong conclusions on model efficiency. I agree with the authors that it is important to raise awareness and to point out the issues related to the adoption of only a few cost indicators. I believe that the key message of the paper is valuable to the community. It should Please specify if this is the case.<|endoftext|>If this paper is about how the existing papers have misleading reporting, then it would be nice to have some quantitative analysis (based on some statistics). The main claim is a proposed list of recommendations for reporting of efficiency of new ML models and approaches. Good paper that describes the issues in results reporting in ML/AI papers. I believe any engineer from the industry takes care of any of the aspects.
Reject; rating score: 1; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper presents a method to reduce the dimensions of resting state functional connectivity for psychiatry disorders such as autism spectrum disorder, major depressive disorder, and schizophrenia. Extensive evidence showed that these neuropsychiatric disorders share symptoms and neuroimaging features. Therefore, I believe the clinical motivation of this paper is wrong.<|endoftext|>The authors propose a novel variational autoencoder to utilize functional connectivity (FC) features from resting state fMRI (rs fMRI) scans in order to uncover latent nosological relationships between diverse yet related neuropsychiatric disorders. The methodology and main technical contributions are clearly articulated and explained. Overall, this discussion suggests that the proposed framework is somewhat limited in scope and clinical applicability.<|endoftext|>This paper presents a new conditional variational autoencoder (VAE) approach to learn a low dimensional embedding of neuropsychiatric disorders from resting state functional connectivity data. The writing in the paper needs improvement   there are many long and awkward sentences/phrasings that make it difficult sometimes to understand the authors  points. That is, across runs does the proposed method consistently produce the correct order of clusterings?<|endoftext|>This paper proposes a novel type of conditional variational auto encoder that incorporates dual utilization of diagnostic information in learning an optimal embedding space for high dimensional functional connectivity features. The paper is generally very well written, clearly presented, and a pleasure to read. The technique look sound and the contributions look solid.<|endoftext|>If not, you should apply it and please, state that clearly in the main manuscript. It is somewhat surprising that the distance between SCZ and MDD is shorter than between SCZ and ASD as often the latter two are viewed as closely related. They compare their approach on synthetic data to various related approaches and ultimately apply it to two functional connectivity datasets from patients with different psychiatric diagnoses, which appears to give good results. The paper is to a great extend clearly written.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This work aims to identify the source of transfer learning on neural models. Further analysis are meant to test of the scrambling maintains sentence semantics (it doesn t ), if BERT is just retraining (there is some transfer but inconclusive), keeping BERT frozen (it depends on the layer) and finally identifying word identity reassociation (it seems there is not reassociation). Why didn t the authors try that configuration? The analysis section can be largely improved. Despite the interesting results and large number of experiments, the inconclusive results make unclear the scientific contribution of this research piece. Authors use just BERT as pretrained model and a LSTM with GloVe embeddings as part of the experimentation. Finding are unclearThe paper is difficult to followWriting can be improvedExperiments are inconclusive.<|endoftext|>The paper studies the generalization of several architectures (BOW, GloVe, LSTM, BERT) under scrambling of vocabularies   with and without frequency constraints. The paper is well written and does a reasonable job evaluating the robustness of architectures under an artificial form of vocabulary shift. However, the authors fail to convince me why studying this distributional shift would bring new and relevant insights to the table. I think for two reasons: (a) The shift is not interesting in the sense that it is related to cross domain or cross lingual or cross time shifts, or any other shifts that we observe in the wild. (b) The shift does not seem to add value over other synthetic shifts that have already been studied, e.g., word level shuffling, character level shuffling, etc. There’s the popular alternatives (RoBERTa, GPT 2, t5, etc.), but also a lot of faster, fairer, more interpretable alternatives. (iii) As shown in related work, the GLUE tasks are relatively easy (e.g., solvable without word order information).<|endoftext|>This paper proposes an evaluation pipeline for pre trained models by testing their transferabilitywithout word identity information. Strength:1. the paper is mostly well written and easy to follow. 2. the experiments are well designed to support the claimsWeakness:2. The confounder factor   pre training, is not considered. And the reason why LSTM is more robust is that the model size is very small such that it can relearn all weights. Some claims in the paper can be further substantiated if the author can experiment with a BERT that is pre trained on scrambled English. However, as explored in Hewitt & Liang, those labeling supervisions (although in POS tagging) can be easily learned by fine tuning. 1.The method proposed in this paper is widely used in many other studies, e.g., in adversarial attacking.<|endoftext|>This paper elaborates on transfer learning and domain adaptation in language models. They used different strategies to randomizetraining data including frequency matching and completely random replacement. Their experimental results exhibit importance of pretraining on sequence labeling tasks whererandomness occurs. For example, they showed that identity of words are important and whenthey are swapped with completely different frequencies (random) the performance dropssignificantly. Overall, I think the paper is well written and debates interesting points in transferability. My concern is that these scrambling techniques might push yourdataset outside of real word adversarial attacks.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; This paper proposes a new compression algorithm for distributed deep learning where the compression results are addable). The algorithm itself seems to be a simple trick based on QSGD (and indeed), but this simple trick seems to be quite effective.<|endoftext|>The paper introduces a randomized compress to integer operator with a shared scaling factor for use in data communication in distributed SGD.<|endoftext|>* Can this approach be extended to popular optimization methods such as Adam? The authors propose a clever $\alpha$ shared across workers. There is no guard against overflow in IntSGD as well. * In Tables 1, 2, why are the communication latencies so different for IntSGD(Determ) and IntSGD (Random)?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; As far as I know, the idea is novel in this setting. The three run averages of results present large variance, and it is not very convincing to me ARPOreally achieves statistically better generalization results. Based on the results in Table 1, there is no significant improvement from ARPO compared to baselines. This paper proposes a novel idea that combines style transfer with RL to improve thegeneralization performance. The results could be better improved by adding more runs and more ablations that approved the effectiveness of ARPO.<|endoftext|>While I think the idea motivating this paper is interesting and potentially useful, I think the execution of the idea in this paper, the lack of strong or varied empirical results, the overselling of the effectiveness of the method, and the incomplete discussion of related works makes me recommend the paper is rejected. The style transfer network is trained using StarGAN, based on domains created through a Gaussian Mixture Model clustering of the observations from the policy s experience. # Strengths  The idea of using style transfer combined with adversarial perturbation of the visual input to improve generalisation in RL is interesting and novel. In the abstract it claims to outperform SOTA algorithms, but SOTA on Procgen is not PPO or RAD, but IDAAC (https://arxiv.org/abs/2102.10330). Further, RAD isn t even the SOTA method which applies data augmentation on Procgen, as that s UCB DrAC (https://arxiv.org/abs/2006.12862). I think the language around the comparisons to baselines should be made less grand and more accurate. # Suggestions for improvements.<|endoftext|>To achieve this, the paper proposed first clustering the state space into $n$ clusters/domains and use StarGAN to style transfer from one cluster to another. To learn robust features for decision making, the generator of the StarGAN is encouraged to style transfer the state space such that the action distribution under the new transferred state will be distinct in terms of KL divergence with the original policy. The generator is further trained with a cyclic consistency loss. Experiments are done on the Procgen environments in comparison to PPO and RAD(data augmentation). Ablation study is done with respect to different linear coefficients, combining the different losses in the adversarial learning. Experimental results look promising. Simply pool all state observations and do the clustering? (6)?I like the innovative point that policy is not only learned to optimize return but also robustness against style transfer of state observation. Compared to data augmentation methods, the proposed methods showed promising methodology and empirical advantage.<|endoftext|>Is there any intuition as to why? The general idea of this paper is to train a GAN like architecture to increase the robustness of RL algorithms to shifts in the input distribution. This is a good idea and the authors explain it very clearly in section 3, which I found enjoyable to read. There is always some worry that this sort of KL constraint will lead to mode collapse. This method represents an incremental improvement gained by developing a novel architecture. But of course the authors of course should receive credit for getting things to work so well. The quality of results generally ranges from acceptable to quite good, although the quality of ablations falls far behind some comparable papers and can be improved. I suppose that’s fair because this algorithm is based on a PPO style objective. Would the algorithm not benefit from keeping a buffer as in SAC?
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper proposes a variant of the transformer architecture called pyramidal attention. the ordering would stay the same I guess, because fixed set of A, C are used in all experiments. As mentioned in the summary it makes a lot of sense for time series data as it has an implicit bias of summarizing the past at different resolutions. 2.This architecture as claimed, can handle long sequences with computational complexity O(L) where $L$ is the number of elements in the sequence. Can the authors comment on this or better yet try this? 8.Generally, it would be better if we could rank pyramidal attention vs state of the art models on the well known benchmarks used in the DCRNN paper and newer papers like https://arxiv.org/abs/2103.07719. 3.I would like to verify my understanding of the multi step forecasting modules.<|endoftext|>This paper presents a new hierarchical transformer architecture with constant connection path length and linear time and space complexity for long range time series modeling. The module at core is a pyramidal attention network that makes multi resolution representations in a tree structure and perform attention operations on the tree. The extensive empirical results demonstrate the effectiveness and efficiency of the proposed method3. A proof is provided to guarantee the linear complexity of long sequence encoding## Comments1. Overall I find this paper quite interesting with great potential contribution to the community, and recommend an accept.<|endoftext|>The authors propose a new architecture to tackle the problem of long sequence temporal forecasting (LSTF) – which looks at capturing long range dependencies in time series data by providing more direct paths between the output and distant history. The paper makes a strong contribution to long sequence temporal forecasting, although there are some aspects that need to be verified before it is ready for publication. The architecture is well motivated, tackles the important problem of LSTF, and improves both forecasting performance and computational efficiency of state of the art baselines.<|endoftext|>This paper proposes Pyraformer, a low complexity pyramidal attention model for long range time series modelling and forecasting. The paper is well written and the proposed architecture is easy to understand. 2.Extensive evaluation is provided, and the proposed model consistently outperforms baseline models. 2.The paper states that Pyraformer simultaneously capture temporal dependencies of different ranges in a compact multi resolution fashion. Overall it is a solid paper.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper introduces TOGL, a new layer for Graph Neural Networks (GNN), making the GNN "aware" of topological information during this training phase. Numerical experiments show how, when topological information is relevant, TOGL helps to leverage it. # Strengths   TOGL may be a useful tool for the GNN community, while also opening the door for some further research in Topological Data Analysis (TDA). The paper is well written. Numerical results, though fairly solids, are not striking either. # Minor comments :   I think that writing $f_\theta$ instead of $f$ would improve clarity of some claims throughout the paper (it feels weird to me to write say "the map $\theta \mapsto \Psi(\mathrm{ph}(G,f))$ is differentiable", as the right hand side does not (visually) depends on $\theta$).<|endoftext|>Authors propose a topology aware layer that is compatible with GNNs and can encode connected components and cycles to let the GNN learn better representations based on such global topological features. The authors also explicitly mention the limitations that they had to impose due to computational constraints (such as l1 constraints). They also show that in practice even using limited topological features such as connected components and cycles, helps GNNs perform better. There are three questions that I would like to ask the authors:1  Diffusion based models also capture some global topological information about the graphs and it has been shown that using them in both supervised and contrastive manner significantly help GNNs. How diffusion based models are compared to your model and what if there are any theoretical links between them and your work. 2  I am curious to see how your model can contribute to contrastive learning with linear evaluation protocol as it seems it can introduce a good amount of signal. 3  I was hoping to see this method to be applied to large scale benchmarks on OGB rather than small (and almost overfitted) TUdataets. Benchmarking the model on those datasets can show the scalability and the performance of the proposed model. 4  Finally, expanding more on the theoretical aspects of the work and maybe providing examples would make the paper more eadible. The paper roots in a nice theoretical framework but shorts fall on experimental side.<|endoftext|>The paper provides new insights into topological aware GNN and I summarize the pros as follows:1. I really like the example, which is clear and easy to understand. Besides, I appreciate that the authors provide a comprehensive introduction and research roadmap in the related work section. 2.The work is technically sound and relatively novel as it introduces persistent homology, which is more powerful than the WL test from the view of topological awareness (Theorem 2), and provides a reasonable framework to integrate it. I am confused about the topological structure awareness power of GCN illustrated in Figure 1. The results are a conflict with the analysis of GIN~[1], where GCN should be less powerful. 3.The author used the DeepSets embedding function as in section 4.1 to embed different graph signals into high dimension space.<|endoftext|>The authors present a topology analysis improvement to GCN, using persistent homology, to capture global information regarding the topology of the graph. The method is well formulated and the experiments details are clear. Indeed, as discussed, the aspect of oversmoothing is important in the case of GNNs. However, for a better verification of the existence/absence of oversmoothing with the proposed method, much layers need to be stacked, and preferably more datasets (like Cora, Citeseer and Pubmed) ,should be compared against to other recent methods like GCNII [Chen et al.2020] .2.In the context of topology, which is the main contribution and scope of this work, it would be interesting to see more geometrically oriented data sets like ABC or Thingi10k (this is just an example), where objects with different topologies are given. Some of the experiments can be improved, and since topology is of high interest geometrical datasets and application, I think that some discussion should be added, and will strengthen the paper.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper considers the problem of performing posterior inference for probabilistic topic models. Theorem 3 tries to give intuition for the t 1 case, but I think could be stronger: why should f(x) have an optimal form $p( y v_1 | x)$? Experiments in Sec.5 look at toy data, with results in Fig 2 and Table 1. ## W1: Actual procedure for constructing the "recovered topic posterior" is unclearIn both synthetic and real experiments, the proposed self supervised learning (SSL) method is used to produce a "recovered topic posterior" p( w | x). The bottom of page four suggests that when t 1 and A is full rank, that "one can use the pseudoinverse of A to recover the posterior", however it seems (1) unclear what the procedure is in general and what its assumptions are, and (2) odd that the prior may not needed at all. *Can the authors clarify how to estimate the recovered topic posterior using the proposed SSL method? Overall I think the direction of the paper is interesting, but the main paper at present is missing some key pieces (esp.how the recovered topic posterior is produced once the SSL representation f is learned).<|endoftext|>This paper introduces a new method for performing inference on topic models, based on self supervised learning. The authors explore 2 types of self supervised learning objectives, one based on reconstruction and one based on contrastive learning that was explored in prior work. The authors provide proofs for their main result as well a result robust to approximate minimization of the self supervised objective. In their experiments the authors show that their approach outperforms inference with a misspecified model. What is the “traditional topic inference” used? Simply the sampling approach discussed above? I would expect to see at least qualitative results showing inference with this method on real data. Can this approach be applied within a learning algorithm for topic models? Overall the results in this work are interesting and possibly useful, but I feel that there are some lingering questions about applicability, particularly as the experimental results are underwhelming.<|endoftext|>This paper shows theoretically and empirically that self supervised objectives can be used to extract useful information about the posterior of the topic proportion vector given a document, regardless of the underlying models. It extends the findings of Tosh et al.2020 about contrastive objectives to reconstruction based objectives. It is interesting to see that one simple reconstruction objective recover the posteriors generated from the different probabilistic topic models on the synthetic dataset. It might be good to see how a neural topic model can recover the topic posterior. * The synthetic experimental results seem to say that the reconstruction based objective can recover the posterior of $w$ given a document. I wonder if the authors can discuss this more, like the possible reasons. * The experiments on the real dataset seems to show that the self supervised objectives can learn useful representation for document classification. Furthermore, compared with the other embedding method, like the language models, what is the advantage of this work?<|endoftext|>The authors not only prove that a new reconstruction based objective can also extract posterior topic information for a general topic model, but also strengthen the guarantee for contrastive objective in Tosh et al.(2020) by removing some of their assumptions and the necessity of landmark documents. It seems there is lack of outlook for future work where this analysis could be useful, and the empirical study on real data is also somewhat weak. I suggest to focus on theoretical analysis and clarify where this analysis could be useful. There is no comparison with more recent self supervised and contextual document representation work, such as BERT mentioned in the beginning of the paper.
Reject; rating score: 1; rating score: 3; rating score: 3; This paper proves a Representer Theorem for the composition of functions from Reproducing Kernel Banach Spaces (RKBSs). A Representer Theorem is also proved therein, as in [Bohn et al.2019], except that it applies to potentially infinite dimensional output Hilbert spaces  the introduction of the $\Theta_l$ is not understandable  In section 3.2 the proof is said to be given in the Appendix, but the latter is empty**The contribution is very limited**:  the Representer Theorem (RT) being mainly due to orthogonality properties, it is not surprising to recover it for a class of functions that share this property with RKHSs  I cannot see any novel idea in the proofs, so proving another RT seems not a sufficient contribution for acceptance  I cannot find any motivation for the presented results  the authors themselves acknowledge that their experimental contribution is very limitedThis submission looks more like a working draft rather than a conference paper.<|endoftext|>The paper presents a means of constructing deep (concatenated) reproducing kernel Banach space kernels. The clarity of attribution in the background section needs to be worked on. The premise is interesting.<|endoftext|>The paper proposes to use kernels from a reproducing kernel Banach space, instead of a reproducing Hilbert space in tasks such as deep kernel learning. Nonetheless, it appears that the only contributions are to notice that deep kernel learning can be understood within their framework (Sec.3.2) and to do a toy experiment on 2D data (Sec.4).This is simply not sufficient for a paper at a major machine learning conference, so I am forced to recommend rejection. Very limited theoretical and empirical results.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper introduces two intrinsic evaluation criteria for saliency based model explanation: completeness and soundness. A consistency score is introduced to simultaneously evaluate both completeness and soundness. Experiments demonstrate that the heuristic TV regularization method can help with soundness, as suggested by high consistency scores. To avoid confusion, the definition of correctness needs to be clarified. It would be interesting to quantitatively evaluate the performance of saliency methods on the ImageNet localization task, and to perform a correlation analysis between the completeness/soundness scores and the localization accuracy. While I appreciate the motivation and the overall idea of measuring soundness of saliency, the definition of correctness and soundness is a bit unclear, and the effectiveness of the proposed method is not sufficiently demonstrated by the experiments.<|endoftext|>This paper addresses masking based saliency methods for providing visual model explanations. It defines several new metrics for evaluating saliency methods: "completeness", "soundness", and "consistency score". The contributions of the paper (the proposed saliency method and the new saliency metrics) are modest and their significance is not clearly explained in the paper or supported by experiments. It would be helpful to readers if the contributions were stated more clearly. It proposes a saliency method that is a variant of prior work. If not, could the authors please clarify? There is some discussion of this in the introduction, which ties increased soundness to reduction of artifacts in the mask itself. The proposed saliency method does not outperform other baselines for most metrics.<|endoftext|>This paper discusses the completeness of saliency maps and proposes the concept of soundness. Based on this, they propose consistency score to predict the possibility that the quality of the saliency map is consistent with the classification probability. Besides, the author uses these definitions to implement the intrinsic evaluation of masked based saliency methods. We believe that the example in Figure 1 is not comprehensive and does not explain the difference between completeness and soundness well. The concept of the proposed method is innovative, but many details of the method have not been explained clearly. In addition, more persuasiveness is needed in the experimental performance.<|endoftext|>The paper tackles the problem of model explainibility from the perspective of heatmap based visualization approaches. It argues that achieving a good heatmap to explain the model decision is possible by considering two notions, completeness and soundness, borrowed from logic. The paper brings an interesting concept, i.e., soundness, to the model explainability research. The paper proposes a new score to evaluate the quality of heatmaps. The score is well designed and suited for the proposed approach. And how much this could be a risk to the applicability of the approach? It would be nice to read a paper that is self contained. Reading the paper, it seems the presentation, storyline and experiments could be improved.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; Since this isn t the first work to address the issue of compression, the experimental results given in this paper are not enough to justify the impact of its contribution. This idea clearly ignores the semantic structure of the words, and so it completely misses the point of word embeddings. This is reflected in the poor performance of this approach in the experiments.<|endoftext|>This paper proposes a method for compressing the word embeddings. The experiments are conducted on GLUE and XNLI benchmarks. The results show that the proposed method provides to reduce the model size with the small performance degradation. Therefore, the idea of incorporating sub embeddings is not novel, and the claim for the technical novelty is incorrect. However, this paper mostly ignores such developments. #### *** experiments* 3, In the experiments, there are no baselines for reducing the embedding size.<|endoftext|>This paper proposes an embedding compression method which replaces an $N$ x $d$ embedding table with $k$ distinct $M$ x $(d/k)$ embedding tables (where $M   N^{1/k}$). Similarly, the use of k means in this paper is reminiscent of the Lloyd quantizer (also discussed in [1]), but this method is not discussed either. IEEE transactions on pattern analysis and machine intelligence, 2010. In ICONIP, 2016.<|endoftext|>[1] https://openreview.net/pdf?id ByxZX20qFQ[2] https://openreview.net/pdf?id BJRZzFlRb[3] https://arxiv.org/pdf/1911.12385.pdfOverall, the paper proposes an interesting idea to share parameters across words and reduce the size of the embedding which hasn t been explored in the past with promising results on XNLI task.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper analytically compares the excess risk of two meta learning methods: MAML and NAL. e. Fig 4: Specific experimental details are missing. Sec 5: Current theory seems too simplistic to capture the reality of non convex MAML. 3.This is also validates the popular notion that MAML is better at adapting to hard tasks than baselines. Sec 3.1: a. I hope the authors can provide these in the next revision. b.It is not clear what the hardness means. e. The problem considered seems too simple. This assumption on $m$ is too strong for the results to be interesting. This needs to be derived.<|endoftext|>This paper is about finding the conditions under which MAML outperforms standard multi task learning. The authors also give numerical and analytical results in two layer neural networks. MAML is a widely used meta learning method that can quickly adapt to new tasks via one or a few stochastic gradient descent steps, so it is interesting and important to understand on which conditions MAML works well, and based on which, it may be further improved. The conditions provided in this paper are well supported by both numerical and analytical results in the linear regression setting. One more suggestion: the analysis is based on the ratio of excess risk. Interesting problem, solid numerical and analytical results in the linear regression setting, need more experimental results in the case where DNNs are used<|endoftext|>The authors aim to compare theoretically and empirically the ability of MAML and Non Adaptive Learning (NAL) to different tasks in a multi task setting, where tasks are sampled independently from the same distribution. I also think that the definition of hardness is a bit oversimplistic (or at least not sufficiently motivated) as it measures the variance of the samples in the task. I do not think it is impressive that MAML is more adaptive than the non adaptive learning method, which does not seem to be practical. 2.The text is clear and I can easily follow the story. 4.I think the analysis of MAML at the beginning of Section 3.1 is interesting. Why wouldn t it fail in a more complicated setting? It stems from the problem definition described at the beginning of Section 3. In addition, the paper makes an effort for laying some definitions of task hardness and tries to demonstrate different tradeoffs between adaptivity and hardness.<|endoftext|>Related analysis and simulation results are also available for two layer neural networks. **Strengths:**  The authors analyze the effect of the hardness of task loss on the solution of MAML algorithm, and suggested situations where MAML gains advantage over NAL. The theoretical result does not seem to be able to guide practical applications of MAML effectively. It is not convincing enough why NAL is made into comparison as we do not often use NAL for meta learning. **Clarity:**  The arrangement of the paper is clear and structured. The conclusions of this paper are interesting, and some of the analyses come from a novel perspective. However, the contribution of this paper still remains in doubt as it is unclear how the insights can prove to be useful in practical applications.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper uses a modified AlphaZero MCTS training loop to generate proof tree size heuristics, for use in MCTS or proof number search (PNS). Specific questions and comments  There is an unadressed issue of total computation time. The 90 problems tested? Howeve, I do think that the readability of the paper could be substantially improved through some reorganisation and rewriting of some sections. This would help with two things. Even if it could be further improved, I think the paper is in a reasonable state for publication.<|endoftext|>This paper describes an AlphaZero like approach to train networks from self play, where the networks are trained to predict (logarithms of) proof costs / disproof costs, which are heuristics closely related to proof numbers / disproof numbers. 2) Interesting contribution. 3) Good empirical results. I do have several minor issues (like some notation) and a few small points that confused me, but I expect these should be relatively easy to resolve; see detailed comments below. But the next paragraph immediately seems to contradict this, because it s actually only correct for AND nodes, not for OR nodes. Final sentence of first paragraph of 4.2: the results really don t back up this claim about "closing the gap in performance" in my opinion. The gap is almost identical. **After authors  response**: I am satisfied with the authors  response and revisions.<|endoftext|>As we know that, from AlphaGo to AlphaZero, less and less expert knowledge is used. This paper makes AlphaZero becomes faster by modifying the training target of the AlphaZero algorithm, such that it prioritizes solving the game quickly, rather than winning, and training knowledge based networks. I think the discussion about the relationship between playing games and solving games is questionable. In this paper, they argue, “There are two main goals in the pursuit of strong game playing agents. In my opinion, solving games is the first step for playing games, i.e., playing games is the goal for solving games. Our goal should be using the computed strategy in a game, not just solving a game.<|endoftext|>The number of such leaf nodes is defined as proof cost and the paper proposes to set it as the new learning target based on the AlphaZero learning framework. 2.The contribution of this work is not well discussed in this paper. I wonder whether it is significant to quickly solve a game from scratch. This paper proposed a new learning objective that enhance AlphaZero and FDFPN to solve a game. However, without a sufficient discussion about the significance of the studied problem, I do not recommend acceptance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The work is both novel, interesting, and brings new perspectives to the equivariant networks literature while maintaining an exceptional level of polish in both writing and execution. While this school of thought is different than the $G$ steerable variety as presented in this paper it would be great to harmonize both directions with a discussion on their differences especially when it comes to practical applications.<|endoftext|>The authors developed the general theory of equivariant partial differential operators (PDOs) between feature fields on the Euclidean space. Experiments on the rotated MNIST and STL 10 datasets have been conducted to compare the performance of equivariant PDOs and equivariant steerable kernels. It would be great to see examples where PDOs have a significant advantage over steerable CNNs. The theoretical contribution of the paper is significant.<|endoftext|>The paper develops the theory of equivariant partial differential operators   a steerable PDO which is equivariant under any given symmetry. The PDO and convolution can be unified into a single framework. Thus, I doubt the significance of this work to our community. However, the proposed method is quite limited in terms of performance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes to extend semi supervised learning techniques to address domain adaptation problems and proposes a unified method, AdaMatch, that can handle unsupervised domain adaptation (UDA), semi supervised domain adaptation (SSDA) and semi supervised learning (SSL). The advantages of this paper are as follows. 3) The proposed method significantly extends the state of the art performance. Is it still as competitive as in the datasets chosen in the paper? This is a good paper overall. But overall I like it and recommend for acceptance.<|endoftext|>To boost accuracy on domain shifts, this paper proposes an AdaMatch method for unsupervised domain adaptation (UDA), semi supervised learning (SSL) and semi supervised domain adaptation (SSDA). The contribution is significant and somewhat new, and the experimental evaluation is extensive. However, three concerns in the weaknesses of the paper should be clarified in the paper. + The AdaMatch achieves the state of the art accuracy on the SSL, UDA and SSDA tasks using the same hyper parameters regardless of the dataset.<|endoftext|>This paper presents a unified solution for unsupervised domain adaptation (UDA), semi supervised learning (SSL), and semi supervised domain adaptation (SSDA). Experimental results show that the proposed method performs on par or significantly better than respective state of the art methods in UDA, SSL, and SSDA. Strength  It is interesting and should be practically useful to take a unified scheme for SSL and DA. Weakness  The technical novelty of the proposed method is somewhat limited.<|endoftext|>The paper extends FixMatch as a unified approach for semi Supervised Learning, Unsupervised Domain Adaptation, and Semi supervised Domain Adaptation. Strong results that improve over the compared methods on three setups. Moreover, the paper also proposes a random logit interpolation as means of regularization. Overall, the paper presents an engineer method that works, although why escapes me. For instance, the distribution alignment comes from Berthelot et al.2020, or the loss from FixMatch (Sohn et al.2020).Please, clearly state what are your contributions over the state of the art. For instance, the upper stream should be named as the variable $Z $ and the bottom one $Z $ (if I understood it correctly). Similarly, the relative threshold seems to depend only from the source, yet it depends on the source and target (6). It is not clear what you are trying to do with the random logit interpolation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; In order to do so, it performs experiments in a controlled synthetic environment where a first language model $P_L$ is trained on a corpus of natural sentences, and a second model $P_M$ is trained to emulate the first model. The authors observe that the second model systematically tends to underestimate the probability of rare $P_L$ sentences, the more so the rarer such sentences are, and show that some artificially corrupted sentences tend to receive higher probability from $P_M$ than from $P_L$, partly explaining where the missing probability mass over rare well formed sentences went. A nice experimental paper addressing the difficulty of neural LMs to approximate the probability of rare events from an underlying "teacher" LM.<|endoftext|>The paper proposes to examine the probability mass that a language model trained as usual places on frequent/likely and rare/unlikely sentences by learning from a distribution that is another neural language model itself so instance level probabilities/NLLs of sentences can be compared. I would strongly like to see this paper accepted. The authors make a good point that these scores are not as informative as what this paper presents, but the reason these are crucial is so we know whether the effects the authors find are a property of well trained models or whether these models (through subpar training or another issue) are just poor models that have little connection to the models we actually use in reality.<|endoftext|>This paper investigates how language models allocate their probability mass, with an emphasis on rare sequences that are part of the  heavy tail  of the distribution of natural language sequences. The idea of using a language model as a ground truth distribution was interesting and well suited for the analysis done here. A language model is trained on an empirical estimate of the target distribution, and the authors study the gap between the learned model s and target distribution s probability assignments. The paper was well written and enjoyable to read.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; I would like to see the few points of clarification I raised above addressed in the rebuttal. 3) It would be interesting to highlight the accuracy of the Teacher model for reference in some of the tables. The paper presents an interesting method and good supporting experiments. The main novelty is that the distillation from the teacher to the student leverages the relations learned by the teacher on the dataset, i.e.aims to transfer similarity clusters formed by K nearest neighbours.<|endoftext|>I m looking forward to more experiments under the same training costs. I m looking for the result with the same teacher. The paper is well written and easy to follow. As I have mentioned, I m a bit concerned about the main result. Interestingly, I have reviewed this paper in another venue, but my questions are still not addressed well.<|endoftext|>This paper presents a new self supervised distillation learning schema which aims to address the ``low relation  distillation objective across instances in priori arts. Quantitative experiments on ImageNet classification task, transferring learning and semi supervised learning task validate the effectiveness of proposed method. How is the relation distillation reflected in the inter sample distillation loss? As I never see any description about it. My main concern is the technical contribution of BINGO somehow overlaps with [1] as previously mentioned. And some experimental settings are not consistent or clearly explained.<|endoftext|>Following the literature, the authors present a variant of distillation that achieves better performance than competing distillation methods. Experimental evidence is not very strong, and there are some clear aspects lacking in the evaluation. (the score of 2 on "Empirical Novelty And Significance" is due to lack of completeness of experiments but can be raised with stronger evidence). (updated to 6 after rebuttal) The explanations are very clear and the paper is well written.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The system allows queries to be performed by handling operators using one  and two input MLPs. As minor issues, on page one, in the sentence  directed acyclic graph (DAG) graph which defines ...  the word graph is repeated. I found the paper to be mathematically correct and formal enough. The results obtained are promising and the possibility of solving negative queries is a strong point in favour.<|endoftext|>New model for knowledge graph reasoning over complex logical queries. The main idea is to use neural networks to implement different operators to compute the queries over the knowledge graph. In particular, mixer MLP networks have been used to implement these operations. Overall, the paper shows good results but is perhaps a bit weak on justification for these good results.<|endoftext|>The neural networks are built on top of MLP Mixer architecture and produce good empirical performance. and complex queries should be clearly distinguished. The related work fails to explain the relation of this work to prior work in a meaningful way. However, the paper lacks clarity and the related work is poor.<|endoftext|>The paper proposes a new embedding method for knowledge reasoning on knowledge graphs. From the introduction is not clear the embedding process of the queries and the entities. In section 3 it is not clear how the input embedding for the entities are computed. It is not clear the approach used to transform a FOL query to an embedding. Concluding, while the results seem to be interesting the clarity of the proposed approach should be improved.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; It also provides the first lower bound for the problem. The paper studies an interesting topic and give improved sample complexity guarantees.<|endoftext|>The authors consider the incremental autonomous exploration problem. I did not check the proofs of the lower bound. P12, Lemma 4: You need an extra assumption on the \tau_k such that independence to be able to apply Hoeffding inequality. when does the agent stop and output the policies (is T chosen by the agent)?<|endoftext|>This paper studies the autonomous exploration problem and multi goal SSP problem, and proposes three algorithms with improved cumulative costs on 4 learning objectives. This somehow reduces the novelty of the paper. The paper is generally well written, though some paragraphs need shorten.<|endoftext|>This paper studies the sample complexity of learning algorithms for autonomous explorations (AX). The statement of the problem is clearly stated, and good theoretical results are reported with substantial proof.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper proposes a new training approach, the temporal efficient training (TET). This algorithm utilizes a new loss function to improve the generalizability of SNNs. The TET method is a new temporal loss function and the TIT is a fine tune method. Here are some questions. 3.In table 3, the network performance across static datasets and neuromorphic datasets, why the simulation length of two networks with the TET on the Imagenet are different?<|endoftext|>The paper focusses on an important problem of improving supervised training of spiking neural nets (SNNs), as the current state of SNN training is lacking as compared to that for standard ANNs. While the proposed improvements, TET and TIT, may appear only as minor tweaks as compared to previous work, the demonstrated improvements in speed and accuracy suggest that the paper may have substantial impact for SNN research community.<|endoftext|>This paper proposed a new loss function called TET for directly training SNNs. Strength: The results are good compared to existing works. The paper also has a comprehensive comparison between the proposed TET and regular SDT. Does the network adopted in this paper have a similar number of parameters to the existing works in the comparison?<|endoftext|>The proposed method is simple but its efficacy outperforms the SOTA results for various networks on various datasets. $\textbf{Weaknesses:}$In spike of the excellent classification records achieved in this work, the authors’ work leaves a number of uncertainties that should be thoroughly addressed to not mislead the readers. There are many statements that may mislead the readers. “Another training issue in the memory and time consumption …” on Page 1. Is this true for the use of $L_{TOTAL}$?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper investigates if generative models can be used to improve the robust accuracy of image models. Small comments and questions:  You write that you do a hyperparameter search on γ. The paper is a very good contribution to the research on adversarial robustness. I think the paper should be accepted.<|endoftext|>This paper focuses on utilizing synthetic images generated by a generative model for the task of achieving robustness to adversarial attacks. Towards this, the paper aims to assess the suitability of the proxy distribution defined by the generative model for the underlying task. Towards this paper aims to leverage the recent advancements in the area of generative models.<|endoftext|>While the basic idea is very simple, the paper adds an interesting theoretical analysis that allows us to estimate what generative distribution would work best for robust training. Empirical results are good, good number of datasets. The paper offers some interesting theoretical and practical results, but without the hole in the proof fixed I do not think it should be accepted. In the proof, the authors define a distribution D  by a mapping of average distance epsilon from D. This is clearly an upper bound on the Wasserstein distance, but to show that it is the Wasserstein distance, i.e.the minimal coupling is missing.<|endoftext|>The paper proposed a new metric for how well a synthetic distribution can help to improve the adversarial robustness. And I have an additional questions:* It s quite interesting that the performance of DDPM is much better than other generative models, is there any explanations for this?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The idea is very simple and the authors attempt to bolster this simple idea with some theoretical results and a wide array of experiments. I also think the presentation could use some work (see e.g.detailed comments below). I think the empirical results are quite compelling and show the benefit of the proposed method.<|endoftext|>Theorem 2 is also very imprecisely stated. The idea explored in the paper is potentially interesting, although I find it not very well formulated and motivated. Discussion of the results is very minimal.<|endoftext|>However, runtime is the focus of the paper. There are broken references. The paper does not study this aspect.<|endoftext|>Beyond the consideration of existing schemes to improve the computational cost of decision tree construction, another limitation of the paper is that some of the theoretical results are very imprecise and appear to be somewhat trivial. I also quickly looked through the supplement and did not find anything but it is possible I might have missed it. 2011.I recommend a strong reject based on the weaknesses detailed above.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; In this article the authors propose to incorporate domain knowledge on where good configurations are located in Bayesian optimization (BO). They do so by weighting the acquisition function by a prior that decays and reverts to uniform as iterations go. The paper is clear and with extensive empirical evaluation. Additional details on how to define the prior would make the paper more self contained. I appreciate the theoretical result albeit one could hope to show some improvement from using a good prior. Typos:P1: documentesdIf not particularly original, the proposition is well motivated and linked to the state of the art, plus it comes with extensive empirical results supporting the approach.<|endoftext|>## Summary:PiBO is a very straightforward paper that seeks to incorporate prior knowledge about the optimum to accelerate Bayesian optimization. They achieve this by simply multiplying the acquisition function (Expected Improvement, in the paper) by the prior distribution, and then maximizing that. Synthetic and real world experiments indicate that: With a well specified prior, PiBO demonstrates superior performance over it’s basic EI counterpart. I know it doesn’t have a lot of moving parts or complicated mathematical formulations that the BO community tends to favor these days, but I am in favor of acceptance for the reasons I mentioned above. The authors may be leaving performance on the table by using such a simple prior. * There is a stronger connection here with metalearning, in the context of HPO, (which seeks to build and then exploit priors) as a whole, and perhaps this could be included as a short paragraph in the related work section w/ some additional citations (e.g., more of Hutter’s work).<|endoftext|>The paper proposes a method to incorporate prior information about the optimum in the standard Bayesian optimisation (BO) setting. The prior is specified as a smooth function over the range having high values where the experimenter think the optimum may exist with high probability and low values otherwise. This prior function is incorporated in the BO workflow through multiplication with the EI acquisition function. The synthetic experiments are extensive and convincing. The real case studies are limited, but sufficient. I am quite aware of research in BO and I think integrating prior information is an important aspect that has not been looked into much. This may make the anytime upper bound not useful. The authors should use this insight to restrict the shape of the prior function. Another limitation of the theoretical analysis is sticking to EI acquisition function. I would like the authors comment on this aspect. I would go for weak accept. But I am ready to move up depending on the rebuttal.<|endoftext|>This paper proposes a new method to incorporate prior knowledge about optima location in Bayesian optimization. Overall this paper is well written and appropriately discusses the related literature. While the problem of incorporating the optimum location makes sense, I am not sure if the experts can provide a proper probability distribution about the optimum. For example, even the experiments in section 4.4 rely on the priors which are based on already (manually) tuned deep networks. Finding useful priors may thus be hard in practice. The method seems simple and as stated in the paper is easy to implement. However, it seems to me that while the paper claims the proposed method to work with any acquisition function, it gives analysis only for EI acquisition function? Also, there is a claim that the method can recover from any misleading prior, but it may be true only when there is a nonzero support for the true optimum x*. Why does \pi BO often start from a higher starting value after the initial design? We do not know if \pi BO does well due to the proposed algorithm or due to the better initialization?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Moreover, it seems to be contradictory with the  claim that it does need training dataset. Existing work [1] also simultaneously  optimizes the trigger patterns and model parameters. Also, some details of the trigger generation process is missing, e.g., the  size of the trigger and the perturbation budget of FGSM. * Countermeasures  Potential defenses are only discussed but not validated. I do not see the  significance of this appearing in a AI/ML venue.<|endoftext|>Strengths:  The paper is well written and easy to follow. The contributions of the paper are mainly in steps 2 and 4 of the algorithm where the authors enforce block sparse flipping and only 1 bit change in a byte. While these objectives are not directly optimized for and are merely enforced by heuristics, they are still of interest. Can the authors please provide a 1 1 comparison with this work and explain how they improve/change compared to this prior work? Therefore, it is important to evaluate the proposed attack in face of existing defenses, preferably bypassing them successfully and/or better than prior attacks.<|endoftext|>However, it is written several times that the training process has been modified by the attacker. However, the experimental setup used for Rowhammer profiling is not clear. This should be discussed more comprehensively. This work is promising and contributes to generating successful backdoor attacks in practical settings with good results. However, there are some concerns that affect the clarity of the method and of the threat model.<|endoftext|>The model and its parameters are studied offline to determine a trigger pattern and the bits that must be modified in the target model. Overall an interesting paper, but I think some additional justification for why such attacks are not easily defended against is perhaps required. what is the size of the trigger?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Since this paper focuses on scene compositing, it may be better to have one more paragraph in related work discussing recent scene compositing methods. I will appreciate it if authors can address my concerns in the rebuttal and I will change my reviews accordingly. I feel this is an issue that should probably be corrected. 3.The novelty of the paper may be limited. Therefore this contribution may not be fully supported. The other is the rendering algorithm. 4.Necessary comparisons may be missing.<|endoftext|>In the paper, to train the OSFs, the authors assumes the object is captured with the point light with radiance of (1, 1, 1), is the position of this point light is known as well? The results on both synthetic scene and real scene shows improvement over baselines (o Nerf, o Nerf S) that didn t consider the lighting transportation. 5.The proposed the rendering technique for secondary lighting effect (shadow) is great, however, in principle, the proposed approach should also be able to backwards through this?<|endoftext|>This paper proposes a NeRF based method for composing photo realistic scenes from captured images. Also, the experiment is not very convincing due to the lack of comparisons. The baseline (o NeRF) compared in the paper is not very convincing. Comparison with existing appearance modeling methods (e.g., Neural Reflectance Fields, NeRV) on single object rendering under novel views and lightings should be provided.<|endoftext|>The resulting training time both on synthetic and real world objects is also not mentioned. N is used in the main text to refer to the number of objects and to the number of point samples along a ray, neither of which seems like the right parameter here. Runtime:  An evaluation of the runtime (training time and inference/rendering time) is missing. That s why I will go with borderline reject for now. Demonstrating plausible consistency while smoothly changing light position/object positions also strikes me as beneficial but I don t believe that acceptance should hinge on it.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; In this paper,  authors improve adversarial transferability of ViT by Self Ensemble and Token Refinement method. This paper successfully improved the adversarial ability of ViT attacks. 3.The method of this paper is a general framework. The author claims that the design of this attack method comes from the different inductive biases between Transformer and CNN style models. 2.In addition, this method seems to be able to improve the transferablity of models with other paradigms.<|endoftext|>By incorporating self ensemble and token refinement methods, the authors have devised a method that takes use of the modularity of ViT models to improve their adversarial transferability. Their experiments have demonstrated adversarial transferability across different convolution and transformer model families for different vision tasks. The problem approached by the paper is interesting and the proposed approach is novel to the best of my knowledge. The work is well structured and written in a clear and concise manner with sufficient experimental verification. enhance the quality of images  Overall, I vote for marginal accept.<|endoftext|>The paper conducts extensive experiments to show the effectiveness in adversarial transferability across different models, including CNN s and ViT s variants, and tasks. In general, I find the paper presents an interesting and important work w.r.t the adversarial security of ViTs. The main strengths of the paper are:  The paper is well organized and well written. The proposed approaches are well motivated by empirical findings. I find that the paper presents an interesting study on the transferability of adversarial samples of ViTs.<|endoftext|>This paper enhances transferability of vision transformers (ViT) by introducing two strategies specific to the architecture of ViT models, i.e.Self Ensemble and Token refinement. In addition, the proposed Token Refinement can potentially enhance the discriminative capacity at each block of ViT. These two methods are well motivated. Through the experiments, this paper empirically demonstrates favorable transfer rates across different model families (convolutional and transformer) as well as different vision tasks (classification, detection, and segmentation). 3) The paper provides a detailed empirical analysis to demonstrate the effectiveness of the proposed methods.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper investigate sampling problems in HEP simulations and neural importance sampling (NIS) based methods. It is unclear what is the major contribution of this paper. The library ZÜNIS seems an important tool in the field of HEP, but is discussed very briefly. The loss function (10) seems new, but it is unclear for readers why such an auxiliary probability distribution q is required and q can be simply selected as a uniform distribution.<|endoftext|>The authors propose a new importance sampling scheme based on a transformation and relying on Normalizing Flows. The paper could contain interesting material but requires additional work on order to be published. The differential is missed in Equations (7) and (8). The state of the art discussion that the authors do not take into account is, for instance,M. F. Bugallo et al, "Adaptive Importance Sampling: The Past, the Present, and the Future", IEEE Signal Processing Magazine, Volume 34, Issue 4, Pages: 60 79, 2017.<|endoftext|>The toolbox is a modular implementation of Neural Importance Sampling (NIS, Muller et al.2018).The experiments demonstrate the toolbox and its usefulness compared to a popular software (VEGAS) and uniform sampling. # Weaknesses* There is no discussion regarding activation functions and their role in the behavior of the resulting distribution (IS proposal). # Observations* In Section 2.3 onwards, the authors use the notation $p(x, \theta)$ — and $q(x, \theta)$. Therefore, I’m suggesting a rejection. However, since I’m not a connoisseur of high energy physics, I am not able to exactly gauge the utility of this toolbox to the specific demographic.<|endoftext|>The paper describes ZüNIS, a neural importance sampling (NIS) library. The main application area is high energy physics (HEP) simulation which is used to, for example, interpret experiments at the LHC. # Pros1.The paper introduces ZüNIS, a fully automated software library for easy NIS. # Cons1.The paper is in most parts a description of the ZüNIS package but does not contain much original research. Why should it be easier to choose a suitable $q$ than to choose a suitable proposal for the main task (i.e.solving $\int_\Omega f(x) dx$)? There seems to be an enhanced version, VEGAS+, which performs superior to the original formulation (see Lepage, 2021). I could not reproduce your results by following the instructions in "Reproducibility statement" due to import errors. It seems that figures 3, 4 and 6 are not discussed in the main text.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; In particular, the authors showed in a toy example that the theoretical lower bound is close to the empirically observed value. The authors provided extensive experimental analysis of transition bounds for different random subspaces. Pros:  A new theoretical result that explains the existence of threshold training dimension. The paper is well motivated and well structured.<|endoftext|>It studies an interesting phenomenon with mix of empirical and theoretical results. They observe that the lower the dimension, the less likely one is to reach a loss $\epsilon$. This would make it possible to compare the empirical transition with your theory in the DNN case directly. Finally they propose a notion of lottery subspaces inspired by the lottery tickets paper and compare their results.<|endoftext|>The paper aims to provide a theoretical explanation for recent observations (lottery tickets, training in random subspaces, spanning pruning) that deep neural networks can be trained using fewer parameters than necessary. It would be interesting to see at least experimentally the change in depth/width for large classes of problems, as this ties in with numerous results on expressivity that say in general , increasing depth is more useful than increasing width (from the lens of expressivity alone!).<|endoftext|>The contribution is a straightforward application of Gordon s result weighing mainly on the empirical side. I believe nonetheless that the paper gives new insights for training in random subspaces. Overall, I like the paper and would be happy to raise my score based on the authors  response.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; rating score: 3; This paper presents a new federated learning approach named Split Mix FL that allows clients to train customized models efficiently while considering heterogeneity in data and computation resources. Experiments are thorough, demonstrating the effectiveness of the proposed approach on multiple datasets with multiple metrics. ## WeaknessesIt s not perfectly clear how the split and aggregation are done in the proposed approach. HeteroFL aggregates multiple models with different sizes into a single one, and SHeteroFL asks clients to train all affordable base models. The proposed work argues that "Without loss of generality, we assume exponentially distributed budgets in uniformly divided client groups:..." I wonder how much this assumption is reasonable and how dividing clients into four groups in this way is optimal for various situations. Further clarification on this point would make the experimental results stronger. Also, the assumption about the computational resources of clients would be better clarified.<|endoftext|>This paper proposes a customisation strategy named "Split Max" for federated learning. And in the inference process, the results are aggregated based on the coefficient. Secondly, to provide devices with models with different robustness, it trains two similar models together to capture both the standard training accuracy and adversarial training accuracy. 5.Regarding the evaluation setup, it is recommended to have more details on the clients  budget, e.g.the distribution of client budgets. Are there more clients with a large budget? Experiments show that Split Mix achieves better accuracy than naive approaches. This paper proposes to tackle the heterogeneous budget in FL scenario.<|endoftext|>The paper proposes a new federated learning scheme that is suitable for devices with heterogeneous resources. The proposal, namely Split Mix, trains multiple models of different sizes and adversarial robustness levels, tailored to the budget of each device. This breaks the flow of the paper. ### Weaknesses [Significance]   The evaluation results are heavily based on the assumption of exponentially distributed budgets.<|endoftext|>Split Mix trains a model that can later on be customized in terms of model size and robustness. All base models are trained on all clients (should these meet the compute requirements of a sub model) in a federated manner. This conflicts with the argumentation in the paper, aiming at considering more realistic FL settings accounting for the `different dynamics` and heterogeneity. is it do do inference using the learned based models? Summary: The Authors have motivated well the paper but need to improve the presentation of the technical contribution. It is not clear which architecture was used for Digits. Adding a 1 line description of what the task is for Digits and DomainNet would be useful for readers not used to work with these datasets.
Reject; rating score: 3; rating score: 6; rating score: 6; The paper provides an interesting idea by unifying flow based models under a non adversarial framework to realize multiple distribution alignment. ## Concerns:My major concern is why adversarial learning is challenging for distribution alignment? Do the authors justify this only according to, "However, adversarial learning can be quite challenging in practice", with two references are both earlier than 2019. Using FID to measure generate data quality on MNIST is not well acknowledged. 2.There exist lots of multi domain translation models with GANs, e.g.MUNIT and StarGAN v2.<|endoftext|>The current experiments (most of them) show the produced samples, compared with two SOTAs. It seems to me that the advantage is not very significant. It proves the equivalence and offers a computational and feasible solution that can be inserted into flow based non adversarial approaches in a plug and play fashion. As the approach is mainly for domain alignment, it would be interesting to show, e.g., visual domain adaptation results.<|endoftext|>This paper proposes a flow based method for the unsupervised data set alignment problem. Moreover, the author also experimentally verify the statement in the discussions about prior methods and a regularization term. When the number of distributions is large, is there some problems when the proposed method to align these distributions? The authors state that a fixed normal distribution Q is insufficient.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This is a paper working for molecule generation, especially considering the high frequency and important subgraphs called graph pieces. The graph piece extraction algorithm is not clear. Line 10 finds only the most frequent piece? What if these two most ferquent pieces are not connectable in molecules? For molecules with rare atoms, their piece level decomposition will be a set of single atoms? However, section 3.3. introduces that GNN is applied on the whole graph G, like what has been done in previous GNN based molecular graph representation. Another concern is about the generated results. Evaluation and comparison on this QM9 dataset can strengthen the evaluation results. An error to correct:  We try to add bonds which has The details of the proposed model were not clearly introduced.<|endoftext|>how is the fine structure (ie the nodes and edges) of each piece generated so that the bond completion model can work only on nodes across different pieces? The summary offered is too high level and crucial information is missing (also Fig 5 is too generic). The properties of the proposed way to generate pieces (fragments) are unclear: what is the size distribution of the fragments generated? What is the computational complexity of the fragment generation phase? How often are novel fragments appearing as a function of the training set size? What are the consequences of this, with respect to the generalisation/representational capacity of the approach? Why is the vast literature of graph mining completely absent in the review section? It would be of interest if the paper would analyse this statement empirically or theoretically, but the only results available are the usual weakly informative experiments on validity, uniqueness and novelty, and property optimization experiments (without any notion of statistical significance for the comparative results). The presentation is overall unclear and the literature review incomplete.<|endoftext|>The trained model is expected to generate a variety of graphs with desirable properties. In particular, it is interesting that the proposed method uses graph pieces in its training instead of directly using a given collection of graphs, which is the key idea of this paper. Presentation of this paper is good. Recently it is widely known that benchmarks such as the penalized logP and QED are not appropriate for evaluation and using Guacamol is highly recommended (see https://pubs.acs.org/doi/abs/10.1021/acs.jcim.8b00839). So please evaluate the proposal on Guacamol. Ablation study is not convincing. Therefore, there should be various subgraphs that cannot be extracted by the proposed method. Some discussion would be desirable.<|endoftext|>3.The authors claim that the proposed method has two benefits: (1) captures correlation; (2) accelerates computation. Maybe the authors could theoretically analyze this. Then, how to determine proper N for different down stream tasks? 4.The experiment is less convincing. The proposed method is only evaluated on one dataset ZINC250K. In Table 1, JT VAE achieves perfect results, while the proposed method seems to show limited competitiveness. The authors should discuss the differences between them, and compare the performance in the experiment. In Section 3.1, the authors state that: "$\tilde \varepsilon$ contains all the connections between the atoms in different graph pieces". 8.Algorithm 1 is not clear. The authors are expected to clearly explain each line of the proposed algorithm. The idea of this work is reasonable, but some technical details have not been clearly presented.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Authors proposed to use nonparametric methods for sampling data point for contrastive learning. I will not question neither the motivation nor the importance of the problem. Paper is well written and mostly connects previous art. How does proposed method deal with such problems? JMLR, 2021. The proposed kernel in the paper is a simplified version of kernels described in Section 5.1 of [1]. If authors would have explicitly assumed a model than it is clear that ‘fairness’ is coming from the model. (After the rebuttal, I have decided to increase my score)<|endoftext|>The idea of CME allows them to compare the conditional data (ie usually a smaller set) with the batch and then still perform contrastive learning now will the batch. However, it is undeniable that the objective function proposed in this paper is a simple derivation from [1] and adapted to this representation learning setting. What is the computational complexity of the methods ? Could the authors please elaborate why methods such as SimCLR? the results look. [1]: Ton et al 2021 https://arxiv.org/abs/1906.02236I believe that the paper as its merits and really like how they have seemingly incorporated CME into representation learning and obtain good performances.<|endoftext|>It uses a Kernel Conditional Embedding Operator to sample from all the available data and assign a kernel similarity to each sampled data, which is based on the values of the conditioning variable. The CCL K also extends conditional contrastive learning to deal with continuous conditioning variables. 2.The motivation of this paper is clear. The paper is well written and easy to follow. Computational complexity of CCL K is missing. The experimental result of Fair_{CCLK} is not very convincing because it is not compared to other fair contrastive learning baselines, such as Fair_{InfoNCE}. 3.I notice that the loss functions of Fair_{CCLK} and HardNeg_{CCLK} are exactly the same. The idea of using a kernel function to address the conditional sampling problem in contrastive learning is interesting and novel. The experimental results validate its effectiveness in some contrastive learning tasks.<|endoftext|>This paper presents Conditional Contrastive Learning with Kernel, a kernel sampling method for contrastive learning. The authors present their method in both clear formulation and extensive experiments in three different tasks. Some motivations mentioned in the introduction are not well addressed in the method and experiments part (e.g.CCL K can work with insufficient data, for continuous learning, etc. From the side of technique in the paper, the proposed sampling method is not novel, as the weighted sum form has been discussed by various paper [1 3]. I am inclined to recommend a reject of this paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 3; The paper proposes a new framework to detect out of distribution samples for deep neural networks without retraining or modifying the model. The method is based on statistical hypothesis testing which is applied to activations of all or multiple layers. Specific comments:  There are few instances in which authors are used statistical terminology that are unclear for wider audience. page 3, "... the p value is given by $q(t_{obs}) P(...)$ ...", this would be less confusing if it would be mentioned that p values are denoted by q  Eq (1): I is not explained, I guess it s indicator function  Section 3.2: the procedure of obtaining p values was not unclear based in this description (as I do not have too background in hypothesis testing). Perhaps the content of footnote 3 and/or Algorithm description from appendix could be moved to this section? Algorithm is missing t >q calculation in this point? As the framework is novel and results shows good performance both in accuracy and computational complexity, I feel that the study deserves to be published.<|endoftext|>S2) The paper proposes a new, computationally efficient method (MaSF) for performing hypothesis testing which uses multiple layers from a deep neural network. The authors report many empirical experiments which supports their method. This sounds like a problem of "calibration," which the paper does not mention. One way that the authors could help with this issue is to describe the Fisher and Simes tests in the body of the main text, instead of leaving it for the appendix. A better wording could be, for example, _incorrectly predicting "OOD" for an actual in distribution sample_. I think it should say $N_{train}$. Why is the global null hypothesis denoted with an intersection $\cap$ symbol?<|endoftext|>(3) It is great to see the computational cost is much lower than baselines such as Gram and Mahalanobis, but some baselines do not require retrain or expensive computation (e.g.Energy score (Liu et al., 2020)) are missing. Strengths:(1) Different from previous work, the paper considers the empirical distribution of each layer and channel in CNN and proposes to use global null tests with Simes and Fisher statistics to aggregate the p values. (2) The proposed method is more computationally efficient than compared baselines while preserving comparable performances. (3) The effect of the proposed method on different layers and channels is discussed in detail.<|endoftext|>This paper poses detecting out of distribution (OOD) samples as a hypothesis testing problem. Finally, the OOD samples are detected by thresholding the  p  values. 2.Proposed approach considers an  Observer  style approach that does not require updating the underlying model. This is more efficient and easily accessible. 3.Proposed framework achieves high efficiency while detecting OOD samples and can be deployed in real time setups. What are the novel contributions compared to these works. Here are a few recent approaches on OOD detection and authors are encouraged to compare with these approaches. a. Hendrycks et al., "Deep anomaly detection with outlier exposure"b. Liu et al., "Energy based out of distribution detection"c. Lee eta al., "Training confidence calibrated classifiers for detecting out of distribution samples"The approach lacks novelty and experiments are limited to justify the efficacy of the proposed approach.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The paper proposes to use metropolis hasting Monte Carlo to draw samples from energy based distributions over sequences. They construct proposal distributions based on trained masked language models (MLMs). The proposed method could approximate energy based sequence models with a good proposer (i.e., MLM), and the paper clearly discusses its general theoretical advantages over a Gibbs sampling alternative. However, I would appreciate more details on the specific target distributions used in the experiments: e.g., I am not familiar with MASK PREDICT and it is not obvious to me how the general technical remarks apply to this specific case. Overall, I think it is a good paper. I believe my concerns are more about presentation instead of the actual methods.<|endoftext|>This work makes a step towards sequence generation from MLMs such as BERT. This is an interesting task itself but also managing to do so will enable more applications about sampling from Energy functions of text, think of combining MLMs scores with sequence level scores obtained by the same MLM and using the resultant energy to obtain samples directly. Generation examples: For transparency, one would like to see examples generated from the MH algorithm, perhaps dump a subset of all generations of each method in a table in the appendix. In this work, the authors extend on this clarification both theoretically and empirically. Figure 2 is not clear that is the green curve Overall the paper is theoretically sound experiments are sufficient in my opinion and shows that the proposed method works. The rationale behind (wang and cho 19) faulty assumption and the use of MH as an alternative. I see they have been used both in previous work and empirically they perform differently, are there any conceptual differences that motivated you to use both formulations? If it was merely experimental it is fine just to mention it clearly in the text.<|endoftext|>The paper tries to interpret masked language models as energy based sequence models and proposes two energy parametrizations derivable from the trained MLMs. The primary contribution in paper is to rectify the incorrect assumption in the prior work and propose Metropolis Hastings (MH) based sampling algorithms for these energy networks. 2.There are many experiments between proposed method with the degenerate Gibbs sampling, but do not compared with other generation models like GPT, BART, etc. Which means the model cannot applied in other datasets except the dataset which is used for training the pre trained model. typos:In section 4,  "We empirically study the proposed Metropolis Hastings scheme .... and the task of uncondtional generation." This paper interprets the masked language model with a novel perspective, and introduces  energy networks and Metropolis Hasting sampling to text generation based on MLM models.<|endoftext|>This paper proposes a Metropolis Hastings based sampler, which can be used to draw high quality samples from non probabilistic masked language models (MLMs). Sampling experiments are conducted for both open ended unconditional generation and a conditional generation task of machine translation. StrengthsThe authors make good points, pointing out that the prior attempt to interpret a MLM (like BERT) as an MRF is incorrect and proposing to define the proposal distribution by masked conditionals for the MH sampler. EBMs (a.k.a.un normalized models, random fields) have been successfully developed for language modeling in recent years. A recent work in [6] also defines an energy based language model from MLMs. "Pre training transformers as energy based cloze models." The proposed samplers seem to be computational very expensive, and the authors do not provide any discussions on how to reducing the cost.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; Gradient Clipping Summary: DP SGD consist of the following steps: Over T rounds: 1) the algorithm samples a batch of samples from a dataset I_t. 2) it computes the gradient of each individual samples. Therefore in global clipping each sample in a batch has gradient norm bounded by R. The intuition behind global clipping is that each gradient in a batch gets multiplied by a constant factor, this means that the direction of the aggregated gradient should be preserve. The paper provides two main theorems: The first one claims that local clipping cannot guarantee convergence because local clipping can break the conditions for convergence under the NTK framework. Specially in the hard CIFAR10 setting, using global clipping does not seem to make a big difference. My understanding is that the figure is comparing the average clipped gradient with global and local clipping.<|endoftext|>**Unfair comparison between global and local clipping. 1.From a theoretical point of view, the authors claim that global clipping offers better convergence guarantees than local clipping. However, the comparison does not seem fair to me. In this particular case, taking $Z \geq M$, the global clipping amounts to rescaling each gradient with a fixed constant; therefore, it does not hinder the convergence (it is actually equivalent to changing the learning rate). As the authors point out in Footnote 5, this assumption is central to the proof of Theorem 2 and cannot be relaxed. But if we were to assume the existence of such an upper bound $M$ on the gradients in Theorem 1 as well, then we could also show that local clipping does not impede the learning procedure simply by taking $R \geq M$. As a result, the paper does not present a fair comparison of the two clipping methods, and as I understand the results at this point, there is no way to argue that global clipping is better than local clipping from a theoretical viewpoint. For the above reasons, I will argue for rejection.<|endoftext|>Experiments in the paper then verify that the proposed clipping performs well in practice. However, is this the case in practice? Theorem 2 relies on the global maximum norm bound $ Z $ for the batch gradients. My initial interpretation of it was that local clipping is $(\varepsilon, \delta)$ DP iff global clipping is $(\varepsilon, \delta)$ DP. The simplicity of the approach and the improvements shown in the experiments show that the global clipping has promise. However, there are a couple of aspects in the paper which need to be addressed. In particular, due to how the assumptions of Theorems 1 & 2 are not the same due to the assumption on the gradient norm bounds.<|endoftext|>The paper proposes a new clipping method for DP SGD. Simple and elegant clipping  Nice theoretical analysis   Nice empirical showingI am in favor of this work. The global clipping is simple but elegant. I think that using NTK to analyze differential privacy is actually a method with potentially widespread applicability. Furthermore, the empirical analysis of the method seems sound.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors present an offline performance model for optimizing neural net hardware accelerators. The paper is clear, describes its problem and solution well, clearly showcases its results across a variety of tests, and generally succeeds at convincing the reader that the approach works as advertised. This is a better paper.<|endoftext|>The paper proposes an approach to optimizing parameters of hardware architectures to design architectures that are more efficiently able to execute neural networks. This is a generally hard problem in high dimensional space. How sensitive is the optimization to the number of particles, and the parameters of the negative mining optimization?<|endoftext|>Writing is clear and the paper is easy to follow. This work presents an effective framework PRIME to tackle the challenges of automating hardware design optimization.<|endoftext|>The authors evaluate PRIME architects accelerators tailored towards both single  and multi applications as well as unseen applications in a zero shot setting. The strengths of the paper are as follows: + This paper provides a new perspective for tackling the overfitting of hardware efficiency predictors by leveraging adversarial training and infeasible design samples;+ The developed tool can be useful as an early stage development tool, and the released accelerator dataset can be useful to the community;+ This paper is the first to consider unseen applications in a zero shot setting when it comes to accelerator optimization.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors conduct experiments on SVNH, CIFAR10/100 datasets to verify the approach s efficacy. The authors investigate the calibration properties of focal loss, observe that fixed $\gamma$ is not optimal for better calibration,  and find a correspondence to adjust $\gamma$ in each iteration adaptively. 2.The authors present many figures and investigation experiments for analysis and verification of effectiveness. On the other hand, the optimal hyperparameters for the proposed method may also vary between datasets. Besides,  it would be better to report the large scale datasets  results, such as ImageNet. The current experimental results are not convincing. Some legends even block important information. The authors may present less but important figures with concise conclusions in captions. This paper presents an in depth analysis of the calibration problem and proposes an effective method to improve it. However, the proposed method has not been proved to generalize well in different datasets. Another suggestion is to improve the figures of this paper.<|endoftext|>This paper considers the problem of model calibration. To mitigate the issue, they propose adjusting the hyper parameter $\gamma$ in focal loss according to the model s under/over confidence. # Strong points  The paper proposes an adaptive version of focal loss for model calibration. The proposed method outperforms other baselines on extensive datasets# Weak points  One glaring issue of the current version is the readability of figures. There are even lines with duplicate colors in the second row figures of Fig (2).b. For example, if $C A > k$ for $m$ steps, then $\gamma \ge e^{mk}$, which in turn makes the focal loss rather small. The authors propose to use a threshold to rein in the explosion. However, it seems to me that the explosion will commonly happen, and at most of the time $\gamma \gamma_{max}$. Could the authors provide the dynamics of $\gamma$ during the training process? # Minors  In the second paragraph from the bottom: $\mathcal{L}_f$ should be the Focal loss L_Focal? Classification error on validation set?<|endoftext|>Empirically study on Cifar 10 and Cifar 100 data with various of benchmark networks show better ECE as compared to the other methods. The idea of the paper is to optimize \gamma tuning in focal loss to improve the calibration of neural networks model. The motivation makes a lot of sense. Some concerns to this paper are [1] The paper seems written in a rush and the paper organization and writing is weak. In abstract, FLSD 53 is ambiguous to in its abbreviation when first used  The plots in the paper mostly have tiny legends[2] In table2, it seems AdaFocal  improved calibration but not accuracy in Cifar 10 data, while in the opposite way on Cifar 100 data, as compared to FLSD 53. Overall the paper proposes a new strategy to optimize calibration via tuning \gamma in focal loss.<|endoftext|>The authors propose a new focal loss method of calibrating a model via regularization during training. They propose an adaptive method of adjusting the focal loss parameter by empirical validation performance which adjusts the parameter based on specific bin over/under confidence. The experiments are extensive and cover important calibration and OOD metrics. Figure 5 is too cluttered and extremely hard to read  Table 1 and the preceding paragraph: In both places, the text states that AdaFocal’s performance is averaged over 5 runs, which implies that the baselines are not. How many runs were the baselines trained for? This would explain why there is not much improvement with temperature scaling and AdaFocal. Section 5: During majority $\rightarrow$ during the majority  Section 5.1: hyperparameter$\gamma$ $\rightarrow$ hyperparameter $\gamma$  Section 5.2: this update rule do $\rightarrow$ this updates rules does  Section 5.2: reign in $\rightarrow$ rein inThe authors do a good job in motivating their method and empirically showing why it is needed and why it should work.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The authors proposed to leverage static and dynamic sparsity in efficient robust training. The proposed methods can significantly mitigate the robust generalization gap while retaining competitive performance (standard/robust accuracy) with substantially reduced computation budgets. The paper investigates two sparsity forms: static and dynamic.<|endoftext|>The authors show that injecting appropriate sparsity forms in training could substantially shrink the robust generalization gap and alleviate the robust overfitting, meanwhile significantly saving training and inference FLOPs. This paper lays out a series of options to mitigate that barrier for adversarial training (AT). Their proposed methods are found to reduce robust generalization gap and overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR 100 with ResNet18; and similar competitive performance on other settings. Here I have only two nitpicks. post rebuttal  I have read the rebuttal and the rebuttal addresses my concerns.<|endoftext|>This paper studies an important topic of improving adversarial training with different sparsity forms, and the authors made positive discoveries that injecting sparsity properly would make a win win between efficiency and generalization. (+) The idea is very interesting and promising. It is reasonable to find sparsity helps both reduce overfitting and improve training efficiency. They considered two alternatives for sparse adversarial training: a static Robust Bird (RB) training, and a dynamic Flying Bird (FB) training. (+) It is great the authors show that their proposed methods can be combined to boost previous SOTAs. The authors did compare with Fast AT, but that was placed in the Appendix only.<|endoftext|>This paper proposes two methods for learning a sparse architecture called robust and flying bird. I would recommend the authors to revisit their claims under the light of the existing literature dealing with the relationship between sparsity and generalization ability, and to present their experimental results at a higher accuracy target. FlyingBird improves over Robust Bird in teh sense that the learning mask can be dynamically adjusted over time, i.e.pruned params may be recovered later on. The authors then experiment at training multiple architectures over different datasets in the experimental section, showing better generalization abilities and lower computational complexity (MACs) for their proposed methods Robust and Flying Birds.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The paper also presents a dataset, for offline benchmarking, comprising of 60 different multi cloud configuration tasks across 3 cloud service providers. The paper discusses a direction of research which will become more and more relevant in the years to come. This is one of the key contributions of the paper. SMAC and RB. The authors argue that other approaches cannot be compared against as there is no public implementations available.Thus it not clear how this method compares against the SOTA approaches. The key contributions are: 1) CloudBandit a best arm identification problem for multi cloud configuration optimization. Also there are some weaknesses in the evaluation results and the comparision with SOTA is not clear.<|endoftext|>This pager presents CloudBandit to solve the multi cloud selection configuration problem. While the problem is heavily explored in several domains, this paper leverages the hierarchical aspect of the selection configuration problem. Overall, the motivation and idea presented in the paper is clear. However, it is significantly missing in details wrt to datasets, experiments and cloud provider configuration space. 2.Dynamic environment: How will the results change across the cloud providers if the cloud itself is heavily loaded. Overall, the paper is well written. The dataset presented could be meaningful but currently it lacks the rigor in generating the dataset or providing meaningful motivation for selecting "dask" tasks. This paper has the seed for significant contribution but the current quality of work is not sufficient for publication in a conference.<|endoftext|>They test this approach on 60 different configuration across 3 providers using a proposed dataset and show that the algorithm provides cheaper, faster config. They propose an algorithm (CB) to help solve this and compare the algorithm for its effectiveness against SMAC, RB and show that it performs well at solving the optimization problem. The main contributions of the paper are: 1) A method to choose the best cloud provider in a multi cloud scenario to avoid vendor lock in given a task. The authors study the problem of multi cloud configuration in this paper, where the problem can be defined as how to choose the best cloud provider based on runtime or cost for a given workload.<|endoftext|>They built a benchmark for the multi cloud configuration task including  60 multi cloud configuration tasks across 3 public cloud providers. The experimental results show the proposed method can find best cloud provider with best instances and other parameters more cheaper or faster, compared to BBOs. The cons and questions：1) Can you justify the dataset is correct? Is the special character in the multi cloud configuration case helpful to solve the multi cloud configuration problem? In this paper, the authors try to solve the multi cloud configuration problem with an algorithm from the AutoML domain. And it also requires more justification of the correctness of the evaluation data.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; For deep clustering, this paper explores the non contrastive representation learning based on BYOL to handle the issue of the class collision caused by inaccurate negative samples. ProtoCL is proposed to encourage prototypical alignment between two augmented views and prototypical uniformity, hence maximizing the inter cluster distance. Experiments on various datasets demonstrate the superiority of the proposed method. Pros:(1) The writing and organization are good, which makes the paper easy to read and follow. (2) The authors conduct extensive experiments, including the ImageNet to show the superiority. Cons:(1) My main concern lies in the novelty, which is very limited. This paper simply combines two existing unsupervised learning methods, BYOL and PCL, and then applies it to the clustering task. The class collision issue for negative samples contrastive learning methods is well addressed by BYOL instead of this paper. The difference between the proposed ProtoCL and PCL comes from the BYOL. The EM framework is also presented by the PCL. Besides, the neighbor examples based positive sampling strategy has been well investigated in both unsupervised feature learning and deep clustering areas [1,2]. Therefore, the only contribution is the combination and application to the clustering task, which is obviously insufficient. Therefore, the positive sampling strategy in NCC is not new. Please also compare the results with these methods. (3) Though the reported results are very high, the comparison might be unfair. Some existing contrastive learning based clustering methods, such as CC and GCC [1], can also adopted the framework of BYOL and PCL to improve the results. It is a combination of two existing methods.<|endoftext|>This paper proposes a novel deep clustering method with non contrastive representation motivation. The authors provide detailed experimental results to show the superior performance of the proposed method. the paper is well organized. detailed appendix, including ELBO, convergence analysis, experimental setup, etc. large performance improvement. 2. weaknesses the non contrastive representation motivation maybe overstated. From Tab.4, L_{pcl} may be more important than L_{aug ins}. This loss is actually class level contrastive loss, which is proposed in the work (Contrastive clustering, AAAI 2021) the EM based framework is not novel enough. the reviewer would like to see more clustering results in some challenging datasets, such as Tiny ImageNet, ImageNet 50/100/200 subsets (in SCAN, ECCV 2020). Although the reviewer provides some negative feedback in Main Review, the reviewer still likes this paper. Therefore, I will give a positive rating.<|endoftext|>This paper proposes a novel method of self supervised representation learning. To circumvent the class collision issue arose from building a large set of negative samples in contrastive SSL based methods, it is built from non contrastive SSL methods, such as BYOL. The goal is to handle the weaknesses of non contrastive SSL methods, training instability and representation collapse. The two proposed methods for alleviating the two factors are i) augmented positive sampling and ii) optimizing uniformity of representation space via prototypical cluster features computed from k means clustering. * The efficacy of the proposed methods are well proven with the experimental results. ### Weaknesses* Class collision issue arose from k means clustering    I think it s kind of a chicken and egg situation, cause if the space is not well uniformly distributed, the result of the k means clustering would be quite degenerated which would result in a severe class collapse issue. * Lack of analysis on the over clustering results    Based on the results in Table 4 and Figure A5, it seems that the pre defined value of the number of clusters, K, has the largest impact on the performance (more than 10% in terms of accuracy score). It is quite intuitive to think that the more the clusters are set, the better the classification performance (cause there are more exemplars to infer the class), but the results are the opposite of this idea for the CIFAR 10 dataset. I think this paper is well written in that the motivation, the description of methodology and the analysis of the results are all reasonable. Though I feel more details are needed for some factors that i described in the main review section, I think this work is good since the proposed method has pointed out the weaknesses of the previous methods and improved it experimentally.<|endoftext|>The method is tested on multiple datasets outperforming all compared methods. It is unfortunate that the method is not tested on the same settings as PCL in Table 2, but Table 3 does seem to show that there is a large enough gap between the two. The method already shows to work better than PCL in Table 3, so why not include it in Table 2 to make it VERY clear that the proposed way is the way to go compared to PCL? This is personally a must have experiment for me. For example, what happens if you simply add that term to the NCC formulation? Also, from the ablation study, it is slightly questionable whether the alignment loss is necessary. In fact, shouldn t the positive sampling also do the same thing as the alignment loss? The paper is not too hard to follow but could be presented in a more top down way. This might be a presentation style issue, and it does come to personal taste, so please do take this into account only as a suggestion. The abstract does not read well. There are multiple novel terms introduced, "a positive sampling strategy" and "prototypical contrastive loss" that are not explained, which makes it hard to follow. Can be simply written as> First, we relate between neighbouring instances in the learned latent space to improve within cluster compactness leading to less collision of non related classes. The paper shows promising results, with a great motivation. However, there are some issues that can be easily fixed to greatly improve the quality of the paper. Specifically, ProtoNCE, or PCL, should be included in Table 2, and an additional baseline that empirically shows the negative loss harming performance would be great.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The intuitive explanation offered by the authors are not intuitive enough, in particular, for the examples of using rotation as augmentation (the authors state they use rotation in this paper), what are the intuitions of the definition, how different degrees of rotation corresponds to different strengths, and any numerical evidence can be reported? An interesting definition of the strength of the augmentation**Weakness**However, I also have several major concerns about this work, for example   If I understand correctly, Theorem 1 is essentially a re use of the standard generalization error bound with a replacement of the original hypothesis space to the regularized hypothesis space, and then, the main argument is that since regularized hypothesis space is believed to be smaller, then the new bound is tighter. Also, what s the intuitive explanation of Assumption 1?<|endoftext|>Data augmentation is a common technique to improve generalization, especially when data is scarce. I hope the authors would find these comments useful in revising their paper for a future submission. Can you please provide a more intuitive explanation? While some of my previous concerns are addressed, there are many remaining concerns that would require another careful/extensive revision and would require another round of review, which is why I still don t think the paper is ready to be accepted. This is a big discrepancy between the theoretical setup of this paper and the empirical setup, and a discussion around this shortcoming is warranted.<|endoftext|>This is unclear from the introduction and it serves as a motivation for this work. For the latter, it can be demonstrated theoretically that it minimizes the variance over the neighborhood in the instance space defined by augmented samples. I can see that Eq.(1) might provide more flexibility when it comes to the enforcement of label alignment over such neighborhoods but that needs to be demonstrated. The problem is non convex and that might impact the generalization performance. In such cases, augmentation typically adds many more samples and it would be nice to see the results with 10+ additional samples per training instance, as in the case for the experiment in Table 1.<|endoftext|>If the authors believe that this is a central point, a subsection in the main text should be created to provide the details. I think the framework of the paper is original and the ideas are intuitive with several interesting results and implications. The ideas are simple but novel, with several meaningful results and implications. I thus raise my score from 5 to 6
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper proposed a proof of the Monte Carlo Exploring Starts algorithm given by Sutton and Barto (1998) for a class of MDPs. Strengths:    Presentation is mostly clear    A clear comparison to related works    An important theoretical contribution to the field. Weakness:   The assumption of the uniqueness of the policy is not presented in the theorem. More discussion about the empirical results would be helpful. For example, Tsitsiklis s algorithm requires a uniformly sampling of the start state in order to guarantee convergence, but empirically it also works well with a different way to sample the start state. I would like to see how the authors think about it. Some expressions can be improved:  the word "iteration" is not in the algorithm but is in the proof to refer to something in the algorithm. Some typos: 1. Algorithm3 line 9, it should be Q_t(S_t, t, A_t), 2. when citing a paper by two authors, sometimes it lists two authors  names and sometimes it uses et al.Overall I like this paper because of the strengths listed in the main review. There is some weakness but it doesn t weaken the main contribution of the paper.<|endoftext|>This paper studies Monte Carlo with exploration starts algorithm for solving the reinforcement learning problem. As for the results, asymptotic convergence of the algorithm is established without needing strong assumptions in related literature. As pointed out by the authors, the result resolves an important open problem in RL. The proof is simple and intuitive. Main comments:Overall I really like this paper. This paper uses induction along with strong law of large numbers, resulting in a much simpler proof. This paper does seem to be of interest to the broader community of ICLR. Other comments/questions:1 There are several typos, such as (1) "Corrolary 1" should be replaced by "Corollary 1", (2) the sentence before Corollary 1, the "snd" should be replaced by "and". 2 As for the numerical experiments, I am curious to see the convergence rate comparison between MCES and model free Q learning, which is probably the most popular value based RL algorithm in the literature. 1 Convergence rate. While the induction technique can be used to show asymptotic convergence, it is not clear if it can be used to show the convergence rate. It is an interesting future direction to show the convergence rate of MCES and compare it to that of Q learning. The next question is about extending the result to the function approximation setting. I am curious to see if one can show any theoretical guarantees on MCES with function approximation, or if it suffers from the same difficulty as Q learning. I would like to keep my score and vote for acceptance. This paper provides convergence guarantees of MCES algorithm under mild assumptions, hence resolving an important open problem in RL.<|endoftext|>The paper studies the convergence of Monte Carlo Exploring Starts (MCES), in which the Q function is estimated by averaging Monte Carlo returns and the policy is defined as the greedy policy w.r.t.this Q function. The authors provide a technically simple proof of the convergence under different assumptions on the underlying MDP: (i) stochastic feed forward MDP (in which states cannot be re visited in an episode); (ii) optimal policy feed forward (in which under the optimal policy states are not re visited); (iii) finite horizon MDPs. Basically, it is required that (in the less restrictive case under the optimal policy) the states are never re visited along one trajectory. The authors succeed in providing justifications for these assumptions, especially providing several realistic examples in which the assumptions are fulfilled. Nevertheless, I think that these assumptions are ensuring that within one episode a single (s,a) pair will be updated at most once, significantly simplifying the analysis. My main concern is about the significance of these convergence results under such strong assumptions. To put it another way, is this analysis a step towards the understanding of MCES for general MDPs or just the analysis of a particular restricted case? I think that this holds only when the reward penalizes the steps needed to reach the goal state, not for general rewards. Lemma 1: I think that some condition on the almost sure boundedness of the involved random variables is needed. However, given that the considered assumptions are, in my opinion, very restrictive, I have concerns about the significance of the theoretical results. For these reasons, I opt for a borderline score.<|endoftext|>The paper studies the Monte Carlo Exploring Starts algorithm on MDPs where states are never re visited and provides convergence results for such MDPs. The paper is written well; I enjoyed how it provides a thorough yet accessible introduction to the MCES algorithm and known results for it. The theoretical results are also clear and intuitive, as well as their proofs. I agree with the claim that the main advantage here compared to previous work is in using all states of the trajectory for the update, instead of only the first one. With that said, this seems like a classic case where the simplicity and elegance of the theory is because it is indeed simple and easy to derive. Once you limit the environment to a DAG and are able to sample each state action pair indefinitely, it seems obvious to expect to converge to the optimal policy. This follows from the basic dynamic programming concept where you begin from the last state and gradually update your value function to being optimal. In that case, you will not necessarily sample the terminal state or its neighbours and a probabilistic analysis would most likely be needed. Alternatively, if the rate of convergence would have been analyzed, we could understand how the sampling procedure affects convergence, which might help guide more efficient variants of the algorithm. Lastly, two questions: I m not sure that AlphaZero updates all states within a trajectory instead of just the initial one as claimed in the middle of p.2. The combination of the highly restrictive realm of tabular MDPs in which states are never revisited, together with the strong assumption of ability to sample every state action indefinitely, generate an easy problem that is not particularly hard or interesting to solve.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This is clearly an interesting approach that could lead to improve conformal predictors, which are gaining increased attention in ML (but existing as a consolidated approach since about 15 years). Conformal prediction is a method that intends to provide set valued, calibrated prediction, in the sense that they cover the true class with a guaranteed marginal statistical accuracy. In terms of accessibility to a larger audience, this is clearly a weakness that could be resolved by being more pedagogical (possibly in the appendices). The paper proposes a differentiable scheme to learn conformal predictors.<|endoftext|>2.The proposed method combines several novel elements and appears to be theoretically sound. In this paper, the authors integrate this post processing step into the training procedure in order to reduce the inefficiency of conformal prediction while providing the same coverage guarantee. Conformal prediction methods generally operate via post processing on black box classifiers to produce a confidence set.<|endoftext|>In split conformal prediction, conformalization is applied using a model that has been trained on a separate training set. To my knowledge, the method represents a novel contribution. ## StrengthsThe paper builds a convincing case for the proposed method by calling attention to the mismatch of goals in traditional training procedures and conformal prediction.<|endoftext|>* The goal is to reduce the size of the prediction sets generated by conformal inference   and more specifically to reduce the size variably in different parts of the input space. **Strong points:*** The idea of putting conformal inference into the neural network training loop is interesting. * The goal of reducing confidence set size is important.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper provides the finDML (fairness in non balanced DML) benchmark in the scope of deep metric learning to characterize representation fairness and provide the method for reducing subgroup gaps in deep metric learning methods. 2.This paper studies a new problem and provides a new perspective for the research in this field. 3.The experimental results are sufficient. This paper is well written and the proposed problem is intrersting. The innovation of the method is general, but it systematically studies a new problem, so it is recommended to accept it.<|endoftext|>The paper investigates the fairness problem in the deep metric learning task, which is underexplored by the research community. The authors propose finDML to benchmark previous methods on multiple imbalanced datasets with three newly proposed metrics. The experimental results show that all previous deep metric learning methods have the fairness issue, i.e., larger performance gaps across different subgroups. The paper is well written and easy to follow. After Authors  Response:I appreciate the authors’ feedback, which addressed my concerns.<|endoftext|>This paper proposes three measures that evaluate the fairness of learned representations in multiple aspects. The authors empirically demonstrate that the existing metric learning approaches become less fair (i.e., shows larger performance gaps between attribute based subgroups) when there is a class imbalance in the training data. First of all, the paper addresses an important problem, i.e., the fairness of the learned representation in deep metric learning. In addition, they provide extensive experiments on five datasets with several DML methods that shows the decreased fairness for imbalanced dataset and motivates this work. Most of my concerns are addressed.<|endoftext|>The paper presents a study on the effect of training dml techniques on imbalanced data and show the negative impact of learned representations on the downstream tasks. The fairness is analyzed through 3 properties of the representation space; a) inter class alignment, b) intra class alignment, c) uniformity showing that the bias in the upstream task (dml) is propagated to the downstream classification tasks even when the data for training the classifier (downstream task) is balanced. The experiments are conducted on 4 benchmark datasets. Given the nature of current datasets and the bias with the current machine learning techniques, this is an important topic to explore. Although the proposed technique is currently applied/evaluated under restricted settings, this is one of the first work to do so for deep metric learning.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; rating score: 3; The authors propose a normalisation method for cross lingual text representations. The goal is to normalise the monolingual embeddings based on spectral normalisation. The study shows that produced text representations keep their meaning and improve performance on downstream tasks.<|endoftext|>The paper proposes a new spectral normalization technique that improves the cross lingual mapping of monolingual embeddings by rigid, orthogonal transformations. Any idea where this large effect comes from or whether this is due to the particular language or dataset? This is an excellent sign that the spectral normalization does something reasonable and tends to help. This raises the question of how the proposed SN compares to *other normalizations*, especially on downstream tasks.<|endoftext|>Building on these results, the paper proposes a normalization method that regularizes the spectral properties of monolingual word embeddings. Strengths:  The proposed method is simple to implement. The experiments are extensive and confirms that the proposed normalization method consistently improves cross lingual word embeddings. The paper proposes a simple method for improving CLWE with good empirical results.<|endoftext|>This work is not an appendix on prior publication. The main contributions are (a) the introduction of a simple, portable algorithm to improve pre processing of embedding spaces for cross lingual alignment and (b) empirical improvements in multiple domains of cross lingual embedding oriented tasks. The work does not make a self contained case for what  spectral normalization is a solution towards. The breadth of domains of study is commendable as the authors have clearly taken great care to illustrate that their method has wide applicability.<|endoftext|>This paper describes three new methods for normalizing word embeddings before cross lingual alignment. The second method does mean centering, SpecNorm, and L2 normalization. Experiments on a wide variety of tasks generally show improvements. I C+SN+L is a more complicated method. Besides being difficult to follow, it creates a feeling that the results are selectively reported.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper studies the performance of contrastive learning under both poisoning and backdooring attacks. This paper focuses on the multi modal classifications, especially on the recent multi modal classifier CLIP. The paper is easy to follow and provides a very detailed study of this behavior.<|endoftext|>The authors show that this use of "uncurated" data makes contrastive learning vulnerable to poisoning and backdoor attacks and they show that poisoning even a small number of instances can be very effective. Are the threats to contrastive learning simply artifacts of that dataset or are they also present in other multi modal datasets used for contrastive learning? I agree with the authors that their findings are especially alarming given the fact that contrastive learning is often used on uncurated data.<|endoftext|>The paper describes poisoning and backdooring attacks on CLIP, a recent method to (pre) train multimodal networks with a contrastive objective. I only have a small number of clarification questions: * Why do you think there are dips in performance in Figure 2 with increasing number of samples?<|endoftext|>Meanwhile, the difficulties of the poisoning attack heavily rely on the type of supervision provided (the authors also discuss part of this in Section 3.2). While the generation method is quite simple, the attack success rate is impressive (only 3 out of 3 million images to conduct target poisoning attacks). (2) The proposed method is simple but effective, and the experimental results are convincing. In particular, it designs an image text pair generation to poison the dataset, driving the model to misclassify a particular test input or a group of images with a small patch.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper applies lexicase selection towards training deep neural networks using a hybrid evolutionary gradient descent optimization method. Results in a variety of benchmark image classification tasks demonstrate that the proposed method can improve upon standard stochastic gradient descent methods. The paper is very well written, flows smoothly and is a pleasure to read. I presume this could affect the exploration/exploitation tradeoff quite noticeably. The proposed method builds on ideas of lexicase selection from related domains and applies it successfully to train deep neural networks on benchmark image classification tasks demonstrating improved performance. The work is novel and potentially valuable to the community.<|endoftext|>This paper presents a neural network training strategy that leverages lexicase selection and evolutionary algorithms. This is just one experiment I can quickly think of, and I think the authors should consider including something like this to enhance their claim. What can I learn from this? The experiments only showed accuracy as the metric. 1) Even for accuracy, the proposed method (with the significant sacrifice on training complexity), in some cases, performs poorly compared to the baseline. 2.Please include experiments that justify the reason of the performance improvement (see above). Please consider to remove it and make more discussion to justify the claims.<|endoftext|>Authors propose a method to optimize deep networks through a combination of gradient descent and a population based mechanism. In particular, authors propose an adaptation of "lexicase" selection, an exotic selection mechanism proposed in evolutionary computation, in order to be used in the context of deep learning and deep networks optimization. The paper is well written and organized. Despite all above, the major flaw of the paper are the empirical assessment and obtained results, that in my opinion, render this work unsuitable for acceptance. Although the proposed approach seems interesting enough, it is simply no attractive enough to switch from SGD. The performance improvements over baseline methods are too marginal, nor the method is novel enough, for the work to be considered above acceptance threshold.<|endoftext|>It seems that the computational cost of the proposed method is about $p$ times higher than that of naive training because the authors set the number of training epochs to $200(p + 1)$. In the proposed gradient lexicase selection, a parent network is selected based on the lexicase selection from several candidate networks trained using different subsets of the dataset. The proposed method maintains several deep neural networks in training. [Strengths]  This paper is well written.
Reject; rating score: 5; rating score: 5; rating score: 8; rating score: 8; The proposed model takes as input a pointcloud and predicts a set of part poses as implicit functions. 2.For the experiment in Table 4, can the authors clarify how is the F score computed? My major concern for the paper is related to its clarity. First of all, the goal of the proposed method is not crystal clear. Can the authors provide some intuition regarding why this is the case? However, if the authors address my concerns I am happy to increase my initial score. Intuitively, I would have expected the proposed method to perform comparable to BSP Net, however BSP Net seems to be significantly better. My intuition is that for BSP Net the authors are using significantly more primitives, thus the discrepancy. For the final version, I would like to see additional qualitative results on more object instances for the various categories.<|endoftext|>This paper addresses the problem of unsupervised parts decomposition for articulated objects with kinematic structures. 4)	Is the parameterisation of the part pose correlated to the part semantics? Based on the listed numbers, the proposed method does not achieve the best performance in terms F score and Chamfer distance. However, the paper is not well written. The results are not quite promising as demonstrated in Table 4.<|endoftext|>The paper presents a method for unsupervised generative part decomposition based on part kinematics. In terms of part pose estimation, it achieves comparable performance to the NPCS which assumes part segmentation supervision, even though the proposed method is unsupervised. Additionally, the part parsing produced is more parsimonious and intuitive qualitatively, in terms of interpretability. Also, the authors have made significant effort to assure that comparison with other baselines is fair. Regarding reproducibility, all the implementation details are provided, making it possible to reproduce the results. Another issue of the overall presentation regards the hyperparameters.<|endoftext|>This submission proposes an unsupervised method to segment point clouds of objects that are articulated. A more fair (or additional) comparison would be to use this method on non articulated objects and compare it to BAE and BSP. The symbols used for encoders and decoders are too cryptic and hard to parse while reading the paper (e.g.$F^{p,z}$, $F^{p,c}$, $F^{s,z}$, etc.). I d encourage the authors to rename those to more meaningful names to ease reading. The paper is well written and the results look convincing.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The paper has the following results:  All verifier leading Stackelberg equilibria in a verifier leading sequential PVG formulation in which the problem instance is revealed after the verifier picks its strategy give a desirable proof verification protocol. Strengths  The paper does a good job of formalizing the various PVG settings and relates the effectiveness of the settings in realizing the goal of finding a desirable prover verifier system. It is not clear if this would transfer well to more complicated settings. Although the theorems stated in the PVG formulation apply to a more general setting, the convergence guarantees for the games seem to need a lot more work. For the empirical evidence justifying the effectiveness of this strategy, I would like to see it applied to a more practical setting. I feel the paper introduces a novel approach for learning prover verifier systems. Still, I think the theoretical and experimental treatment of the topic in the current version of the paper is not rigorous enough at the moment. I feel the authors need to address the comments above.<|endoftext|>This paper proposes a new learning methodology for training neural networks based on Prover Verifier Games (PVGs), which are inspired by interactive proof systems (IPS). This paper studies eight possible game instantiations, depending on player order and when the problem instance is revealed, and the theoretical analysis shows that two instantiations are superior to others, which is also confirmed by an empirical evaluation. Weaknesses:  chosen tasks in the evaluation seem fairly simple, so whether there is a great practical potential of the new methodology is not yet clear;  the claim of learning in a "checkable" or "verifiable" manner could be somewhat misleading, as it seems _not_ really verifiable in the sense of classic software analysis and verification (i.e., a piece of code meets its formal specification for all possible executions). The learned protocol and messages seem to be embedded in neural networks and real valued vectors, so how could they be interpretable? The idea proposed in this paper is novel, and its effectiveness is backed up by theoretical analysis and experimental evaluations. Although no strong practical results have been shown yet, the idea deserves to be published soon. I would like to recommend acceptance for this paper.<|endoftext|>Based on the ideas lying behind interactive proof systems (IPS), this paper proposes a prover verifier games (PVG) framework, which aims at solving decision problems in a verifiable manner, where two learning agents interact with competing goals: a trusted verifier agent tries to get a correct answer to the problem while an untrusted (but more effective) prover tries to convince the verifier that its answer is correct, regardless of its actual correctness. First of all, I have to admit that I am no expert in this area and hence I do not feel qualified to fully assess the merits of this paper. Also, for this reason, I find the paper really hard to follow. The presentation of the material does not offer a reader any help in this regard. The paper is not self contained and a reader is supposed to go from one paper to another in hope of grasping what the authors are talking about. The notation and the problem being studied aren t exemplified nor well motivated either (as an example, what is PAC verification?). Having said that, the paper seems to offer a rigorous theoretical study with a wealth of propositions, theorems and their proofs (there are several appendices attached). (In this sense, I am inclined to think that another venue might possibly fit this work better than ICLR.) To summarise, although I personally find the paper close to impossible to follow, I don t really feel in a position to judge it based solely on this fact simply because I am not an expert in the area, and it is unclear to me what presentation is deemed standard here and what level of preliminary descriptions is accepted as adequate.<|endoftext|>The authors also applied their framework in two experimental settings: the binary erasure task and the cross detection task, and show that the method would results in robust and interpretable decision rule. I don t have a good sense of the significance of the theoretical contribution and would allow the other reviewers who are more familiar with the topic to comment on this point. **Request for clarification:**I am having some trouble understanding the practical usefulness of the proposed framework, and could really use some clarification from the authors. However, I don t think the experiments have demonstrated effectiveness in these two settings. For example, one can simply look at the saliency map of a naive classifier on the dataset and should be able to identify the cross region quite easily or one can simply ablate each region of the image and study how the ablation changes the model s prediction. With respect to robustness, the author demonstrated the robustness of the verifier s answer with respect to the prover s message, but I am having trouble understanding why being robust to the prover s message is important. For the most part, we care about robustness with respect to changes in the input distribution as machine learning systems are deployed in a different environment, but in what scenario would robustness to an internal prover s message be useful? updates after rebuttal After the author s clarification, I am able to understand the theoretical contribution more, and found it to be a valuable contribution. The empirical weakness of the paper remains. However, I have updated my scores to a 6. The goal of coming up with a neural network that can justify its decision is an important task.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; This work introduces a novel method for self supervised image animation using a source image for the identity of the subject to animate (e.g., a face) and a driving video, involving another subject, showing the motion to replicate (e.g., another face speaking). Instead, the authors rely on an implicit representation of the motion as a trajectory in a learned latent space. The two transformations combined can be decoded into the desired animation. Overall I found the paper quite interesting and the results are quite impressive. I’ll list below some of the strengths and weaknesses of the paper in the current form. STRENGTHS+ The proposed method is significantly more streamlined w.r.t.to the main competitor that relies on breaking the motion to animate into smaller local motions. **b.** It’s not clear why the two images $x_s$ and $x_d$ should be processed by the same encoder with shared parameters.<|endoftext|>This paper presented a method for animate images of a certain category, e.g., faces or human bodies. Different from some previous work with explicit structural representations, this paper focuses on learning a set of linear latent space directions that controls image animation without explicit structural representations. 5.Some Typos exist in the paper. The paper conducted a re animation experiment on the human face and body motion dataset. The idea is novel to me. 2.Experimental results are generally supportive. The quantitative results also suggest the proposed method outperforms previous art. I have some concerns about the paper as follows:1. I think it might also be beneficial to show all of them.<|endoftext|>This paper proposes a self supervised auto encoder based pipeline for animating images via latent space navigation. Results are shown on the task of (1) source reconstruction from video. The video for the real world case is impressive. 3.The main contribution of the proposed method is to use a learned linear motion dictionary, and can improve the generated image quality. However, I have found some considerable issues as described above.<|endoftext|>The paper proposes a method to animate images using a guide video. The authors pose the problem of synthesising motion as that of traversing the latent space. For example, linear displacement of the latent codes using a motion basis is also used in [2] for video generation (not re enactment). However, I believe that the significance of the paper lies beyond the novelty of the method itself. Therefore, I would like to see some experimental comparison with some of these works, as well as discussion regarding the differences and advantages of the proposed method. It would be helpful to see some qualitative results containing failure cases. I lean towards accepting this work if the issues that were raised are resolved.<|endoftext|>This work proposes LIA, a self supervised AE that animates images by linear navigation in the latent space. The second step decodes the target code to dense optical flow and warp with the source image. The framework is trained in a self supervised manner. Experiments demonstrate the effectiveness of the proposed method on high fidelity datasets. Visualization of some basic transformation $d_i$ or the generated optical flow can also be beneficial in showing the effectiveness of the motion dictionary. This work proposes a relatively novel method that aims to animate still image via latent space navigation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; Anchor boxes are also adjusted layer by layer to learn the optimal anchor setting. + The proposed query formulation introduces marginal computational overhead compared to other formulations in terms of GFLOPs. Although more analysis and experiments could further improve the manuscript, overall this is a good paper.<|endoftext|>Therefore, I am leaning towards a reject for this version of the work. I have provided more details on how to improve the paper. 6) Hyperparameter study is limited to temperature and number of decoder layers. However, these are not provided side by side in comparison to other methods, So that the proposed method can be judged on its capacity for improvement compared to the alternatives discussed in the paper. The empirical approaches are lacking as also described in the main review which damages the credibility of some of the claims.<|endoftext|>Based on the analysis of the learned queries in DETR, the authors conclude the slow convergence of DETR is likely caused by the multiple mode property of the queries. This paper only makes a combination of those methods, while offering little novel insights. The observations in Section 3 are almost obvious, and the reviewer did not feel that new information was brought to the table after reading. This is incremental work.
Reject; rating score: 5; rating score: 6; rating score: 8; This paper proposes a complex valued DP SGD mechanism, named \zeta DP SGD, to train privacy preserved neural networks on complex valued data. Pros: This paper is well structured and its motivation is clear. The experimental results show that the proposed algorithm is possible to achieve high utility under tight privacy guarantees. Cons: More significant results are needed. My main concerns are as follows:  On the theory side, the idea seems too natural. It would be better if authors can provide a tighter bound for the privacy loss. On the experiment side, to make the results more convincing, please provide more comprehensive experiment results, such as accuracy for differenAlthough the privacy preserved complex valued DL is an interesting problem, I’m not sure this paper reports enough contribution for pushing the development of this research field.<|endoftext|>The authors also show how to adapt the private gradient descent algorithm into a private algorithm to minimize real valued functions, defined on the complex plane. It is then shown, experimentally, that the new algorithms and notions perform very well on a variety of tasks related to signal processing (and hence to complex valued functions). However, I would like to contend that there the introduced ideas $\zeta$ DP and complex Gaussian mechanism are not inherently complex. The complex Gaussian mechanism reduces to the standard Gaussian mechanism when the dimension is doubled. In light of the above though, it seems that the main contribution of the paper is to suggest an alternative to DP SGD, at least when the dimension is even. As the authors note this can be due to the extra information contained in the additional dimensions. While the authors do use neural networks (none of which are particularly deep) in their experiments, it is not the main focus of the paper.<|endoftext|>For this new privacy, a new Gaussian mechanism allows novel DP optimizers to be developed. Overall, the paper is very well written and insightful. are very clear and exciting. So this is moderately interesting. The true highlights of this paper are the new Gaussian mechanism (as well as the new DP optimizers) and good motivation of complex valued deep learning. Another key part is the optimizer. If there is no fundamental difficulty, the authors should discuss $\xi$ DP Adam/Momentum/Adagrad and new clipping method like global clipping. First of all, MNIST is too simple even for DP learning. I would expect to understand the performance on CIFAR10. Notice that while DP learning can get 99% on MNIST, it at most gets 70% on CIFAR10 without transfer learning. Why MNIST has accuracy but these tables have ROC? It provides new DP notion, new Gaussian mechanism, new DP optimizer and exciting experiments. Minor weakness exists: the extensions are not well discussed and experiments can be enhanced.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposes a new pre training technique to induce a logical prior in the language model representation. Do the authors just used a pre trained model (BERT_{base}) and then trained on their logic pre training objective for 200k steps?<|endoftext|>This paper proposes to add the dependency parsing information of sentences as part of the pre training objectives to the pre training of BERT. 4.The logical connective masking proposed by the authors is very strange. And the current state of the art dependency parser is even built based on pre trained language models.<|endoftext|>I like the self supervised approach to logical reasoning in pre training. My major concern is the clarity of the paper, which hinders some explanation of methodological choices and proper understanding of the technical approach. If the baseline model is fine tuned for 200k steps, why is that a sufficiently fair comparison with your pre trained model with 500k steps?<|endoftext|>To enable pre trained language models to capture logic relations contained in natural language, this paper presents a pre trained language model, PROPHET, which aims to encode logic relations (facts) with three pre training objectives. This paper proposes a new method to enhance the logical reasoning ability of PrTM via syntactic parsing that avoids complex knowledge injection.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; It  considered the spurious surprise states in execution and proposed to address this problem through the proposed free energy minimization framework. Strengths: The presentation is clear and easy to follow. The empirical results and ablation studies are sufficient. As the authors only defined observations $O$ for each agent in Sec 3.1, is  $\sigma$ the deviation within all agents’ observations? 4  Table 1 shows that EMIX improves the winning rate in most of the considered tasks. But it seems the improvement is marginal. This is also the case for the results in Figure 3. 5  Only using SC II micromanagement is not convincing enough. It is better to include results on other different benchmarks. This paper is attempting to extend the surprise minimisation to the multi agent learning regime. My major concern is that Eq.(4) seems to be solely a marginally new way of representing the Q function. It is not different from the typical Q Mix or Q learning. The core message of how Eq(4) or Eq(7) can minimise surprise is not clear to me either.<|endoftext|>The method uses insights from energy based models, deriving an energy operator that forms a contraction operator on the appropriate value functions, and uses this to make a variant of QMIX, which promotes surprise minimization across the multi agent system. This work builds in the interesting new directions around surprise minimization and energy based models. Though the theoretical results are strong, the empirical results are not as compelling. For an evaluation of this method, I would expect some sort of qualitative evaluation showing that the agents in fact behave in a way to minimize surprise. It seems like most of the error bars overlap, so the results are statistically insignificant, but only one of the methods is bolded. To be clear, I do not think that the method needs to surpass all others on success rate, it isn t designed to make those sorts of improvements, but because that is all we have for empirical evaluation the best I can conclude is that the intervention into QMIX does not break the method. As it stands, I understand from the introduction I understand that EBM are a natural way to approach surprise minimization in multi agent RL, but it isn t clear to me why you would want surprise minimization in multi agent RL particularly, and what is different about multi agent RL that would effect the sorts of behavior we would expect out of surprise minimization.<|endoftext|>I am fairly confident that the authors have a strong technical understanding and justification for what they have done, but this is not expressed clearly in the paper, at least not in a way that I can decode. ## Update after rebuttalMy concerns have been partly addressed by clarification on what measures of surprise can be used and how these would be estimated. It is clear that the results show advantages over the other SoTA methods, but I am not sure that the method could be replicated by a reader and so I am unsure whether these experiments could be validated. p2 3: Is  the joint action a cross product over identical action spaces? Or is it probabilistic but the same distribution for all agents? Or does/can it depend on the agent? The policy and value function seems to depend on the hidden state. Also, sampled from memory R is ill defined. Maybe I am missing something.<|endoftext|>Can the authors show some laws or insights on how to select $\beta$? In summary, this work proposes an interesting problem and give a thourough analysis on the proposed method. However, due to the multiple concerns on the theoretical and experimental results, at the moment I can only recommend reject (but marginally below the threshold). Also, the authors showed that the convex conjugate of the suprising value operator is analogous to the minimization of the uncertainty among agents. The writing of this paper is clear enough and the presentation manner is concise. 3.The comparison between the proposed method and other related works is good. 4.Ablation studies are conducted to demonstrate the effects of hyperparameters. In my view, it could be more appropriate to write this paper as a single agent RL work and the contributions would not be reduced. For now, this work is actually a theory based on the single agent scenario and an trivial extension to the multi agent scenario given the assumption of the global suprising value being equal to the sum of decentralized surprising values. 2.As the authors said in Theorem 3, the proposed surprising value objective can be reduced to the soft Q learning objective. The biggest concern from me is that it still needs to learn the thermal equilibrium of the surprising value. Could the authors discuss more about this issue? 4.The authors have compared the proposed algorithm with MARL algorithms, but most of them are irrelevant to the surprise term or intrinsic reward (as the authors claimed the relation in the paper). Can the authors give a convinciable reason or rerun these results by the latest baseline implementation from Pymarl.
Reject; rating score: 3; rating score: 3; rating score: 5; Why do NaNs appear in some of the tables? The paper concludes with experiments on an "acrobot" simulation. Ordinarily I would avoid commenting on writing style in a review, but it must be addressed. 2.Further, if we are to understand the above comment as the main contribution, the issue is that "the guide" is a concept, not a new concept at that (just a renaming), and this paper doesn t clearly state a hypothesis for any particular implementation.<|endoftext|>The paper empirically shows that the proposed changes involving exploration, bootstrapped returns, and other additions improve performance of dyna algorithms in Acrobot. However, this seems like a limited result as currently presented. In addition, the changes proposed here, especially the automated exploration techniques are not theoretically investigated. The main contribution of the paper are the empirical results on Acrobot. With only a single domain studied and marginal gains I think more studies are needed here. Lack of theory and concrete description of the hyper parameter less explorationOrganizationally, the paper seems unfocussed. The related work section lists many different dyna style algorithms but it is hard to find most of them in the actual algorithm results.<|endoftext|>The studied problem on an approach to sample efficient MBRL is clearly a very significant problem in modern research on RL. The authors have nice ideas, they enhance the traditional MBRL approach with two models (called the guide and the explorer), the guide is based on a Dyna style policy, applying of which is known to be challenging in a long horizon setting. An improved version of the paper may have a major impact. In particular, why it is necessary to define and name an MBRL study subgenre, rather than use the existing framework of online/offline MBRL. However, I would appreciate a formal statement and assumptions behind the studied iterated batch RL setting. Table 3 given in the appendix indicates that the authors did not, in fact, perform an exhaustive hyperparameter search.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; PDT is then evaluated by fine tuning on the same task. The specific setting that this paper operates in though is quite odd. This also means that the offline datasets are biased towards high reward trajectories. (Task transfer)   * Pre training on in the wild videos. I would recommend authors to use a more appropriate experimental setup, or show results for transfer, which would make a much more compelling case. The paper has major shortcomings with the experimental setup considered to evaluate the efficacy of pre trained decision transformers, and falls short of outperforming simple baselines.<|endoftext|>Similarly, there need to be error bars in Figure 2. The authors present a simple strategy for such PT for RL that shows benefits with limited reward annotations. It seems to me that you have largely just split the training phase of the DT into a PT piece where rewards were occluded then a FT piece with rewards present. There are several previous approaches to this problem, including some the authors readily identify in their study, such as the CQL+BC method which uses unlabeled data to perform semi supervised offline RL by adapting policy prior data via unlabeled data. ### How do they solve it? The authors propose using the (already published) decision transformer model, but pre training that model first on a dataset with only action sequences, without any reward information. The authors compare their proposed framework against existing models, including both the traditional DT, a BC baseline, and the CQL and CQL+BC methods (the latter of which is a semi supervised approach). This is relevant because your performance #s don t appear to match those in the decision transformer paper (for the DT or CQL runs). 2.I m not as convinced by your results.<|endoftext|>This paper introduces a self supervised approach in deep reinforcement learning with decision transformers, which is widely used in CV and NLP tasks. There should be an explanation for this change. Why not use the real rewards from environment at evaluation time? For pre trained methods, we use the entire dataset in the D4RL benchmark to learn the behavioral prior. As for the experiments results, I noticed that in some cases, when there are 25 trajectories, the performance is worse than 10 and full, and the gap is quite big, what is the reason for this? The paper introduces a self supervised approach to improve the performance of decision transformers in deep RL when data is limited.<|endoftext|>This paper presents Pre trained Decision Transformer (PDT) for semi supervised offline reinforcement learning. PDT first pre train a decision transformer model on the trajectory dataset without rewards, and then fine tune on a smaller dataset with reward annotations. 2.The proposed method is simple. The pre trained decision transformer model has already achieved sufficiently good performance without reward annotations. Is it better than the reported PDT results? If the pretrained decision transformer outperform the PDT fine tuned on smaller downstream datasets, then the fine tuning is not that significant. Despite its simplicity, the novelty and significance of the proposed method are limited. Hopefully the authors can address my concern in the rebuttal period.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The following are the strengths of the paper:(+) The paper presents a solid theoretical result for a very (at least mathematically) natural problem. After the rebuttal the paper is more of a 7 for me.<|endoftext|>This leads to the first DP streaming algorithm for this problem with polylog space. The result is interesting and satisfying. Strong and interesting results.<|endoftext|>But this is not the case with this paper, rendering the papera little bit dangerous for the DP research community. The proof of Theorem 3 assumes that $\rho_p\geq 1$. Their algorithmprovides, sometimes, vacuous privacy guarantees and inadequate accuracy. This is a valuable contribution to the DP literature.<|endoftext|>What theorem in that paper gives this result? By now this is known for the Johnson Lindenstrauss sketch of the $\ell_2$ norm, and the Flajolet Martin distinct counts sketch. The result is technically interesting, and provides further evidence that techniques from sketching are inherently privacy preserving.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; * The privacy objective is defined as a function of KL divergence with respect to any underlying probability distribution on the perturbed gradients. This appears as a rather difficult objective to achieve in general. In fact, the paper then assumes a Gaussian distribution as the underlying distribution (and the perturbations) for each of the classes while evaluating the objective function. This assumption is not clear. The first thought that comes to mind is the central limit theorem, but then, as mentioned in the paper, one class may have very few samples (e.g., the disease prediction example). Moreover, it appears that the empirical mean of each class is treated as the true mean. However, the validity of this assumption would also depend on the number of samples collected for each class, which could be highly imbalanced between the two classes according to the motivated problem setup.<|endoftext|>Together they wish to jointly train a model in such a way that the non label party does not learn of the users  labels. The paper [Ghazi et al.2021] you cited also yield label DP in the local setting without aggregation (by flipping the labels). In fact, *local* DP would still be applicable to non aggregate data. I think this point has to be clarified in the revised version. For the objective, the authors show that AUC is upper bound by a certain quantity involving the KL divergence of the distributions of the gradients from the two labels. For the utility constraint, the authors instead try to restrict the amount of the noise added by having an upper bound on the trace of their covariance matrices. Furthermore, the authors show that when the (unperturbed) gradient distributions are assumed to be Gaussians and only Gaussian noises are considered, then the optimization problem simplifies greatly to just a single constant size optimization problem (Theorem 2). ## Comments for Authors  As stated above, it would be good to justify the current split learning setting more. And how would such a method, in conjunction with those presented in the paper, effect the utility?<|endoftext|>The authors in the paper consider stealing the private label information from the party that does not know the label of training data during split training for binary classification. The paper considers a novel setting in split learning: stealing binary labels during training and the setting could not be solved directly by differential privacy. Overall, I think the paper makes solid contributions. My biggest concern is that the paper is limited to (class unbalanced) binary classification. I carefully check the proposed attacks, but find it difficult to extend the multi class classification. Besides, since the proposed defense MARVELL at each update step requires solving an optimization problem, the (time) efficiency of MARVELL should also be discussed and demonstrated in the experiments. 2.Time efficiency of the proposed methods should be discussed and more ablation study can be done.
Reject; rating score: 3; rating score: 5; rating score: 5; This leaves the reader guessing as to what the authors are actually performing. The costs and drawbacks of the restricted architectures/weights compared to   say, a baseline with no privacy   is also not reported.<|endoftext|>The motivation is that the existing works have focused on developing cryptographic techniques in a fixed network architecture and have not considered the neural network optimization perspective to enhance the efficiency of existing cryptographic computations. The main weakness is that the technical contribution is limited.<|endoftext|>The paper uses the existing DARTS neural network architecture search algorithm to automatically a ternary neural network for secure inference based on garbled circuits. As such, the paper has a limited novelty from the ML perspective. 3.The paper is also very well written and has a nice flow.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper introduced FastSHAP, a new method for estimating Shapley values, reducing runtime significantly. Instead of comparing the performance, a more extensive comparison of the runtime is needed. I agree that this paper provides a substantial empirical benefit for the XAI literature.<|endoftext|>The paper is well written with strong results, but there is further room for improvement. How can one pick the learning rate of the method?<|endoftext|>Overall, the paper introduces an interesting approach for estimating Shaley values in real run time. The effectiveness of the method is well demonstrated across different tasks/datasets.<|endoftext|>3.The discussion for the experimental results are not sufficient. Since the presented method is based on the prior work KernelSHAP, the detailed information of KernelSHAP should be explained well. The experimental results and the deployment efficiency shows the superiority of the presented method. Please see the Cons in the main review.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposed a new problem called Optimizer Amalgamation and made an attempt to obtain a more powerful learned optimizer from several analytical optimizers. Although the authors claimed that they tried to distill the knowledge from traditional optimizers, it is also related to imitation learning. Maybe the authors can try the gradient matching loss in [1], which is shown to be effective for dataset distillation and works with a similar number of parameters. The paper made an interesting attempt to distill the knowledge from analytical optimizers with three amalgamation methods and conducted comprehensive experiments to show the effectiveness of the amalgamated optimizer.<|endoftext|>This paper presents a new optimizer amalgamation method to combine a pool of optimizers into one in order to achieve stronger problem specific performance. The proposed method is empirically shown to be effective when compared to a large number of baselines. 2.The proposed optimizer amalgamation method is interesting and effective. Weakness Selecting an appropriate optimizer for a given problem is not a new research topic, and many papers exist in the areas of algorithm selection, algorithm portfolio, and meta learning. I think one weakness of the paper is that it lacks the discussion about the related work, and how the paper can be better positioned in the literature.<|endoftext|>This paper discusses the problem of selecting a neural network optimizer from a pool of possible optimizers. * I have some concerns with the experiments discussed in the questions below. The algorithm proposed is well motivated for a specific task, and the authors conduct experiments with various ablations to their methods. It would be interesting to explore further the choice of optimizer pool and its effects on performance.<|endoftext|>4.It would be helpful to add some experiments to justify that it is necessary to distill from multiple optimizers instead of just one since the "small" setting just uses two optimizers anyway and the gain of the proposed approach from baseline might come more from the better stability in training due to perturbation. This work presents an approach to distill several "teacher" optimizer into a "student" optimizer through learning to optimize (L2O). The idea of distilling from multiple "teacher" optimizer is novel although distilling from one "teacher" optimizer has been proposed before as imitation learning in L2O.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper presents a Neural SLAM based approach for tackling embodied multimodal tasks in ALFRED benchmark. The approach, called Affordance aware Multimodal Neural SLAM (AMSLAM), utilizes several modalities for exploration, predicts an affordance aware semantic map, and plans over it at the same time. The approach achieves 40% improvement over prior published work. It is unclear what makes the proposed representation "affordance aware". However, the authors seem to have made many design choices specific to this benchmark which are likely to not result in better results on realistic tasks. What is the meaning of the "actual" task? I can not imagine a robot operating in the real world in this manner. My worry is that the authors have exploited unrealistic approximations in simulation environments (such as discrete action and state space, no noise in motion and pose sensors and use of high level interaction actions), and over optimized the design choices specific to this benchmark, which are likely to not result in better performance in more realistic settings.<|endoftext|>This paper proposes a new framework that improves the state of the art performance of the ALFRED benchmark (accomplishing navigation and interaction tasks given language instructions in AI2THOR environments) by 40% relatively. Results that are 40% relatively better than the state of the art are observed and several ablation studies over key network modules are also presented. the proposed technical module of learning an affordance aware semantic representation is valid, reasonable, and somewhat novel. weaknesses:  my biggest concern is that the paper presentation is not selling the framework well as an easily readable research paper, for example, a) there are no qualitative figures showing the performance of the work, only tables with numbers are presented. b) the writing for Sec.4 can be improved. c) for the experiments, the authors spent many more words on Sec 5.1 and 5.2 than Sec 5.3 (the main results). The main results subsection is very short and does not contain analysis over the tables or show qualitative figures to analyze the results. another of my major concerns is whether or not using the language instructions during the exploration stage is a reasonable and realistic setting. But, the confusion regarding this point prevents me from recognizing the claimed technical contribution on leveraging multimodal inputs during exploration. Please clarify if I missed or misunderstood the main differences from previous works. On one hand, the performance is really good and significant. On the other hand, the two claimed technical contributions are not stated clearly about their significance of differences than the previous methods or the validity of the design.<|endoftext|>The paper proposes a method for solving the ALFRED task (following language instructions to perform a set of household tasks). The model has two main components: (1) Affordance aware Semantic Representation (2) Multi modal exploration. The latter provides an exploration strategy by using instructions, images, previous actions and previously explored areas as input. **Strengths**  The method achieves the state of the art performance on the ALFRED benchmark, which is quite challenging. I am not sure if any of these modules can be used for generic embodied tasks. The paper makes a big deal about the "affordance aware" representation in the introduction and other places in the text, but it is actually a simple heuristic that the agent backs up a few steps when it is next to an object. Several previous works have made similar assumptions. However, those approaches are suitable only for simulation. The paper proposes an approach which is heavily engineered for the ALFRED benchmark and I do not see a major novelty that generalizes to other embodied tasks.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a distributed training technique for GNN. This technique includes local computations done in parallel by several machines and a correction phase done by a centralized server. A theoretical analysis is given for this technique, showing that the server correction phase reduces some irreducible error that happens due to splitting the graph and doing a local computation on each subgraph. The paper is well presented and easy to follow. Theorem 1 & 2 only shows a bound on the norm of the gradients, averaged over T iterations. Can the authors elaborate on this?<|endoftext|>This paper deals with the problem of distributed training of GNNs. Essentially, this method captures the idea of transmitting only local averages but adds a centralized step on the server to account for global structural information lost in the subgraph partition. The paper is fairly well written and the proposed result, albeit simple, is powerful. The authors further provide theoretical convergence guarantees.<|endoftext|>The paper considers the problem of distributed training for graph learning tasks, under a setting where data privacy is significant for each individual machine and communication to/from a central parameter server is expensive. In the LLCG algorithm that the paper proposes, each machine trains on its local graph partition for some time before sending the parameters to the server. The server averages the received parameter, but additionally also does its own training using the full graph available to it. Theoretically the authors show that the proposed method avoids an error gap in the gradient norm that would exist if server correction is not performed. The paper is well written and is enjoyable to read.<|endoftext|>The idea is not very novel. This paper provides the convergence analysis and shows the proposed method can address the residual error. This submission inserts an additional update on the server to eliminate the sampling error. In my opinion, It is difficult to say that this method was designed specifically for training GNN since one can extend this procedure to any dataset. The theoretical results are also not novel. The only additional condition is the sampling error, which doesn t provide any new insight. The motivation regarding the proposed method is not well justified. The motivation discussed in the introduction mentioned local devices lack the global graph structure.
Reject; rating score: 5; rating score: 6; rating score: 8; rating score: 8; LTH (lottery ticket hypothesis) was mainly proposed for unstructured pruning. The key for them to achieve so is the newly proposed post processing techniques, refilling(+) and regrouping. They show by these techniques, structural winning tickets can be found, with up to 6.67x on hardware platforms. They also admitted this in the paper: “There is almost no difference between the performance of IMP Refill and IMP Refill+, and both can not find channel wise structural winning tickets”. This paper is meant to bridge this gap, which could be a significant step forward. 3.From the methodology perspective, the proposed techniques are “post processing”. I would say, it doesn t look like an elegant neat scheme (although “elegant” may not be really important). One contribution this paper keeps emphasizing is that they find structural winning tickets for the first time. However, some important concepts, baseline results are either not established or (potentially) flawed. I do not think the presented results can faithfully support their claimed contribution. It is pretty hard to read the radar plots. This paper does not have this kind of comparison at all. They can be presented in a better way. Many baseline accuracies are far below the average reported by other papers. They claimed what is not really realized in the paper. More than 10% gap. Then, only look at the channel sparsity.<|endoftext|>Starting from unstructured sparse sub networks, the method uses a "re filling and re grouping" manner to enforce the formation of structural sparsity. However, in previous works about the lottery ticket hypothesis (LTH), the winning tickets are typically unstructured. Therefore, the idea of finding structural winning tickets is meaningful, and the existence of structural winning tickets fills in a crucial piece of the puzzle for the LTH. In this work, the authors propose a method to find structural winning tickets. In some experiments (such as Figs.3 and 5), the authors report time saving when using a specific GPU and batch size. The time saving might vary when other GPUs or batch sizes are used. Therefore, I believe it would be better to also include the number of FLOPs in the results. This work demonstrate that lottery tickets can not only be unstructured but also be structural, which makes it a novel and interesting work. There are minor issues in experiments.<|endoftext|>The paper presents an exciting new finding – by properly reorganizing elements during the IMP process, it’s possible to achieve structurally sparse winning tickets at high sparsity levels that can be easily hardware accelerated. The authors explored hardware friendly structural sparsity (including channel wise and group wise patterns) to find lottery tickets. One should note that the sparsity level discussed in this work is not as the usual high as in unstructured tickets (often >95%), but the hardware acceleration is way more apparent. I have just a few clarifications or curiosity questions:  It seems the acceleration effect on the structural ticket (especially after regrouping) requires some specific hardware accelerator and efficient implementations such as GEMM. How much performance gain of the reported numbers comes from hardware specialization (Rumi 2020)? Whether the same techniques can be applied to lottery tickets found not from random initialization, e.g., how about the pre trained model lottery tickets as in (Chen et al., 2020b)? The paper is in a good shape that displays the first set of positive results bridging the gap between the lottery ticket hypothesis and practical accelerations on real world hardware. Algorithm details, as well as results, are sufficient and convincing.<|endoftext|>This work demonstrates the existence of structural winning tickets for the first time by leveraging post processing techniques. It is well known that lottery tickets so far are only successfully found with unstructured pruning. This paper challenges this commonsense, and its positive finding could be of broad interest and impact. The paper for the first time achieves high sparsity (up to 80%) with structure in the lottery ticket literature and shows practical GPU hardware runtime savings at around 50 60%. I suggest the authors to tone down their claims. There definitely exist similar works in relevant problem domains, such as sparse training with structures: for example, “Learning N:M Fine grained Structured Sparse Neural Networks From Scratch” ICLR 2021, should be discussed and compared. I don t understand for ImageNet, why the current results can show "our picking rule (i.e., channel weights l1 norm) provides a great estimation for channel importance"No theoretical insight on why the proposed algorithm can work is provided. The contribution of this paper is solid. I hope the authors could clarify the weak points.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; In this work, the authors mainly combine the GNN and Transformer to learn both local and global information in large scale graph. The paper is easy to follow, the ideas combine the GNNs and Transformer is interesting, take benefits of the GNN, local aggregation and Transformer, global aggregation. The novelty of this work is not very high, it combines a lot of existing of techniques, for example GNN, Transformer, Graph Coarsening, Sampling, PPR etc. These methods are widely explored[1, 2].<|endoftext|>Overall, this paper tackles an important question on scaling Transformer to large graphs. The proposed method is technically sound. With that being said, I have the following concerns for the paper:1 The proposed Coarformer is not well motivated and seems overly complicated. It seems that the main contribution is to operate the Transformer on a coarsened graph. How scalable is the algorithm being used? Overall, I like the motivation of the paper. And the experimental results are not very strong.<|endoftext|>To be more specific,* Since directly using the transformer architecture to the large scale graph is computationally prohibitive, the authors use the existing graph coarsening algorithms to the large graphs, and then use the transformer on the coarse graphs, which can capture the global information of the given graph in contrast to the GNNs capturing local information. * The paper is well written and easy to understand. Also, the idea of alleviating computational complexities, both memory and time, in the paper is similar to the mentioned previous work [1]. * In Table 2, it seems the authors do not incorporate the time for graph coarsening. I think the extra memory usage required for the proposed method is relatively increasing, when we use the large size graphs that authors tackle.<|endoftext|>The paper proposes hierarchical neural network model for processing graphs. Strong points* I find the empirical part of the paper solid. However overall the model performs strongly and it is great that authors report the confidence intervals. No citation is in that paragraph. * Loss on coarse graph is just auxiliary, correct? * How does the number of Coarformer s parameters compare to other compared models? * Why are you testing just transformer based architectures on the global scale? E.g.GraphSAGE can be used for both local and global level.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper presents a simple algorithm named ConAdv to incorporate adversarial training into the large batch training setting such that one can further increase the batch size without harming too much accuracy while maintaining the high utilization of the hardware. The core idea is to use adversarial training to improve the accuracy for large batch training and at the same time use stale weights to allow parallel computation of the adversarial example and the normal gradient updates. The proposed approach is simple yet novel and useful. As demonstrated in the experiments, using stale gradients can achieve a great speed up over the DisAdv baseline with no noticeable accuracy differences. Overall, I think this paper is well written and it provides a novel, simple, yet useful algorithm for further increasing the batch size for large batch training.<|endoftext|>And furthermore, the authors proposed a simple method to conduct adversarial example generation and gradient computation w.r.t.to weights concurrently to accelerate the adversarial training in distributed setting. The paper is clearly motivated and well written. To the best of my knowledge, the findings regarding the adversarial training for improving generalization performance in **large batch training** is new. Although the proposed method is simple and straightforward, and the theoretical analysis is easy to conduct, the empirical results are nice. However, the results lack of reasonable interpretations. Could you elaborate a little bit why should be this? This appears contrary to your finding. It is a really interesting point. I think the authors should provide this result to deepen the understanding the role of adversarial examples for generalization.<|endoftext|>This paper proposes a novel strategy which adding adversarial data for training to improve the performance of large batch training. Above all, the authors should improve the theory for a better understand of the adversarial data. The experiments mainly focus on ResNet 50/ImageNet training. 2.The authors actually prove a non asymptotic convergence rate and I also notice that LARS is used for the proposed methods in experiments.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposes a novel unsupervised scene decomposition model that infers object shapes, appearances and 3D poses. Strengths:+ The paper is clearly written and easy to follow. + It is novel and interesting to see object centric learning models to infer 3D poses. In segmentation experiments, the results are worse than 2D baselines. Perhaps Aloe is better because it is designed for that task, but there is neither comparisons to 2D general object centric baselines (such as original slot attention) or unstructured baselines. It would be good to demonstrate on datasets other than CLEVR. Although this paper is interesting in the ability to infer 3D properties of objects, the fundamental flaws in experiments are the main reasons that I recommend revision and resubmission.<|endoftext|>The paper aims to decompose a scene into objects and infer the representations of 3D occupancy, color, and pose for each object from a single image of the scene without supervision. To this end, the paper proposes an autoencoding solution by combining the Slot Attention encoder with the GIRAFFE decoder. However, my main concern is that the experiments did not focus on testing the  3D ness  of the inferred representations. The results on the CATER snitch localization task did not strongly support that the  3D  representations learned by the proposed model are more beneficial than 2D representations. However, this was not demonstrated in any of the experiments. The paper could be strengthened by experiments on more complex scenes, like the ones used in [1]. I would imagine that under different camera viewpoints, the same manipulation would have different effects, but the results in Figure 2 seem quite consistent. I recommend reject, because the main claim, learning 3D representations, is not well supported by the experiments, and the benefit of the learned representations over prior work is not well demonstrated.<|endoftext|>This paper proposes a model which is able to segment 3D scenes into objects by a combination of slot attention (For inference) and a mixture of object NeRF functions which mix together (in 3D) to compose a scene. Results are demonstrated on CLEVR data as well as CATER (which is visually very similar) and some downstream tasks. Strengths:* an interesting combination of slot attention and NeRF mixture models. * paper is nicely presented* results are nice, though see belowWeaknesses:While I like the paper in general I feel there are several points missing which are probably worth addressing to:* The objects are transformed using the pose   but that means the appearance is transformed as well   this could be a problem if some of the appearance is pose dependent, for example shading will change, or reflections. Can the authors comment how this is handled here? This would be more convincing to demonstrate these are "3D" segmentation. * I feel the experimental validation is quite limited   using only a single dataset (even if it s two variants) is not very convincing as to the generality of the method. How does it work on other datasets? more/less complex.<|endoftext|>This paper proposes a model to infer structured 3D object representations from a 2D scene in an unsupervised fashion so as to represent the visual scene in an object centric way. Otherwise, it is hard to see the real impact of this proposed method. The technical novelty is somewhat limited. The paper mainly combines the Slot attention and NeRF. It is claimed that the model is proposed to infer 3D representation for objects that are interpretable and amenable. In terms of scene understanding in the form of object segmentation, this model cannot beat Slot Attention. In terms of downstream tasks, this model is not compared with other object centric models. Thus, based on your current experiments, the motivation for learning such 3D representation for objects is not well justified. If I understand correctly, NeRF learns the radiance field of a scene with 2D images rendered from different viewpoints. This model tries to recover the object radiance field from a single view of the objects. It could be more convincing to provide some novel view synthesis (not novel scene) to claim it as a NeRF. 5.All experiment datasets are a bit too simple.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; This paper presents a binary classification training framework, SphereFace2, for face recognition. Comprehensive experiments demonstrate the effectiveness of the proposed method. It would be better to clarify this. The paper is well written and easy to follow.<|endoftext|>This paper proposes a novel face recognition model called SphereFace2. The key idea is to replace the softmax based multi class training with multiple binary classification training, which is more consistent with the testing phrase and generalized to both closed/open set evaluations. ** Strengths  The discussion on the advantages of the multi binary classification strategy is interesting and persuasive.<|endoftext|>This paper has proposed a relatively new idea on training face recognition models by using multiple binary classification losses. The paper is well written with nice presentation and clear description. Experimental results ascertain the superiority of the proposed method as a unified solution.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper introduces a novel setup called Anytime Learning at Macroscale. In this setup the learner receives the examples as a sequence of large batches, and is required to output a model after processing each batch. The overall performance is then sum of the average losses on the individual batches. Concern with the motivation: the setup is not that different from online learning after all.<|endoftext|>The authors describe a framework to perform empirical evaluation of an anytime learning setting where data is available in a streaming minibatch fashion. Paper is well written. The authors document the approach they have considered, and the metrics used to evaluate the various experimental settings used. Though the authors state that the primary differentiator to the stream setting is that the models use what they call as "meta batches" as streams rather than streaming single data instances, I fail to understand any theoretical or empirical difference in the two approaches. So, the primary contribution of the paper seems to be in extensively evaluating the model complexity and approaches over various data and problem settings. Furthermore, it is important to note that there are other factors that influence the classifier performance, more than the batch size and data size available for training. So, by using more datasets, these issues can potentially be elevated. In summary, the original contribution is not very clear.<|endoftext|>Summary: This paper proposes anytime learning at macroscale (ALMA), which is anytime learning under the assumption that data is observed as a sequence of large batches. They evaluate multiple learning models on different datasets in the ALMA setting. Though they show some theoretical results, it is hard to relate them to the objective and the algorithm steps explicitly and tightly. What is the definition of macroscale? Unclear the difference between ALMA and other learning frameworks. Quality: The submission is technically sound. It is a complete piece of work. This paper proposes a novel learning setting called anytime learning at macroscale (ALMA). However, the contribution is unclear, and the presentation needs improvement.<|endoftext|>The authors consider a batch learning problem, in which large batches of data arrive in series. So, the problem is reasonably well motivated. From a practical point of view a data engineer might be concerned with the questions; how good is the model I have now? 2.The non iid nature of the data is mentioned, and it is mentioned that cross validation is carried out only on the current batch. Should I hold out a portion of every batch to use for evaluation? That is not addressed in this work.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; The authors propose a deep equilibrium (DEQ) layer that provides certifiable robustness via the interval bound propagation technique. The main contribution is a theoretical result that says that when parameterised in a certain way, this IBP DEQ admits a unique fixed point (Theorem 3.1, 3.3), and also that the model provides valid IPB (Proposition 3.2). Motivated by a theoretical result (Proposition 3,7), the authors show empirically that such restrictions imposed by the specific DEQ and parameterisation achieve comparable or improved performance compared with explicit models on MNIST and CIFAR10. Creating DEQ models that admit unique fixed points is an important research question, as is developing deep learning models that are certifiably robust. Nevertheless, the empirical evaluation serves its purpose at demonstrating the utility of the model. This work addresses both questions simultaneously, and its theoretical contributions serve as useful tools. Still, I think that addressing these problems for fully connected and convolutional layers is an important contribution and a good first step, and the empirical evaluation serves its primary purpose of instantiating the theory presented in this paper. Please address these questions/comments in your rebuttal. It is mentioned in section 3.2 but I think it would help the reader to move this definition to before it is used for the first time. But this matrix is not symmetric. I think it should be \hat{W}. 6.I can t see where the matrix S comes from in the proof of theorem 3.1.<|endoftext|>The paper presents IBP MonDEQ, a modification of monotone deep equilibrium layers (MonDEQ) that allows for the computation of lower and upper bounds on its output via (a fixed point version of) interval bound propagation. As a result, the authors can train a certifiably robust DEQ model, whose performance is shown to be competitive with an explicit model trained via IBP. I believe this implies that the output of the fixed point solver is a valid bound on the DEQ layer activations only if it is run until convergence. If so, do the authors take this into account (that is, is it always run to convergence in the experiments)? The experimental results show that IBP MonDEQ networks are competitive with IBP trained explicit networks. IBP MonDEQ is an interesting and non trivial extension of IBP to deep equilibrium layers that allows for relatively effective robust training of DEQ networks.<|endoftext|>This paper considers the certified adversarial robustness of Deep Equilibrium Models (DEQ) and derives the Interval Bound Propagation (IBP) on DEQ for training certifiably robust models, namely IBP MonDEQ. Strengths:* This paper derives parameterizations for DEQ such that the fixed point solution of the DEQ augmented with IBP bounds exists and is unique. This paper is interesting and can be meaningful for further research as it derives the IBP computation on DEQ for certified defense. Post rebuttal updates Thanks for the response from the authors. The authors have improved their experimental results where the baselines match better with [Shi et al.’21] (currently this update is not visible to the public). This paper does present interesting findings in terms of verifying DEQ and can be potential interesting for the area of DEQ, and this is the first work for verifying DEQ which is different from explicit networks.<|endoftext|>The paper presents a new class of neural networks called IBP MonDEQs that are an extension of the recently introduced implicit networks MonDEQs. Further, many key details were not clear to me including comparison with the classical Knaster Tarski theorem and whether the explicit networks are certified with complete verifiers or IBP. The construction of IBP_MonDEQ is motivated by the goal of obtaining networks that can be certified to be robust. The authors then train such networks comparing against explicit networks of the same architecture constructed using the certified IBP training. The results show that IBP MonDEQs can obtain better certified robustness than explicit networks on the MNIST and CIFAR10 dataset. The work focuses on addressing this issue and therefore the problem considered here is an important and challenging one. Cons:1.Some of the details about theoretical formalism and empirical evaluation was not clear to me (see my comments below). I have a few other questions:1. The text in the evaluation says that M is a learnable parameter which makes things clearer but it comes too late. This is important as it seems that the IBP MonDEQ cannot be analyzed with anything else besides the IBP analysis while the explicit networks support other analyses. The paper makes solid theoretical and empirical contributions for obtaining new implicit architectures with certified guarantees on their robustness.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes an online meta learning method which can remove the discrete task boundaries. Different from existing online meta learning method, FOML utilizes online learning to remove the task boundaries. The FOML is able to continually update online parameters with new datapoint, and meanwhile perform meta gradient updates on a set of meta parameters using a buffer of previous data. The authors compare the model to state of the art like FTML on benchmarks such as Rainbow MNIST and CIFAR100 datasets. Online within online meta learning. However, it lacks sufficient experiments and comparison with baseline methods to support the claim that FOML can learn without the task boundaries. Continuous Meta Learning without Tasks. The paper is reasonably written with clear background and diagrams for the overall architecture. Experiments are not strong enough to support the claim. The idea is not new, authors are missing some closely relevant and major literature in online meta learning [1] and meta learning without task boundaries. Is this loss related to specific tasks? 3.The experiments are weak. I assume the OML represents the FOML? I’m not sure about that. 4.According to my previous point, important baselines are missing, the work [2] also targets meta learning without the task boundaries. 5.Is there any mathematical guarantee to show that FOML can learn without task boundaries? ?”  For figure 3, the method name should be FOML rather than OML in the diagram.<|endoftext|>The authors propose FOML, a new online learning algorithm based on MAML. Rainbow MNIST and online CIFAR100  experiments show that FOML outperforms TFS, TOE, FTL, and FTML. It would be interesting to see some ablation experiment or some discussion about this scenario, as well as possible ways to deal with it. In their case, rather than optimizing a regularizer, they optimize the inner loop learning rate, which in the end, has the effect of preventing the online weights from drifting too much when the new encountered task has a high interference. "La maml: Look ahead meta learning for continual learning." * Pag 9. depends on  > depend onThe authors present a simple approach that is sound, effective in a specific online continual learning scenario, where a replay buffer is kept, and new tasks are not completely unrelated to previous tasks.<|endoftext|>This paper extends meta learning algorithm for continual online learning. Different from previous meta learning based online continual learning, this algorithm removes assumption of knowing task boundaries of data beforehand. The method name in the legend is OML6. One key difference besides removing reset is to change the replay buffer (memory) strategy used by meta updates, where prior approach always sample data of all previous tasks and the proposed approach samples all data points seen so far (so there is no need to know which task we are currently solving). 5.Figure 3.Does not your approach called FOML? 4.Equation of FTL, maybe it would be more clear to reader if you rewrite phi as phi_{TOE}^{t 1}. Based on my understanding, I think this is a solid and well written paper, with extensive results that demonstrates the improvement of presented approach to its comparing prior work. To validate the proposed approach, authors conducted experiments on a synthetic continual image recognition benchmark (of 1,200 tasks) based on CIFAR100, where they outperforms previous continual meta learning based approach. 2.The algorithmic changes to previous FTML are incremental (to suit the new assumption) but the justification behinds these design choices are reasonable. In meta update, with a large K, you essentially need to store K copies of all network parameters? Why don t you also compare to other non meta learning based online continual learning? Why don t you consider realistic online continual learning datasets, such as [1] and [2]? [2] Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual DataMinor Comments: 1.<|endoftext|>FOML is evaluated on two datasets and compared against a few other similar approaches and baselines. The paper is written clearly and is easy to understand. ConcernsLack of clarity on the problem statement: The authors cast the problem as meta learning that must be done online. As I understand, the goal in meta learning (particularly the MAML framework adopted by the authors) is to learn a good initialization for quickly adapting to a new task. However, the current problem seems to be more on the lines of online learning and avoiding catastrophic forgetting, as there is no adaptation step after meta training. The authors explicitly mention that their goal is not to solve catastrophic forgetting. There is a large body of literature on regularization based methods (e.g., Kirkpatrick et al.2017, Ritter et al., 2018), memory based methods, online continual learning (Caccia et al.2020), etc. I find the discussion on the related work to be quite narrow, missing out on approaches trying to achieve similar goals. Experiments on these datasets are warranted to conclude FOML’s efficacy. Finally, the lack of theoretical justification on the effectiveness of the FOML is also a cause of concern. I acknowledge that the experiments comparing other online learning algorithms (such as LwF, iCaRL) and algorithms such as MOCA is a step in the right direction, but requires more analysis (using more datasets).
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper proposes generalized DEO scheme (DEO*) that extends DEO with a window where at most one exchange could happen within one window. This paper also extend existing analysis on expected round trip time into DEO* and authors show that O(P logP) expected round trip time could be achieved even for finite number of chains and non diminishing rejection rate. This paper also proposes multiple approximation to make DEO* practical: The exploration chains substitute gaussian noise with gradient noise, where noise magnitude is controlled with step size. Deterministic swap condition is used to avoid specific temperature. Adaptive learning rates and adaptive correction buffers are adopted to tune acceptance rate. The authors evaluate DEO* on ResNet model over CIFAR100. However, DEO* doesn t converge to the target distribution. The following example constitutes a counterexample of soundness of DEO*. Consider an energy function $f(x)   8 x^2$ on the domain $[ 1,1]$. We construct only two chains with temperatures $T_1 1$, $T_2 \infty$. Two posterior should be $p_1$ as a truncated Gaussian distribution highly concentrated around origin, $p_2$ be a uniform distribution. Let window size to be large enough, such that swap will happens in one window with high probability. The output distribution is no longer $p_1$ and $p_2$. 2.This paper is clear until section 4 where DEO* SGD method is proposed. While using SGD as approximated local exploration kernel in (6), it seems only gradient noise is considered. However, discretization error is another source of noise. More specifically, for Langevin dynamics $d\beta_t    \nabla U(\beta_t) + \sqrt{2} dW_t$, the discretization error for Euler discretization is $\int_0^\eta \nabla U(\beta_t) dt \eta\nabla U(\beta_k)$. How does this discretization error affect the distribution and temperature? + b. on second line at page 5, the authors state that "$\beta_k$ converges approximately to an invariant distribution, where the underlying temperature linearly depends on the learning rate $\eta$". What is this invariant distribution? + b.No uncertainty approximation quality (e.g.expected calibration error) is reported. + c. Figure 7.c has incorrect y axis value. The proposed method does reduce expected round trip time but fails to converge. The practical method is not well understood and experiments need to be amended.<|endoftext|>The authors propose a modification of the existing deterministic even odd (DEO) swap scheme. Their investigation of stochastic approximations for big data, optimal number of chains, analysis of round trip time are detailed and through. I have minor comments about the paper. They are:1) The approximation analysis theorem (theorem 2) assumes uniform ergodicity, which is stronger than “geometric ergodicity” as currently stated. Secondly, note that the bound given for theorem 2 is vacuous if (1 \rho)<\Delta. Some discussion of this point (that we require small \delta and \rho much less than 1 for the given upper bound \Delta/(1 \rho) to be useful) would be helpful. 2) There should be some discussion about the choice of W (or target swap rate S) in practice, what algorithms/ heuristics are recommended when choosing the parameter W, and what additional computational cost this incurs. For example, at the moment the optimal window size is given as a function of the number of chains and the target swap rate  but how is the target swap rate chosen? I could be misunderstanding something here. 3) If possible, the authors could present a version of Cor 1 which does not assume that the optimal W is chosen, but a generic W. Alternatively, some discussion about the importance of the choice of W should be included. This manuscript proposes new swap schemes for parallel tempering. The methodology is supported by theoretical analysis, detailed discussion of methodological details and simulations. It will be of interest to the ICLR community. EDIT: Having read the review of "mbau" and the corresponding discussion, I agree with the reviewer that the DEO* scheme may not converge to the target distribution of interest. I also agree the current argument in Section C.2 needs to be made more rigorous. Based on this point, I am inclined to revise my initial review score for the paper.<|endoftext|>This paper considers sampling multi modal distributions by the powerful parallel tempering methodology. An existing exchange scheme, DEO (deterministic even odd), is improved by generalizing even and odd chain indices to even and odd windows of width W. Some nontrivial theoretical and empirical results are provided. The theoretical components seem to be strong too. I’ll be happy to be corrected and increase my rating, but I’d love to see more integration of various theoretical components, as well as how their implications get reflected in the empirical results. For example:* Is there any theoretical guide on the choice of $a$? By the way, what is the relation between $a$ and $\mathbb{S}$? * If I understood correctly, Theorem 2 is just for one chain, not all the $P$ chains. How does that scale with $P$? * How can Theorem 1 and Theorem 2 be combined to produce a theoretical guarantee of the final algorithm? It will be great if notions built around `round trip’ get translated into final iteration complexity. Since improved efficiency seems to be a key contribution, it is a little odd to only show improved accuracy. * Can results for various W values be compared in the experiments, so that the theoretically optimal value could be justified? What is the prior? This is a very interesting paper. Since the treatment is rather nontrivial and I feel it deserves appreciation, I’d like to see just a little more about how different theoretical and empirical ingredients of this work mesh together.<|endoftext|>One example of parallel tempering is when one uses Langevin dynamics with different temperatures to balance exploit and exploration for sampling from a multi modal distribution. The balance is made by swapping running particles with different temperatures according to a particular schedule. The round trip time (RTT) is a computational complexity metric for measure for the swapping schedule. It is known that the best known algorithm suffers from $O(P^2)$ RTT for $P$ parallel particles. By adjusting this algorithm, the authors improve the RTT to $O(P\log(P))$. Then, they demonstrate the application of nonconvex with stochastic gradient descent. $O(P)$ improvement is significant 3. The proposed algorithm is based on a slight modification of an existing algorithm 4. The paper is very well written 5. Literature is well reviewed in section 2. Is possible to modify the rejection probability in DEO to get the same rounding time (for example use the rejection $O(r^{\log(p)})$? 2.I could not understand why the paper focuses on the gradient Langevin dynamics. Is it possible to show that it is not possible to get a communication cost lower than $O(P\log(P))$? I think that the optimality is about the optimal window size for the adjusted DEO. 5.The transition from the main result to applications for SGD can be more detailed and more clear. This application can be motivated more and explain why this particular case of parallel tempering is studied in the paper. The difference between SGD and Euler s discretization of Langevin dynamics can be explained in more detail. 6.I recommend presenting experimental validations for Thm. 1 after this Thm. I liked the contribution and narrative of this paper, and my opinion about the result is rather positive.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; In this paper, the authors propose a novel differentially private approach to generate both continuous as well as discrete valued synthetic data. The paper presents a novel and well motivated approach to privacy preserving generative modeling. The main strengths of the paper in my opinion are: 1) The approach is quite well motivated and the paper is well written.<|endoftext|>In this paper, the authors proposed a new framework that uses deep generative models to synthesize data in different private ways. Both theory and extensive case studies verify the performance of the proposed framework.<|endoftext|>Paper proposes a new method (PEARL) to generate synthetic data based on CFs. 2.I am also confused by the sensitivity calculation, especially the factor k, can authors explain where does that come from? 4.It will be nice to compare PEARL with other generative methods such as PATE GAN, CTGAN for tabular data etc.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper introduces a variation on the DREAMER model based RL architecture that aims to remove elements from the latent state representation that cannot be affected by the actor. To realize this, there is an inverse model objective included in the value function. To a task, to control, to an understanding of the robot s odometry? Also some expressions are a bit handwavy and could have used a bit more rigorous writing.<|endoftext|>This paper tackles the problem of prioritizing functionally relevant information from complex observations for model based RL. The paper can be strengthened by adding empowerment to the baselines. It is also shown that the similarity between learned states can match well to the similarity between groundtruth simulator states, according to the proposed metric. However, I have some concerns regarding the experiments and the clarity of the paper.<|endoftext|>The paper introduces a method to learn representations for model basedRL. The paper is well written and motivated. Strengths:I like the idea of the paper in that it appears to be logical andalmost obvious, after reading about it here.<|endoftext|>The paper proposes a new non reconstruction method for model based RL from high dimensional observations. However, this can be left for future work. In general, the proposed method is promising, but the current presentation of the paper has major issues that need to be addressed and clarified.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 6; Paper proposes a new scenario for source free domain adaptation where the domains undergo a measurement shift (characterized by a change in the measurement system). The proposed method outperforms existing methods. The strengths of the papers: a) The paper is very well written, well motivated, well compared with existing literature and it was a pleasure to read.<|endoftext|>### *Strengths:*  The paper is well structured and easy to follow. It is well written and the method is easy to understand. The experimental analysis is very extensive. The paper also provides nice analysis of the activation statistics under the *measurement shift*, ablation analysis on which components are useful and discussions on the observations. However, it is not a strong argument to avoid comparison with these methods which have previously addressed the problem of source free adaptation. Furthermore, they are very closely related to the proposed method idea of activation distribution matching as compared to their approach of feature distribution matching.<|endoftext|>This paper focuses on the improvement of the source free domain adaptation problem. Thus, I want to see the analysis of the measurement shift to explain the reason for the proposed architecture. + The state of the art performance was validated well by using various types of datasets and the compared algorithms were recent and reasonable.<|endoftext|>This paper tackles the specific domain shift, named measurement shift on source free adaptation setting. They analyze the drawbacks of existing entropy based SFDA methods and instead propose a method to retore the source feature with a lightweight approximation. The paper is very well written and easy to follow. 2.The proposed method is simple and effective. The ablation and analysis are sufficient to support the claim of this paper.<|endoftext|>This paper addresses the problem of source free domain adaptation (SFDA) under measurement shifts. The proposed method aims to resolve this problem by restoring the target features to the source feature distribution. Towards this, the source distribution is approximated using a lightweight and flexible approximation, namely softly binned histograms.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a full binarization of BERT (including 1 bit activation), called BiBERT. It is also interesting to see that the activation distillation can cause the optimization direction when the quantization becomes more aggressive, which well motivates the proposed direction matching distillation term. The experimental results are solid and impressive. In the 1 1 1 setting in which the paper mainly focused, the proposed BiBERT model achieves significant improvement over the 1 1 1 baseline model.<|endoftext|>This paper addresses the problem of fully binarizing BERT model including the network weights, embeddings and activations. This main contribution of the paper is that it is the first work on full binarization of BERT models. The proposed BiBERT model also surpasses the previous BERT quantization model on the setting of 1 1 1 quantization. While the BinaryBERT with 1 1 4 quantization, which has similar model size and a little bit higher computation as the BiBERT, can achieve GLUE score of 81.9 with almost neglectable performance loss. One possible way to justify this is to enlarge the network size while keeping it binary and see if the performance can consistently improve. The author should clarify in the paper how the results are obtained.<|endoftext|>This paper introduces an accurate fully binarized (i.e., 1 bit weight, embedding, and activation)  BERT, BiBERT, as a robust model compression method. It might be a positive inclusion towards an accurate fully binarized BERT architecture. Moreover, it addresses the substantial performance issues in the straightforward full binarization method. The paper contains an evaluation of the proposed model on GLUE benchmark and a comparison with SOTA quantized BERT models. 2.In sec 2.1, it is evident that a stochastic sign function is used to binarize the BERT, therefore, why the deterministic function is used in sec 3.2.1 is not clear.<|endoftext|>This paper focuses on the full binarization of BERT. Experimental results on GLUE benchmark demonstrate that the proposed work outperforms several popular baselines. The authors also find that the direction mismatch hinders the accurate optimization of the fully binarized BERT. It would be better to explain why the higher results are not reported in this paper. The empirical contribution is a little bit marginal since there is a large gap between the proposed approach and the best result in [1].<|endoftext|>This paper proposes to binarize both the weights and activations of the BERT model. Thus the authors propose new binary representations that maximize entropy and distillation methods that maintain direction. However, it is not clear why this assumption holds. Even though the softmax function is order preserving, maximizing the entropy of sign(A   \phi(\tau, A)) is not equivalent to the same as the original problem in (9). The experiments also require some more clarification.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes a sequential encoding decoding shemes and can compress irregular pruned weights to a regular (block wise) structure stored in memory and shows experimental results in large scale models / datasets. I like the motivation of this paper which is a very important problem in practice. To the best of my knowledge, the solution is novel and interesting, and I believe it is an orthogonal contribution to many other literatures in the compression community (e.g., magnitude based compression, variational dropout etc). However, it s a pity that the paper doesn t show any practical acceleration on ImageNet or WMT. I would recommend spend some time and revise them. An individual section (Related Work) for discussing compression literatures would be preferred. The author also demonstrate the effectiveness of their method on large scale datasets with a number of compression techniques.<|endoftext|>But I would be happy to change my evaluation if the authors can improve the clarity of the paper and address my concerns. After rebuttal:I raised my evaluation from 5 to 6, given the improvement included in the new revision. And related works section is only included as an appendix section. This layout is unusual, and it made their manuscript hard to comprehend for an audience that is not familiar with the specific topic. I only managed to comprehend this work after going through those two prior works. P2: “Since higher sparsity leads to higher variance on pruned weights in a block”. Are you sure this is correct? P4; the sentence just above Section 3.1. : I believe the logic of this sentence is not valid. Even though the conclusion is correct.<|endoftext|>The authors proposed an encoding scheme and decoding mechanism for efficient communication of irregular sparse DNN parameters, and demonstrated with sparsified transformer and resnet50. The method is novel. How does compute overhead of decoding scale? On page 2: ``higher sparsity leads to higher variance on pruned weights in a block". Is this true? Variance of the number of zero/nonzero should be nonmonotonic w.r.t.sparsity; extremely sparse or dense cases have zero variance. Am I missing something? Novel idea of significance and potential practical value.<|endoftext|>I.e., with additional manipulation of non zero weights (lossy or not), does this method offer avenues for improvement? The paper feels a bit isolated and theoretical, which is odd given its stated goals. The latter (context) is a bit weaker. The authors choose to treat their problem in isolation, which means that the paper sometimes loses sight of why the method was proposed to begin with. Walking the reader through the pitfalls of a new technique makes the paper more enjoyable to read. Weaknesses:  No mention of the actual computational performance of the method. This is minor and admittedly subjective. Q: This work seems to share many parts with the references it seems to build on: Kwon20 and to a lesser extent Ahn19. While I have a general sense of where this paper diverges, can the authors briefly summarize what they feel the novel contributions to the community over those works are? I would probably have appreciated a direct comparison (perhaps in a related works section, seeing as one is not supplied).
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; This work proposes a generative model to convert emotion by adaptively modifying the duration of speech components without the need for reference utterance. A major concern over the evaluation is the absence of a proper baseline method, which is crucial for presenting the motivation of the work and the contribution of the paper. Recent works on speech synthesis [1, 2] aim to make the duration of generative speech as stochastic as possible. ### Correctness  In section 3.3, the authors state that: "In contrast, the sequence to sequence models for voice conversion requires hundreds of hours of training data along with sophisticated noise removal models to generate actual speech." Since the MOS results of a single model widely vary across the experiments and the dataset, the MOS of the existing models should be re measured using the same evaluation protocol to be directly compared with the proposed method. Hence, in order to fairly compare the speech quality with existing models, it should not be directly compared with vocoders. "The neural network then processes the input frames and generates an embedding for the decoder operation and to predict the target sequence length."<|endoftext|>This paper proposes a model for adaptive duration modification of an input signal. The model is a graphical model with neural components. Why Laplace distribution was chosen for length T and target Y_t in Eq12. There is not a novel modeling paradigm which be of interest of general machine learning crowd (as a reader I did not learn a new ml method). Of course, the paper proposes a novel method by combining some existing machine learning tools (like graphical models, sequence to sequence modeling). Unfortunately, it is not clear to me if the speech duration modification is such a problem.<|endoftext|>The paper proposes the use of an encoder decoder framework with attention masking to estimate a candidate target utterance length to overcome the need for a priori knowledge of a target utterance on a speaking rate modification task. Evaluation on 2 standard databases show that the proposed approach performs better than 2 sequence to sequence models that were originally developed for different domains. This statement, as it is stated in the abstract is misleading/incorrect. "We propose the first method to adaptively modify the duration of a given speech signal." 2.There are lots of strong claims in the Introduction that could be avoided. Drawbacks of DTW and neural vocoders are unwarranted without explicitly stating the practical applications goals being considered by the authors. The statement on data needs is misleading. A couple of decades of work after PSOLA (90s   2005) could be added to provide a more comprehensive review for the reader. A better comparison would be also related approaches in pitch and style modification.<|endoftext|>This paper proposes a generative sequence to sequence encoder decoder architecture with attention for modifying the length of an input sequence. The authors derive the training loss for learning the network parameters from a principled Bayesian formulation and a variational inference bound, and use this to train the model from paired {input, output} sequences. A better literature review is needed. "Furthermore, our method can be trained on limited data resources. I perceive 2 issues with this claim: One is that it seems exaggerated. And secondly, these bigger voice conversion systems are tasked with performing a far more complex objective than what the authors have proposed to do, and evaluated, in this submission, namely duration modification. The work is evaluated in terms of objective metrics and shown to outperform some baselines.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 8; It builds on a work of Cohen et al.(2020), where it is shown that if, upon insertion of a row, we only keep (a multiple of) it with probability roughly equal to its current leverage score (otherwise we discard it), then the total number of rows that will be inserted is only $\widetilde{O}(d)$ and the matrix spectrally approximates the "true" matrix where all rows are kept. At a technical level this part is not extremely new. The paper is well written in general. What were they missing? Some of the technical content overlaps somewhat with previous work. The experiments show good results but are limited. I thank the authors for the answers, they have answered my questions.<|endoftext|>Some experiments are provided. # Quality and ClarityOverall, this paper was a joy to read. The technical core of the paper certainly exists, but I m uncertain if it is sufficient. I lean "marginally above accept" on this point, since I could see myself referring to results these in the future, but it s just not a lot. The experiments are barebones, but if the theoretical contribution is sufficient, then that s something the authors can fix up for the camera ready version. I m particularly interested in discussing with other reviewers if the technical contributions are sufficient. If the JL trick also works there, that would be notably compelling, and feel like a stronger and more general theoretical guarantee. Flesh out the experiments more. The proof sketch is compelling, though I haven t reviewed the full proof; if I have time I ll make an update on that.<|endoftext|>The main task is to maintain a good approximate solution x (meaning that |Ax b|_2 <  (1+eps)*OPT) throughout this process, with the update time as small as possible. The algorithms and the proofs are all similar to Cohen et al.’s paper. But the result on the regression problem is nice. The reduction in the proof of the lower bound is interesting and I like it, though the Online Matrix vector conjecture feels a bit strong in its own formulation to me, which already places the regression problem in a good position for the reduction. Although there is not significant technical innovation, this paper is a solid technical paper that solves a basic linear algebraic problem in the online setting and deserves to be published.<|endoftext|>Thus it seems the main contribution of this paper is to improve the lower order runtime through the rank one updates. The paper shows a reduction from the online matrix vector conjecture, which essentially says that maintaining the product of a matrix and a vector whose entries are iteratively revealed requires roughly $\Omega(d^2)$ update time. The paper also includes some experiments but the details are not entirely clear. For example, it was stated that the experiments were all repeated at least 5 times. Minor: Woodburry  > Woodbury in a few placesPg. I also acknowledge that I did indeed misunderstand the significance of the upper bound with respect to the previous work. Regardless, I hope the authors will consider improving the presentation of the technical statements.
Reject; rating score: 3; rating score: 5; rating score: 5; One argument repeatedly made in the paper is that existing structure learning algorithms are computationally expensive. This claim and others are far from justified. The contribution is a heuristic to choose the "best" intervention target based on a comparison between samples from an intervened SCM with the current best guess of an underlying causal discovery algorithm (in which the proposal is embedded). If the main contribution is the score, this is not sufficiently discussed or justified.<|endoftext|>I look forward to more clues from the authors. The experimental results verify the effectiveness of the active strategy compared to random selection. However, according to my understanding, although the novel point of this paper is to present an active causal discovery method based on continuous optimization framework, the tackled problem is still actively causal discovery. The comparison between the proposed method and ICP seems not quite fair. I have one another question. The authors claim that the proposed method is applicable for multi node intervention. How do the authors address this problem? Different from some theoretical causality paper, this paper tackles a practical and widely researched problem.<|endoftext|>The paper proposes a method to select interventions that enable efficient identification of the underlying causal structure. After reading the authors  response and the other reviews, my concerns about the experiment setup persist. The proposed method is interesting and novel. Weaknesses:  The proposed method lacks theoretical analysis. It would be better to include similar experiments in Tables 1 and 2 for DCDI. How did the authors ensure a fair comparison between the proposed method based on active intervention, and the baselines based on observational data and/or random interventions? It would be better to make this clear in the main paper.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors present U WILDS, an extension of the multi task, large scale domain shift dataset WILDS. They propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain shift. The data collection and baseline implementation could be an important stepping stone for the ML community to address its biggest challenge yet: domain shift in the Wild.<|endoftext|>### New large scale dataset for unsupervised transfer learningThe paper proposed an extension to the popular WILDS benchmark dataset by augmenting the different domain data with additional unlabeled examples. Additionally, many recent methods that leverage unlabeled images to achieve generalization are bench marked on the proposed datasets. ### Strengths   The dataset is a useful addition to the WILDS dataset, and the provided unlabeled data would be very useful to design many unsupervised generalization algorithms. ### Required Clarifications  The models chosen for benchmarking the methods are not suitable. If any of the questions above have a direct answer in the suppl. Post Rebuttal ****************I thank the authors for answering all my queries patiently, which answered most of the questions I had regarding suitability of self supervised and semi supervised algorithms to the task.<|endoftext|>The authors propose U WILDS, which extends the WILDS benchmark (typically used for domain generalization or subpopulation shift) to the unsupervised domain adaptation scenario. The problem is well motivated, and the use of realistic problems to benchmark unsupervised adaptation methods presents a major gain over prior datasets which rely on different stylized images (e.g.PACS).The datasets used span a wild variety of modalities and domains, and are fairly robust with large sample sizes. They benchmark a comprehensive set of algorithms which make use of unlabelled data, and find that most methods do not significantly outperform ERM, except for some limited cases which the authors characterize in detail. Though the paper does not propose any novel methodology, I believe that it is a solid step towards the use of real world datasets for benchmarking unsupervised domain adaptation algorithms, and would be a valuable contribution to the conference.<|endoftext|>The authors introduce an extension to the new, but popular, data shift benchmark WILDS data sets called U WILDS. Such an extension allows unsupervised domain adaptation techniques to now be evaluated on the various WILDS data sets. The authors suggest that this could motivate the need for better data augmentation techniques for other modalities. This makes the extension of the WILDS data set to this setting a very useful contribution that can facilitate impactful future work. 2) The empirical evaluation was fairly extensive in it s inclusion of a variety of unsupervised domain adaptation techniques. 3) Perhaps a necessary consequence of this kind of work is that the paper serves as a nice survey of modern unsupervised domain adaptation, both in terms of data sets and methods. Weaknesses:1) Ultimately, the novelty of this work is low. 3) Data preparation was not discussed at all. While I recognize the low novelty of the work, I believe the contribution of the extended WILDS data set as well as the insights provided in the empirical evaluation are of significant enough value to the machine learning community to warrant acceptance.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; In this paper, the authors propose a new framework, Bayes Augmented with Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget and demonstrate that BAM generalizes many popular Bayesian update rules for non stationary environments. If it is difficulty, the comparison results in terms of the running time are needed. I have only two minor concerns:For simplicity, this paper focused on binary values for the readout weights as it allowed for a simple greedy discrete optimization algorithm to be used. The paper could be improved if the authors can provide the time complexity of the proposed BAM.<|endoftext|>In this paper, a method BAM is proposed for Bayesian learning in non stationary environments: basically, at each time step, each previous datum may or may not be incorporated into the new posterior, so that old data from different states can be ignored, while old data from the same or similar states is remembered. Strengths:  The theoretical development is principled;  This approach seems to be novel (though I m not familiar with the literature on Bayesian methods for non stationary environments);  The paper is clearly written. Experiments demonstrate it to work well in various scenarios. Assuming that this idea is as novel as claimed, I think this is a valuable contribution to the community and recommend acceptance.<|endoftext|>BAM explicitly models this procedure with the binary selection variables, and by greedily optimizing those selection variables at each entrance of new data, quickly adapt to the change of environment and still leverages the past experiences. Various experiments with non stationary environments demonstrate the usefulness of the proposed framework. The paper is well written and easy to follow. The proposed learning framework is a reasonable solution in principle. All the elements of the algorithm, including the posterior updates and greedy selection procedure, assume the traceability of the marginal likelihood, which is not true in most non trivial real world applications. As the authors pointed out when relating BAM to the existing works, and also suggested as future work, one can allow the selection variables $W$ to be any real number between $[0,1]$. Therefore, I think the current submission is quite incomplete, although I agree that the research direction itself is interesting. This is a reasonable assumption because keeping all the past data requires huge memory. If it is for the past data being used for the update of the posteriors, isn t the vanilla recursive Bayes also augmented with memory?<|endoftext|>The paper aims to solve the downside of the posterior shrinkage of Bayesian online learning when applied in a non stationary environment. The paper could be an interesting contribution in the area of Bayesian online learning in a non stationary environment. To solve this misspecification problem, the proposed method constructs an adaptive prior that is correctly specified for current observations. This adaptive prior distribution is constructed by selecting the previous relevant data samples, which requires storing the whole history. The previously posterior (a.k.a., the current prior) could be misspecified for current observations with the posterior shrinkage due to the environment change. The method is evaluated on various analytical experiments. *Bayesian inference with weighted likelihood*:      Wang, Yixin, Alp Kucukelbir, and David M. Blei. 3) **Large scale experiments**: Current experiments are all analytical. Without demonstrating the applicability in these environments, the approach may be not convincing. More adaptive baselines like Bayesian Forgetting or Bayesian filter can be desirable to exemplify the usefulness of memory in a changing environment.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; (ORF and QMC can be also used for softmax/Gaussian kernel approximation.The proposed algorithm might be inferior to ORF and QMC due to the standard sampling strategies. I increase my score to 8. Besides, regarding to the experiments on Transformers for language modelling, the following refs is missing. This paper proposes a hybrid random features that combines the classical trigonometric and positive random features, which aims to achieve good approximation performance for softmax kernel with both small and large values. I would like to see the response from the authors The algorithm enjoys theoretical guarantees on smaller worst case relative errors. 3, Experiments on various applications are good to support their findings. However, the proposed algorithm in Sec.2.3 to approximate softmax kernel is the exact combination of them. In this case, this estimator (the approximated kernel) under a general matrix A is not well defined in kernel methods. 4, it’s good to see many experiments but evaluation on kernel approximation error is needed.<|endoftext|>The composition is a linear function, such that the new estimator (called hybrid) is unbiased, whose coefficients (called $\lambda$ coefficients) are independent of the base estimators and are also kernel functions (to better adapt the estimator to the compared data) hence also need to be linearized for scalability. In particular the angular one (i.e.$\lambda$ coefficients depending on the angle between the pair of data points) is shown to have low variance and low maximal relative error for small and large kernel values. What is the perplexity score? Are the differences between methods for the three metrics significant? I d suggest to select the most important experiments and present them thoroughly in the main text, while reporting the rest in the appendix. Minor:1.MSE in this context should be defined in the introduction. Three instantiations of HRF that provide low variance estimators. I suggest to improve on this point and I am willing to increase my score accordingly. I am hence increasing my score to 8. Moreover, it would help the presentation of the complex exponential estimators to start from defining the new feature map $\Phi_M^m(u)$ s components $\exp(wMu)$ to clarify first the goal of defining a generalized form of random features that can be specialized to well known ones depending on $M$. I understood only later, when examples of instantiations are provided, the interest of this choice which is to reduce the variance of the estimator. (b) Section 4.1: What study is referred to as complete ablations?<|endoftext|>However, I believe the generalization of Bochner’s theorem is Lemma 2.3, which seems to only be used for the HRFs for clustering structure; these make only a brief appearance in the paper: there is no theory for them and they earn a small synthetic experiment (Section 4.2), but there practical utility is never shown. The idea of the hybrid random features is to activate a particular approximation for pairs (x,y) for which that approximation is accurate. HRFs provide a way to combine existing random feature algorithms. The theoretical results show that HRFs can be superior to either of the “base” random features used. The experiments in section 4.1 on pointwise kernel estimation also nicely support this narrative. ## ExperimentsSection 4.1: This was a nice clean demonstration of the theory. This forces the author(s) to write about $\phi$, which can be either $\alpha$ or $\beta$. Section 4.4: This is a potentially strong experiment. The differences are either statistically significant or not, given a particular test and choice of test size. ## Recommendations 1. 1.Abstract: “By generalizing Bochner’s Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF  mechanism provides strong theoretical guarantees”.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; While nuclear norm based regularizer s solutions are low rank (and therefore more scalable) and have a closed form, their performance is limited. The authors classify these algorithms among two families: 1) the nuclear norm based regularizers and 2) the Frobenius norm based regularizers. Hence if the designed methods bring neither substantive improvements in terms of accuracy nor in terms of complexity, the proposed theoretical framework does not seem finally enough for an acceptance. STRENGTHS   The paper is rather well written with several emphases on the different contributions of the authors.<|endoftext|>The authors studied some linear recommendation algorithms and their relations and also proposed two new methods to benefit from bothnuclear norm and Frobenius norm regularizations. The two new methods have similar performance as compared methods. 1.My main concern about this work is that the performance of the proposed methods is not better than SOTA ones like DLAE and EDLAE. Also, the authors did not analyze or compare the computational complexity. Besides, if two problems do not share the same evaluation criteria but have the same or similar objective function to be optimized, they are still quite correlated. The overall structure should be reorganized. Please unify ‘closed form’ and ‘’close form’3. which make the —> which makes the Please polish the English writing and correct all grammar errors.<|endoftext|>2)	The authors claimed that the proposed method can have closed form solutions. The paper is well structured and easy to follow. My main concerns about this paper are as follows:1)	This paper may help researchers to understand linear recommendation methods from a new perspective, but the theoretical contributions are limited.<|endoftext|>This paper categorizes various linear models based on the applied regularization: nuclear norm vs Frobenius norm regularization. The paper also makes connections between nuclear norm regularization and Frobenius norm regularization, and also discusses L_p norm regularization of the singular values. Hence, obtaining a closed form solution is not a property of the nuclear norm regularization, as suggested/implied by the authors. Section 5:The experiments show that the 2 proposed methods achieve high accuracy. This is only true for the full rank EDLAE model, while it is not true for low rank EDLAE, which is considered here.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; RelNN’s claimed unification of GNN, Transformers, NLM seems far fetched and lacks justification. While I have no problem with the resemblance, how GNN’s computation graph can be rigorously expressed using RelNN’s framework is completely unjustified. The correctness of this claim on unification appears questionable to me before more details are confirmed. Besides the first point, there are many other places where the reasoning seems ambiguous or even arbitrary with abused terms and typos. In summary, I appreciate that the authors make a great effort to unify several important relational neural networks. However, the paper suffers from its weak justification (on the claimed unification), lack of scalability, insufficient experiment, and ambiguous writing style in general. Therefore, I do not vote for its acceptance.<|endoftext|>The paper proposes a unifying study of models for relational learning, namely Transformers, Graph Neural Networks (GNNs), and Neural Logic Machines (NLM), under the RelNN framework, and provides a series of results pertaining to their expressiveness relative to the arity of relations they support, as well as the depth, i.e., the number of iterations, they use during computation. The paper offers an interesting perspective on relational models in that it studies them via a unifying framework. I also find that the RelNN framework does not produce any novel understanding of relational models. However, the results presented in the paper are not significant enough.<|endoftext|>This paper explores the expressiveness of neural network based relational learning models, such as graph neural networks and Transformer. The theoretical discussion of the unified relational learning framework is provided with theoretical analysis results. Different hyperparameter selection may provide different prediction performance, which has been demonstrated in previous research work on graph neural networks and Transformer. It is necessary to present how the models are tuned to achieve good performance under a fair experimental setting. Several concerns for this work: (1) Key experimental setting information is missing in the evaluation section.<|endoftext|>The authors present a framework to study the expressiveness and the generalisation capacity of a class of forward message passing algorithms that encompass graph neural networks, transformers and neural logic machines. First of all, it is not clear if the graphs in the experiments are labelled or unlabelled: if they are labelled it would be of interest to report the label alphabet size and distribution; if the results refer to unlabelled graphs, it should be of interest to extend the experimentation to the more realistic case of labelled graphs. Presenting result on some more challenging tasks would also be of interest, e.g.a graph diameter regression task. Please consider using an adequate testing of the significance of the result differences (e.g.following [Benavoli, Alessio, Giorgio Corani, Janez Demšar, and Marco Zaffalon.<|endoftext|>Neural Networks for relational data have been an active topic of research interest in recent years. There is still a lack of full understanding of the expressivity, generalisability of these models. 2) RelNNs unify three neural network architectures on relational data (GNNs, NLMs, Transformers). 3) The paper studies expressiveness and generalisability of RelNNs. 2) The experiments seem to not compare with existing methods, e.g., [Barcelo et al., ICLR 20]. Adding such a comparison would help making an empirical argument for why the results are significant. Positioning with missing prior work and empirical evaluation on real world hypergraph reasoning tasks would greatly improve the quality of the paper.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The motivation of this paper is not described and supported well. Results of MB \xi learning in the second environment are not provided. Some descriptions need to be improved. MB \xi learning refers to Model based \xi learning, not Modle \xi learning. Also, there are some details, analysis of results should be well discussed. Based on this, I give a borderline reject currently.<|endoftext|>The propose method is based on learning cumulative discounted probability of SF rather than cumulative discounted sum of SF as in linear SF framework. discuss the difficulty of simultaneously learning features and estimating the density of features. There seems to a number of different issues all conflated into one set of experiments. I encourage the authors to divide the tasks into training set and hold out test set, and show the average return on the hold out set. But why the linear assumption in SF framework is bad is not well supported, especially when the features can be an expressive and arbitrarily non linear function of observations. It would be a nice contribution if the concerns are resolved.<|endoftext|>The authors compare the performance of their methods against standard Q learning and SFQL in two different domains. While I like the formalism and the theoretical guarantees, to recommend the paper to be accepted, I would need more convincing on the necessity of the xi function, specially given how difficult it will be to learn in large and stochastic environments. On the experiment:  Authors should specify how the weights \tilde{w_i} were learned for SFQL in the general case. It seems to me that the continuous environment is rather a discretized environment? What did the authors mean by "xi learning reduces the complexity for the function approximation of the xi function compared to the phi function in SFQL"?<|endoftext|>This paper tackles the important problem of transfer in reinforcement learning. The main premise is described as:> Nonetheless, this assumption also restricts successful application ofSF&GPI only to problems where such a linear decomposition is possible. (Any individual reward function is representable by setting $\phi_t   r_t$.) I would expect this to work well in the diagnostic environment studied, but mostly because the underlying state is low dimensional enough that discretizing it and treating it as tabular is a reasonable strategy. 2.If the feature vectors $\phi$ are restricted in such a way that $\xi$ is learnable in practice, then $\xi$ likely ends up in the same spot as SF: able to represent any _individual_ reward function, but not any arbitrary set of rewards due to limitations on the featurization itself. I want to make sure to list the good too: the paper positions itself well in the SF and GPI literature. There is clean code provided.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper builds on top of the assumption that there exist objects with accurate distance information. Then, pairwise relationships can be established between such objects and the anchoring objects. By formulating such relationships as a graph, distances can be better estimated for long range objects. Using such a consensus based formulation, it is easier for the framework to figure out what can be better estimated. Although open source academic datasets can only provide lidar measurements for up to 80 meters, in practical industrial applications much better lidar sensors are available. This actually raises an interesting question as well.<|endoftext|>This paper addresses the task of Long Range Distance Estimation. In particular, it proposes R4D, a framework to estimate the distance of long range objects by utilizing the pair wise relations between the reference objects (objects with known distance) and the target objects (objects of which the distance is to be estimated). 3.It is unclear whether the training/test splits are the same for all the evaluated methods. 5.The distance augmentation is quite interesting. However, it can also confuse the network since the appearances of the objects should look different when the distances of the objects change. However, since I do have some concerns about the paper, my final recommendation will be heavily dependent on the responses of the authors.<|endoftext|>Pros:1.The long range distance estimation task is practical for industry applications. It will benefit the community definitely if the proposed datasets will be released as claimed by the authors. The ablation studies on data augmentation, relative distance supervision, number of reference objects are very helpful for better understanding of the contribution of each component. The work addresses a practical problem in autonomous driving. The motivation and the proposed method are clearly desribed. Nevertheless, I lean to acceptance to the paper.<|endoftext|>since the network is very dependent on such detections it would be interesting to know this problem is tackled. The paper tackles an interesting problem of long distance estimation by introducing a novel dataset and an attention based learning framework. Weaknesses:I think there is a fundamental issue with the data augmentation.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This is an important problem since the high computational cost prevents the use of spherical CNNs in many applications. *Weaknesses*1) The scattering transform utilized has complexity $\mathcal{O}(L^3)$ and consists of cascaded convolutions with axisymmetric wavelets and nonlinearities. In this case, perhaps using cascaded Driscoll Healy convolutions with random, localized axisymmetric filters might make sense? It would also be interesting to see the effects of the scattering pre processing for $L 32$   I wonder if it can improve performance even on lower resolutions, which would make it more widely applicable. 3) There is no mention of a code release in the paper.<|endoftext|>The paper proposes an approach to scale rotationally equivariant convolutions on spherical domains to work with signals with arbitrary resolutions. This is accomplished by leveraging the properties of the scattering transform, by converting the input signal to the corresponding wavelet based representation, before feeding it to some arbitrary spherical CNN architecture. This can be considered as a preprocessing step for the data, as the transform does not contain any learnable parameters. 3.The paper is well written and enough references and context are provided for non experts in the field. Wouldn t in this case the CNN be invariant to rotations as well? does the network converge faster applying the scattering transform?<|endoftext|>The authors propose to process signals on the sphere via an equivariant CNN where, as a pre processing step, a scattering network is used. As these scattering networks can be computed efficiently at high resolutions, this allows for spherical CNNs to be used on higher resolution signals. So how are the scattering network outputs used? I find the presentation of the wavelets and scattering transform unclear. In thm 1, $V_\zeta$ is undefined. The experimental section is severely lacking, and the method has limited novelty. Hence, I cannot recommend acceptance.<|endoftext|>This paper proposes spherical scattering networks, which are scattering networks defined on the sphere and carry the nice properties of scattering networks such as invariance and stability. I thank the authors for this submission. It is a nicely written paper with a thorough explanation of the theory behind scattering networks and also some of the fundamentals such as Sec.2.3.However, there are two fundamental problems:  The technical contribution is not really high as most results come from previous works and the paper essentially contains a rather simple idea. This is a nicely written paper that contains an interesting idea.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Third, they also consider some expressivity properties of simple networks with cosine activations that have width only 1 and depth 3. The reviewer finds the topic studied in the paper very interesting. Expressivity bounds have been studied to understand benefits of depth. However, the case of width is also interesting and appears to be less studied in the literature. Given this, I find the question studied in this work well motivated and natural. The only concern with the present work is the novelty. Even though it is technical, its main message is not too strong. The reviewer does not follow how their "results give theoretical insights on the kind of feature extraction earlier layers need to implement in order to allow the later layers to solve a givem machine learning task", as they state in Section 2. Moreover, the reviewer thinks that the paper has an overall nice collection of (relatively) small results, but none of them is particularly strong. The maximum principle derived is perhaps the most interesting; however, it seems incremental given the Beise et al.(2021) studying the decision regions of narrow deep NNs and Johnson (2018). The other two contributions about width restricted ReLU nets of depth 4 and cosine nets of depth 3 are nice but not in par with an ICLR paper. Nice motivation about the question, but the results presented are relatively weak. There is not a "main" message of the paper that is in par with ICLR standards.<|endoftext|>It first shows the existence of compact subsets that do not allow for such universal approximation guarantee. Then the paper presents sufficient conditions on certain compact subsets so that some universal approximation/exact interpolation guarantee can be attained and these results are targeted for clustering/classification applications. The assumptions on the activation functions are general which includes most of the commonly used ones in practice as well. Strengths :The narrow network regime is not as well studied and is a useful pursuit to obtain a complete characterization of NNs and this paper definitely takes some steps in that direction. Section 3, which describes the maximum principle is interesting as it talks about the existence of compact subsets that cannot have an universal approximation guarantee when the activation functions are monotonic and continuous, even if you consider classes of NNs with arbitrary depth! This can be considered a large class of functions that are used for clustering/classification applications. Or is it that in this case exact interpolation is as hard as universal approximation? It would be nice to have a discussion to consider the applicability of non monotonic activations as well. 2) In  page 4, when the authors mean $\sigma$ is partially constant, do they mean piecewise monotonic? I think the problem is definitely interesting, to understand the limitations of narrow networks and of course the extent of positive results that can be extracted from it. I think this work takes some steps in that direction, but definitely requires a bit more investigation (as specified above) and also the presentation could be made better with some illustrations/visualization of the sets in the main results, which may help communicate the ideas in a better way.<|endoftext|>In the mathematical theory of neural networks, universal approximation theorems typically establish the density of a class of neural networks within a certain function space. In recent years, it has been shown that a width larger than the input dimension is needed to allow universal approximation theorems for continuous function spaces defined on arbitrary compact sets. In this paper, the authors investigate what kind of subsets of R^n allow for a universal approximation theory with neural networks that have a width <  input dimension. The manuscript has three main contributions: (1) A maximum principle when the activation function is continuous and monotonic (such as ReLU), showing that the neural networks must be at least n_0 + 1 wide, (2) An interesting condition on two compact sets (one of them being contained in a cone like sector that doesn’t intersect with the other) that is sufficient to allow exact fits of piecewise constant functions, and (3) A width 1 and depth 3 neural network with cosine activation is a universal approximator for a finite sample set. Strengths:    The paper is well written and easy to follow. The discussion regarding prior work is quite clear and extensive. The authors are investigating an unusual question by restricting the types of compact sets to prove universal approximation theorems. Therefore, I think the manuscript has significant merit in the way that it is investigating universal approximation theorems. I agree with the authors that universal approximation on certain subsets is often what is needed in practice. Weakness:     Some of the theoretical results only apply to monotonic activation functions, which covers many important activation functions, but certainly not as general as related recent papers on deep narrow neural networks. In my opinion, the article is asking an interesting question by restricting to certain compact sets.<|endoftext|>This paper presents three results concerning the universal approximation of functions on compact sets under a width regime for which universal approximation in general is not possible. The authors make a convincing case that this represents an important frontier both for applications and for a better understanding of what neural networks can represent. These results are the following:1) The maximum principleThis is an interesting observation about the maximum (and minimum) value achieved by neural network with continuous monotonic activation and width bounded by the input layer on a compact domain is always at the boundary of such domain. Notably, most of their work there focuses on exact representation, which reflects the fact that they are considering functions that are constant on each disjoint set. For that reason, I do not think that it is that relevant because it has nothing to do with the main theme of the paper. # Major observationsIn Section 4, repeating what I said above, the authors claim through Theorem 2 a result applicable to a "finite collection of disjoint n0 dimensional compact sets" However, except for Corollary 2, which is not explicitly linked to Theorem 2, the entire discussion for the second contribution focuses on only 2 sets. Hence, in Section 6, it is not clear how they claim to be validating the result by using domains consisting of more than 2 disjoint sets. "on arbitrary compact sets is impossible:": does not seem to be the right place to use a commaBriefly repeating the meaning of technical terms can make wonders to accessibility. In Theorem 1, I would recommend expaning the statement as "takes its maximum value *at the boundary* ∂M". # References cited[1] https://arxiv.org/abs/1711.02114[2] https://arxiv.org/abs/1906.00904In full disclosure, I am curious to know what the other reviewers think of the first contribution, but at this point this is the most positive part of the paper in my view. I am not comfortable with the current organization of the results related to Theorem 2 and the corresponding experiments in Section 6.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper proposes and analyses the use of Natural Evolutionary Strategies (NES) to train a VAE with a discrete latent space representation (Spanning Trees). The use is the approach in this contexts requires the use of a number of sampling tricks which makes the approach quite subtle. The authors show that not only is it competitive and often marginally better than state of the art techniques for this problem, it is also much more straightforward and easier to generalise. This is an elegant paper.<|endoftext|>In this paper, the authors proposed to use the Natural Evolution Strategy (NES) algorithm for learning discrete structured VAEs. This algorithm estimate gradients with forwarding pass evaluations only and do not require propagating gradients through their discrete structures. The classical model VAE optimizes both these parameters. too poor experiments, the authors showed experiments that presented effective optimization of the ELBO values and how the latent space size affects the method’s run time. It is theoretical work, the experiments only show the effectiveness of the ELBO function optimization.<|endoftext|>* The presentation of the background material logically leads the reader into the proposed methodology, and the math is included as a meaningful and coherent addition to the story. **Weaknesses and suggestions for improvement:*** Some relevant works are missing from the literature review [1 6]. Contextualizing the proposed method in these works would make the overall paper stronger. * In the experiments, additional discussion would be helpful for why SST might be expected to do better than NES in Table 1, and the disadvantages of SparseMap compared to NES. Overall, the authors present an interesting method for optimizing discrete VAEs with compelling theoretical and experimental results.<|endoftext|>In this paper, the authors use Natural Evolution Strategies (NES) to learn discrete structured VAE. And they also prove that NES converges for non Lipschitz functions in discrete structured VAEs. This is different from the contemporary trend that relies on Lipschitz functions. And how is the effect of the proposed method? Mirrored sampling is used in the experimental setup.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper studies the inductive bias of natural gradient flow. The authors highlight the invariance property of natural gradient flow under reparameterization, and study the training dynamics of natural gradient flow under different problem settings for deep linear models. This paper provides some interesting analyses on natural gradient descent in training deep linear models. The observation that natural gradient flow is invariant to reparameterization looks particularly insightful. However, I also find several limitations of the paper:  The presentation of the paper is not clear. However, Section 2.2 and Section 4 discussed matrix factorization, which, if my understanding is correct, is a regression type problem. Therefore, it is not clear how the analyses in this paper can be applied to matrix factorization problems. The results of this paper are not very comprehensive. Theorems 3 and 4 seem to be much weaker compared with the inductive bias results for Euclidean gradient flow. Moreover, the claim that there exist learning problems where natural gradient descent perform much worse than Euclidean gradient descent is only demonstrated experimentally. For the reasons listed in the previous section, I think this paper is at the borderline, and I currently tend to recommend rejection.<|endoftext|>The paper studies inductive bias of natural gradient descent (NGD) in deep linear networks. Utilizing _reparameterization invariance_ properties of NGD, the authors attempt to eliminate the role of parameterization and isolate the effect of solution found by gradient descent. The paper studies gradient flow dynamics for 1) separable classification under logistic loss and 2) deep matrix factorization. The main subject of the paper is to utilize properties in NGD and study the inductive bias of NGD. However, either current reviewer or the authors are misunderstanding what the “reparametrization invariance” properties of NGD means. In general, the descriptions in section 2.3 where the NGD is described looks okay. The authors even say that “NGD with infinitesimally small learning rate (i. e. NGF) always follows the same trajectory in model space and this finds the same optimum, irrespective of how it is parametrized, provided that the parametrization is _smooth_ and _locally invertible_.”However, as far as I can tell, the authors study parameterization changes that’s not considered reparameterization. Without the justification of connecting optimization of different architectures, I don’t quite understand the point of the analysis. To delve into this issue in more detail, for example in Figure 5 / Section 4, where the authors identify a problem where NGD doesn’t generalize but GD does well with L 2, L 3. I do wonder what the author is actually showing is that L 1 is a bad way (in an obvious way) to solve matrix completion problems. Can authors confirm the NGD was also run on L 2, L 3 cases and obtain non generalization behavior? Also intermediate layer width (e.g.extreme bottleneck width) could break local invertible map, although the paper simplifies the analysis to equal large hidden layer widths. It would be probably worth pointing out and explaining how reparametrization invariance can be applied to models under study. I ve raised my score and lowered my confidence based on the discussion. Without proper justification of their methodology, the reviewer suggest that the paper shouldn’t be accepted to ICLR.<|endoftext|>The authors consider two settings both theoretically and empirically: separable classification with deep linear models and matrix completion via deep matrix factorization. **Strengths:**  I think studying NGD as "a form of ablation by eliminating parametrization dependence" is a very clever way to better understand the complex interaction of the learning rule (and hyperparameters) and the parametrization of the model when it comes to explaining generalization performance in modern deep networks. I thought the background on the interaction of parameterization and EGD in separable classification and matrix completion was very well done. **Weaknesses:**   The major weakness of this work is that while I think studying NGD (either theoretically or empirically) as a means of understanding the role of parametrization in generalization is very original, in the two settings studied (linearity separable classification and matrix factorization) the parameter to hypothesis mapping is already well understood. I was hoping to see how this "ablation" technique could be applied to settings where there wasn t already a good understanding of the role of parametrization to provide *new* insights. Clearly stating what new insights can be gained from this perspective or considering a not well studied setting (empirically or theoretically) would be helpful. If the contributions of section 3 are to provide new, useful insight into the inductive bias of NGD, then I think this could be emphasized and discussed more. I think you could expand here on the question in section 5, "Q: Does this mean NGD does not generalize well?". Correct me if I am wrong but in general these should only be the same if $\mathcal{P}$ is linear. In summary, I think this work is very well written, clear, and addresses an important question (how to understand the role of parameterization in generalization) through an original lens, the study of natural gradient descent/flow. However, the authors did not convince me that this perspective could lead to new insights that were not already well known.<|endoftext|>In this work, the authors study the inductive bias of natural gradient descent in some simple and ideal settings. The authors argue that the invariance of NGD could lead to a worse generation error compared to standard gradient descent (EGD) in some settings. ## Strengths This paper in general is well written and easy to read. This paper is a findings paper. ## Weaknesses There are some issues that could weaken the arguments made in this paper. Some arguments made in this work seem to be reasonable. The authors should clarify whether the FIMs considered in the Experiments in Sec 3.1 and Sec 4 are indeed non singular. However, there are issues that weaken the arguments. I am happy to give a higher rating. I also wonder why the FIM is non singular in this case since Eq 9 implies that the FIM is invertible. This question is closely related to pointer 4. I think all experiments considered should report these three numbers. I do not think the empirical approximation of the FIM over 2500 training data points is good enough since the FIM should be at least a 1000 by 1000 matrix. I would like to see the performance of NGD when the number of input features is changed from 1000 to 50. It will be great if the authors can comment on cases when damping is used. The statement about initialization should be more carefully stated. "Limitations of the empirical Fisher approximation for natural gradient descent."<|endoftext|>This paper studies the inductive bias of natural gradient flow in deep linear networks and in matrix factorization. They show that the solution to the empirical risk minimization problem using NGF is invariant to pixel permutation, and conclude that the desirable property of Euclidean gradient descent under logistic loss of finding the minimum l2 norm solution does not hold (theorem 1&2). They also contribute an efficient NGD algorithm for deep diagonal linear networks. The illustrative figures, and the toy experiments are well designed. The theorems are simple, yet as far as I know novel, and not trivial. The simple conclusions of these theorems provide useful insights into the inductive biases of natural gradient algorithms. ## Weaknesses   It is not clear from the text what is N and D in the beginning of section 3    For theorem 1 to hold true, I think that $\beta_t$ and $\beta _t$ also must start from the same initial conditions (it is not specified in the theorem statement)   I found some parts to be not very clear: between theorem 3 and 4 there is a discussion that starts with "OLS works very differently from large margin methods" and end with "when $D>N \log N$ they end up finding the same solution". And on a different subject, I would have appreciated an experiment on an actual dataset/architecture to illustrate the consequence of your theorems in actual deep nets. ## Typos   section 3: conclusion missing $_t$ in $\beta_t$   "perfectly)" (second to last paragraph end of page 5)   page 6 "to unobserved entries"   "develope"## Other related works that could have been discussed  https://arxiv.org/abs/2006.10732  https://arxiv.org/abs/2008.07545Overall this is an interesting discussion of the different inductive biases of EGD and NGF.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; This paper proposes a few shot learning method, which tries to measure the affinity degree between different tasks. Based on the affinity score, the relevant tasks are exploited for training to boost the performance of target tasks. 2.The experimental results are promising. The paper exploits the empirical Fisher Information matrix. This paper exploits the fisher matrix to measure the relevance between different tasks which is an interesting attempt. In addition, the experimental validate the effectiveness of the proposed method.<|endoftext|>The authors propose a task affinity score based on maximum bipartite matching algorithm and Fisher information matrix. pros:The paper is well written and neatly presented. The idea that leveraging task affinity score to find most relevant tasks for better few shot learning is impressive, and seems to be technically sound. There is no ablation study in this paper, the experimental results are insufficient to validate the idea and many detailed experiments are not provided. Which method does the author use? 9.What is the baseline of the paper?<|endoftext|>This paper presents a new affinity score based on the Fisher Information matrix from a source to a target task. The authors introduce a new score based on the Fisher Information matrix; its mathematical analysis is also presented. The performance is compared with many meta learning methods in the empirical study. The paper is well organized. However, this is not enough to convince the effectiveness of the proposed method. Conditional neural processes. This paper proposes a new few shot learning procedure and shows its effectiveness in terms of prediction accuracy.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; This work shows that, in the setting of few shot classification, MAML is a (noisy) contrastive learner. Complementary theoretical and experimental results are provided. The theoretical results also lead to a (to my knowledge) new proposal, namely to zero out the linear layer after each MAML outer loop update. In their experiments, the authors show that making this small change to the MAML algorithm can lead to meaningful improvements in performance. This seems critical given that the rest of the paper provides conditions and theoretical/numerical results supporting that MAML is a (noisy) contrastive learner under certain conditions. Sorry if I missed this, but did you freeze the encoder in the inner loop for your simulations? This would help to give some experimental indication of how general your theoretical findings might be. I found the proposed zeroing trick (and the supporting theory/experiments) to be interesting.<|endoftext|>The main result is that under the assumption that the inner loop updates are only applied on the top linear layer, MAML actually performs supervised contrastive learning (SCL). SCL shows that MAML learns the feature transformation that makes the intra class feature distances small, meanwhile the inter class feature distances large. The zeroing trick is proposed based on this result, showing performance gain in the experiments. These drawbacks make the paper less insightful as I expected.<|endoftext|>Several concerns are listed above. I would be happy to re consider my recommendation if the responses are reasonable. They conduct experiments to support the contrastiveness in MAML and performance improvement using zeroing trick. Strengths  The analysis is quite interesting. They discover a noisy supervised contrastive loss term in the outer loop loss using the support features as positive and negative samples. The trick is simple and reasonable given their analysis results. Overall, the paper is quite clear and easy to follow. Can the authors experiment other classes, especially some semantically similar classes? In particular, the results show that support and query features of same classes become similar as training progresses. I am ok with the assumption of freezing the encoder in the inner loop and update only linear layer. The analysis focuses on classification.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Basically, this paper simply combines the work of Shi et al.2017 and Chen 2017, Both the innovation and experimental improvement are mariginal. In such a way, the topic assignment of a word can be factored in the computation of the skip gram probability. In regards to the experiments, the proposed model is compared mainly compared with the models in Shi et al, 2017. The writing of this paper could be further improved, particularly those notations. If that is the case, most likely, then it column is not a word embedding.<|endoftext|>3) The experimental results reported are validated on a single dataset,and no human evaluation and error analysis. Experiments demonstrate better topical word embeddings using document vector and better document classification results on the obtained document embeddings by the proposed method over the recent related models. strengths1) This paper is well written and easy to read. This approach is simple combination of  Doc2Vec and STE.<|endoftext|>The proposed TDE method is just a simple mix of the ideas of Doc2VecC proposed by (Chen 2017) and STE proposed by (Shi et al., 2017). But this paper only presents very limited experiments and is lack of clear interpretation of experimental results.<|endoftext|>The experimental results have shown improved performed in terms of document classification, topic coherence and word embedding similarity evaluations. Strengths:+ introduced a single framework that jointly learns topically aware word embeddings and document vectors where topic embeddings are learning using global content and each word embedding can be part of different underlying topics of the document. + clear problem statement + paper is well written  + sound qualitative evaluation of word embedding visualization Weaknesses:  missing empirical comparison to related works in compositional models [1, 2, 3] of jointly learning topic and word embeddings   experimental setup is unclear for document classification task: How is classification task performed? Empirical evaluation is incomplete for document and word embedding evaluation: missing comparison with several strong and most related baselines such as [1, 2, 3] etc. quantitative evaluation is weak   missing related works and quantitative comparison   incremental work
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper presents a framework for likelihood based training of Schrödinger bridge based generative models using the theory of Forward Backward SDEs. To make the paper more accessible to readers unfamiliar with Schrödinger bridges, it would be helpful if you provide a brief review in the Appendix. The paper presents a novel framework for likelihood training of Schrödinger bridge (SB) based generative models. The technical contribution of this paper is significant: it generalizes the SBGM framework and provides a practical algorithm for likelihood training of SB models. It draws interesting connections to (and generalizes) the framework of previous diffusion based generative models (e.g., SBGM). Is this after time reversal? "... which can be _probability_ expensive on high dimensional datasets" — *prohibitively*. I believe it should be $f$. It s good that the authors have reported results from several previous works; however, most of the results are not directly comparable, so it is important to specify what the main baselines are.<|endoftext|>It is also unclear how much difference is this method compared to the original formulation of score SDEs. Different from other Schrodinger bridge works, this paper connects the training to maximum likelihood, and provides a way to compute the log likelihood of the model. 2.Compared to other works based on the Schrodinger bridge, the approach described in this paper seems to be more scalable, without the need to hand design part of the transformation as in Wang et al., 2021, or iterative proportional filtering as in De Bortoli et al.2021.## Weaknesses1. However, it is unclear with respect to which random variable is the expectation taken. What s the definition of $y(t, x)$? In this case, you can derive Theorem 4 and Corollary 5 automatically from the theory of score based generative models. The paper proposes a useful Schroding bridge framework for generative modeling that is simpler than previous counterparts. However, there are concerns on efficiency issues, writing errors, and fairness in experimental comparison.<|endoftext|>In this paper the authors introduce a score based generative model which relieson a Schrodinger bridge formulation. Also the proof seems to be very heuristic, theauthors do not check the necessary regularity conditions to apply Itoformulas. The concept of viscositysolution is not reintroduced in the paper (even in the supplementary material). Finally I have also concerns with the experiments. STRENGTHS: Overall the presentation of the paper is good with a clear presentation of thescore based generative models and the Schrodinger bridge problem. The idea of using the FBSDE framework is new and original. The toy experiments and the generative modeling experiments are satisfactory. The classical ideas of [1,2]cannot be used since the $\nabla \log p_t^{SB}$ is not given by aOrnstein Ulhenbeck process. However the method has a lot of overlap with theworks of [3,4] which is not acknowledged by the authors. I think that the paper is not mature enough and that a true justification of Algorithm 2 with FBSDE is still missing. The authors claim thattheir work is different from [3,4] but in practice Algorithm 1 is the sametraining algorithm as the one derived in [3]. Based on these comments I recommend the rejection of the paper.<|endoftext|>Inspired by stochastic control and forward backward SDE theory, this paper proposed an iterative algorithm to approach the Schrodinger bridge (SB) problem, generalizing variational likelihood based training of score based generative models (SGM). But the paper is not very well written and the practical limitations of the proposed method are not sufficiently addressed. (See more detailed questions below)* Differences with some prior work (such as iterative proportional fitting) are not not well explained, even though the proposed likelihood training of the forward SDE and the backward SDE seems very similar. A thorough discussion of these limitations and perhaps an experiment on compute cost analysis would be needed. Some more detailed comments and questions:* Throughout the introduction, it is not very clear what “computing the log likelihood objective of Schrodinger Bridge” means. The narrative needs more work. The notation hasn’t been introduced. * The last sentence of page 7: can be founded  > foundThe paper introduces a new framework for likelihood based training of the forward and backward SDEs that are inspired by the Schrodinger bridge problem, which is quite novel.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors propose a new  black box optimization  method for learning RL policies in stochastic environments. The simple new method, called Hypothesis Driven Coordinate Ascent (HDCA), does not explicitly require a gradient estimation, and proceeds instead by rolling forward multiple candidate policies in parallel and taking as best performing policy parameters as the iterate update. I found the paper to be clearly presented and reasonably well written. The authors present a simple idea for a new approach for a  black box optimization  method for learning RL policies. In so doing they contribute to a growing literature noting the extent to which simple optimization methods and linear policies are able to achieve comparable results to current SOTA methods on many RL benchmarks. This work is critical to the RL community. The new method presented contains a number of magic numbers however (the number of alternate policies permutations to be considered at each step, the number of steps each policy must be rolled forward, the number of times each policy is to be rolled forward, and the  significance level  /alpha). To my mind there is simply insufficient experimental evidence to assert the claims made in the paper. The authors themselves note, for example, that /alpha is like a "problem specific parameter" but do not make mention of the sensitivity to that parameter in the experiments. Overall I think the authors have presented an interesting idea   one that finds itself in good company in the literature. But ultimate do not present sufficient evidence to support their more general claims. nit: if the experimentation presented for a new optimization approach is only to be run against linear policies or single layer networks that ought to be called out early, perhaps even in the title. The method is simple and clearly explained. The paper is clearly written and reasonably presented. However, unlikely notable previous working in this space (in particular, "Horia Mania, et al.Simple random search of static linear policies is competitive for reinforcement learning" who s work this paper references a few times), this paper presents only a limited experimental and sensitivity analysis. I think the paper would benefit strongly from a more robust experimental treatment of the idea.<|endoftext|>The paper proposes a derivative free optimization algorithm which is applicable to stochastic black box functions. (The target application area is reinforcement learning, but the algorithm is more broadly applicable.) The maximization is performed by random search over the parameters being optimized, so the function evaluations can be performed in parallel. 2.Hypothesis testing: Because of the high variability of Monte Carlo policy evaluation, a one sided two sample hypothesis test is performed to determine whether or not the candidate policy is in fact better than the current policy, and the policy is only updated if this is the case. The paper is well written and clear. However, the algorithm is not very novel, essentially consisting of a combination of existing techniques. My main concern about the algorithm is how well it would scale to more difficult environments. In particular, the paper which made the case for evolution strategies (ES) in RL had experiments on the Humanoid task and Atari games, which raises the question of why HDCA does not compare on these tasks. There are a few things that could make the paper stronger:* Experiments on a domain with sparse rewards, or using high dimensional parameter spaces, as these seem like challenging cases for HDCA. The HDCA algorithm is in principle applicable to any stochastic black box optimization problem, but the paper limits itself to RL problems. I also had a small/unimportant question which could be interesting: why does the block (i.e.indices telling which variables are being optimized) have to be fixed within each iteration? The proposed algorithm makes intuitive sense and performs well on the tasks presented by the authors, but it is not particularly novel, and I have doubts about whether it can solve more difficult tasks. Further experiments could change my evaluation.<|endoftext|>As the paper notes early on, the core of the proposed method is based on two central insights. The second notes that the objective is in fact stochastic: The return can be thought of as a random variable distributed according to the mixture of a possibly stochastic control policy and the possibly stochastic transition dynamics of the environment. In contrast, prior black box approaches to optimization have tended to ignore this stochasticity in favor of optimizing for the expected return (the value). To account for this, the paper proposes a one sided hypothesis test that can inform whether to update the policy based on the significance of a given rollout. Results support the improvement of the approach over ES and ARS on this domain. As the paper notes, there are several conceptual advantages to an approach like HDCA: it is gradient free, and well suited to parallelism. Similarly, core statements are made that I found a bit hard to follow: "the noise in the reward incurred during an RL policy rollout"; what noise is present in the reward? It seems we have assumed that reward functions are deterministic. If so, it would be helpful to be a bit more precise in some of this language. 1.I believe block coordinate ascent has been used for policy optimization previously: See "A block coordinate ascent algorithm for mean variance optimization" by Xie et al.(NeurIPS 2018). Unless I have missed something critical, it seems important that the authors read this paper and understand what HDCA does beyond their proposed algorithm ("Mean Variance Policy Gradient" or MVP). 2.The methods of this paper bear a resemblance to _high confidence policy improvement_ methods, too, in which hypothesis tests are conducted on the value of a proposed new policy to ensure that a better policy is found, based on a set of data. This work and its successors (Chandak et al 2020, for example) seem conceptually quite similar to the method proposed here. I am willing to increase my score (pending the perspectives of other reviewers) if the paper s exposition can be sharpened and the relation to the above two bodies of work can be made more clear. **Minor Questions/Comments**  "with a potentially non convex reward function": Are reward functions typically non convex? "... that ABCA is able to take better advantage of the benefits...": Take better advantage than what, gradient estimation? If so, it might be useful to state that more explicitly. To me, the title does not capture the spirit of the paper. This could be true of both deterministic and stochastic environments, but the sentence seems to emphasize stochastic environments.<|endoftext|>This paper proposed a new random search algorithm for reinforcement learning. This algorithm features a block based parameter search technique and the use of statistical testing techniques to determine reliably the next step search direction. While the algorithm designed in this paper sounds interesting, it does not appear to be sufficiently novel. In fact, random search methods for black box optimization have been studied for many years. Especially, in the evolutionary computation community, various approaches to improve the search performance, including block based search and statistical testing techniques, have been explored extensively in the past to solve various numerical optimization problems. In comparison to existing EC and local search algorithms, it is not clear which part of the new algorithm is truly novel. The authors claim in the paper that reinforcement learning problems are black box optimization problems. On such problems, critical gradient information is hard to obtain. However, the prominent success of many state of the art deep reinforcement learning algorithms, such as TD3 and SAC, have already demonstrated the effectiveness of calculating and using gradients to train policies. In comparison to these deep reinforcement learning algorithms, it is not clear what key advantages the new algorithm has. In particular, the theoretical strength of adopting a gradient free approach for reinforcement learning is not clearly justified in the paper. If so, what is the practical value of the new algorithm? In fact, the authors claimed that the new algorithm can converge to the optimal solution on Swimmer v2. Third, the hyper parameter settings used to solve different problems appear to be different. Finally, it is claimed in the abstract that this paper has the aim to learn robust policies for stochastic environments. However, to my understanding, most of the benchmark problems utilized in the paper are not highly stochastic in nature. This paper introduced some interesting ideas for gradient free policy training for reinforcement learning. However, the novelty of the new algorithm design should be clearly highlighted and strongly justified. The theoretical strength of adopting a gradient free approach for reinforcement learning also requires better justifications. Meanwhile, the empirical evaluation has some limitations.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Graph pooling and graph attention are used for finding key pathways, and Bayesian feature selection is used for finding key imaging features. GUIDE is evaluated on a schizophrenia dataset. 3.The experiments carefully test each property of GUIDE, i.e.compared structured vs. random vs. fully connected graph as well as with and without imaging data. The motivation for this work is partly flawed. How was LD accounted for? 3.There are a lot of hyperparameters in the model. How to set the important ones? Post revision SummaryWhile the authors have addressed all my comments, the results are not quite at the level for a score of 10.<|endoftext|>This work introduces a framework for fusing genetic and imaging data for schizophrenia classification. They also implemented a Bayesian feature selection strategy using a dropout mechanism in discriminative imaging feature extraction. It is interesting to use the ontological gene hierarchy to define a graph for gene embedding and design imaging features selection in a Bayesian approach. There should be more rigorous experiments to validate the proposed method, concerning the target classification task, (1) the existing GCN based methods, (2) the use of preselected SNPs directly, instead of the hierarchical ontology representation, e.g., replacing G EMBED with G MIND in GUIDE, (3) random dropout in imaging feature selection in GUIDE. In this sense, it should be clarified whether the contrast maps generation, regarded as a hyperparameter, was conducted on the training samples only. While the paper is well organized and written overall, the methodological innovation is limited and the arguments were not well supported from the experiments.<|endoftext|>For interpretability, a Bayesian feature selection strategy is implemented to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. The genetics network uses hierarchical graph convolution and pooling operations to embed subject level data onto a low dimensional latent space. The paper is generally well written, clearly presented, and a pleasure to read. Experiments on a schizophrenia dataset show that the proposed method hasthe better classification performance than state of the art baselines, and the biomarkers identified are reproducible and confirmed to be closely associated with deficits in schizophrenia.<|endoftext|>The key idea here is to use the prior knowledge of biological processes as an inductive bias to set up the graph architecture. Major commentsThe paper is very poorly structured with no cohesive approach. For example, the imaging and genetics inputs are discussed way before the actual data is introduced. Given that the key insight of the paper was to use the ontological network, it is important to explain this in detail. Are they the t values or some other measures? It is important to give the exact information, dimensionality of these features.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper works on this problem with top down and bottom up structures. By modeling the input document in a bottom up and top down manner, the model can model long documents in coarse and fine granularity levels. The performance on benchmark datasets looks good. The author mentioned that they use BART as the initialization. The top down and bottom up framework proposed in this paper achieves good performance on long document summarization tasks. However, despite the results, the novelty of this design is somehow limited since the bottom up and top down idea is not new.<|endoftext|>Aiming at the task of long text summarization, this paper proposes a top down and bottom up reasoning method to improve the traditional bottom up converter encoder structure, which has higher memory and computational efficiency. The model captures remote dependencies on a coarser time scale at the top, and the token level at the bottom retains details. At the same time, it has achieved good results on a large number of long text summary data sets. Strengths 1、This paper conducted a variety of experiments on long document data sets of different levels, and proved that the top down structure can effectively improve the performance of long document summaries. 3、The experimental result has proved that the proposed model can effectively improve the performance of long text summaries, which has been greatly improved comparing with other baseline models. 3、In this paper, the experimental verification for calculation efficiency and memory usage of model is not sufficient.<|endoftext|>The paper proposes a new model architecture for abstractive text summarization. The framework assumes a hierarchical latent structure of a document where the top level captures the long range dependency at a coarser time scale, and the bottom token level preserves the detail. StrengthEven a little surprisingly, the performance is super good on all datasets. It beats all state of the art models, including Longformer, LSH, BigBird, etc, by a decent margin (usually more than +1 R 1). The model is just initialized from BART. A lot of prior work shares the same conceptual design principles (hierarchical, coarse and fine grained, local global, etc.), but this model works very well indeed. I don’t mean to criticize the paper because it’s easy to follow. Minor: I am not a huge fan of the usage of “inference” in this paper. Despite the empirical success, it lacks more principled insights and justification about what works behind it.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; This paper studies how to make data unlearnable towards adversarial training. In prior works, including dataset poisoning, no methods can prevent data from being learned in adversarial training. This paper proposed a robust error minimizing noise to solve this task. Experiments are comprehensive. An ablation study would be very helpful. Why is the REM on ImageNet can show the content of the image while not on CIFAR 10? Existing works have demonstrated adversarial training (AT) can remove the effect of both data poisoning using targeted adversarial attacks and unlearnable examples. This paper demonstrated an effective method that can prevent the data from being used for AT.<|endoftext|>However, error minimizing noise is only effective to the extent that adversarial training is not used. Adversarial training was identified as an effective method of overcoming the error minimization noise in prior work. In this paper, the authors proposed a variant of the error minimization noise, which continues to be robust even in the presence of adversarial training. The authors are thorough with the experiments to illustrate the effectiveness of the proposed approach compared to the prior methods  The authors have provided sufficient details for the experiments to make me feel comfortable reproducing their resultsWeakness  The proposed method seems quite straightforward. It seems like the numbers between the two columns are quite differentWhile the novelty of the method is weaker, I do find the study of error minimizing noise to be important. The proposed method is sensible, and the experiments are quite thorough to convince me of the effectiveness of the proposed method.<|endoftext|>Based on a previous work (Huang et al.2021), this paper proposed a method, called REM, that generates makes a dataset "unlearnable" to adversarial training. The authors should 1) cite the work (Yuan et al.2021) and discuss whether the problem targeted by this paper still exists for blackbox methods, and 2) discuss in more detail how REM perform against adversarial training with multiple perturbations, and 3) discuss how to choose the perturbation budget when running REM for different applications in practice. The REM finds unlearnable examples under adversarial training by approximating a min min max problem, which can be very time consuming and limits the practicability of REM. There’s no evidence to prove that "the poor accuracy is not come from overfitting on the training data." Table 7: "AP" should be "TAP"Edit after rebuttal:The authors have partially addressed my concerns, so I am willing to raise my score provided that the authors agree to further improve the paper in the following aspects:1. The paper is well written and the experiments are extensive.<|endoftext|>The paper proposes a min min max formulation to generate robust unlearnable examples in order to protect data privacy in adversarial learning. The basic idea of this paper builds upon a previous paper "Unlearnable examples: Making personal data unexploitable" where the error minimization noise is generated to reduce the training loss such that the model performs badly on the clean data. This paper finds that such unlearnability is fragile in adversarial training and fragile to minor data augmentations. Although this work improves the reliability of the noise generation by replacing the original loss with adversarial loss and considering the expectation of data augmentation, the proposed method is quite straightforward and a bit trivial. Otherwise, it might be more appropriate to put it in the context of data poison attacks.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors develop an ensemble based procedure for semi supervised novelty detection (SSND). It utilizes a mixture of unlabeled ID and OOD samples to perform on near OOD data. A regularization technique is further used  to promote diversity on the OOD data while preserving agreement on ID data. 2.Theoretical insights on the benefits of training with early stopping and the usage of artificial labels Weakness:1. The setting of this framework is too restrictive and thus makes the proposed method less practical for real life applications. Also, it is necessary to investigate how the distributions of the validation set will affect the final performance. Thus, it is necessary to have a more fair ``Vanilla Ensembles" by training all the baseline method 5 times with some randmization techniques, such as random sampling on the training set (bagging). How will the choice of this artificial label affect the final performance? The proposed method have some empirical results on the Hard/Near OOD. Also, the experimental comparison is not fair.<|endoftext|>The paper presents an ensemble based semi supervised learning method for novelty detection. The goal of their training scheme is to create an ensemble of models that has a high disagreement on the out of distribution (OOD) samples in the unlabeled set. The final decision of whether an input is considered as out  or in distribution is based on a hypothesis test using the average disagreement between the softmax outputs in the ensemble. The setting of the paper assumes that a mixture of in  and out distribution samples is available during training and the authors argue for the validity of this assumption in the conclusion section of the paper. However, I am not sure how this relates to disagreement on actual out of distribution samples, which is arguably the goal of the training scheme to achieve best OOD performance. As the method heavily relies on implicit regularization of disagreement between models in the ensemble, I think there could be more experiments in the paper that show that there is sufficient disagreement even on close OOD tasks and how this changes during fine tuning. Also, it would be nice to clearly demonstrate that the improvements over the baselines methods are caused by both the new disagreement score and the early stopping based training.<|endoftext|>The authors introduce a semi supervised ensemble approach to novelty detection. The main assumption is the availability, for training, of clean labeled in distribution (ID) data, and of unlabeled data which include both ID data and out of distribution (OOD) data. The unlabeled data follows the same distribution of the data the ensemble is tested on. Each base classifier is first trained on the labeled in distribution (ID) data. Strengths:  simple idea; easy to understand  a theoretical justification of the approach is provided (in the appendix)   good resultsWeaknesses:  The semi supervised requirement of the approach limits its applicability. The main set of experiments assume that the unlabeled data used during training follows the same distribution of the data used for testing. The paper does not clearly identify the limitations of the proposed approach and scenarios in which the approach would not work. Would the early stopping criterion still be effective to achieve the desired disagreement in the ensemble? The authors need to make it clear in Algorithm 1 that the validation set V is made up of in distribution data. This is not discussed in the paper.<|endoftext|>In turn, the authors develop a new ensemble based procedure for semi supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. The paper is generally well written, however, there is some confusion listed as follows:1. How is the ensemble number selected? 3.In figure 5, is it possible that the decrease of validation accuracy is due to the fitting on the labels of the OOD data rather than the noisy in distribution labels? Since early stopping requires you to do inference per training epoch and it may be more expensive than tuning the hyperparameters using grid search. Since these methods do not require ID labels at all, it has fewer requirements than the proposed approach. I recommend weak reject at this stage.<|endoftext|>The authors proposed a novel transductive novelty detection method using the disagreements of the ensemble models. Then, this framework tries to identify the OOD samples using the disagreement of those samples. The authors provide some promising experimental results to discover the OOD samples in transductive settings. Robustness of the model training is one of the biggest concerns about this paper. Relying on the validation error for early stopping to avoid overfitting to the ID unlabeled samples seem somewhat heuristic. OOD means that the samples from the out of the training set distribution. However, there are still remaining concerns. 1.Practicality of the settings  Not only me, other reviewers also concerned on the setting of this paper. Also, the authors did not answer about re training problem with new test set. 2.Fundamental limitations of the methods  The robustness of the proposed method is highly related to the diversity of the submodels for the ensemble. 3.Early stopping  As I mentioned above, the best validation performance would be the model "before the fine tuning" because there is no distribution mismatch between training and validation. In that case, I think the randomness that the ensemble can get is not enough. It would be good if the methods can work with different OOD data ratios in the unlabeled data. The main idea "Introducing the randomness to multiple models using different label assignments to the unlabeled datasets" would be promising to discover the samples which are not presented in training set.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 3; The authors provide a novel and well thought task and corresponding dataset ComPhy which requires a model to use 4 reference videos to infer intrinsic properties of the objects and thereby apply these findings to a target video and answer corresponding factual/predictive and counterfactual questions. However, in its current form, the exact motivation of the paper is unclear, the reliance on using 4 reference videos compared to simply providing the information is not justified, and some dataset choice such as using only target video for given reference videos are not clear. Then a new query video (which could have more than 2 objects) is given on which 3 types of questions are asked: factual (about what happens in the video), counterfactual (what would happen if some condition changed), and predictive (what would happen after), with later two being multiple choice questions. A particular contribution is the focus is on latent properties of the objects (mass and charge) which needs to be deduced from the reference videos. Multiple video reasoning model baselines are reported along with a newly proposed oracle Composition Physics Learner (CPL) model which shows considerable improvement over the baselines. Models working on other physics based (such as Phyre, Cleverer, Cophy) datasets may or may not work on this dataset (Comphy). 2.Multiple baselines are tested and the proposed model CPL shows significant gain over these. On Motivation: The paper doesn t motivate the problem, and I am struggling to see any direct application. Authors should show that just finding mass or just finding charge from reference videos is doable, but having both in properties together makes the task difficult (I believe this to be the case intuitively, but an explicit expt would be helpful). (ii) The reason for choosing 4 videos as reference is unclear as compared to directly providing the values as input. This could suggest if the model is is able to answer some questions by chance or did it actually infer the physical properties. 4.On Model and Experiments:(i) Intermediate results: Baselines showing results with ground truth object properties should be reported. (iii) The physical property inference is (to the best of my understanding) is applied on 4 reference videos + target video. This setup of being dependent on target video seems confusing to me. As such a model which has seen 4 reference videos should be able to work on a completely new target video as well (see related point 3.(iv)) Could the authors show results without using target video?<|endoftext|>This paper proposes a dataset and a method to study the hidden properties of objects present in videos. The method can extract object attributes and use them for predicting subsequent scenes, with the additional capability to handle counterfactual scenarios. With a focus on inferring hidden attributes, this work tackles a crucial aspect of visual reasoning. Experiments are well designed to illustrate the claimed contributions. My main concerns are about the complexity that the proposed method can handle. Overall, the method is only demonstrated on a simple synthetic dataset with two attributes (mass and charge), while mass only has two values (heavy vs light). Despite it does demonstrate the claimed contributions, to what extent the proposed method would work and generalize is unknown. Additionally, it seems that all objects in a video in the datasets are of interest; there are no irrelevant objects. For a visual reasoning task, the proposed method seems to struggle in the following scenario: If A and B are attracted, and B and C are attracted, A and C should be repelled. This is a good work that fills the gap of visual reasoning of hidden attributes, but experiments could be more comprehensive.<|endoftext|>A synthetic dataset, called ComPhy is created. For each query video, there are four reference videos that the models can use to learn the properties of objects. The dataset also includes three types of questions: factual, counterfactual and predictive. The paper also presents a pipeline approach called CPL that leverages graphs to model the relationship of objects and solve the QA tasks. Reasoning about the physical properties of objects is a critical requirement for designing agents that interact with the world around them. The proposed CPL approach outperforms a number of baseline approaches. The only difference is that this paper infers hidden properties instead of collisions. (2) The dataset is not comprehensive enough. I don t think electricity charge is a useful property to infer since it can be used only in a limited set of applications. Moreover, the synthetic videos are so simplistic (cubes and spheres moving on a plane). It is not clear if a system that solves this task can be useful in any scenario that is slightly more complex in terms of appearance and object interactions. I encourage the authors to create more realistic scenarios in simulators such as AI2 THOR and iGibson. Some metrics should be reported to show train and test splits are different enough. ": This paper is not a video question answering model. "We think the reason is that these models are based on massive training videos and question answer pairs and have difficulties adapting to the new scenario in ComPhy": It might be simply due to the large networks and small amount of data. There is an assumption that all objects are already known and (it seems) there is only one object per each category. It would be nice to see the results of generalization to unseen shapes. It is not clear why CPL shows better performance on counterfactual questions. There is no component in the model to better handle that type of question. Why is CPL called oracle? The reasons for my low rating are: (1) lack of novelty with respect to previous works, (2) dataset not being comprehensive enough, (3) simplicity of the dataset and lack of generalization to slightly richer scenarios in terms of appearance and object variability (4) lack of clarity on creation of the dataset.<|endoftext|>It doesn t seem to me that this kind of knowledge emerges from direct observation or experience in humans. The paper also presents VQA results on the dataset from a modular architecture (similar to NS DR from CLEVRER) trained by supervision. The dataset will not enable any new research directions. The paper also needs more work. I can t recommend accepting at all. Table 2 compares baseline methods (with almost the same column headers as the corresponding table in CLEVRER). And so on. **Strengths**:  the code is public  human baseline scores  some attempt to avoid mistakes in past datasets (e.g.the exclusion of counterfactual questions on objects which have no interaction with other objects)**Weaknesses**:  Dataset: there s no innovation in the type or structure of questions asked. The model: neither the CPL framework nor the implementation of any module is novel. It is entirely supervised, and exhibits little generalization. This shows the effectiveness of CPL for property identification." Several parts are unclear (e.g."To run NS DR successfully in ComPhy, we provide NS DR with extra ground truth physical property labels. Are the modules trained end to end or one by one?). Empirical results:     despite the numerous baselines in the paper, some important ones are missing. For instance: how would an oracle model perform if it understood all object properties but had no inkling of the latent properties (mass and charge)? That would help reveal the significance of latent properties over the remaining properties. More philosophically: do we actually need models which can infer/understand physical properties like charge from visual observation?
Reject; rating score: 6; rating score: 6; rating score: 6; In this paper, the authors consider the global convergence and stability of stochastic gradient descent in a fairly general non convex setting. They are able to remove the often assumed unrealistic uniform bounded assumption on the noise, and also relax the global Holder assumption in the literature. Their discussions in Appendix A provide an example for which the uniform bounded assumption on the noise commonly assumed in the literature fails. This excludes the bad outcomes, e.g.limit cycle or oscillation. The strength of the paper is that it studies the global convergence and stability of SGD under a fairly general non convex setting.<|endoftext|>In this paper, the author studies the global convergence of SGD. Compared with previous results, the authors show that under weaker conditions, the iterates of SGD will either converge to a stationary point of the objective function or diverge. Analyzing the global convergence of SGD is a very important research problem in the area of machine learning and optimization. 2.The paper is well written and very good to follow. Weakness:Compared with previous results, the main difference is the weaker assumptions, i.e.(1) global Holder continuity (literature)  > local Holder continuity (this work)    (2) bounded variance of stochastic gradient (literature)  > the stochastic gradient is upper bounded by some upper semi continuous function (this work). The author claims that their setting is more realistic. However, they don t provide any example for supporting this claim (especially for the second one). I found the manuscript to be clearly written and technically sound.<|endoftext|>In this paper, the authors study the behavior of SGD under very general assumptions. This paper relaxes the global Holder continuity assumption to a local Holder continuity assumption, and consider general noise model assumptions. The main results include global convergence and stability. By global convergence, the authors show that either the iterates converge to a stationary point or they diverge. By stability, the authors show that the objective function remains finite along any iterate sequence. The paper is clearly written and the results seem to be interesting. I would suggest the authors to give some discussions on its motivation and its connection to the existing assumptions in the literature. The results in the paper are of asymptotical nature, i.e., related to the limiting behavior of SGD if the iteration number goes to infinity. Is it possible to provide some nonasymptotical results under the local Holder continuity  assumption and the general noise model assumption? Note that $L_R$ is defined as $L(0,\psi)$ for $\|\psi\|_2 R$. However, there are many $\psi$ with $\|\psi\|_2 R$. If not, then $L_R$ would not be well defined. The authors also introduce some techniques in the analysis.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 6; This paper proposes a message passing formula derived from the tree representation of a graph. I see no difference between the classical message passing scheme and the tree structure proposed in this paper. The classical message passing scheme can also be described by a tree where each node s neighbors are its children. The specific formula proposed by the paper is almost the same as APPNP [1][1] Predict then Propagate: Graph Neural Networks meet Personalized PageRank. ICLR 2019This paper has no novelty.<|endoftext|>However, there is no interpretation on how their proposed model can address this issue. And in the experiment, they only evaluate their proposed model in terms of effectiveness, which is hard to give a good end to the tale in the introduction. The approach section is pretty chaotic. I am really hard to follow the key idea of their proposed model, and the model formulation is not tidy enough. In the conclusion, the authors further propose the Heterogeneous Graph Tree Attention Network (HGTAN) with its formulas. But this proposed model hasn t been validated by any means, and should not be in the conclusion section. The writing is a critical issue for this paper. There are too many grammatical issues throughout this paper. "self interpretive " has not to be interpreted and justified in the paper. The writing of the paper should be significantly improved. Many statements are too subjective and are not be well justified.<|endoftext|>This paper provide two deep learning models to dealing with the tree representation of the graphs. The message propagation is clear and explanable, which is dfifferent from conditional vanilla CCN， GAT and other dervatives. 2.The method is easy to follow and intuitive. 3.The performance reaches that of SOTA. This paper provides a simple and intuitive way to carry out graph learning based on the tree represnetaion of graph. Source code is give for evluation. But I still have the following concerns:1.<|endoftext|>I understand that the time complexity is comparable with GCN, however, I am still curious to know about training time (per epoch) for al the proposed models. The authors propose a novel neighborhood aggregation technique for GNNs based on the tree representation of the graph. I would encourage the authors to apply their model to some of them. The authors have never addressed that in the paper.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 5; If this message is the right one, I would suggest the authors to be more clear on this, and to drop the $\delta$ criterion, which seems to be a proxy for the task difficulty. Although the question raised by this paper is interesting, the proposed answer is not clearly exposed, not rigorous enough on the math side, and not enough significant to warrant acceptance.<|endoftext|>The experiments are not strong enough to support the claims made in the paper. Sections 2 and 3 are long but not informative.<|endoftext|>For example, the threshold 0.6 in Theorem 3.2 Is mysterious and does not seem to stem from any grounded argument. Unfortunately, I have to recommend that the paper would be rejected for the following reasons:1.  the analysis concerns binary classification, which is one among many learning tasks to study.<|endoftext|>The paper investigates the circumstances under which quantum kernel methods will be superior to classical kernel methods. The background section was clear and thorough. The proof hinges on an assumption   "the Z ZZ feature map can effectively simulate the efficacy of the feature map proposed by Liu et al.(2021)"   but I don t see any proof of this conjecture, at least in the mathematical sense.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes a self supervised idea for unsupervised anomaly detection. Specifically, this framework enables high performance AD without any labels via SRR, which is an ensemble approach to propose candidate anomaly samples that are refined from training. Multiple examples are used to demonstrate the proposed methods on effectiveness and robustness. 2.The performance is improved by leveraging the ensemble of the OCC, which works well but may result in additional computational cost. The performance is enhanced by an ensemble of multiple tricks. Overall, the paper is well written and the results and performance look solid. In addition, the code is not uploaded, so I am not sure how it works for reproducibility and time cost.<|endoftext|>This paper proposes an ensemble approach, called SRR (Self supervise, Refine, Repeat), for robust unsupervised anomaly detection. The proposed approach trains an ensemble of K detectors together with a joint self supervised feature extractor g on K disjoint subsets of the data. + The paper is overall presented and written well, as well as technically sound. *Cons*  The methodological novelty of the proposed SRR approach is rather low (ensemble learning is standard to improve robustness and the individual detection method from Sohn et al.(2020) is not new)  The experiments do not contain a comparison to any specifically robust AD approach (e.g.robust PCA for the tabular and robust autoencoders for the image dataset). Is there much of a difference left? The GDE abbreviation is used before it is explained.<|endoftext|>The paper tackles an unsupervised anomaly detection problem where the training set contains an unknown portion of anomalies. When anomalies are contained in the training set, it is known that classical AD approaches  performance degrades. How long does it take? Each model in the ensemble is trained on a disjoint set of training data and then used as a classifier to determine potential anomalies. [Post rebuttal]The authors addressed all my concerns. [Stregnth]The effectiveness of the proposed framework is validated on top of contrastive learning based models, which are state of the arts. Discussions about the important difference between the proposed method and the previous iterative methods would make the paper more convincing. The framework can be applied to other types of anomaly detectors but it is not shown. Although they use AE based models, the idea of iterative refinement can be deployed on top of contrastive models as well. What is the main factor that makes SRR more competitive than these methods? Although it is shown in Figure 7, that any value of gamma improves over baseline, it is still important how to choose this value.<|endoftext|>The authors propose a data refinement approach combined with self supervised representation to robust one class classification, which is commonly used in the anomaly detection scenario. The proposed data refinement approach is designed based on an ensemble of one class classifiers. The proof and experiment results are well organized. The paper is ready for acceptance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; A novel deblurring method is proposed by introducing the concept of reblurring loss. By using such observation, a reblurring module and a deblurring module with reblurring loss are proposed. Following the observation, the proposed reblurring loss is logical and experimental results are consistent. I m concerned that the concept of deblur reblur is similar to the GANs, but the authors well addressed the issue in the introduction. The paper introduces an interesting observation and proposes novel modules based on the observation where the development is very logical. Overall, the paper is easy to read, however more proofreading is required. Experimental results improve the SOTA but there should be justification for some results.<|endoftext|>Additionally, the idea seems to be specific for dealing with blur. If the deblurred image was perfectly deblurred this shouldn t be possible. Strengths:  The main idea of the paper is interesting and the proposed formulation seems to take advantage of the observation. This needs more analysis in terms of: What happens if we minimize this loss (gradient descent till convergence)? The overall idea is that a deblurred image should be close to the target and also shouldn t have information about the original blur. The paper idea seems to be limited to deblurring. Analysis.There are many experiments but there s not too much analysis. This paper introduces an interesting idea for improving image deblurring models: the deblurred image should not contain information/traces about the original blur.<|endoftext|>The deblurring network is then trained in a sense to fool the re blurring network, by generating images that would be left untouched by the reblurring network. This is an interesting idea. The paper also needs more comprehensive results reported for comparisons to state of the art. There are no quantitative results, and the qualitative results are only with TTA. Figure 1 is great in terms of layout, but the captions / legend is not informative making it very unintuitive. A lot of the explanations in the paper are very hand wavy, and not at all clear to the reader.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper studies the relation between generalization and the flatness of the loss landscapes. It would be better to move the last paragraph of the related work (which is currently in the appendix) to the main paper, and discuss exactly the new contributions in light of the three mentioned prior work. Although I believe that empirically showing this is interesting, for this paper to be an empirical study, many more ablation studies and settings should be studied to make the claim practically convincing. Minor comments:  There is a (rather new) study on neural network loss landscapes [1], where the authors show that the best accuracy is achieved when not only the model has converged to a locally flat region but also to a globally well connected region. It would be interesting to compare the Bayesian prior with the combination of global connectivity and local flatness. Weaknesses:  The novelty of the paper is very limited:      Previous work already suggests that care must be taken when interpreting local measures of flatness. In [1] it has been shown that local flatness alone is not enough, and we need global well connectivity as well. The inconsistency makes the results/text less readable. Instead, the main focus is to find scenarios where the correlation between the two and therefore the correlation between flatness and generalization breaks. In contribution number 2, explanations should be avoided. The novelty of the work is very limited and the empirical results are insufficient. All these would help with the flow of the ideas and would help the reader to stay focused on the main outcomes of the paper. In the current shape, the main outcomes are in the shadows: the results of the paper only come after page 6.<|endoftext|>This paper shows that for certain variants of SGD, the commonly held assertion that flat minima imply good generalization may not hold. The overall conclusion from this aspect of the paper is that popular measures of flatness sometimes do, and sometimes do not, correlate with generalization. The authors continue on to propose an alternative measure, the Bayesian prior, which they claim correlates well with generalization. Critical: Section 5.2 it is stated that "In order to generate functions f with zero error on the training set S, but with diverse generalization performance, we use the attack set trick from Wu et al.(2017)."This seems like a critical flaw in that the training process is not SGD, and therefore the distribution of parameters could be substantially different than that of using SGD. and "Eq."tend to look more professional. What does negative error mean? Not introduced as far as I can find. Overall, its very long, there are a few critical concerns about the soundness of the claims, there are some typesetting issues. However, this correlation is not always tight, and can be easily broken by changing the optimizer, or by parameter rescaling". Overall there s very little consistency across figures.<|endoftext|>This paper studies the correlation between flatness and generalization in deep neural networks and show that, consists with some previous studies, the correlation could sometimes be broken. As an alternative, it propose a new measure based on the Bayesian prior upon initialization, and empirically demonstrate this criterion could maintain a positive correlation with generalization even in the case where flatness breaks. It studies an interesting and important question that characterizes the generalization performance of a trained neural network. **Weakness**: I think the paper is still a bit weak in a number of aspects:1. It would be great if in addition to the optimizer comparison experiments, the paper could add experiments to compare *across* different architectures. So it would be good to see how the number of model parameters correlate with flatness and the Bayesian prior as well. 2.There is a new optimizer called Sharpness aware Minimization ( https://arxiv.org/abs/2010.01412 ) that minimize the loss sharpness during optimization and are shown to greatly improve the generalization performance in some tasks. It would be interesting to add SAM to the existing optimizer comparison experiments. After rebuttal: Thanks for the clarifications. It is OK for the paper to be primarily empirical studies, but in this case, a more systematic set of experiments could help strengthen the paper.<|endoftext|>Within the main text, the authors use a more acceptable argument about p(f) having different orders of magnitude but that s different from the claim in Figure 1. They provide arguments for why this is better than using sharpness of the local minima, and support their arguments with empirical results on small datasets. The paper proposes using the log of the prior of the function as a predictor of generalization. The paper is well written. However, the main contribution in the paper seems to be identical to previous works. The authors note that the posterior is related to the prior p(f) by a constant term since neural networks are expressive enough to have zero training error. However, it is not clear where the novelty/contribution is. The only addition in this work is replacing p(f|S) with p(f). In addition, I do have concerns with the way p(f) is computed. This is fixed using reinitialization. One can draw a picture in which that statement does not hold (i.e.that the volume of f is not correlated with p(f).<|endoftext|>In binary image classification setting, the paper shows that when using an attack set (augmenting the training set with additional intentionally mislabeled datapoints) to vary the generalization performance of a fixed neural network (NN) architecture, the (approximation of) Bayesian prior $P(f)$ of the outputs of the trained NN on the test set correlates _very_ well with generalization. This is inconsistent with definition but also makes it hard to understand if it s the train / train + attack / validation set loss. ## Weaknesses:The main weakness of the paper is that all measurements are done under a somewhat exotic setup, that IMO does not resemble a realistic situation where one would do model selection based on some generalization measure. For this reason I find strong claims about the Bayesian prior being a more robust predictor than flatness (e.g.in the abstract and in the conclusion) unsubstantiated. Therefore $P(f)$ cannot be used for model selection between different architectures or priors on the weights. Only the training set and the training procedure can vary.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposes to evaluate out of distribution (OOD) detection methods on adversarial distribution to detect unexplored space of outliers. The proposed benchmark is reasonable in a different way, so I recommend to redirect the goal and rewrite the paper. The Mahalanobis distance method (MD) [Lee et al., 2018] leveraged adversarial attacks for validation in some of their experiments, which is worth to note as a prior work with a similar idea. It is true that there are "adversarially perturbed outliers which can be confirmed as OOD from human eyes," but I think detecting such artificially generated samples is a different problem from the general OOD detection problem. Here I summarize the logical flow in the abstract below, where the transition from 2 to 3 is not convincing to me. 3.To this end, we evaluate with samples from its adversarial distribution.<|endoftext|>This paper studies the problem of constructing adversarial distributions to fool "naturally trained" OOD detectors. This paper contains some interesting ideas, and in general I like the fact the derivations and experiments are clearly presented. This is far from the first work that consider constructing a distribution that is "adversarial" to the OOD. I am also worried about the very strong claims made in the paper, such as "In this paper, we have addressed the limitations of the current evaluation protocol for OOD detection and proposed a novel framework, adversarial distributions, that can be used to investigate failuremodes of OOD detectors." I am worried about the handling of previous work in this paper, and it seems to me to be overselling its contributions.<|endoftext|>This paper introduces a new method for measuring an image classifier s robustness against out of distribution data by using adversarial search/distributions. The paper conducts experiments on various recent SOTA OOD models, and finds traditional metrics don t capture the whole picture when it comes to OOD detection. 1000?This could be clarified in the main text.<|endoftext|>The paper proposes a novel evaluation framework for out of distribution (OOD) detection under worst case scenarios. ## Pros  This work studies an important problem with OOD detection, namely that the arbitrary choices of test datasets may lead to biased/overoptimistic estimates of detection quality. The idea of extending search space of outliers by using a learned autoencoder makes intuitive sense, and the proposed solution appears to address most issues with existing evaluation protocol. If the major concern is the coverage of OOD data used in existing evaluation protocols, can using a larger scale test set (e.g.ImageNet) address the problem? This makes the proposed approach closer to a benchmark of adversarial robustness, where images can be manipulated adversarially to fool a target classifier, especially when the latent dimension $D_z$ of the autoencoder is large. This paper presents a very interesting approach towards an objective and unbiased evaluation of OOD detection.
Reject; rating score: 3; rating score: 5; rating score: 6; The authors do provide a theoretical analysis for robust linear regression models, but the connection/applicability to DNNs is unclear and I also come to the opposite conclusion from the authors, regarding the eigenvalues of the most robust regression model (see below). Based on these observations, the authors propose a regularizing term, which penalizes the variance in the direction of the first principal component (the largest eigenvalue of the feature covariance matrix). In this respect, I doubt that the proposed method would be used in practice.<|endoftext|>The paper has presented a new method to improve the robustness of features under adversarial attacks. Authors developed a new metric for the change of features subject to attacks and key findings are eigenvectors of small eigenvalues are more inclined to change under adversarial attacks, i.e., non robust. Authors propose to suppress the largest eigenvalues during training, i.e., spectral regularization, which show positive results in adversarial defense when working together with SOTA defense models. The theoretical analysis on the other hand also supports the proposal of flattening the spectrum. One of the major issues with this paper is the marginal improvement over the SOTA adversarial defense methods, which can be seen in Table 1 5.<|endoftext|>By analyzing the spectral difference between the natural and adversarial examples, this paper finds that eigenvectors with smaller eigenvalues are more non robust and adversary trends to add more components into these directions. Based on the feedback and the available reviews and discussions, I will keep my current score because 1) this method might not be practical to handle large scale dataset; 2) According to some of the updated experimental results when there are no adversarial attacks in the dataset (and in some types of adversarial attacks), the proposed can actually hurt the performance (compared with the AT). The overall presentation of this paper is clear and the authors have conducted experiments on different datasets, adversarial attacks, and different adversarial defenses. To eliminate the dominance of the top eigenvalues, the paper proposes Feature Spectral Regularization (FSR), which adds more penalties to the largest eigenvalues while relatively increasing the smaller ones.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; Strength:  The paper is well written for most of time. The problem of learning quasimetrics is interesting, and the motivation is clear. It seems that the acronym NTK is never defined.<|endoftext|>I think the paper provides a valuable contribution for the learning of quasimetrics and would be useful for many real world applications.<|endoftext|>This paper considers the problem of learning Quasi metrics. The problem of learning Quasi metrics is an interesting problem with many applications.<|endoftext|>The task of the current paper is to estimate a distance, and I find that the large distortion undermines this approach. One highlight of the paper is Lemma 4.4, which rules out some common approaches to distance learning for this problem.
Reject; rating score: 3; rating score: 3; rating score: 5; The paper proposes an edge independent graph generative model. Besides the model, the paper also proposes the training process of the model and some theoretical contributions. The paper proposes a generative graph model (GGM) and the evaluation is based on node clustering. The paper also explains a type of GGMs and most importantly, it develops the training algorithm and some interesting theoretical results, which seems correct. First, the related work does not cover generative graph models (GGM). There are several works about GGMs that must be included in this paper. Third, the paper should explain why this model is able to model heterophily. So, why does this minimization can model heterophily?<|endoftext|>In this work, the authors claim to propose the first edge independent graph based generative model that can capture heterophily and via producing non negative embeddings, it allows link predictions to be interpreted in terms of communities. Subsequently proposes an interesting approach to solving the same. 2) Via usage of non negative matrix factorization (NMF), the approach in the paper can supposedly output link probabilities which are interpretable in terms of the detected communities. To be more clear, the authors should have added more details about their training procedure and how they exactly achieve the different steps. 3) Building on weakness 1), I did not like the authors complicated three stage approximation procedure. The current version of the draft needs some work.<|endoftext|>This article deals with the task of community detection in the case of mixed membership networks, where a node can have multiple comunity assignments at once. The authors introduce a new mixed membership graph model, where the expected adjacency matrix has the form$$ \bar A   \sigma( V W V^\top), $$with $\sigma$ the logistic function. The negative entries of $W$ allow this model to represent the heterophily of the network, wherein vertices are less likely to be connected when they belong to the same community. In a second part, the authors provide several numerical experiments on real world datasets, studying both the reconstruction of the matrix $\bar A$ and of the community membership matrix $V$. This is an especially interesting question since step 2 involves the eigendecomposition of an $n \times n$ matrix, which implies an important computational cost. It is also further exacerbated by the fact that the supplementary material only implements step 3 of the algorithm, with $k_B$ and $k_C$ set to appropriate "magic" values and $B, C$ randomly initialized. This paper introduces a general model for mixed membership community detection, along with an approximation algorithm to recover its parameters.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The proposed method is named Logit Attenuating Weight Normalization (LAWN), which constrains the weight norm in training with the projected gradient. The experimental results show LAWN is more effective than weight decay in Adam and LAMB optimizers. 1.The proposed method is almost the same as AdamP [1]. The difference is there are two gradient projection operation. But the author did not compare with AdamP and even not mentioned it in     related works. 2.The compared method Adam is not fair. It is well known that Adam with L2 regularization weight decay leads to bad generalization. Although the author mentioned AdamW in related      work section, it seems not be a comparison method in the experimental part. There should be a weight     projection step. But I did not find it in the proposed method. Why do not use weight  projection step to keep the norm of weight     unchanged? If the norm of the weight changes in each step, the author should not state that  the proposed method optimizes the     constrained objective function in Eq (2). [1] Heo B, Chun S, Oh S J, et al.AdamP: Slowing down the slowdown for momentum optimizers on scale invariant weights[J]. The similar idea has been conducted in the work AdamP. And the compared methods are not convincing in the experiment part.<|endoftext|>This paper proposed a network training algorithm to overcome the bad local minima for better network generalization. The proposed method, dubbed logit attenuating weight normalization (LAWN) is to constrain the weight norms to constrain the value of logit / softmax. The experiments justifies that the proposed method can be easily combined with different optimizers and improved the generalization ability of learned network. My major concerns are on the reason/analysis of the effectiveness of LAWN, the sensitivities to its hyper parameters, the clarity on the details of the model, and comparisons. (2) is effective compared with the other approaches, such as the weight decay, LSR, etc., should be more deeply analyzed. (2)?3.The sensitivity of performance to parameter c^l in Eqn. Though it is heuristically set as |\bar{w}^l|, whether the proposed algorithm is sensitive to this parameter should be analyzed. There are several related works on improving generalization of training algorithm by escaping the bad local minima. The authors are suggested to more thoroughly compare with more related state of the art training algorithms in literature. The proposed training algorithm is simple, but showed to be able to improve network generalization ability apparently. I mostly like the results, but the proposed algorithm should be more thoroughly analyzed and compared as pointed out in the above comments.<|endoftext|>It is also shown that LAWN improves network calibration. STRENGTHS:  The results are promising and the paper contains a lot of experiments. However, some of them are very specific to recommender systems. (See below.) The LAWN modification seems to improve the performance of adaptive optimizers on image classification and recommendation tasks as well as the calibration of deep neural network classifiers. The author(s) is/are very transparent about the limits of their method s applicability. WEAKNESSES:  Although the author(s) state in the abstract that they want to address the generalization gap between adaptive optimizers and SGD in the image classification domain, most experiments are limited to very specific recommender systems datasets. The only image classification datasets considered in this work are CIFAR 10, CIFAR 100 and Imagenet, which limits my enthusiasm for this paper. There are some issues with the notation: using $nc$ to denote the number of classes is problematic. Perhaps it would be better to define Equation 3 also depending on some time step t to make clear that this is what line 4 of Algorithm 1 is referring to. I find it strange that SGD failed in the MovieLens and Pinterest datasets for very large batch sizes. Which test accuracy does one get when setting $E_\text{free}$ to 0 epochs, i.e., when using the norm of the initial weight vectors for normalization? MINOR REMARKS:  Multiple wrong citations, e.g., "Hoffer et al (Hoffer et al., 2018)" instead of "Hoffer et al.(2018)" or "Salimans and Kingma (Salimans & Kingma, 2016)" instead of "Salimans and Kingma (2016)"Overall, see some issues with this submission that could be addressed by the author(s) during the rebuttal.<|endoftext|>The paper argues that as a result of scale variant loss functions in classification and ranking tasks, the loss can be made small by pushing logits to large magnitude. This loss adaptivity phenomenon is argued to hurt generalization since the escaping condition in SGD training is difficult to satisfy if the logits are large. The proposed LAWN algorithm is clearly presented and some hyperparameter settings are given in the appendix. # Weakness:My major concern for this work is that it neglects batch normalization (BN) layers which are crucial to the performance of ResNets but are invariant to weight norm scaling. In other words, the loss flattening problem cannot be alleviated by constraining weight norms in networks with BN because scaling weight norms in BN has no impact on the logit scales. If the paper argues that even in networks with BN the LAWN still has its advantage, this argument should be explicitly made in the paper. Though a ResNet50 is used in the ImageNet experiment, I did not find any details on how to deal with BN. Another concern is about the implementation of weight norm constraint. The difference is not so clear from my point of view. Overall, I think the paper s analysis on the loss flattening is interesting and I appreciate the effort of authors. But the current manuscript is not ready to be published at ICLR. I hope the paper can be revised to address my above concerns.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper investigates a method for improving the cross lingual transfer of pretrained multilingual models. The paper first empirically analyzed the influence of representation invariance and distributional class shift. Then, the paper proposed a method to improve the representation invariance and correcting the class shift. Strengths:The proposed method is well motivated based the empirical analysis. The proposed method is well motivated given that the empirical analysis reveal the influence of representation invariance and class shift.<|endoftext|>By also synthetically modifying the class prior for the target language, the authors demonstrate how increasing differences in class priors result in decreasing zero shot cross lingual transfer performance. Building on their observations, the authors propose an approach that:(i) Introduces an adversarial loss term to penalize distortion in average class conditional feature representations between the source and target languages. (ii) Adds an importance weighting term to ensure the approach doesn t fail under class prior shifts. Overall, the paper first presents insightful analysis that highlights the role of feature invariance and class prior shifts on the extent of zero shot cross lingual transfer. Well written and easy to understand. While the analysis in Figure 1 suggests that F1 score is directly correlated with conditional feature shift, is it possible there are other confounds in this analysis; for eg. The paper produces some valuable insights and develops a well grounded approach that improves the robustness of zero shot crosslingual learning.<|endoftext|>In this paper, the authors provide substantial analyses on the cross lingual transfer performance in the multilingual neural language models and reported that the performance is strongly correlated with representation invariance and negatively affected by distributional shift in class priors between data in the src/tgt languages. Based on these findings, the authors propose an unsupervised cross lingual learning method, called importance weighted domain adaptation (IWDA), where it performs feature alignment, prior shift estimation, and correction. The paper is mostly well organized, and provides extensive analyses and experimental results. Considering this, the paper gives a good start with substantial analyses that are helpful to understand what are the key factors to successful cross lingual transfer learning.<|endoftext|>Overall, the paper provides some nice insights, especially treating the problem of cross lingual transfer as largely a domain transfer problem, which allows it to use some domain adaptation (DA) machinery from prior work and apply it to the UCL problem. With the current set of experiments, I believe that the paper will have very limited impact. The main strength of the paper imo is Section 2 which delves deeper into analysing factors that affect cross lingual transfer. After empirically analysing and validating that distributional shifts in class priors might cause a huge problem for UCL (which wasn t tackled in previous research), the authors proceed with an introduction of a new method that aims to mitigate that problem.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposes the LongEst chAiN (LEAN) method to perform structured pruning of CNN networks. It uses the operator norms as the weights of edges. Then it prunes the network by keeping the longest path in the graph iteratively until it reaches the target pruning ratio. Also, the paper shows that a MS D model pruned with LEAN is 10.9X faster than the unpruned network in practice. The idea of using a graph based algorithm for structured pruning is novel and interesting. Since LEAN prunes the network by keeping the longest path in the graph, it can use the global pruning ratio without worrying about generating a disconnected network. Therefore, it may prune much more aggressively in some layers than the other two structured pruning methods. A primary concern is on the evaluation. (a) The paper only compares LEAN with two naive structured pruning strategies, but not other more advanced methods. Why not use those commonly used datasets (e.g., Cifar10, ImageNet) and networks for evaluation? Then I wonder what the speedups of networks pruned with other structured pruning methods or unstructured pruning methods are with such customized implementations. 2.I would like to see some theoretical analysis on why LEAN outperforms the basic structured pruning methods such as structured magnitude pruning or operator norm pruning. It may help make the paper more rigorous. Figure 1 (B) is confusing. I would like to see more discussions on the most recent related pruning works. 3.It might be better also to report the accuracy/F1 score/mIoU of the unpruned network in the paper. 4.It would be good to see the structure of the pruned networks with LEAN in either the main paper or the appendix. But it doesn t give any explanation or discussion. Overall, the idea of the paper is novel and interesting, and the experiment results look promising. However, the paper fails to compare LEAN with the most advanced related works on structured pruning. The datasets used in the experiments are too small, and the experimented tasks are not commonly used in CNN pruning works. So it is hard to justify the advances of the proposed pruning method.<|endoftext|>The authors represent each input, output pair as an operator node, with edges measured by operator norms between operators. The authors then propose an iterative structured pruning algorithm which prunes layers based on the longest path in the graph. They then evaluate their method on three image segmentation tasks. The method is simple in its approach, which allows the method to be easy to understand. Suggestions for improvement are minor: Comparisons to additional methods presented in the related work and corresponding benchmarks would strengthen the results (e.g.ResNet per Dong & Yang 2019). Benchmarks for the full network could help better contextualize pruned results. Replicates are presented in a way that is difficult to distinguish methods from one another. For example it s difficult to distinguish light blue vs gray lines in Figure 3. Additionally, variability of speedup between training runs is not presented in figure 4. Moreover, the relative number of remaining convolutions metric is presented a bit vaguely. It s not clear why some panels have these metrics missing. Additionally, it s not clear how one would calculate what accuracy is used to calculate "relative number of convolutions at equal accuracy" for each graph. Overall, the method is easy to understand and the paper is clearly presented. Some organizational and presentation changes could help make the results easier to interpret.<|endoftext|>This paper abstracts the AI model as DAG and proposes to use the longest chain path with accumulated operator norm as criteria to perform a greedy pruning method. Strengths:Propose the new concept of the longest chain path over DAG, which is used in model pruning. The algorithm is actually in a greedy manner to remove the path one by one. Actually, in previous existing work, there are some discussions about how to deal with the wrongly pruned channels at the early stages of pruning. Any discussion about this issue? 2.Any comparisons on classification benchmarks on imagenet? I think as a model pruning method, it is necessary to compare with other many SOTA pruning methods on classification benchmarks, to demonstrate its superiority. 3.For image segmentation tasks mainly discussed in the paper, the comparing methods are the only magnitude and operator norm, which I think is also not enough. Due to the aforementioned weakness, I temporarily think this paper is marginally below the acceptance threshold.<|endoftext|>This paper proposes a structured pruning approach by building a graph according to the structure and weights of the CNN to be pruned. Pros:1.The proposed approach Use graph based concepts for network pruning, which is a new approach that raises researchers  interest recently. 2.The proposed approach can prune the network with large pruning ratios. 3.The proposed approach works on segmentation tasks. The motivation of the proposed methods is not described well. Avoiding disconnectness seems to be a weak reason. The related work section lists a number of previous pruning works but it seems difficult to connect them with the proposed approach. On the other hand, related graph based pruning approaches are missing. For example, [1] proposes to build a graph for each conv layer and prune filters based on the graph complexity. [1] Convolutional neural network pruning with structural redundancy reduction. 2.The proposed approach lacks theoretical justification. It is not clear why extracting the longest chain in the graph is the optimal choice for filter pruning. 3.The methodology is not presented well and is hard to follow. It is presented narratively, without formal definitions of the concepts used in the approach and equations, making it difficult to understand the implementation details. Specifically,     In Fig.1(B), what do the blue line, (i) and (ii) mean? According to Section 4.1, a node is a channel and an edge is an operator. However, in Fig.1(B) it seems that a node represents the output of a filter rather than a channel? I think the calculation of operator norms is very important and should be moved from the Appendix to the main text. 4.The experiment settings and results are not satisfactory. The proposed approach is not evaluated with benchmark classification tasks such as CIFAR and ImageNet. Besides, only structured magnitude and operator norm pruning are used for performance comparison. Both approaches are proposed many years ago. State of the art methods should be added for comparison. Overall, I think there are several nontrivial weaknesses in the manuscript as listed above. I currently give a negative score but I will consider changing it if the authors  feedback resolves the concerns.<|endoftext|>This paper proposes a pruning method for CNNs by using a graph based algorithm. It was evaluated by using CS, CamVid, and dynamic CT dataset. It is proved to be effective in the evaluation. The size of the databases used for the evaluation seems rather small. It would be better to evaluate this method by using datasets with a larger size to clarify this point. Also the comparison with the existing pruning methods are not sufficient.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes a method for utilizing labeled synthetic data in order to characterize and control the latent space of a VAE trained on individual frames of speech spectrograms. These labeled data are used to identify subspaces of a VAE latent which correspond to each property, essentially by a principal components analysis of the latent vectors from each point in the synthetic dataset for that property. Strengths:   Clear and well written introduction to the source filter model of speech production. At the very least the paper could use a detailed discussion about the how the proposed approach might be applied to other domains of interest to the generative modeling community, e.g., to natural images or at least to more complex speech tasks. But it s also not very surprising, as the problem of decomposing speech into separate source and filter components has been well understood for decades. Sec 4:  It s interesting that $M_0$  is as high as four, when the fundamental frequency should be easily captured by a scalar. Is there an explanation for this?<|endoftext|>This paper analyzes the latent representations of speech spectrograms learned by unsupervised variational autoencoders (VAEs) and discovers that the VAE learns to model the variation of fundamental frequencies (F0/source) and formant frequencies (filters) using orthogonal subspaces. The novelty is also limited and might have limited interest for the machine learning community. Strengths  This paper could be seen as an extension of Hsu et al.(2017a), both of which found that unsupervised VAEs learn to model speech properties such as formants in orthogonal subspaces. However, the novelty and applicability is limited and the generality of the proposed method for controlling other attributes is unclear.<|endoftext|>This paper analyzes VAE latent embeddings to extract subspaces that relate to pitch (f0) and formant frequencies (f1 through fN). The mapping from f0, f1, .. fN values and the subspace coefficients is done through a linear regression mapping independently for each fj which is also learned from synthetic data. The paper is interesting in analyzing the subspaces of the latent space that correspond to each formant and the pitch in a trained VAE. The subspaces were obtained using synthesized speech which is not ideal and it is not clear whether the method would work well on all speech data. Another possible issue is that the model performance is analyzed with very simplistic data: English vowels. Also, I am not totally convinced that this latent space post analysis way is the best way to obtain disentangled latent information. It could be possible to have multiple embeddings that correspond to each fj from the start. The subspaces are found to be "mostly" orthogonal but not exactly, so there may be some leakage between subspaces.<|endoftext|>This paper shows that the fundamental frequency and formant frequency information is encoded in a speech VAE model. This can be found by using artificially controlled/generated dataset. strengths:The fact that one can control the fundamental frequency and formant frequencies of speech by manipulating the latent space is an interesting finding. Finding the subspaces of latent the VAE latent space using artificially controllable dataset is an interesting idea. However, I wonder how much implication this work could give to general audiences. This paper shows interesting findings on speech VAE. I recommend [6: marginally above the acceptance threshold]. 3.Can we still find subspaces when we do not have the artificial generator? References:Some papers the authors can consider to add in the reference:[1] shows that one can disentangle source and filter in a supervised manner.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper does not properly discuss, compare to, and improve upon related work that also uses OOD scores of the form $\frac{\partial}{\partial \ \cdot} \log p_{\theta}(\mathbf{y} | \mathbf{x})$. This paper proposes an anomaly detection score based on the derivative of the log likelihood: $ \mid\mid \nabla_{\theta} \log p_{\theta}(\hat{c} | \mathbf{x}) \mid \mid_{2}^{2} $ where $\hat{c}$ is the predicted class (as we want the method to be applicable to test points).<|endoftext|>This paper proposes to use the norm of gradients as the score to detect OOD data. The idea of using gradients for OOD detection is interesting but not new. Please consider using the ICLR official format. These issues signal a rejection.<|endoftext|>The paper proposes a method for detecting out of distribution examples by applying the ideas of the popular ODIN method to different scoring functions. As far as I understand, the authors try to justify this by creating a vaguely defined dichotomy between "light weight" and "heavy" OOD detection methods.<|endoftext|>The basic assumption is that the expectation of gradient descent should be close to zero vector on the in domain training set. The paper discussed an important problem and proposed a simple yet interesting solution. Please see detailed comments below. Or add some references? This is an interesting topic and I hope to see more discussions of the proposed method.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; 1.This paper introduces the problem of multi objective online convex optimization. 2.The authors propose to use PSG as the performance metric, and show that it is related to the dynamic regret. Writing: The writing of the paper is clear in general and easy to follow. This paper introduces multi objective online convex optimization, which is a novel and very well motivated. 2.The performance metric proposed in the paper, i.e., Pareto suboptimality gap (PSG), has been used in the bandit setting. In this paper, the authors do a great job in Proposition 1, which shows an interesting connection between PSG and the dynamic regret and transforms the metric to a max min form by introducing a weighting vector. 2.It would be great if lower bound wrt m could be obtained. 4.Eq.(1): $x$  > $x_t$I think the proposed problem, regret formulation and methods are interesting and novel, so I vote for acceptance. I have a minor question about the proof, and I hope the authors could help me understand it.<|endoftext|>Based on the Online Mirror Descent (OMD) paradigm, two versions of multi objective OMD are provided and analyzed for the dynamic version of Pareto regret (the static version being left as an open issue). As indicated above, the multi objective online learning problem has already been studied in the bandit setting, but the adversarial full information setting was left open. For the static version, it seems that one can still use the standard version of OMD by pursuing a single objective. Under this assumption, and using the optimal bound in $O(\sqrt T)$ achieved by OMD for the single objective case, we should have a bound in $O(\sqrt T)$ for the multi objective case. Nevertheless, for this dynamic version, the bound achieved by the online multi objective mirror descent algorithm (second version) looks tight. This paper provides a non trivial regret bound for the multi objective online learning problem in the full information, yet possibly adversarial, setting, using the online convex optimization framework. However, I am not entirely convinced that this is significant enough for the ICLR conference since this Pareto regret bound only holds in the dynamic setting. Furthermore, it would be interesting to provide a lower bound in order to justify the tightness of this bound. *** After Rebuttal Phase ***The authors have addressed several issues, notably related to the tightness of the bound for the dynamic regret, and an $O(\sqrt T)$ bound for the static regret. So, I have slightly changed my score. I would recommend revising the paper by incorporating the interesting results obtained in Section D.<|endoftext|>The paper formulates a novel framework for multi objective online convex optimization. The novel framework, similarly to the single objective online convex optimization framework, can be viewed as a two players repeated game where at each round the online learner selects a point $x_t$ and the (possibly adversarial) environment selects a vector valued loss function $F_t (\cdot )$. Two algorithms (OMMD I and OMMD II) which upper bound the extension of the dynamic regret in the multi objective setting are designed. Strengths: A novel framework is proposed. Weaknesses: (1) The suboptimality of $x_t$ in each round is measured using the Pareto suboptimality gap. (2) The most common notion of regret is the static regret which is not bounded by the two proposed algorithms. Moreover, although I read the Remark at page 5, it is not clear to me why having a discrepancy metric which could be negative is a problem. The main weakness of the paper is that the static regret is not bounded by the proposed algorithms.<|endoftext|>There are some concerns about the connection to the literature, assumptions, technical and analytical steps, which are all detailed above. The paper addresses an important question to bound the regret of online multi objective optimization. 2   A deep review of the problem background is given in sections 1 and 2. As a consequence, several closely related works on the dynamic regret of online mirror descent are missed in the literature review, namely:  Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Publicly available at https://openreview.net/forum?id RepN5K31PT3The last reference seems to have a similar idea and even acronym OMMD, but in over a single objective online learning domain. I wonder how often this assumption holds? However, the general form of dynamic regret still can be used in multi objective case. 6   The paper is mostly clear, and I enjoyed reading it.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The reviewer is very interested in this research work. In addition, the authors also conducted experiments on many vision datasets to illustrate the superiority of the proposed method and achieved performance improvements. However, there exist some confusing parts or weaknesses to be further strengthened. Much more details should be provided here. 5.Another concern of the reviewer is that evaluation results of this paper are not convincing to the reviewer.<|endoftext|>Experiments on rotated MNIST, CIFAR10/100, and Galaxy10 show that the method outperforms the non separable versions both in accuracy and speed. *Weaknesses*W1: The submission has somewhat limited novelty. If not, what would be the applications where the method is useful? W4: In Table 5, I believe a few experiments are missing and would be needed to disentangle the effects of the separation along the group dimension and the channel dimensions. I suggest to show, for each group, the performance when the convolutions are separable over the channel (depthwise) but not the group dimension. Why is this the case?<|endoftext|>The paper builds group convolutional neural networks based on the depth wise separable convolution operations, which are commonly seen in modern CNNs. The authors demonstrate that the $Sim(2)$ equivariant can be achieved in such separable convolution operations. I have a few questions to ask:1. The motivation is explained as the group convolutions learn redundancies. Do separable convolution operations address this problem finally? 2.Although the utilization of SIREN is the highlight of the paper, the motivation behind is unexplained and unclear.<|endoftext|>The paper adresses redundancy in group convolutional filters and the scalability of group ConvNets. It allows to mitigate some artifacts of raw data The weaknesses are as follow :1/ The proposed model is quite generic. 2/ Lack of comparison with state of the art models3/ Need a further theoretical study Small typos in appendix, before equation (9) "the group group"Following the aforementioned consideration, l recommend to accept the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; rating score: 5; The empirical results are really impressive. Concerns:  The design of this method is complicated and may introduce more parameters. This may cause problems that the model may tend to overfit the data and make it unclear if the improvement comes from the increase of the number of parameters (I wonder why the #Param does not increase so much as listed in the tables as the sizes of A_i s in (14) all double?). How the positional encoding proposed in the paper compares to relative positional encoding in GNN, e.g., that used in Graphormer (Ying et al.2021)?And is it possible to further boost the performance by combining it? The proposed method has strong motivation and empirical results and I vote for an acceptance.<|endoftext|>This paper introduces the idea of learning positional representations to enrich node representations of graph neural networks(GNN) to improve the representation power of message passing graph neural networks(MPNNs) while keeping in linear complexity. **Strengths*** The problem of message passing neural networks(MPNNs) lacking positional information of nodes is well motivated by molecules. * Well written and provides detailed related works. Also, the motivation of this paper is elucidated via molecules where learned node embeddings do not differentiate between positions. Thus, the existence of this newly introduced loss seems questionable. * For the large scale graph dataset( around 430K graphs), OGBG MOLPCBA, which is bigger in the number of graphs and more complicated in terms of learning, the benefits are questionable. It seems the choice of aggregation outweighs the effect of positional encoding[2,3]. My main concern is that the paper claims it is doing more than the listed contributions. First of all, introducing loss for positional encodings seems not working at all. Idea working on the molecular domain only does not imply that this idea works well. Experimenting on domains like knowledge graphs would be nice to see its results since the position of nodes also plays a significant role in that domain. I would expect a discussion on why these methods outperform the proposed plan as I believe it is important to tell readers why these methods outperform yours. Overall, it is a good paper with a simple idea, but I would expect more reasoning on why this method can fail and more experiments on different domains.<|endoftext|>Then it may be beneficial to show that the existing approach actually does not work with larger graphs. This paper is concerning the Positional Encoding (PE) for GNNs. I find the proposed idea reasonable and straightforward (in a positive manner). But I m not fully sure the experimental results well support the claims. The manuscript tests two ways of PEs, one is based on the Laplacian, and the other is based on the random walk. The manuscript also proposes the PE only loss to foster the training. After author feedbacks:I feel the answers from the authors are largely satisfactory. Other reviewers raise a concern about the novelty of the proposed methods. ( ) Initialize of PE and the LSPE architecture, which item is important? Therefore I modify the review score one step upward. I am rather new to the PEs for GNNs, but the manuscript helps me understanding the current PE research status and how we can place this work in that context. (7 9) are clear to understand. My main concern is related to the experimental results. If the LSPE obtains small p values in the MOL datasets, then, it is great! Another is to test more diverse datasets. If the LSPE is really powerful (I believe so), the LSPE will achieve clear score improvements in many additional datasets.<|endoftext|>In this work, the authors mainly focus on the node positinal embedding for GNN. The proposed positional embedding is geneal and can be applied to many GNNs. And also update the positional embedding as well e.g., in Eq.(9). 3.The results are good in the experiements. As claimed in the paper, the main different is that the Eq.(9) use the tanh function. Experiments on different activation function may be more convicing. However,  the use of the tanh activation function to allow positive and negative values for the positional coordinates. Position aware graph neural networks. In summary, I think the proposed method is this work is very practical. It seems that using a simple positional embedding help a lot. However, it seems that the motivation is not very strong, like how to explain this positional embedding.<|endoftext|>This paper proposes a framework that utilizes Random Walk Positional Embeddings (RWPE) as extra features to boost the performance of GNNs. In particular, positional embeddings are updated as a separate forward network in each layer. The framework has demonstrated improved quality by injecting its positional embeddings and feed forward structures in several GNNs. The separate feed forward network of RWPE demonstrates improved accuracies and can be a simple add on to multiple GNN structures. The added LapEig makes the two options of LapEig and RWPE vague in their motivations. While the authors claim that RWPE is better, I was wondering why having the positional embeddings in the “final” layer would contribute to a better output. 2.The RWPE itself is not novel, as the authors have also pointed that the idea is inherited from a previous. Specifically, it is not clear whether the gain attributes to the specific structure instead of the added parameters of the feedforward network. Moreover, the added positional embeddings are also adding more capacity to the network. The paper discusses new position embeddings and the feed forward networks.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 6; This paper proposes a context optimisation (CoOp) approach that can automate prompt engineering and allow more efficient and task specific transfer for pretrained vision language models. The description is rough. It is not super clear why adding extra dimensions (claimed as the same number in that of word embedding) to the class token can help the prompt.<|endoftext|>The simple yet effective approach substantially beats hand crafted prompts with a large margin. Meanwhile, CoOp also exhibits better robustness to distribution shift than CLIP. Weakness: 1) Though effective, the technical novelty of this paper is quite limited. Since the soft prompt tuning approaches (e.g., Prompt Tuning [1] and P Tuning [2]) have already been proposed in NLP domain. “The power of scale for parameter efficient prompt tuning.” EMNLP 2021. Hence, can we say that CoOp is just a better version of fine tuning?<|endoftext|>The paper s motivation is clear, experiments thorough, and resultsconvincing. While the authors cite the arXiv version,I think that, given the timing, the authors should perhaps not pitchtheir method as a "novel" approach (as is done in the abstract);rather, this seems to be applying prefix tuning to CLIP. While there were a few presentation/technical concerns,the main drawback of this work is novelty: the proposed idea isidentical to Li and Liang (2021) [arxiv in january], Zhong etal. I also had a few presentation concerns:1.<|endoftext|>This paper provides a novel approach named CoOp for prompt engineering based on  CLIP. Only the results based on CLIP are compared, and there are no more experiments of other visual language models. 2.It is an improvement of CLIP, and the idea is similar to many existing works such as [1]. [1] Prefix Tuning: Optimizing Continuous Prompts for GenerationThis is a somewhat novel but solid work. The comparative experiment based on clip is very sufficient, but it also lacks other important experiments, such as the effect of CoOp on other vision language models, just as the title of the paper is learning to prompt for vision language models
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors attempt to improve online continual learning (CL) performance by eliminating the logit bias of the classifier used. They use a nearest class mean (NCM) classifier and a multi similarity metric learning loss coupled with an auxiliary loss to achieve a good plasticity stability balance. In combination with the NCM classifier, the authors propose a hybrid generative discriminative loss. This makes sense but it does not justify using a NCM classifier instead of a softmax. Moreover, the authors are never in this paper involved in actually generating latent space data. Regarding the experimental results, the authors should probably compare their approach to the SCR method as well (see [1]). In my opinion, the explanation provided as to why that is is only speculation. Hence, I recommend rejection.<|endoftext|>The paper proposes a hybrid loss of generative classification by NCM as a form of Multi Similarity (MS) loss (Wang et al., 2019) and discriminative classification by Proxy NCA (Movshovits Attias et al., 2017). **Weaknesses**  W1: The proposed method is a combination of existing loss. But there is no descriptive analysis for it. W5: Lack of details of *Smooth* datasets in Sec.4.3.W6: Missing some citation (or comparison) using logit bias correction in addition to Wu et al., 2019 and Anh et al., 2020	  Kang et al., 2020: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp &arnumber 9133417	  Mittal et al., 2021: https://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mittal_Essentials_for_Class_Incremental_Learning_CVPRW_2021_paper.pdf  W7: Unclear arguments or arguments lack of supporting facts	  4th para in Sec.1		   It should be noticed that in online CIL setting the data is seen only once, not fully trained, so it is analogous to the low data regime in which the generative classifier is preferable. Why?5th line of 2nd para in Sec.3.1.3		   This problem becomes more severe as the the number of classes increases. > (1) is confused with Equation 1. Captions and legend s font should be larger (similar to text size) in Fig.2 and 3.Despite the benefits of the theoretical arguments and somewhat convincing results, lack of novelty and missing details prevents this paper from being a quality to be accepted now.<|endoftext|>To address the problem of logits bias in class incremental learning with deep neural networks, this paper proposes to replace the typically used softmax output layer by a nearest mean classifier (on the feature space), which can be interpreted as a generative classifier. Strengths: 	Identifying generative classification as a promising alternative to softmax based discriminative classification, and demonstrating that nearest class mean classification (NCM) can be interpreted as generative classification. The reported performance of the proposed method is substantially better than the existing methods that are compared against. It is unclear to me exactly what the authors mean by “online continual learning”. Does it mean that there are no clear task boundaries, or does it mean that task boundary information is not provided to the network? It is also unclear to me why task incremental learning cannot be performed in the online setting? My feeling is that for all problems considered in this paper, if the data is provided to the algorithm in an i.i.d.stream, the discriminative classifier would perform better than the generative classifier. It would be good to expand a bit on that. There are some differences with these papers, but I think it is necessary that this paper clearly discusses what it contributes on top of these papers. My current scores are my “expectations” for this paper after a discussion of the above papers (and corresponding moderation of some novelty claims) has been incorporated.<|endoftext|>This work develops a novel generative framework to bypass logits bias in online continual learning. At test time, they leveraged reserved samples to generate a generative classifier instead of a softmax classifier. Furthermore, they introduce an auxiliary loss to improve the ability of multi similarity loss to learn from new data. Why the multi similarity loss alleviates catastrophic forgetting in continual learning? •	The hypothesis of this paper is based on that the feature is well discriminative. •	It is not suitable to train the model 5 epochs on i.i.d offline setting for almost datasets. However, some unclear explanations and analyses are existed.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The contribution of the paper comprises:* the observation that the Multi Stage Vision Transformer (MSVT), as opposed to the "monolithic" Vision Transformer (VT), does not produce discriminative patch representation, and* a loss function for self supervised pre training of MSVTs, that encourages discriminative patch representation. It misrepresents the effect of the contribution on performance in the teaser figure and in the first table with quantitative results. Weaknesses:1) I find the contribution, in the form of the loss term, incremental with respect to Swin, and the performance gain, demonstrated in most experiments, modest. This is a solid work and deserves to be published. 3) In the current transfer learning results, the impact of the authors  contribution on transferability of the trained networks is not clear. Could you clarify this? 1, please specify W 14 is the window size. I think the paper deserves a publication.<|endoftext|>The paper investigates how to use self supervised learning for multi stage visual transformer models. This work tries to merge these two trends together. The solution is a new region based loss that can be applied to the local features. It is interesting to see that the loss on local features can work well in the self supervised learning case. The experiments are comprehensive and convincing. In addition, the detail of Table 2 is too scarce to understand directly. Overall, the manuscript looks like a good paper.<|endoftext|>This paper develops an efficient self supervised vision transformer for learning visual representations. It introduces a multi stage architecture with sparse attentions to reduce computation complexity and proposes a new pretraining task of region matching to capture fine grained region dependencies. Strengths:  (1) It introduces the new pretraining task to capture fine grained region dependencies;(2) The experimental results are good and better than other compared approaches. Weaknesses:(1) The patch merging module and sparse self attention in the multi stage ViT are very similar to the patch merging in the paper Swin Transformer [Liu et al 2021]. The authors should clearly explain the differences between this paper and Swin Transformer.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper studies the properties of gradient descent with large learning rate in the matrix factorization problem. The goal is to understand when gradient descent converges to a global minimum where the two factors are roughly balanced in norm, which is a shallow minimum that may be hoped to generalize better. For small learning rates (i.e.where classical optimization results guarantee convergence to a critical point), this does not happen. I marginally recommend rejection. The paper is for the most part very clearly written. In particular, the main theorem for the general case assumes that the learning rate is such that GD converges to a global minimum. This seems to ignore one of the main difficulties, which is proving that even with a large learning rate GD will converge (since classical optimization arguments break down for large learning rate, and it seems quite plausible that GD could have chaotic or periodic behavior).<|endoftext|>This paper studies gradient descent optimization of matrix factorization (various forms, including the most generic case) under large learning rate. The authors prove convergence results that apply when the learning rate is larger than 2/L and up to ~4/L, and also show that optimization with large learning rates leads to dynamic balancing between the two matrix factors in the factorization, an effect that does not arise when using small learning rates. However, I do think the citations need to be improved. In particular, a number of the findings & discussions in Lewkowycz, et al.(2020) have some overlap with the results proven here but are not cited. I think this paper derives useful theoretical results on large learning rate optimization of matrix factorization that will be of interest to the community.<|endoftext|>The authors study the problem of minimizing matrix factorization squared error with gradient descent. They show convergence with larger learning rates than previously considered. They further show that gradient descent has an implicit bias towards finding matrix factors which are magnitude balanced in a certain sense. The technical contribution looks significant. However, I am not able to give a recommendation of acceptance due to the following issue:* In line 160, the upper bound does not seem to be $\frac{4}{L}$. Given this problem, I request the authors to argue how the proposed learning rate is large. Specifically, large learning rate compared to what?<|endoftext|>This paper is concerned with the non convex matrix factorization problem. The same is true for Theorem 4.1, 4.2, and 4.3. This paper aims to understand to which minima gradient descent with large learning rate ( step size) converges to. Also I feel that the presentation of the paper could be improved. I  feel that the theorems do not really support the claim that only large step size leads to balancing (in contrast to small step size). Then the theorem gives an upper bound on the balancing between x and y. (A larger choice of different step sizes and also a larger amount of initializations, for example scale of initialization and balancing at initialization.) Also it might be made more clear in the caption that the left two figures correspond to one learning rate and the two figures on the right correspond to one learning rate. What does "more complicated convergence" mean?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; Recently several works have proposed semi supervised learning methods to leverage unlabeled biological sequences for learning their general purpose representations. In this work, the authors proposed the Self GenomeNet, a novel contrastive learning method for nucleotides based on the reverse complement (RC) context prediction. The authors claimed that the proposed method considerably outperforms previous self supervised baseline models on three benchmark datasets in both self supervised and semi supervised evaluation  	While the paper has its own merits, my biggest concern is that the connection between motivation and the proposed method is not crystal clear. The proposed method used one subsequence and the RC of the other subsequence as the positive pairs. Please provide some missing experiment setups. To be self contained, please provide more algorithmic details for the compared baselines. Do you have any specific reasons for using the recall as the evaluation metric? What is the difference between the forward model and the CPC model?<|endoftext|>The authors proposed a self supervised learning method for nucleotide level genomic data utilizing reverse complement of genomic sequences. In addition, the authors proposed an architecture called Self GenomeNet that handles varying length genome sequences. The domain specific data augmentation (e.g., reverse complement) improves the representation power considerably. * The improvement compared to baselines CPC, Language model, and even supervised learning method is considerable. **Weaknesses*** The proposed method is constructed by existing methods and beyond the genome data, the benefits of the proposed method are limited. The proposed method is a simple and effective contrastive learning method and shows great representation power. The proposed method showed the best performance in many different settings such as linear evaluation and semi supervised learning. However, the proposed framework is overall a simple combination of existing methods and beyond genome datasets, the impact of this proposed method is questionable.<|endoftext|>This paper presents a self supervised learning approach using contrastive loss for representation learning of genomic sequences. The contrastive loss has been used for self supervised learning in the NLP and computer vision domains and the paper presents its application for genomics. Its performance is compared to the supervised model, generative language model, and self supervised learning models   CPC and Contrastive sc. The results show improved classification performance over the baseline for both supervised retraining and semi supervised training settings. Strengths:+ The idea to use self supervised contrastive learning for genomic sequences is interesting and well motivated+ Proposed changes to the existing model are domain specific and relevant for the application of the method for genomic tasks+ The improvement in classification performance over supervised models is promising, especially for 0.1% and 1% labels cases, is promising. Weaknesses:  The paper s central claim is that self learning based methods can help with genomics tasks with limited data. However, the datasets used in the paper do not reflect those tasks and have an ample amount of labeled information. While the label scarcity is simulated in some results, it would be helpful to demonstrate the applicability of Self GenomeNet for classification for genomic tasks with limited labels. The rationale for the choice of the datasets, models, and baselines is not entirely clear. Are there existing deep learning models performing this task?<|endoftext|>The self supervision is doneby predicting the end of a sequence from its start (both broken intosmaller subsequences), through a contrastive loss against other randomsequences. The method is extensively tested on severallearning tasks, where it shows good performances. The proposed contribution is novel, and doesseem to improve the prediction performance on a variety of tasks. Theevaluation is thorough, including large datasets from differentdomains of the tree of life, relevant baselines and assessments onfine tuning, semi supervised and transfer learning settings. My main concern is on the clarity of the manuscript, in particular:  The terms "encoder" and "context" networks are used without being  clearly defined. It seems like the network could learnto be RC invariant, but that the invariance is not built in. Maybe moreimportantly, is the relevance of this self sueprvision universal, or could it beless efficient on some other tasks? "Self GenomeNet uses the embedding representation of a contextnetwork for downstream tasks which learns from reverse complement as atarget for its predictions."
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper presents a novel graph contrastive learning model with bootstrapping latent objectives. Extensive experiments demonstrate the effectiveness of the proposed BGRL method. + Extensive experiments demonstrate the effectiveness and efficiency. # Weaknesses  The novelty of this work is a bit limited as this is a simple adaptation of BYOL on graph structured data. It feels to me that no obvious weaknesses can be spotted. In this case, how to choose appropriate representations (e.g., graph  or node level embeddings or some sort of context representations) for both branches is important.<|endoftext|>This paper introduces Bootstrapped Graph Latents (BGRL) to learn graph representation by predicting alternative augmentations of the input. The paper can be evaluated mainly from two perspectives:1. The novelty of the method and the conceptual contribution is extremely limited since the proposed model is a pretty straightforward application of the BYOL [1] model on graph data. Explanations from the following points may be considered: 1. Considering the empirical contribution of the paper, I am inclined to accept this paper.<|endoftext|>The paper proposes a self supervised graph representation learning algorithm named BGRL. I appreciate the empirical results provided by the paper, which is useful to the graph representation learning community. (2) The paper tests the effect of introducing label supervision to the BYOL based scheme on learning node representations for large graphs, which is not studied in previous / concurrent BYOL based methods. Weaknesses:(1) My major concern is the novelty of the paper.<|endoftext|>This paper proposes BRGL, a method for graph representation learning based on ‘bootstrapping’ (in the same sense BYOL is). An extensive suite of experiments shows that BRGL scales better than previous works while yielding state of the art (SOTA) performance in node classification. * An extensive suite of experiments shows BRGL achieves SOTA while avoiding the performance memory tradeoff of contrastive methods. # Weaknesses* BRGL is a direct analogue of BYOL for graphs (but without a projector network). * The authors provide no intuition on why their method works. The paper shows good results, but i) the novelty is relatively limited; and ii) there is no intuition on why the method works.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; rating score: 6; In this work, the authors propose the WARM method to help conduct iterative and interactive weakly supervised learning. Active learning is used here to refine the labeling functions by focusing on data points that are once labeled. Experimental results show that the WARM method can improve the quality of training data. 2.The general idea of active learning with weak supervision is not novel, which can be seen in [1]. NIPS 2015: 703 711The authors should compare with more related works and provide some theoretical analyses to make this work more convincing. The ideas are totally heuristic and the experimental results are also not satisfying.<|endoftext|>This paper proposes WARM, an active learning approach to weakly/programmatically supervised learning. In the WARM approach, which bases off of the data programming/Snorkel paradigm for weak supervision, users write labeling functions (LFs) to programmatically label training data; these labeling functions are then modeled by the Snorkel framework for weak supervision and used to train downstream models. The paper then proposes an active learning approach to sampling labeled data points to tune the parameters of these LFs, and validates this approach on several medical datasets. (S2) This paper introduces a clean formulation of / argument for LFs being cast as differentiable functions, whereas to date most LFs have been non differentiable  (S3) The paper shows some strong results relative to recent approaches. From an active learning perspective: the query function is just based on model uncertainty, which is the most basic type of active learning query function (the only tweak being that it is the label model, i.e.model over LFs, but this does not change anything from an algorithm perspective). (W1.a) The authors state that the data programming label model is not differentiable with respect to the tunable LF parameters introduced in WARM, which is not true. How should we think about this more broadly beyond the medical settings treated?<|endoftext|>This paper proposes an algorithm for choosing a small set of labels to improve labeling function model performance both directly and for downstream tasks. Additionally, the authors provide a general method to convert standard labeling functions to "soft" labeling functions which are differentiable with respect to some parameters (e.g.a threshold). Finally, experimental results show that the method introduced outperforms other active labeling approaches for weak supervision. Why are the labels not used in estimating the labeling function accuracies? In particular, the curve without noise peaks with 20 labels and then starts to deteriorate. It s interesting to note that WARM s downstream performance never improves over the accuracy of the label model. Perhaps using a more expressive model (random forests?) Additionally, it appears that weak supervision is not appropriate for the paper s empirical settings as seen by the lack of improvement from the downstream model and the stronger performance of non weakly supervised methods (active learning).<|endoftext|>This paper proposes a new method for data programming, i.e., using weak supervision to generate probabilistic training labels for unlabelled points using heuristics devised by domain experts. In particular, the authors propose WARM, a framework for iteratively improving these weakly supervised models by modifying the parameters of labeling functions and directing users to a subset of data points that, when labelled, would most improve the model. 2.To my knowledge, the authors’ proposed method (i.e., actively refining the voting weights of each labeling function and the parameters of the labeling function) is novel. I greatly appreciate the application to a real world scenario! 2.As mentioned by the authors in Section 5, the active learning baseline outperforms WARM on half of the tested datasets. The authors say that this is due to some datasets being “simpler” than others, though I was not clear as to what “simpler” meant here.<|endoftext|>The paper gives a method to iteratively and interactively improve the label model in weak supervision. Novelty and improvement mainly comes from the second step, where true label for most uncertain data point is queried using which the parameters of the labelling functions are improved, which in turn lead to a more accurate weak supervision model. Empirical results on various real world datasets in medical domain show that in some cases this approach can yield a more accurate model in comparison to pure active learning approach and some recent baselines which combine weak supervision with active learning. These results also show that the paper s approach can get accuracy comparable to fully supervised model as well. The proposed approach incrementally improves the labeling functions by using the true labels obtained in each round. To achieve this they need LFs to be differentiable, which can be a drawback in some cases. From the results it looks like the cases where one can obtain good labeling functions from experts then this approach works better than active learning otherwise not. Overall, I think its a nice paper with a sound and simple approach to improve weak supervision s label quality using active learning. The contributions are novel and useful in practice. I need some clarifications in the experiments section to be more confident in this assessment.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; In one approach, they propose to replace the Jacobian in the backward update with the quasi Newton matrix, which is being already used/estimated in the forward pass solved by quasi Newton method. They provide theoretical analysis of their proposed method and show that under certain conditions/assumptions, the forward pass still converges to the desired solution and the sequences of backward estimates converges to the loss gradient of needed to parameter updates. They provide numerical results in bi level optimization (regularized logistic regression) and training DEQ for classification. They provide detailed numerical results and experiments. Please explain why the quality of the inversion and performance are not correlated. The paper s main motivation is to computationally improve the backward pass.<|endoftext|>The paper proposes a way to improve on the computational cost of bi level optimization problems. Figure 3: it seems that the Jacobian free method and SHINE are almost equally performant in terms of top 1 accuracy while SHINE takes longer due to additional updates? The theoretical part of the paper is written very well for someone unfamiliar with the literature. Again in Figure 2 left: The variance in the convergence curves of SHINE methods requires further comment by the authors.<|endoftext|>The authors establish various convergence results showing that the approximate hypergradients converge to the true hypergradients, under different sets of assumptions. Experimentally, the proposed method is comparable to or outperforms the Jacobian Free method by Fung et al.(2021) in hyperparameter optimization for logistic regression and DEQs. This paper proposes to use quasi Newton matrices from the forward pass to approximate this inverse Jacobian matrix in the direction needed for the gradient computation which appears in the computation of hypergradients.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; rating score: 5; This work first implements a byte level (character) tokenizer, which is learned as part of the model rather than as a preprocessing task. To limit the computational burden of character level encoding on the downstream architecture, the method creates byte n grams and combines them via a scoring network as outlined by the authors. Common Transformer architecture can use the tokenizer and even be narrowed. Finally the model is evaluated using multiple NLU and noisy NLP tasks. + The tokenization method is based on a weighting of pooled byte n gram embeddings. Score calibration and downsampling are technically sensible and the authors make a noticeable effort to ease optimization and not introduce foreseeably brittle complexities like extra hyperparameters. + Instances of reasoning about technical choices, e.g."narrower encoders", provide instructive details and help make the foci of the experiments more deducible. This work first implements an end to end trainable byte level (character) tokenizer, while limiting computational overhead via a learned token weighting. It is compatible with common Transformer stacks, but can reduce their width for computational gains, without sacrificing performance, and in some places gaining performance. Therefore, I recommend accepting this work.<|endoftext|>A gradient based subword tokenization module (GBST) is designed for replacement of the general tokenization. The paper proposes a novel gradient based tokenization module to address the issue of rigid subword tokenization algorithms and attains competitive performance. 2.It is interesting to find that this module can deal with multilingual and noisy datasets to some extent, which gives credit to learning subword representations from characters. The performance is not surprising, though the model indeed saves model size. 2.The evaluations are based on base models. Besides, it would be impactful if this paper showed that GBST module could actually generalize in other kinds of models. I wonder if average pooling is good way to down sample though it is really fast, but how about MLP? In section 2.1.5, “latent subwords $\hat{X}   [\hat{X}_i, \dots, \hat{X}_M]$  should be “latent subwords $\hat{X}   [\hat{X}_i, \dots, \hat{X}_L]$This work is well motivated. The overall design of the model is technically sound, and it provides moderate improvements on several datasets, while being memory efficient, faster, with fewer parameters. It is not clear if the advance of the method can generalize to other languages or larger models.<|endoftext|>This paper focus on a gradient based subword tokenization (GBST) method for byte level transformers. The GBST module is simpler and faster. Comparing with the lighter ByT5+CANINE model, it has similar speed and memory usage, but more straight forward and has better accuracy. Though this idea to have deeper encoder is from previous work, it’s still informative to have the experiment. 3.The paper is very well written and has enough experiments to show the model’s performance in accuracy and capability in different tasks. This is important since in the ByT5 paper all the model variants have much heavier weight in encoders, and there is also an ablation study to prove that heavier encoder can improve performance a lot. Thus it’s a bit strange that the best settings of both models are not used for comparison. Con1.Lack of experiment on different sizes of the model, especially performance when with a larger scale.<|endoftext|>This paper aims to remove the reliance of NLP models to external tokenizers. It proposes a gradient based subword tokenization module that can be used in any neural model. The module scores predefined candidate blocks in a soft way for each position and then it weight averages them to obtain a mixture of subword representations. The resulting sequence is then downsampled in a fixed way to make processing easier. * First, it does not learn to segment the input to different chunks to be fed in the contextualizer but rather downsamples the sequence in a fixed way. The latent subword representations could be captured by combing multiple convolutions and a simple pooling function per position, which makes the method seem less novel. 4.Studies that focus on tokenization like the ones cited below evaluate on tasks where tokenization is important for reaching state of the art performance like machine translation. It might be worth evaluating there. Even though some of the results are competitive, the experiments do not provide enough evidence that the learned  tokenization is crucial for achieving the results.<|endoftext|>The authors propose a soft gradient based subword tokenization (GBST) module with the aim of improving tokenizer free end to end training of language models. It seems like T5, but I don t find it mentioned anywhere. The authors use the GBST module followed by an encoder decoder Transformer stack similar to that of T5 and call it Charformer. All the tasks in the paper seem to be classification tasks. (Unclear) How is the proposed GBST different from convolution, despite the scoring part? (Pro) Charformer seems to achieve similar predictive performance to Byte level T5 while being more computationally efficient (Table 6). How does it save computation? Figure 2.How to interpret this heatmap for block size > 1? (Mixed) Experimental results do not show strong improvements on accuracy metrics. Table 1 3 shows that Charformer is generally on par with Byte level T5. However, some important design and implementation details are missing or confusing which are not helped by the lack of source code. The extensive experimental results themselves would benefit the community.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The description of the method is not very clear. The results, while strong, are not convincing. between", and many more.<|endoftext|>I suspect that the authors may have limited grasp of recent work in the field. uses 1s EEG as a sample. The experimental settings and data processing of the proposed method and the baseline methods are different, which does not seem to be a fair comparison. 5.Most of the baseline methods used by the authors are not recent methods.<|endoftext|>by Zhao et al., which is another copy and paste step from previous publications. The experimental SEED database used in the project, but not available anymore online, seems to be related to affect recognition from brainwaves (here EEG) in response to emotional videos, but not the subject s emotion itself, which seems to be not reported in the original experiment. This is an interesting incremental improvement of the previously published approaches to the not available online SEED dataset.<|endoftext|>Minor Correction:Thoroughly read for structural and grammatical errors. WeaknessThe authors should also review literature where the non stationary nature of the EEG signal has been handled using appropriate feature selection.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper has a short analysis on rounding schemes, comparing nearest rounding to stochastic rounding for quantized training. However, it s unclear to me what the effect of this is on the gradients themselves. It would be good if the authors worked this out. Similarly, The argument that RDN should be used in the forward pass goes much too fast for me as well. This greatly limits the scope of this work in it s practicality. A discussion of the overhead/complexity of this method already here would be much appreciated. The suggested method is not a general method for quantized training and more of a  hardware and software go hand in hand  kind of method, but many of the practical details of the hardware implementation and costs/overhead are left out. The paper would likely have been better if it honed in on a single aspect that they present, and did so in a clearer fashion with more bases covered.<|endoftext|>This paper proposes techniques to quantize the gradients in the back propagation algorithm down to 4 bits. In Section 2, conclusion, the following claim is made:  "the forward phase should be quantized deterministically (using RDN) since stochastic rounding will not make help making the loss estimate unbiased (due to the non linearity of the loss and activation functions) while unnecessarily increasing the mean square error". Why is that the case? Can the authors comment on what happens in the final quantization region? Related to the above, this is more of a suggestion. I urge the authors to perform a spell check. The paper is interesting, tackles an important problem, and presents promising results.<|endoftext|>The proposed approach is somewhat novel, it combines stochastic rounding with logarithmic quantization and stochastic pruning. The main strength are that most parts are well motivated and the strong 4 bit training results. Strong points:* Theoretically well motivated choice of quantization scheme for forward and backward path. * Strong results for 4 bit training of various ImageNet models. Weak points:* It is unclear what the contributions of unbiased stochastic pruning and logarithmic unbiased rounding in LUC are. * The SMP method for reducing variance requires further clarification. If the latter, then this seems to likely imply that a higher bit width needs to be used in the matmul of the next layer. This seems significantly more complicated than adding uniform noise as it is the case for uniform quantization. I could not find any details on this in the paper.<|endoftext|>The paper proposes some techniques to train a deep neural network in 4 bit and provides a theoretical analysis. Although some part of the method still needs high precision, the Reviewer thinks that it could open a door to training the deep neural network with ultra low bits. ** Strength **1. 3.The authors not only provided the 4 bit training algorithm but also suggested dedicated hardware blocks. 3.There is a typo on page 5. This paper proposed several techniques for effective 4 bit training.
Reject; rating score: 3; rating score: 3; rating score: 5; In this paper, the authors mainly study the problem of missing data in treatment effect estimation and highlight the importance of addressing this problem. The authors propose a selective imputation scheme which is more well suited for addressing missingness in such scenarios. Authors also present several sample scenarios illustratively which indicate potential issues with current methods while doing TE with missingness. Empirical results compare different scenarios where general imputation schemes (imputing all data, no data, wrong data) can be much worse than their proposed method. **Strengths of paper**  Paper presents a very elaborate writeup with a detailed introduction to causality and missingness in treatment effect estimation which makes it a good read for audience who are new to this field. Problem addressed is a relevant problem and is applicable across several real world problems. **Weakness**  The paper has a lot of content on introducing causality which instead could have been used to add more use cases on how selective imputation can be useful for TE  It does appear that the authors could have studied the exhaustiveness claim of MCM in depth to tease out the contributions of this paper better. This is a very broad claim with insufficient justification in the current draft. It is not exactly clear how this paper advances the technical state of the art to clear the ICLR bar of acceptance. The general writeup and contributions make it a better fit for a venue like UAI and other similar avenues. Overall, I would like the authors to think more broadly on the imputation problem itself as industry problems rarely rely on using any kind of imputation method (mainly due to skepticism and avoiding corrupting the data). It might help if authors can talk about broader adoption for this work for such a real world setting. I have highlighted my major observations from this paper above. While the paper is written in a very elaborate manner, I do believe it is still not clear if the technical contributions are advancing the state of the art to clear the ICLR bar of acceptance. Authors should tease out the technical contributions around exhaustiveness and others more clearly and remove big sections on assumptions, metrics (can be written briefly with citations).<|endoftext|>This paper studies dealing with missing values in estimating treatment effects. Authors identify a new missingness mechanism, mixed confounded missingness (MCM), including missingness that determines treatment selection and missingness that is determined by treatment selection. The authors show that both imputation and no imputation lead to poor treatment effect estimations. The authors present a selective imputation strategy that informs which variables should be imputed and which should not. Authors use graphic causal models to model the various missingness in causal effect estimation. This is an interesting exercise. In Figure 1(d) the relationships between the rightmost four variables are "W \to Z_2 \to X _2 \leftarrow X" (Note that I use Z_2 and X _2 to replace the superscripted Z and X in Figure 1(d)). We should understand the relationships in the following way. This does not mean that W will determine X_2 when it does not include missing values. Using the authors  example, the participants of the job training program provide additional information whereas the non participants do not. So, the missing values in X _2 are determined by W. However, if we have a magical way to collect the additional information from non participants (i.e.the missing values are imputed perfectly). If X_2 is imputed correctly, there is not an edge W \to X_2 and the discission before "selective imputation" on Page 7 is invalid. There is a soundness problem with the paper. If X_2 is a part of the covariate set, there is no edge W \to X_2 without missing values. If X_2 is imputed correctly, the causal effect is also unbiased. Interestingly, there is not a discussion of how the missing values are imputed.<|endoftext|>"we identify a new missingness mechanism, which we term mixed confounded missingness (MCM), where some missingness determines treatment selection and other missingness is determined by treatment selection." MCM is a type of MNAR (Missing Not At Random). MCM cannot be listed parallel to MCAR, MAR and MNAR. The questions they are trying to solve is an important problem. For example in the discussion the author emphasize that "more care and thought shouldbe put into imputing missing data when estimating treatment effects." which I agree with completely. The authors are using simplified DAG graphic tool to modeling the exposure, mediator, outcome variables with missingness and propose the causal relationship of missingness. In the real world data, one covariate may cause missingness of an exposure, which could be the predictor of the outcome variable. However, the missingness of this covariate may have no direct correlation with outcome variable. Therefore, to determine which variable will be imputed or not will not be simply determined by strategy will be limited. Missingness in one variable may have direct or indirect relationship with missingness in the other variable or outcome variable. Which variables will be imputed or not imputed for the prediction of outcome variable will be  determined using machine learning approach for the feature selection. Finally, even though it is difficult to use real word data for experimentation, it is important to attempt to do that. The imputation procedure has to be designed for a specific field as pattern of missingness may be very different among disciplines. The pattern of missingness and mechanism for a given dataset should be recognized. Whether the competition is favoring a certain method or procedure has to be determined in the “real world” data with “real world” missingness by considering recognized and unrecognized missing pattern/mechanism, as well as the plausible distribution of missing data. A recent paper in the field was just published this month: https://www.nature.com/articles/s41746 021 00518 0 Therefore, even though the idea of improving imputation is excellent, the logic used in this study is not robust. Even though the idea of improving imputation is excellent, the logic used in this study is not robust and does not take into account true realistic situations. How the synthetic data was created has a direct effect on how the methods will work. It is possible to use real world data and introduce additional hold out values to evaluate the methods.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper studies the online inference and learning problems for nonsymmetric determinantal point processes (NDPPs). The authors use the online greedy algorithm for MAP inference and modify the learning objective for being suitable in the online setting. Experiments with real world datasets show that the proposed online algorithms are comparable or even better than state of the art offline algorithms. Moreover, some algorithms are already proposed in prior works, but there is no reference. For example, Algorithm 2 (Online LSS) was proposed in [1]. It would be good to place Algorithm 4 in the main manuscript. Hence, the paper should be improved for acceptance.<|endoftext|>This paper introduces MAP inference and learning algorithms for nonsymmetric determinantal point processes (NDPPs) in the streaming and online settings. This idea was introduced in [1] for the MAP inference of (symmetric) DPPs, but the explicit connection between these two works is not made in this paper. The derivation of the gradient updates for the online algorithm can be removed from the main paper. Startlingly, the authors show that their online algorithms are competitive with (and often outperform) their offline equivalents. My main concern with this paper is novelty: there is significant overlap between the MAP inference section of this work and previous work by Bhaskara et al.(2020), both in terms of the key ideas (using a stash) and in how the algorithms are analyzed.<|endoftext|>This paper proposes online and streaming algorithms for MAP inference and learning for nonsymmetric determinantal point processes (NDPPs). The authors provide some theoretical guarantees for the proposed algorithms, and perform experiments that demonstrate that their performance is comparable to (or better than) offline algorithms for these tasks. Thus, claims 1   3 seem to be incorrect. This paper has strong contributions in the area of streaming and online MAP inference algorithms. However, there are some notable issues with the contributions regarding the online learning algorithm, including the comparison with prior work, and the correctness of the approximate optimization objective (Eq.4), as described above.<|endoftext|>This paper introduces the streaming and online MAP inference and learning problems for NDPPs. For streaming MAP inference, an algorithm is proposed with total time linear in $n$ and memory that is constant in $n$;  For online MAP inference, several algorithms are proposed such that at any point in time a valid solution is maintained;  For online learning algorithm, a single pass algorithm is proposed with memory that is constant in $m$;  Experiments are conducted to show that these streaming and online algorithms achieves comparable performance to state of the art offline algorithms. The problems that this paper studies are very interesting. It would be great to include some intuition in the main paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper studies using bonus for guiding exploration in reinforcement learning with large action spaces. LinUCB, developed originally for stochastic linear bandits, has been a theoretically successful algorithm for problems admitting a linear reward structure. However, using such bonuses entails computing a matrix inversion, which may not be tractable in high dimensional problems. The variant for deep RL, however, is examined through numerical experiments where the algorithm is shown to be competitive with state of the art. Please comment on whether this is correct and whether this is improvable. This paper presents a novel exploration bonus, called ACB, that can be used to design viable exploration strategies for linear bandits and deep reinforcement learning.<|endoftext|>This paper proposes a new intrinsic reward method for continuous action space deep RL algorithms. The proposed algorithm is inspired by the bonus term of LinUCB. The proposed algorithm improves the efficiency of its reward bound’s computation by using a randomized variant of the original reward bonus. The authors theoretically justified the optimality of this ACB bonus term in the linear case, showing that this is a reasonable replacement of the LinUCB reward bonus. Although this algorithm comes with no theoretical guarantee, experiments show that the ACB intrinsic reward is competitive compared to adopted baselines. Although the proposed ACB is not completely novel (as also mentioned in the related work section), the paper is well motivated and the resultant algorithm is effective. Therefore, I vote for acceptance. Therefore, although the current experiments are sufficient, it would be nice for the authors to show how different ways of setting $y$ influence the performance.<|endoftext|>This paper proposes a novel exploration method called anti concentrated confidence bounds (ACB) that provably approximates the elliptical exploration bonus of LinUCB by using an ensemble of least squares regressors. While doing this, ACB bypasses costly covariance matrix inversion, which can be problematic, especially for high dimensional problems. It is shown that ACB enjoys near optimal performance in linear stochastic bandits when the cardinality of the action set is polynomial in the action feature dimension. However, the main contribution of this work comes from extending ACB principle for computing exploration bonuses in deep RL. Comparison of ACB with state of the art deep RL exploration methods on Atari benchmarks demonstrates the competitiveness of the proposed approach.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposed a simple linear programming model for learning logical rules for KG completion. The model selects candidate rules from KG with explicit constraints and then solves a linear programming problem. The authors conduct experiments on several public datasets and the model has better efficiency than baseline models. * The proposed model is very simple and seems to be effective. I am not sure the proposed method is also faster than the simplified version of Neurallp and DRUM. There are several methods that the authors did not use for comparison.<|endoftext|>The paper proposes a simple method to generate logic rules, where rules are generated by a shortest path heuristic and rule weights by solving a linear program. First, the paper does have it s strong points:S1. Simple, easy to understand approach. However, in the light of the current state of the art, the paper falls short severely (W1). There has been prior research in how rules are combined (e.g., [A] uses the most confident rule); it s not clear whether a linear combination of rules is a good idea in the first place. Here an ablation and comparison to prior approaches is missing. These candidates are useful in that they are generated sequentially based on the candidates so far, but this idea is also present in prior work (e.g, RNNLogic, [A]).<|endoftext|>Less research has focused on the problem of creating scoring functions based on an (implicit) list of rules. The core idea is to formulate a linear program whose solution corresponds to a scoring method. The authors cite exhaustively from the literature on knowledge graph embedding methods and rule based approaches. I believe this is work that should be cited as related. https://arxiv.org/abs/1206.3282https://arxiv.org/abs/1304.4379Good paper but some discussion of highly related work is missing<|endoftext|>To solve the problem that rule based methods in knowledge graph completion tasks are lack of scalability to large datasets, this manuscript presents a simple linear programming model to choose rules from a list of candidate rules and assign weights accordingly. Weaknesses:There is no in depth comparison of the algorithm with previously known work. However, it is still hard to judge whether the algorithm is an improvement on previous work. This is a well written paper having some interesting results. * The idea of using column generation techniques to add new rules is interesting. Related work has not been investigated sufficiently before just presenting the author’s model.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper proves a high probability convergence rate for the first order algorithm, achieving the optimality of the probability and the convergence rate under this type of algorithm. The writing and presentation clearly shows the main steps of reasoning. I can barely find weaknesses of the paper. Does this paper apply to that case, and will there be an acceleration if the Hessian is truly diagonal? 2.I m not quite familiar with the high probability reasoning, so it could be helpful to add in appendix a comparison with the papers in "High probability results" paragraph, say, what algorithms did they use, what are their result. Does a standalone argument about probability benefit? I think the contribution is meaningful and the writing is clear. If other reviewers are not doubtful about the novelty, then I ll give an accept.<|endoftext|>This paper provides an analysis of adaptive learning rate scheme in stochastic non convex settings. In order to accomplish this, the analysis must deal with the standard difficulty for adaptive learning rates, which is that the learning rate depends on the current iterate and so makes it harder to apply standard martingale inequalities. The final results include a dependence on the smoothness constant $L$ and variance $\sigma$ which are not explicitly used in the algorithm. I have two main concerns about the results currently. Notably, SGD also doesn t require any knowledge of the smoothness or Lipschitz parameters. For the in expectation bounds I can actually produce an iterate that has a small gradient norm in expectation by randomly picking an iterate. Here if I randomly select an iterate I think the high probability guarantee will be lost, so unless there is some other construction or better argument, it cannot actually provide any high probability guarantees about any iterate. The paper considers and interesting and difficult problem.<|endoftext|>This paper proposed a new analysis for AdaGrad method in smooth and non convex optimization, to get high probability convergence toward stationary points. Based on some assumptions (Eqs.(2), (6) and (7)), i.e., Lipschitz, bounded variance of gradient estimates, and bounded stochastic gradient, the authors analyzed Algorithm 1 (AdaGrad), which does not use any information of the quantities in the assumptions. weakness:  I am concerned with the results do not tell us much information about why the AdaGrad and its variants enjoy good performance in practice. In particular,  The main results in Theorem 4.2 claims that after $T$ iterations, the averaged cumulated gradient norm is small with high probability. Does this mean the last iteration is not guaranteed to have small gradient norm (I can imagine somehow the algorithm could have oscillation behaviour, but not sure if this is the case here)? If this can be turned into a more direct result of last iteration convergence result with high probability, that would be better, since as I am aware of, there are expectation convergence results for last iteration convergence. Overall, I think this work provides a neat and elegant analysis for AdaGrad and several variants. It shows that AdaGrad behaves similarly in terms of expectation convergence and with high probability convergence. However, I found the results still unsatisfactory in terms of I did not see how those results could explain the good performances achieved by AdaGrad and its variants.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The paper describes some lower bounds for differentially private empirical risk minimization (DP ERM). * An $\Omega(p/\varepsilon n)$ lower bound for pure differential privacy. Theorem 5.2 shows a similar lower bound for $\ell_2^2$ loss, and it’s trivial to see that the theorem holds for unconstrained ERM. Theorem 5.2 thus nearly matches one of the main results in this paper, and matches it exactly when the error $\alpha$ is required to be constant. Another weakness is that, even where the results were not published before, the techniques are very similar to prior work.<|endoftext|>This paper presents tight lower bounds on differentially private ERM, a well studied topic in the DP literature. It obtains tight bounds for both the constrained and unconstrained settings. But given the results are strong, I am not overtly concerned. **Strengths:**  The main strength of this paper is that it gives tight lower bounds for a topic that has been well studied in the literature.<|endoftext|>## Summary of ContributionsThis paper studies the unconstrained empirical risk minimization (ERM) under differential privacy (DP). Here we assume that $\ell$ is 1 Lipchitz; the results easily extends to $C$ Lipchitz functions with an extra multiplicative factor of $C$ in the excess empirical loss. The main contributions of the paper are:1. 2.In the pure DP setting, the authors show a lower bound of $\Omega\left(\frac{p}{\epsilon n}\right)$. Page 9 typo: "oversome"  > "overcome"## RecommendationOverall, although the techniques are somewhat similar to previous work, I still think that the problems considered in this paper are important enough that the contributions of the paper (i.e.closing the gaps in excess empirical loss) are above the bar for ICLR. The proof requires non trivial extensions of previous works (e.g.the mean biased property of fingerprinting codes) which may be useful elsewhere.<|endoftext|>This paper studies differentially private empirical risk minimisation (ERM) in the unconstrained setting. It also gives a lower bound for unconstrained pure DP ERM that recovers the result in the constrained case. Strengths: This paper is the first to give tight lower bounds for approximate DP ERM for general loss functions. New techniques were developed as a modification of the regular fingerprinting codes, which are of independent interest as well, especially for those problems that are not easily reducible from one way marginals. The work appears technically strong and non trivial.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The authors propose a Bayesian model to infer relations across heterogeneous views of data, which can be of structured and unstructured type. The computational complexity is reported for a single dataset, while it should be possible to give a sense of the time dependency of the proposed approach  w.r.t.the training set and test set size (does it scale linearly?). With regards to the case with multiple views, it would be of interest to study the sample complexity as a function of the number of available views, since, as the authors have stated in the introduction, leveraging the information from multiple sources should improve the performance on tasks with a limited number of training samples. The approach deals with problems that could not be previously easily tackled, however  a more thorough empirical investigation of how well the approach can deal with those cases is lacking.<|endoftext|>The authors propose a Bayesian framework to learn relations among multi omic datasets. Experiments on two biomedical multi omics data sets partially demonstrate the effectiveness of the proposed method. The paper may be interesting to a much broader community rather than just the biomedical use cases the experiments have shown, as the methodology is presented for any multi omic data in a very general way. Weaknesses:My main concern is with the experimental part. The experiments are not extensive and cannot fully demonstrate the advantages of the proposed approach. First, the datasets are restrictive. Second, many of the advantages the proposed method claims cannot be fully demonstrated by the two datasets in experiment, though the authors do manipulate the structure of datasets manually to show that the method still works when some key assumptions of BayRel do not hold. The proposed method is convincing with respect to an important problem.<|endoftext|>The authors propose a hierarchical Bayesian generative model for multiview learning aimed at application to integration of multiple modalities of omics data. The definition of the experiments and the data used in the Experiments section is not self sufficient. The evaluation is focused on the comparison to BayReL a related approach mentioned throughout the paper but not described in the related work. However, focusing on BayReL only, in the related work the authors miss a large body of work, old and new. The authors demonstrate the ability of MoReL to learn from unpaired data and in a setting with missing data.<|endoftext|>The paper proposed a deep Bayesian model for heterogeneous multi omics data integration. The Gromov Wasserstein (FGW) regularization between latent representations of heterogeneous views is used to align nodes/features in every pair of views. In order to align nodes/features in every pair of views, the Gromov Wasserstein (FGW) has been used. Can we apply BayReL and FGW into a pipeline to learn heterogeneous views? And how is it compared to the proposed models if possible? It is difficult to follow the generative process of the proposed models. In short, the paper is trying to solve an important challenge of heterogeneous multi omics data integration by proposing an advanced model compared with BayReL.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper proposes a simple method, "instruction tuning", to improve the zero shot learning capability of large language model, which 1) annotates prompts for a wide range of tasks and then 2) fine tunes the model to "answer/respond to" those prompt. The empirical results are impressive: after instruction tuning, the 0 shot performance is better than GPT 3 (0 shot, sometimes few shot) on a wide range of datasets; nevertheless, on datasets with formats already similar to language modeling, the performance gain is negligible or even negative. The only outlier seems to be ReCorD where the performance drops significantly after instruction tuning, and this probably deserves some discussion. For example, Sahn et al.(https://arxiv.org/abs/2110.08207) finds that even models with only 11B parameters can generalize to unseen tasks, using T5 (MLM) and a larger set of prompts. What does “LAnguage” mean here, "natural language instruction" or "language model"?<|endoftext|>The paper creates a dataset of over 60 NLP tasks described via instructions (using templates for each task) and finds this boosts zero shot performance on unseen tasks. This could be another area to investigate Misc:    UnifiedQA seems potentially worth citing as prior workOverall, the paper s idea is powerful (but of somewhat limited novelty) and the results are good (but not great). Easy but probably not great thing to try:  held out tasks with wrong/useless templatesA final thought:  It s not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset specific spurious correlations.<|endoftext|>Authors take a 137B parameter pretrained model and finetune it on multiple tasks verbalized via natural language instruction templates. As the result, the instruction tuned model performs well on un seen tasks with the zero shot setting. Pros:1.The problem addressed has high practical value: it tries to make large pre trained language model more accessible to a range of NLP tasks. The "instruction tuning" idea will significantly reduce the cost for task specific fine tuning, labeled data and prompt engineering compared to other approaches. 3.The analysis presented in the main paper and the appendix is thorough enough. Cons:There are still a few questions that can be addressed to make the analysis comprehensive. Will it drops any knowledge of any tasks, which will be a disadvantage when the task s labeled data is available? Overall, the paper proposed an interesting idea and showed strong empirical results, hence I vote for accepting.<|endoftext|>This paper describes an approach to fine tuning large language models which can improve zero shot accuracy on unseen tasks. Overall well written with compelling results, this paper describes a new language model (FLAN) and shows how it improves upon the zero shot task performance of previous language models such as GPT 3. The paper is missing important details about hardware usage and training time   Some possible issues which might be resolved by the additional questions belowAdditional Questions:   "For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset." Although qualitatively useful, the analysis in 4.1 does not conclusively show that the number of instruction tuning clusters aids performance, or that this trend is likely to continue with more clusters.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; It may provide some insight on how to model individual object dynamics from videos. Moreover, the proposed method is very effective as experimental results show superior performance. Strengths:  The paper presents a very interesting idea of using transformer structure to align latent features and distillate object dynamics, and it seems very effective.<|endoftext|>This paper aims at learning object dynamics from unlabelled videos for multi object representations. How does that make sense? The modules proposed in this work are well motivated and designed and produce strong results on video reasoning. This shows and enhances the need for a baseline as discussed above. ICLR 2020 Missing related work. This work does not deal with the inherent ambiguity of the video prediction task itself. That is why I urge the authors to implement the baseline as pointed out above and to report the resulting numbers in the rebuttal.<|endoftext|>This paper presents an unsupervised distillation of disentangled object dynamics from raw video inputs. The distilled dynamics model is capable of causal reasoning and future frame prediction. [Strengths]The authors provide a novel distillation method to understand object dynamics. The proposed system shows state of the art performance on CLEVRER.<|endoftext|>The model consists of two modules, one to distill individual object dynamics and a second relation module to understand interactions between objects. The model is shown to have SoTA results on several downstream tasks including video understanding and reasoning, video prediction, reconstruction, and segmentation. In addition where does “velocity magnitude” come from? This was not explained previously. It would be useful to explicitly mention the colors to look for in the images in the caption or in Section 4 in the text. ### Typos:  Section 4.3: “weather”  > “whether”The paper presents a method with two key insights/contributions that are not present in prior work.<|endoftext|>This paper proposes a method of extracting the latent space that describes objects in a video. The encoder part, Inference model, encodes an input image frame by frame from a video based on IODINE. But, no example is shown in the paper to confirm the effectiveness of Dynamic Distillation. In the experiment of prediction, it is difficult to recognize how is the motion of objects and interaction each other in Fig.4. And the indexes are omitted in many parts of the explanation. It seems no experiment is shown that object dynamics is distilled by the proposed method.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; The paper proposed a deep generative model named iterative refinement graph neural network to generate antibody CDR for Y shaped antibodies. Specifically, it sequentially generates the CDR residue sequence and refines the global structure iteratively. The proposed method is novel and the paper is well written, validated by thorough empirical studies.<|endoftext|>This paper proposes a joint generative model (co designing the sequence and the structure at the same time) for the CDRs of antibodies, with the goal to enhance binding specificity or neutralization capabilities. The joint modeling approach of structures and sequences is novel and interesting. The empirical results are convincing. For example, the method requires already knowing the frame of the antibody. When is a fixed frame variable CDR H3 design a reasonable assumption? Interesting approach to an important problem. The approach might be limited to specific use cases in practice depending on the availability of a predictive model for the properties of interest and the knowledge of the antibody frame.<|endoftext|>Weaknesses:  The use of a predictor to evaluate neutralization is justified based on very recent work, and it is unclear that this practice is in line with broader norms in the antibody engineering community. This paper provides an interesting study of antibody loop generation with both novel methodology and extensive empirical evaluation. Evaluations are thorough and span a number of datasets, one of the biggest strengths of this paper.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; The proposed mechanism is based on masking and selecting a random client to generate DP noise. There is a fundamental flaw in the security guarantee of the paper   the source client and the noise leader could collude, this would reveal the aggregation without noise (the noise can be subtracted out). 3.I could not understand the problem setting   the motivating examples are hospitals and banks. But would not every patient visit a particular hospital or a particular bank (based on locality etc), how is this different from horizontal data split? 4.I could understand why would the info that client A holds label a be made public (Sec 2.3)? There are several writing issues in the paper.<|endoftext|>The paper proposes a new approach called FEVERLESS to conduct vertical federated learning over distributed labels. Also, does the algorithm still work in such as case? The authors should better conduct experiments on a dataset with at least a million samples. 6.It is not clear to compare the exact performance of different approaches in Figure 2 and Figure 4. Overall, I think the paper can be significantly improved in writing. Moreover, I do not fully understand how the algorithm work.<|endoftext|>This paper studies vertical federated learning for fast and secure XGBoost training where labels are distributed among multiple parties. Cons:1.Several typos: for example:  “multiply hospitals” should be “multiple”. As such, the paper has a limited novelty from the ML perspective. The problem addressed in this paper is of practical importance for many real world applications. Thus, I would like to see its presentation at the conference.<|endoftext|>The paper introduces a novel setting for vertical federated learning in which labels are distributed among clients, and proposes a novel fast and secure protocol for this setting based on XGBoost . 4.Thorough security analysis proves that the proposed approach is robust to collusion of up to n 2 clients. 6.The proposed novel setting for vertical federated learning with decentralized labels is very well motivated as shown from the COVID19 example highlighted in the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper studies the learning dynamics of gradient flow for kernel ridgeregression. By studying the relation between empirical and oracle world inthe setup of kernel ridge regression, the authors investigate this phenomenon ina setup under precise theoretical control. The authors perform such a study by leveraging the analysis of Ghorbani etal. The authors confirm this picture also for invariant kernels, saytranslation invariant ones, using technical tools from Mei etal. This paper is also well written: the structure is clear, previous work isacknowledged, the schematic plots in Fig.1+2 are helpful. I was only unsureabout how to interpret Fig.3: is it a purely schematic plot, i.e.a drawing? I think this paper would be a valuable contribution to ICLR,and recommend acceptance.<|endoftext|>The main contribution is proving a three stage learning dynamics which were observed in neural networks in existing work. In sum, the strength and weakness of the paper are (Strength) Explaining/ solving certain phenomena that were previous observed in DL setting using "Kernels". (Weakness) Technical innovation is not very high and largely depends on the framework of [1]. The gradient flow dynamics in the current paper seems to be the limit of GD dynamics rather than SGD. [2] Learning with invariances in random features and kernel models Although this is not a breakthrough paper and the technical contributions is not very high, I believe this is another nice paper showing that a lot of DL phenomena are can be explained by Kernels.<|endoftext|>Studied the evolution of generalization error of the kernel gradient flow trajectory with respect to the training (empirical world) and population (ideal world) MSE loss. While it is not very surprising that the training time of kernel gradient flow (up to the sample size bottleneck) plays a similar role as the sample size in kernel regression, to my knowledge this is the first work that analyzes this correspondence in the high dimensional asymptotic limit. A few comments and questions. 4.It might be a good idea to elaborate on how results in this submission differ from the classical nonparametric rates for kernel regression, in which the generalization error rate is typically specified by the source and capacity condition. I believe the contributions in this paper are solid, and the take home message is of interest to the ICLR community. I will consider adjusting my score if the authors can address some of the aforementioned concerns. In my opinion this submission is above the acceptance bar (I think 7 would be more appropriate for my evaluation, but that is not an option...)<|endoftext|>Strengths: Nice empirical results studying several cases around the three phases in the MSE curves. In my opinion, the paper would benefit from more motivation for comparing the oracle risk together with the usual train and test errors. As it stands, the paper s attraction is rather limited. Either way, there seems to be a problem with the linear dynamics on the eq.at the bottom of the page. In theorem 1, I could not find where w_d(1) is introduced. The paper does study several cases including random features. Nice experiments are included supporting the main message of the paper.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; What s the difference between multiplying learning rate by $k$ and multiplying WD by $k$? But I find the interpretation on the effect of weight decay is not convincing, and the experiment results has marginal improvements. Since there is no re definition on "stationary point" in this paper, I think the stationary point in this paper still indicates the points whose (full) gradient w.r.t.loss function is zero. Then what is the loss function in this paper? Does it involve $l^2$ regularization part? Theorem 1 is trivial. The fact that GD with weight decay as Eq.(1) cannot converge to any stationary points has known long before. The experiment results are not convincing either. On one hand, this paper refer to the results of Li, et al.(2020), Van Laarhoven, 2017, etc,  so I assume the authors have taken batch normalization into account. This is totally different from quadratic cases. Again, theorem 2 is trivial and well known as a result of naive linear regression GD case. They are essentially full batch cases.<|endoftext|>The paper proposes to understand the effect of weight decay and design schedules for weight decay in Adam based on the theory. In summary, I would say that I enjoy the paper s clarity and structure but some main technical issues prevent it being published as its current status.Thus, I suggest settling the conflicts in theory and practice in an updated version. "Theoretical analysis of auto rate tuning by batch normalization." Overall, the quality of this paper is satisfactory in terms of clarity and empirical evaluation. The loss function is a scale variant function in Assumption 1 but ResNet and DenseNet are both scale invariant networks. This results in a severe conflict between theory and practice in this paper, so I would suggest a further revision for this manuscript. 1.$\eta_t$ $\eta_2$ in step wise learning rate decay, except for 2 or 3 steps, which should not be an issue for stability of stationary points. 2.The paper claim that *Fig.2 suggests given long enough training time, the optimal weight decay hyperparameter tends to be zero. * However, I think Fig.2 does not show the *optimal* learning rate tends to be zero when the training time is long. To support the claim, the figure should show the result of the same long training time using different weight decay. 3.Figure 3 shows that increasing weight decay is more effective than increasing learning rate when a large batch size is used. 4.The test error curves in this paper (Fig.4 and 5) show a severe overfitting effect when using AdamW, which is not consistent with my experience. Could the authors give some comments on this effect?<|endoftext|>The main contribution of the paper is to propose AdamS a new version of Adam with weight decay which has an implicit decay of the weight decay. In section 4, they propose AdamS  which is more "stable" version of Adam. In text, weight decay is not used as often, so it is not clear if there are applications there, it might be interesting to explore that further with a more realistic training set/architecture than TreeBank and LSTM. the connection between section 2 and the rest seems a little weak. Why does increasing weight decay improve convergence but not increasing the learning rate? It seems like AdamS is only beneficial to training and I think it might be helpful to add this modification whenever using Adam in combination with weight decay. The improvements are not superstrong but there does not seem to be any drawback.<|endoftext|>This paper focuses on Weight decay and its importance in Regularization during training. A more popular version is: θt   (1 − ηtλ)θ_(t−1) − η_t δL(θ_(t−1))/δθwhere η_t is  the learning rate at the step, and the weight decay is coupled with the learning rate scheduler. By citing current research, the paper explained some of the effects of weight decay, such as: Biasing stationary points and accelerating convergence of backpropagation. Novelty1.Previous work hadn’t caught on why the linear scaling of learning rate isn’t entirely helpful for higher batch training data because of low convergence. The current paper came up with a distinct approach that sorted this issue out. 4.SWD, the technique proposed by this paper, can be used with a slight modification to the current learning algorithms. It’s not an entirely new algorithm in itself, but it utilizes benefits from the currently popular methods and helps provide a better regularizationand thus better performance.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes a new knowledge distillation framework for directed graphical models based on the reparameterization trick. However, I have the following concerns/questions,1. The title claims for a unified KD framework for deep DGMs. However, the proposed distillation loss only applies to DGMs where the latent variable z has a reparameterization form. I believe the author should highlight this limitation of their framework. Though they are no longer equivalent to equation (2), my intuition is that, for the original objective (equation (3)), the conditional KL divergence is computed with respect to the teacher s distribution $p_{\phi}(y_{<j}|x)$, however, at inference time,  the conditional distribution of the student model is computed based on its own distribution $p_{\theta}(y_{ <j}|x)$. This makes the training objective does not consistent with the test objective. I don t understand why such a change will better penalize the dissimilarity of latent variables $z_i$? 4.Can you provide some qualitative comparisons between the samples from the distilled DGMs on the CelebA dataset? Overall, I like the solution proposed by this paper to address the issues in marginalized distillation and local distillation. I believe a more rigorous analysis and carefully designed experiments are needed to illustrate the argument (e.g., as I mentioned above). I will increase my rating if the authors can provide more convincing results.<|endoftext|>The authors compare their method with marginalization methods, which integrates the latent variables out, and the factorized (local) method, which distills knowledge between teacher and student on each factor. * The experiments are not well chosen and the results are not convincing. There are questions about the generalizability and scalability of the proposed method. * I am not convinced by the idea of the semi auxiliary graph that yields the loss function in Eq 3,4. For example, when we factor out the z s, the resulting graph on the observed variables is no longer DAG. * The advantage of the method is mostly for DGMs with continuous latent variables. For discrete latent variables, the local distillation is used. I m not sure how well this scales to models with the large number of discrete latent variables. * One limitation is that the teacher and students should have the same architecture. # Experiment:* The results in the experiment section are not convincing. * In figure 5: it is not clear why the authors claim that the proposed method produces better results. For example, why (c) is better than (e)? VAE is a very simple DGM, and there is no real structure in the graphical model. Several metrics and experiments presented in that paper can be adopted or adapted for the DGM knowledge distillation.<|endoftext|>This paper proposes to use the reparameterization trick to convert the latent variables in DGMs to deterministic variables in the context of KD. It then proposes a surrogate distillation loss and latent distillation loss and evaluates the performance of the proposed method in three applications. Strengths: The paper proposes a knowledge distillation framework for deep DGMs on various applications. This is a well known technique. The VAE, for example,  uses this technique during training. Although the paper argues that "we do not primarily use reparameterization trick for model training. Rather, we leverage it to convert the latent variables z in DGMs to deterministic variables so that we can effectively distill knowledge from a compact form of DGM", but isn t this very straightforward? I don t see any big difference between using the reparameterization trick during training and KD. The authors should provide a discussion on this. (B) I don t see a difference between equations (4) and (2) when applying to VAE because, during VAE training, the sampling over the auxiliary random variables \epsilon is implicitly included even though we just apply equation (2). (C) For experimental evaluation, could you compare your model with more state of the art KD baselines? (eg., Figures 4 and 5 and table 1). I am mainly concerned about the novelty and clarity of this paper.<|endoftext|>The authors propose an unified Knowledge Distillation technique for general deep directed graphical models. They use the reparameterization trick on the intermediate latent variables of the original DGM network and the student network. Then they use a surrogate distillation loss (combined with latent loss) to reduce the error accumulations over the chain of random variables. They discuss the similarity of their technique with others and demonstrate its performance for 3 applications. After the compact DGM reductions for both the teacher and student networks, each target variable has direct dependence on the input x and prior y_i’s. This is a neat approach. without loss of accuracy. Please give a detailed example. 4.I wonder how the performance is with only using L_{sd} in eq(7), \lambda 0 setting? Each layer is considered a latent variable `z’, I presume. In this case, what is the student network chosen (I might have missed it)? The work is good and the paper is well written. I feel the contribution of the work is not that novel. My confidence in evaluation will increase once the authors address the concerns raised above.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6;   The authors propose a study on generalization in Federated Learning (FL). The best learning rates chosen for the experiments in Table 1 are not clear. This is a direct consequence of the heterogeneity in FL: in heterogeneous settings, clients have access to different data distributions, leading the model to worse performance when new distributions are introduced. The authors propose a three way split for federated datasets in order to  perform an estimation of the risks and gaps: by identifying participating and non participating clients first, and then training and validate data within the participating clients’ local data distribution. I recommend acceptance. **Pros**      The paper is well written and easy to understand for most parts. To the best of my knowledge, existing works do not take into account the participation gap in their analysis or in their datasets split, therefore this work can be an important contribution to the FL community. Have you verified that?<|endoftext|>The paper proposes a framework for disentangling the performance gap in federated learning into out of sample gap and participation gap, which should serve as a better tool for explaining the model generalization performance. More analysis on the relationship between gaps and data heterogeneity is provided to support the validity of the proposed splitting framework. Decomposing the performance gap into out of sample gap and participation gap is not new. The very simple way is to split the whole dataset into training and validation sets, and the training data is distributed across workers, then training data on each worker is further split into local training and local validation data. Another question is if some clients never participate in training, how can we evaluate the participation gap since the loss should be calculated on these data? How can we deal with the performance gap due to data heterogeneity? After reading all the reviews and the rebuttal, some of my concerns have been addressed, I am willing to increase the grade. If the authors can propose some remedy to this participation gap, this will be an interesting piece of paper.<|endoftext|>This paper studies the decomposition of the generalization gap into the out of sample gap, i.e., the generalization ability of participating clients to unseen data drawn from their local distribution, and the participation gap, i.e., the difference between the generalization ability on unseen data drawn from the local distributions of participating clients to data drawn from all possible local distributions. The paper proposes a three way split of clients and local datasets to estimate these two quantities separately. The issue of the different performance of participating and non participating clients in federated learning   in particular for heterogeneous datasets   is interesting. This fact has been studied extensively and methods have been proposed to address the difficulties in training on such heterogeneous distributions. This can indeed provide valuable insights about the performance and nature of the overall federated learning system. My main concern with the paper is that this contribution is not presented clearly enough and that its main assumption and limitations are not discussed in detail. Thanks to the great discussion with the authors, I am now convinced that the decomposition of the generalization gap into out of sample and participation gap has merits in the setting considered.<|endoftext|>The authors propose a different view of performance in federated learning, by measuring separately the performance out of sample  and the performance on novel (but related distributions) from a “non participating” client. They propose one approach of structuring experiments that allows these kins of measurements using a three way split. Strengths The paper analyzes performance measurements in the popular federated learning paradigm. The experiments are thorough and use best practice The authors discuss releasing a code base for running their experimentsWeakness/Comments The experiments are done using FedAVGM. For example a natural non participating client might have a label unseen in other clients, yet the model is relevant, (Minor) Figure 2 makes it hard to notice the per column similarity in the digits (and thus the distribution shift).
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The authors propose 2 impactful methods to aid in the design of numeric precision for neural network training: a method to quickly determine which formats work for weights and activations using angular deviation of gradients between low precision and FP32, and a hysteresis method for dealing with low precision representations. While there are few nitpicks I have (these should be fixed prior to publishing), the ideas in the paper are well founded and useful. Under Effect of quantized activation, it says "...suggesting that the misalignment angle is a better metric to predict the ranking of various formats based on the training performance."<|endoftext|>2.The authors find that the performance degradation of 8 bit training is due to the fluctuation issue of quantized weights. To solve this, the authors propose a hysteresis quantization scheme to improve the performance of from scratch training. **Questions and points needed to be improved:**1. It would be better for the authors to add more discussions on the difference between the proposed method and AdaRound. 4.In Table 4, many notations are unclear.<|endoftext|>The authors are encouraged to address the weakness. To mitigate the fluctuation issue caused by network quantization, they propose a hysteresis quantization scheme to avoid frequent changes of quantized points. 2.The technique part of this paper is simple and easy to follow. Compared with the static FP143/FP134 mode used in the current draft, they use an on the fly adaptive format. 4.It seems that Figure 8 (in A.3) has almost the same tendency as Figure 2. Why introduce a more complicated metric to determine the optimal format? However, the effectiveness of the performance indicator needs to be further clarified.<|endoftext|>This paper proposes a method to find an optimal quantization format based on error angle estimation and hardware overhead. Then, the authors need to prove that FP134 can be applied to a wide range of models. Is Hysteresis generally helpful for other quantization formats and larger models as well? Overall, even though this reviewer finds some interesting ideas including Hysteresis and experimental results are good for ResNet 18, the followings need to be addressed. (b) Do the authors suggest FP134 as a format to be applied to a wide range of models (especially even with large model size?).
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper first provides a theoretical analysis, under some assumptions, of the effect of data augmentation on the singular values of the network Jacobian and then proposes a method to improve the sample complexity of data augmentation, that is to select a subset of the data on which to perform the transformations to speed up training. Geometrical transformations can hardly be approximated by additive perturbations. As argued above, this seems a strong claim given the limitations. This has to do with the motivation for the need for methods to select subsets of data on which to apply data augmentation. The authors mention that "multiple augmented examples are usually generated for a single data point to obtain better results, increasing the size of the training data by orders of magnitude", as though this was a weakness of data augmentation, while it is actually one of its main strengths. In that paper we see that training with light data augmentation (translations and horizontal flips) on the full training set provides large performance gains with a marginal increase of the training time.<|endoftext|>This paper shows that data augmentation can speed up learning by enlarging the smaller singular values of the Jacobian. Following this idea, this paper proposed a framework to iteratively extract small subsets of training data that captures the alignment of NTK with the residual when augmented. Strengths:1) The motivation of the paper is clearly justified. 2) The presentation of the theoretical analysis and the algorithm are clear and sound. 3) It would be interesting to show the performance when transferring the core set found on one architecture (e.g., ResNet20) to train on a different architecture (e.g., Wid ResNet). The presentation and the proposed method are sound. However, the lack of experimental results on large datasets such as ImageNet makes the results of the paper much less convincing.<|endoftext|>Using all augmentations for a dataset may slow down training. This alignment score is used to select the coreset using submodular optimization, which allows the model to be trained on a subset of augmented data (e.g., 0.1% to 30%) while preserving most augmentation benefits. The speedup of the method is reported to be up to 6.3x. Strengths:* The paper presents a convincing reason why the approach can work in theory. * I also though the Section 5 was rushed. Overall, I think this figure can be improved to give a clearer and more convincing story. * The theory is appreciated but maybe it s worth contextualizing what values of the constants L and epsilon_0 are common in practice. I think there is enough theoretical justification for why the method could work, though I think empirical evidence is necessary to still show that. In any case, I found the writing good enough to follow.<|endoftext|>The authors model data augmentation as an additive perturbation and analyze its effect on training dynamics and how it enlarges the smaller singular values of the network jacobian. Then they propose a new method to iteratively extract a subset of the training data that when augmented closely capture the full augmented data dynamics. Authors show that by augmenting this subset combined with full training data they can outperform the state of the art method by 7.7% on CIFAR 10 and 4.7% on SVHN while achieving 6.3x and 2.2x speedup respectively. In my opinion, one of the main strengths of the method is when we have a very large dataset but unfortunately, the authors didn’t provide any experiments regarding large datasets like ImageNet. I think the paper is of interest, especially the theoretical analysis. But I believe the main application of the proposed method is in large scale dataset settings, while in small or medium size datasets augmenting the full dataset is not an issue, but that wasn’t presented (see my concerns in the previous section).
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; What else can be the root cause of the deviation of u from the data manifold? I agree with the authors that this approach require few gradient evaluations, however, the paper lacks rigorous justification on (i) why the method is effective, (ii) whether it minimizes some neighborhood criteria, (iii) how to tune its parameters. 3.It is not clear how only 2 gradient ascent steps (generate u and then x ) would suffice. The experiment results report for a specific value of epsilons, without a clear intuition and/or guidance on choosing these two parameters.<|endoftext|>So, I encourage the authors in pursuing this avenue of research. However, for the moment, the paper has clarity issues, related to both the technical part and the experimental part. This approach is empirically compared with baseline techniques on several benchmarks. As we know, counterfactual explanations are not just perturbations of the input data instance that change the output class. The literature on XAI has identified several properties which should be satisfied by counterfactual explanations. On the other hand, we have a model based approach to counterfactual explanations (this study), which focuses on SPN models.<|endoftext|>The paper present a new method for counterfactual explanations that relies on a two step perturbation of the original data instance. While the method has some attractive properties, the paper needs to compare against recent counterfactual explanation methods and the evaluation section needs to be more compelling. The first perturbation nudges the original data instance towards the counterfactual class. I agree with the authors that the optimization problem for deep learning counterfactual methods is tricky and they can produce unrealistic counterfactuals. The paper presents a novel counterfactual explanation method based on sum product networks.<|endoftext|>This is a XAI paper presenting a method to find counterfactual examples (i.e., contrastive example leading to a different prediction from the one currently obtained for a given example). The approach is based on tractable generative probabilistic circuits. I have found the description of the method in sect. As execution times are the most notably advantages, more details should be provided about that part in the experimental section and the different results of the other methods on the different datasets should be commented. Results are in line with the SOTA and considerably faster in terms of execution times.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; There are several approaches in knowledge distillation for neural networks. This paper studies kernel based knowledge distillation for multi view data set. The authors revealed that the assembled feature map obtained from the ensemble teacher network effectively improves students  generalization ability. Some numerical results are presented to show the practical effectiveness of the learning method based on theoretical findings. This is an interesting paper. In practice, however, max pooling is also widely employed. So my recommendation is to accept this paper.<|endoftext|>In this paper the authors consider neural networks as data dependent kernel machines and propose applying a distillation method directly on the pairwise kernel matrix of the models. As a minor point, I feel that some claims are quite bold, e.g., "This enables us to *prove* that KD using only pairwise feature kernel comparisons can improve NN performance in such settings". However, I have several concerns regarding this paper:1) Authors claim that "We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD)." In other words, this work does not only provide a theoretical discussion on feature kernel distillation, but also it is claimed that a novel feature distillation method is proposed. 2018.Lan, Xu, Xiatian Zhu, and Shaogang Gong. "Knowledge distillation by on the fly native ensemble. What kind of kernel should be employed in this case? Overall, this is a well written paper, easy to follow and the proposed method is   mostly   sound and backed up with some theoretical evidence.<|endoftext|>In this paper the authors consider extending the knowledge distillation (KD) framework to cases where the student and the teacher do not share prediction spaces. Instead of regularising the distance between temperature scaled logits corresponding to each data point of the student and the teacher in vanilla Knowledge Distillation, the authors instead regularise based on the distance between feature kernels of a pair of data points (inner product of weights of the pre final layer of the NN). The experimental results shows that the method is competitive in various settings. The theory is interesting though it seems to be a fairly straightforward extension of Allen Zhu and Li. The experiments show that FKD is quite competitive even without ensembles. The paper examines an interesting problem.<|endoftext|>This paper proposed a new knowledge distillation framework, feature kernel distillation (FKD), by matching the student network s feature kernel and that of the teacher network s feature kernel. The entire paper is well organized and the authors provided detailed and intuitive explanations when they introduce the theorems and derivations, which makes it easier for me to follow the paper. Cons:  The technical novelty is very limited. I roughly went through the appendix, and I found most of the derivations are based on the results in  Allen Zhu & Li (2020). So, what s the advantage of FKD over KD in theory? However, the theoretical novelty is limited, and also the empirical evaluations are not thorough (only on some very small datasets).
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; Strengths:+ Welcome empirical study of fine tuning strategies specifically for source code applications. Indeed, much of the motivation is very generic to the motivation for, and problems with, fine tuning in general. This is based on an analysis that shows that higher layers are updated (rather) slightly more in terms of parameter change when fine tuning the full model. The use of the word "Custom" throughout the paper is rather odd. For one, it is often used interchangably with "customized", but those evoke rather different ideas   "custom" suggests a specially designed model for a new task, which does not resonate the contributions here. The variant called "Custom fine tuning" (2.2) would be better called something like "Full model fine tuning".<|endoftext|>  This work studies customizing models for code towards specific projects/coding standards/preferences for unit test case generation task. It specifically studies this in the context of server side customization, where one entity would need to maintain multiple customized models, as opposed to client side customization where each client can store its own customized model. Overall, I found the problem well motivated but the empirical study and conclusions a little underwhelming. The domain of personalized unit test case generation is novel, and could be useful for the broader code generation research community. **Weaknesses:**  This paper is mainly an empirical study of the effectiveness of various model fine tuning strategies on the performance of the fine tuned model. My concern here is that BLEU score is designed primarily for natural language evaluation and has been shown to not be a good measure for code evaluation [1].<|endoftext|>It is unclear whether these observations hold in general for code generation as the evaluation is performed only on a single task. The paper evaluates four fine tuning strategies that select different subsets of weights to be fine tuned. These strategies are evaluated on the task of generating Java unit tests on 20 different projects, where the personalized models outperform the baseline model on BLUE4, perplexity and task specific measures. The project wise fine tuning strategy is not practical.<|endoftext|>The paper analyses three types of fine tuning approaches: (a) custom, (b) lightweight and (c) prefix on AthenaTest model for the task of unit test case generation in Java. The metrics used in the paper are pretty standard as well and there is no novelty at that end. Lacking thorough evaluation: If empirical analysis is the only contribution, then I expect authors to be more thorough and extensive in terms of the following:  i) considering more tasks like bug repair or code autocompletionii) considering more datasets and programming languages per taskiii) considering different models of source code especially the large language models like Codex  Other questions/ concerns:i) Is tf idf similarity the best way to measure coding styles? I am not sure how exactly will this work. I d give more space to the analysis rather than specific implementation details.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 3; Summary:  Over course of training, deep RL agents experience "capacity loss", where networks are unable to quickly fit new functions. Ablate on this! This problem has also been shown in supervised learning, where neural networks become less adaptable over time, and I believe in several other works in RL, such as in Kumar s prior work. The paper, especially in introduction, present it like it is a new problem. Overall:This is a well written paper which gives strong evidence that capacity loss/representational collapse is indeed a root problem for deep RL. As long as the authors address the weaknesses, I am willing to bump this up from weak accept to strong accept.<|endoftext|>This paper proposes a new regularization method for deep reinforcement learning, that is designed to prevent representation collapse and thus retain the ability of the network to fit functions that evolve in time, such as the sequence of Q functions generated by approximate dynamic programming methods. This is a very well written and very insightful work. The contributions are significant, their evaluation is convincing, both in the method and in the results, and the discussion has an appreciable depth. I identify two weaknesses. Is it caused by early or deep layers in the network? By the way, what is the network architecture used? Could the authors elaborate a little on this (both here and in the paper)? Overall this is a solid contribution on the question of preventing capacity loss in deep RL. I would appreciate seeing more details on how these $g_i$ projections are designed in the first place. Is there a rationale in their construction?<|endoftext|>Summary of paper:  The authors identify a problem of "capacity loss" during RL training with neural networks, this is the problem of agents gradually losing the ability to fit new functions while trainig. Other concerns and comments:  One thing seems a bit lacking to me is comparison to other methods that improve representation learning. Though I currently do have some concerns. I will consider increase my score if my concerns are fully addressed. However the authors should go through the paper carefully and try to make things more clear in the paper, would be nice to incorporate some of these reviewer discussions into the paper or appendix.<|endoftext|>To solve this issue the authors propose adding a regularization term that forces the trained networks to regress to a random function of their initial output. The paper tackles a super important question which is the stability of training in RL. The paper has no theoretical result, and even its capacity definition is thrown away quickly for a more practical measure. Since the point in this Figure is to show a connection to sparsity, why not include a plot of the sparsity as well (like num steps where R!0 divided by num steps)  Figure 5 (b): The results for Robotank doesn t really follow the theme of the paper   both methods reach the same score even though one loses capacity. For Montezuma there is no capacity loss. 5.The proposed method of Infer looks a bit naive to me   why preserve random functionals and not for example an autoencoder of the state or some other meaningful feature. This method forces the network to keep random functions of the state instead of learning the underlying structure. While the paper tackles an important problem, the proposed solution and it s empirical exhibition are insufficient in my view for acceptance.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; The topic is important and challenging. The results are novel, and the experimental section provides a nice illustration how the joint Shapley values can be used.<|endoftext|>This paper introduces joint Shapley values, which extend Shapley values to measure contributions of *sets*. Here are some comments:1. In practice, how can we choose k? 2.In the simulation, what is the explicit form of the set function?<|endoftext|>The theoretical results look clean, intuitive, and well presented. This work presents rigorous mathematical results for the joint Shapley values approach. At the very least, a justification for why such a comparison is impossible would be helpful.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors propose to learn energy based models with a flow based model as "backbone". Additional to this, can alpha be jointly learned with theta? The main contribution is a novel combination of existing methodologies.<|endoftext|>The motivation here is that EBMs are typically trained via contrastive divergence which requires MCMC sampling in a high dimensional space and from  a mutli modal distribution. To address this problem, the authors propose to train an energy based model with a (pre trained) backbone flow; the EBM can be interpreted as a correction step to the flow model.<|endoftext|>The paper uses the technique for both training and sampling. Such a model can be learned using both MLE or NCE. Thereby, it is important to verify that it is the training with neutral transport MCMC that solves the non mixing problem instead of sampling with neutral transport MCMC. A key technique used in this paper is the neutral transport MCMC.<|endoftext|>The authors raise good questions, which are exmained by empirical evaluations. This paper studies the learning of a special class of EBMs, which is a correction or an exponential tilting of a flow based model. First, note that the EBM model in [c] is parameterized by CNNs and its MCMC sampling is mixing.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; The authors propose a method of constructing a proxy reward function (and the loss function for learning that reward function), which generalizes two prior methods, allowing it to trade off the advantages and disadvantages of those methods. They create an RL algorithm based on this, analyze it theoretically and relative to prior work, and show that it performs well empirically. The notation around (6) is a little confusing, \mathcal I is not defined formally (it is a subset of \mathcal Z_T, but that’s not defined) or intuitively in English, except to say that it is sampled from rho_T. rho_T “denotes an unbiased sampling distribution”, but of what? What is the contribution that makes it algorithm authors’ (and not prior work)? I am concerned there could be bias here: 10 runs is not a large number for RL, where variance between runs is often enormous, and if some plots or curves were given additional runs after the authors viewed the results for 10 runs, that could add significant bias to the results in favor of their methods. **Update after final comments below:**I am updating my score to an 8; see below.<|endoftext|>The authors propose an algorithm, randomized return decomposition (RRD), that learns a proxy reward function to provide immediate reward feedback to the RL agent. Theoretical analysis shows that RRD is an interpolation between two existing methods, namely deterministic return decomposition and uniform credit assignment. Empirical results in the Mujoco continuous control benchmark show that RRD performs better than algorithms that are based on deterministic return decomposition and uniform credit assignment, and two other methods that learn auxiliary rewards for facilitating policy learning. **Quality**:Overall the paper quality is good. In many of these continuous control tasks, the main factor of the reward is the forward progress of the creature. 2.Related to the previous point, the return in these continuous control tasks is mostly the sum of the forward progress at every step. It will be interesting to see more empirical results in domains that are more challenging from a return decomposition perspective. This paper addresses the important delayed reward problem in RL.<|endoftext|>This paper targets how to decompose delayed reward signals obtained at the end of a trajectory to a more immediate form of reward feedback. The authors introduce an algorithm, "Reward Distribution vis Randomized Return Decomposition" that serves as a proxy reward function for episodic reinforcement learning. The paper would benefit from more text here. The graphs / figures might be more readable if some smoothing is introduced (and the smoothing factor is mentioned in the text). It appears to introduce a novel algorithm for an important problem and should serve as an important alternative to training agents on long horizon delayed reward problems.<|endoftext|>The paper addresses the problem of credit assignment in delayed reward setting. The authors predict the return of a trajectory by using only random sub sequences in the trajectory. This has to be corrected in the paper. While in the paper it is mentioned that the resulting reward is a “Markovian proxy reward function”. Please provide clarification on why this is the case? Equation (4) 	The section where equation (4) is introduced seems to imply that all methods use such a loss function. Missing related work:[1] also does reward redistribution for credit assignment for the complex task of Minecraft. But the work is not too novel and the writing is not satisfactory.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The proposed solution inspired from information theoretic methods namely maximum rate reduction. Manifold clustering is a very hard problem. The aim of the paper is to learn a representation space in which the manifolds are co linear. Although authors proposed an interesting solution to a very hard problem, I believe proposed method and experimental work needs to be improved especially taking in to account missing references and supplying a detailed discussion. One of the proposed constrained is orthogonality (Section 3.1). Similar objective can be achieved by consensus loss of [2]. In my understanding, objective wants to condense each manifold as much as possible (summation term) and all manifolds should cover the largest available space (first term). This can be desired property however how realistic it is? Would you please explain how will eq 3 type loss will handle this issue? I am assuming that the authors are calculating this quantity for each batch. Some of the self supervised learning methods are trained by using 8192 batch size. Does one need to do this computation on one gpu? Experimental results: 	Would you please compare your CIFAR 10, CIFAR 20 and STL 10 results with [1] and [2]?<|endoftext|>This work proposed a general manifold clustering algorithm called Neural Manifold Clustering and Embedding (NMCE), which utilize Maximum Coding Rate Reduction (MCR2 ) as the objective function and data augmentation to enforce constrains. In the implementation stage, given that even the toy experiment is difficult to optimize with the full NMCE objective, a multistage training procedure is applied with the first stage actually trying to optimize the Total Coding Rate (TCR), which is another kind of self supervised learning objective claimed in this paper. On synthetic and real world datasets COIL20, COIL100, CIFAR 10, CIFAR 20 and STL 10, NMCE achieved comparable and sometimes better results, compared to baseline methods or some alternative manifold clustering methods listed in the paper. ### Strengths & Originality: The idea of combing "Maximum Coding Rate Reduction" with data argumentation techniques is interesting and seems reasonable. In the experimental parts, this work have results on a number of real data sets, which can be considerate a fairly enough set up to support the effectiveness of a newly proposed algorithm. Alternative methods: in the experimental results & section 4, there are a number works are cited and compared to NMCE but still lack some important references. Reference 1 and 2 listed below are two methods in the field of nonlinear manifold clustering and some in depth discussion or comparison to them are quite helpful for reviewer to evaluate the contribution from NMCE. Indeed, there is one survey paper about nonlinear manifold clustering is cited in section 2 but this is not enough.<|endoftext|>The proposed method leverages previous results on subspace clustering via neural networks and a penalty function based on concordant classification of samples and their augmented versions. The paper provides an intuitive extension of the MCR^2 embedding approach by regularizing it with a "concordant data augmentation embedding" constraint. $L_{clst}$ is described as "some objective function that will force $f$ to cluster the dataset" but again there is no practical example of such a function. In the appendix the authors say "the performance of [MCR^2] is rather poor, which is expected based on our understanding"   this does not illuminate a reason for the reader or make it clear what "CTRL" brings in. Also there are no descriptions of the computation cost of the different approaches compared, including the proposed one. The authors should provide some discussion on this aspect of the implementation   this can be done in the appendix if needed.<|endoftext|>It seems the authors had several options and made their choice, so this section and table 2 can go to the “additional results” appendix to justify the choice. The paper combines two previous lines of work into a novel loss function which is then used in both supervised and unsupervised manners and achieves state of the art results. # Paper strengths #The paper suggests a loss function for learning manifold clustering and embedding by combining two ideas from the literature. It is not clear why the unsupervised method is needed, while the supervised method is not presented at all. Restate the paper contribution to make the distinction between the loss function, the unsupervised, and the supervised methods. No motivation is given for the role of such work in the ML landscape. Most notable are the mysterious references to MCR2. Also, provide the (poor) result of the full algorithm. (3) The statement “we need augmentations that perturbs style information (...) but preserves content information, so that the clustering will be based on content but not style” seems weird as this is almost the definition of data augmentation. You should provide some evidence stage 2 is helpful or if not skip it for the sake of simplicity. (6) I don’t understand the discussion which makes most of section 4.2.1, about using representations at different levels of the embedding network, and with or without averaging.<|endoftext|>Through their analysis, to achieve the goal of manifold learning in unsupervised fashion, in principle the authors emphasize the importance and necessity of geometry awareness constraint to be added upon the MCR2 objective. And then apply the intuition that the learnt feature embedding of augmented data points should be similar if they are generated from the same point as the geometric constraint. I think this is a very interesting and useful way to enforce the local structures in the data. The authors show a toy example with their code for the performance of the proposed principle, although no code for reproducing large experiments is provided. It is hard to say what performance could be given the high complexity of data while a simple data augmentation strategy is used, although the past contrastive learning objective works well in practice. The paper can be further improved by increasing readability. Can you give some discussion on the dimensions d1,d2,...,dn? What is their role in the algorithm? The proposed method builds upon the existing existing Maximum Coding Rate Reduction (MCR2) objective with a data augmentation as a constraint to enforce manifold learning. It seems that the performance is satisfactory comparing with other state of the art methods.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper theoretically analyzes the Feedback Alignment (FA, Eqn 2) algorithm in the optimization of deep linear networks. * Under the assumption that the data matrices, FA matrices and initial weight matrices can be diagonalized simultaneously, the optimization can be divided into several one dimensional problems, and the paper proves the convergence of both continuous and discrete dynamics. This paper focuses on the theoretical analysis of FA. The authors find a setting (linear networks with diagonalizable matrices) to simplify the original problem to one dimensional dynamics, then analyze the convergence rigorously. I am not familiar with the related work about FA, but my main concern is that the one dimensional dynamics may oversimplify the original problem; and for $L > 2$, the convergence is proved only for a special initialization. May we get similar results as GD (given the FA dynamics seems to be simpler than GD)? I am afraid the current one dimensional analysis is still far from filling this gap.<|endoftext|>The authors prove the linear convergence rate of the feedback alignment algorithm on fully connected linear networks. They also identify the implicit anti regularization phenomenon for certain initialization of the algorithm and further propose initialization schemes that provide a form of implicit regularization and facilitate the learning process. It provides convergence results on deep linear neural networks for feedback alignment for both discrete and continuous cases. They also show that the convergence results hold for networks that are not necessarily over parametrized. Feedback alignment method is named after one of its outstanding characteristics that the forward weights get aligned with the random backward weights. However, this paper does not seem to have even mentioned any attempts on such explorations. One of the key steps of reducing the original system of ODEs into a scalar system relies on the assumption that the initialization of the weight matrix for the second layer is deterministic given the one for the first layer. However, the convergence results seem to assume strong conditions that only works for specific cases.<|endoftext|>This paper studies the Feedback Alignment (FA) algorithm on deep linear networks. In continuous time, FA algorithm is an alternative to Gradient Flow (GF) In particular, to compute the time derivative of the weights, FA algorithm starts with the gradient of the weights at each layer, and replace the part that depends on the weights of succeeding layers by a fixed matrix. The FA algorithm on deep linear networks under spectral initialization is studied regarding both convergence and implicit bias, and the analysis are provided for both continuous time and discrete time FA algorithm. I believe not all linear regression problems admit spectral initialization. The introduction of spectral initialization in Section 2 is misleading to me because it is written in a way that suggests one can do spectral initialization for any linear regression problem. The author should clarify this. 2.Spectral initialization alone is too restrictive to understand the training algorithm. This makes some statements in the paper much easier to understand. Exact solutions to the nonlinear dynamics of learning in deep linear neural network.<|endoftext|>The dynamics of linear neural networks trained with backpropagationhas been studied extensively, thus providing an ideal test bed for thisstudy. This paper is a nice theoretical development, extending the theory of linearneural networks to the case of DFA. The authors first study the shallow casebefore extending their arguments to the deep case. I found the results concerning the implicit bias interesting   it is surprisingthat FA would exhibit this inverse learning order, and I am not aware of animplicit bias that would depend on the initial conditions in such aqualitatively different way. The numerical experiments nicely illustrate the results, and show very goodagreement with the theory, even if some of the assumptions (e.g.oninitialisation) are relaxed in the experiments. Further comments / questions:  In the analysis of the discrete update equations, what exactly is the  motivation for the additional term $W_1^{t+1/2}$ in Eq.25?Is this system of  equations simply more convenient for the analysis? If so, how does it connect  to the algorithm?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper considers the problem that differential privacy disproportionally degrades the performance of minority groups in the federated learning context. While the proposed approach is novel and interesting, various motivational aspects (e.g., comparison with prior work) are unclear or lacking. I believe there is also such a tradeoff in the proposed approach. `The paper does not compare to prior work on group fairness.` While the paper shows that the proposed method can reduce various group fairness metrics in the federated setting, it would be interesting to see how it compares to other methods for group fairness in the centralized setting (e.g., Zafar et al., (2017)).<|endoftext|>In this paper, authors propose a federated learning algorithm that is able to satisfy group fairness while maintaining privacy. 2.Paper is easy to follow and clear. It is true that they might not satisfy privacy considerations, but they could still be included for the reader to have intuition of how your method compares against such methods especially that the results are not promising in the paper. Some baselines to consider is Cui et al or Tran et al.Authors claim that they do not compare with Tran et al since Tran et al does not consider privacy, but still I believe these types of results should be included as I am not sure now how effective your approach is in enforcing fairness compared to such baselines. 3.How do authors justify some of the incompatibility results shown in previous work between fairness and privacy? e.g., What do clients do? Discuss why? Baselines should be added along with discussion around some of the results which does not seem to be promising. More details can be found under weaknesses discussed above.<|endoftext|>This paper studies the problem of training neural networks with differential privacy and fairness guarantees. The authors proposed an algorithm using a modified method of differential multipliers. They compare their proposed algorithm with federatedSGD and its variants with clipping and adding noise. I think the main contribution of the paper is adapting the MMDM algorithm to satisfy fairness constraints. Similar techniques have been proposed in the federated learning literature. It seems that these subsets may depend on $w$ in general for example for accuracy parity, which makes the optimization problem in (P2) more difficult. It is not clear how those parameters are set. It will be nice if the authors provide further clarifications. should it be $k$ instead?<|endoftext|>The fact that DP (and in particular DP SGD) makes models unfair (e.g.disparate impact on accuracy for minor groups) is well known (Bagdasaryan et al., 2019), however it is possible to have a satisfying trade off between these ethical measures (Jagielski et al., 2019; Cummings et al., 2019). I cannot find any parameters for the DP training in the main text (what was the epsilons / noise level etc.?) The role of hyperparameter tuning does not seem to be addressed really (at least in the main text). In DP, things get more difficult as the dimension increases, does it affect here? “perhaps simply “belongs to a group $A$”. However quite a few questions remain, I would be happy to see this paper accepted in case the authors can make those small modifications (that I have listed) and also address my questions.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; I have carefully read the paper and like the idea of this paper in making self supervised representation leanring towards dense predicton tasks like segmentation/clustering without any explicit mannual supervision. Hopefully the authors can address my concern in the rebuttal period. Pros:  The paper is well written and easy to follow. The idea of pushing features from self supervised representation learning towards segmentation task specific ones is really interesting, simple and effective.<|endoftext|>The paper introduces STEGO (Self supervised Transformer with Energy based Graph Optimization), a novel feature correlation refinement method that builds on top of modern self supervised visual backbones (visual transformers) that generate dense semantically correlated features in an effort to improve scene semantic segmentation without any type of labels (unsupervised). The paper is well structured and the method is sound.<|endoftext|>In this paper, the author proposes an unsupervised semantic segmentation approach STEGO. Weakness:1)	Novelty: The main contribution is that it trained the additional segmentation head with the proposed contrastive loss over the frozen visual backbone. For example, there are too many hyperparameters (eg, b and lamba) in the contrastive loss definition and it seems that the tunning of the parameters is complex.<|endoftext|>The paper proposes a novel transformer based unsupervised feature distillation method for semantic segmetation  STEGO (Self supervised Transformer with Energy based Graph Optimization). The pipeline has two separate stages, feature learning and cluster compactification (feature distillation). This is not shown in the main paper, but I have tried the provided code on CocoStuff and those are my findings. One would assume that future work would boost the proposed method, which has a significant performance boost over the baseline (DINO). At the very least, I would like to see numbers with knn only, self correlation only and random correlation only.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The authors propose a technique based on tensor decomposition to improve the robustness of GNNs in practice. The numerical experiments are indeed relatively promising. The intuition is explained by the authors, but not much more. In particular since the tensor decomposition is effectively a low rank approximation approach, I do not quite see how certain graph features would be maintained if the "robust" adjacency matrices included in the tensor would not all pick them out? The computational complexity of their approach is not discussed at all as well. This is a very large additional cost and there seems to be no way around this? In addition, computing a tensor decomposition is itself a computational difficult problem. How do the authors guarantee that they have found the right decomposition? How to chose which decomposition to use, with which number of components etc? All this seems not very clear. What is a however lacking isa) a solid theoretical foundation for why this method should work in practiceb) an analysis of the computational costsc) a more thorough discussion on the computations and the choices made for the tensor decompositiond) a stronger empirical validation (given in particular the apparent lack of theoretical results).<|endoftext|>By leveraging the low rank and feature similarity properties of graph data, the authors use three predefined robust graphs with the adversarial graph to form a graph tensor, and then apply tensor decomposition methods to learn a robust graph. 3.The experiment is comprehensive, and the results demonstrate that the proposed method outperforms baseline methods by a large margin. For example, if there are adversaries perturbing the node features as well, the perturbed information can also affect the generation of the robust graphs. This will jeopardize the robustness. Based on the discussion above, the methods in the paper may not have great novelty. We would vote for “weak accept” to this paper.<|endoftext|>This paper studies the problem of defending against adversarial attacks for graph neural networks. Then low rank tensor approximation is applied to generate a more robust graph structure while jointly learning the GNN parameters. It is interesting to see that the author formulates the graph learning process as a 3D tensor decomposition problem. As shown in the empirical results, the improvement is impressive under various types of attacks and the learned graphs are shown to preserve the common patterns of robust structures. However, there are still some concerns that need to be addressed. 1.Analysis of time/model complexity should be included. 2.It would be more convincing to include larger datasets to demonstrate the effectiveness of the proposed model. 3.The method seems to rely on the unperturbed features.<|endoftext|>In this paper, the authors propose a tensor based framework for GNNs to learn robust graphs from adversarial graphs by aggregating predefined robust graphs to enhance the robustness of GNNs via tensor approximation. 2. the proposed method is efficient and effective. 3.The idea seems can be used in more than the adversary attacks. ### reasons to reject1. This article lacks the necessary ablation experiment to demonstrate the necessity of tensor approximation. For example, in most cases without noise, it is able to outperform different GNNs. Thus I guess there is theoretical justification to prove something. But I really hope the authors can dig something more rather than just propose a method for the adversary scenario. If it is not, I will turn my score down.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The authors consider to maximize the minimal value function $\mathbb{E}[g_{min}(\bar{V}_{1:T})]$ and achieve a regret bound of $O(\sqrt{logK}T^{\frac{2}{3}})$ given an oracle to solve the RL problem with scalar reward. The intuition of ex post max min fairness is not clear. The authors try to show difference between ex ante max min fairness and ex post max min fairness. However, the two problems are almost the same for weak communicating MDPs. 2.The requirement of the proposed algorithm is too strong. However, given the concerns above, I tend to reject it.<|endoftext|>This paper considers ex post max min fairness, where an RL algorithm is provided with provable guarantees. The difference between ex ante and ex post are explained. 2.Note that even though the E[min] and min(E) are shown to be different in an example, the gap is O(1/T), and not really relevant to the problem of regret.<|endoftext|>The paper studies an RL problem, where the rewards are given as a vector at each time step. The goal is to find a policy that balances the total rewards on different dimensions of the reward vector, i.e., one that maximizes the total reward of the worst dimension. The authors also conducted experiments to evaluate the proposed algorithms. The paper also clearly illustrated the difference between ex ante and ex post optimizations. The main algorithm presented has a theoretical regret bound.<|endoftext|>The paper considers the ex post max min objective for the vector reward RL problem. An MWU based algorithm is proposed to solve the problem with a provable regret guarantee, where the algorithm resorts to an approximately optimal policy oracle episodically. The algorithm also features a variant that can fully rely on offline solutions. Say, if we adopt a linear objective with equal weight or a CVaR objective to the two rewards for this example, would we end up with the same optimal policy as the ex post? (ii) Knowledge of the underlying dynamics (reward and transition): The oracle used by the algorithm requires the knowledge of the underlying reward and transition, at least some estimates of them.
Reject; rating score: 5; rating score: 5; rating score: 6; The paper introduces a connection between reward free and constrained MDPs. While the paper is well executed and seems sound to me (I didn t have time to check all the proofs), I have doubts due to the limited technical novelty. I understand the proposed meta structure allows to account for multiple problems, but the core idea is not very novel. At a high level, the idea is that the considered problems can be formulated as a saddle point problem that can be solved with different incremental techniques. In particular, the second approach used an estimate of the model to compute an optimistic policy (in the best response step), and they were able to prove regret guarantees. I think this is quite a limitation of the current paper. ICML 2021I have concerns about novelty and contributions, and the fact the bounds may be improved.<|endoftext|>The resulting approach is indeed simple as the title suggests. The approach is general and gives sample complexity guarantees to any constrained RL formulation for which reward free algorithms exist. Can the authors please elaborate on regret? Are there any lower bounds for constrained MDPs? the authors do not mention them at all, and I think that they are crucial in order to put the presented algorithms and their guarantees in perspective. 3.The sub optimality also comes into play in the $H$ dependence. How does the computational complexity of the proposed approach compare with other approaches? I am not sure what is the answer, but if it is worse then this is definitely another shortcoming of the presented approach. The paper presents a simple approach to an interesting (and still open) problem, but in my opinion the mentioned flaws of the approach must be addressed in much detail before the paper is published.<|endoftext|>I think in the current form, it is not clear what the benefits of such an approach are. The proposed approach comes with an overhead factor that is logarithmic in the number of samples. The paper is well written and generally clear. This concern also follows the other parts of the work. Moreover, the motivation for VMGs is missing from the introduction. I think there are some really interesting results in this work, however, there seems to be a conflict in the motivation (constrained RL problems) and methodology (reward free RL solutions).
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper developed an adaptive aggregation method for federated learning. The theoretical analysis shows how the interval affects the convergence rate. The experiments show that it can reduce the communication cost. 2.This paper provides the convergence rate. Cons:1.There are some errors in theoretical analysis. Eq.(15) is not correct. Eq.(25) is also wrong. Thus, this bound is much worse than [1][1] On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non Convex OptimizationThis paper proposed a new idea for reducing communication costs.<|endoftext|>This paper proposes an adaptive interval schema for layer wise model aggregation in federated settings. The aim of the proposed method is to reduce communication costs by adaptively decreasing the frequency of layer wise aggregation with consideration of model discrepancy. It is uncertain that the proposed method can really reduce communication costs. Moreover, at each communication round, the client still needs to download the full model from the server. Moreover, the communication cost should consider the number of communication rounds to achieve a convergence state. Algorithm 1 should explicitly state which steps are executed on the server or client. 2.It is necessary to draw the convergence curves of the proposed methods and baselines. 4.There are some federated learning methods that only update parts of its model. This paper should also consider these baseline methods.<|endoftext|>In this paper, the authors propose a layer wise model aggregation scheme in federated learning cases to reduce the communication cost. By increasing the aggregation intervals and relaxing the aggregation frequency, the method can reduce the communication cost in federated learning cases. Some questions are the following. They do not show the total time in terms of communication cost, just show the reduced ratio for communication. In some cases, especially in industry, the model s accuracy matters more than training time (e.g., the total training time is within several hours.) Thanks to the authors for their efforts for their theoretical analysis and experimentally proof of the proposed federated optimization scheme. The experimental results are a little weak to show the effectiveness and the practicality.<|endoftext|>This paper proposes a novel adaptive model aggregating schema under a federated learning setting, where it effective reduces the communication overhead without significantly sacrificing the generalization performance. Both empirical study and theoretical analysis are included to justify the effectiveness and efficiency of the proposed method. Effectively reducing the communication overhead is an important problem for federated learning, with the recent advance of layer wise model freezing training, this paper manages to extend this idea for network traffic reduction in federated learning. According to the empirical study, this method seems to be effective and efficient. + The paper also includes tight convergence analysis for the proposed method. Although some part of the cost model in the simulation is a little sloppy, I tend to advocate the acceptance of this paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper claims that optimization of the Kalman filter parameters are needed in cases where the filter assumptions are violated. It looks like that the paper tackles the problem of noise estimation of the Kalman filter in an alternative method. However, the paper is not written in a cohesive way which makes it very difficult to follow what it narrates. The title of the paper does not explain what has been tackled in the paper. The paper is not written well and does not have a flow. Therefore, I can not recommend it for publication.<|endoftext|>This paper proposes using gradient based optimization to tune the state and noise covariance parameters defining a Kalman Filter via supervised learning (i.e., assuming access to ground truth state measurements during training). That the Kalman Filter is only optimal under strong assumptions, and that it can fail spectacularly when these assumptions are violated, is very well known and hardly a contribution. Strengths+ The problem of estimating state from noisy observations for nonlinear systems is an open and important challenge, and the proposed method is intuitive and simple to implement, and appears to yield good empirical performance. Definition 2 is formulated under the assumption of a linear time invariant system; however, Kalman Filtering approaches in the time varying setting also exist, see for example the textbook "Linear Estimation" by Sayed, Hassibi and Kailath.<|endoftext|>The paper studies the problem of using Kalman filter for estimating the state of a dynamical system when the noise covariance matrices, for both state dynamics and observation, are unknown. The paper assumes access to trajectories of both state and observation. In this setting, a natural approach to solve the problem is to use the data to form an estimate for the covariance matrices. However, the paper argues that an optimization procedure to find noise covariance matrices to minimize the MSE is favorable and should become the "new standard procedure for KF tuning". Therefore, the MSE optimization problem can only recover the ratio between noise covariances.<|endoftext|>This paper targets the design of a classical filtering method   the Kalman filter (KF). The linearity assumption is a strong limitation of KF models although a wide range of variants have been demonstrated for non linear systems. Different from these studies, the authors focused on the estimation of the noise models in the KF( class) models, in order to improve the accuracy and robustness of the KF estimates, through an optimization method. Also theoretical analyses are provided in the appendices. ( ) The proposed method is a supervised learning method   ground truths are required   this is the major downside as compared to the regular KF methods. Although a few samples of the filtering results over the real world data were given in the appendices, no statistical comparisons were shown. However, the ground truths are enforced, which are the most costly and most difficult to obtain in practice.
Reject; rating score: 3; rating score: 3; rating score: 3; The paper proposes a method of calculation for filtering and control in continuous time linear systems based on using a simulator to generate many trajectories and use their statistical properties to empirically calculate the needed estimation and actuation signals. Incomplete literature review: Most of the existing references focus on discrete time settings (for which the literature review is still incomplete, just search adaptive linear systems in google scholar), and none of the modern works on reinforcement learning, estimation, and control in continuous time linear systems are reviewed in the manuscript (again, google scholar helps). An incorrect benchmark is adopted on p6 in Relationship to RL: the reasonable benchmark is to solve DRE/ARE. The problem understudy is not well motivated. and, further items discussed above.<|endoftext|>The EnKF is also not well motivated for this setting and the experiments are very limited and seemingly not reinforcement learning tasks. While RL is mentioned in the title and abstract, it seems the eKF is actually applied in an optimal control setting rather than RL. Given the linear setting, the experiments a small scale for a venue like ICLR. Minor comments:5) The paper title does not need to include abbreviationsThe paper does not reference a large body of existing work on the Kalman fitler / LQG duality which the paper claims is their contribtion.<|endoftext|>The paper is about designing a simulation based ensemble Kalman filter algorithm for learning the optimal control policy for the Linear Quadratic Gaussian (LQG) control problem. And it is shown to be exact in its mean field limit (N infinity). The proposed algorithm would be more computationally efficient for the above case where N<<d. Although N is the number of particles, and we could use N<<d. Based on my above comments, I think the paper is interesting but not good enough to be accepted at this time.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; To show this, the author proposes a computational framework named OntoProtein that makes use of structure in Gene Ontology into protein pre training models. Good paper but more efforts are needed. This paper seems to be the first to impose the knowledge graphs of protein. 3.To demonstrate the effectiveness of the proposed method, the author constructs a large scale KG dataset, promoting the research on protein language pre training. Cons: 1.To make the paper more friendly for the general ML/DL researchers, I think it should be made clearer the definition of the downstream tasks used in this paper. It is possible to compare the proposed method with [1] and [2]? 3.The presentation of this paper should be further improved. (2021).Deep geometric representations for modeling effects of mutations on protein protein binding affinity.<|endoftext|>This paper introduces a method to enrich the representations that are learnt by protein language models with knowledge encapsulated in gene ontologies. to jointly train knowledge embeddings and protein embeddings, with the objective to enhance the latter and subsequently improve the performance on various downstream tasks (namely TAPE benchmark, protein protein interaction and protein function prediction). The research direction is important and well motivated   a lot of valuable information is captured in gene ontologies and ought to be valuable to improve protein representations and performance on downstream task. with no adaptation to the specificities of the GO modality. The research direction of this paper is very important and that the work described here is adequately framed and motivated. A valuable contribution is the ProteinKG25 knowledge graph put together by authors, which could be valuable to other researchers.<|endoftext|>This paper introduces OntoProtein a comprehensive pre training framework for protein embedding with the knowledge of gene ontology (GO). More specifically, OntoProtein jointly optimizes on both masked Protein Model and Knowledge Graph Embedding model which results in knowledge aware protein embedding for downstream applications including protein protein interactions and protein GO association prediction. The authors also create a new benchmark of proteins with aligned annotations to facilitate the OntoProtein training. Detailed comments: * One of the major concerns of this paper is that some existing works with joint training on protein with other domains. The critics of this work are mostly about some missing references and baseline approaches, as well as the limitation on combinations of existing works.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; It does not provide empirical validation of the method, or an implementation. This paper provides an algorithm for training neural networks in the over parametrized regime. Do the authors have such an implementation handy? I believe this paper is a good contribution to the field   on the other hand, it would be made much stronger with such an implementation. The limits of the method are hardly discussed in the main text. Other practical second order methods have been proposed for neural network optimization.<|endoftext|>This paper aims to propose a second order algorithm for training of neural networks with very large widths. The main claim is that the computation cost of each iteration of the proposed methods is sub quadratic in terms of the network width. I cannot agree with this. For example, in both Du et al (2019a) and Liu et al (2020a), exponential convergence rates on over parameterized neural networks are obtained for gradient descent methods. The authors should provide evidence. The main text of the paper is not self containing and not justifiable (suggest to reorganize the paper).<|endoftext|>This paper studies the training algorithms for multi layer over parameterized neural networks. In particular, this paper starts from gauss newton methods and incorporates tensor based sketching techniques and preconditioning to improve the per iteration computational complexity. As a result, the proposed algorithm can find the global solution within the time that is subquadratic in the network width. This could be a more fair comparison to justify the better performance of second order methods.<|endoftext|>1.This paper proved that the second order method can minimize the training loss in linea rate on multi layer over parameterized neural networks. This paper is clearly written. The detailed proofs are left in the appendix and the authors explained the high level proof techniques in the main text. 2.This paper reduced the per iteration cost of second order methods by combining the Gram Gauss Newton method, tensor based sketching, and preconditioning. It s a crucial challenge in second order methods to reduce the per iteration cost. It can be helpful if there are empirical experiments that verifies the performance of the algorithms in standard architecture and realistic data set. Above all, I think this paper is marginally above the acceptance threshold. Thanks for the response.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper introduces Adversarially Compounding Complexity by Editing Levels (ACCEL). ACCEL is an Unsupervised Environment Design (UED) algorithm, a method of generating a curriculum of environments so as to train agents that generalize well to either a training distribution of environments or off distribution environments. The paper is well written and fairly easy to understand. While Lava and MiniGrid comparisons of this form are standard for UED methods, they are not the only ones, and in particular, they seem to be the most obvious contenders for an editing type approach to be successful.<|endoftext|>The work proposes an extension to the recently proposed "Robust Prioritized Level Replay" (R PLR) method that aims to train RL agents capable of generalizing to a large range of parameterized tasks by curating a curriculum over training tasks. Strengths:The main idea of the paper is intuitively reasonable and the experimental section shows promising performance on the investigated tasks. I understand that the authors take this result from the Robust PLR paper, to which their method is very similar. Does this change the theoretical results? It would be interesting to see how a method following a different methodology performs in these experiments.<|endoftext|>A simple method of curriculum design by editing existing levels solvable by the agent to more challenging ones. The paper is easy to follow, the method is clearly explained, and the experiment is well presented. It is still unclear to me why the proposed method solves the problem of not having the editor generating levels that are straight up unsolvable. there is not much to say except that this is an intuitive solution to a good problem. the results over existing tasks look promising, but it would really benefit from more complex environments and edits, where the agents and environments interact in more ways than simply pathing.<|endoftext|>The paper introduces the idea of adding small incremental changes to grid world levels, to create an automatic curriculum for navigating agents. The paper feel like it s a small incremental change to existing literature and since I can t really imagine how this is going to generalize to non gridworld domains, I can t recommend acceptance at the moment. The paper is well written. If the authors could maybe include additional discussion of how their method applies to other environments or better, include experiments in non gridworld settings, I would change my rating. Because if it s from during training, the level difficulty would dictate the meaning of the return plot. So maybe a different metric would be better like return divided by  difficulty metric (like shortest path length) or something like that. Wouldn t these methods also work (similarly) to generate the curriculum for your agent?
Reject; rating score: 3; rating score: 6; rating score: 8; rating score: 8; The paper amis to porpose a novel MLC method within a budget limitation. The technique is very old, the novelty and the experiments are very limited. 1.The paper claims that "FrugalML requires a large amount of training data and involves solving a non convex optimization problem with complexity exponential in the number of distinct labels". However, the paper does not provide the time cost of their proposed method and also do not report their time cost in the experiments. Since the paper aims to design an efficient ML API selection strategies for multi label classification tasks, but the paper did no show any benifit of time of their proposed method from both thoery and experiments. This is a major flaw. 2,The paper just combines existing technique, but did not bring any new technique, or new theory or new insight. 3.The paper presents online algorithm. Then what is the regret bounds, i.e.bounding the regret of the proposed algorithm over the worst case sequence? What are the advantanges and disadvantages of the proposed methods and other existing MLC or XMLC methods?What is the different and connection between them? Can you provide the theory and experiments guarantee? 6.Can you test your method on XMLC datasets?<|endoftext|>Given several mutli label machine learning APIs, this paper study how to select those APIs under a budget constrant while striving to improve the overall accuracy. The author first formulate the budget API select problem as an integer linear programming problem, then relax the integer contraint and solving the relax problem in dual. The advantage of such modeling is to have a fast decision function for online deployment of their API selection systems. The experiment results are promising and consists of several ablation studies. ## Strengths:  novel problem setup of MLaaS, which is very practical for industrial practionioners  solving the API selection problem as relaxed integer programing seems techincal sounds  most of the writing is clear and easy to follow## Weakness:  While the paper aims to solve for multi label problem, most components in the proposed FrugalMCT seem not tailor for the mulit label problem, except for label combination part mentioned in Sec 3.4. FrugalMCT needs call all candidate APIs do prediction on the training set so that it can collect supervision to learn the accuracy predictors. This step induces extra costs, but did not show up in the experiment results. ## Major Comments  It seems to me that we can also apply FrugalMCT framework to an online production systems, where we have several ML predictors each with different inference latencies. We would like to select multiple ML predictors under the total latency constraint while striving for the best overal accuracy. It would be very interesting to see the impact of FrugalMCT in such cases. ## Minor Comments  In the caption of Figure 1, "is is high"  > "is high"This paper is novel and techinical sounds. The proposed framework is not specially optimized for multi label problems, but it is still a good paper to see in ICLR. Thus, I am incline that is paper is marginally above the acceptance threshold.<|endoftext|>In the paper "FrugalMCT: Efficient Online ML API Selection for Multi Label Classification Tasks" the authors present an approach to learn how to select online APIs that machine learning as a service (MLaaS) for the problem of multi label classification. In theory the authors  approach has convenient performance guarantees and can be implemented efficiently due to leveraging the specific structure of the problem. In the empirical evaluation the authors also prove the practical benefit of their approach demonstrating that their approach can achieve a higher predictive accuracy at a lower cost than the single best API, thereby outperforming the single best MLaaS platform in terms of both accuracy and cost which is not a really surprising result as long as the performances of the different APIs are complementary but still a strong result with practical benefits. # SignificanceThe contribution is significant and I estimate that this work will have great impact on the respective community. # NoveltyAlthough the basic idea of selecting APIs has already been done in previous work (FrugalML), except for the basic idea, the remaining paper is to the best of my knowledge novel. # RelevanceThe paper is definitely relevant to the machine learning community. Overall, I really enjoyed reading the paper and it was refreshing to see such a practical application and benefit for the end user when using the approach in terms of both predictive accuracy and reduced costs.<|endoftext|>This paper addresses the practical task to use the combination of ML APIs for multi label classification. Different from the related work FrugalML which ignores the correlation between ML APIs, the proposed FrugalMCT allows selecting and combining the different ML APIs based on a budget. Sufficient theoretical and empirical analyses are provided to demonstrate the effective of FrugalMCT. It is a practical problem to combine the existing ML APIs for better performance under a budget. This paper focus on three multi label classification tasks, which is more challenging than single label classification. The most related previous work FrugalML ignores correlation between different ML APIs, while FrugalMCT could select from the combination of them. And the GitHub model cost is also analyzed. The case study demonstrate that the proposed FrugalMCT improves the classification results compared to the single ML API. The authors mention that the complexity of FrugalML is exponential to the number of labels. It could be interesting to compare the training and inference time of FrugalML and FrugalMCT with different ML APIs. Recommended for acceptance.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The authors present a VAE for multivariate time series generation. The authors add additional decoder building blocks modelling level, trend and seasonality components of time series. Their method, named TimeVAE, is evaluated on four different datasets. I would be interested in the authors thoughts on that. Although the authors perform experiments on 4 datasets to compare to previous works, the hyper parameters of the core building blocks are not evaluated properly. Therefore, this paper is not ready for publication at ICLR in my opinion.<|endoftext|>The paper presents a method for a generative model of time series data based on variational auto encoders. They suggest two models, a simplified base version as well as a conditional "interpretable" one that can be used to customise trend and seasonality aspects of the generated data. The paper is well written and I think clearly conveys the main points of their models as well as the related work. Primarily however, it is not clear to me why the proposed models (and definitely the proposed "base" model) are not just essentially a standard VAE (or conditional VAE) applied to time series data. Equations should be formatted correctly in the middle of the line and numbered.<|endoftext|>This work proposes a generative model architecture for time series that is trained using the variational auto encoder framework. The paper has some interesting ideas regarding incorporating domain knowledge and interpretability into a deep generative model for time series, but falls short in a large number of aspects. Results are carried out on a number of new datasets and compared to the aformentioned GAN based time series models. Results on the proposed datasets and proposed metrics clearly show the advantages of TimeVAE over a set of chosen GAN based models. * Weaknesses *I do not think this paper is ready for publication as there are substantial limitations in its current version. The generative model decodes the latent vector to produce the whole output sequence. Have the authors considered such a design? While interpretable components in the decoder are introduced, these are presumably not used at all in the experiments and no evaluation is provided as to the benefit and/or intepretability of these components.<|endoftext|>This paper proposes an architecture for synthetically generating time series data with the use of VAE. The authors claim that the contributions for this paper are interpretability, capable of encoding domain knowledge, and reduced training time. This paper is motivated by the limited data access for the time series domain, so the authors propose a synthetic data generator that they claim will be valuable for deep learning research. 1.There is no discussion about the interpretability in the results session. 2.In Table 1, there is no obvious better performance for the proposed model besides the dataset sine. Based on the results, I didn’t see significant performance achieved compared to the other state of the art methods.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The idea is that as one smoothens the decision boundary of a piecewise linear function $f$ (e.g., from ReLU Net) its saliency map ($g$) obtained by $g \frac{df}{dx}$ gets closer ($||g   n||_2$) to the normal of the closest boundary hyperplane ($n$). This is then used to motivated a variant of IG, called BIG which integrates SM over a line path from the sample to the nearest adversarial example. The results seem to corroborate as for a better alignment with the nearest boundary hyperplane s normal. However, it is unclear to the reviewer how $\frac{df}{dx}$ can be guaranteed to be prependicular to the decision boundary of a general function $f: \mathbb{R}^d\rightarrow\mathbb{R}^K$ with K being the number of classes. In fact, I believe for the simplest case of linear binary classification, as soon as we (redundantly) model each class with a separate linear model (to become analogue to the multiclass setup), the gradient of each of the linear functions i.e., $\frac{df_1}{dx}$ and $\frac{df_2}{dx}$ will no more be orthogonal to the decision boundary. Has the authors pushed enough to find some indication of this trade off? In light of that, how reliable are the observations in the experiments section? Following up on the previous question, could the fact that BSM does not show improvement on standard models be due to this approximation? In Theoretm 1, it might be better to use $O(\frac{1}{\sigma c})$? However, there are some questions that makes me keep my rating only at borderline accept.<|endoftext|>The paper focuses on the intersection of gradient attribution and adversarial robustness. The paper has an interesting topic: adding theoretical insights to explainability methods. However, under which conditions does it resemble a normal distribution? Limitation 2 (not applicable to perturbation attributions) arises from focus of the paper and I think there is not need to mention it. While the initial submission was a clear reject, the rebuttal was well done. I agree with the concerns of the others reviewers about novelty. The other limitation of Theorem 1 is that it only holds for one layer ReLU networks. Results are computed on ResNet50. ### EvaluationI think the evaluation of the normality to the decision boundary can be improved.<|endoftext|>a) First it shows that one reason behind the attributions being more interpretable for adversarial robust models is that for these models, the gradient with respect to the input is more closely aligned with the normal direction to a close decision boundary. b) Using the previous fact, they devise two new attribution methods, BSM and BIG which can be used to get more interpretability/explanation from even a normal (non robust) model.<|endoftext|>This paper introduces boundary attributions, which leverage the connection between boundary normal vectors and gradients to yield explanations for non robust models that carry over many of the favorable properties that have been observed of explanations onrobust models. It also proposes a BIG to explain models. My major concern for this paper is that the conclusion has already known. For example, Ilyas et al shows that robust models can produce better perceptual aligned features when gradient descent, and adversarial robust models are known to have smooth decision boundary [1]. The conclusion is not novel to me, which is already known to the community.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper presents a nice conceptual overview about the need for human compatibility of actions associated with observations in zero shot coordination scenarios. It is surprising that the idea has not been explored in POMDP networks before, and a more comprehensive related work section could help in understanding why this is the case. There could be several other strategies used during game play. Would the model still work (or probably work better) if higher order embeddings are added to reduce the actual feature distance between input action and output? Topical: Looks into a relatively new problem of generalization during cross play after training using self play. It might help to show the performance accuracy tradeoff of those methods vis a vis currently proposed approach.<|endoftext|>While more explanations and analysis are preferred for the experiments, my current rating leans to acceptance. Weaknesses:1) Some details of the experiment setting may need further explanation. Besides, the paper develops a novel human interpretable environment for analysis. Strengths:1) The paper is well organized.<|endoftext|>Even for me, someone not in the specific field of multi agent RL, the text seems a bit redundant. The paper offers an interesting study showing that action and observation information should both be leveraged to form interesting communication / coordination strategy. However, the paper is limited by the features are explicitly available, making strategy generation less impressive.<|endoftext|>It would be good to have more evidence that the approach is generally useful. This paper is quite far from my expertise, and there are a number ofconfusions I had about the paper:  What is special about the new hint guess game? A number of the architectures seem to have poor performance on the game. The evidence shown supports this claim.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a generative approach to model explanations, where explanations are composed of automatically generated counterfactual examples that differ by the observed magnitude of a given learned concept. Through a series of extensive experiments, the authors show that the proposed approach, DISSECT, can successfully disentangle useful and realistic concepts. This is an original paper that introduces a novel approach to model explanations. It builds on several advancements in the literature, and aims at producing a solution that is useful for the users that will actually run it. First, I am worried about the dependency on a strong generative model that can successfully disentangle concepts. It could be that this approach would fail on data with structures that do not allow such an easy generation process, such as natural language. While the authors are clear about their use of this work, the similarity of the approaches is significant.<|endoftext|>The method creates counterfactual explanations for which the concept increasingly influences the classifier s decision. **Strengths**:  novelty: the approach taken in this paper is very different from other counterfactual explanation methods. This is the first paper I ve read that creates several explanations which increasingly rely on a concept. I think this aspect is very interesting and could be relevant in practice. For example, as highlighted in the paper, it can be used to depict how a mole turning from benign to malignant may progress. It would be great to see some more justification on whether the evaluation metrics actually measure what they are trying to target. to a certain extent, the method is designed to target these metrics. Overall, I would recommend an acceptance. The authors introduce a novel method, called DISSECT, which brings a meaningful contribution to the aforementioned field.<|endoftext|>This paper proposed a generation based explainability method, DISSECT, that generates Concept Traversals to provide a counterfactual explanation. The Concept traversals (CTs) are sequences of generated examples with increasing degrees of concepts  influence on the classifier s decision. Significance and experiments: 1 Baselines method and evaluation setting are thorough, while it would be better to show ablation study and show the necessity of each component, especially the difference between EPE mod and DISSECT. 2 I am curious about the generalization to a novel dataset, how to define the meaning of each concept that influences the classification? How to rank the importance of the concepts for an explanation?<|endoftext|>In line with further work on counterfactual explanations, this work proposes to provide multiple counterfactual explanations in terms of VAE style disentangled latent variables. The idea of using VAEs to obtain multiple counterfactuals is novel and quite useful for multiple practical applications. Authors have evaluated their work on 3 image datasets and a number of useful evaluation metrics and baselines. The results are promising. This can be quite helpful in practise.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper investigates different variants of behavior cloning. The methods investigated aim at achieving better policies for use in offline RL through supervised learning than the average behavior contained in the data. Strengths\The paper provides a broad overview and makes a categorization. Weaknesses\The finding that regularization of the learned policy is particularly significant suffers somewhat from the lack of a recipe to perform this regularization offline. The text " RvS methods can often attain results that are comparable favorably to the best prior methods," should be corrected. Please check the bibliography for accidental lower case letters, like „rl“, „q learning“The paper gives a good overview of different variants of behavior cloning. Therefore, I think it is also possible that the benefit is small.<|endoftext|>This paper studies the importance of the design decisions for supervised learning type reinforcement learning algorithms. This paper provides empirical studies of offline RL via supervised learning. ## StrengthensThe biggest findings are two aspects:* The authors find more complex design choices, such as the large sequence models and value based weighting schemes used in some prior works, are generally not necessary, which is important in my opinion; especially, obtaining good performance using simple feedforward neural network is interesting;* The empirical results show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, e.g.when goal or reward is carefully chosen. 2.Other than the goal and reward aspects, what are the other aspects that matter? Therefore, I prefer to accept at the moment.<|endoftext|>The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results. Given the above observation, it is difficult to believe the results reported in the paper.<|endoftext|>This paper considers design choices involved in offline RL approaches that consider a reduction to weighted/conditional behavior cloning, a class of approaches referred to as Reinforcement Learning via Supervised Learning (RvS). The paper studies various issues including expressivity of policy architecture, regularization, choice of conditioning variables (e.g.based on goals or rewards). 1.While the paper presents interesting insights, I am not sure how much these findings can be generalized particularly in offline RL for locomotion tasks, where, typical benchmarks consider all 4 types of data collection strategies (random, medium, medium replay, medium expert)   this paper considers only the latter two variants for their analysis. The other two variants are highly challenging in the offline RL context and require leveraging on elements of pessimism/conservatism to guide policy learning. 2.The comparison of this paper with the Decision Transformer is not well accounted for.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper presents a multi sense word embedding model for polysemy words by distilling knowledge from the contextual word embedding model (BERT). For example, why are two individual embeddings d_i and v_i used in sense representation and disambiguation, respectively? Without comparison and analysis of these critical works, it is not easy to verify the technical contribution of this paper.<|endoftext|>This paper has the following contributions: a two stage method for knowledge distillation from BERT to static word sense embeddings, a sense embeddings model and application of such embedding models to the topic modelling task. Another concern is about the corpora the models are trained on. The models introduced in this work demonstrate state of the art results.<|endoftext|>2020.The technique adopted in this paper is simple and not novel. One of the main weaknesses is that the results are not significant enough. 2.Another main weakness is the novelty of the technique of this paper.<|endoftext|>In my opinion, from technological point of view, their idea is very novel. Surveys on the area are quite decent in the related work section. Thus, I found this paper is marginally significant.<|endoftext|>This suggests that the model is able to both disambiguate and also learn strong underlying representations. A set of nearest neighbor results shown in sec4.2 are used to suggest that the sense disambiguation network largely selects a single word sense. The output multi sense embedding is the probability weighted sum of the individual sense embeddings. Suggestions & Nit:Would like to see a Related Work discussion of contextual word sense induction models (especially those using BERT style transformers).
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; UPDATE:I acknowledge that I ve read the author responses as well as the other reviews. However, I m still not convinced about the novelty of the paper. I hope the authors can clarify further issues about the point. The paper studies ensemble methods to achieve improved in distribution (ID) and out of distribution accuracy. The proposed method seems to improve the ID OOD accuracy tradeoff. Strengths1) A simple algorithm that works in a wide range of datasets. I think the authors should provide more explanations why the newly introduced temperature parameter, which is tuned on the validation dataset, should show improved OOD accuracy.<|endoftext|>This paper discusses the trade off between in distribution and out of distribution accuracy, where ERM based learners have good ID but poor OOD performance, and robust learners the opposite. This is an interesting paper that discusses an important and timely topic, the trade off between in distribution and out of distribution performance. The paper empirically analyzes this trade off and shows that standard ERM based learners perform better ID at the expense of OOD, while robust learners are the opposite. The proposed solution relies on resembling both types of models. While good performance with a simple method is always a good thing, it would be beneficial to try and further improve performance with a more advanced re calibration approach or an end to end calibrated model. The same critique also stands for the ensemble approaches, which at the moment only consider a combination of two models. Lastly, the paper is quite thin in analysis and discussion. If deleted, the authors should use the 1+ pages remaining space to add a detailed analysis of when and why this approach works in reality.<|endoftext|>The submission considers a very topical issue given the active ongoing interest in OOD performance — models developed with OOD robustness in mind can come with a significant loss of in distribution performance, and since we expect to mostly encounter in distribution settings in deployment (assuming deployment is being done responsibly), we need to find ways to trade off performances for in distribution and unexpected situations. The submission illustrates that a fairly simple approach of Platt scaling done in distribution for a “standard” and a “robust” model followed by averaging their predictive distributions can sometimes work pretty well: significant performance improvements appear to be achieved both in  as well as out of distribution. The strengths of the submission are that  — the topic is highly relevant and timely — the range of datasets is broad, and the empirical results are fairly promisingSome points that could strengthen the submission are as follows: — It would be interesting to investigate qualitatively the OOD cases where the (calibrated) standard model has a higher confidence, as well as a quantitative analysis of how often such events occur. — While I believe the range of experiments (datasets + models) is sufficiently broad, methods aimed at distributional robustness such as groupDRO [1] or domain adversarial learning [2] (to name some) might be interesting to study under the proposed method (on whatever datasets they’ve been showcased to have been successful upon).<|endoftext|>However, their relative ID and OOD performances are not sufficiently explored. There is room to include this as the paper is currently just over seven pages, and it would improve the paper. There are also some baselines that are discussed but absent, e.g., in Section 3 the authors mention that they found combining the softmax probabilities to work better than combining the logits. Calibration could then be interpreted as a way to choose this mixing coefficient, which would be very interesting. One drawback of this method is that it is more expensive than a single model, but this experimental detail is absent. This is substantially lower than what is reported in [1,2] for the same model type (ViT B/16). However, the paper would substantially benefit from a more complete exploration of baselines and thorough explanation of experimental details. For instance, even some of the baselines mentioned are partially included or absent, and accuracy of ImageNet R is substantially lower than usually reported.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper proposed an interesting grammar learning based data efficient molecule generation model. The key idea of this method is to learn a set of production rules via a hyper edge selection process that optimizes a set of evaluation metrics. Strength:This method is very data efficient given grammar based learning. Weakness:It seems like the model can only work on a small set of molecules to generate the rules due to the computational complexity.<|endoftext|>The paper presents a data efficient graph grammar based approach to molecular generation (with a focus on polymer generation). The model requires that the molecule be represented as a hyper graph. This, in my opinion, is a plus. Nonetheless, I have a few points I would like the authors to address before I give complete acceptance. 1) It is unclear to me what is the computational cost of constructing the grammar and training the model since generating the production rules involves graph matching (and although you affirm this isn t an issue). 4) This is more of a request for clarification. If I understood correctly, the bottom up construction process generates a lot of near duplicates, as well as potentially useless production rules, especially during the initial iterations.<|endoftext|>This paper proposes a sample efficient hypergraph grammar learning algorithm for generating molecules. The bottom up search algorithm conducts iterative contraction according to a learned policy, where in each iteration, several hyperedges are sampled, contracted into a single node, and written into a production rule as part of the final grammar. Overall, this paper makes decent contributions in both technical and empirical aspects, and therefore I recommend accepting the paper provided that the authors correct the writing issues. The method is general and can have a large impact on the graph learning community. Also, the idea of learning a grammar search algorithm using RL instead of directly learning the grammar is novel and interesting in the domain. The proposed method is well supported by experiments on datasets of different scales. How do you decide these parameters? Do you do any pruning for the production rules? How to jointly learn the generation policy together with the rule search policy?<|endoftext|>This paper introduces a new grammar based generative model for the molecule generation task. The graph grammar is learned by iteratively contracting hyperedges (i.e.edges that can connect multiple nodes, defined by simple chemistry inspired rules) into non terminal nodes and the submission proposes to learn how to sample the hyperedges by a REINFORCE algorithm that optimizes for several molecule generation metrics. Notably, the proposed method achieves strong performance while being very data efficient. The proposed method is novel and opens potential for new line of research in graph grammar. However, I have few more comments that I hope to help improve the paper. My first request is for the authors to report the actual computation time for the proposed method on the experiments they have done. the paper is clear and well written.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper proposes a new method for evaluating the disentanglement of object centric representations. **Weaknesses**: The clarity of writing, framing of the claims, and experimental evaluation are weak. In *International Conference on Learning Representations*. See points below. **Clarity**: While Fig.2 and Alg 1. nicely convey the main contributions, the clarity of writing could be improved as the authors tend to: i) dump in results and expect the reader to analyse them themselves (rather than taking the reader through the results with good explanations); and ii) often make statements without direct pointers to evidence to back up the statements. (2018).A framework for the quantitative evaluation of disentangled representations. Or to their metric better evaluating unstable methods?<|endoftext|>This paper introduces a new metric for evaluating disentanglement of latent representations, which   unlike prior work   supports object centric, structured latent spaces with a set of latent variables (as opposed to a single global latent variable). It is possible that I misunderstand the argument by the authors, and I would appreciate it if this could be clarified. This paper is well written and of overall high quality. Overall, this paper presents a significant advance in evaluation of object centric generative models and provides a sound new metric for this task.<|endoftext|>The proposed metric is based on the projections of the affinity matrix proposed earlier in the DCI metric. This allows for slots to be permuted wlog. Empirical studies are performed to study the disentanglement of existing compositional representation learners. This paper addresses the problem of evaluating models that learn compositional representations. + Permutation invariant representation probing is a necessary contribution that extends the mapping between latents and generative factors to the object centric case. In the experimental results, the way we find the dimensions to project on is not clear. Minor Comments:Theorem 1 in main paper is referred to as Theorem 3 in AppendixThis paper addresses a timely problem and the proposed evaluation metric and representation probing methods are sound. However, this paper needs to explain some points more clearly to better understand the work.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; When the data is scarce for a task, existing offline RL methods trained on limited offline data could perform poorly. The proposed method augments the reward function by penalizing the state action next state transition tuples that are rare in the source environment. This reward augmentation strategy directly accounts for the dynamics shift between the source and the target domain. * Figure 3 clearly shows that the reward penalty obtained through domain classifiers can aid the knowledge transfer when the dynamics shift. Then I will get a wrong estimate of the environment dynamics. It would be nice for authors to compute the average improvement for all six algorithms over Hoper and Walker2d and see if there s any consistent trend. This well written paper studies the important problem of transferring knowledge to improve sample efficiency for offline RL.<|endoftext|>They propose modifying reward in source offline dataset, penalizing it with the dynamics aware reward transition, setting the lower bound. Weakness:* The motivation of the proposed approach needs to be expanded further. "Motivated by the (approximate) equivalence1 between the dynamics change and the rewards mod  ification (Kumar et al., 2020b; Eysenbach & Levine, 2019; Eysenbach et al., 2021), we propose to modify rewards in the source offline data to compensate for the dynamics shift. " As one example, the "Related Work"(Section 5) is towards the end of the paper, after the approach has been extensively described. ": This line isn t very clear to me. The results seem promising. I m willing to reconsider my decision, based on the feedback addressed by the authors. Edit: Updated score based on reviewer feedback<|endoftext|>It proposes DARA, a framework that relaxes the requirement of data (both the amount of data and the reward information contained in the data) needed from the target task. By capturing the dynamics shift between the source and target environments, DARA modifies the reward gained in the source environment to discourage transitions that have a smaller probability to happen in the target environment. The paper shows several strengths:(+) The method proposed in the paper contributes to improving the data efficiency in the offline reinforcement learning setting. (+) The theory behind the idea is well explained. (+) The implementation of the method is also discussed, making the method applicable. Therefore it could be better if more random seeds can be tested. ( ) It seems like the proposed method is limited to the case that the target environment is restricting transitions that exist in the source task. I recommend acceptance.<|endoftext|>The authors propose their method, DARA, which applies a reward penalty that correct the dynamics shift between the source and target domains. Pros:1.The paper is clearly written and easy to understand. 2.DARA appears to be the first method that addresses the problem of domain adaptation in the offline RL setting. It seems to be a bit straightforward and therefore the novelty is a bit limited. It is also important to see the error bars of the results. It would be really intriguing if the authors could show good transfer results with image inputs in the sim to real setting. I think the paper proposes a new algorithm that tackles a somewhat new problem, but the methodology lacks originality and the hyperparameter details are missing. Therefore, I would vote for a weak reject.
Reject; rating score: 10; rating score: 3; rating score: 5; rating score: 6; Having said that, the although am working in contrastive learning and self supervised learning, time series data augmentation is not my cup of tea. So the contribution of this paper is a new data augmentation approach based on information theory, a meta learning approach and an approach to select optimal data augmentation for contrastive learning. The paper proposes a new data augmentation approach based on information theory (mutual information) and meta learning to identify optimal data augmentation approach.<|endoftext|>This paper describes an information aware approach to representation learning for time series. Detailed comments are listed below. 2.About high variety: The technical correctness of this part needs to be justified. 3.About the meta contrastive learning in Section 2.3: While the local wise contrastive loss in (7) follows Tonekaboni et al.(2021), the positive neighborhood and the negative neighborhood should be described in more detail. The authors are expected to justify the connection of the proposed approach with meta learning. This work seems to be not ready for publication yet.<|endoftext|>This paper proposes InfoTS, a method for learning augmentations that improve contrastive learning of time series data. While one can argue that hyperparameter tuning is an instance of meta learning, I felt that this naming is misleading in this context. Minor comments  Minor grammatical inaccuracies and typos throughout the paper. e.g., "the information theory," "important weight," "combing candidate," to name a few. page 3, "High Fidelity" paragraph: The information bottleneck paper of Tishby et al.is not a suitable reference for the general phrase "information theory." L_y 24+w/o Local+MAE, L_y 48+w/o Variety+(MSE, MAE) should be bolded.<|endoftext|>The paper is well written. Or are batch and mini batch two different entities in this work? This addition to the model feels a bit strange, as it is also not further mentioned in the rest of the paper. For example, which of the proposed benchmarks is actually also a contrastive learning model, and which of the benchmarks is supervised? The authors propose an interesting solution to the challenge of seeking data augmentations on time series data. Also, some of the (ablation) analyses can be made more complete. Same remark for p8, where the authors argue that InfoTS can adaptively select the most suitable augmentations. At this point of reading, there is no proof of that yet.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; This paper focuses on the learnable neural representation of the 3D human pose. This paper focuses on a very interesting topic and proposes two datasets that may facilitate the research area. (3) The dataset is one of the most important contributions in this paper.<|endoftext|>This paper develops a system to generate a complete and realistic human pose based on a sparse set of inputs such as look at direction and end effector positions. “Diverse Trajectory Forecasting with Determinantal Point Processes.” ICLR 2019Overall, the proposed system is a practical and usable system for pose authoring based on sparse input.<|endoftext|>The paper proposes a neural network architecture with an encoder to extract pose encodings from partial user inputs and a decoder to generate complete poses from the encoded features. This paper is interesting and technically sound in general.
Reject; rating score: 3; rating score: 5; rating score: 5; The teacher is a reinforcement learning agent that observes the status of FL clients to inform which data mini batches the clients should use to train their local model. How can we confirm the effectiveness of the scheduler? Experimental results using multiple hospital datasets as well as standard image datasets (MNIST and CIFAR 10) show the effectiveness of the proposed approach over some existing methods. ### Strengths  FL has been one of the most important topics in recent years. Finally, it is currently hard to judge the validity of robustness experiments due to the lack of their setup detail. Making FL frameworks robust against backdoor attacks is particularly an important direction. #### (2) SchedulerWhile I think it interesting to learn a scheduler to select nodes, it is not clear how the scheduler is actually used to do so. #### (3) ExperimentsCurrently, many technical details are missing in both the main paper and supplementary material. The number of clients and data non iidness are both critical factors for federated learning.<|endoftext|>The setup and the details of the whole framework are well presented in the paper. Pros:1.It is worthwhile to study the scenario of training models across different hospitals based on federated learning so that the privacy of patients is well protected. 2.Sufficient experiments are conducted to illustrate the performance of the framework as well as the effectiveness of each component. 2.The authors need to point out the original novelty of this paper apart from the existing conclusions they used in the paper. 3.The experimental details are not well presented in Sections 4 and 5. It is better for the authors to give a further explanation on this.<|endoftext|>In this work, the authors propose a new federated learning algorithm by adopting a neural scheduling technique. This work applies federated learning with a neural scheduler to improve the effectiveness and robustness of the global model. Adopt deep reinforcement learning (DRL) as a neural scheduler is not new. 2.The experiments are not sufficient to demonstrate the efficacy of the proposed algorithm. 3.The experients demonstrate the proposed algorithm is robust to backdoor attacks. This phenomenon is interesting. However, the reason why this approach works is still unknown.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper highlights the fact that when doing unstructured pruning with extreme sparsity targets, we end up pruning entire neurons, which effectively detaches neurons from the input/output. The authors argue that the effective sparsity levels in these cases is higher than our target levels, and taking it into accounts exhibits differences between various pruning method that are otherwise not obvious. While indirect pruning is a reasonably known phenomenon, it is an interesting question to see whether it can give us any insights when comparing various pruning methods. I think that s a strong claim that s not well supported in the paper. In the introduction, the authors do say that typical compression rates are 10x (0.9 pruning) and 100x (0.99 pruning) and motivate extreme pruning because of models with billions of parameters, yet their biggest models are ResNet 18 and VGG 19, where indirect pruning does not seem to be as big of a problem anyway. I think random is misleading here given that you re still applying layer wise pruning ratios. This paper draws too many conclusions based on very small networks, datasets, and for a pruning regime where the network is simply trainable and has nowhere near the performance of the unpruned model. However, the performance in this extreme regime is very poor, and the architectures are simply "trainable". So why is this an interesting problem to look at? The authors do not discuss what happens to the disconnected neurons as a result of indirect pruning. or perhaps into the batch norm parameters? What is the justification that neural networks behave like gasses?<|endoftext|>The paper points out that in pruning, there is often a difference between the number of zero parameters and the number effective zero parameters due to disconnection during pruning. Strengths:* The paper is clear and presents an interesting analysis about the discrepancy between pruning models and the resulting, underlying graph. * The detail and analysis is thorough, and the methodology is clear and concise. * The presented layer wise sparsity quota Ideal Gas Quotas (IGQ) are interesting and produce compression distributions very similar to other methods (such as in Fig.9).Weaknesses:* Many important points/figures in the paper are on very small datasets where models are extremely over paramaterised. Comments:* Would it be possible to adjust for effective FLOPs in a correctly pruned graph? * I d be interested in a bit of a comparison of different model architectures  for example, sparsity in convolutional filters will have a different impact in the effective sparsity metric than sparsity in MLPs (if I understand correctly). The paper is well written, very clear, and well presented. The authors raise and clearly document the discrepancy between raw sparsity and the actual effective sparsity of a model. However, the results are shown on small datasets without comparison with many modern architectures. Replacing the MNIST/LeNet/VGG results with more modern architectures would clearly increase the impact of the paper.<|endoftext|>The paper highlights the difference between the explicit and effective (through disconnection) number of zeroed parameters in a model   and argues it is the effective parameters that should be used as the benchmark for comparison. Further comparing pruning methods and re affirming their performance relative to random pruning equivalents. Overview:The issue of effectively disconnected parameters [Tenaka et. While disconnections are known, and to this reviewer at least. Beyond the methodological importance of such explicit clarification, the practical relevance and expectation of their impact could use further discussion and motivation. The issue of proper methodology in the context of counting of parameters is important and by extension so is accounting for effectively defunct parameters due to disconnection. 2.The paper is well written and clear. This discrepancy (between effective and direct parameter count) is known (as the authors mention [e.g.Tenaka et al]) and thus the major contribution of this paper is of limited novelty. 2.While the actual methodological claim (one should account for disconnections via effective counting) is very much valid and orthogonal to experimental settings, the practical discrepancy (and thus implications) between direct/effective is very much dependent on the settings:2.a. As such, for practical architectures   the level of actual discrepancy in practice is not demonstrated to be (or as expected to be) meaningful. 2.c.The paper demonstrated the results for small datasets (and weak networks) (mnist/lenet , VGG/cifar10 100, Resnet18/TinyImagenet). The paper can further engage with practical implications. I agree that counting effective rather than direct parameters is methogologically good practice. However, the limited scale and demonstration  limits the insight applicability in practice   especially, as SOTA methods and architectures have structural characteristics which call into question the the level of discrepancy (missing discussion/dimensions of exploration).<|endoftext|>Overall, the paper is well written and has well designed experiments. Thus, the paper was a pleasure to read, providing useful insights in understanding sparse neural networks and pruning techniques. I suggest to add an extra paragraph which clearly discuss the related work with respect to the “effective sparsity” concept. The experiments are performed just on dense to sparse training methods. This incorporation can be indeed let for future work, but just studying effective sparsity in sparse training should not be so difficult and the results can be of interest (e.g., same as in Figure 3). Nothing to change in the experimental section, but I suggest clarifying (discussing better) this aspect. A discussion has been started on this topic in the last paragraph of page 2, but I would expect the Discussion section to come back to this topic and perhaps some empirical validation would be necessarily to support better the statement “…but find the truth to be more nuanced at higher compression rates…”. I wouldn’t expect a detailed study as it is outside of the scope of this paper, but rather some hints in order to avoid cutting from start future works on this topic.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper introduces WalkPool, a model for link prediction that relies on GNNs. The main idea is to jointly encode node representations and graph topology information into node features which could be learned in an end to end manner. The proposed methodology is presented clearly. The execution of the proposed methodology is also interesting.<|endoftext|>The paper proposes a new link prediction method called WP, based on treating link prediction as a subgraph classification problem, where the link of interest is surrounded by the subgraph. Essentially, the proposed WP method is a feature extractor for links (or its surrounding subgraph). 2.The extract features somehow encodes path information which may be useful for link prediction. 3.Comprehensive experiments are conducted. The paper proposes a pooling method (i.e., a latent structure feature extractor) for learning features that describe links (i.e., the surrounding subgraphs of the links). The framework remains end to end learnable. Also, I think the word "topology" does not well describe the proposed idea.<|endoftext|>This paper proposes a neural architecture for predicting missing links in a graph by designing a so called WalkPool scheme. Strengths: The paper is well written; the introduction of the previous work is organized. Overall the paper is nicely written and organized, and provides useful thought in the problem of link prediction with or without node features. Experimental results are also promising. Can authors add more details of the pooling used in the SEAL and why WALKPOOL is considered a pooling scheme?<|endoftext|>The paper provides a method for link prediction based on random walk based pooling method (WalkPool). Finally, it shows superiority of the method against baselines via AUC and AP. Loss function: At the end, LP is a classification task. Did the authors try them? AUC: I suspect that for most of the datasets, the difference of WALKPOOL with baselines is not statistically significant. The results are weak too.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors propose an inductive graph partitioning (IGP) framework across multiple snapshots of a dynamic graph to produce an effective partitioning of the graph. The embeddings are then used in the assignment of the node disjoint cluster labels. Finally, there is literature   both in terms of discrete algorithms and learning representations   that addressed the problem of incremental partitioning of an evolving graph. It s not clear how this can be achieved. In this context, comparison to an algorithm with known approximation factors would help. I feel the paper in its current form can be significantly strengthened in terms of the presentation of the empirical results by clearly explaining the trade off achieved as well comparing with more algorithms from the literature as baselines.<|endoftext|>The authors consider the problem of inductive graph partitioning, which they formulate as clustering or partitioning multiple snapshots of a time evolving graph for which we have no node correspondence. It took me several minutes of staring at the figure, zooming in and out, and examining multiple legends to understand what is being shown. Methods that use node correspondence (e.g.incremental and evolutionary clustering) could possibly do better on these datasets.~~  No motivating application for the no node correspondence setting. *After discussion period:* I have added a strike through the incorrect portions of my review that have been clarified by the authors. I also have concerns regarding the problem setting and evaluation, so I do not view this paper as ready for publication at this time.<|endoftext|>This paper considers the problem of solving the graph partitioning problem repeatedly over many different graphs. Graphs are sampled i.i.d.from an unknown but fixed distribution and the goal of the algorithm is to solve the GP problem on each of the graphs. The strengths of this paper are as follows. + The problem consider is important and practical. The reasons why their method is better, that the authors state, seem somewhat shallow/arbitrary. Adding intuition about some of the choices would make it much better. The paper does not report CIs in the experiment. Thus, I am giving a weak reject.<|endoftext|>In this work the authors focus on an important problem, building an inductive framework for graph partitioning. Is this what you mean by saying that the graphs are snapshots of a given complex system? For the synthetic experiments using the stochastic blockmodel, it would have been nice to see a comparison with the paper  "Supervised community detection with line graph neural networks" by Chen et al.that provides state of the art results.<|endoftext|>The authors propose an inductive graph partitioning framework across multiple evolving graph snapshots to alleviate the NP hard challenge. Strong points:Overall, the paper is well written and the experimental part seems comprehensive. It seems not good to add discussion on one concrete example in Figure 2 in a statement of a theorem. I would suggest that please move that outside and discuss the potential meanings and implications of theorem 1 on examples separately. Here are a few typos.<|endoftext|>**Strengths**Both the inductive graph partitioning problem which the paper attempts to solve and the dual GNN adversarial approach are interesting. The paper does not provide a theoretical guarantee on the generalization to new unseen graphs. It would be nice if a guarantee on modularity or NCut is provided. 2.The proposed framework does not work with attributed graphs. 3.The authors assume that the set of graphs, which includes both offline training and online testing graphs, follow similar distributions. This assumption easily breaks in practice for graphs that come from different domains.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper studies the backdoor attack problem and proposes a minimax formulation for the defense against adversaries. Theoretically, the paper analyzes the convergence bound and the generalization bound for the proposed method. 1.In most cases, the proposed I BAU is not the state of the art method. 3.In page 2, it is assumed that the norm of the backdoor pattern delta is bounded by C_delta. 4.There is no empirical validation and justification of Theorem 1 3. For instance, in Theorem 3, the generalization bound can be measured by certain norms of weight matrices, the number of samples, Lipschitz constant, and so on. Hence, after a model is trained and fixed, it is possible to empirically justify if the theorems are true. It is also interesting to empirically investigate the bounds with regard to different weights (i.e.network architectures), number of samples, etc.<|endoftext|>The paper formulates adversarial training against backdoor poison attacks and proposes to use implicit hypergradient to solve the minimax problem instead of breaking it down into separate inner and outer optimization problems. The authors perform theoretical analysis of the algorithm and find convergence bound and generalization bound for the method. Also, the authors evaluate the proposed adversarial training routine, called I BAU (Implicit Backdoor Adversarial Unlearning) in three attack settings:  (1) One trigger one target attack, (2) One trigger all to all attack, (3) Multi trigger multi target attack and compare with six defenses. For each attack setting, the authors test 7 different backdoor attacks. The results show that the efficiency of I BAU is comparable to the best baseline for each of the attacks and is more time efficient than other defenses. Finally, the authors show that I BAU leads to a more stable training comparing to using universal adversarial perturbations in adversarial learning. Also, I BAU is not compared to the algorithm in [1]. 2.The paper s clarity could be improved, in particular grammar. I am confused with how the proposed approach should remove the backdoor triggers.<|endoftext|>This study investigates defense against backdoor attacks for models that have already been trained. It proposes, in particular, a min max formulation for backdoor defense, in which the inner maximum seeks a powerful trigger that leads to a high loss, while the outer minimum seeks to suppress the "adversarial loss", so as to unlearn the injected backdoor behaviors. To solve the minimax, the authors also propose a method, Implicit Backdoor Adversarial Unlearning (I BAU). In addition, the authors also provide theoretical analysis including the convergence bound and generalization bound. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method. The proposed algorithm seems natural. This is not very intuitive.<|endoftext|>In this paper, the authors proposed a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. Unlike previous work, which breaks down the minimax into separate inner and outer problems, the proposed algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. Extensive experiments showed improved backdoor defense performances and less computation time on several backdoor attacks over various attack settings. 1 .The min max formulation of backdoor removal is quite intuitive, however, the perturbation constraint is a bit hard to quantify. Maybe extending the framework to also Linf norm would be beneficial. I wonder what is the uniqueness of this result presented in the paper? 4.The trigger removal experiments in Table 1 showed that the proposed algorithm is indeed more comprehensive, however, it does not achieve the best performances on many test cases, especially in terms of ASR. 5.In Table 5, where the authors showed the relationship between performances and the number of clean samples. This is actually quite counterintuitive. However, there still exists some concerns towards the applicability of the proposed method as well as the uniqueness of the theoretical result. I think it is on the borderline.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; The proposed algorithm is evaluated on tasks that requires a single manipulation motion or a sequence of motions to shape a deformable object into desired shapes.<|endoftext|>The use of optimal transport priority as a heuristic for contact areas is reasonable and intuitive. Compared to previous papers like DiffTaichi and PlasticineLab, this method can better handle multistage tasks. Some concerns that I have,1.<|endoftext|>However, as the primary aim of this paper is to design a gradient based solution to multi step contact rich manipulation tasks, I believe there are other gradient based variants that can be evaluated in this setup.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; The paper is well written and the authors supported the claims made with theoretical and empirical analysis. The P Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List. The observations are supported both theoretically and empirically.<|endoftext|>This paper proposes to improve the negative sampling process for training pairwise ranking models in personalized recommender systems. The “debiased” term in the paper title is confusing. The novelty of this paper is limited.<|endoftext|>The method it proposed is intuitive and clear. this paper proposed a method on negative class sampling by considering the degree of vertexes. It proposed a rejection function in order to sample with higher probability from high degree vertex instead of using edge level random sampling.<|endoftext|>2.The theoretical analysis provided by the authors can well support the claims. 2.The presentation can be further improved with more discussion of the main content. Could you please give a justification about how to deal with the false negative issue? Sufficient experiments are provided. Overall, it s an interesting, solid and well organized paper studying an important problem.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; They showed that the fair learning algorithms can be made robust against such adversarial data poisoning attacks. What about their fairness violations? In particular, the authors ignore (and/or misunderstand) the prior works on "robust fair learning" by writing "Although several fair learning algorithms have been proposed to achieve different robustness properties (Wang et al., 2020; Rezaei et al., 2020; Taskesen et al., 2020; Roh et al., 2020; Cotter et al., 2019), those algorithms do not consider the adversarial bias and fail to be robust against adversarial bias." The authors may want to report how good this approximation is.<|endoftext|>The main message of the paper is that robustness increases the cost of fairness. The strength of the paper is the particular problem considered. Although the problem of designing fair and robust classifier is interesting, this problem now has been looked into by several papers. For example, the fact that adversarial training increases accuracy disparity has been observed by [4]. 2.In the experimental section, the choice of fair classifiers is not exhaustive. Furthermore, the choice of datasets is not exhaustive either. The authors have not provided any justification for the proposed approach. I found several missing references that are quite relevant to the current work.<|endoftext|>This paper explores the robustness against certain adversarial train data poisoning (sampling or label flipping) for some prior fair machine learning algorithms, under equalized odds. In addition, the evaluation is based solely upon experiments, although some intuitive remarks are provided along with the experimental results. Actually, if that is the case, I am wondering if the raised issue is really a challenge. The claims are based mainly on experiments even under some particular algorithms and models. So it is hard to make a scientific judge as to whether the claims are really the case.<|endoftext|>From the results, authors show that fair models are more likely to have lower accuracy in the adversarial setting where fairness is targeted in the training data compared to regular models. I think these are interesting questions that this type of work can answer by expanding on some experiments and discussion. 3.Lots of experimental results which are interesting.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This proposed approach can also be instantiated using any univariate feature based explanation method. Empirical results on image, text, and tabular data show the ability of the proposed method to accurately identify mutual and directional redundancies. Experiments are performed on 7 datasets with image, text, and tabular data using deep learning and tree based models; the paper also compares the proposed approach to an appropriate set of relevant baselines. Also, why are the univariate approaches methods designed specifically to identify the most influential features not performing just as well as the bivariate approaches on this task? Minor WeaknessesWhy is BivShap K not included in Figure 2? The proposed approach is better able to identify mutual and directional redundancies compared to existing methods on a wide range of datasets.<|endoftext|>In this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method.<|endoftext|>They then propose four different types of interactions based on this definition: least/most influential features, directional/mutual redundancy. The experimental results indicate that the proposed method can efficiently discover these four types of feature interactions. ## Strength:    The paper is well organized. Despite the fact that the Shapley value has been applied to a wide variety of AI applications, there is precisely no method to explore how the features affect each other non symmetrically. In figure 2 and table 2, the authors only show BivShap S and not the BivShap K which is used in all the other experiments. How does the performance look like with BivShap K.  The bivariate Shapley explanation in  Eq (6) has a conditional interpretation as the authors describe, specifically, $E^2(u)_{ij}$ represents the importance of feature j conditioned on feature i being present.<|endoftext|>The proposed method is a graph based explainer and the data can be considered as graphs. Then it studies the Bivariate Shapley values to consider the directional feature interactions. Experiments on several datasets show very promising results. + Experimental results are very promising and the analysis is very useful. I believe this can be done with simple modifications.
Reject; rating score: 3; rating score: 5; rating score: 6; The writing is very poor and makes the technique difficult to understand. In the federated learning setting considered here, the current paper tries to balance the mutual information between the user updated parameters and the global data ("optimality") and the user updated parameters and the meta parameters ("adaptability"). The proposed approach is similar to MAML in that the meta parameters are a globally shared initialization for gradient descent, and the user updated parameters are the result of gradient descent from the shared initialization (i.e.the meta parameters).<|endoftext|>In order to achieve the appropriate trade off between global optimization and local adaptability, the authors utilize the recent work [1] to add a meta learning regularization term for better personalization effect. The paper borrows the techniques from [1] to solve personalized federated learning (PFL) tasks. In addition, the writing is not in good shape.<|endoftext|>This paper is a Federated Learning (FL) extension version of "Meta Learning without Memorization" (Yin et al 2020). I need to cross read this paper and Yin et al multiple times to understand the differentiating factor. It is an interesting work interplaying between meta learning and FL.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper improves a model based reinforcement learning method by Kaiser et al.(2020) by introducing three different types of neural network layers to the transition and reward model. These modifications are motivated by intuitions about what would be beneficial to model for object based environments (in Atari games). There are experiments on 12 games in which the proposed method performs the best most of the time. Ablations show that using any one of the three proposed layers is also beneficial. This paper proposes a straightforward and easy to implement modification to the SimPLe algorithm (Kaiser et al., 2020)   modify three layers in the transition and reward model they are using. This simplicity is good, but it requires much more rigorous experimentation since users of such simple modifications would want to know why and when these modifications work or don’t work. My main concern is that there are only 3 independent runs from which it is difficult to judge whether EVaDE is really better than SimPLe. A simple modification to an existing architecture is nice, but it’s unclear from the experiments that this particular modification is really helping, due to the small amount of seeds.<|endoftext|>This paper proposed some new ideas on neural network architecture construction built on the algorithm of PSRL. But overall I feel this paper does not tackle the key problem of reinforcement learning. As a research paper, I feel it should provide more principle algorithm design ideas rather than adaptation on one or two specific domains. Why posterior sampling is done by multiplyingeach parameter of these EVaDE layers by a perturbation drawn from a Gaussian distribution? I respectively disagree on "an interesting aspect of designing for exploration is that the variational distributions can be helpfuleven if they are not designed to approximate the posterior well, as long as they assist in perturbing the policy out of local optimums." "it is easier to incorporate inductive biases derived from the domain knowledge of the task for learning the model, as the biases can be directly built into the transition and reward functions." I do not quite understand how you can do this and how it is supported. How about value based methods? I hope the authors could use accurate and well supported arguments. "object based" and "event" should be clearly defined in the beginning. How do you interact with the environment and how do you do the planning? Overall, I feel it lacks principle explanation and is quite specific to a small set of domains.<|endoftext|>My grade is explained by the fact that I am not very familiar with the domain of vision and games, hence I might be unaware of important related works. This paper proposes an approach for Model based reinforcement learning relying on Posterior Sampling for Reinforcement learning. Posterior sampling for reinforcement learning uses Thompson sampling to balance exploitation and exploration. The system model presented in the paper specifically applies to object based tasks. Only three event convolutional layers use dropout: one for object interaction, one for event weighting and one for event translation. These layers can be inserted into any existing neural network model to provide an approximate posterior. The authors combine this model with a PPO agent trained on the model to show its empirical performance on Atari games with a limited number of interactions with the real environment. the paper is well written and easy to read. Even though sometimes only one of the layers leads to better performance, using the 3 layers improves the performance of Simple(30) on most of the games. However I like that it incorporates domain knowledge. Are the authors aware of any other work applying dropout to perform Thompson sampling in MBRL? Isn t there a risk of the policy overfitting to this specific model? why is "z" changing?<|endoftext|>The paper proposes a method to equip model based deep reinforcement learning with posterior sampling for exploration where posterior sampling is approximated using a variational distribution approach. This paper applies the posterior sampling idea to the model based deep RL setting. These dropout layers induced variational distributions are used to approximate the posterior distribution which is then used in posterior sampling for exploration. The proposed dropout layers for variational distributions are appealing, and the ideas to use three types of layers to capture three kinds of effects is very interesting. But little is discussed in the paper on why to consider these three effects. In the experiments, the paper equips the EVaDE layers to an existing model based RL method SimPLe and compare EVaDE SimPLe with four baselines in 12 Atari games. EVaDE SimPLe achieves good performance compared with the baselines, but It would be better if the paper provides more discussions on why the proposed method performs better in some games but not in some others. Can EVaDE SimPLe achieve the same performance if trained longer, or the noisy layers prevent the agent to achieve the best performance when exploration is less important? Cons* Not enough explanations and discussions for the three proposed layers.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper proposes a method for few shot graph link prediction by using a domain discriminator. The authors also introduce the concept of “shot” in the graph. This paper is not well motivated and needs further justification.<|endoftext|>This paper proposed adversarial training for few shot link prediction problems. The authors introduce a domain discriminator on pairsof graph level embedding and address the issue of few shot learning in graph data. why the last domain has to be 1 shot? There is a lot of prior work which is not discussed in the paper.<|endoftext|>The experimental results support the effectiveness of the proposed method. The paper’s experimental settings might not be close to a real world scenario. How likely is this a real world scenario? This simple scenario makes it hard to justify the universal applicability of the proposed method. The paper should clearly claim its novelty and run more experiments that are closer to real world scenarios. I am going with a rating to marginally reject the paper, but willing to change the rating if authors can address the above concerns.<|endoftext|>This paper presents a novel approach to  link prediction methods in graph representation of imbalanced domains using adversarial training. The approach takes is similar to few shot learning that is popular in computer vision. Discussions on how to define shots for graphs, and how to design experiments for addressing imbalanced domains contribute to the novelty of the paper. Will have applications beyond the benchmark datasets used in the paper. Weaknesses: I like this paper and so do not see any obvious weaknesses.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper formulates a contextual bandit problem with causal side information, where the agent has the option to perform targeted interventions. Then, the paper proposes an algorithm for solving this problem and shows that its regret is bounded. I appreciate the addition of the paragraph "Intuition behind the Unc measure" in the new version, which addresses this concern.<|endoftext|>The paper studies a contextual bandit setting with two unique features: (1) the learning agent has ability to perform targeted interventions during the learning phase (ability to select target sub populations or context) and (2) it also has access to and integrates casual information in the setting. The key motivation is that this setting captures real world scenarios such as software product experimentation. The paper studies a new contextual bandit setting motivated from real world scenarios and proposes a new algorithm and presents theoretical guarantees on the same. 3.The paper also presents theoretical analysis of the proposed algorithm.<|endoftext|>This paper proposes a contextual bandit problem where an agent is capable of intervening on a targeted population (e.g., a user whose characteristic matches a specific context) instead of getting a context stochastically sampled from the underlying distribution over contexts. The paper has a certain interesting problem with an intuitive solution. Yet, the theoretical and empirical analysis seem a bit lacking. Hence, the authors may focus on clarifying the assumptions and analyze the method thoroughly comparing to other possible stronger baselines.<|endoftext|>This paper considers the problem of interactive decision making given some context variables from the environment, which is referred to as contextual bandit setting in the paper. The paper provides a complete analysis of a reasonable theoretical guarantee on regret. Does this claim hold when the proposed framework uses targeted intervention where the values of contextual variables are restricted? Overall I think it is an interesting problem to account for context variables in interactive decision making. The proposed method is intuitive and the authors provide theoretical and numerical evidence for its performance.
Reject; rating score: 3; rating score: 5; rating score: 6; The paper proposes a reduced rank and sparse approximation method to purify the graph structure for better robustness against adversarial graph attacks. Overall, the contributions of the paper are unclear and the novelty seems to be limited.<|endoftext|>This paper proposes GARNET, a spectral approach to depend adversarial attacks on graph structure. Yet this does not seem to be the case. All the 3 steps of GARNET achieve low computation complexity and thus can be applied to large graphs. Strengths+ The paper is clearly written.<|endoftext|>This paper proposes a spectral approach towards robust and scalable graph learning, namely GARNET. Strengths:  The paper is easy to follow with clear motivation and is well written. The acceleration of TSVD is remarkable and the scalability of the proposed method is clearly a strong contribution to the research on the robustness of GNNs. For experiments, the experimental setup should be more specific for better reproducibility since no code is available for examination. Several questions include: what package do the authors use to conduct truncated SVD?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 5; 2.I think the only reason people would first train a numerical L2O model and then distill it into a symbolic version is to apply this rule to other datasets. However, the paper lacks an experiment of the generalization of the learned symbolic L2O update rule. In their experimental results, they first demonstrate the capability of symbolic regression for some know functions and then they show the good distilling performance of their proposed symbolic regression method on empirical evaluation, where the learned symbolic L2O method is better than existing baseline optimization methods. However, I think the paper would be improved dramatically by showing the generalization performance of the symbolic L2O model as well as the performance difference between the numerical L2O model and the distilled symbolic one.<|endoftext|>This paper proposes using symbolic regression for making simpler, more interpretable learned optimizers. The paper shows that the method recovers aspects of the ground truth rule when applied to baseline optimizers. Details of the symbolic regression algorithm? Overall, this paper addresses an important problem (better understanding what learned optimizers are doing) and does so by proposing  a new technique: using symbolic regression to fit interpretable equations to pre trained learned optimizers. I think the paper could be greatly improved by:  Adding many more details about the experiments and methods. The whole point is to have interpretable equations that we can stare at. Can you provide more details about the optimization problem (ResNet on Cifar10, but what was the batch size / number of epochs)?<|endoftext|>This paper proposed symbolic learning to optimize (L2O), in which a neural optimizer will be trained and then distilled into a symbolic form via symbolic regression. This symbolic optimizer will then be applied to solve large problems, with some adaptation and fine tuning. However, the current experiments have left out too many details and results for validating the effectiveness of the proposed method. This can help the interpretability and scalability of the learned optimizer. However, the major issue of this paper is that the proposed method and the claims are not well supported by the experiments. This set of experiments are meaningful but symbolic regression is not the main contribution of this paper. The main paper didn t even describe the concrete algorithm for symbolic regression.<|endoftext|>The main idea of the paper is to explore symbolic learning to optimize (L2O) by distilling a numerical L2O optimization rule into a symbolic rule. The motivation for using a symbolic distillation is to provide interpretability and scalability of the trained optimizers. However, I think that some important details and experiments are needed before the paper is published. # Stengths* The paper tackles and important problem with a sound approach, which is presented well overall.<|endoftext|>This paper proposes a symbolic L2O framework that aims to improve the scalability and the interpretability over the numeral L2O methods. The proposed framework uses symbolic regression to turn a snapshot of the numerical method into a surrogate symbolic optimizer. I suggest the authors provide formal definitions of the algorithm and other details (figures, pseudo code) to show how this algorithm fits into the framework. However, this is demonstrated only with simple known equations in Table 1, instead of real numerical L2O modelsI m also concerned about the methodology:   While I agree with the motivation of improving the interpretability of the L2O model, the resulting symbolic equation is difficult to understand as well. If only consider the inference phase, the author claims the optimizer can be further meta tuned for better performance. The experiments are lacking and cannot support the claims made by the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The problem of training an infinitely wide neural network with one hidden layer can be written as an optimization problem in the space of probability distributions, where each atom in the distribution corresponds to a hidden neuron. The paper proposes to solve instead the dual problem (of an entropy regularized variant), which in the case of finite data is a finite dimensional optimization problem. The presented dual particle optimization scheme appears to be novel and might be useful for other problems on probability density than mean field neural networks.<|endoftext|>The paper considers optimization of MF two layer neural nets (the infinite width version with entropic regularization). Specifically the paper proposes to do dual coordinate ascent, which allows for exponential convergence rate, together with a particle approximation scheme. The paper is clear in its exposition, despite involving many parameters. I also appreciate that the authors have made some honest remark concerning $\lambda_2$, which is clearly a deficiency in this approach. It would be good to be more quantitative about this. The paper proposes an interesting variant of dual coordinate ascent that achieves improvements in the convergence rate.<|endoftext|>This paper presents an optimization alogrithm for mean field shallow neural networks. The algorithm optimizes the parameter measure using a particle based implementation of the stochastic dual coordinate ascent algorithm. The authors establish convergence results, namely an exponential convergence rate owing to the convexity of the dual problem. This paper concerns optimizing mean field neural networks. Here, the authors propose to instead work directly with a space time discretized dynamics. Because the algorithm requires sampling a high dimensional distribution, this step is performed infrequently and the particles are reweighted iteratively. Is there some reason we should expect that it s easy to sample? The algorithm appears to be effective in a simple synthetic example.<|endoftext|>This work solves the entropy regularized mean field model for a two layer neural network. As the Fenchel–Rockafellar dual of that problem is a finite dimension problem, they consider an SDCA type scheme to solve the dual problem, which still contains an integral term and they use the Langevin iterate procedure to solve the subproblem approximately. Comments:This paper introduces techniques to directly solve the infinite dimensional mean field model for two layer NN. After computing step 4, the "weights" are updated by step 5. Is there any connection with a primal algorithm given this similarity? This should be made clear. This work introduces a dual algorithm to solve the infinite dimensional mean field training problem. The new technique is interesting and the new linear convergence rate improves the prior result.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; Strength:This paper appears to be the first in bringing and realizing the concept of group based disentanglement in unsupervised VAE. The method was rigorously motivated and backed by theoretical analysis. Weakness:Some of the metrics seem to be weaker than usually reported in the comparison methods. This is an overall well written manuscript describing a novel method backed by in depth theoretical analysis. There are some questions that can be addressed and improvements that can be made to the current manuscript, but overall I consider this to be an interesting work that will have good discussion potential for the community.<|endoftext|>This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement based on $n$ th dihedral group. In fact, there are a lot of related works about disentangled representation learning after 2019, such as TamingVAE, ControlVAE, and other works [1,2,3,4]. Conducted extensive experiments to verify the proposed frameworkCons:1. 2.Please also compare the proposed framework with the baselines in recent work as mentioned above.<|endoftext|>This paper provides a theoretical framework on the learning of group based disentanglement representations. It proposes a method to learn a cyclic group representation with the Abel loss and Order loss based on VAEs. To me they are all the same in the context of this paper.<|endoftext|>The paper proposes a theoretical framework for unsupervised representation disentanglement (in the sense of Higgins et al.) I find this paper to be overselling the importance of its theoretical contribution, and will therefore recommend rejection. based on three constraints (group structure, data and model). Strengths:  The mathematical part of the paper and experiments appear sound.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The theoretical results in this paper are strong, and they serve as a significant first step in understanding the generalization and robustness properties of polynomial networks. The only weakness of the paper is that the proposed regularization technique can feel a bit disconnected from the main theoretical results (due to relaxation for tractability consideration), making it a bit less clear whether the practical regularization is actually controlling the Rademacher complexity effectively. This seems to be quite a significant relaxation and it is unclear how loose the practical bound is.<|endoftext|>Again the algorithmic insights of coupling regularization with adversarial training are not novel, but the idea is verified to be useful also in the context of polynomial networks. The paper provides first generalization bounds for polynomial networks, which have been found to be empirically effective in prior work. Rademacher complexity bounds imply theoretical generalization guarantees. Formal guarantees of low error and high robustness for empirically used techniques are an important contribution of the paper.<|endoftext|>The paper derives empirical Rademacher complexity bounds for two variants of PN, along with their Lipschitz constants. The derived bounds motivate practical regularization schemes that are implemented based on projected gradient descent. Empirical results demonstrate the usefulness of the proposed method. It provides the first known generalization error bounds and Lipschitz constant bounds. Weakness: The theoretical contribution of the paper is useful but not significant. 8.Although in the summary of main contributions, the author mentioned the sweet spot for regularization parameter and accuracy robustness trade off, there are little to no descriptions/explanations/justifications for them in the experiment section. Besides the problem of paper presentation, I have concerns about the overall performance of the method on more challenging datasets.<|endoftext|>This paper analyzed the empirical Rademacher complexity of polynomial nets, the Coupled CP Decomposition (CCP), and Nested Coupled CP decomposition(NCP) nets. The Projected SGD and the Projected SGD + Adversarial Training algorithms are used to optimize the CCP and the NCP with the Lipschitz constants. Pros:  The Rademacher complexity bounds of the CCP and the NCP are derived. The Lipschitz constants for the CCP and the NCP are derived. Is there a bound that can keep the performance of clean data the highest and the accuracy for the adversarial data also the highest? Why not compare with some work that is proposed for adversarial defense/attack?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; To tackle these two challenges, it proposed the SIMFED framework which first calculates the similarity between the embedding of different local models and then conducts a kernel factorization to solve the problem of heterogeneous data domains. I have the following major concerns. The comparison baselines for Label Heterogeneity do not seem informative. A more reasonable way is to compare the proposed framework to those methods that take label mismatch into account. I would like to see more comparisons based on such kinds of baselines. It is unclear why we should use the cosine similarity. Each local model may have different preferences of scaling resulting given different domains of the data source. Overall, I found the paper is a heuristic work on personalized federated learning with little justification and little motivation. The scope of its application is unclear.<|endoftext|>This paper studies two challenges of personalized federated learning: (1) Label Heterogeneity where label schemes are not synchronized in local clients and (2) Domain Heterogeneity where the datasets owned by the clients can be semantically dissimilar. The authors propose a method called Similarity Matching and Kernel Factorization which measures semantic similarity/dissimilarity between locally learned knowledge and aggregates the relevant ones. Furthermore, the method factorizes the model parameters into two basis vectors and sparse masks to capture representations of the heterogeneous knowledge. The authors did not give sufficient rational analysis for this assumption. FedBN (ICLR2021) can handle the feature heterogeneity. The authors do not well position their own work and the main technical idea is not well analyzed yet. 3.The kernel factorization is not well analyzed yet, mostly are simply supported by improved results. This paper studies the novel and interesting scenarios in federated learning, i.e.label heterogeneity and domain heterogeneity, and the experimental results show that the proposed method outperforms other federated learning methods on single  and multi domain datasets. The main logical structure of the paper is relatively clear. Overall, this manuscript is not mature enough.<|endoftext|>Authors propose an algorithm for clients to learn from each other even under the setting of label heterogeneity and domain heterogeneity. The main contribution is to propose an approach to support clients to benefit each other across different types of labels and domains. They provide extensive experiment results to support their claims. + The experiment results are sufficient enough to support their claimsWeakness:  There are some research works discussing how to use general knowledge across clients by dividing parameters into different parts, like continual learning in FL, where each client has several tasks to conduct [1]. Also, the kernel factorization of FC layers and convolutional layers happens on the server side. I am not sure how much extra computation it would take and whether it would cause a delay in the iterative FL update process. More discussion about the effectiveness of similarity matching and kernel factorization is expected. Furthermore, in most FL work, convergence guarantee is also another point that is expected to discuss besides the experiments. "Federated continual learning with weighted inter client transfer." PMLR, 2021. However, if we consider each part of the techniques, like model parameter kernel factorization, grouping users based on similarity, I may expect more novel contributions.<|endoftext|>The main contribution of the work are as follow: 1. introduce the problem of Agnostic Personalized Federated Learning (APFL), and discuss its two possible issues (Label  and Domain Heterogeneity)2. propose the method called Similarity Matching and Kernel Factorization (SimFed) to tackle the problem. 3.Validate the method in both label  and domain heterogeneous scenarios, and show its superiority. Strengths: Well motivated challenges to tackle: label heterogeneity and domain heterogeneity. Then the authors propose similarity matching and kernel factorization to address the issues and show compelling results in experiments. In table 2, it shows that Similarity Matching + Factorization did not outperform Stand Alone for the "Fruits & Vege" dataset. It would benefit if authors could clarify why, and share some intuitive explanation why or in what scenarios that the proposed algorithms underperform comparing to the "stand alone" approach. 2.The paper would benefit with some explanation why each baseline models & architecture were chosen here for comparison. 4.Kernel Factorization outperform SimFam in most datasets, excepts some. Overall it was well motivated problem, and the authors proposed approaches and show compelling results. As pointed out in the above for its weakness, the paper would be strong by providing these insights and intuitions.<|endoftext|>The paper proposes a personalized federated learning algorithm to deal with label heterogeneity and domain heterogeneity. However, the paper has some weaknesses:1  It seems that the paper assumes that models, are neural networks (either with dense layer of convolutional layers). However, this is not mentioned in sections 1 and 3. Furthermore, this paper studies only classification problems. and "we can measure similarity between the local domains in the embedding space, which eventually enables us to determine which models are beneficial each other accordingly." are not clear that what they want to express. In fact, the authors do not make it clear that what it means by stating the models benefit each other. 3  There are some personalized federated learning algorithms as well as agnostic federated learning algorithm. This can better shows the effectiveness of the proposed algorithm dealing with label heterogeneity.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; The paper proposes an extension of Decision Transformer (DT) that can work with distributions of features. It proposes 2 different extensions of decision transformer (DDT and UDT), furthermore introducing two variants of DDT (Categorical (CDT) and Gaussian (GDT)) and two variants of UDT (auto encoder (AE) and contrastive loss) together with 4 settings (SL+UL, SL, UL, UL Fix), at the same time it describes offline state marginal matching and inverse RL imitation learning and develops a hindsight information matching (HIM) framework. The unifying perspective presented in the paper is very interesting, as well as the proposed distributional and unsupervised extensions of decision transformer. There are too many variations of the algorithm considered and the experiments are not expressive enough to illuminate them. After rebuttalThe authors addressed my concerns with evaluations and baselines. Furthermore, the connection to distributional RL was clarified.<|endoftext|>Learning to perform SMM offline may also aid in downstream online exploration, which is an important research direction. If the authors are interested in continuing this work, I think (6) would make the paper significantly stronger. The figure font is also a little small at times. After rebuttal (11/18):I think the updates made by the authors during the rebuttal period are significant, adding writing clarity, experiments, and the Bidirectional DT. 3.In the contributions, it is written that the work "proposed the first benchmark for offline state marginal matching".<|endoftext|>This paper presents variants of the Decision Transformer that can condition on desired state or feature distributions instead of scalars. "unsuccessful learning ,": remove space before comma**Post rebuttal update**Thanks to the authors for their responses. It consolidates a number of existing techniques, including TDMs, Learning from Play, GCSL, and Decision Transformer (DT), into the same umbrella of "hindsight information matching" algorithms in the spirit of hindsight relabeling. 2.It describes a variant of DT, called Categorical DT, that conditions on a desired future state or feature distribution (as opposed to scalar cumulant). 3.It offers a version of the Categorical DT that includes an unsupervised learning component, with the goal being to learn representations $\phi$ such that matching the first moment of $\phi$ is equivalent to matching the distribution of raw features.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; Combining all the empirical design and choices, the method ViTGAN achieves comparable performance to the SOTA CNN based GAN models on mainstream metrics. Pros:1.The architecture of the proposed method is quite clean and simple. The design of the ViT architecture part don t change much, compared with the original one. 3.The paper is well organized. I suggest experiments of resolution 256 or 512 would be helpful to provide. 4.Some of proposed techniques are well introducted and involved by recent works. If all my concerns could be well addressed, a score of 8 is deserved.<|endoftext|>In this paper, the authors study employing ViTs for GAN based image generation. It would be better to investigate the feasibility of scaling up to high resolution images (e.g., 1024x1024). The paper is overall well written and presents a solid study of using ViT models to build image GANs. However, the lack of computation/model size statistics may lead to unfair comparison; the novelty might be limited since most of the techniques are already proposed. I would like to hear from the authors for the final decision.<|endoftext|>The authors propose an adaptation of spectral normalization that works better when using ViTs, and two modifications to the ViT generator. The paper is very well written and structured. Maybe I missed it, but I could not find it in the paper. **Baselines**: the code for TransGAN is available and it is the primary ViT based baseline. As for empirical insights, there are several interesting findings, but one of the main contributions (ISN) needs to be compared to a stronger baseline, ie., a sweep over gamma values. The authors aim to close the gap to the current state of the art GAN, but GANs can operate on >1MP resolution. Lastly, for an empirical study, the experiments are somewhat lacking and I would like to see more and stronger baselines and more datasets.<|endoftext|>This paper has presented a new GAN architecture based on the ViT. The authors have found that high variance gradients in the transformer based discriminator cause unstable training. The new regularizers are proposed to stabilize the training. The paper is presented clearly, particularly the method. Swin transformer works with the same mechanism. 5.The state of the art GANs have advanced high resolution image synthesis. I think the authors need more important experiments to support the advantages of ViT based GAN over CNN based GAN.<|endoftext|>The experiments show the proposed framework can achieve competitive performance with CNN based GANs on several datasets with image resolutions up to 128x128. The whole paper is well organized and written, and easy to follow. L2 attention is adopt in ViT based discriminator duo to the consideration of training stability (i.e., Lipschitzness). How does it perform if using in the generator? 3.The authors also point out the limitation of current method on high resolution image generation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Categorizing by the degree of interaction between learned and algorithmic components is an option to consider. With all of this in place, the paper would put itself in a position to frame its claims in much wider relevance (which I believe the claims deserve). I suggest the authors consider the following suggestions:a) explaining tree search basics in the main textb) explaining (some of) the heuristics in the main text   it is important for the analysis of the results anyway. I believe they require significant changes in the paper structure so I cannot, at this point, recommend acceptance.<|endoftext|>This way, the reader can visually compare different methods without having to zoom in and read hundreds of numbers. I would like to see the authors’ responses to my questions and concerns before making a final decision, but am generally positive about this submission. This makes the problem much harder than MIS for integer programming solvers.<|endoftext|>The benchmark data set is an important contribution and the detailed results presented in Table 2 can immensely benefit future performance comparisons. The writing of the paper is excellent and the paper and the literature review is extensive. The paper also presents a benchmark suite to make comparison of this task easier. Citation counts often work as a proxy for the perceived importance of a paper.<|endoftext|>Strengths:  I think the paper addresses interesting and important questions. Understanding what work and what does not work, and which parts of a solution actually contribute to the performance, is important. Further, other works have looked at generalization of GNNs (e.g., [2]). I am not sure this is an important contribution of the work. Overall, I think this type of works is important and can lead to important insight.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper focuses on the online attack problem, where two elements are emphasized that attackers must operate under partial knowledgeof the target model, and the decisions made by the attacker are irrevocable. The whole story is complete with the theory supported. 2.The experiments are also good. However, i still have some concerns w.r.t.experiments. Could their methods be adapted as more strong baselines for comparison? Have you tried different defence strategies for comparison? I would like to raise the score to accept.<|endoftext|>Unlike traditional threat models, the online attack model assumes the data feed as stream and the decision made by the attacker are irrevocable. Towards this, the author made a connection between the online adversarial attack and the k secretary problem, and proposed a new algorithm Virtual+. Strengths:  This paper proposed a new threat model called online adversarial attacks, which could open the door for a new research direction. Based on existing algorithms, the authors proposed Virtual+. This algorithm could also be used for tackling other online problems. I agree it is a new problem, but I am not sure about what kind of attacker need to launch online attack on what type of dataset? This does not support the author s motivation that "Countless real world applications involve streaming data that arrive in an online fashion (e.g., financial markets or real time sensor networks)."<|endoftext|>This paper formulates an online threat model and use the VIRTUAL+ algorithm their proposed to solve this k secretary problem. The main contributions are built upon the proposed new algorithm towards k secretary problem, and the online adversarial attack is only one of the possible applications of this k secretary problem. Similarly, we can also train f_s through receiving the streaming data. It is possible that the attack is hard to train for some data, so a larger nb_iter may help diagnose this. It would be great if discussions can be provided for the technical difficulties of Theorem 1.<|endoftext|> This paper studies the online adversarial attacks on deep learning (DL) models. Unlike other works that focused on adversarial attacks on  DL models, this work studies, on half of the attacker, how to choose a subset of data points to attack in an online fashion. This setting aligns with the situation given in the well studied k secretary problem. Hence, the authors obtain theoretical bounds on the performance of the proposed VIRTUAL+ algorithm for the stochastic setting. * Can the authors provide more real world examples/situations where the attack model studied here can be applied? * For MNIST and CIFAR 10, data points usually arrive in batches instead of single data points. Can the authors also justify that in the manuscript? How would the authors deal with data batches? Another major concern is the performance of the VIRTUAL+ algorithm. * Figure 4 appears before figure 3In the review, I proposed two major concerns toward the paper along with some minor ones.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; To achieve this, the method learns a latent style embedding of behavior styles and condition the policy on it. The idea of the proposed approach also has little novelty. This again raises the question of why not just use VAE based approach such as [1]. There are also no ablation studies to understand the design of the method better. Thus, I vote to reject this paper.<|endoftext|>Strengths  + The paper is well written and the proposed idea is conveyed very clearly. This is a plus. I am also very surprised that simply training the embedding with the highest prediction score works so well. There is no comparison with strong baselines in the gridworld environment. However, there is no ablation study of the design choices, and overall experiments are not thorough.<|endoftext|>The paper is about imitation learning, and it particular it considers the task of replicating the style of behaviours. Although I find the task very interesting and with a wide potential of application, I have a few concerns about the clarity of the paper.<|endoftext|>The proposed method is validated based on multiple benchmark environments and obtains supportive experimental results. However, I would recommend the authors to address my questions in the rebuttal. This paper is generally well written and easy to follow.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper introduces a method, called Efficient Neural Causal Discovery (ENCO), to determine the structure of a causal graphical model of a set of random variables, which uses both observational and interventional data. Overall, the paper is well written and clearly structured. I acknowledge that the authors have an example of how to check the conditions of Theorem 3.1 in appendix B.2.1. For large graphs, it seems to be very complicated to estimate $\lambda_{sparse}$ and, hence, there will be no guarantee that we recover the full causal graph. Although the true causal graph is not known in this setting, the plausibility of the result could be discussed in this case to evaluate the performance of the method. Overall, the paper is a significant contribution to the area of learning causal graphs.<|endoftext|>This paper proposes a new gradient based method to learn causal DAGs from observational and interventional data. The claims seem sound and the experimental results are convincing. The authors build a neural network for each variable to model the conditional distributions of that variable, conditioning on any variable set. Overall, this paper is well written. The claims are supported by rigorous theoretical analysis and extensive empirical studies.<|endoftext|>The problem of score based structure learning using observational data as well as a set of interventional datasets is increasingly relevant in domains where experiments are readily available. 1.A starting question I have is why acyclicity is necessary to enforce if interventions are available on all variables. 2.It is not clear that the authors ensure that the recovered graph is acyclic. What is this step? 4.Theorem 3.1 assumes access to conditional distributions if my reading is correct, is it plausible to expect that there will be no local optima with finite samples? Interesting proposal if some of the claims can be better justified and if we can bring the algorithm closer to practice.<|endoftext|>This is the basic idea that has been used in several constrained based methods in the past when hard interventional data is available. The main difference is that   since edges are learned separately   acyclicity need not be enforced. Any comments on this as well will be appreciated. After the rebuttal:      I would like to thank the authors for the additional experiments they conducted and the further explanations about their assumptions. It would be nice to list all the assumptions in 3.1 under a numbered list. The conditions of Theorem 3.1. seems to have been developed by essentially reverse engineering the proof.<|endoftext|>The paper works on causal discovery on both observational and interventional data. It proves that under some assumptions, when all the variables have interventional data, the resulting graph converges to the ground truth one. The paper is in general well written. The discussion on the latent confounder case and few interventions are appreciated. It introduces a certain parameterization method for enforcing the acyclicity of the model without imposing acyclic constraints on the optimization problem. 3.1.Especially, how realistic is the condition?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; From this perspective, these settings are somehow ill posed  The empirical results are not convincing enough, as most of the empirical justification are based on one single dataset CIFAR 10. The PU OCC models are developed based on existing studies; I cannot find any major technical noveltiesThe studied problem is important, but there are a number of major issues in the main claims, the empirical justification and technical novelty. So, I do not recommend it for publication at ICLR. Further, for those unusual settings, there are more one class classification or anomaly detection problems than PU learning.<|endoftext|>This work empirically investigates using the PU loss in Positive Unlabeled learning in the one class classification task. The authors propose several PU modifications of the original classification algorithms that can leverage the unlabeled data. positives:  The authors study a critical problem of leveraging unlabeled data to improve the performance of one class classification tasks. The novelty seems limited. The proposed methods are straightforward combinations of one class classification algorithms and PU loss. The authors report the experimental results on different PU modified algorithms, but it is still short of new insights for using unlabeled data in the one class classification tasks. The authors propose several modified one class classification algorithms, where part of the original loss is replayed by the PU loss in Positive Unlabeled learning.<|endoftext|>The paper describes an empirical study performed on one class (OC) and positive unlabeled (PU) learning  settings. Here too, the empirical results shown in table 3 shows mixed results. This is not clearly explained in the paper. Though the authors mention 3 extremely important questions that provide recommendations on the methodology to be used, the test falls short in addressing when only a few unlabeled data is available. However, there still exists key unanswered questions needed for practitioners to incorporate the recommendations in practical settings.<|endoftext|>Results are not surprising, lacks fresh insights, does not present concrete actionable steps2. This is well known in general. The paper focuses on a specific class of algorithms to demonstrate this. The paper s conclusion is rather fuzzy in the end (true \alpha probably is not known)... 5.It would also be good to discuss how we can discard data that looks obviously unreliable and retain what is reliable. The experimental results might still be useful to researchers.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 8; Both the high and low level policy networks are fine tuned at meta test time. They show that DMIL converges and evaluate their approach against several ablations on the ML10 and ML45 setting of the meta world benchmark. **Pros**  The authors show that DMIL is competitive against a thorough set of ablations on both ML10 and ML45 settings from meta world. From the results, it s difficult to compare this method against hierarchical imitation learning approaches that they cite (Yu et al., 2018b; Finn et al., 2017b; Yu et al., 2018a)  The authors do not show that the results are robust across different tasks/environments (e.g., "Hierarchical Few Shot Imitation with Skill Transition Models" looks at maze and kitchen environments) or number of subskills. I believe this is important because the manipulation tasks in meta world may have compositional task structure that can t be assumed in other environments or tasks (such as navigation). If more environments and subskills could be tested, I think the paper could be a candidate for submission.<|endoftext|>This paper proposes a meta imitation learning framework aimed at learning long horizon robot control tasks with fast adaptation capabilities. Specifically, the proposed approach adapts the model agnostic meta learning framework for learning a hierarchical policy, where both levels of the hierarchy are meta trained and fine tuned at test time to learn new tasks. On the metaworld benchmark, the proposed method outperforms prior work, and qualitative analysis shows that the method discovers meaningful skills. This paper studies an important research problem  — the ability to discover meaningful abstractions of behavior from prior experience and to transfer these abstractions to efficiently learn new tasks2. While the proposed approach integrates existing elements from MAML and hierarchical imitation learning, it does so in a unique and novel manner4. The experiments consider a thorough set of baselinesAlongside these strengths, I also have numerous concerns, all regarding the quality and clarity of presentation:1. how sensitive is the method to the number of skills K? While I think the technical contributions of the paper are sound and the experiments are generally thorough, this paper at its current form is limited by the writing quality.<|endoftext|>The paper proposes an approach for few shot imitation learning that jointly meta learns a high level policy and a set of low level policies from a diverse set of demonstrations (multi task). ## Strengths  few shot imitation is an interesting and impactful problem  the paper solves a hard problem: jointly learning HL and LL policy + quickly adapting them on unseen tasks  the method is clearly described, the figure help understanding the approach  the method is evaluated on a representative benchmark and compared to meaningful baselines  the qualitative results show that the approach learns meaningful skills in the MetaWorld benchmark## Weaknesses  My main concern with the method is that it is trying to solve too many things at once: (1) inferring skills from unstructured datasets, (2) jointly learning high level and low level policy and (3) meta learning both these policies to have them adapt quickly on new tasks. All of the abovementioned works first learn a set of low level skills and then learn a high level policy over them. If the authors can provide experimental evidence that the method is no harder to train / tune than alternative approaches for few shot imitation I am leaning towards accepting the submission, but I am open to changing my opinion based on the other reviewer s feedback.<|endoftext|>The paper describes Dual Meta Imitation Learning (DMIL) which is a hierarchical meta imitation learning method where the high level network and sub skills are iteratively meta learned with model agnostic meta learning (MAML). The authors provide theoretical proof of the convergence based on the connection with the Expectation Maximization (EM) algorithm. They showed the state of the art few shot imitation learning performance on the meta world benchmark. The total algorithm is reasonable, and its convergence is proved based on the theory of the EM algorithm. The connection to the EM algorithm is natural and understandable. The experiment showed the usefulness of the method as well. The weakness of the paper is the limitation of evaluations. The evaluation using a task meta and hierarchical nature is more explicitly required is expected. The paper proposed a new meta HIL method called DMIL. The background is described in a proper manner and is informative.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; In this regard, an unconstrained normalization function is proposed as alternative in the transformer. The authors compare it with baseline transformer and several variants to validate the effect of the proposed normalization function in transformer. I did not check the proof precisely. Weaknesses:The proposed NAP operation is quite related to layer normalization (LN). Most ablation studies and empirical analysis are conducted on synthetic tasks which are less convincing to demonstrate the effectiveness of the method. The authors theoretically analyze the limitation of using softmax function in attention module and propose an unconstrained surrogate in the paper. However, the empirical evidences shown in the paper are insufficient to validate the method.<|endoftext|>This paper looks at one specific aspect of transformer architectures   the normalization step that constrains the attention vectors in a probability simplex. Then, with several experiments on a synthetic dataset, the authors demonstrate that the proposed architectural change leads to models that are robust to hyperparameters, and less sensitive to biases in the data. The finding of the paper is interesting   the arguments made about the restrictive nature of the softmax operator makes sense. The experiments conducted on the synthetic data are thorough and the discussions were made well. However, the authors could have shown gone beyond synthetic experiments and shown more results on real world datasets. While I enjoyed reading the paper, my biggest concern is lack of real world experiments. Can this be used for training language models or benchmark NLP tasks? To this end, it would have been really nice if authors had shown one experiment on real world datasets. I would also like to point out that the authors have one experiment on protein protein interaction graph prediction, but I am not aware of this benchmark and I am unable to gain much insights from this experiment aside from the fact that the model works. While this issue is certainly concerning to me, I believe the findings in this paper are a reasonably good contribution that could help others in the community.<|endoftext|>This paper starts from the observation that current self attention modules is sensitive to hyperparameter changing. The authors conjecture that this is due to the softmax operator in self attention, and give some intuitive examples to support their hypothesis (eg., bias towards local information, gradient vanishing). To solve the problem, the authors propose to replace the softmax with a normalization operator. They show that normalized attention is more robust to hyperparameter in a synthetic dataset, as well as real world settings. Strengths:+   The paper is clearly motivated by the problem that current self attention is not robust to hyperparameter changing. The authors also give some intuitive examples on this, such as bias towards local information and gradient vanishing. +   The paper designs synthetic tasks and does a through analysis of NAP and other baseline models in this setting. However, it seems lacking some convincing results on real world dataset. The experiments on the synthetic task are sound. Is it because the tasks are still too small in scale to show the difference between NAP and baseline models? Do they have any relation to your work? [2] Schlag I, Irie K, Schmidhuber J.<|endoftext|>2.The implications of using softmax attention are thoroughly analyzed in theoretical and experimental settings. Lack of real world NLP benchmarks makes it harder to believe that the implications of the softmax use actually matter. The synthetic tasks chosen are intuitively similar to the specific NLP problems, but in the end, it’s not clear how well the synthetic results will generalize. 2.The experimental results demonstrate the advantage of NAP over sum and max pooling, but the case of NAP vs NON is less obvious. The paper would benefit from NAP experiments that are not MTE based to confirm that NAP isn’t brittle and can be used in practice. Is it because local information bias is actually beneficial? While the practical viability of the proposed method is still uncertain, the insights on using softmax attention from this paper are valuable.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The theoretical results show that the high expressively of unrestricted energy based models comes at the cost of un computability and in approximability of the partition function. This paper tackles the important question of what we loose in theoretical guarantees if we remove all restrictions on the energy based model. One might argue that the findings are somewhat abstract as they only hold for energy based models without any restrictions on the model expressively. I have a couple of further remarks and questions:* I cannot follow the logic from Prop. * In Sec.4 it is stated that no estimate exists with a bias within a multiplicative factor. * Most arguments hinge on the undecidability of the HALT problem. I get the argument, but I was left wondering whether it might be possible to quantify the approximation error one introduces when relying on randomized methods nonetheless.<|endoftext|>This theoretical work highlights uncomputability issues arising with energy based sequence models. pg 19: "for any halting trace $x $ of $M_b$" seems to be a typo. The proofs seem to be correct. Also the proof of Lin et al s Theorem 5 uses a very similar construction to the paper under the review, and this should be mentioned. There is also a lack of discussion on polynomial time computation of partition functions in less pathological situations (see [1,2,3] below)   Only rather specific asymptotic estimates of the partition function are ruled out. It seemed that asymptotic estimates defined here capture the idea of the sample size going to infinity, but I am unclear on the details. To my understanding, the work under review continues the conversation about uncomputability and says some interesting new things about it. I think the conclusion should be rephrased to acknowledge these points.<|endoftext|>Superficially the connection between EC and the halting problem does seem quite straightforward, but if this was not observed or studied in any depth previously in the EBMs literature, I think the submission is a worthy conceptual contribution that should be accepted. In practice, it is common for these weights to be computed by neural networks. That said, the formalism doesn t figure that heavily in the proofs as the arguments are ultimately primarily about Turing machines anyways. This paper studies "efficiently computable (EC) energy based sequence models (EBMs)" which are simply sets of strings equipped with nonnegative weights that can be computed by a poly time Turing machine (upon normalization, the weights induce a probability distribution over the strings).<|endoftext|>And that is even in the case the energy of a sequence could be computed in poly time. The paper then shows some corollaries and variations of this result, showing that model selection is undecidable as well. The paper concludes by suggesting scenarios where the partition function is computable but this naturally comes at the expense of the expressivity of the model. The paper is generally well written with a good exposition. This is crucial for understanding the paper, yet it is done quickly and I found it hard to follow. It seems to me the big  insight  is Turing completeness of the model, but this is (mostly) done in previous papers. 2.I m not sure the paper is a good fit for ICLR.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This paper studies and compares the performance of self supervised representation learning methods against supervised representation learning when there are class imbalance. It shows empirically that self supervised methods are more robust to class imbalance, i.e.the performance gap between models trained on balanced and imbalanced datasets is smaller. The paper also investigates the quality of the features learnt by SSL vs SL, and shows both theoretically in a limited setting and empirically on synthetic datasets that features learnt by SSL are diverse and capture characteristics that might be useful for rare classes. Finally, they provide a regularisation method for SSL to encourage model to learn better features for rare classes without having access to labels. It reads well and has a robust set of experiments to investigate the effects of class imbalance on representation learning.<|endoftext|>However, in the OOD setting, representations pretrained with SSL outperform those pretrained with SL and are even robust to the imbalance factor. The paper is well written in general and its motivation is clear. The imbalanced datasets used in the paper are CIFAR 10 LT and ImageNet LT, which are versions of CIFAR 10 and ImageNet that are originally balanced. It considers a problem that is relevant to the machine learning and computer vision communities. The proposed analysis also makes sense and is interesting to the ML and CV communities.<|endoftext|>The authors explore the robustness of feature learning via self supervised learning (SSL) and supervised learning (SL) with imbalance datasets. Generally, SL can learn better features than SSL, and features are better from balanced than from imbalance datasets. The robustness is observed from both in domain (ID)  and out of domain (OOD) tasks (different downstream tasks). Strengths:Empirical analysis, with theoretical justification, shows that SSL is more roboust than SL in feature learning from imbalance datasets. The presentation can be improved as indicated above.<|endoftext|>This paper focuses on class imbalanced learning and shows that self supervised pre training yields representations that are more robust to dataset imbalance. Strength:1) This paper is well written and the conclusion is clear. For example, on what conditions self supervised learning is better than supervised learning, and on what conditions are not? In section 4.1, the authors report the performance of the proposed rwSAM on imbalanced datasets. But the results are mainly observed from empirical results, the generalization of the conclusions is not clear. The novelty and contribution of the paper are limited.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; This paper empirically shows that the Class Activation Map (CAM), a simple technique for discriminating the learning patterns of each class in images, is better at making accurate predictions than the model itself on selecting the true label from candidate labels. Experiments validate the effectiveness of the proposed method. CAM is well known as a simple technique for discriminating the learning patterns of classes. This method is novel while keeping its simplicity. So I would like to how can the proposed method be used in another weakly supervised learning problem like semi supervised learning. (2) The authors mentioned the average based strategy and the identification based strategy for partial label learning. Comprehensive experimental results support what the authors claimed in the paper.<|endoftext|>This paper intends to exploit the learned representation of the model to tackle partial label learning tasks. The paper begins by a PILOT experiment to show that the class activation map is better at selecting the true label from the candidate set than the model output itself. The paper proposes a novel CAV based method to solve PLL problems and shows a promising performance than several state of the art methods. Based on CAV, they propose a CAV learning method to select a potential true label for training and thus transfer PLL to supervised learning.<|endoftext|>This paper focuses on the problem of partial label learning (PLL), where each training instance is assigned a set of candidate labels that include the true label. This paper shows that class activation map (CAM) could identify the true label from candidate labels for PLL. 2.This paper proposes the class activation value (CAV) for identification via capturing the learned representation information in a more general way instead of CAM as CAM is confined for image datasets and CNN model. However, the selected “true” label may be false positive and the authors also claim that the potential true label would be updated. 3.There is no theoretical result provided to validate the effectiveness of CAV for PLL. Suggestion:The authors are suggested to provide source code as supplementary material so that the reviewers could check the reproducibility of the experimental results.
Reject; rating score: 3; rating score: 5; rating score: 6; This is an extension of existing works based on finite dimensional exponential families. The authors investigated some theoretical conditions such that the RKHS defines the probability density functions. However, the results are rather a straightforward extension of the existing work by Martins et al.(2020; 2021), in which finite dimensional exponential families and their deformed variants are used to formulate the continuous attention mechanism. However, this paper does not reveal the significant advantage of kernel based modeling as an ingredient of continuous attention mechanism. It would be nice to show a data dependent method of selecting the deformation parameter. The proposed model in the paper seems a straightforward extension of existing works by Martins et al.(2020; 2021).<|endoftext|>The manuscript considers an extension of the attention mechanism framework developed by others in recent work (by Martins et al.). This is then handled by using a probability distribution function (pdf) over the collection to compute an expected value. The main theoretical contribution seems to be the statement of conditions under which a kernel exponential distribution exists (i.e., the normalization constant is finite). I don t have any technical objections to this development. But an attention mechanism is a means for obtaining improved performance, and I would have expected a clearer demonstration that the theoretical development is worth considering from a practical standpoint (i.e., is computationally light enough to be part of a deep network, and improves performance significantly). The improvement obtained by using the proposed mechanism was also not clear to me   for instance in the IMDB example, the results are close to when the attention mechanism is continuous sparsemax (from previous work). I would suggest the authors to focus more on the experimental section. The contribution is mostly theoretical, laying out conditions under which a pdf from a family exists (which is a fair question to consider).<|endoftext|>Previous work in the literature proposed to extend the softmax based attention mechanism by using different distribution families. In particular, the authors of this paper focus on variants of the attention mechanism that allow for continuous and sparse attention. This work is well motivated and the authors clearly highlighted differences with previous work (section 3). Although this opens up possible research direction for future work, I would like to have more information about experrimental speed efficiency of kernel deformed exponential families  compared to standard sparsemax/softmax. Moreover, the benefit of the attention mechanism proposed by the authors is that it allows sparsity and multimodality: the paper would benefit of including experimental evidence that this mechanism is important, beside test set accuracy. Interesting work, however the experiments section lack of qualitative results regarding the proposed approach (i.e.multimodality benefit is only quickly showed for one dataset in Figure 1)
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The experimental results show that the proposed method can change the memorability of images measured by the scores of the SENet50, and this method can be extended to modify real facial images.<|endoftext|>While the paper claims that the proposed technique aims at modifying memorability without affecting identity and other facial attributes, there are some cases in the examples whose identity clearly changed after memorability modification. ## Weaknesses  Technical novelty is not clear. The authors first train a memorability assessor network on the 10k US Adult Faces Database and used the assessor to provide memorability scores on generated face images from StyleGANs pre trained on the FFHQ dataset. Essentially, this paper provides qualitative results and quantitative evaluation in terms of face realness (FID and KID metrics) and memorability (assessor score) without any comparison with baseline approaches.<|endoftext|>+ The paper is easy to follow and presented the experimental results with StyleGAN and StyleGAN2. Experiments with more datasets are needed because only two StyeGANs are used for evaluation. Although they tackle an interesting problem, the proposed method is rather simple, and it is a combination of existing methods. Although this work tackles an interesting problem, the novelty is weak. It is limited to StyleGANs.<|endoftext|>Also some of the quantities should be added in the figure and tables captionsOverall the idea of of the paper is nice but the evaluation is barely convincing. The observation, which again is very interesting, needs to be proven by experimental evaluation and the paper does a good job.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The novelty and scope appear limited and the evaluation is incomplete. The paper could be improved by comparison against state of the art and the use of more standard metrics. How can this method be adapted to a more general case? Cons:  The novelty and scope appears limited and incremental.<|endoftext|>Similarly the expression subspace for FLAME is also different and the impact of that will be in the DECA’s rendering. The pipeline of the paper can work, but the impact of the method and the novelty is not very clear as it depends heavily on Chen et al.and to some extent like DECA’s principle in rendering. Is this improvement due to the method or the choice of the underlying face model? But the AUs could be correct.<|endoftext|>2) The impact of Age and FaceID features is not studied in the ablation. Then they propose a Rendering Network. More compared methods could be added. * It would be better to separate the contributions of the Detail Hallucination Network and Rendering Network.<|endoftext|>Also, the readability of the paper could still be improved. However, I think this paper is still under the bar of acceptance as the experiments are not very convincing. Also,I would suggest the authors to show the zoom in details in the rectangles as in Fig 1 rather than ask the readers to zoom in for details. The readability of this paper can still be improved.
Reject; rating score: 5; rating score: 5; rating score: 6; Specifically, the main difference is to replace the standard objective (i.e., the expectation of Q function over some initial state distribution) with the expectation of Q function over both the initial and an $\eta$ weighted future state distribution, where $\eta$ is some probability distribution over the support set of nonnegative integers. Built on this formulation, this paper proposes GIRL (and the resulting algorithm MEGAN), which follows the framework of GAIL to learn a policy that matches the $\eta$ weighted variant of occupancy measure of the expert policy. Clarity: The paper is well written and well organized. The proposed method is in most places well explained. Is there any specific reason why MMD is selected? I like the insight provided by this paper, but the overall technical novelty is somewhat limited.<|endoftext|>The paper proposes a new formulation for inverse reinforcement learning that aims to address the _bias against policies with longer mixing times_. Although I am not fully up to date with the current IRL literature, the problem addressed in the paper strikes me as novel, and the proposed method (whose derivations I did not check in detail) follows naturally from the formulation proposed. This said, there are several aspects in which I believe the paper could be improved. In other words, although I understand the technical argument, I am not sure that this translates into an actual problem in practice, and some more discussion on this would be welcome. Following on the previous item, and if I understood correctly, it seems to me that the introduced weighting is tantamount to considering a policy dependent initial distribution $p_0$ in the standard formulation. Although the premise of the paper is intriguing, it is not clear to me the relevance of the proposed approach, since the motivation is not convincing.<|endoftext|>The paper extends the maximum entropy inverse reinforcement learning (IRL) framework by changing the optimal criterion used in reinforcement learning (RL). This novel criterion is an expectation of the Q values over a weighted distribution over states and actions induced by a policy, which is in contrast to the standard criterion that is an expectation over the initial state dsitribution.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper investigates annealed importance sampling (AIS) techniques and multi sample bounds to improve mutual information estimation. Assuming the knowledge of extract likelihood (joint or marginal), this paper shows the mutual information bound can be tightened via exploiting a series of intermediate distributions interpolating between a (convenient) proposal and the joint density. Better visualization of the bounds has been provided to help understand the relationship between different estimators, and there are many in place remarks and discussions on the connection to existing bounds. As such I recommend acceptance. While the importance sampling perspective for MI estimation is novel, this is not new in the broader context of variational inference. The discussions on these works are currently missing from the manuscript, which I think would significantly boost the significance of this work by making strong connections.<|endoftext|>* The experimental results are quite impressive and convincingly demonstrate the utility of the proposed method. This work combines the technique of Annealed Importance Sampling (AIS) with Importance Weighted Autoencoder (IWAE) bounds on mutual information to obtain new and significantly tighter bounds. I recommend this work be accepted. Strengths:* The authors present a comprehensive framework for mutual information bounds, called GIWAE, that unifies many existing approaches.<|endoftext|>In this work, the authors proposed some novel estimators for mutual information, namely the GIWAE, Muti Sample AIS and MINE AIS bounds. In both theoretical and empirical analysis, they prove the bounds are much tighter than the existing ones in the literature. Strength: (1) This work unifies the view of mutual information from the perspective of importance sampling. (4) The experiments provide evidence on the effectiveness of the proposals. Weakness:(1) This paper is more of a theoretical flavor and there is no experiment to show this method is practically useful.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper proposes a regularization term to augment loss functions, where the regularization term effectively minimizes the calibration error of the model. The term itself is a kernel density estimator over the K simplex space (hence Dirichlet kernel is the natural choice). The authors claim the estimator is consistent (but not unbiased, though they partially debias it) and empirically verify that their method yields tradeoff between accuracy and calibration that is near Pareto optimal. Strengths:   Idea is conceptually simple  The work focuses on the stronger notion of calibration (canonical)Weaknesses:  The method claims to be well suited for differentiable training. The scalability of the O(n^2) estimator can be a challenge. A breakdown of "training without CE term" vs "training with CE term" would be very helpful in assessing the utility of this method, especially in evaluating whether the improvement in calibration error relative to other methods is worth the additional computational budget.<|endoftext|>I also find the proposed method to be appealing in its simplicity and how easy it would be to apply to generic classification tasks (e.g., regardless of architecture). Minor Comments:  It would have been nice to go into depth about how to choose the various hyperparameters introduced by this method. The authors describe a method for choosing the kernel bandwidth, but do not discuss how to choose $p$ in the $MCE_p$ regularization term, or how to set $\lambda$. I suppose $p$ should be chosen based on which canonical calibration you want, and $\lambda$ might be chosen using cross validation, but it would be good to provide users some guidance. While it seems totally reasonable that the Dirichlet KDE should do much better (it s visually striking in Figures 2 and 8), Figures 3 and 9 are a bit difficult to interpret. The paper presents a conceptually simple approach to improving the calibration of classification models.<|endoftext|>This is an easy to follow paper and the proposed kernel estimator is simple to implement. As for the experimental results, I m also not convinced that the proposed KDE regularized training significantly outperformed the cross entropy baseline (Table 1 2), as it seems that on many NN architectures the CE one obtained comparable or even better ECE. A major contribution of this paper is the introduction of Dirichlet kernels, but the authors missed an opportunity to demonstrate the benefit of using Dirichlet kernels over existing kernel density ECE method [2,3] or advanced histogram estimators (such as [4]) in the experimental section, such as their better capability to handle the multi class cases, other than showing that they are "closely correspond to" the histogram ones. I believe this can be improved. "Verified uncertainty calibration." Overall, I think this is an interesting paper, but not ready to publish in ICLR in its present form. I hope that the authors can improve the empirical aspects for this paper.<|endoftext|>The paper proposes a new approach for calibrating neural network outputs. The calibration error is measured as the Lp norm of the difference between predicted class probabilities and the expected true class probabilities given the predicted class probabilities where the latter term is computed using a kernel density estimate with a Dirichlet Kernel. 2.The  approach seems a bit computationally expensive especially since the simple CRE approach has competitive marginal L1 ECE performance (both pre and post T) and is much less expensive. Is it possible to consider f divergences instead of the Lp error in CE under your approach? If yes, have you already looked into it? However I would like to see more of a discussion on the comparison of the proposed approach with the less expensive CRE baseline (see my recommendations above) to be completely convinced. Comments after rebuttal: I am satisfied by the additions made to the paper. The experiments on the Kather (medical) dataset do illustrate the superior L1 ECE (canonical) of the proposed approach, and the time measurements show that the overhead introduced by the proposed approach is not significant.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Specifically, the author discussed the weakness of the existing dynamic convolution operations, and based on the analysis the author proposed a novel framework with the name ODConv. In order to overcome this issue, the author proposed to dynamically weight all four dimensions for the convolutional kernel. This idea can be viewed as a more general form of dynamic convolution and SE, which I think is quite novel. Another strength of this paper is its comprehensive experiments. 2.The experimental results are just built on ResNet and MobileNet V2. I am prone to accept this submission.<|endoftext|>This work proposes a dynamic convolution that is equipped with attention layers in all dimensions. The authors provide a detailed analysis of dynamic convolution operations and reveal the limitation of previous works. 2.Based on the analysis, the authors propose omni dimensional dynamic convolution (ODConv), where multiple attention layers are employed to generate attention weights of different convolution dimensions. 4.The technical part is easy to follow, and the experimental part is comprehensive. So it seems that the improvement of strong baselines (#param > 100M) would be limited. The paper is well written and properly structured, and the extensive experiments prove the effectiveness of the proposed method.<|endoftext|>The authors present ODConv, a type of dynamic convolutional operation. ODConv combines two prior ideas, i.e.(1) filter recalibration with attention in SENet and (2) additive kernels in CondConv/DyConv, and also generalizes to all remaining dimensions of convolutional filters. First, I think the topic of the work is an interesting and valuable one. This is understandably due to the many factors that are involved. For example, ODConv can be seen as a separable version of predicting full kernels. The idea and technical approach are well motivated and justified.<|endoftext|>Pros)+ The paper is written very well and easy to follow. Cons)  Involving orthogonal attentions seems complementary to each other, but any intuitions why decomposing like those work well compared with the naive ones (in dyconv and cordconv) are not clearly stated. If possible, it would be nice to report the model accuracy as well. I am just curious about a trend of the learned attention vectors and which one is dominant. This paper is well written, and the results about the accuracy improvements look promising. I recommend the authors study more the way of speedup the model in practice for practitioners.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; The introduced certification algorithm, ANCER, utilizes the idea of data dependent randomized smoothing to find the anisotropic shape with maximal certification volume. In experimental evaluation on Cifar 10 and Imagenet the authors show that the obtained certificates permit higher isotropic certificating radii than other methods in the per sample optimization setting. The paper is well written and easy to follow. The mathematical ideas introduced in sections 4 and 5 mostly follow directly from prior work (as is acknowledged in footnote 2). ANCER, presented in sections 6 and C, also seems straight forward and correct. The evaluation section is quite thorough and obtains SOTA results. While the memory based approach seems to fix the unsoundness of DDRS when applied to perturbed versions of previously certified samples, I have a few questions regarding its suitability as a practical certified defense:  Neither this paper nor [1] discuss inference/prediction (e.g., PREDICT in [2]). Can you clarify whether the order in which inputs are presented can influence the output of the model? Consequently, would any obtained guarantees only be valid for a model with the exact same history, hence preventing parallel application? However, my reception of the results, and thereby the overall paper hinges on the data dependent optimization procedure considered in the paper, of which I am currently not convinced.<|endoftext|>The paper proposes the anisotropic version of randomized smoothing. Evaluation metrics based on the volume of the certified region are proposed, allowing comparisons with the certified regions provided from isotropic randomized smoothing. Experimental results show the usefulness of introducing anisotropic randomized smoothing as it certifies larger regions. The discussion and adoption of Alfarra et al.2020 in maximizing the volume through proxy radius are also interesting. It will be helpful if the authors can include comparisons theoretically and empirically with this prior work, allowing a more fair evaluation of the significance of the paper. Minor error: There is a redundant  the  in the first sentence of section 7.3.<|endoftext|>The paper is well written, polished, and easy to follow. The anistropic part of the proposed approach is well motivated, however the sample wise part has several issues (see below) making it unsuitable to use in practice. Moreover, some of the theoretical results are not novel (see below). The authors already discuss a know issue of data dependent classifiers which when not tackled can lead to certificates that are not sound. To address the issue they adapt the memory based procedure introduced in Alfarra et al.While this procedure does make the certificate sound it has other problems. A second, and more important, issue is that the memory makes the certificate dependent on the order of the incoming test samples. This provides a new avenue for attack, i.e.the adversary can optimize the order of the test samples to decrease the utility of the final obtained smoothed classifier. Finally, the success of this memory approach also somewhat depends on the "sparsity" of the test samples. Namely, by using a small test set since the samples are in a high dimensional space the distance between them tends to be bigger than the (proxy) radii of the certified regions. However, in a real world application we are likely to have many more test samples which would increase the number of intersections when running Algorithm 1. Another issue with the proposed approach is the optimization procedure described in section C. The optimization suffers from issues such as: inconsistent estimation due to clamping and not using confidence bounds, sensitivity to initialization, high gradient variance, etc.<|endoftext|>The authors provide a technique for a data dependent randomized smoothing that provides anisotropic certificates of robustness. This may be viewed as an extension of the work of Alfarra et al, where the certificates provided are axis aligned cross polytopes or ellipsoids; however surprisingly the proposed approach provides even tighter bounds than Alfarra s in the isotropic setting. The experiments are thorough and fairly compare against prior work. **Weaknesses:**A primary pitfall of this work is that anisotropic certificates are not particularly well motivated. Importantly, this also means AnCer inherits the main inelegance of Alfarra s approach: the memory based classifier. This raises serious concerns in that the certificates provided are dependent upon the order in which the model is queried. While this is perhaps borderline not kosher, it has not been a concern in practice in prior works, but it would be nice to see some evidence of this claim here. Searching over orthogonal matrices (for the rotation) in addition to the diagonal anisotropic components could drastically increase the number of optimization variables as well as complicate the optimization landscape. In particular, how does the minimum distance between a pair of cross class test data points compare to the bounds provided?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a novel black box decision based space adversarial attack method based on the evolution algorithm. Cons:1.The comparison with the pointwise attack in the targeted attack experiments is somehow unfair. The experimental results are good. 2.Using the L2/L1 distance as the fitness function and using an evolution algorithm instead of some estimated gradients to generate adversarial examples is novel.<|endoftext|>This work proposes a novel sparse attack method called SparseEvo. Based on evolution algorithm, the SparseEvo searchs a sparse adversarial perturbation in limited query budget. It can significantly reduce the queries compared with the SOTA method, i.e., Pointwise. The proposed methods are well motivated and novel. The amount of detail is good, it seems sufficient to reproduce results. Overall, I think this paper is a good one.<|endoftext|>This paper proposes the use of an evolutionary algorithm to construct decision based black box adversarial examples with L0 or sparsity constraints against image classifiers such as CNNs and Image Transformers. Overall this is a solid paper that makes a reasonable contribution to a problem of some interest to the community. ***Strengths***  The paper is largely clear and well written. +++++++++++++++++++++++++++++++++Having read the rebuttal, I retain my score.<|endoftext|>As a paper proposing a blackbox method, it also shows comparison with a white box attack to showcase its superiority (my concerns are described in the weakness part below). and some of them show L \infty metrics (e.g., RayS), many of them can be easily adapted to L 0 case with projections based on my experience. The submission can try to compare with these stronger baselines on ImageNet to showcase its method performance. 2.The paper can discuss its relationship/difference with the existing literature more clearly. Are you using different images for different curves so that the white box attack PGD is not the upper bound of the attack performance in this plot?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This work looks at the problem of link prediction from the lens of causal inference. In particular, the authors introduce counterfactual links, which are links which would have existed under a different treatment (with a running example being the neighborhood identifier). The authors use this definition to define a training procedure which builds on prior art in neural net based causal inference that uses an IPM penalty to ensure balance in representation between treatment and control outcomes. Experimental results are provided which show strong empirical performance for the proposed model. Most importantly, it s not entirely clear what the causal estimand is that is being measured. I find this to be a very interesting and compelling approach, but unfortunately it appears to be a bit obscured by the presentation. A few more minor points:* Sherman & Shpitser (Intervening on Network Ties, UAI 2019) should probably be cited given the relative similarity in task* It would be helpful if the definition of ATE provided by equations 13 and 14 appeared earlier in the text. It would give a lot more clarity to the presentation to my eyes to have a more formal definition of the estimand provided during the problem setup. Overall, I think this is an interesting approach, but unfortunately I think the framing of the paper as a causal estimation paper, rather than a paper which uses counterfactuals a mechanism to improve invariance and performance makes the contributions of the paper slightly obscured.<|endoftext|>In this work, authors presented a counterfactual graph learning method for link prediction (CFLP), where authors introduced the idea of counterfactual prediction to improve link prediction on graphs. Such results shed insights that a good use of causal models (even basic ones) can greatly improve the performance of (graph) machine learning tasks. In addition, OGB DDI authors model should have ranked second best if author submit the results, I wonder why authors do not submit the results in the official leaderboard? CFLP w/ JKNet consistently achieves the best performance across all dataset, while JKNet itself is only very outstanding. For the node representation, authors used MVGRL. Also authors mentioned the embedding is learnt from the observed graph, so my understanding the link in validation/testing set is removed during embedding learning? In page 4, "That is, we want to find the nearest neighbor with the opposite treatment for each observed node pairs and use the nearest neighbor’s outcome as a counterfactual link." I would recommend adding reference to some matching based methods in causal inference. In page 4, authors first used d for a distance function in (2) between two node pairs, while later used similar d for a distance function for two nodes. This is confusing and I would recommend using another symbol. For RELATED WORK, it seems not clear how some literatures (especially in the causal inference section) are relevant to this work. From the quality and novelty perspective, the proposed method is interesting and the experiment is thorough. However, my major concern is the causal model in the paper. The proposed approach is more like a data augmentation, where similar pairs are found with different community relation to enrich the training set. It would be helpful to spend more effort to illustrate why such causal graph design makes sense. I highly recommend make a submission for a fair comparison, or explain why the submission has not done yet.<|endoftext|>This paper targets an interesting question in network analysis: what are the main causes leading to the creation of a link between a pair of two nodes in a given network observations. Most previous related models assume a latent structure underlying observed network data, and suppose the creation of each link is driven by the latent structure behind two nodes associated with that link. Hence, the main cause effect resulting in the link between two nodes, may not be truly reflected by the underlying community structure. To address this concern, the paper resorts to a counterfactual learning framework by learning counterfactual links between the most similar node pairs with a different treatment. Experimental results demonstrate the improved link prediction by the counterfactual graph learning method compared with latent community based methods. correctness:To my knowledge, the idea and technical contribution look sound. additional questions:  can you discuss how to generalize your method to continuous valued network data in the supplement? say, A_ij \in [0,1], represents the connection strength between nodes i and j.  if possible, can you discuss or even conduct additional experiments to demonstrate the robustness of your method to missing (false zeros) and noisy link data, which is prevalent in large scale networks. except AUROC score, have you considered PR score to evaluate the link prediction performance as PR score is only sensitive to non zero links? typos:   pp.1 only were  > are  Algo 1 model inferencing  > inference  many places causal model  > causal model(s)Overall, I vote for accepting this paper considering its novelty on solving link prediction from a new perspective of counterfactual learning. Nonetheless, my background on causality does not allow to check all of its correctness in such a short time.<|endoftext|>They define a "treatment" for a node pair as whether or not they belong in the same group (e.g.from running a graph clustering algorithm). They then find the most similar node pair with a different treatment and treat its outcome (whether there is a link) as the counterfactual of the outcome of the original node pair. They use two link decoders (both multi layer Perceptrons) to predict both the actual and counterfactual outcomes. They demonstrate impressive improvements on link prediction accuracy on 5 real data sets. The closest related work I have seen is to add and remove edges to a graph to try to improve link prediction accuracy, but that is very different to the counterfactual learning approach proposed here. Proposed approach is conceptually easy to understand and could lead to many variants in the future. The authors claim that the estimates of average treatment effect (ATE) are generally close to the observed ATE, but I don t see this at all from Tables 3 and 4. The estimated ATEs are close only for a few specific cases. I do agree with the second conclusion about the ranking of estimated ATE being useful to select the treatment, however. Also, what is $|\mathcal{E}|$? Is this the total number of edges in the graph? Typo:  Page 3, 4th paragraph: "Traditional causal inference methods hence statistical learning approaches". "hence" should probably be replaced with "use"*After discussion period:*I have lowered my score slightly following the discussions but continue to support the paper. Despite the unusual framing as counterfactual learning as compared to data augmentation, the empirical gains are highly impressive. I think it could inspire future research investigating the use of other structural properties of the graph as the "treatment". The authors propose a highly creative approach to improve link prediction accuracy by an innovative definition of treatment for a node pair in a counterfactual learning set up.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; **Summary**  While a large number of previous papers evaluate DNN models on pre defined concepts, this paper interprets the models (i.e., BERT) on latent concepts that are learned in an unsupervised manner. How about first assigning each word to a "large cluster" (e.g., by the part of speech), which should be very accurate, and then run the cluster algorithm within each "large cluster"? Interpret BERT by analyzing latent concepts learned in the network2. Methodology (clustering, dataset annotation, etc.) The dataset would be beneficial for future researches. This paper involves a lot of empirical effort. E.g., what if the words themselves are not "meaningful", but the way they are used in contexts appear meaningful? Do the annotators mark them as "not meaningful"? What is the percentage of matches? In 4.3 Concept labels: there seem to be some existing databases with hierarchical concepts.<|endoftext|>This paper studies the latent concept learned in BERT representations. The authors use a simple and effective clustering method to discover latent concepts in an unsupervised way. Then a layer wise comparison is conducted where the results are similar to previous work but is done in an unsupervised way. The authors summarise this paper by organizing the discovered concepts into a BCD dataset. Generally, although the method is simple, I think it is effective and solid. I think a direction for improvements is a more in depth analysis since this paper spends a lot of writing on development (which is fair) but less on analysis. What are these non aligned concepts? Why they do not align with existing annotation? My major problem is that this paper spends a lot of writing on development details that could be simplified to make place for more in depth analysis. I am willing to further increase my score if the authors can present a more in depth analysis.<|endoftext|>These clusters are manually tagged by 3 annotators to connect these clusters to human concepts. The authors additionally release a dataset with these concepts labeled. In summary, I find the main paper lacking in sufficient disclosure on methods, and I do not believe the techniques or findings of the paper to be sufficiently novel, though I can see that the dataset would be useful. I still vote reject for ICLR 2022. The paper reads well, and the authors have clearly presented some interesting clusters within BERT. Their results really do reveal some interesting clusters, and the paper s message could be bolstered if they included some tool to easily explore the concept space learned in BERT outside of the results shown in figures in the paper. I am also confused by this limitation. 6.The annotation analysis performed in this paper is limited in that it only explores the representations in the final layer in BERT.<|endoftext|>The core idea is to use agglomerative hierarchical clustering method to discover latent concepts (i.e.clusters) with hierarchy. The authors then used these labelled clusters to analyse concepts that BERT captures, across several syntactic and semantics aspects. The labelled clusters were also used to build a concept dataset (namely BERT concept dataset   BCD) for 1M instances (i.e.words in contexts). The proposed solution to the problem requires carefully manually annotation which was carried out thoughtfully. This leads to the problem of replication. It does t seem useful when other models are examined. Although clusters were manually annotated, it is unclear how accurate an instance is assigned to a specific cluster. The proposed methodology however has problems which limit the contribution of the paper.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper identifies how adversarial training (AT) algorithms for robustness may negatively affect the notion of robust fairness and proposes two methods ME AT and ME TRADES, which combine existing AT methods with a maximum entropy (ME) term, to improve the accuracy robustness tradeoff and robustness fairness. How do these techniques work and why are they the important ones to consider for fair robust training? Experiments show that combining ME with these AT methods outperforms PGD AT. * The empirical results on how adversarial training increases inter class similarities, which reduces robust fairness, is convincing.<|endoftext|>In particular, the problem studied in this work is closely related to the accuracy parity problem in the fairness literature, yet there is no discussion in this line of works in the related works. Given the lack of technical contributions and the discussion to a closely related line of works in the fairness literature, I would vote for rejection. Besides the label smoothing, the authors also argued that using the maximum entropy regularizer could help reduce the inter class similarity, and then proposed two variants of existing works, i.e., adversarial training and TRADES, with the additional maximum entropy regularizer. The organization of the paper, however, could be improved.<|endoftext|>A few works have shown that the improved adversarial robustness from label smoothing might be the effect of gradient masking and thus LS could be volatile to other attacks. Strengths:  The problem is well motivated, and the authors presented a very thorough analysis on the per class accuracy and inter class similarity on why AT could lead to a trade off over accuracy and robustness. This paper presents an interesting observation that adversarial training might decrease inter class similarity, which might in turn hurt the robustness accuracy trade off. The proposed method that combines label smoothing, or maximum entropy, with both AT and TRADES, seems to improve the accuracy tradeoff by quite a margin.<|endoftext|>The paper provides extensive experimental analysis (e.g., single adversarial training) for the potential reason of trade off between robustness and accuracy and robust fairness. The paper concludes that high inter class similarity might be the main cause. 2.The paper investigates the insight behind why TRADES performs better than PGD AT. 3.The paper proposed new ME based methods to further improve the adversarial robustness over baseline. The main weakness of the paper is its technical novelty. But on the other hand, the paper provides extensive experimental results and a (potentially) reasonable explanation for the big thing in adversarial robustness: trade off between robustness and accuracy and robust fairness.
Reject; rating score: 5; rating score: 6; rating score: 8; This paper proposes to address the complex problem of cross modal semi supervised few shot learning with noisy data. A rather complex approach is proposed with combines Bayesian mixture mode, VAE, GAN, and prototypical learning. However, I have major concerns about the problem formulation, the motivation of the method, and the experiments. This paper aims to address four problems at the same time: (1) few shot learning, (2) multimodal FS learning, (3) semi supervised FS learning, and (4) FS learning under data noise. It is not clear what are the unique challenges posed by each problem, and why this paper combines all the problems together to create a new and complex problem. There need to be more intuitive justifications on the importance of the problem studied. It is also difficult for me to understand what value does the proposed mixture model offer that is specific to the problem studied. The authors either need to adapt the baseline, or provide more justifications on why the baselines are strong enough. This paper has limitations in the problem formulation, the motivation of the method, and the experiments. In my opinion, it needs major improvement.<|endoftext|>This paper focuses on semi supervised few shot learning with multi modality data. In addition, a GAN is developed for the data sparsity in few shot learning. Experimental results demonstrate the effectiveness of the proposed method to some extent. The idea of introducing Bayesian deep learning to the semi supervised few shot learning problem sounds reasonable. How to ensure the relationship between the uncertainty prior and the noisy data? However, it is hard to see the advantages of the robust variational lower bound intuitively from the paper. 5.The writing of this paper needs to be further polished. Overall, this paper provides a new perspective on the cross modal semi supervised few shot learning.<|endoftext|>Post rebuttal comment:The rebuttal has helped to improve the paper. Paper strengths:+ The proposed approach relies on existing methodologies that are combined in a new way. In the introduction, this is can be only implicitly inferred after reading the introduction. It should be clarified what is heterogeneous and why it is novel. The current structure of the paper does not help to understand the complete approach. It would be more interesting to see semantic label noise being considered. It is also a standard benchmark. Overall, the paper addresses the problem of noisy labels next to cross modal semi supervised few shot learning. This problem formulation makes the proposed work different from the existing approaches. The main issue of the paper is the difficulty to follow the proposed approach. One needs to read several times the paper for understanding the structure of the proposed method. This is a major point that needs improvement.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; However, the novelty of this paper is pretty low, as a bisimulation type metric is well known. Though adding a new distributional component in the offline setting helps, it does not well justified at all. Given this, I recommend a weak reject for the paper. The paper is well written and it is easy to follow.<|endoftext|>In UAI, 2004. Weakness:    The proposed BMA is a simple extension of the bisimulation metric [1, 2] for offline RL. This paper has interesting experimental results, but the proposed method and the theory are not novel enough to accept. Strength:    This paper is well written to follow.<|endoftext|>## GoalsThis paper sets out to define a new action metric for large action spaces which aids in learning on both on line and off line scenarios. * Convincing experimental results: Both in the on line and off line case the approach works well, and is shown to work better than other action embedding schemes. * Not always easy to follow explanation of the method. They also show that compared to other action representations their approach performs better.<|endoftext|>This paper proposes a framework for learning Behavioral Metrics of Actions that combines both behavioral relation and data distribution relation between actions to help the learning of offline RL algorithms on large discrete action tasks. The empirical evaluations show good improvement of the proposed method than other existing ones. In summary, I think the proposed method BMA is interesting and the evaluation is promising. Offline reinforcement learningwith pseudometric learning, 2021.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes an adversarial attack, RAP, to boost the transferability of adversarial examples, based on the intuition of flatness of loss landscape and model generalization. Experimental results show that the proposed attack is more effective against real world APIs. However, the technical novelty of the paper is limited. The algorithms to solve the minmax optimization is standard and follows the existing work. Overall the paper is well motivated, and the problem statement is clear.<|endoftext|>Paper proposes a novel adversarial transferability attack (i.e.an adversarial attack when a surrogate model is used to attack an unknown model). Proposed method works by modifying iterative proceduce to find adversarial examples, such that it tend to find adversarial examples in flatter regions of a loss surface. Paper is well written, evaluation is reasonably good and thorough. Results show that method helps to improve transferability of adversarial examples. Authors use I FGSM as one of the baseline attacks, but it’s not clear whether they do random restarts (as described in https://arxiv.org/abs/1706.06083 ). Reasonably good paper. There are few potential improvements for evaluation procedure.<|endoftext|>This paper focuses on the transferability of the adversarial examples and proposes to boost the transferability by reversing the adversarial perturbation. The motivation is that the flatness of the loss landscape can help to alleviate the overfitting to surrogate models, and thus improve the transferability. Specifically,  instead of purely minimizing the adversarial loss at a single adversarial point, this paper injects the worst case perturbation for each step of the optimization procedure. Instead of purely minimizing the adversarial loss at a single adversarial point, the paper proposes to find a flat region of loss landscape by bi level max min optimization, so as to eliminate the overfitting problem of adversarial attacks. The experimental results demonstrated the effectiveness of the proposed method. Overall, the paper proposes a novel adversarial attack, which is effective and easy to follow.<|endoftext|>This paper proposed a min max formulation to improve attack transferability. The key idea is motivated by the fact that the smoothness of the loss landscape could improve model generalization ability. Numerous experiments are provided to demonstrate the effectiveness of reverse adversarial perturbation (RAP). I understood why authors draw a connection between the flatness of loss landscape and model generalization in the context of attack generation. I think this is an Okay submission, but several technical and experimental questions remain.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; In this paper, the authors proposed methodologies to estimate the covariance matrix of embedded features under the meta learning setting. Some discussion and literature review of set input functions, e.g., Set Transformer, Deep Sets will also be helpful. 3.Is there any benchmark on the computational requirement of the proposed methods, compared to competing methods? 4.In the numerical studies, it seems that the proposal performed comparatively better in the low way low shot setting. If so, is there any intuition behind this? The paper is well written with clear motivations and easy to follow presentations of the methodology. The proposal is relatively novel, and could be a meaningful contribution to the meta learning community.<|endoftext|>To solve the challenges that the few samples in few shot learning are usually insufficient for providing reliable estimation to the covariance matrices, the authors introduce meta learning covariance matrices parametrized by a set encoder (implemented as set transformer). Strengths:The overall presentation of this paper is both clear and straightforward. The topic that the authors selected is important and largely overlooked by the research of meta learning and few shot learning. Weakness:My major concern to this paper mainly arises from the use of an additional meta learned set encoder to predict the task specific covariance. It has been shown in previous work that some meta learners have natural advantages in quantifying uncertainty. Additionally, I might be wrong, but I m curious that if this class level covariance can be obtained through aggregating sample wise information so that an additional set encoder can be removed.<|endoftext|>This submission focus on improving the performance of uncertainty quantification and OOD detection using bi Lipschitz regularized neural networks. This estimation is challenging under low sample size. To resolve these issues, the paper focus on two innovations:(i) a meta learning model to estimate the class conditional covariance matrices,(ii) a modification of the inference procedure with the energy function to detect OOD samples. This submission is empirical: the paper is well motivated, and the paper adapts existing, popular techniques to engineer a solution to the standing problem (using quadratic discriminant analysis, modeling the covariance matrix as a sum of a diagonal matrix and a low rank psd matrix, etc.). On the downside, there are several theoretical questions that are left unanswered:1. Why can t they be used in this application? Is it possible to show that the Set Transformer is better than these traditional approach (for any possible criteria)? Thus, I am not fully convinced that a diverse set of eigenvalues is of any advantage. In my opinion, the paper is a good engineering attempt to solve a specific problem. Post rebuttal  The authors have provided further empirical results to justify the use of the ST to estimate the covariance matrix.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The motivation and the results of the paper are very interesting and definitely will be of importance in a more applied community focused on NLP or ML for mental health. I would suggest to the authors to consider a different venue that appreciates this kind of findings.<|endoftext|>This point may not be the main scope of ICLR (i.e., which may put more emphasis on technical contributions,) but I believe it is very important for application oriented papers. Although the paper tackles an important problem, it does not have a sufficient level of novelty and technical contribution. (https://arxiv.org/abs/1906.11565)(W3)I would expect to see new findings of depression detection on Twitter. Pre trained Transformer based language models should be compared.<|endoftext|>So, the novelty of this paper is not enough. This paper proposes a stacked embedding recurrent neural network, named SERCNN, to detect depression from Twitter. The reason why the authors use RCNN and what problem the authors want to solve should be explained, rather than only for getting better results.<|endoftext|>Further, the authors conduct a set of experiments on a public Twitter dataset for depression detection and compare it with different baselines. Overall, I rate this paper as marginally below the acceptance threshold (5). * The authors discussed adequately different related works to the problem of depression detection. This is not clear to me that all models are optimized by the same hyperparameter values, given that these models have different architectures. Here, I suggest the authors conduct more experiments to highlight this contribution in their experiments as well.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The problem of imbuing systems with a theory of mind is an interesting one and one that is getting attention in multiple areas in AI. Unfortunately, the current version of the paper doesn’t contain a related work section. For example, in the paper, there was no discussion on how the current problem being proposed compares to multi agent planning frameworks like DEC POMDPs [3] or I POMDPS [4]. Particularly since one of the baselines used in the paper is specifically a solution method for DEC POMDPs. If that is not the case, then the paper should explain why it is not so. This brings me to the specific problem being studied in the paper. Also, the beginning of the paper makes some connections to human agent interaction, if this is one of the problems the authors are interested in, then they are generally incompatible with any centralized planning mechanisms.<|endoftext|>The authors admit that the proposed SymmTom cannot be completely solved. This work considers a broader range of tasks by removing a pre defined role of the agent. Even though the paper has merit, the reviewer has the following concerns. However, the process that happened in the recharge bases is not fully explained. 4) The experimental results in Table 1 were written with only 3 runs, which is a very insufficient number of trials to explain the claim. 6) The paper should be reorganized. 7) No parameter tuning and hyperparameter setting methods for experimentsThe multi agent reinforcement learning with the Theory of Mind is an interesting topic to be discussed.<|endoftext|>The paper proposes a multi agent environment to develop and test theory of mind capabilities for agents. Most of which are not captured in this paper at all. E.g.I have trouble with the definition or model non ToM. It s also not true that the agent is not "aware". It would be important to have more random seeds and also report measures of dispersion of run outcomes. The discussion of potential tests for ToM in 7.3 is interesting. The proposed baselines should be explored more and specific tests outside of reward should be part of actualy analysisPRO* paper is written well* the challenge is well described  * paper presents some sensible baselinesCONS* no technical contribution* potentially shaky results (not clear if these results are significant)* evaluation relies on reward only. other evaluations are discussed but not really explored in detail<|endoftext|>This paper presents a multi agent environment and task, termed SymmToM, for analyzing machine theory of mind emerged from multi agent RL training. The experimental results show that this extension improves over the generic RNN based extension to MADDPG (i.e., RMADDPG), but is still not performing as well as the simple heuristics based baseline. In CVPR.I think this paper has a lot of potentials but is incomplete in its current form due to 1) a lack of interactive review, 2) insufficient evaluation (missing systematic evaluation of the proposed tests), and 3) a lack of discussion of how scalable and generalizable SymmToM and the approach are. Please see specific concerns in my main review. So this is certainly a welcomed contribution in the area of multi agent RL in my opinion. There should be a more thorough discussion and comparison.<|endoftext|>This paper introduces SymmToM, an environment aimed at benchmarking agents  Theory of Mind (ToM) capabilities. With this, it defines the environment, a simple grid environment with several modifiable parameters. The paper makes a compelling case for SymmToM with both the motivating discussion and the results. It is thus a bit unclear just how much this is capturing in our intuitive, psychological notion of theory of mind. Failing massive amounts of coordination, planning in this environment seems to be very challenging as well, easily being difficult for long range planning algorithms simply because of the number of subgoals one needs to achieve in order to get the recharge reward. If it turns out that the models are already estimating K well, then this is perhaps more a benchmark for other challenges, not ToM.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper studies fairness in generative models. They assume that they have access to scarce reference samples that are balanced. They measure the unfairness of the model through total variation and propose a fairness aware model that is regulated by total variation distance between the generated samples (biased) and reference samples (unbiased). The authors further assume that the sensitive attributes are not available. This paper is not ready to go under review. I would suggest the authors improve the presentation of their paper.<|endoftext|>This paper proposes a new generative model with “fairness” constraints by adding a total variation regularization. Overall, the paper is well written and easy to follow. The problem of studying fairness in generative models is significant. However, I have the following major comments. In particular, I have the following main concerns. 3.The authors observe that “the fairness performance exhibits slight degradation with an increase in the reference set size” in the experiments. The definition of fairness adopted in this paper needs further discussion.<|endoftext|>This paper focuses on the fair generative model problem. In particular, "fairness" lies in the balance of representation among groups in the data. Overall, the idea and approach are clearly presented. Is it all about the TV metric, which does not seem to be the case based on the msg conveyed by Table 3, or something else? It would be very helpful if the authors can kindly comment on the questions in the `Main Review`, so that the significance of the results can be better appreciated.<|endoftext|>In their paper, the authors propose a novel generative adversarial network (GAN) which, more than realistic outputs, promotes fairness in the generative distribution. _I wish to end my main review by stating that I look forward to discussing further with the authors and reviewers of this paper. This is a clear step forward from existing work. There are some remaining questions I have regarding the paper in general (listed in my main review above). However, I do believe the points I raise can be handled during a rebuttal period.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; Maybe further investigation on this would provide some ideas for future exploration   which are also missing in the paper. The paper introduces novel frameworks to evaluate and perform slice discovery. The understanding of the paper suffers from the way how it is organised and should be further improved.<|endoftext|>Slices that contain examples on which the model underperforms, or has a high error rate. The proposed method could also quantitatively compare SDMs, which has not been done before. Then, the authors could summarize their work in the conclusion instead. Are textual descriptions of the Slices actually actionable for real life experts? I would tend to accept this paper as it is novel enough and supported by empirical experiments.<|endoftext|>The paper propose a framework for identifying on which subsets of data machine learning models make systematic errors. Also, the paper is often written in a confusing manner that makes it difficult to follow and understand the the contributions of the paper. The authors term this as a "slice discovery method" (SDM). The definition of the slice discovery problem (section 2) is rather imprecise and uses formulations such as "exhibits degraded performance".
Reject; rating score: 3; rating score: 3; rating score: 5; The authors make use of a theoretical framework for viewing neural networks as representation of quivers to introduce a reparametrization strategy for neural networks with radial activation functions. This reparametrization is based on a QR decomposition, and leads to a lossless compression of the number of parameters by factoring redundant symmetries of the model. Additionally, I feel that the overall results in this paper are not surprising (given the assumption on the activation functions), and I fail to see how such a phenomenon may generalize to typical activations encountered in practice. **On the “radial activation” assumption and generalizing to further activation functions. **The authors make use of a fairly new formalism of viewing neural networks as representation of quivers.<|endoftext|>For lossless model compression, an algorithm employing QR decomposition for parameter compression is proposed. The proposed neural quiver is a nice application of matrix representations in NNs, which have been studied in the literature as reviewed in the paper. However, the proposed theoretical and experimental results should be improved to justify the proposed main claim, which is proposing a framework for efficient model compression. However, the proposed theory and its experimental analyses are incomplete. To improve theoretical results, I recommend first explicating the theoretical results in comparison with the related work.<|endoftext|>I am not saying that there is not one, and in fact deep learning has benefited a lot from the many approaches brought in from other disciplines, but I believe that the paper would benefit immensely from an even smoother introduction of the topics that are not commonly used in our literature. However, the authors may (and perhaps should) cite subsequent work that does prune before training [5 8]. The paper somewhat implies this work as the first lossless compression approach of neural networks: "leading to a lossless model compression algorithm [...] Whereas previous approaches to model compression are based on weight pruning, quantization, matrix factorization, or knowledge distillation."
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes a degradation attack on certifiable defenses GloRo and randomized smoothing aiming to find correct inputs within robustness region but hard to certify, causing rejections. The proposed attack is simple which is just a smoothed variance of PGD attack. Experimental results show that the two evaluated certification methods are susceptible to this attack. Experimental results show that randomized smoothing and GloRo have degraded verified rates under the proposed attacks. I doubt the practical usefulness of this degradation attack. In practice, verification is a plus for the model robustness evaluation. It is safe to trust the results if a sample is verified but a sample that cannot be verified does not necessarily mean we have to reject it. There are many other ways to measure the robustness and give confidence scores of the predictions. 2.I do not think the example in figure 1 makes sense. When the ground truth robustness region of x  is already crossing the decision boundary, it is truly an adversarial example. Please note that over cautiousness is the intrinsic property for certifiable defenses and should be the correct thing to have. The local verified robustness should only focus on the samples belonging to the original input distribution (x belongs to M) while the verified robustness around other samples like perturbed ones that out of the distribution (x  not belongs to M) should not be encouraged. 3.The smoothed PGD attack has very limited novelty. The attacks should at least discuss and evaluate over [A]. 4.The proposed attack should really be discussed and evaluated with other SOTA complete verifiers like alpha beta CROWN [B, C] (the winner of neural network verification competition this year) or incomplete ones like CROWN [D] and K&W [E], and certifiable training methods like CROWN IBP [F] and IBP [G]. Also, I am not very convinced by the motivation of the proposed methods. Post Rebuttal  Thank the authors for the extensive discussions. Some of my concerns have been clarified in the discussion and the revised submission. Some insights of the paper are quite interesting. However, I am still concerned about the novelty and SOTA evaluation of the proposed methods.<|endoftext|>This work investigates an observation that certifiably robust neural networks flag inputs for which local robustness checks fail, but are classified consistently with all valid inputs within a distance of $\epsilon$. Given the commentary prior to experiments, the authors should also evaluate without re training all models using twice the values. While a number of concerns on clarity, the overall observation made by the authors is simple, useful, and evaluated thoroughly. It is, however, unclear whether the observation is novel under the current manuscript. "None of these definitions capture the idea that, to be protected from adversarial examples, a model only needs to be locally robust at every input in the support set, M." This makes the claim on novelty unclear. The formalization of the support set was only present in the presentation, it was not leveraged in neither the proposed attacks nor the proposed defenses (as sec.4.2 leaves it to future work). The authors need to further clarify the contribution relative to the existing works from the perspective of the proposed algorithms evaluated in the work. While Figure 1 is clear, it would be useful if the motivation could be based on a practical example that could come up in practice, s.t. the practical application of the method presented in the paper is clear to the reader. Can the authors clarify why Algorithm 3.1 is effective? For attacking stochastic inputs, the authors should cite any attacks which have used a similar formulation. Evidence of a literature review should be provided. $\text{test}\_{A}$ is identified by the degradation attack algorithms presented in the earlier section in 3.3.1, and $\text{test}\_{A}$ is identified by modifying the local robustness check in 3.3.2? Regardless, these sections should be integrated and presented more clearly. The commentary on "overapproximation" requires an extended discussion, where the concepts are introduced in a less rushed manner. While emphasising the different ways these models are evaluated is appreciated, that by itself isn t a reason to present results in different ways. If by "reflection" the authors are implying they have to present results in different ways, then this should be explicitly clarified. Likewise, if the authors can make the plot made for randomised smoothing for GloRo Nets, they should do so.<|endoftext|>The paper considers a different perspective of attacks to certifiably robust ML models at test time   the attacker can add imperceptible perturbation to force false positive of adversarial example (AE) detection, and thus lower the utility of the model/detection mechanism. It finds an empirical lower and upper bound to the degradation in performance, and validates the efficacy of such attacks against popular detection mechanisms. Since the learner only gets the AE at test time instead of the original input, some information about the original input is already lost. However, I find the discussion for the double radius defense confusing. From Algorithm 2.1, it seems that a certified run time defense will abstain at a radius. However, in Section 4.1, the models are said to be "trained" with parameter $2\epsilon$. If yes, then it just opens a larger feasible set for degradation attack check. Could you please explain this more formally with helping lemma stating the guarantee of the defense mechanisms? My other main worries about the paper are on the experiment results and design. 1) What s the choice of your attack parameter $\epsilon$? The choice now looks quite arbitrary. Is there any pattern/conclusion we can find from the gap? 3) How about the result for other $\ell_p$ distances like $\ell_{\infty}$? The idea of degradation attacks on certifiably robust AE detection is novel. However, the experiment setting is a bit too simplified. I recommend the authors to strengthen the experiment part so that the severity of the threat is better motivated. I m giving a marginally below acceptance for now, but I m willing to increase the score once my concerns are addressed.<|endoftext|>PGD will fail to find an adversarial example at such points of course, but checking whether the (failed) attack point is still certified robust would be an interesting measure to see how more efficient at finding such points the algorithms proposed in the paper are. Further, one could consider for the other points in the test set, how effective the proposed algorithms are at finding adversarial examples compared to vanilla PGD. The presentation of the paper was generally clear and well laid out. Given this, in section 3.2.2, it is stated that if a point x is 2*epsilon robust, then every point within the epsilon ball around x is epsilon robust. Though the idea of the paper is interesting, and the basic attack techniques proposed seem effective, I had, besides some more semantic issues, some concerns with the correctness of the argument of the paper with respect to the upper bound determination. But it is then said that “and so (the classifier) cannot be forced to unnecessarily reject the input”. Edit: The author revisions have addressed some of the main issues with the upper bound determination (including a discussion of monotonicity). Similarly, the statement in 3.2.2 that “if the 2*epsilon CR check fails at x, then even though the model is epsilon robust at x, there exists an x’ in the epsilon ball at x where the model is not epsilon robust” (paraphrasing a bit). We can find the set of points on which it is certified robust by our check, but this will presumably miss points at which the test fails even though it is robust. This establishes a lower bound on the fraction of test points which are vulnerable to degradation attacks. Hence I question the wording of the title of this section and the way it is presented. It is claimed (rightly) that an input point may be perturbed within the epsilon ball and no longer pass the robustness check, even though the original point did. So the perturbed point is then flagged as not certifiable. In this case (the attack), it no longer is certifiably robust, and the system indicates this. The statement (section 5) that the system can “flag even non adversarial inputs as adversarial” I feel then is a bit of a misrepresentation of what it is doing (And to call these perturbed points non adversarial inputs also is a bit disingenuous.They are constructed to mislead the system and so, in any reasonable sense, are adversarial).
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; My main criticism is that the paper was quite terse and hard to read on a first pass. The paper uses tools that are likely to be unfamiliar to many in the ML community with relatively little introduction. I think this could likely be remedied by moving some of the details that aren t core to the paper to the appendix, and spending more time on exposition during the main body of the paper. In this setting, which the authors call UserRec, the recommender system must determine whether to recommend an item to a user at a specific point in time given their prior interaction history, or whether to delay the interaction.<|endoftext|>Weakness:  It is acceptable to me but the position of this paper might be a little bit awkward   it s apparently different from traditional item recommendation scenarios, attempts to address the marketing problem, but doesn t touch the causal aspect or deep dive into the applications such as online advertising or push notifications. The proposed methods were tested on three real world recommender system datasets against standard baselines on both item and user recommendation tasks, in both offline and online matching experiments. As a researcher working in industry, I d like to emphasize that addressing the producer/marketer utility is crucial not only for the business success, but also for the market fairness as well as the long term health of the ecosystem. This property is consistent with the user inductive experimentation setup in the paper, also close to the real world online scenarios.<|endoftext|>Specifically, the authors convert ItemRec to UserRec based on the temporal point process with the proposed RIM. The authors evaluate the approach on three widely used datasets. 2.Modeling users  behaviors by a temporal process makes sense, and it has been demonstrated effective in many works. Baselines are weak and old. It is obvious that RNN will have better performance since all of the other methods cannot leverage temporal information. Even some basic explanations, such as the difference between UserRec and ItemRec, are not well presented. Therefore I have raised my score. Overall the paper is not written well. In the current review stage, I would like to recommend rejecting this paper, waiting for author feedback to check whether my concerns can be addressed or not.
Reject; rating score: 3; rating score: 3; rating score: 3; The paper constructs a classifier of whether or not an image is offensive. The contribution of the paper is not wholely clear to this reviewer. (3) applies it to ImageNet and provides a few categories with images deemed immoral. I am left essentially knowing that a classifier can be built to predict morality based on an obscure dataset. Technically, the paper combines currently trendy and exciting techniques in fairly straightforward ways to create a classifier from little data for morality judgements. This technical contribution is not fully explored, and in itself does not appear novel.<|endoftext|>Lastly the authors choose ImageNet for a proof of concept validation and show that the proposed approach can discover previously neglected offensive images. This paper focused on an important aspect of fairness/dataset bias in modern deep learning models, and showed an interesting direction to reveal questionable instances from the dataset. The technical contribution of this paper is very limited. Although it s arguably as important to identify new question and explore new application of existing techniques, essentially what this paper did is just an extension of CLIP based models. Maybe this work is more suitable for a dedicated conference/workshop rather than a generic top tier AI conference like ICLR. This is different from the true offensiveness detection. Alternatively a user study might be useful, that within the 40,501 detected images how many are truly "offensive"? For top tier AI conference like ICLR this paper is not good enough.<|endoftext|>Since “offensiveness” is a concept that is a function of social norms, over what period of time are we considering or is the modeling a function somehow of time? > "...we demonstrate that our approach can be utilized to annotate offensive images in vision datasets and, therefore, reliably assist the curation process of such datasets." The logic seems be that large models trained on large data have undesirable biases and to address this, they propose to use another large model trained on large data to filter out the undesirable biases because they believe a priori that it, like other such models is attuned to the undesirable biases and can be used to pre train a supervised task to identify one dimension of these biases: “offensiveness”. I will grant that the authors acknowledge this in the Ethics Statement, but I actually think it’s far more central the methods proposed and there s a need to address it head on versus just having it as an Ethics addendum. In addition, the ultimate impact of the work is only shown qualitatively with example concepts (and occurrences) that it’s identified in ImageNet1k, but the results of these are not validated and its impact on say models trained on ImageNet1k without these “offensive” examples is not quantified.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper studies the double descent phenomenon in the adversarial training, with an aim of explaining the wide observation of robust overfitting. The authors made the connection by showing that with a large enough model, the robust overfitting can be seen as the early stage of an epoch wise double descent. A method is proposed to mitigate such noise and the consequent overfitting. * The experiments conducted in this paper are extensive. 2020.This paper offers some novel understanding about robust overfitting, but the findings are not surprising as similar observations have been made in the previous literature on the model wise double descent. I m also not fully convinced by the equivalent impact of label noise and implicit label noise on the overfitting.<|endoftext|>In this work, the authors aim to show that double descent in adversarial training might be caused by implicit label noise, that is, the distribution mismatch between the true label distribution and the assigned label distribution of the adversarial examples. They empirically support this claim by showing that training with static adversarial examples would also encounter the double descent phenomenon. To further solve this problem, they propose to apply temperature scaling and interpolation to create a soft label for each adversarial sample in adversarial training. This work provides a novel and interesting analysis for explaining the robust overfitting problem of adversarial training. The authors give a clear analysis from the perspective of implicit noisy labels and further propose a simple method based on the analysis.<|endoftext|>This paper proposes identifying the robust overfitting as the early part of epoch wise double descent, which is caused by implicit label noise derived from the mismatch between the true label and assigned label distributions on the adversarial examples. The authors further provide a method that combines temperature scaling and interpolation to mitigate robust overfitting. Both theoretical investigation and experiments on realistic datasets are included for demonstrating the effectiveness of the method.<|endoftext|>This paper demonstrates that "robust overfitting" during adversarial training is an early part of an epoch wise double descent phenomenon for relatively large models. The authors explained this "double descent" phenomenon using "implicit label noise" and finally demonstrated that by temperature scaling and knowledge distillation, we can significantly mitigate the overfitting behavior for adversarial training. The paper is well written and the proposed technique to mitigate the overfitting behavior for adversarial training is clearly motivated in sections 3 and 4. However, I have a few concerns regarding the experimental studies and comparative methods: 1. 4.Appendix should have been submitted along with the main paper, after reference.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The authors describe a "focused" belief propagation strategy for learning NNs. Novelty:  The general approach (with the prior) seems novel to me, but it is a bit unclear exactly what other pieces are novel here compared to existing work. Significance:  While it is interesting that a BP style message passing approach can achieve comparable levels of performance to SGD, the overall significance of the works seems somewhat limited. In particular, I m not sure that the authors really present a compelling example of when this approach would be preferred over pure SGD based solutions. The experiments make claims about deep neural networks, but most of the experimental results are on shallow networks. The number of repeated runs in the experiments (5 in some cases) might be a little small. An interesting approach that is hampered by a poor presentation, somewhat limited experimental results, and only minor justification for why it should be seriously considered as an alternative approach to more well studied approaches for training neural networks.<|endoftext|>This manuscript provides an interesting try on alterative training algorithms for deep neural networks, based on (approximate) message passing algorithms based on the well known belief propagation (BP) algorithm. **PMF  is proved to be equivalent to a proximal version of the mean field (MF) method**. **Strengths of the paper**   A new paradigm of deep neural network training is prosed based on well known belief propagation, which is a very interesting and positive try. If so, then maybe the results are expected to be similar or the same? Indeed, it is noticed that the authors had already made an effort in this direction, i.e., the evaluation of local Bayes error and local energy, and the application in continual learning,  similar to the Bayesian neural networks. EBP also has a forward pass and backward pass as the paradigm in the current manuscript. 2.In the experiment parts, e.g, Figure 1 and Table I, why results of BP are missing? Please illustrate clearly in the main text. In particular, for binary neural networks considered in this manuscript, there also have been some studies on SGD based Bayesian training algorithms well adapted to continual learning, e.g.,[5]. Is it using the learned posterior of the previous task as prior over the next task to learn, similarly as [4,5]? [13] Osawa, Kazuki, et al.. "Practical Deep Learning with Bayesian Principles". This paper applies the well known belief propagation (BP) algorithm and several variants to train deep neural networks with binary weights. Several closely related works on previous attempts in training deep neural networks using BP like algorithms are missing. In addition, given the comparable (or slightly worse performance with higher complexity) but not competitive results of the proposed BP based algorithms, it is worth a discussion of the intuitive reason behind it.<|endoftext|>[Summary] This paper develops a class of fBP based message passing algorithms by adding a “reinforcement “ term to the BP equations and shows equivalent performance to the binary networks in experiments. [Main Strengths] This paper s main strength is that the authors  method of using message passing to train NNs is pretty interesting, the introductory section is well written, and the implementation GitHub repo is intended to be provided. [Main Weaknesses] The fundamental shortcoming of this study is that, while the basic concept is intriguing, it does not appear to make a significant impact, much like the various flavors of SGD. 2) Furthermore, substantial message passing successes have occurred in the past, particularly for sparse factor graphs. As a result, future applications of Graph neural networks or sparse Transformers will be quite fascinating. Using message passing to train NNs is intriguing, but, like the various flavors of SGD, it does not appear to make a significant difference.<|endoftext|>This paper introduces a belief propagation message passing training algorithm for multi layer neural networks. However, the paper s theoretical contributions are somewhat limited as they seem to be very similar to the contributions of (Baldassi et al., 2016). Although the authors claim that the novelty of their approach is that their results apply to mini batch training and deep neural networks, it is unclear how the results derived for deep neural networks differ from those derived for shallow ones in (Baldassi et al., 2016). In the first numerical experiment, the proposed method does not have any advantages with respect to SGD other than a lower training error. The authors mention that this could suggest that their algorithm leads to better NN capacity, but this is not explored any further in the text. Although the paper is a close extension of (Baldassi et al.2016), the idea to use belief propagation to estimate the marginals of the weights of a neural network (NN) in order to make probabilistic predictions is a novel paradigm and the empirical results on continual learning seem promising. I recommend acceptance subject to the clarification of the concerns discussed in the main review.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper poses an interesting and important question   where are the bottlenecks in long tailed classification. The authors use empirical experiments to show their observations: (1) representation is more critical than classifier, (2) data augmentation is helpful. The paper defines several simple mathematical and statistical metrics to measure the differences between representations. The useful insight from this paper is limited. I did not get the motivation why the authors come up with these metrics to show the differences between representation. The citation format is not consistent. An overall feeling is that the paper is an ongoing work and needs to be carefully written and improved. My recommendation is to reject it in the current form.<|endoftext|>References:[1] Deep long tailed learning: A survey. [2] Exploring balanced feature spaces for representation learning. This paper seeks to empirically investigate the importance of representation learning, which may provide a new understanding of deep long tailed learning to the community. 3.This work also analyzes the influence of data augmentation on long tailed representation learning, which provides a better understanding of data augmentation in deep long tailed learning. [5] Distributional robustness loss for long tail learning. However, this argument is too strong and the obtained results cannot support it. 3.The vital problem in this paper is the used balanced set: i.e., CIFAR 10/100 and ImageNet 1K. Please note that the data number of  CIFAR 10/100 or ImageNet 1K is much more than their long tailed variants, i.e., CIFAR LT and ImageNet LT. The experiments would have been more persuasive, if the balanced training set is a variant of the long tailed training set with a similar total data number but each class has the same/similar data number, like [1,2]. On ImageNet LT and CIFAR100 LT, the variance trends of long tailed representations and ideal representations are quite consistent. Figure 3 is not clear enough.<|endoftext|>This paper tries to prove that there is a bottleneck in feature learning for long tailed classification and data augmentation can help relieve the issues in long tail feature space. The weakness: 1) The second objective of this paper is to discuss "why data augmentation helps in representation learning". However, in the paper, only positive effects from data augmentation were shown, the reasons and mechanisms were not fully discussed. How good is good enough? I want to see results when unseen samples are added to D* as well. I think the intuition of this paper is not clear and the experiments are not persuasive.<|endoftext|>The authors study the long tail dataset problem in order to determine the true bottleneck for the task. I really enjoyed reading the paper. It has a very clear direction from the beginning with good experiments to back it up. I believe long tailed classification is an interesting problem with clear real world applications, so studying it in depth is necessary for the community. However, apart from difference in the shape of distribution (long tail vs balanced) there is also difference in the amount of data between those two which might play an important role, especially when considering learned representations. It would be good to see what is the difference as well between two datasets that have equal number of examples, but different distributions: normal ad LT. Since otherwise the authors conclusion might raise a question. It would be good to see what the authors think about such direction and what impact on the feature space and measured statistics it would have.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; 4.What does the active regularization period correspond to in terms of the progress in loss or performance? In particular, Theorem 1 provides an interesting relation between eigenvalues of (per sample) logit Hessian and corresponding probability output. When is this valid in general?<|endoftext|>The paper investigates the dynamics of leading eigenvalues in the so called logit Hessian matrix and tries to correlate that with the convergence to a good local optima. This notion is not defined, there is just a bound of the top eigenvalue in terms of Gini impurity.<|endoftext|>In this paper, the relationship between the Jacobian (the gradient of the final activation w.r.t parameters) and the Hessian is analyzed for the softmax cross entropy loss. It is interesting to see how the diversity (i.e.entropy) of the softmax outputs is connected to the sharpness of the loss landscape. Why you can do that? Why the sharpness in the early phase influences the final performance? The same paragraph says "λ acts as a regularization coefficient".<|endoftext|>The authors study the largest eigenvalue and eigenvector of the Hessian of the loss function. They show that GD has implicit regularization effects on the Jacobian norm weighted with the impurity of the probability output, which is also related to the Fisher information matrix. A more analytical example is needed to improve the current paper. E.g., how do the neural network structures and activation functions take effect in the logit Hessian matrix?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The premise of the paper is that true negative examples should be considered in ELECTRA style token level discriminative pre training. The framework uses cosine similarity to find true negatives and softly regularize with semantic distance, or finding true negatives by synonyms and simply ignoring the prediction before feeding to the loss function. Results on GLUE and SQUAD with ELECTRA based models show that handling true negatives during pre training improves performance. Strengths  S1   The idea of handling true negatives for ELECTRA pre training is an interesting idea. S2   Experimental results show that proposed pre training objectives can improve the performance of ELECTRA. If not, the results are not convincing. what about (1) random (2) false negatives? Q1   It would be great if the paper could provide the correction statistics of HC. Q2   ELECTRA style token level discriminative pre trained models are showing good performance in token level classification tasks such as NER.<|endoftext|>This paper proposes to avoid false negative examples in language model pretraining via modifying the training objective. Two methods, referred to as soft regularization and hard correction, are proposed to reduce the false negatives based on continual training on the ELECTRA model. This could benefit the performance on downstream task in some cases. But the limited experiments in the paper do not assure that this is a general approach in pretraining that can be widely adopted. The motivation of reducing false negative examples is reasonable but still does not convince me that this is necessary in pretraining, given that the main goal of pretraining is to learn good contextualized representation that could be useful for various downstream tasks. The authors claim the false negative examples are harmful to pretraining, but it is not fully supported by the experiments in the paper. The synonyms from the dictionary could also be wrong without considering the given context. This is a great limitation as a pretraining strategy, as it is not clear at which training steps one can use it. 2)	The results in Table 2 are inconsistent with Table 1 (ELECTRA_small and ELECTRA_base on GLUE test set). Moreover, since all the experiments are conducted on ELECTRA small and base model, it is unknown whether the proposed methods work on larger scale models and other types of pretrained models such as BERT. My current recommendation to this paper is reject.<|endoftext|>Empirical results on GLUE and SQuAD show that the proposed solution improves ELECTRA small and ELECTRA base models for both performance and robustness. 2) The experiments are conducted on two benchmarks and the proposed methods show consistent gains over a strong baseline model. Weakness:1) For the soft regularization in equation (6), I am not sure how this loss can be backpropagated since the token prediction step is a discrete argmax function. 2) For the hard correction, it is unclear whether the WordNet will also bring false positive, i.e.a synonym which should not fill in the mask. Could this method be applied on other MLM PrLMs such as RoBERTa? 4) I also expect some statistics of the hard corrections in the pretraining experiment. Presentation Issues:1) In table 2, the test set result of ELECTRA_small on QQP is the best but the number is not bold. The paper proposed two simple methods for a novel issue in PrLMs. The empirical results are promising in two benchmarks with ELECTRA model. However, more technical description, empirical analysis, and experimental result are still needed to fully support the claim of this paper.<|endoftext|>To tackle this problem, the author proposes two methods: 1) the Soft Regularization method minimizes the embedding distance between the predicted and gold tokens; 2) the Hard Correction employs WordNet and maximizes all the synonyms of the gold token. They demonstrate consistent gains and better robustness by applying this to pretraining ELECTRA, and testing on GLUE and SQuAD downstream tasks. The false negative problem tackled in this paper is critical for pretraining and seems to be ignored by the mainstream. First, for the Soft Regularization method, it is not clear how the loss L_reg backpropagates through the model. It doesn t make sense to me that this can improve the model. 2.For the Hard Correction method, it looks to me like the author is just using a synonym set as the target labels. 3.The author mainly compares their method with the original pretrained model itself, rather than other methods in solving false positives. This paper studies a critical problem of false negatives in pretraining language models and demonstrates good results. But the proposed solution lacks enough novelty and might have some technical flaws.<|endoftext|>This paper focuses on the issue of misconceived false negative predictions and encourage pre training language models on true negatives or more true negatives to ensure the auto constructed data in masked language modeling (MLM) is true enough. A motivation behind this is that  false negatives may potentially hurt the pre training in both efficiency and robustness to a great extent and this vulnerability of pre trained language has never been studied before. Based on ElECTRA, the authors present two alternative pre training objective auxilary to MLM one, which aims to make the model focus on true negatives during MLM. The first one, dubbed as soft regularization (SR), measures the distribution similarity between the predicted token and the original one, to smooth the tough cross entropy by minimizing the semantic distances. The other, named as hard correction (HC), hinder the gradient propagation of the false negative samples to further avoid training with false negative predictions. Cons and Questions:  The motivation of this paper is not that convincing to me. I fully agree "it is critical to ensure the auto constructed data is true enough" but how can we measure the severity of "false negative problem"? There are also some weaknesses in the proposed pre training objectives. As for SR, the regularization is only applied to embedding layer. Why did the author only apply the proposed pre training objectives on the top of ELECTRA instead of standard MLM, e.g., RoBERT. The latter should be a better testbed for the objectives targeting problems in MLM.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes "generalised kernel thinning", which builds on the "kernel thinning" (KT) algorithm of [Dwivedi and Mackey, 2021]. Firstly, the paper was written in a clear manner, with a very convincing motivation of the problem. The significance of the theoretical results were clear, and as far as I could see, the results were correct. The results are significant, and as far as I can see, correct and sound. I recommend this paper to be accepted.<|endoftext|>The paper s claims seem correct and sound. Claims are substantiated by a theoretical analysis, though the bulk of the paper s ideology comes from the previous publication (which the current paper extends). The paper seems as an extension of results of a previous paper. The algorithm for finding a subsample is the same, the first part of it (KT split) is applied with a general kernel (split kernel), which may differ from square root kernel. Is there any explanation of that? I believe this is a borderline paper. A new analysis is presented (based on RKHS covering number).<|endoftext|>In terms of writing, the paper is quite technical and lacks intuition. I also have some concerns regarding the significance of the results. As for the current submission, the improvements are in the log factors. The second contribution of the submission is  "we show that, for analytic kernels, like Gaussian and inverse multiquadric, target kernel KT admits maximum mean discrepancy (MMD) guarantees comparable to square root KT without the need for an explicit square root kernel." The experiments seem to be generated by the same software/scripts Dwivedi and Mackey (2021) used. I would think that a more significant contribution would be reducing this algorithmic complexity, especially important for practical applications. The writing is also technical and lacks intuitions and insights. I suggest the authors focus on improving the quadratic time complexity instead, which should increase the significance/popularity of both works.<|endoftext|>This article studies a generalization of kernel thinning previously proposed in Dwivedi and Mackey (2021). The authors proposed a new variant of kernel thinning that does not require the knowledge of the square root of the kernel, for which they proved single function guarantees as well as the MMD guarantees. Up to my knowledge, the proposed algorithms and the corresponding theoretical guarantees are new in the community. The presentation of the paper is non conventional.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper utilizes a transformer architecture based on T5 to generate tokenized IUPAC names of molecules. I have a number of smaller comments below  The advantages of using IUPAC names are not clear at all. I think the idea of using IUPAC names is intriguing, but neither the theoretical arguments not the empirical results (due to lack of benchmarks) are convincing. If the authors believe those benchmarks are relevant, then it is more appropriate to make a case why explicitly (and probably use them anyway)Furthermore, making local modifications to starting scaffolds has a long history in cheminformatics.<|endoftext|>In this way we can appreciate how significant the results are, especially given that the method proposed in this paper is self supervised. The sequence representation of molecules is based on IUPAC names, which can be more semantically meaningful and much easier to model than the SMILES or graph based molecule representation. **Summary**This paper proposed a way to modify the molecule based on language pretraining techniques. The preliminary results are interesting, but it would be more solid with more quantitative comparison with existing methods on existing benchmarks.<|endoftext|>The specific difference of this paper is the IUPAC names (a standardized molecular representation), and the method is totally self supervised. 2.The method is straightforward and simple, which uses a pre training method T5 to model a conditional language model, where the condition is the property value. Though I feel happy to see a new representation method that seems to be better than other methods, the comparison is not clear and there seems no strong evidence or numbers can be shown in the paper. In this way, this can not clearly convince the superiority of IUPAC. 4.It is required to make a result comparison to see the advantage of this method. 5.Other concerns have been discussed by the authors, for example, the training cost is high; it is not always possible to generate valid and satisfying molecules since there are no constraints in the modeling. I personally feel good about this work, but unclear parts are main concerns and there are no comparison with other works.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper proposes a dequantization scheme for categorical data, where unlike with ordinal data, element wise uniform noise cannot be used. The paper is mostly well written and easy to follow, although I have some reservations about the motivation:1. Is it really better to have the flow as compared to increasing capacity for m and s? 2.Part of the motivation is enabling the dequantized variable s dimension to not depend (logarithmically) on the number of categories. Finally, I did not find the experiments particularly convincing: while the proposed method does seem to be slightly better than the baselines at the evaluated tasks, the improvements seems small and do not in my view compensate for the issues I have with the motivation. UPDATE AFTER REBUTTAL I have increased my score after seeing the authors  updates regarding motivation and the promised ablations regarding point 3 above.<|endoftext|>This paper proposes a Flow model called TRUFL designed for discrete data. The main selling point of the proposed model is that it can handle discrete data better than other dequantization schemes. In fact, I can imagine in certain cases we don t need lossless dequantization, as some variation of the data might be irrelevant to a task. However, due to the similarity with CatNF and the additional need for rejection sampling, it is not very clear to me why TRUFL can be learned easily. Although the authors provide two explanations in the introduction, they are not very well justified. Apart from the truncation technique, the proposed model doesn t seem to differ too much from existing approaches.<|endoftext|>Unlike previous approaches, TRUFL aloows the dequantization layer to have a learnable truncated support. The authors perform several experiments to demonstrate the advantages of the proposed method to Categorical Normalizing Flows (CatNF) and Argmax Flows. **Pros:** The paper is very well written and the method is explained clearly, concisely and in appropriate detail. Furthermore, the problem considered in the paper is a valid and interesting problem for modelling discrete data using NFs. The experimental analysis by the authors consider a diverse set of problems from graph coloring, molecular generation, and language modelling with reasonable performances. **Cons:** I found the empirical results to be mixed for TRUFL as compared to Argmax flows and CatNF.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; To deal with this problem, the authors propose to learn a latent dynamics models based on encoded observations. ## Cons/questions/suggestions* __Comparisons that demonstrate the practical benefits when observation quality is improved are missing__ The example of upgrading sensors to improve observation quality is a great motivating example for the problem of transfer RL between different observation spaces. Based on this understanding, I think an experiment that demonstrates the usefulness of the proposed algorithm in this setting would greatly strengthen the current work. ## Minor comments* The authors claim that the proposed method can make it easier to learn a good representation from the new observation space. The novel problem setting proposed by the authors is of practical interests and the authors s proposed solution is well justified.<|endoftext|>This paper proposes to leverage models of the environment and rewards as regularizers to perform explicit transfer between different observation spaces in RL. They tackle it using ideas very similar to the recent wave of work leveraging bisimulation metrics, hence novelty / comparison on this aspect will be important. Overall, I found this work interesting but slightly lacking in focus, and especially the theoretical parts are slightly divorced from the effective implementation. The empirical work is also quite similar to previous work, despite the rather different problem setting considered. However, I am not yet fully convinced of how novel Definition 3 and Lemma 4 really are.<|endoftext|>The problem that the authors consider is interesting and relatively novel. There are three main issues with the paper: motivation, technical clarity, and experimental evaluation. The authors claim that it is an important problem, but I do not find the motivation for this very persuasive. Why are the proprioceptive states of the environments modified? Firstly, I still think that the empirical results are not strong enough. I would encourage the authors to discuss sim to real more, and maybe use some baselines from there. Thus, while their methods improve performance for these toy tasks, I am not convinced their methods will improve a real world problem. While the paper often is well written and considers an interesting problem, the motivation for the problem is lacking. The authors define a representation mapping to be sufficient if it enables accurate Q values. Also, do the theoretical results apply to the empirical setting considered later?<|endoftext|>The paper addresses the problem of adapting a policy to a novel representation of the environment. The idea is to learn source and target models of the environment jointly with the policy. I wonder why the authors formulate the experiment like this. I generally like the idea that the paper follows and its theoretical rigorousness. Also, I think the number of real world applications this solution addresses is very limited, making it a very narrow field of application. However, the results are not interpreted at all? Also: what is the motivation of using DQNs in the first experiment and SAC in the last? The main reason for that is the lack of experimental evidence and lack of comparison to previous work exposed to ablated variants of the proposed problem setting. The originally proposed gradient mapping could also be applied to the embedding where there is a sufficient representation. This work could hence also serve as a baseline. The reason while I am still not leaning towards acceptance is because I still have concerns:The fine tuning experiment is not what I actually meant.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper improves upon Stackelberg Deep Deterministic Policy Gradients by proposing a set of strategies on how to deal with the Hessian part of the gradient. The authors provide a formal justification on why removing parts of the Hessian and using a block diagonal approximation is still achieving convergence. The authors analyzed many different Hessian approximation scenarios and I believe that this line of work can be relevant in order to make the Stackelberg approach feasible for larger neural networks. Given the issues above, I propose to reject this paper in the current version. The authors claim it is the "discounted state distribution of s", but under which policy? How is this affecting the theoretical results?<|endoftext|>Additionally, the authors propose approximation schemes including dropping small order terms and matrix inversion via block diagonal matrix approximation. Strength: This paper seems to provide numerical experiments on Mujoco that are comparable to the state of the art. It seems unclear what advantage such a Stackelberg view brings to the analysis. The main contribution of this work is to (i) additionally use deterministic policy gradient and drop a term involving TD error, and (ii) use block diagonal approximation in the calculation of the inverse Hessian. It would be nice to provide examples on which the assumptions hold. c. The theoretical arguments depend on strong and ungrounded assumptions and the proofs are hard to follow.<|endoftext|>This paper proposes a method to accelerate Stackelberg Actor Critic by ignoring the terms that contain Hessian in the indirect gradient term and applying block diagonal approximation technique to remaining inverse terms. They prove a faster convergence rate of the proposed method to a fixed point. Weaknesses: My concerns about this paper are three folds. Second, the theoretical analysis is pretty weak. They don t show any theoretical results for stability, which is stated as one of their two main contributions (faster convergence rate and maintained stability). The fast convergence rate somehow lacks significant novelty.<|endoftext|>This paper aims to establish a computational efficient Stackelberg training scheme for deep learning based actor critic methods. The proposed approach,  Fast Stackelberg Deep Deterministic Policy Gradient (FSDDPG), considers the block diagonal approximation technique to reduce the training complexity while improving the convergence rate. This paper is well organized, and the discussion of the literature background is thorough. The proposed method is novel and sound, which has solid theoretical analysis and empirical grounding. I recommend acceptance.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The authors separate the modeling of the static background from the dynamic foreground by segmenting out the faces (using landmark detection and fitting). I don t consider the quality to be on par with Nerfies or HyperNeRF. 4.Limited evaluation: there are only 4 subjects being used as evaluation dataMy current recommendation is "3: reject, not good enough". There is limited novelty with important baseline missing (HyperNeRF). The synthesized videos are also blurry with artifacts.<|endoftext|>Considering the novelty and quality in the context of existing methods, the initial rating is reject. 1)  Low resolution: While many previous works introduce high quality view synthesis results with high resolution (512x512 or more), this paper shows low resolution results (256x256) for some reasons. + The writing is clear and easy to follow.<|endoftext|>This is a paper that clearly explains a straightforward but sensible approach. I judge it to be slightly below the acceptance bar mainly because (1) the method heavily depends on the success of two non trivial preprocessing steps and has no way to recover from their mistakes, and (2) the complex dynamics among the expression parameters and deformation fields remains unexplored. The paper makes three contributions: Firstly, it proposes a dynamic NeRF model that supports certain explicit control. This deserves at least some discussions, if not experiments. Fig.5 shows really similar quality results between the proposed method and NerFACE in terms of just the reenactment, although I agree the proposed method does perform better overall with fewer artifacts.<|endoftext|>Both qualitative and quantitative results seem to be convincing. **Strenghts:**:  Writing quality. The paper is well written and is easy to follow. Experimental evaluation. Related work   geometrical prior. Although those works do not model the background, ideas presented in those works will likely generalize better as they also encourage spatial disentanglement between different parts of the articulated object. Thus I am leaning more towards rejecting this paper, but I am eager to reconsider my rating if authors address the concerns raised above.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; This work analyzes the adversarial attacks on BCI systems. Without the effort on physical systems and real world attacks, this paper is still good but not good enough for ICLR. implies that this study has a smart device that can be attached to the left ear that will inject well designed perturbation to collected EEG data. So I decided to give a rejection.<|endoftext|>For instance, authors argue that the perturbations should be imperceptible and smooth. Main limitations of this work is on the presentation of the general motivating perspective, as well as a general lack of methodological comparisons/evaluations and discussions on possible countermeasures that one can consider for DoS attacks on BCI systems.<|endoftext|>Would transformers be more robust to adversarial attacks in this application, given the fact that they were proved more robust in other studies? This is an interesting study, but in my opinion requires some additional work to make the contribution more solid, specifically comparison with other smooth adversarial attacks on electrical signals should be provided, and selection of CNNs vs transformers should be explained. It would be beneficial to a reader to compare those. The flow of the work should be also improved for better readability.<|endoftext|>It includes a proper explanation of BCI, SoA, background of the attack, and a dataset that, although limited, given the scarce public dataset for this domain I consider adequate. The overall contribution is adequate, being my only concern related to the completeness of the work and the overall reproducibility of results.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 10; Efforts are however made in an interesting direction, and the several discussions around the Cheeger constant gives depth to the paper, and its main messages. The smaller this measure, the less $i$ "feels" $s$, the larger the over squashing effect.<|endoftext|>The authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature to mitigate over squashing. This is motivated by a link between negatively curved edges and graph bottlenecks. I found the paper very interesting in that it analyzes the over squashing problems through a new, geometric lens.<|endoftext|>The paper proposes a novel rewiring method based on negative curvature to construct graphs that are less susceptible to oversquashing.<|endoftext|>Is it $\hat{A}$? The paper shows that negatively curved edges are responsible for over squashing. Further, the paper proposes a method (called SDRF) to re wire the graph based on these insights and shows empirically how this method alleviates over squashing.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Many Bayesian models have been proposed for multi task learning. Why does the proposed multi task neural process achieve superior performance? The Gaussian likelihood is not suitable for classification tasks. Authors should consider other likelihood function. In experiments, important baselines are missing. Authors should compare with MTL models such as cross stitch network and MTAN, which can be modified to solve the multi input multi output setting. Moreover, multi task Gaussian process should be compared. This paper proposes a multi task Bayesian model. Important baselines are missing in experiments.<|endoftext|>This paper presents a multi task neural processes approach in which function priors are derived in a hierarchical Bayesian inference framework to incorporate the shared knowledge into its context of the prediction function. Authors introduced a higher level latent variable derived from the context data of related tasks to control the sharing of common knowledge between the tasks. Previous works used context data from its own task to generate the task specific latent variables, whereas this work uses data from other tasks as well. In the main text, on page 4, it seems to suggest the \mu of and \Sigma of \psi are functions of m_l. (2) In section 3.1 it is mentioned that concrete formation of global variable M depends on the learning scenarios. And how they were determined? If both are task specific, I do not see mathematically it would make much difference to train one single network comparing to the two that the authors chose to do. More discussion on this would be very helpful. al., 2020, ICML, Sun et. al., 2020, NeurIPS, and Yu et. The results could be more persuasive if tasks from diverse domains are included.<|endoftext|>The paper proposes a multi task learning model with neural processes. The model is based on a hierarchical construction whereby each task is conditioned on global and local information. **Strengths:**The general idea of the model seems to be a valid one for multitask learning, and to my knowledge, it seems original. For the most part, the experimental section shows the strength of this model and the performance gains it brings against some other models in the literature. **Weaknesses:**I was a bit confused with the model construction. Is it simply the collection of the context datasets for all the tasks? Or is it a latent variable? I think this needs to be clarified further in the manuscript. There are also some errors in the model construction section. The Gaussian likelihood is used to derive the generic multitask model. While this is a reasonable choice for regression tasks, it is a wrong assumption for classification tasks which are also considered in this paper. I would have liked to see a synthetic example exploring the properties of this model. It also looks like the performance difference might not be significantly different from NPs for this example. How stable is the training of this model in comparison to standard neural processes? In my opinion, this work is original and has the potential to be an impactful contribution. My main issues are the ambiguity in the model construction exposition and the lack of experiments that explore the model’s behaviour and properties.<|endoftext|>This paper proposes a new multi task neural process based on the classical neural process. The idea is to introduce additional global variables to share knowledge from different tasks. That is to say, a hierarchical bayesian model is constructed to link single task neural processes together. The new model is able to handle multi task applications contract to the classical neural processThe strong point is the extensive experiments. The hierarchical model design and inference are straightforward. The authors did not compare with other possible solutions to show why this design is the best. The design of the original NP is guaranteed to be a valid stochastic process. It is pity that this discussion is missed for multi task NP. is any marginal NP on a single task in multi task NP still a valid stochastic process?
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; The paper proposes Fast Advprop which is an modified implementation of adversarial propagation. The paper suggests to apply different existing techniques and small modifications to increase the training speed while using AdvProp. However, each of the modifications is well justified by ablation studies and the overall experimental evaluation is convincing. Table 1 points out the gained/lost accuracy in comparison with the vanilla training for some of the entries. This should be explained in the caption of the table. The paper is well written and the results are good.<|endoftext|>This paper aims to improve the training speed and decrease the computation cost of AdvProp, which is a method that leverages the adversarial example to improve the image recognition accuracy. 3.The paper is well organized and well presented. In this work, the proposed method Fast AdvProp reduces the computation cost by reusing some gradient computation during training. The proposed solution Fast AdvProp demonstrates good performance with less computation cost and can potentially be applied to various vision tasks.<|endoftext|>This paper proposed an improved version of AdvProp, to speed up the training process. The trick used for fast adversarial training has been explored by many other previous works. 3.Some claims and empirical supports of the paper are confusing. For me, AdvProp has more advantages to improve the clean accuracy. a.https://github.com/tingxueronghua/pytorch classification advpropb.Adversarial Masking: Towards Understanding Robustness Trade off for GeneralizationOverall, I think the paper is interesting but has some technical details to be addressed. In addition, the novelty of the paper is increamental.<|endoftext|>The paper is also written very clearly and the motivation is well communicated. This paper is mostly a report on empirical insights, that decoupling or reducing the number of adversarial samples does not really hurt performance that much. Most of the gradient recycling discussed in the paper is prior work but with engineering insights. On experimental results. I take some issues with this "The AdvProp also shows compelling robustness compared with the vanilla training, verifying its effectiveness. I will be interested to see what the other reviewers think and I am opened to discussions and changing my feedback.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 10; This work presents some additional tweaks and tricks in using neural networks for feature selection. The experiments are rather comprehensive and illustrative. I look forward to get some feedback from the authors.<|endoftext|>This paper presents a novel feature selection method for tabular data prediction. It tackles two challenges by respective novel designs, i.e.labeled data scarcity issue by an unsupervised self supervision phase, and feature correlation issue by multivariate Bernoulli gate vector with learnable correlation matrix. The experiments on one synthetic and two medicine/biology datasets demonstrate effectiveness. This paper is impressive.<|endoftext|>This work aims to use self supervision to identify the most useful features for downstream tasks particularly in the context of correlated features. This is an important aspect that is lacking from many feature selection approaches commonly used within the highly correlated datasets of healthcare. Supplement   SEFS outperforms all benchmarks   benchmarks is misspelledI found this work to be focused on an important problem as well as technically interesting and correct.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper gives an important finding that overperformed upstream performance may not transfer well to better downstream performance especially in higher upstream accuracies region. Appendix C and D show an case study of US weight decay in the projection layer of ViT on the effect of downstream DS accuracy, showing that at the expense of hurting US accuracy could instead lead to the improvement of DS tasks. If there is any case study that showing US accuracy can accurately predict DS accuracy but (model size, US data size, compute) cannot? This paper gives an interesting study on the prediction power of US accuracy. Extensive results are provided.<|endoftext|>The paper presents an empirical study (respectively meta study) of large scale supervised pre training for image recognition tasks. By analysing lots of experiments with varying model sizes, dataset sizes and training durations, the paper reaches the conclusion that simply scaling them up for a generic pre training task will not lead to proportional gains for downstream tasks that build on the pre trained model. would be a lot more relevant  the paper is overly dense  and overloaded.<|endoftext|>In general, increasing the scale of these PT models has yielded better performance on downstream tasks, though this is not universally the case. Pre training large models is very expensive. 3) They examine in which cases upstream and downstream task performance may be at odds with one another  4) They profile their analyses under various dataset sizes, few shot settings, and architectures. This problem is very important, and these anlyses targeted to assess a key part of that problem. This presents itself in two ways.<|endoftext|>3.Besides the saturating behavior, some insights drawn from the observations are good to be heard in the community, such as increasing the data density of upstream tasks, predicting downstream performance for a given upstream accuracy, and etc. This paper is just focusing on the task of image recognition.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 1; In this submission, in my opinion, the authors consider using relaxied queries to replace exact labeling for active learner and define a new criterion with IGP  for learning. One the whole, the proposed method can have more flexibility, e.g., being applicable to a large variety of GNN variants. Due to GNN characteristics, IGP inherits possible oversmoothness problem.<|endoftext|>The paper proposes a new innovative approach for graph active learning with soft labels. So it will be interesting to see how this weak labeling by the remaining classes performs when the number of classes is large and hence the prediction problem is hard. The key idea is to ask a human regarding whether the model prediction is correct or not (a binary classification task) as opposed to asking them the correct "hard label" of the node.<|endoftext|>This paper proposes a new GNN based active learning (AL) method for graph data, under a relaxed query setting where the oracle can only judge the correctness of the predicted labels. Under this setting, a new AL query criterion is proposed to incorporate soft labels and information gain propagation. e.g., would it be v_i   in the last term? This paper considers a new AL setting with relaxed queries.<|endoftext|>The paper proposes an active learning method for GNNs that is based on an information gain maximization where ethe information gain is obtained by querying a data point and looking at the influence of the queried node on the neighborhood relative to their previous information. 2.The setting of relaxed queries seems to be facilitative in this case. What about the query criteria itself? What are the probabilities? 6.Experimental validation: I am left unimpressed by the rate of improvement shown by the proposed algorithm.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper attempts to investigate the reasons behind why non contrastive SSL methods such as BYOL and Sim Siam do not collapse to trivial solutions, how they learn representations that are related to the data distribution and augmentation process, and how these reduce the sample complexity of downstream tasks. Overall, the paper is well structured. *Weaknesses*:    *Major*: The paper seems to be a derivative of previous work (Tian et al) and hence lacks novelty in my opinion. Why aren’t more cases with ꭤ > 1 explored? Why is this the case? I think the paper itself can benefit from clearer writing and some emphasis on how their analysis scales up to explain other non contrastive SSL methods.<|endoftext|>In this paper, the authors make theoretical progress on understanding non contrastive self supervised learning (ncSSL). The analysis of DirectSet and DirectCopy succeeds at proving that it can successfully learn a projection matrix onto an invariant feature space subspace, but essentially boils down to a similar approach as DirectPred (albeit more efficient).<|endoftext|>The paper provides a new method for self supervised learning (SSL), called DirectCopy, based on a previous work DirectPred. An important contribution of this work is theoretical analyses on DirectSet($\alpha$), a theoretical model on linear neural networks that work with arbitrary $\alpha$. DirectCopy is a special case of DirectSet($\alpha$) when $\alpha 1$. \textbf{Originality & Novelty}: The motivation of the paper is to understand why non contrastive SSL can learn meaningful representations. This argument is very interesting and deserves more investigation in the future. (3) Another interesting empirical analysis overlooked by the authors is evaluation on more down stream tasks, like object detection.<|endoftext|>The paper "Towards demystifying representation learning with non contrastive self supervision" presents and analyzes a family of algorithms, DirectSet(\alpha), for non conctrastive self supervised learning with only positive pairs. The theoretical analysis assumes linear layers, and focuses on a special data distribution assumption, where the input space is separated in two linear subspaces, one invariant under the  data augmentations , the other being the complement. It is then shown that the proposed algorithm converges to the projection matrix onto the invariant sub space. the empirical analyses follow the same experiments as in Tian et al.2021, with comparable results. Like this it feels like the results are hidden. It presents a nice analysis that illustrates how the representation learning happens in a toy data setting and proposes a slightly more simple algorithm than in previous work.
Reject; rating score: 3; rating score: 3; rating score: 6; Classifying the semantic type based only on the string value is too simplistic. This task is of importance to data cleaning, schema matching,  etc.<|endoftext|>Thus, I wouldn’t recommend publishing the paper in its current form. As discussed in [Zhang et al.(2019)], the table context is essential to semantic type detection. ### Weaknesses  (W1) The novelty and technical significance are limited.<|endoftext|>Main ReviewThe paper studies a very practical problem called semantic type detection. The problem is fundamental to tabular data that consists of columns and headers.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper focuses on the formulation of a graph neural network approach towards the learning of update rules of constrained systems, extending prior work (Interaction entwork, EGNN) by learning the updates of constrained components in the generalized coordinates expressing those constraints and incorporating forward and inverse kinematics in the algorithm. Strengths:The paper is decently clearly written (modulo some typos/phrasings noted below, but nothing that impedes the understanding of the reader), provides all necessary proofs and makes 2 main contributions  formulating an algorithm which incorporates Generalized Coordinates and inverse/forward kinematics  proposes an extension of the EGNN framework from working on single vectors to matrices, including some proofs about the equivariance and expressivity. doesn t change much for the final results but it confused me a bit Typos/phrasings  (not accounted for in my review, just as feedback for the authors):  page 2: "have achieved desired performance on the N body system...lacks of constraint" is meant to  say "have achieved desirable performance ....lack constraints"? page 3 "is proportion to the its" "is proportional to the" ? page 5: "can be generalized to the function with multiple input vectors"  > to functions with multiple input vectors? The paper presents and incremental improvement over previous works and was done cleanly, so a marginal accept. If it had been exploring a fully learned generalized coordinate transformation that uses an inductive bias but does not need to hand design the coordinates and kinematics I d give a higher score.<|endoftext|>The paper proposes a new graph neural network model for predicting the dynamics of n body systems. EDIT: Updated my score to 8. 2.A new 3D transformation equivariant layer with universality properties. This could also be thought of as a novel instance of object centric based learning. Table 2 provides a nice empirical confirmation that the model works as expected and that the constraints are satisfied. The generalization of the proposed models to novel system configurations is tested in Table 3 and the results look good. While these simulations are nice and insightful, I would have expected some real world evaluation as well. Even the original work of Kipf considered a few real world datasets. For instance, the authors could have looked at some motion capture data where different parts of the human body could be treated as rigid body constraints. Related to the point above, the forward kinematics part of the model relies on having a perfect understanding of the nature of the constraint and relies completely on domain knowledge.<|endoftext|>The authors propose Graph Mechanics Networks (GMN), a E(n) equivariant model that explicitly models systems with rigid constraints, with exact constraint preservation by means of using generalized coordinates. In my opinion the main contributions are as follows:C1. Establish math for writing models that operate on cartesian coordinates, make predictions in very generalized coordinates and then converted back to cartesian. model to build an E(n) equivariant model for exact constraint preservation. Actually, if the particle 0 has only 3 degrees of freedom this means they are point particles (e.g.no solid rotation), so would not this mean then sticks 01 and 02, would only have 2 generalized coordinates each (e.g.latitude and longitude), instead of 3? The model requires specifying the forward kinematics function for the constraint (to go from generalized positions and velocities to cartesian positions and velocities) as a differentiable function, which in many cases is of a big ask. W2.The work focuses a lot on the specific combination of adding exact constraint preservation to the E(N) equivariant model. I would perhaps recommend including "E(n)" or "equivariant" in the title of the paper to reach a larger audience. "first recording data and then inducting formulas".<|endoftext|>This paper develops a model that augmented interaction networks with mechanical constraints and develops equivariant message passing schemes to ensure physical realism in the model outputs. The work focuses on a set of toy systems that contains particles connected by sticks and hinges and the developments are made for these systems in particular. The task is well defined and has some physical relevance. **Weaknesses**There is a lack of novelty and scope in the work. The following problems are fundamental to the contribution and it is not clear to me how the authors could remedy these faults within the rebuttal period unless they would like to flatly refute the points:1. 2.The scope of the theoretical and empirical contributions of the paper are limited to very small systems of sticks and hinges. It is also in the spirit of the work presented here to include such a physical inductive bias (i.e.that the rate of change of the velocity $v$ is set to be exactly the acceleration $a$ and so on). I appreciate that the results suggest that the GMN is the dominant model in this system but it is still necessary to indicate the variation in the recorded performances.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; They use contrastive learning for this, in conjunction with GAN losses. Their representation can be used in the downstream task of image generation, where they use their representations  strengths to show improved resilience to mode collapse, while displaying better intra cluster variance. The quantitative results for clustering, and the qualitative figures in the supplementary material are both impressive, and they support the authors  claims	The method has novel components which are provably attributed to these results. There are no obvious weaknessesThe paper combines several techniques to achieve unsupervised fine grained clustering of semantic classes. Their representations  high quality is able to drive GAN generation without mode collapse, thereby achieving great two fold contributions.<|endoftext|>+ The proposed method effectively avoids mode collapse while training generators, and are able to generate, with good control, various objects with varying background (while keeping foreground fixed), and varying foreground objects (while keeping background fixed) better than related methods. *The fine grained class clustering is more challenging than coarse grained due to lower sample representation and large scale  and color  variations between the fine grained classes. To this end, the paper proposes C3 GAN which uses the ability of InfoGAN with contrastive learning to learn feature representations that maximize the mutual information between the latent code and its corresponding observation.<|endoftext|>  The authors propose C3 GAN, a method that learns "clustering friendly" feature representation for fine grained clustering (main goal), by learning features of cluster centroids (latent codes) using contrastive loss on the mutual information of image latent code pairs. The method should improve the GAN s performance in terms of image diversity. The method focuses on learning cluster granularity for the object only, and not for the background. Overall proofreading is required. It would be great to add some of the model s notations to figure 2 (e.g.D_base, psi_r, psi_h)I tend to vote for accepting this paper as I think it proposes a great approach and presents a convincing comparative performance.<|endoftext|>This paper studies the problem of fine grained image clustering. There are also some modifications in the foreground and background synthesis mechanisms compared to prior work such as the FineGAN. The method is straightforward and the paper is easy to follow. If the proposed method has a similar performance as others in the coarse setting but a clear advantage in the fine grained setting, that will be a very strong signal. I checked the architecture it seems to be just the discriminator score (a scalar). It should be written clearly L    log q_k for example. 4.Table 1 last sentence: I think OneGAN is an unsupervised method.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This paper presents some empirical observations about the relationship of a persistent homology based measure of learning dynamics and validation set error, during the training of deep neural nets. The main claim of the paper is that   this correlation suggests a conceptual link between the two   quantities. 4.The paper does not seem to connect well with the existing   theoretical literature on the generalization gap in deep   learning.<|endoftext|>The authors tested their method on a variety of datasets. However, I find the paper to be still in early stage of development and still requires work. The authors mentioned "that there exists a high correlation with the corresponding validation accuracy of the model."<|endoftext|>This paper analyses the training of neural networks from a topologicalperspective, presenting a pipeline that can measure (pseudo) distancesbetween the network s weights during training. 2.Experimental depth: the experimental section is lacking depth, in   particular given the specific goals of this paper. The discussion of chains and boundaries requires more explanations (or  could be partially relegated to the supplementary materials).<|endoftext|>I do not understand this claim. The methodology proposed by the paper is original and interesting. (I guess a normalization has been applied). In the same vein, the sentence "*(...) validation accuracy depends on the specificity of the data sampled in the validation subset, while the homological convergence is independent of the validation data*" (end of section 6) is somewhat contradictory with the claim of the paper that homological properties of the network "*captures the generalization of neural networks without a validation set*". end of p7, "the homology does not seem to converge".
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; The theoretical contribution of this paper is good to me (unless other reviewers find technical errors in the proof). I think it is a good supplement to the current theory of adversarial DNN training.<|endoftext|>This paper characterizes the bias of adversarial training toward specific minimum norm solutions or KKT points of a particular optimization problem. Strengths:  The results are novel and extend prior theoretical results. Figure 1: This is an interesting plot confirming the increase in adversarial margin.<|endoftext|>The paper tries to show the implicit bias of adversarial training in a simplified setting with a deep linear neural network and linearly separable data for a binary classification problem. While the results in section 3.1 are written clearly, I think section 3.2 lacks a clear presentation and puts several limiting assumptions.<|endoftext|>The paper s results place a milestone in the theory ofadversarial robustness. This theorem demonstrates that for a class of neural networks and adversarial perturbations adversarial training hasan implicit bias that can be expressed in closed form. *Implicit Bias of Adversarial Training for Deep Neural Networks*contributes a milestone result to the theory of adversarialrobustness. The majority of the paper is clearly writtenand correct.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This paper proposed a recurrent Pade network fro learning non linear operator approximations for IVP. The paper showed that Pade network does not suffer from the issue of gradient explosion and the boundedness of the gradients can be establishedThis paper uses a rational polynomial to approximate the exponential function and proposed a Pade network. Markov neural operators for learning chaotic systems, 2021.<|endoftext|>However, it is less satisfactory not to address the more interesting non linear term N. I recommend borderline acceptance. The paper proposes a novel and natural model for operator learning. 3.The paper gets the state of art results on both PDEs and real life benchmarks.<|endoftext|>The approach follows the basics in PDEs, resulting in an efficient method. There are multiple issues with this paper that I can not recommend acceptance at its current stage. The authors are encouraged to be consistent. 1i) what is F in the proof of the theorem? what are the a and b coefficients when dealing with operators?<|endoftext|>This paper introduces a new approach for solving the operator map problem which is based on the non standard form and its approximation via exponential operators. Finally, while the evaluation is not extensive with respect to state of the art baselines and datasets, it does convey the message that the proposed method attains SOTA or potentially even beyond SOTA results. The technique is evaluated on two synthetic PDE problems and one real life example. The authors integrate their method in the recently introduced neural operator framework.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposes a novel algorithm for learning prototype based nearest neighbor regression model. The algorithm minimizes an average of the quadratic loss on training examples w.r.t.the prototype centers and the prototype outputs by a block coordinate descent. The main contribution is the optimization algorithm finding the prototypes. The description of experiments lack important information.<|endoftext|>This paper proposed a new method to extend the synthetic reduced nearest neighbor model for regression problems. This paper provides an analysis of the consistency of the proposed method. They have provided theoretical analysis to show the consistency of the proposed method under mild assumptions. Their experimental results show that the proposed method can achieve better or comparable results than existing methods. The following are my concerns of this paper:(1) The novelty and contribution of this paper seem limited. The proposed initialization and EM approach seem straightforward based on existing studies. (2) Some important information is missing in the experiment part. (3) In Figure 1, what is the difference between SRNN and SRNN0? Please refer to my main review.<|endoftext|>The paper presents a novel regression model based on Nearest Neighbor. The main intuition is to simplify the regression problem by finding clusters of inputs and targets. [COMMENTS AFTER REBUTTAL] I thank the authors for the rebuttal, I carefully took a look at it, as well as at the reviews of the other reviewers. In Section 3.1 I would also add the description of the SRNN for classification: the proposed approach is an extension of it, thus it would be beneficial to summarize it. Moreover, it is clear from  such methods that the initialization is always an issue. Potentially interesting paper with some problems in the presentation and in the experiments. How many repetitions? Here are some comments: 1. Many important details are missing.<|endoftext|>The main contributions of this submission is to adapt Synthetic Reduced Nearest Neighbor (SRNN)  for regression tasks. In my opinion, the main contribution of this submission is to adapt Synthetic Reduced Nearest Neighbor (SRNN) for regression tasks, while the main contributions claimed in the last part of Section 1 only correspond to some necessary adaptions. [*] Geoffrey W. Gates: The reduced nearest neighbor rule (Corresp.). However, this submission doesn’t mention such motivation or authors have other more reasonable explanations? (c) Authors can add the standard kNN model for regression task as a baseline. From my perspective, the motivation is unclear, the experiments are weird and incomplete, and the presentation quality is poor. In summary, I think that this submission is below the standard I expect for an ICLR paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; When it comes to more details of this paper, the proposed methods perform very similar with or even worse than some existing ones while needing more network parameters (Table 4). For example, performance of swin transformer on imagenet 1k and 22k are both superior to this paper.<|endoftext|>Vision transformers scale quadratically with the number of pixels. The idea of this paper is novel and interesting. The ablation studies should also be conducted on the ImageNet dataset for image classification.<|endoftext|>I find it weird that the paper on vision transformers does not cite the original Dosovitsky et al.Despite limited novelty and significance, I found the work interesting and recommend acceptance. The differences with GFN, i.e.adding MLP and channel mixing, do not seem particularly insightful or significant.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The eventual goal of this manuscript (analyzing Adam) is interesting and of importance to some degree. If the authors are able to include the analysis of Adam, then surely this is a self contained story. However, without this piece in the puzzle, the description flow becomes quite divergent and more like just putting all known partial results together to form a paper. The naming also gives readers the illusion that the real Adam is analyzed and thus falsely boosted the contributions of the manuscript. In the 6th line of the same set of inequalities, +n(t)^T \tilde x_i should be   \mu_  n(t)^T \tilde x_i.<|endoftext|>One of the main contributions lies on the new theoretical analysis that constructs new Lyapunov functions. This paper provides theoretical results for the implicit bias of momentum methods. This topic is interesting and important. Although the authors make a contribution in the theoretical side, the theoretical results are still far from many applications in practice since this paper makes many assumptions such linear model, separable data, and smooth loss. I am wondering if some of assumptions such as separable data can be removed? The authors provided new theoretical results for implicit bias of momentum methods.<|endoftext|>The paper proposed a variation of [Soudry et al.2018] solution, that allows for the analysis of the more complex SGDM. The authors need to add "on linearly separable datasets", or some variation, in the title. The title of the paper is extremely misleading and (for what we know) can be false. We do not know whether momentum changes or not the implicit bias in general. In particular something like figure 3 in [Soudy et al.2018].The paper found interesting result that in my opinion are significant. However the paper has two major issues that should be considered before acceptance.<|endoftext|>In this paper the authors analyze the implicit bias of SGD with momentum and ADAM with momentum. I think this is a good paper. However, I think the title is quite exaggerated. Questions: 	What are the practical implication of the results ? Can you comment on that ? Overall, I think the paper is interesting. The authors push forward the understanding of implicit bias by analyzing the effect of momentum. The tools in this paper might be used to analyze more complex models.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors propose an improved algorithm for supervised contrastive learning, which they term “self contrastive learning”. Instead of only contrasting representations at one point in the network, self contrasting uses readouts at multiple stages in the network (remapped into an appropriate feature space by an additional readout network). This makes it possible to propose two variants: A “multi batch” variant that first augments the images, and then contrasts views between samples of the same class, both augmented and non augmented, and a “single batch” variant that only contrasts between samples of the same class. Improvements are shown for CIFAR 10,100, TinyImageNet, as well as a smaller version of ImageNet with 100 classes, referenced as ImageNet 100. The motivation is clearly outlined, and the discussion of related work is extensive. I should note, while the numerous analyses added to the paper are nice, a few well executed experiments including the “gold standard” models for self supervised learning would have made the paper much stronger (and a clear accept). ### WeaknessesI have the following concerns about the experiments:* The batch size in the supervised contrastive learning paper was 6144   how do you justify the must smaller considered batch size in the paper? * Any reason why Table 3 does not include measurements for full scale images, i.e., ImageNet 100? Some weaknesses about the interpretation of results:* The paper contains a lot of causal statements about a link between quality of MI estimation and the performance in downstream classification performance. Yet, the only evidence is Figure 2, which is a correlation analysis with a few point estimates. To make this result more stronger, what about evaluating the existing full imagenet trained checkpoints of SupCon and including this as a comparison? If this is the case, what was the motivation? I do not get the reference. This is a major concern, because strong results are concluded from these plots. * Figure 6: Do not use the jet colormap. Please use viridis (as in Figure 5), or a similar variant with less issues. Especially for the dog images, the results conveyed by the jet map are misleading. So I suggest to disambiguate this. The paper proposes an interesting addition to supervised contrastive learning, which removes the lack for data augmentations during training and yields improved results. ), and hence I am worried about the significance of them (how well baselines were tuned, etc.). To consider an accept, the authors need to minimally show a proof of concept result on ImageNet (for example in a very controlled setup, reproducing the SubContrast result, adding their method and showing an improvement with no special additional tuning, to make the comparison as fair as possible.As I see it, the method already works on full scale images.<|endoftext|>This paper provides a new way of supervised contrastive learning which does not require multi views. The idea of not using multi views in contrastive learning is important. Good quality of data augmentation is an issue in contrastive learning. 2.The authors did a lot of analysis and qualitative experiments to explain their method and motivations. The main theoretical contribution which is self con can maximize the mutual information between intermediate and last feature is incorrect. Then the authors claim that the negative self con loss is a lower bound of $I(F(x), T(x))$. However, the equity may not hold. Especially, in the authors  prop, maximize negative self con loss can only increase $I(F(x), G(x))$ but not $I(F(x), T(x))$. Maximizing $I(F(x), G(x))$ does not imply maximizing $I(F(x), T(x))$. The authors somehow shows that the mutual information between the intermediate layer and the last layer is finite in figure. I think this is the problem of the estimation. 3.In Table 2, why does the authors only report one seed? Multiple seeds are needed. Confidence interval needs to be reported. 4.In Table 4, why does single view benefits from larger batch size? What about batch size 512? If you already use the labels, why do not you compare with fully supervised approaches? Updates:I thank the authors for detailed rebuttal and answer further questions. I increased my overall score from 3 to 5 and empirical novelty from 2 to 3. However, in the original paper, the authors explain their method from the prop 3. Prop 3 does not tell the readers more than minimizing the mutual information between $G(x)$ and $F(x)$. I am against motivating the method from an invalid theoretical statement. So I am still against accepting the paper. The authors propose a new method which does not require multiple views in contrastive learning. However, the theoretical motivation seems to be incorrect. The author does not consider the gap in the lower bound.<|endoftext|>The paper proposed a contrastive learning framework to remove the requirement for additional data augmentation techniques for creating positive pairs, by leveraging the idea of multi exit network and conduct "self contrasts" among multiple outputs of the model. Theoretical analysis on the designed loss shows that the proposed SelfCon loss is a lower bound of MI between the intermediate and last feature. Extensive experiments and ablation studies demonstrate that the proposed method has better performance than than the supervised contrastive baseline. Below are some of my concerns/questions:  There are two versions of the self contrastive loss, the single view and multi view version. In the discussion below Eq (2), the paper defines SelfCon M loss as exactly Eq (1)? I wonder if this is the best design of the loss since with the multi exit network (different from previous method that only has one network output), when facing multi view batch, there could be a similar multi exit extension for the multi batch setting and should not be just Eq (1)? Also I think even with multi exit network, the multi view setting does not just provide redundant information as inductive bias from data augmentation and inductive bias from encouraging similarity within multiple outputs should be different. What is the conclusion for using more than one subnetworks (i.e.multi exit network instead of just 2 exit network)? It seems the current observation is changing the structure will increase the computation a lot but does not really help? A degrade case is, if we put an identity mapping between the intermediate layer and the last output of the backbone, the MI is also very large but it will not help for the downstream task. Any discussion and comparison to these methods? Also in section 2.1, BYOL is introduced following "with negative pairs", which is not accurate? MI is a measure of the mutual dependence between two random variables. After rebuttal   Thanks for the detailed response to my feedback. I appreciate the efforts on updating the paper and I think some of my concerns have been addressed. I thus raised my score to weak accept. Nevertheless, after reading the paper, I also have a number of concerns/questions on some statements, method derivation, theoretical analysis and empirical evaluation.<|endoftext|>While they have shown remarkable performance improvement, they usually require a multi viewed batch for defining the positive samples. In this paper, the authors proposed Self Contrastive (SelfCon) Learning which self contrasts within multiple outputs from the different levels of a multi exit model. The authors theoretically demonstrated that SelfCon loss guarantees the lower bound of label conditional mutual information (MI) between the intermediate and the last features. To the best of my knowledge, this is the first work to propose self contrastive learning via sub networks. While the authors state that SelfCon has also potentials for an unsupervised setting (Appendix D.2.), the current version is still mostly focused on the supervised setting. I think it will be better if it is more clearly stated both in the abstract and introduction. In the 3rd paragraph of the introduction, the authors stated that “the multi exit framework already generates positive pairs from a single image, making the data augmentation redundant.” However, I think this is not entirely correct considering that SupCon S outperforms SupCon even without the multi exit framework (Table 1). Can you provide additional explanations for why increasing the lower bound of label conditional MI between the intermediate and the last features is desirable and improved the classification performance of the encoder network? (cf.increasing the MI btw the last feature and the label). To define the positive pairs as samples with the same ground truth labels, shouldn’t the indicator functions in the numerators be something like I(y_i   y_p)? While I believe the paper is clearly above the acceptance threshold, I have a few comments to clarify or supplement some points in terms of the supervised settings, the rationale for increasing the label conditional MI, and the proposed loss equations.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; This paper considers an autoencoder with one hidden layer. Some references on the data being MNAR and ranking metrics:  B. Marlin and R. Zemel: Collaborative Prediction and Ranking with Non Random Missing Data, Recsys 2009  H. Steck: Training and testing of recommender systems on data missing not at random, KDD 2010And some references on rating prediction done right, i.e., accounting for the data being MNAR:  J.M.<|endoftext|>This paper proposes to use an additional element wise neural network to enhance the reconstruction ability of the autoencoder for recommendation. However, the missing ratings in recommender systems are usually not missing at random. The proposed method is easy to implement. 2.The authors investigate the rating prediction problem in recommender systems.<|endoftext|>In this paper, the authors study a traditional collaborative filtering problem with users  ratings, where the goal is to predict ratings that are unobserved. Specifically, the authors propose an enhanced autoencoder based method called NE AECF. Strengths:1 The idea of introducing an elementwise function (approximated by a neural network) is new. Weaknesses:1 Collaborative rating prediction is a very well studied problem, for which there are lots of existing works. Moreover, in most real recommender systems, item ranking is more consistent with a real setting. 3 The authors do not provide sufficient details or justification on using a large number of hidden units and an additional elementwise function.<|endoftext|>This paper proposes an imputation method that consists of two networks. One network is AE, and one network is element wise non linear transformation. The authors proved the proposed method has a better generalization bound for unknown ratings. Major concerns: 1. 2.The empirical results are not solid enough. Also, the experiment of ML 10M is missing.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper shows that finding a Nash equilibrium in team zero sum games is CLS hard (i.e., it is unlikely to have a polynomial time algorithm for computing a Nash equilibrium). I think this is inappropriate. The claims in this paper are supported by theoretical and experimental results. That is the similar result presented in this paper. 2.How to obtain Eq.(2.5) from Eq.(2.4)? ”About the related work on TME, the following two latest papers should be discussed:\Zhang, Y., An, B. and Černý, J., 2021, May. ***After the discussion:***It seems that the authors have misunderstood the relation between different solution concepts. Similar to TME, a two team maximin equilibrium is still a per player Nash equilibrium, but a per player Nash equilibrium may not be a two team maximin equilibrium.<|endoftext|>This work considers two team zero sum games where two teams with opposite objectives are facing each other. This paper first shows that this problem is much harder than classical zero sum games by showing that the computation of a NE is CLS hard. I am satisfied with their answers and do not have any concern other than the lack of clarity at some points. My main concern is about the comparison with related work, which seems weird/incomplete. The case of average iterates is only mentioned in Figure 4 and Remark 3.6, but having a longer discussion (+ a theoretical result) on this would be great in my opinion. For example, Figure 2 makes less sense as soon as we are considering average iterates. This is mentioned by the authors in the conclusion, and I agree with them that this can be left as future work. Minor comments:  p.2: "is computationally harder of finding pure NE in a congestion game". This paper is overall interesting and significant for the problem of two team zero sum games. My main concern is how it relates to previous works and I hope the authors could clarify this point.<|endoftext|>The main contributions of the paper are as follows. First, the authors show show that the computation of Nash equilibrium in two team zero sum game is CLS hard. I think it would have been clearer to say "its extragradient variant". Overall, I am a bit conflicted for this paper. I wish you had been way more specific. I also think the writing could be significantly improved. I welcome a thorough discussion with the authors. Showing hardness of TME is definitely interesting, but I believe that a robust discussion about TMECor and its benefits should be present in the paper. #2.The paper is a bit hard to read. What does around mean here? #3.I think the paper would benefit from polishing the language.<|endoftext|>This paper studies the equilibrium compution in two team zero sum games. The reduction for CLS hardness is somewhat straightforward. 2.There are several issues related to the presentation. The most important one is the ambiguity in Section 4 (last two sentences). Basically, I cannot understand the settings and the results provided in the experiments, although this does not influence the theoretical parts. I think this paper is blow the bar of acceptance. Currently I am negative on it due the weaknesses I mentioned, but may change to be positive if those issues can be addressed.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; HANN contains a hidden layer of binary activations, and can has a VC dimension smaller than the number of network parameters. If this is the case, I think the result is only relevant to the low rank assumption, not the HANN itself. HANN can be potentially a VC theory for studying the generalization of overparameterized neural networks. I m still confused about what role does the binary activation play here though.<|endoftext|>The architecture of this class of networks consists of a first hidden layer of width r and a second hidden layer of width k with Boolean (sign) activations. The name HANNs seems to be inspired by this Boolean layer which introduces a hyperplane arrangement. An upper bound of O(k^r) is established on the VC dimension of this network that is independent of the rest of the network (a Boolean function). Nice graphics are used to present the ideas of hyperplane arrangement and resulting classifiersThe analytical results nicely complement each other. The VC dimension is known to lead to vacuous generalization bounds in full precision overparameterized neural networks. The bound still seems quite large though: O(k^r). The paper is written very well and has nice complementary results. I am however uncertain about whether the main result would be effective in explaining generalization error using VC dimension in reasonable scenarios.<|endoftext|>The authors show a bound on the VC dim of HAC of O(k^r) which is independent of the number of weights. However, I have some reservations about the paper. First, reading the title and introduction has led me to believe that a much more general class of QNNs have small VC dimension. Regarding the experiments, in my opinion, using the number of weights to measure overparameterization is not indicative. A much better way to know whether the models are overparameterized is to look at the training error/loss (if it is very close to zero we are overparameterized).
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; (2018).Randomized prior functions for deep reinforcement learning. This is a desirable feature, and the ex machina argument made by the authors that "it would be preferable if we could place more trust into the predictions of value networks beyond the training dataset" is not supported. And this is not what the authors propose to do: they simply propose a way to estimate uncertainty and to enforce this assumption: LCB means that the training is going to be conservative. RL algorithm can be viewed as policy iteration: loop over policy improvement and policy evaluation. * Why would the LCB of independent networks would have a better uncertainty estimate than an ensemble network learnt on common LCB targets? As pointed out by the authors, the use of deep ensembles for offline RL is not novel. And when they train them all together, they share a form of pessimistic target (reusing the paper terminology: independent double Q, shared LCB, shared min), not the expectation (shared mean), otherwise it does not make sense! Following my previous point, it would have made the analysis much stronger if the other forms of pessimistic updates were accounted for and analyzed. It is straightforward that using a common target will provide a collapse of the inter network variance. The use of infinite width NN is a big hammer for such a small nail. This, combined with the lack of compelling arguments on the theory side, cast doubt in the empirical results.<|endoftext|>The paper propose to study ensemble methods for offline RL in the NTK framework. They derive The paper looks interesting, so are the approach, the theory, and observations. * Lack of rigor in the theorems: the authors should have a formal theorem followed by its interpretation on which they base their intuitions  * Lack of rigor in the results presentation: what bold format means? * Too many claims not well supported  * Section 5.2: wouldn t it have more impact if you used the git attached to Lee et al.2019 in Section 5.2 to match the framework the authors are working on? * p.8: footnote of p.7, "otherwis The"  * p.9: "computationl efficieny"I found the paper quite hard to follow and overall there is a lack of rigor. However, looking at the experiments, there might be something to explore.<|endoftext|>However, this  is reviewed at all. For example, LSPI (model free), LAMAPI (model based), pseudo MDPs (model based) and RKHS embedding (model based). The paper went straight to discussing the reduction of the value estimation error constraint. The use of mean or max is a fundamental question to investigate. It is not necessarily formalized as constraint. One major motivation of the reviewed paper is to “some of the unique challenges of uncertainty estimation in reinforcement learning ”. The DLTV paper has an observation that there are both intrinsic and parametric uncertainties going on with DRL. The DLTV paper uses distributional RL to estimate the upper confidence and lower confidence bound from the distribution of Q values, also using neural networks. The presentation of the paper can be improved. Did you try the other baseline? With only baseline and it’s poor, it is hard to see the proposed algorithm is strong. Questions:What is the connection of using ensembles and distributional RL for uncertainty estimation? However, identifying their connections and differences are interesting, isn’t it? For distributional RL, e.g., see the DLTV algorithm:https://arxiv.org/abs/1905.06125Actually the method proposed in this paper (estimating LCB, see Section 4.1) is very similar to DLTV. By independence, do you mean the learning rule sense?<|endoftext|>This paper provided a model free algorithm for offline RL. It also conducted experiments to validate its algorithm. The algorithm in this paper is model free, which means it can be applied to realistic settings. The detailed derivation of the std should be included in the proof, and there should be more details, like descriptions of the network initialization,  in the description of the theory part Typo in Line 8 of page 4. "This an ..."Although this paper proposed an algorithm that can be applied to real life applications without imposing strong structural assumption to allow uncertainty quantification, the theory part of this paper should be improved to be more complete, rigorous and self contained.<|endoftext|>Strength:The work proposes a novel method to use independent ensembles, which might be interesting towards a better understanding of uncertainty quantification in offline RL. I Since this is related to the core theme of the paper, could you explain more on such relations, or on the uncertainty this work targets at? The writing of the paper might be improved. There are also some missing citations and references as well as typos in the paper which hurts the reading. In theorem 5.2, the difference in ensemble variances is only discussed very vaguely. I don’t quite understand the meaning   it would be better if a direct comparison with equations is presented. Figure 1 shows the mean value and the standard deviations of the predictions, where the randomness can be viewed as from the training process, i.e., the random initialization, instead of the randomness in data. I would expect directly using the result of shared LCB/min can somehow provide a valid uncertainty quantification, without referring to the standard deviation induced by the training process. Is there a missing superscript i for y(r,s’,pi) in equation (4)?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes a method to identify the controlled effects from actions based on the concept of  blame . The proposed network consists of two branches that conditions on both current observation and action and only current observation for predicting the difference of two consecutive observations, respectively. By introducing these two branches, representations from the controlled branch can capture the controlled effects from actions which cannot be adequately captured by normal branch. First of all, what is Attentive Dynamics Model? What s the expected message from the experiments comparing the proposed method to ADM? Just stating that  `ADM produces an attention mask for pixels controlled by the agent therefore it is used as baseline in these experiments.` does not convey meaningful information and makes paper not self contained. Ablation study to show that the proposed component is indeed important for the improved performance is missing. Missing comparative evaluation in terms of episode returns / success rates on standard benchmarks. More investigations on hard exploration games in Atari and DeepMind Control Suite are required for demonstrating that the proposed method is indeed better than inverse dynamics model for learning representations useful for capturing the novelty of states without affected by random aspects of the observations. How do the predictions from controlled and normal branch differ qualitatively? How does the method perform on such environments, and compare to baseline of inverse dynamics predictor? I like the general idea of the paper, but several empirical evaluations are missing to support / justify the effectiveness of the proposed method.<|endoftext|>This paper proposes an approach, called Controlled Effect Network (CEN), for agents to predict the controlled parts of the environment. CEN is a dynamics model that separately predicts the "controlled" and "normal" change in observations, given the current observation and action. In addition, it improves exploration when combined with NGU (Never Give Up) on a couple simple tasks, compared to the inverse dynamics model used in prior work. The method is clearly motivated and well explained, and the paper is well written. The experiments are presented in a logical order, and they systematically and thoroughly evaluate several important aspects of the algorithm. I was a bit disappointed that the only experiments on using CEN as intrinsic motivation for exploration were on an empty world and a grid world task. In addition, I have a couple questions about the applicability of CEN. Would it get confused by the entire observation / camera image changing as the agent’s gaze direction changes from frame to frame? If so, this a limitation that should be mentioned and discussed in terms of how it could be addressed. For Montezuma’s Revenge, the pixel prediction of controlled effects is less accurate. Does this imply that CEN would not work for more realistic domains? CEN is a simple, elegant solution for disentangling controlled and normal effects, that is backed up by thorough empirical evaluation. However, it leaves a few open questions that I m hoping the authors can answer in the rebuttal: 1) can CEN scale to domains with more rich and complex observations?, 2) can CEN handle first person observations, and 3) does CEN improve exploration for more complex domains? Additional experiments and discussion on these areas would significantly strengthen the paper.<|endoftext|>The authors  introduce the Controlled Effects Network (CEN), an approach to separating the controlled aspects of the environmental dynamics from those that are agnostic to the agent s actions. This occurs on a per pixel basis. In order to accomplish this separation, the network architecture forces the next step prediction to be the sum of two separate predictions (only one of which has access to the agent s action). The reliance on training under the uniform random policy is a big issue e.g.preventing the usage of CEN to improve the NGU paper s results. Training occurs on a fixed dataset collected with a uniform random policy, which is important since the best action agnostic prediction is highly dependent upon the behavior policy. The utility of this approach is demonstrated in 3 ways: comparisons to the ground masks of controlled effects, decodability of attribute changes in controlled effects (and not un controlled effects), and RL performance when used as the representation for an intrinsic motivation system. This paper introduces a representation learning system, CEN, that is both conceptually simple and well motivated from the perspective of causal learning. Why is Equation 6 a mixture loss instead of only trained the parameters of the action agnostic stream with the second term and the parameters of the action aware stream with the first time? However, I cannot currently endorse this paper s acceptance due to several significant issues with the experimental results, and (to a less extent) some methodological limitations that deserve discussion. A clear and well motivated algorithm is let down by (1) experiments that fail to provide evidence to support the paper s claims. In its current state, I must therefor recommend rejection. It is thus unsurprising that CEN wins out on this metric. To address (2), acknowledging the limitations (or correcting me if I m mistaken) is enough. If that was the goal of CEN then fine, but it is not. The agent is the aspect of the environment that the authors claim to modelled by all of these methods and emphatically not the thing they re trying to improve upon. Both ADM and Inv model box, button, and light (the indirectly controllable objects) at least as well as they model the agent. And would the performance gap be eliminated if this were the case?<|endoftext|>This paper studies a new method to identify controllable effects in a learned dynamics model of the environment. If the authors correct (or further elaborate on in the rebuttal) some parts of Sec 2, then I think the paper would be a good contribution to ICLR. Their proposed network architecture and experiments do make sense, and show improved performance over inverse dynamics models. Experiments show that their approach is better able to disentangle controllable effects than the baseline, and can also aid as an intrinsic motivation reward. Instead, they come up with a new specification of ‘blame’, based on the difference between the observed effect of an action and the average/normal effect otherwise observed (under other action). * The experiments are extensive and varied, and show good results. In this case, the ICE of action ‘right’ would be the biggest, since the other two actions (in the expectation) suggest that a decreasing health is actually normal. You mention in the paper that ‘keeping full health’ is normal in this case, but I do not understand how you get to that conclusion. * Your practical experiments do work, which I think is caused by the fact that your neural network automatically generalizes over similar states. I think this is what needs to be corrected in Eq5: the expectation should probably also run over other states in the problem, not only over actions. In any case, the jump from Eq 4 to 5 does not make sense, and I personally think Eq.4 does not make sense in general (the dependence on your own policy). Right now I found it hard to learn something from the figure. * Table 1: Why is CEN for example so good at predicting the Skull in MZR (61%), for an object that is actually not controllable? In general, I think this paper studies an interesting and relevant topic (controllable factors), and brings interesting new insights (inverse models do not work well, and there may be better methods).
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 6; The paper studies the problem of uncertainty quantification in deep neural nets. It introduces a concept called "probability estimation" and an uncertainty calibration method called "CaPE" based on this new concept. The main message of the paper is not clear. What is essentially the mathematical definition of probability estimation? I am having hard time to make sense of the calibration loss presented as an alternative to the discrimination loss.<|endoftext|>The paper tackles the problem of uncertainty calibration in the special case of binary classification. The authors propose the so called CaPE to iteratively minimize the discrimination loss and the calibration loss. The definition of of ECE and MCE is a special case (for binary classification) of the more general one that handles multi class classification. The current discrimination loss will not work probably when there are more classes since it only focuses on the single correct class. I was wondering how many models are used for Deep Ensemble, since the performance of Deep Ensemble is also related to the number of individual models. Some related work on probabilistic neural network for uncertainty estimation is missing [a,b,c].<|endoftext|>This work tackles the problem of probability estimation. The authors address an interesting problem. However, it would be interesting to further justify why current models do not offer an accurate estimate of uncertainty, particularly bayesian modelling. It is also important to note that calibration is a key issue in the survival literature and penalties have been developed to tackle this issue [1]. Finally, it is interesting that the authors use their loss only as a fine tuning of the neural network. Deephit: A deep learning approach to survival analysis with competing risks.<|endoftext|>The authors propose a method to ensure calibration of probability outputs during training. They do so by adding an explicit penalty to push the output probability values towards an empirical estimate of the actual probability for the current inferred probability. The writing is confusing though, and the authors could have done a much better job on that side. Detailed comments:  "classification problems without inherent uncertainty", I am not sure that such problem exist. Please rephrase or elaborate further. This should be explained to a greater detail. In the last paragraph in Section 6, please explain better how each scenario aligns with the data sets, it is unclear as is.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper propose an end to end learning based method to extract heart rate from facial videos. The paper shows superior results on multiple public datasets that are moderately difficult. The machine learning aspect of this paper seems solid. For example, how does the model perform in each sky type group, each motion amount group, etc. At face value, this paper produce superior HR detection result, and more efficient algorithm. However, at closer inspection, a more direct comparison, and evaluation on a more challenging dataset are still needed to validate the effectiveness of the algorithm. Some of the comparison in Table 1 is misleading, and I would love some additional clarification from the authors. I recommend the author to actually retrain comparisons on the exact same dataset, so that we can be sure that the proposed method actually improve on the state of the art. Secondary to evaluation, it is unclear how swin transformer and tensor shifting actually help authors avoid preprocessing steps required by other methods. These are the components that have been used in previous work, and they still require some preprocessing in order to achieve good performance.<|endoftext|>In this work, the authors present simple and efficient models for camera based vital measurement. The proposed approach does not rely on any pre processing steps but uses end to end neural networks. The experiments show the approach archives state of the art performance on three public datasets with fast inference speed. + There is an exploration of different designs of the backbone networks. The weaknesses of the paper are as follows. The authors are recommended to have a few revisions on the paper. There are no ablation studies on different designs in the presented model. The results on UBFC by the proposed method are not very competitive. I do not get the point of presenting a model with transformers. In summary, I feel the work lacks sufficient technical contribution in terms of novelty and empirical motivation. The experimental results are good but somehow expected.<|endoftext|>4.What are the limitations of this method? This evaluation shows that the proposed approaches, in particular the convolutional approach, perform well. The approach seems clean. 6.“the task of heart rate estimation... is considered as the gold standard benchmark for the field of camera based vital measurement”   what is meant by gold standard benchmark? It is no fault of the authors that Dual GAN’s method is hard to reproduce, but it would be good to be more transparent here even if it makes the narrative a bit murkier. The estimation task in the paper is concerned with photoplethsymography/heart rate. Although the authors hint to the use of the model for other estimation tasks (e.g.blood pressure) in future, it is not actually assessed. It is useful to understand the variation in performance per model when starting from multiple random seeds. While the results on the cross dataset experiment look strong, I find the technical contributions and analysis to be a little light for the level I would expect of ICLR and I would have preferred to have seen more careful study of a single method to understand its benefits.<|endoftext|>Nonetheless, they obtain good performance in terms of accuracy (MAE, RMSE, ro) and latency, with the convolutional model in the Pareto frontier. The resulting lightweight architecture makes both variants easy to be implemented in multiple devices such as mobiles, and to be applied to the elaboration of different physiological signals. EfficientPhys is tested on three public datasets with state of the art approaches obtaining good results in both accuracy and latency. Interestingly, it also shows that the this variant is more efficient than the transformer based one since it obtains a greater accuracy if the models are constrained to the same size. They also perform in a very different way: only   C  outperforms other approaches in terms of MAE, RMSE, ro (except DualGAN), and is the Pareto optimum for MAE/latency;   T1  is good in terms of MAE, RMSE, ro, but not in latency;   T2  in contrast show interesting latency but not so good accuracy. As explained by the authors, this is due to the lack of results of DualGAN on the public datasets PURE, MMSE, and cannot be totally addressed to them. The paper is an interesting contribution in the task of camera based physiological measurement.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper claims semi supervised learning (SSL) does not uniformly improve performance on all the constituent (latent) sub population of the group. Finally, they briefly discuss strategies that could avoid such disparate impact of SSL. mean.The prime objective of the paper is to demonstrate the Matthew effect of SSL and its fairness consequences. ## Strength  Disparate impact on the constituent sub population when training through SSL is an important problem. As described in more detail above, I have concerns regarding both the aspects of their contribution. Their theoretical analysis is sound.<|endoftext|>This paper aims to theoretically study the  Matthew effect  (or disparate impact) of semi supervised learning (SSL). Overall, I think this paper is studying a very interesting and important problem. Please see my concerns regarding the submission as follows. Some analysis are needed. Also, the authors might want to correct their claim  We consistently observe that the class labels with higher baseline accuracies have higher benefit ratios on both CIFAR 10 and Yahoo!<|endoftext|>The experimental results confirm that the discrimination of the SSL frameworks can be a problem in various datasets. Strengths:  The paper provides meaningful analyses to investigate the disproportionate performance of semi supervised learning (SSL) on different subgroups. Dev.2019.This paper provides useful analyses on the fairness issue in the semi supervised setting.<|endoftext|>This paper presents the disparate impact on semi supervised learning, where the sub population that has a higher baseline accuracy tends to benefit more from SSL. The paper targets an important problem to ensure the fairness of machine learning algorithms. The paper studies the sources of disparate impact on SSL from both theoretical and experimental demonstrations. Theoretically, the SSL problem was transformed to supervised learning with noisy labels by assigning pseudo labels. In general, the paper is well written.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper is written clearly, and presents the main contributions in a straightforward manner. The inclusion of the code for reproducibility is appreciated. The motivation for doing this study is not very convincing; why would researchers need to retrain using DQN (or the variant of DQN introduced here) in Atari? If this concept could somehow be generalized to other domains, then this would potentially be useful. Section 1, paragraph 2 refers to GPUs as "costly", but this method still uses a GPU. Section 5.1 refers to "should be affordable"   it would be useful to have some kind of relative pricing   for instance, what would be the relative cost required to get an analogous speedup just by improving hardware? The main contribution of this paper is not sufficiently well motivated or general; the re implementation of DQN with more parallelism does not pose enough independent interest to be practically useful for most researchers. Suggestions for improvement would be to (1) show that this method could actually extend to settings beyond DQN and improve other RL algorithms; (2) perform a more in depth hardware study comparing the cost to speedup via more powerful hardware vs. software improvements.<|endoftext|>This paper introduces an optimized version of DQN which speeds up training 25 >9 hours. Additionally, the empirical evaluation could be improved by using stronger baselines. Secondly, the authors compute actions for multiple environments in parallel on the GPU. I think there are two main issues with the paper: the novelty and the empirical evaluation. The difference might be entirely due to hardware differences, so this statement does not say much. Are results just computed over 1 seed per environment? For a paper whose main contribution is on sharing a good implementation, this is not ideal. ## Minor commentsI would recommend that the authors show the learning curves (of e.g.averaged human normalized scores) in the main paper.<|endoftext|>A new method for benchmarking RL algorithms like DQN, focusing on Atari suite and experiments from seminal DQN paper. The results are comparable to ones in the original paper overall, but vary on per game basis probably due to mentioned changes, slight change in hyper parameters and high variance of a single run. Considering that F is usually considerably bigger than the memory size it should not matter if we experience/train in sequence or in parallel. Figure 1: "Synchronize" step could be more explicit I guess. It seems to do what it strived to do (speed up) and there are no issues with the paper.<|endoftext|>This paper introduces a DQN implementation to leverage a novel concurrent and synchronized execution framework for better utilizing a heterogeneous CPU GPU desktop system. This important discussion has been largely ignored  in the manuscript. The generalization of the proposed method is highly expected. The paper proposes an acceleration framework for DRL based on synchronized execution of GPU.<|endoftext|>The authors propose an implementation of DQN that focuses on maximising the data / training throughput in a common CPU GPU machine setting. They show considerable improvements in speed while producing comparable results with previous DQN results on the Atari suite. RL is often not optimized for academia like hardware requirements, and we need more research and engineering work to figure out how to utilize these kinds of resources better. Point (2) technically could be consider novel (AFAIK) when employed to produce this accelerator utilization scheme, but it is also a concept that exists in many existing libraries as an implementation detail. ## Clarity of methodsI think the paper could use with a solid paragraph of definitions regarding what the authors mean when they say "distributed". Secondly, I could not understand the point of the extrapolation done in the Pong experiments. I would really like to understand why this procedure was chosen.
Reject; rating score: 5; rating score: 6; rating score: 6; This work considers the stochastic shortest path (SSP) problem with linear function approximation. The overall presentation of this paper is great and the theoretical results appear to be sound and clear as far as I have checked. Therefore, this work lacks in terms of originality and novelty. The authors provide an algorithm with a regret upper bound in high probability while the provided lower bound is for the expected regret. This is a standard practice in the literature but it remains an unsatisfactory presentation since one would hope to get a lower bound for regret in high probability. Most of the technical tools and ideas used are borrowed from prior work with minor changes. ( ) My third concern is that the proposed algorithm requires certain inputs which might rely on unknown quantities.<|endoftext|>This paper considers the stochastic shortest path (SSP) problem, a particlar RL problem, where a cost function $c(s,a)$ is associated with a state action pair. Strengths:The paper is overall well written. Overall, the algorithmic and analytical contributions seem to be adapting the results of (Zhou et al., 2021a;b) to the SSP setting. Can authors comment on the motivation of this model? The authors consider SSP frameork under the linear mixture models (that is particular structure for the transition kernel). Adapting ideas from recent works on linear mixture modes in other settings to SSP setting, a model based algorithm is presented and its regret bounds are provided.<|endoftext|>This paper studies the linear mixture SSP problem, where the transition kernel is a mixture of unknown models. There are subtle issues with extending these ideas to SSP which this paper resolves. This work has three contributions:1. New Setting and Algorithm: Introduces a new setting for SSP problem: linear mixture SSP problem and a novel algorithm LEVIS for this setting. This work is a nice extension of ideas from RL with linear function approximation to the SSP problem and provides a new setting + new algorithm with upper bound on regret and almost matching lower bounds in some settings.
Reject; rating score: 3; rating score: 3; rating score: 3; The proposed method is tested on continuous control tasks. **Concerns**: (i) The proposed decomposition $\phi(s,a,s)^Tw(g)$ feels very similar to that used in (Ma et.al, 2020). Moreover, (Ma et.al, 2020) also incorporates the learned successor features into an actor critic framework and uses it for continuous control tasks. Hence, I am not sure what novelty the proposed work is bringing. (ii) The environments used in the experiments are limited (only reacher and door close tasks are considered). Ma et.al, 2020. https://arxiv.org/pdf/2001.04025.pdfWeighing the strengths and concerns as listed above, I am currently recommending the paper for a rejection.<|endoftext|>This work looks at the use of successor features for solving simple continuous control tasks (in particular, reaching to different locations and door closing). The two contributions they enumerate are a ``practical implementation of SF framework for continuous state and action domains  and jointly learning $\phi$ and $w$ (that is the successor features and the task weights $w$). 1.The paper states that it introduces an actor critic approach to SF in continuous action spaces. 2.I have some concerns about the novelty of this work. It would be helpful if the authors consider releasing code for reproducing the experiments in this paper.<|endoftext|>It proposes an actor critic architecture (a variation of the Soft Actor Critic method) that learns disentangled representations for the environment dynamics and the tasks. Although the paper is well motivated and presents interesting implementation details, the proposed method misses the discussion of concrete similar work. This related work makes the proposed method seem fairly incremental and only marginally novel. In more details:  The contribution of this paper is fairly incremental, and the novelty is questionable. There are other works with different representation mechanisms [3,4,5] and such comparison will help validate the use of SFs for representation in the continuous multi task setting. I would like to ask the authors if they think that this is an issue and why.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The paper presents a new method to train neural networks with stochastic discrete variables called stability regularisation. The method pushes the outputs of functions of Gaussian random variables to be close to discrete, and unlike other methods used with discrete variables, such as Gumbel Softmax which requires temperature annealing, it is easy to tune.<|endoftext|>The paper describes a computational approach to discrete optimisation, based on a probabilistic regularization procedure that enforces the output of a continuous function to be quasi discrete. Strengths:The method implemented in the paper is innovative and exploits a nice and not so popular property of the Gaussian distributions. The paper has an extensive set of experiments. One is the computational cost of the proposed method.<|endoftext|>The paper proposes a method for regularizing neural networks with boolean and categorical stochastic variables. The proposed stability regularization is based on the Gaussian noise stability, which is maximized by the indicator functions of half spaces. Unlike most of existing methods for handling discrete stochastic variables, the proposed method promotes the output of continuous functions of Gaussian random variables to discrete, which leverages the nice property of the Gaussian noise stability. The authors have conducted various experiments on different benchmarks to showcase its superiority over existing methods. The proposed stability regularization has nice theoretical foundation and shows good performance across different problems involving discrete stochastic variables. But this does not seem to be the case here.<|endoftext|>The paper proposes an interesting regularization method for encouraging function values to be discrete. The submission has done extensive experiments comparing the proposed technique against Gumbel softmax. My hypothesis is that the temperature value is hard to tune while the regularization strength is easier to tune. 2.The performance improvement does not seem to be consistent. The authors may want to increase the number of clusters and check the performance. Do you see a performance improvement by using mean centering?
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; rating score: 5; This paper proposes a natural and simple idea to improve membership inference attacks: train a few reference models on the same distribution, and then use these as a "prior" for the distribution of losses of non members. This idea is not entirely new: it was proposed and analyzed both in Sablayrolles et al.and Long et al.which the paper references. It would be helpful to clarify this paper s contribution with respect to these two papers. In particular, it seems to me that the attack proposed in this paper is strictly *weaker* than the one of Sablayrolles et al.In "Connection to posterior inference", this paper claims that the proposed approach is equivalent to that of Sablayrolles et al.But this isn t quite true: Sablayrolles et al.train reference models on subsamples of the data so that they can approximate both the average loss of an example when it is a non member *and when it is a member*. The approach in this paper is similar, but only considers reference models that do not contain a victim point. So this attack is necessarily weaker. A few minor comments:  Some of the model accuracies in Table 1 are fairly low (e.g., CIFAR 10/100). The finding that data augmentation leads to less privacy leakage is contrary to what was found by Choquette Choo et al.Did you also use augmentations for the attack as in that paper?<|endoftext|>This work proposes a simple output calibration method to suppress false positive errors of membership attacks. The core idea is to define a calibrated decision score as the difference between the original membership score and the expected membership score trained by a randomized training algorithm using a shadow dataset (a dataset following the same distribution as the true training data set). The calibrated membership decision score can provide a lower false positive rate than the original membership score. Strength: Controlling false positives of membership attacks is important for practical membership inference scenarios. Research efforts along  this direction should be encouraged. Weakness: The theoretical rationality of the proposed calibration method is not provided in the study. Though it is mentioned that the proposed method is associated with posterior inference, more formal study establishing the link is still needed. The discussion at the end of Section.3 is not convincing. The theoretical reasoning the performance of the proposed calibration method is not provided yet necessary.<|endoftext|>The paper aims to solve the issue of high false positive rates in modern membership inference attack methods by computing the difference in model prediction metrics (e.g., loss, confidence) between the target model and a reference model. The intuition is that “easy to predict non members” will have a small difference between the target and the reference model, while “hard to predict members” will have a large difference. The idea of avoiding high false positive rates is indeed important for membership inference research. The idea of difficulty calibration is not new. As the authors already pointed out, Long et al.(2018) and Carlini et al.(2020) already used similar technique to compare the difference (or ratio) between target models and reference models. What are new things in this paper? 2.The paper fails to compare with existing attacks which are also designed to decrease false positive rates, specifically, the following paper. It would be great if the authors can also include entropy metric as well. I like this paper’s focus on reducing high false positive rates.<|endoftext|>This paper focuses on the analysis of membership inference attacks (MIAs), in  particular addressing the problem of high false positive rate (FPR) that still affects the  state of the art solutions. This technique appears to be particularly beneficial in helping to distinguish between  member and non member samples that are overrepresented in the data distribution. The main contribution is the application of difficulty calibration (inspired by Long et al., 2018) to MIA. The analysis with different member non member ratios is definitely interesting and well done. Several results have been mentioned in the literature but this aspect has not been discussed in this work. This assumption should be justified and further investigated. In order to better understand the improvement coming with the proposed method, it would be useful to have a deeper understanding of the reason for which it is indeed beneficial. However, this does not seem effective on some datasets. Could the authors investigate those issues and shed more light on the relation between the type of data and the success/insuccess of their method?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; To begin with, these are interesting observations and likely have practical implications. The strength of the paper are the empirical reductions in sample complexity and increase in performance. On a more practical level, I would also expect more AL querying functions as this is largely an empirical papers and I see no reason why uncertainty sampling is guaranteed to be superior (although it is intuitively appealing that pathological cases probably disappear). There are enough ‘additional’ experiments to demonstrate that the authors have a promising path to a theory. However, to be a more impactful finding, I lean toward better contextualization, more experiments that vary the active learning querying function, and more empirical/analytical results to support a clear theory. Interesting, but I don’t believe ready for acceptance in ICLR. The authors make some interesting and potentially impressive observations regarding the performance of ‘simple’ active learning (i.e., uncertainty sampling) in the context of pre trained models — showing positive results both on image and text classification problems.<|endoftext|>This paper investigates the active learning performance of pre trained models vs their non pre trained counterparts on both vision and NLP tasks. Empirical results generally show that the pre trained models with the uncertainty acquisition function performs much better than the random baseline and their un pre trained counterparts. The advantage of pretraining is also clearly established. 2.It is not clear which part of the pre training is most helpful for active learning. In addition, there seems to be a disconnect between the setup described in Fig.1 and the actual experiments conducted. 3.A reference to [2] is recommended in footnote 1 when discussing calibration, since it is the original paper that investigates the confidence calibration problem of neural networks.<|endoftext|>The authors set out to investigate if active learning is an emergent property of pre training. That is if running active learning with pre trained models gives better result than using the same models without pre training. However, a major problem the paper faces is that there is not enough experiments in the paper to validate the claims made by the authors. Additional experiments on different datasets including text data will be needed. Lastly, it is important to consider different architecture models for the pre trained models and not just variants of the BiT model. It s an interesting paper but the authors need more empirical validation to establish some of the claims.<|endoftext|>In both cases large pre trained model is finetuned on a small amount of seed data and then an active learning procedure is used (in this paper the AL procedure is an uncertainty sampling procedure) to collect more data. Experiments are performed to show that using an active learning procedure indeed helps improve performance using only a small amount of actively labeled training dataset. The direction of investigating other active learning techniques (such as query by committe or expected model change) also remain open. 2.All the pre trained models considered are supervised pre trained models. Do the same results hold true?
Reject; rating score: 3; rating score: 3; rating score: 3; The paper aims to improve AAEs with the intervention loss (Liang et al., 2020) and SVGD (by constructing "bridge distributions") and demonstrates some empirical results. Pros+ The topic is interesting. Cons  The technical correctness is very concerning. The novelty is quite poor. Fig 4 and 7 are not referenced in the maintext.<|endoftext|>This paper introduces an intervention adversarial autoencoder and claims this could improve VAE learning. ## Strength:The intervention loss for regularization seems novel in VAE training. Lots of related works are missing in this paper, e.g.ARAE [1] and VAE SVGD [2]. Recall the baseline works that have been published several years ago, this paper is not even close to them.<|endoftext|>This paper targets on some important issues in generative modeling such as unstable training process and mode collapse. Novelty of this paper is low and evaluation is not sufficient. There are many methods targeting on unstable training process and mode collapse in GAN, and more comparisons are needed, which should also be included in related work and reference. The paper targets on several serious concerns about generative modeling, such as unstable training process and mode collapse.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Strengths:  Tackles important practical problem of the result differences from ML APIs  Presents accuracy change results from a number of actual ML APIs from leading providers  Authors created a novel algorithm (MASA) to efficiently detect and evaluate result differences for ML APIs. Only few systems were analyzed and only for two dates (spring 2020 and spring 2021). As authors noted, confusion matrix difference is a good measure as a result drift only for certain (classification like) APIs. It would be good to see how to deal with non classification APIs. I think that the problem of ML API result shift is real and important. I believe authors made interesting and useful contribution in evaluating such shifts. Although the paper has some weaknesses, I would recommend accepting it.<|endoftext|>A method to estimate the confusion matrix of a black box classifier, using as few samples as possible. The paper is focused around one use case: tracking changes in ML APIs, considering that such changes may go unannounced. As the paper stands, it is unclear just how much of MASA s performance improvement over uniform and stratified sampling is due to K, versus the uncertainty based sample selection. Given the importance of this idea, it would be better to explain this via an intuitive figure. As a suggestion, the authors could visualize how a partition in which the ML API classifies all data points with the same class (whether right or wrong) will have low uncertainty, while the opposite gives high uncertainty. The paper makes atypical but important contributions to ML ethics.<|endoftext|>This work proposes an efficient way to measure shifts in the confusion matrices of ML models using limited number of API calls. The problem addressed is important and the method seems to yield good results in practise in comparison to random sampling. After all, the goal is to estimate elements of the confusion matrix. API shift is an important problem that needs to be addressed in order to operationalising AI. This work proposes a way to assess these shifts using limited number of API calls and has a lot of practical importance.<|endoftext|>It formalizes the problem as estimating the change in the confusion matrix over time. The main theoretical contribution is an adaptive sampling method to more efficiently estimate this shift. The experiments could be improved by comparison to baselines other than random sampling. This work opens a discussion around the problem of estimating the performance shift in commercial ML APIs (for classification). The theoretical contributions of the paper are small but non trivial. The experimental analysis is detailed and interesting, but could benefit from further ablation studies on the effect of the client side difficulty gauge model. The problem is of interest to the ML community and the release of the annotated dataset used in this work would be useful to the community.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposes a learnable pooling method (GNP) for GNN based on $L^p$ norm, which is general to simulate the behavior of max, min, sum, and average. 3.The extrapolation performance of the GNP outperforms simple min, max, mean, and sum pooling methods. 4.GNP is an interesting idea and performs well on small synthetic datasets, but it would be better to see how GNP works on state of the art model architectures on real world applications, like PNA[3] and DAGNN[4], instead of the vanilla GIN and GCN. A fair comparison of graph neural networks for graph classification. The GNP idea proposed in the paper is interesting and works well on small tasks.<|endoftext|>This paper focus on the extrapolation ability of graph neural networks and propose a new pooling function for graph level readout based on vector norm. The proposed method can be applied to replace the commonly used pooling function like max/mean/sum in GNNs and is proved able for extrapolation in a simple example. The technical contribution of this paper is very weak.<|endoftext|>However, this simple extension is not a well behaved function for gradient based optimization, hence the authors tweak the layer to behave better using techniques that are well known. The contribution to this paper is incremental. It builds on top of the well known L p norm pooing function and extends it to allow negative values of p, and an additional learnable parameter q.<|endoftext|>This paper presents Generalized Norm based Pooling (GNP), a L^p norm like pooling function with the trainable p for Graph Neural Network to achieve extrapolation. Equally splitting the GNP function, the model can learn both positive and negative p. Experiments were done to demonstrate GNP at the node /graph /set  levels. The pooling function performs steady and well on set level tasks. Hence, the proposed training is not able to push p to reach +/  infinity. The proposed method contains a one layer MLP, which introduces extra d*(d + 1) parameters of the GNP functions compared to traditional pooling functions.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; This paper introduces a layer of controllable expression parameters for the audio rendering of MIDI files. The authors introduce heuristics to extract note wise expression parameters from low level synthesis parameters. Also, it transfers the manipulation of the sound from the mentioned contours (which are frame level) to note level expression controls, more expressive and controllable. Overall, this works is very well written. In addition to the paper, the website provides very convincing audio examples which further assess the quality of this work.<|endoftext|>This paper proposes a music performance modeling network using three sub modules which are expression generator, synthesis generator, and DDSP inference. However, I see there exist many challenges (e.g.the model relies on CREPE in DDSP synthesis part, also the type of an instrument set between transcription part and MIDI DDSP should be matched. The paper is well written, so I only have few minor questions. The authors argue that the method can be easily extended when multi instrument transcription model is ready.<|endoftext|>The paper thoroughly relates this work to related work in the field, making many thoughtful connections to both the symbolic & audio generative modeling literature. Does this additional stage also account for the performance improvement of MIDI DDSP over MIDI2Params? I am under the impression that the expression step is an innovation of this work (Section 3.2). This is a well written, creative paper. The connections to related work are extensive and thoughtful.
Reject; rating score: 5; rating score: 5; rating score: 5; A proof of this proposition is not presented and so it is impossible to evaluate the rigour here. The results linking adversarial robustness to distributional robustness in Section 2 are both presented without proof, and already known and published. I believe it would substantially improve the paper to provide a proper discussion of the literature and characterising how these results improve upon existing ones.<|endoftext|>However, the presentation and writing of the paper need to be improved to make it a more solid paper. Appendix needs to be organized better and all proofs need to be consistent with the main paper. Moreover, Appendix is not well organized to connect to the main paper. **Strengths of the paper:**  The technical contribution of this paper is significant because it shows a connection between standard adversarial training such as PGD and distributional robustness.<|endoftext|>And it would be good to provide more theoretical insights. Also, in the numerical results, how about the training time comparison between the proposed method and baseline methods? The strength of this paper is a new adversarial training method that treats the problem as distributional wise, as compared with traditional sample wise adversarial training methods, and the proposed method has been shown to have better performance than some baselines.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The experiments are evaluated with particle based fluid dynamics simulation dataset and scanned human action dataset. TPU GAN is well designed to learn the underlying temporal coherence from a point cloud sequence, but additional experiments and additional explanations about the model and modules are necessary. Please provide a short summary justifying your recommendation of the paper. TPU GAN can determine for each point whether it should be upsampled or not, and perform upsampling only for the points that need upsampling.<|endoftext|>This paper proposes a GAN framework for super resolution on the dynamic point cloud sequences. Although addressing an interesting problem, the novelty and contribution seem a bit limited. The direction that spatial temporally upsamples point cloud sequences without scene flow supervision is correct and promising.<|endoftext|>4.In equation (3), why is it used 3 as the threshold? 5.Please, proofread the work. The authors investigate proposed methods both quantitatively and qualitatively on two different datasets. The paper proposes TPU GAN,  an approach for point cloud generation concerning temporal coherences. The authors  results are reasonably better than others analyzed methods. "Flownet3d: Learning scene flow in 3d point clouds." The novelty here is to use a GAN that incorporates learning temporal coherence. However, it remains the question about the novelty s significance. If the authors provide reasonable explanations, I am ready to recommend the paper for acceptance.<|endoftext|>This paper proposes to use a discriminator to enhance the temporal coherence for point cloud upsampling. The paper has clearly several weaknesses, but it is undeniable that it has brought forward an interesting topic of point cloud upsampling with temporal cues, and developed two relevant tasks that are useful in real life. I thus opt for "marginally above the acceptance threshold" as the initial rating and hope the authors could address my concerns during the rebuttal period. I would assume adaptive upsampling is especially critical for point cloud sequences as it is highlighted in the abstract.<|endoftext|>This paper proposes a framework called Temporal Point cloud Up sampling to capture temporal and spatial features, in light of the difficulty to obtain point correspondence annotation. Overall, the main issues of this paper are missing hyper parameters experiments and unsatisfactory presentation quality. How about using other losses? 2.Add visualization results: It is interesting that this work adopts a GAN to achieve super resolution of points cloud. For this block, It would be better and convincing if the input and output point cloud during the training stage (for example, the first epoch and the final epoch) could be provided.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The demonstrated method allows the use of lots of untranscribed data for TTS, which is very useful. The experiments and analysis are weak. Even with the use of more data, the method is not able to outperform existing models.<|endoftext|>Besides that, one of the main benefits of using untranscribed speech data, which is to reduce the amount of annotated transcribed data required, is not showed in the paper. Overall, this paper proposes a new architecture of utilizing both untranscribed and transcribed speech data to generate speech with good quality.<|endoftext|>Strengths:(1) I really like the direction, doing TTS by decoupling audio modeling, text (phoneme) modeling and speaker modeling. The paper also propose practical trick to make it works better as norm based guidance. The biggest advantage of the proposed framework is it was able to use large amount speech data so it potentially could modeling wider range of prosody or other aspect of speech. However, the paper s experimental results cannot well support it.<|endoftext|>This enables them to train a generative model on unlabelled speech data. The latter of which is quite important for TTS. I find the level of control that the use of a duration predictor offers intriguing. This paper presents a novel method for TTS with convincing experiments and results.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors in this submission choose images as cover content and audio signals as secret messages. This is achieved by cleverly designing a neural network pipeline with recursions to tackle the variable length message issue. This means that the proposed approach should be compared to such methods in the experimental section of the paper. •	there is no notion of secret key in the authors’ approach. The authors did not evaluate their approach wrt these opponents and thus fail to prove the operational interest of their steganographic scheme.<|endoftext|>This paper presents a steganographic approach called Variable Length Variable Quality Audio Steganography (VLVQ) that encodes variable length audio data inside images with varying quality trade offs. StrengthsThe paper is written well and the approach is explained clearly.<|endoftext|>The paper presents a method for steganography, that is to encode and decode an audio signal into an image in an unnoticeable way. But the technique does not seem novel enough, the discussion on the method does not seem necessarily comprehensive, and the performance does not seem convincing. Also, are the second to fourth images showing audio encoded images or the difference from the original image? The feature of the proposed method is that it can handle audio with variable length, and can flexibly trade off between audio encoding quality and image modification unnoticeability in the inference stage.<|endoftext|>In the paper, the authors propose a method to  hide audio inside an image with variable length and variable quality leveraging a deep learning based steganographic method framework capable of hiding variable length audio inside an image bytraining the network to iteratively encode and decode the audio data from the container image. 2) In section 4 (Experimental section), the varying trade offs of gamma parameter and data on varying length of signal hiding could be valuable/important for researchers in the field of steganography. Please mention this as it could be any length. The design justification is missing from the Method s section. 6) In section 4, the experiments are lacking in comparative study. Please refer to the points before the paper could be considered for presentation at ICLR.
Reject; rating score: 1; rating score: 3; rating score: 3; This paper introduces a method that combines the representation of text and time series to make financial crisis predictions. The proposed method achieves the best performance in the empirical study,Weaknesses:  There are too many apparent and fundamental mistakes in this paper:      Though it looks similar, this paper does not follow ICLR s official format. The authors did not hide the acknowledge section in this submission, though there is no information leak. There are grammatical errors in most of the paragraphs. The technical contribution and novelty are relatively limited or not well presented. It would be much better if using a PDF version. This paper proposes a new and powerful method to solve an intersting problem.<|endoftext|>The paper proposes a method for predicting stock market crises using a deep learning approach which combines time series stock market data with text from news articles. Experiments find that the method works better than the same model using only news or only stock prices, and a couple of deep learning baselines. There are insufficient baselines. BERT is not consistently capitalized throughout.<|endoftext|>The authors propose a model for stock prediction by aggregating the embeddings of automatically mined news keywords and the embeddings of stock series. The authors evaluate their model in a large dataset and show that it outperforms a TF IDF based model and a CNN based model. I’d like to also remind that my task, as a reviewer, is to provide a helpful review so that the authors can receive an outsider view. More specifically: The language is below standard, the related work coverage is very limited, the claims are contradictory, the model is too naive, the experiments are insufficient, the baselines are weak, and the analysis is superficial. The paper has no contribution in my opinion. The reader should infer this himself. [1] Fune et al., Stock prediction: Integrating text mining approach using real time news. The presentation of the paper is below standard, it has no contributions, and the experimental setup is not convincing.
Accept (Poster); rating score: 8; rating score: 8; rating score: 3; The authors derive a neural tangent kernel for soft trees, and prove several properties of the kernel. The authors define the TNTK and prove a number of properties which one would like to know about the kernel. Such as positive definiteness, and change during training. The kernel shares some of the properties of the NTK, indicating that the TNTK can be used to understand training behavior. This is not surprising, considering that soft trees can be viewed as a neural network. The depth degeneracy property is a result with practical utility. This paper represents the development of an important tool for understanding tree ensembles.<|endoftext|>This paper proposed the Tree Neural Tangent Kernel (TNTK) for tree ensembles. The proposed idea extends the NTK concept to tree ensemble models and enables ensembles of infinite soft trees. Their numerical experiments support their theoretical results. This paper is well written and clearly organized. Compared with MLP induced NTK and RBF, one of the advantages of TNTK will be the low computational complexity. It would be better to provide a detailed complexity analysis (e.g., memory cost, training time complexity, and inference time complexity) of the proposed TNTK. Please refer to my main review.<|endoftext|>The paper presents an extension of a neural network analytic toolcalled Neural Tangent Kernels to its use for decision trees andforests. The focus is a previously proposed notion of soft trees. A few analytical results are presented about the properties of theextended notion, called Tree Neural Tangent Kernel, such as theexistence of some limiting deterministic form, how alternativeprobabilistic tree ensembles would have their kernels converging tothis deterministic form, that this limiting kernel is positivedefinite, and that the kernel remains stable during training,equivalence of the kernels for one level tree ensembles to those of two layer perceptrons, etc. Some support for the analytical claims is provided by simulation experiments. For the rest of the audience, the paper has not made an attempt toclearly lay out all the basics and the logic. It should also be stated upfrontwhat is expected of the proposed extension. What insight contributedby the NTK do you expect to reproduce for tree ensembles? Many other improvements are needed in the presentation of the logic,especially on the sudden appearance of certain claims and argumentsaround or against them.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The paper uses linear models trained on latent dimensions of an existing deep generative model for molecular property prediction. The normal directions of the learned decision boundaries are then used to manipulate molecules. The work does not consider comparison with.<|endoftext|>Finally, some things are also unclear, and the relation to prior work is somewhat misrepresented. Interpreting and dissecting generative models of molecules is an interesting and important direction, and one that so far has not been sufficiently explored. Distance in the latent space?<|endoftext|>I find this awkward and not well explained **(major )**. ## OverviewThis paper addresses an important problem which few prior works have addressed before in the literature. I think that the choice of experiments was generally good, but I have concerns about some choices that the authors made. I object to the use of the word "continuous" in your definitions, which you use several times.<|endoftext|>The authors also developed an interface for interactive molecular discovery. My first concern about this study is that there are many methods that have simlar concepts (model agnostic and does not require re training of the molecule generative model). I m wondering if the proposed method is superior to or is advantages in any aspect over existing methods.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; Connections between neighborhood prediction and the XMC problem are also established. 3.Both theoretical analysis and experiments are convincing. This paper develops a self supervised learning framework to extract node features supervised graph. It is an interesting problem. Theoretical analysis is also provided.<|endoftext|>The paper proposed a self supervised learning framework for learning node feature by exploring the correlation between the node feature and the graph structure, which leverages the graph information based on neighborhood prediction. Partial theoretical analysis is also presented. Strengths:+ Introducing the idea of neighborhood prediction to guide self supervised node feature learning is interesting and somewhat novel.<|endoftext|>This paper presents a new self supervised learning framework to enhance language model based on graph information. Strengths: The proposed method is simple and reasonable. The experimental studies are extensive. (4) What is the dataset used for pre training GIANT?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The ROI on any clinically relevant ML system can be called into question due to computational costs. The lack of performance boost is relevant and the authors point this out. Weakness:While I think the authors report on an important topic, the work lacks a strong enough negative or positive contribution to meet the threshold for publication in ICLR.<|endoftext|>It shows that the RL based method, which adpatively computes the sampling mask, does not offer benefits over fixed masks. Pros:  The paper focuses on a clinically relevant topic  The paper presents a solid comparison  The writing is clear and easy to follow. Cons:  There is a lack of methological novelty. But it lacks the typical methological novelty necessary for an ICLR paper. This paper is more suited for MICCAI or medical imaging related conferences.<|endoftext|>This paper explores the benefits of current deep Reinforced learning in accelerating MRI sampling and concludes that the benefit brought from current SOTA RL methods is not aligned with the computational complexity of these methods. I am not optimistic that RL based sampling methods can be really applied in MR imaging industry. As for the experiments, are these experimental settings well set for a fair comparison? However, I m not sure if this paper is suitable for submission in ICLR.<|endoftext|>Pros:The paper studies the real world problem of how to accelerate the acquisition of MRI scans and whether RL methods can help in this scenario. The display of the results is not very clear to me, e.g.: What s the difference between the AUC for sampling rate or acceleration rate in Table 3? How are they computed? As such it would be useful to present the results of all experiments with all possible metrics and state which metrics would be favourable to use in which scenario. However, later it is enough that the performance of the RL methods are matched. Alternatively, one could add a table to explain the possible options for the different design axes. While is true..."  > "While it is true"? Similarly, the analyses should incorporate the different choices of evaluation metrics.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; This paper develops a new generative model, called CoopFlow, by combining the energy based model (EBM), normalizing flow (NF) and Langevin dynamics (LD). This paper provides a novel cooperative way to train the flow model and energy model. The idea is very attractive, and could be able to solve existing difficulties of generative models. My major concern for the CoopFlow comes from the fact that the training procedure cannot be interpreted as the minimization (or minimax) of a loss function. Some other comments are as follows:1) The Langevin dynamics used in the paper is not an exact MCMC chain due to the time discretization. Is it possible to utitlize some MCMC samplers, e.g., Metropolis adjusted Langevin algorithm in CoopFlow?<|endoftext|>While the Langevin flow is trained with a standard MCMC likelihood toward the data distribution, the normalizing flow chases the Langevin one by maximizing the likelihood of the synthetic samples. The idea is sound and well motivated, however, my main concern on this work is that it seems both the theoretical explanation and empirical results do not support such a motivation very clearly. I don t think every newly proposed model should beat the best model. Learning non convergent nonpersistent short run MCMC toward energy based model. Overall, while the paper is interesting, the lack of justification for the authors’ motivation prevents me from recommending a clear acceptance for this paper. I am sorry to keep my score as 6, mainly due to there is no review score of 7 in the system.<|endoftext|>This paper proposes to train an energy based model with a normalizing flow to achieve rapid and high quality MCMC sampling. It shows the trained CoopFlow is capable of synthesizing toy data and realistic images (reasonable compared to many baseline methods in terms of common evaluation metrics). Strength  The idea of the paper is well simple, and it seems to work well to generate realistic samples. In particular, there is no condition  or evidence to show that the generator converges to a fixed point in (8) and (9). Questions:   It is not clear what is evaluated in Section 5, is it the training set or test set? Can MSE score (often used in image reconstruction) be provided for Figure 3?<|endoftext|>The paper proposes a generative model that consists of two components: a normalizing flow and an energy based model with short run MCMC as proposed in (Nijkamp, 2019). The main issue of the paper is that the particular choice of the generative models is not well motivated. Unfortunately, the paper doesn t address any of these questions. To be more precise, the authors do not provide any motivation for the usage of these particular models. The ability to model the density is not demonstrated empirically. The authors neither prove nor demonstrate empirically the convergence of the procedure.
Reject; rating score: 3; rating score: 6; rating score: 8; According to Figure 3(b) and Algorithm 1 line 6, the PPD head is trained by minimizing binary cross entropy between it and the true label logit from the normal output. This paper proposes an ensemble or mixture of experts method to defend against adversarial examples. * at inference time, for each input, the sub model with the highest PPD produces the output. 2) The experimental results do not show advantage over previous non ensemble method. Experiments on CIFAR 10 with L_inf attacks are reported. However, it is flawed, at least in its current form. However, after the adversarial training has converged, a vast space of adversarial examples has not been explored at all. The above discussion can be easily extended to the general case of M sub models. It s unclear that the proposed method has an advantage. The role of the PPD head is not explained well.<|endoftext|>This paper presents a new paradigm for defending against adversarial attacks with multiple sub models. Different from ensemble, the proposed collaboration paradigm, a representative sub model is chosen to make the decision, instead of letting all sub models vote. The proposed method has been validated on CIFAR 10 dataset, against both white box and transferrability based black box attacks. The proposed method is clearly motivated and defined. And it has been demonstrated effective by quantitative experimental results. It would be better if the training cost of the proposed method is compared with that of the previous methods. This paper presents a new paradigm involving multiple sub models which has several advantages compared to ensemble. It has been validated effective by the quantiative results in terms of robustness, but comparison in other aspects such as training cost could be also helpful.<|endoftext|>To improve the utilization of (multiple ) model capacity, the author proposes an interesting collaboration strategy $CDA^2$ to defend against adversarial attacks. Specifically, the author develop a dual head model structure: one is for making a prediction and the other is for predicting the posterior probability of the input. During training, each model can address the adversarial attacks of other sub models so that it improves the robustness of the collaboration. The experimental results partially verify the superiority of the proposed methods. In NeurIPS, 2020a. They address my concerns on the concept of collaboration, and the newly added experiments are convincing. A novel and interesting collaboration method for advancing the robustness of multiple sub models. It could be a good paper for the ICLR community. Figs.1 and 2 show the motivation of the proposed method clearly, which helps understand it. # Weakness:1. the author states that the insufficient model capacity can hurt its performance in adversarial training. Are there other scenarios that have insufficient model capacity?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper considers the problem of zero shot domain adaptation. The contributions are two fold. The authors provide some empirical evidence of the benefits of their approach, on two different datasets. The paper is well written, clear. Is there another dataset that the authors could scale to, such as adapting the CelebA dataset to a similar task as the MNIST experiments? I have not found flaws in the proofs or reasoning of the paper, and find the theoretical contributions particularly interesting. As noted in my confidence score, I am not overconfident about this, and would recommend weighing my opinion accordingly.<|endoftext|>The paper is very well written, with very few typos, and technically sound. Nonetheless, I think that some aspects should be further clarified. ( ) In the experiments with the MNIST dataset, the authors create linear classifiers on two network layers, namely fc2 and fc3. How are the predictions of the two layers combined to produce the final decision? ii) The experimental results are not totally convincing. Vol.34.No.07.2020.Although I have some doubts about the applicability of the method to practical applications, I don t see this as a reason for rejection. Moreover, the paper definitely has significant novelty and is technically sound. I would like to see an improved experimental section to increase my score.<|endoftext|>In general, I am a little bit concerned about the significancy of the paper s contributions. Strengths:  In my opinion, it is a good contribution for zero shot domain adaptation analysis, it can have good potential future work on the theoretical side. The authors support their analysis with small experiments and by showing the stability of their results. Also the authors define uniform absolute difference in order to keep distribution same with the true distribution. The paper is not the first to do finite sample analysis in zero shot domain, see [1]. Some of the definitions in section 4 can be carried to preliminary since they are borrowed from the previous work. In table 3, what did you choose as the value of K?<|endoftext|>This paper proposes a specific domain adaptation framework where a subset of all possible domains is are observed during training. Given sufficient training samples for the observed source domains, a common latent representation as well as a domain specific linear classifier is learned. Strengths: While the theoretical results are interesting, the underlying assumptions are quite restrictive and will not hold for many computer vision tasks. Unless I see results on more challenging datasets such as WILDS, I will remain unconvinced of the usefulness of this work. It is not just the number of source domains T that matter.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper introduces "signed supermasks", which builds on top of the supermasks line of work of finding binary masks on untrained networks that result in good performance. The proposed method learns parameters of a mask, and converts the mask parameters into a tertiary mask through the use of two thresholds. The paper further proposes to use ELU activation function and a ELUS initialization scheme that is more tailored to supermask training. **Strength:**  This paper is well written and the proposed method is clearly described. **Weakness:**  The paper motivates the signed supermask as a way to improve the interpretability of the trained model. The observations of layer wise pruning ratios are interesting, but are not new and have been well studied in the pruning literature. Is signed supermask a competitive method for training tertiary networks? This paper presents an interesting new approach of training tertiary neural networks with high sparsity levels.<|endoftext|>The main technical contributions are the new thresholding based training method and the new weight initialization scheme that considers the supermasks. Firstly, this paper is written clearly and easy to follow. Also, the new training technique is always coupled with the new initialization method. This is crucial because in (Ramanujan et al., 2019) the authors also investigated a certain form of scaling of the parameters which is found to improve the performance significantly. Minor point:   In table 2, should the baseline accuracy for conv 8 be 82+% instead of 72+%? Overall I think this work proposes an interesting extension to previous supermask works but lack necessary experiments, making the results less convincing.<|endoftext|>This paper proposes Signed Supermask,  an extension of the original Supermask work (Zhou2019) for finding more efficient untrained subnetworks. Instead of learning a binary mask, Signed Supermask claims and shows that adding another dimension  1 to the masks leads to higher sparsity with higher accuracy. [2] Evci, Utku, et al."Rigging the lottery: Making all tickets winners." The paper proposes a method to find untrained subnetworks that can approach the performance achieved by trained networks. The performance improvement achieved by allowing weights to flip is interesting. 2.Overall, the paper is well written. My main concern is the motivation and the usage of the proposed method. While the authors claim that the subnetworks discovered are untrained, I find that it requires training mask matrices with the same size as the model weights. 3.As I understand, the performance improvement of Signed Supermask comes from (1) allowing weight flip, (2) ELUS initialization.<|endoftext|>This paper discusses the signed supermask that can improve the model accuracy of untrained neural networks significantly while enhancing sparsity compared to the original supermask idea. The authors introduce " 1" as an additional mask value to enable flipping a sign of an initialized weight and suggest related activation functions and fixed threshold hyperparameters to achieve sparse networks. If the authors can reveal that the model accuracy can be significantly improved even for untrained models compared to previous supermask, it would be helpful to understand the inherent characteristics of the neural networks initialization. But as shown in Table 1, compared to the work by Zhou et al, even test accuracy is slightly degraded even with additional " 1" mask while  normal training  is still required. Otherwise, Supermask would not be practical in the field. Comparison on the previous works is missing. Supermask needs to involve much higher model accuracy (than previous works) or minimal efforts to be computed.
Reject; rating score: 3; rating score: 5; rating score: 8; rating score: 8; The authors propose to combine point cloud and graph encoders with coordinate and feature decoders in order to learn the geometry first, and then try to predict the adjacency matrix of the graph, to obtain a better overall graph representation of the considered applications. It would be beneficial and enriching if the authors can elaborate on this point. Were other methods considered to learning the geometry, and perhaps the authors can state what is the influence of using such template, which in many cases may not fit to the actual topology of the underlying graph. In what sense is it different/better than using something like GAT and its variants ? However, in order to reach to a more conclusive evidence of the importance and contribution of this work, I would suggest the authors to add more applications, such as learning the geometry and topology of meshes (e.g., ModelNet, ShapeNet). Also, the task of protein folding (e.g., AlphaFold) seems to be appropriate for the suggested method. The authors propose an interesting combination of known methods to learn the geometry and topology of the  graph for molecular based applications. The results reveal that some improvements over existing method is obtained. Some of the limitation could be further discussed, and more experiments would strengthen the contribution of the paper.<|endoftext|>This paper studies the problem of geometric graph generation (mainly focusing on molecule graph generation). Specifically, the authors propose a new method,  namely Geometric Graph Generator (G3), which generates three dimensional geometric graphs. Different from others, G3 can capture both the combinatorial and the geometric information of graphs. The key parts of G3 are the point encoder and graph encoder, and the three step decoding process, that is the coordinate decode, feature decoder, and finally graph decoder. The designed method may be applied to the application of drug discovery, etc. Weaknesses: The technical contribution is unclear/limited. It is unclear why G3 is better than others and why it is so efficient but not other methods. How this designed architecture is useful to guide other works? The authors aim at designing new NN architecture to generate geometric graphs. Compared with other methods, the reviewer does not see the real novelty part of the technical section.<|endoftext|>The paper proposes a generative models for learning over the space of geometric graphs   those graphs whose nodes are associated with geometric coordinates (point clouds). The point cloud serves as basis for decoding graph structure. Overall this is a well written paper with clear motivations and description of each model component. The problem of generating geometric graphs is important and the paper demonstrates that the proposed G^3 can leverage both graph structures and the point cloud geometry well. On the negative side, it looks like many components of G^3 are be studied before, hence reducing the novelty of the paper. This is a nice paper on an important topic and with a successful proposal evaluated extensively. The novelty may be less than ideal due to the integration of existing ideas.<|endoftext|>This paper is well written and focuses on an interesting problem, to design a better framework for a molecular generation while incorporating 3D geometry, using connections between 3D point clouds and GNNs. It is true that restoring a graph is challenging. Works [1][2] at the same period tackle it via matching the representations in the latent space instead of defining reconstruction loss on the data space. 2. a good integration of previous works, especially connecting generative models in point clouds and molecules. Unclear when interpreting tables 1 & 2. 3.Table 3 seems not to well support the claim of "It is clear that both ablated models are inferior to G 3 in nearly all molecular metrics, which attests to the efﬁcacy of G 3 ’s overall architecture. But it would be nice if the authors can make a few clarifications, which also helps with my understandings! I don t find an inspiring explanation in [1] or in this draft.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This work seeks to probe large pre trained language models for indications that they have induced a conceptual space that is structured similarly to one constructed from interactions with the real world, despite being trained solely from text. The experiments seem well controlled, and the authors present an insightful discussion of model performance. Overall I really enjoyed this paper. That is mostly an aside as I can t present it as anything more than speculation. And why were 67 other colors inserted in the color prompts?<|endoftext|>The authors present their work looking at how text only input can create what they call a "grounded" model. The results support the idea that a small input of text only data can give rise to the models behaving in such ways as to generalise over things like left and right or colours. I like that the authors explored a pre trained model.<|endoftext|>Specifically, the authors test whether the LMs can map some conceptual domains (e.g., direction, color) to grounded world representations (e.g., textualized grid world, RGB representation of colors). They conclude that the text only LMs may already learn the grounded representations implicitly, without explicit from scratch training like in the visual language models. I think this paper investigates a very interesting problem. The writing is clear and structured as well. This was not done in the paper and may be a good control experiment.<|endoftext|>The paper reports on the design and results of several experiments to test this in domains which can be easily serialized as text (and hence made accessible to an LM) — spatial directional terms, cardinal directions, and color terms. The ability of a language model to correctly ground conceptual terms is tested in a limited set of unseen environments and for unseen terms, where supervision is provided in the few shot learning paradigm popularized with GPT 3. * I think the framing and conceptual situating this paper does is pretty good. To fix them, it might be okay to just change the framing and back off from the "isomorphism" angle, but that feels like losing the whole point of the paper and leaving it kind of uninteresting. * There seems to be some confusion about the training data.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper makes an attempt to study the dynamic graph representation learning. The paper proposes a couple tricks for the combination with graph transformer networks. The tricks include sampling, union graph, pretraining etc. Some experiments have been conducted. Instead, the paper mostly focused on a few engineering tricks for applying the transformer architecture and some pretraining tasks. 1.While the tricks you propose for Transformer is of interest to some Kaggle audience, it s not clear what s the best way to combine time and cross section domain information. The simplest you can do is either treat each time snapshot as independent samples, and do basic training, this in practice work quite well. You can also use say RNN or TCN etc but unclear whether these will add value. It s also not clear what s the best ways. In this sense, this paper doesn t really touch upon this question at all. 3.If the graphs are quite noisy, say the financial time series, then will attention fail?Since the main focus is on attention, authors need to validate this.<|endoftext|>The authors designed a graph transformer for dynamic graph link prediction tasks. The model provides a scalable solution to graph transformers, and is claimed to be robust to noise. The difference among the three categories classified in "Dynamic graph representation learning" could be stated. For example, Section 3.1 could be moved to later sections after introducing basic paradigms, such as " temporal union graph". 3.It is confusing whether the authors work on static nodes (and dynamic edges) or dynamic nodes. 5.The scalability of the proposed method could be supported by larger datasets.<|endoftext|>In this paper, the author focused on the problem of dynamic graph representation learning and proposed a temporal union graph structure and a target context node sampling strategy. The authors are suggested to utilize or at least discuss these datasets, which could help the readers to compare the proposed method with the existing ones. For the weaknesses, it is suggested to provide more concrete experimental results to better validate the effectiveness of the proposed method. This paper investigates the methods for dynamic graph representation learning.<|endoftext|>In this study, the authors propose a new graph transformer network for dynamic graph representation. To solve the challenges of static graphs learning and the temporal information aggregating, this paper introduces a Dynamic Graph Transformer (DGT) which contains three components: (1) a two tower Transformer based method, (2) temporal union graph construction (3) a complementary pre training task. Extensive experiments on the two datasets of link prediction and node classification demonstrate the superiority of the model. The ablation studies justify the effectiveness of each module in the DGT model.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a partial Wasserstein adversarial network to register two non rigid point clouds. The major contribution is deriving a dual optimization objective to avoid the computation of a dense matching matrix. Experiments on toy/synthetic datasets and one small scale real world face dataset show the effectiveness of the proposed method. I would expect the following experiments:1) Experiments on the 3D Human dataset [Section 4.3, Non rigid Point Set Registration with Global Local Topology Preservation]2) For synthetic experiments, it would be better to use the modelnet40 dataset, which contains many different shapes. Since this paper contains many equations, re implementing it would be very hard. Overall, It s very nice to have a method to avoid the computation of dense matching matrix in an optimal transport framework.<|endoftext|>The paper proposes a method for registration of two point sets by formulating the problem as partial distribution matching problem. The authors utilise the partial Wasserstein (PW) discrepancy, and show how its gradient can be computed. They further propose a partial Wasserstein adversarial network that approximates the PW discrepancy. The authors evaluate the method against four method against two artificial datasets in which the method shows good results. As to the strengths of the paper, the authors are able to make a novel contribution to a classic problem. The theoretical part of deriving the KR duality for the two types of chosen discrepancies is clearly a strength of the paper even though it is a harder part of the paper to read. In summary, the work can be seen as well founded paper with clear theoretic contribution equipped with a practical method based on neural formulation of the registration problem.<|endoftext|>Paper proposes PWAN, a method that can be seen as an extension of WGAN in the presence of noise. Extensive experiments on toy datasets and 3 point clouds show that the method performs well, even in the presence of noise. The paper is interesting and deals with an important problem: how to deal with outliers when registering 3d shapes. I have few concerns that should be clarified:  eq.(2) is a reformulation of eq.(1).If there is a reason to have the two formulations, it should be better highlighted. the dual of the partial optimal transport problem is described in [1]. It would be interesting to add a discussion about it. What happens if their values are under/over estimated? What prevent PWAN to align dataset Y to outliers? An interesting paper deals with an important problem but lacks some appropriate references.<|endoftext|>This paper deals with the problem of non rigid point set registration, where a method for large scale partial distribution matching (PDM) problem is proposed by utilizing the partial Wasserstein 1 (PW) discrepancy. The paper theoretically derive the Kantorovich–Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these theoretical results, the paper proposed a partial Wasserstein adversaria network (PWAN), which approximates the PW discrepancy by a neural network, and learns the transformation adversarially with the network. + Experimental results on the point set registration task prove that the proposed PWAN is robust, scalable and performs more favorablythan the state of the art methodsWeaknesses:   The paper is rather dense, which may to some extent not well self contained. The theoretical contributions are justified with experimental evaluation on the point set registration task.<|endoftext|>Specifically the partial Wasserstein 1 (PW), where only a portion of the probability mass is moved,  isconsidered. The strong point of the paper is the  KR dual form derivation of $\mathcal{L}_{M,m}$ and this is not sufficiently highligthed in the paper. The dual is a constrained optimization problem  that involves a potential function defined as a neural network (NN). During the discussion, the authors clarify how their approach connects to mini batch regularized Unbalanced OT as promoted in Fatras et al.(2021) that solves the UOT problem in the primal. The authors should consider emphasizing more on that and better  highlight the novelty of the theoretical  KR dual form. This is also a good point. Learning of $\mathcal{T}$ by minimizing the PW discrepancy with acoherence regularization  leads to a min max problem similar to  theone of the Wasserstein GAN. The paper proposes a theoretical and methodological framework toachieve robust  point set registration using scalable partialWasserstein based on the dual formulation. The main results of the paper  are the derivation of the Kantorovitch Rubinstein dual of both  problems. Nevertheless the paper does not discuss  reference  [1] dedicated to unbalanced minibatch optimal  transport (UOT) that can be a good alternative to the proposed method. [1]  established theoretical properties of UOT in presence of outliers,  in terms of gradient regularity hence the convergence property. If so, how is it derived from the  prima dual equation?
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; Weaknesses:  The method appears to identify which of some set of transformations one should be invariant to (and learn correspondingly invariant representations). This hinges on the concept of being counterfactually invariant to the symmetry transformations that could appear at test time but don t affect Y. Next, the paper presents a method for learning OOD invariant representations through causal structure discovery. The experiments and comparisons are relatively minimal and are only in a toy environment.<|endoftext|>The authors propose an approach for constructing classifiers that achieve out of distribution (OOD) generalization using a new learning paradigm they call _asymmetry learning_. They consider OOD tasks where the test input is obtained from the training input by applying a sequence of (random) input transformations. Thank you to the authors for addressing our concerns in great detail. How often are the assumptions made about the data generating process expected to hold  in the wild ? The subfigure is also way too small to be readable without zooming in. Despite the lack of real world validation, I think the paper has a strong theoretical basis and is a decent contribution to the literature.<|endoftext|>I have updated my scores and recommend that the paper be accepted. This paper proposes Asymmetry Learning, a new learning paradigm to obtain counterfactually invariant classifiers. If the observed covariates are a result of some transformations applied to a hidden variable, and these transformations differ between training and test datasets, a classifier may not generalize to the test set. I wish the paper offered a greater discussion of prior work on counterfactual invariance as it relates to out of domain generalization. Overall, I think the paper presents a good contribution but I do have some minor concerns that I would like the authors to address as I have mentioned above.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Authors look at a problem where the rewards are non Markovian (but dynamics are Markovian). Throughout the paper, the range of $t_i:t+1$ is used but I do not know if I understand this range properly. But authors provide an interesting example later on confirming the fixed point bias, if the non Markovian structure is not accounted for. Further, using importance sampling it can be made to work in the off policy setting. One can now consider a new MDP that has state   concatenation of the past $n$ steps, and therefore Markovian in this new state space. Using this state space, all the existing policy improvement results will hold directly. Is it assumed that the value of $n$ for any interval is provided to the agent? Further, there are additional parameters for the proposed method which are indeed tuned :(  It is worth calling out before each Fact/Proposition that a tabular setting is being considered for that given result.Further, the proofs in the appendix have lots of writing errors and should be presented better.<|endoftext|>The paper argued that this new decision process is a suitable model for applications where rewards are delayed. For this decision process, the paper proposed analogues of action value function and policy improvement theorem. Strengths\         The idea is pretty new and interesting. It seems from the empirical results that the proposed algorithm performed well in the tested domains. There are just so many parts that are confusing to me.\         a) Generally speaking, it seems to me that this paper tries to present so many different things in one paper without making each of them clear to the readers. For example, the main theoretical contribution of the paper, the newly proposed decision process, once defined, is only briefly discussed in section 3.1. It is hard to tell the commonalities and differences between the new decision process and classic MDP. And I would also suggest the authors compare the proposed decision process with Semi MDP because they seem to be closely related. It is like algorithms, theories, and implementations are all mixed together without a clear explanation of each of them. But the action value function takes the entire trajectory segment as input, which entails more information than s_t, a_t, and t   t_i. I like the idea of the paper and would encourage the authors to keep working on it.<|endoftext|>The paper makes strong assumptions on MDPs with delayed reward that do not align well with knowledge from previous works. Instead of trying to compare and articulate the exact differences and conclusions of their work and previous literature, the authors briefly list some existing works at the end of the paper in an independent section without such warranted explanations. Lastly, the authors add experimental comparisons to several recently published methods. The paper studies the problem of delayed reward, which imposes non Markov behavior that hinders performance of "standard" RL algorithms. It then proposes a new Q function definition based on segments of reward, together with a decomposition to historical current parts. Next, even though the grammar is fine, I find the paper hard to follow. 2 that constitutes the basis for the consecutive results is not clear to me. But how does this make sense in a delayed reward setup where everything depends on past decisions? I would expect more formal and informal (intuitive) justifications for such a strong assumption. Note also that making this assumption on the Q function is stronger than making it on the reward. If not, can they show the opposite and provide a proof that there is always an optimal history that is history independent? This is expected especially as the authors are those who propose this model and later rely on this assumption. Also, what information exactly is required for the policy to be optimal?<|endoftext|>This work considers a modified MDP model where the rewards are subject to random delays. Specifically, the timesteps are divided into random batches and the agent observes the cumulative rewards of the batch when it ends. The authors propose a policy iteration method for learning the optimal policy within a restricted policy class. In particular, the authors consider a more restricted model, named Past Invariant Delayed Reward Markov Decision Process, and a policy class with an augmented parameter that indicates the current time step in the signal batch. 1.This paper proposes a new setting of delayed reward, namely DRMDP. It seems unclear what real problem can be exactly modeled by this setting. 2.This paper proposes to focus on a restricted policy class $\Pi_s$. Does it also belong to $\Pi_s$? 4.How to conduct policy improvement step for $\Pi_s$? But the assumption of the MDP model is not well justified   it is unclear what signal interval distribution justifies the assumption.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a new criterion for learnable representations of Boolean circuits and Turing machines. The criterion "statistically meaningful" is the main contribution along with its application to the above two classes. There are already many proofs that functions and programs can be represented by various classes of neural networks ("universality theorems", notably classical work by Cybenko/Barron which approximate arbitrary functions by depth 2 non linear networks and recent work by Eldan Shamir/Telgarsky showing the trade offs between depth and width). (It seems to me the latter objection only holds for specific universality theorems). The particular loss function proposed is apparently a variant of a recent proposal (but was new to me). I could not follow the second application (transformers can represent Turing machines). Presumably the value of this paper is conceptual; I don t see any benefits to ML algorithms (e.g., the loss function proposed is not robust, i.e., some errors in the data labels will cause it fail).<|endoftext|>This paper studies the approximation of boolean circuits and Turing machines using neural networks. For both the boolean circuits and Turing machines, the authors explicitly construct two classes of neural networks, based on feedforward network and transformers respectively, that achieve SM approximation. Here the surrogate loss function is a ramp loss combined with the all layer margin. I would suggest modifying the claim and giving more credit to the classical generalization theory. Thus, it is not clear how this work is relevant to practical deep learning. It would be great to have empirical results that approximately compute the global minimizer of the empirical loss. 5.In terms of the contribution, it seems that the major contribution of this work is to represent the Turing machine using neural networks, compute its all layer margin, and apply the standard generalization theory.<|endoftext|>This paper proposes a definition of "statistically meaningful approximation" or SM approximation which combines learnability, generalization ability and expressivity (approximation accuracy) in a straightforward way. Two theorems on SM approximation are formulated and proven, one for approximating Boolean circuits and one for approximating a general Turing machine computation with a transformer network. It is argued that the new definition has technical advantages. The presentation is clear and the proofs seem adequately proven. But the novelty and importance of the new definition is not adequately demonstrated for this reviewer. The two examples are improvements on previous work which, it appears, could have been applied with previous phrasings of generalization and approximation accuracy. I think much more discussion is required as well as much more review of previous related work for this to become more than just another definition added to a long list.<|endoftext|>The paper proposes a novel concept called statistically meaningful approximation. fully connected neural networks can SM approximate boolean circuits with a sample size that is poly in the number of gates and $\log$ in the width and depth of the circuit; (2). transformers can SM approximate Turing machines with a sample size that is poly in the number of states, alphabet size, and the log of the computation steps. The concept of SM approximation unifies approximation and generalization, which is novel and maybe interesting. This is interesting because we may expect different surrogate losses for different network structures. Therefore, to some extent, the nice sample complexity results in the paper are not that surprising.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper presents ACH, a neural network policy gradient method for approximating a Nash equilibrium in two player zero sum games. A model based method can also operate on one or more sampled trajectories through a game, using the game knowledge as it considers all actions (e.g., Monte Carlo CFR methods). The Mahjong results are impressive. If that s not what the authors intend, consider rewriting to clarify. Taking the core of this paper to be a general policy gradient update rule for competitive imperfect information environments, with a demonstration of performance in a large game of interest to human players, there is the base of a strong submission. However, the additional theoretical analysis and empirical analysis of small games have a number of issues that do need to be corrected. Algorithm 1 shows an update sampling s from all states. State   agent state / information set? Why is the number of actions being sampled tied tied to being model free?<|endoftext|>This paper presents Actor Critic Hedge (ACH): an actor critic method for approximating Nash equilibrium strategies in large extensive form games. ACH is an extension of the CFR family of algorithms that uses deep learning, and is able to learn model free by training on trajectories and not full game traversals or subgames, which is common in much of the related CFR literature. In Mahjong, the ACH agent is shown to defeat several human players, including a Mahjong champion.<|endoftext|>The authors propose a new, non tabular method for large, two player zero sum games. The method is evaluated on some standard benchmark games (e.g.Leduc poker) as well as on large game of 1v1 Mahjong. Actor Critic Policy Optimization in a Large Scale Imperfect Information Game Positive: I like that the paper uses 1v1 Mahjong, sounds like an interesting game that is not commonly used in the community. It is also great that the authors approximate exploitability in the large game using a RL method. 2) Corollary 1. DeepCFR, Double Neural CFR are actually very similar algorithms. For this reason, we do not include these methods for comparison with ACH, which is model free and uses only trajectory samples to learn.”. This makes the comparison even more unfair, as the prior methods were designed (and reported) using the average policy. Policy averaging in non tabular settings is a necessary part of a non tabular algorithm3) The reported baselines do not match the original publications, and the overall exploitability numbers are quite poor. The authors need to rework the experiments, and also properly compare prior work.<|endoftext|>Impressive results against a top Mahjong player demonstrate that this method can scale to large games. What I would like to see most in this paper is two things: more tabular analysis and experiments and better comparisons to existing methods. What properties does it have compared to normal CFR? The authors claim that they are not related because they are not a policy gradient method, but the proposed method isn’t really a policy gradient method either. However, I think this paper skips some crucial steps in analyzing the tabular method that this is based on. If the authors are able to address this issue and explain why we would want to use this method vs. deep CFR/DREAM or compare to those methods then I would consider raising my score.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposes a black box attack where, by relying on segmentation priors, the perturbation is applied only in the salient region. This allows one to obtain reduce perceptibility with a limited number of queries and a small reduction in success rate. However, there are two main issues concerning the technical approach and the experimental analysis. The approach is not compared with other black box attacks that also have the objective to make the perturbation imperceptible, see references below. In my opinion this is too limited to validate the proposal. However, I still believe that the paper needs much more comparisons with state of the art and that the proposal should be better justified from a theoretical point of view.<|endoftext|>The paper studies how to reduce the perceptibility of the perturbations to the original images produced by black box adversarial attacks (for the $\ell_\infty$ threat model). Strengths  The proposed method is simple and achieves the goal of improving the imperceptibility of the perturbations: using the segmentation prior is effective for avoiding changes on the background, and the Saliency Attack attains better MAD score. However, I think that how visible the perturbations are is more a property of the threat model (set of feasible changes) rather than of the attack used: for example, using a smaller $\epsilon$ for the same attacks would increase their imperceptibility. In particular, [C] show that small perturbations in the $\ell_\infty$ norm are not necessary if limited to certain areas to the image to preserve imperceptibility. Otherwise, the authors should better motivate why improving the $\ell_\infty$ attacks with such fixed threshold is relevant.<|endoftext|>The authors propose using segmentation priors to improve the black box attacks so that the perturbations are restricted to the salient regions of the image. In addition to this, they also present a technique that improves the imperceptibility without forgoing the query efficiency. They also demonstrate that some of the exiting black box adversarial attacks can benefit from their optimisations. In addition to this, they also propose a search algorithm that can further narrow down the candidate regions for adversarial perturbations. In the recent past, a richer class of gradient free black box techniques have been proposed in the literature. Why is the paper considering only two of those? Why was Feature Squeezing chosen as a baseline to evaluate the success rate? I also find the direction explored by the paper to be quite promising but found the experiments & the baselines chosen by the authors are lacking.<|endoftext|>This paper proposed a method to generate imperceptible attack in black box attack scenario by generating local perturbation blocks in salient regions. It used salient object segmentation to obtain the salient region, then applied a tree search method to find smallest blocks within the salient region that can cause the maximal change in predicted class logits. Experiments on 1000 Imagenet examples are conducted, compared to several existing baselines, showing that the proposed method can improve achieve more imperceptible attacks (where imperceptibility is measured by metric MAD). The topic of imperceptible black box is very interesting and not well studied, the ideas of generating perturbing blocks in salient regions are interesting, however, this paper can be improved with more sufficient justifications on motivations and experiments. 3) I can not find the information of which classification network is used in the experiments. However, it needs better justification on motivations, as well as improvement on experiments.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper propose using a multidecoder architecture to to control the style of output summarization. The contribution of each experts are controlled by a gating mechanism. This multidecoder model can be trained without supervision or with guidance. strengths   The task of controllable generation for summarization is well motivated. The finding that different decoders indeed capture different level of abstractiveness and specificity is interesting. The paper is clearly written and the methods are experiments are easy to understand. Lacking such a dataset, human evaluation would shine some light, which is missing in this work. The paper presented a novel architecture for style control in abstractive summarization.<|endoftext|>This paper proposes a neural sequence to sequence (Seq2Seq) model, called HYDRASUM for text summarization. The paper demonstrates that HYDRASUM automatically learns to generate contrasting summary styles using each decoder. The paper also show that HYDRASUM is flexible, during inference, it can be controlled to generate a diverse set of summaries by sampling from individual decoders or mixtures of different subsets of the decoders. **Novelty**The fundamental idea of the paper to use multiple decoders and a gating mechanism to improve abstractiveness in model generated summaries is previously explored in [1]. As the Table 1 suggests, the overall performance of the proposed model does not surpass the baseline in 2 out of 3 evaluation dataset. Is it possible to perform human evaluation to judge the diversity of the generated summaries? The novelty of the proposed method is thin.<|endoftext|>This paper proposes a new architecture for controllable text summarization, leveraging multiple decoders (with the bottom decoder layers  shared across decoders) with the contributions of each decoder controlled by a gating mechanism. The networks are shown to allow some stylistic controllability, most notably in terms of abstractiveness and specificity. Strengths:  the paper is well motivated and tackles a hard and long standing problem with seq2seq models: diversity and controllability. the approach is fairly simple, interesting and to the best of my knowledge, novel. It would be good to mention it explicitly. I believe this work is interesting as is, but were surprised by some of the experimental settings that I would like to see clarified by the authors.<|endoftext|>This paper proposes an architecture for controlled summarization, allowing some type of aspect based summarization, as in   for example   "I want long/detailed summary". They obtain this by using a Transformer (BART like) seq2seq model, but where the last layers of the decoder are multiplied and considered as independent expert which are combined in the end. The proposed solution is simple (good!) The analysis focuses on evaluating the control capacity and diversity with respect to other (smaller) architectures. This provides an alternative to controlled generation. Most importantly (I consider this the main novelty of this approach), they can be combined. The evaluation focuses on the strong point of the models (more diversity) and is somehow unfair (against non controllable models that are smaller). In this sense it is not clear if it is better or just different than existing methods.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper introduces a new method based on learnable hash functions to reduce the O(N^2) cost of self attention in transformers to O(N^1.5). The paper investigates the effectiveness of related approaches regarding bucket imbalance issues by showing statistics for several attention heads of a pre trained transformer. The idea of having a learnable function prior to bucketing is not entirely novel (see a contemporary idea by Treviso et al., 2021), but the formulation is concise and clear, enabling generalizations to previous approaches. Therefore, the paper presents a step forward for this field, and I recommend it to be accepted. Results on the GLUE benchmark show that the proposed method achieves results on par with the original RoBERTa model, which is a good sanity check. Most of my concerns are about the lack of correctness in some parts of the paper, such as choosing hyperparameters or evaluating LSH attention. Therefore, I am learning towards acceptance. However, the authors can easily address these concerns in the rebuttal period. Moreover, from which pre trained model were the results extracted?<|endoftext|>This paper proposes a learning to hash attention (LHA) to learn sparse attention for Transformer. * An approximation of the attention utilities is proposed with random Fourier features to reduce the complexity. * It is not clear how to apply LHA as a plug and play replacement for dense attention layers in pre trained Transformer models. * The implementation is inconsistent with the analyze. If the authors can address my concerns, I would be happy to increase my score.<|endoftext|>This work studies the content based sparse attention in transformer and how to improve the self attention part, i.e., efficiency and effectiveness. The authors figure the bucket imbalance problem in ANN derived content based sparse attention and analyze its weakness on the imbalance problem. The learning to hash based attention model is proposed to improve the effectiveness of sparse attention. Therefore, the authors need to clarify why replacing LSH with learning to hash models is useful in sparse attention. The authors should give full reasons for your contributions. However, there are questionable points that should be clarified.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; In this work the authors introduce a differentiable stride formulation, which allows for learning the stride value. To this end, they propose learning the size of a cropping mask in the Fourier domain, which allows for learning how to perform resize in a differentiable way. Authors present experiments on several datasets on different domains (image, audio), including large scale datasets. + The paper is well written and the proposed formulation is sound. Authors demonstrate that the proposed method can recover from different initial strides and learn the optimal one. However, my most important concern is the lack of appropriate comparisons with neural architecture search approaches, since   in my view   these are the most direct competitors of the proposed method. What is the actual overhead of the using the proposed method in real applications? Overall, I think this is a good paper with an interesting approach on differential stride learning.<|endoftext|>This paper proposes an approach to learning optimal striding parameters in convolutional networks. The proposed approach, DiffStride, is a downsampling layer that builds on spectral pooling to allow for integer output dimensions, but arbitrary strides by cropping in the Fourier domain. Unlike spectral pooling, DiffStride relaxes the parameters of the cropping mask to be differentiable, and using the stop gradient operator in the cropping. Positives Overall I feel that the paper presents a really neat idea well. There is thorough experimentation on a range of tasks and model architectures that demonstrate the power of the proposed approach. Obviously such results can be caveated with a statement that the implementation could be improved as per the existing discussion in the limitations section.<|endoftext|>In this paper, the authors propose DiffStride, a technique for learning the stride of downsampling operations in neural networks by gradient descent. Performances against both standard and random stride policies are reported on a number of datasets, both for audio and image recognition. Moreover, the authors introduce a regularization objective that penalizes the use of small strides, in the interest of encouraging downsampling for improving computational and memory cost. The authors excel in framing their work within related art. The contribution is novel and clearly motivated, and experimental results are encouraging across different datasets. The paper is, to the best of my effort, technically sound. In terms of novelty, one might argue it being a bit incremental over SpectralPool, as it substitutes the cropping hyperparameters with learnable values. CONsI think this paper is very solid, and I consider the following points as minor concerns.<|endoftext|>This paper proposes DiffStride as a drop in replacement to standard downsampling layers. It extends previous work Spectral Poolnig and learns the size of the cropping box in the frequency domain by backpropagation. Experiments are conducted on audio and image classification, and the results show that the model can learn non integer stride and adapt different initial stride well. The idea is interesting and makes sense to me. The authors may want to find more ways, as discussed in Sec.4, to show its value. Post rebuttalAfter reading the authors  response, I raise my rating to 8.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; To me it is more on matrix formulation of ILP problem. This paper proposed a neural nets based approach for Temporal Logic Programming. Strength	1.  formulating combinatorial rule space search as predicate selection indicatorWeakness	1. 3.The experiments baselines are of the authors  own design; it lacks a comparison to the literature baselines  using the same dataset. I am guessing it is elementwise multiplication from the context. This is a good initial attempt to attack the neural temporal logic programming.<|endoftext|>The paper proposed a new approach called neural logic programming to recover composite events from complex time series data using atomic events and their temporal relations. Strengths: the two step approach (i.e., parameter learning followed by structure learning) does seem novel for the problem. Not only that the baselines (two LSTMs) are rather weak, but also that the proposed approach seems to be even worse than the baselines in terms of mean average precision (which is the mainstream metric used on CATER) in Table 1. Although there is some novelty in the proposed approach, the empirical results are too weak to validate the effectiveness of the approach.<|endoftext|>The paper proposes an end to end differentiable strategy (called neural TLP) to learn unknown temporal relations between atomic events (like after(miss, swing), “miss occurs after swing” in the baseball example), subsequently used to predict composite events (like strike). The performance of the proposed strategy is evaluated on video recognition (CATER) and healthcare (MIMIC III) datasets against two baselines, namely a LSTM neural net and a simplified version of the proposed strategy. If so, why is that the case? Why is this considered as parameter learning? I like the idea around the proposed differentiable operators used to predict the temporal relations among atomic events. Also, the experimental results are interesting and promising. However, they are preliminary.<|endoftext|>This paper presents an approach for learning temporal rules from data. For me, it was also quite unnatural to say that composite events are a temporal (e.g.in the baseballexample on page 2). The paper addresses an important problem of learning temporal rules from data, for which many applications exist (as the paper convincingly motivates). Overall, I thought this is an interesting contribution to the ICLR conference.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; **Summary:*** The paper puts forth a greedy algorithm that maximizes the true discovery rate w.r.t.a set of labels (akin to coverage), subject to a user specified restriction (i.e., constraint) on the false discovery rate, for the goal of building small but nevertheless valid prediction sets. * A few calculations/results are worked out showing that the method works as advertised (i.e., TDR maximized, FDR controlled). * Some empirical results are presented, where the method s TDR and FDR (along with those of one baseline from some other work) on three real world data sets are evaluated. * The three real world data sets seem reasonably interesting / well motivated. * The paper is a pleasure to read. I may have missed it, but what are the sizes of the sets you get in Sec 5? In particular, I think you should also report the conformal set sizes required to attain, say, 90% marginal coverage; otherwise, it s hard to say what exactly you are improving on. It does seem though, from Figs 2,3, that to get 90% coverage on the in silico example, you effectively need to include half the label set. * Along these lines, you seem to be "cheating" a little bit   your stated motivation, i.e., cases when when the label space $\mathcal Y$ is large, seems to require ... $\mathcal Y$ to be large. Yet you effectively truncate $\mathcal Y$ to 100 in all the experiments. * Also, it doesn t seem totally fair to not use CQC for the "Inner Sets" baseline, b/c then that baseline produces constant width intervals. Accordingly, can you say how many dimensions there are in each of your examples? * Part (c) of Figs 2,3   TDR appears to asymptote at 0.8. Doesn t this require additional data? Both your method and standard conformal will return a set. But at the end of the day the practitioner still needs to use a single point to make a prediction. I mean I get that you are trying to produce smaller sets, but you are still returning a set (i.e., more than one) of plausible values. I like the paper. Happy to change my mind if I missed something.<|endoftext|>A new conformal method for multi class prediction problems is proposed in which the number of false discoveries is capped while the proportion of true discoveries is maximized. The claim is further supported by experimental results. ## StrengthsThe method is well motivated from a practical standpoint. ## WeaknessesThe main issue has to do with clarity. There is also $m$ in Remark 4.4, which appears to have the same meaning as the first $B$. It is confusing to have the same index $i$ be used to index many different quantities. It is next used to index candidate labels, e.g., $\phi(x, y_i)$. 2.I do not think the paper ever explicitly spells out the precise meta algorithm (the FD CP algorithm) for either $k$ FD validity or $(k, \delta)$ FDP validity that can represent the proposed method. On a related note, the discussion in Section 4 does not appear to sufficiently differentiate between the general construction (e.g., nonconformity scores for nested sequences of sets) versus a particular example (e.g., the scores of Eq.(9)).3.In Section 5, four baseline methods are described, but the reason behind the choice is never explicitly given. In particular, one of the baseline methods is not expected to have validity, raising the obvious question of why the method was considered in the first place. The last two (max and avg) were probably included as alternative scoring methods to the scoring method using DeepSets, but this is an educated guess. Also, if this was indeed the intention, then I feel like more space should have been devoted to a discussion of what makes DeepSets an appropriate choice, especially in light of Remark 4.5. 4.Both figures are hard to read. This appears to be due in part to imposing the same scale on the $y$ axes. Also, the top and the bottom panels are misaligned. ## Minor  (p. 1.The 1st line of Section 1) instead of single prediction  > instead of a single prediction  (p. 2.The 4th line after Eq.(2)) Spell out the initialisms. (p. 6.Theorem 4.3) $\tilde{\mathcal{F}}(X_{n+1}, \mathcal{S}_{n+1, j}) \leq T_k$ or $T_{k, \delta}$, rather? (p. 7.**In silico screening for drug discovery**) I think the correct usage places the comma before the end quote and not after. Do we take the expectation over $E_f$ before using Eq.(23)?(p. 14) w.l.o.g. (p. 14.The 2nd line after Eq.(26)) $\mathcal{S}_{n+1, j} \leq T_k$  > $\tilde \mathcal{F} (X_{n+1}, \mathcal{S}_{n+1, j}) \leq T_k$  (p. 14.The last line of the proof) $\mathcal{S}_{n+1, j} \leq T_{k, \delta}$  > $\tilde \mathcal{F} (X_{n+1}, \mathcal{S}_{n+1, j}) \leq T_{k, \delta}$. ## Additional CommentsI am not sure if the title "Trading Coverage for Precision" is a good fit for the method being proposed. I would say that the method is more about using achieving a different target for validity. However, the issues of clarity are such that I cannot be sure of having evaluated them correctly.<|endoftext|>Conformal prediction is a framework that allows building a wrapper around a predictive model. Subsequently, a point prediction (corresponding to the top ranked label) is replaced by outputting a set of labels that provably contains the true label of a test point with high probability (with the guarantees being only marginal). However, in many applications due to present noise, the resulting prediction sets are larger / more conservative than expected, with many incorrect labels being included, and the presence of many false positives could be an unfavorable event in many real world scenarios from the actionability standpoint (e.g., drug discovery, where adding an extra element to the prediction set corresponds to a significant cost increase of an experiment). The current paper studies a way of trading off coverage for precision, from both theoretical and practical standpoints. In general, the paper is well written, with a detailed review of relevant prior works in the literature. The approach roughly boils down to: (a) fitting a multi label predictor, (b) fitting a model that estimates the number of false discoveries, (c) using a proposed non conformity score and a held out set for calibrating a final threshold (subsequently used for constructing prediction sets) that allows provably controlling the number of false discoveries. I was wondering whether the authors could elaborate more on the advantages of controlling the total number of false discoveries when compared against controlling its normalized version, false discovery rate, for which actually results have been established recently. A side notes about some typos: a. Several typos in Theorem 3.1: in equation 3, $x_{n+1}$ should be replaced with $x$. Also, in the line that follows equation 3, $C_\epsilon(X_{n+})$ should be replaced with $C_\epsilon(X_{n+1})$. The main reason for lowering the score is that several papers have considered post hoc uncertainty quantification procedures for multi label classification settings (possibly a more detailed lit review with focus on this is necessary). The authors propose a way of controlling the total number of false discoveries (either in expectation or with high probability) based on conformal inference. **Update after rebuttal**I would like to thank the authors for the detailed responses. Taking into account the general contribution of this work and points mentioned in this and other reviews, I tend to keep the current score.<|endoftext|>This paper presents a new calibration procedure that builds upon the framework of Distribution free, risk controlling prediction sets (RCPS) by Bates et al.(2020).The latter trades off the coverage guarantees of conformal prediction in favor of precision guarantees. In other words, here the focus is on controlling the k family wise error rate (k FWER) instead of the FDR. The second contribution is the formulation of a deep net model that estimates the distribution of false positives in a candidate prediction set. This model is plugged into the proposed calibration scheme, used to find a threshold for rejecting or accepting candidate sets generated by the model. Lastly, in a series of experiments, the authors confirm the validity of the theoretical guarantees presented in the paper as well as the advantage of the proposed deep net model over baseline methods. I believe it is a good contribution to the ever growing literature on conformal inference. The first comment is about the apriori choice of k in k FWER. Currently, the paper does not discuss this issue, which I believe is central for deploying the proposed technique in real world applications. I understand that forming $\phi(x,y_i)$ as the marginal likelihood of $y_i$ being a correct label for $x$ is simple to implement. However, this seems to be suboptimal. Here, a synthetic simulation can be very useful. Also, when |\mathcal{Y}| is large, it seems to me that using the FDR control procedure is more natural and can be more powerful (as long as the number of true positives is large). Experiments: the authors mentioned (Section 2) that “[...] as most of our target applications have relatively few true positives, FDR control can be somewhat volatile and lead to many empty predictions.” I believe it will be valuable to demonstrate this issue in experiments, to support the need for k FWER control. Lastly, please include in the Supplementary Material more details on the data sets used in the experiments. Also, discussing (e.g., via simulated data) the advantages and limitations of k FWER over FDR seem to be important for the users to better understand which error metric is preferred, given the nature of the data at hand (and the existence of prior work).
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper investigates the nudge recommendation problem in a mobile healthcare domain, and introduces a knowledge graph based recommendation approach. The key challenge is claimed as the cold start problem in this domain. In this paper, the authors developed knowledge graph based recommendation approach for recommending healthcare related nudges to mobile users. Finally, the presentation of the paper could be improved as there are grammar errors and typos that make reading not easy.<|endoftext|>This paper introduces a knowledge graph enhanced recommendation method for healthcare platforms. The authors use user profile information to select entity neighbors for user modeling. The technical contribution of this paper is quite limited. The knowledge graph embedding method used in this work is also the conventional TransE method. The experimental settings such as hyperparameters are not complete. 3.The compared baselines are too weak and limited. 2018.[2] Guo et al."A survey on knowledge graph based recommender systems." Thus, my recommendation is a rejection.<|endoftext|>The technical novelty of the proposed method is limited. The baseline is weak and the experiment lacks an online metric. The paper only compares with DCN that is not a KG model. In addition, in published recommendation system benchmarks, DCN cannot represent the STOA. It is a better way to compare online. This paper proposes a graph based recommender system for diabetes self care management, which is based on the knowledge graph embeddings techniques.<|endoftext|>Paper introduces a deep neural network based recommender that uses the knowledge graph embeddings, called CareGraph to predict health nudges such as content and notifications in a digital health platform to manage a chronic disease. The research question is whether a knowledge graph can be used to mitigate cold start problems to recommend a set of highly structured health nudge messages. StrengthsThe use of knowledge graphs in a recommender system is a timely, trend topic of research, especially in the healthcare context. However, it is not clear regarding how it is used as a recommendation. (c) lack of validation approaches for the output. Some minor comments.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; Their proposed modification guarantees unbiased updates while converging to the true singular vectors. Regarding strengths, the authors clearly explain the weakness of the existing $\alpha$  EigenGame algorithm and update. It is clear that update bias and data parallelism do not solely determine the empirical convergence rate. I have read the author s response. I remain in favor of acceptance. The authors address the problem of the biased updates of the $\alpha$ EigenGame in a simple yet effective and theoretically sound way.<|endoftext|>This paper extends EigenGame to parallel settings; the authors propose an unbiased stochastic update rule to compute eigenvalues of a matrix in parallel, that outperforms previous approaches. Weaknesses:  The main weakness of the paper is that the results are convergence results, and do not deal with finite samples. the paper improves on previous work by showing how to do an unbiased update for eigengame, that can be run in parallel.<|endoftext|>The proposed method introduces an unbiased update which allows greater parallelism over data. The proposed method adopts unbiased and parallelizable updates in the stochastic setting and is guaranteed by global convergence. I think providing a finite sample convergence can largely increase the theoretical contribution of this paper.<|endoftext|>The authors extend previous work EigenGame from full batch updates to stochastic updates. They propose an unbiased stochastic update that is asymptotically equivalent to the deterministic update, but it allows better parallelism. It has a concise introduction and clear motivations to the problem, the proposed solution, and the final results. Although this paper makes the story of EigenGame more complete, I think it is arguable that the contributions meet the acceptance threshold. I wonder which one is the case in their paper. Therefore, I think the paper is slightly below the bar.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper solves a constrained discrete black box optimization problem that employs a surrogate model in modeling an unknown objective function. Unlike the formulation of standard Bayesian optimization, it constructs a surrogate model using a piecewise linear neural network. Under the assumption that a randomly initialized neural network is able to produce an uncertainty for exploration (against exploitation), the proposed method optimizes a surrogate model directly with mixed integer linear programming. Finally the authors conduct their method on several experimental circumstances and show the validity of their method. ### Reasons to Reject  I do not think that a piecewise neural network models an uncertainty of unknown objective appropriately. In my experience, COMBO can be thought of as the state of the art model now.<|endoftext|>The paper proposes a method using piecewise linear neural networks as the surrogate model (and the acquisition function) in a black box optimization framework. The paper presents an interesting direction for using a declarative framework (i.e.mixed integer linear programming) to encode combinatorial structures in black box optimization. The problems studied in the  experiments have few number of variables.<|endoftext|>The paper develops a technique to optimize an unknown, black box function "f" by leveraging a combination of neural networks with mixed integer linear programming (MILP) methodology. However, the paper lacks some justification concerning the scalability of the approach and the fact that neural networks had quite a limited size. Overall, the paper is well written and suggests an interesting and relevant approach to address black box optimization problems equipped with discrete domains. I believe this is indeed the case, but the paper lacks more concrete evidence of this statement.<|endoftext|>The authors present NN+MILP, a framework for the optimization of an expensive to evaluate blackbox fuction with a discrete combinatorially constrained domain. Therefore, in its current stage, the method should be called NN+ILP instead of NN+MILP, as the name otherwise suggests that mixed integer domains can be used. Weaknesses: The MILP formulation limits the hypothesis class of models to neural networks with linear activation functions, runtime limitations restrict the number of neurons in the network. # StrengthsThis paper tackles an interesting problem of optimizing expensive blackbox functions even in the case of complex combinatorially constrained domains, by using a MILP formulation of a piecewise linear neural network. The surrogate model is also limited to b) a relatively small number of neurons in the network due to runtime limitations of the MILP. This means that the runtime comparison between methods is a crucial factor in the comparison, which leads to some questions.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper proposes a safe Reinforcement Learning approach using Latent Shielding. Therefore two environments for evaluation are not sufficient. The paper contribution is minor. Their experiments are limited in terms of test environments and benchmark methods.<|endoftext|>The paper proposes a safe model based reinforcement learning algorithm that trains a classifier to predict whether or not a state is unsafe, and plans trajectories that avoid unsafe states. I’m also concerned about the novelty of the method: it is essentially Dreamer, but with a safety constraint on the planned trajectories. Update Thank you to the authors for the thoughtful response.<|endoftext|>This paper proposes a framework for safe model based RL through latent shielding. If so, then how does the proposed method compare with Dreamer in the normal DeepMind Control Suite environments? The paper is well written and easy to follow, with detailed algorithms mentioning the flow of the approach.<|endoftext|>The paper introduces shielding technique for RL safety into world model RL agents, like the Dreamer model. The new latent shielding has one major drawback and weakness, which is of course mention and acknowledged in the paper, but still a concern to some degree: The guarantees that other shielding techniques have are lost as the method only works on the latent model.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper analyzes the convergence of momentum based SGD (mSGD) and AdaGrad in the sense of asymptotic convergence rather than time average or subsequence convergence in existing results. The paper appears to use a different template other than ICLR s. I encourage the authors to double check this. Weaknesses:  The clarity of the paper is not good, the proofs in the appendix are highly technical and hard to follow. I would recommend to have an outline to better understand how each lemma play a role in the main convergence analysis. As these assumptions are not standard, it is better to include examples of classes of function that satisfy these assumptions. The convergence results are on last iterate different from existing results.<|endoftext|>This paper focuses on developing asymptotic convergence of mSGD and Adagrad, which is stronger than the existing results on subsequence convergence and iterate average convergence. The assumptions are similar or weaker than previous works. I also have several suggestions for the presentation of the paper. In summary, the issues mentioned above must be resolved. 6*.It is inappropriate to say that "We can observe that the convergence rate of mSGD is quicker than that of SGD in this sense." Lemma 3 is a standard result, see Lemma 1.2.3 of the textbook mentioned above.<|endoftext|>New convergence results for momentum SGD and AdaGradPAPER SUMMARY: This paper presents new proofs to the convergence of momentum SGD and AdaGrad. NOVELTY & SIGNIFICANCE: momentum SGD and AdaGrad are extensively used algorithms in practice. This paper proves that under some mild assumptions, momentum SGD can converges faster than SGD (with additional assumption on the properties of the loss function used). Such result for AdaGrad seems a bit weak. In the theoretical aspect, this paper paves new ways to prove the convergence for such algorithms. The paper provies new convergence results for momentum SGD and AdaGrad, showing that momentum SGD converges faster than SGD and AdaGrad s estimate sequence converges.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper presents an approach using reinforcement learning to parse and generate characters. Experiments are performed both with the omniglot challenge and MNIST datasets. I d at least like to see some kind of justification for that choice.<|endoftext|>The paper proposes a self supervised reinforcement learning approach to train a drawing agent for character generation and parsing. It is interesting to tackle these tasks with a self supervised approach and reinforcement learning that operates on stroke space. Overall, the paper proposes a self supervised character generation and parsing by reinforcement learning.<|endoftext|>In this paper, the authors present a method for character generation using the self supervised technology. In general, I think this submission is OK due to the following points:1. Additionally, the paper states that DRAW cannot provide an interpretable compositional and causal process describing how a character is generated, which is not true.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The proposed approach generates data 2 to 10 times faster than the baselines while achieving reasonably well sample qualities. 2.There is no theoretical analysis of the method. 3.It seems that the major contribution is on the empirical side. However, the empirical results are not very impressive especially when NFE is small. The required parameters and their selections should also be explained more clearly. The paper writing can be improved.<|endoftext|>The paper presents a new SDE solver for the reverse process in score based models. The empirical results are good but not impressive enough. There is theoretical analysis on the stability and bias of the algorithm. The writing of this paper is clear. The algorithm seems to be empirically successful. However, the paper lacks justification for why their algorithm can be better. Some of my concerns are also addressed.<|endoftext|>This paper presents a number of treatments to speed up and improve the generation quality in the process of the reverse diffusion process, diffusion based generative models. However, the current experimental result and the method justification are weak. 1)I understand the paper s nature lies in the empirical side. However, the quality could be damaged while I also note that the quality is not being too much hurt from your result report.<|endoftext|>No theoretical understanding of the proposed numerical SDE solver. It would be better to include an analyze on the convergence order of the proposed approach. 2.Results of predictor corrector for VP SDEs in Table 1 are significantly worse than results reported in the original score sde paper. The first page has a footnote "equal contribution", while all author names should be anonymized. How is this implemented in deep learning frameworks? This paper proposes a simple and effective numerical SDE integrator for score based generative modeling based on SDEs.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; This paper derives theoretical results showing that, in node classification problems, GNN performance is negatively correlated to the distance of the nodes in the training set to the other nodes of the graph. Numerical results are presented to validate these findings. The test set is a construct, hence, the accuracy results obtained on the test set should be seen as a proxy to measure generalization. In practice, we want to have good classification accuracy for all nodes, so picking training nodes according to their distance to a given test set skews the GNN to only perform well on this specific choice of test nodes. This assumption is problem dependent and unrealistic in general. It is very strong to assume that just because two nodes are close on the graph they will have the same embedding. The authors should run experiments on other node classification datasets. This paper misunderstands generalization, as it proposes a node sampling scheme for the training set based on a fixed test set of nodes.<|endoftext|>This paper aims at providing a theoretical understanding of GNNs. The major result Theorem 1 is proved under assumptions 1 3 and can only be applied to GNNs ``distance preservation property of p   1 as given in Definition 1. However, the reviewer found these assumptions too strong to be applied to GNNs. However, these constants are actually related to the graph, and such relationships are intertwined in the proof. Also, these "constants" seem to be intertwined, hence it is not clear whether they actually "exist". Please clarify all these constants and how they are related to the data and each other for the reviewer to better evaluate the generality and usefulness of the theory. 3.The analysis only considers the case where the testing sets are already revealed in the training stage (transductive setting), it would be great if the authors could provide some insight on the inductive setting as well. While the paper considers an important theoretical problem, the reviewer finds the assumptions not justified and the constants claimed to exist not well defined. Hence, the reviewer considers the submission not good enough at this stage.<|endoftext|>This paper draws connection between performance of GNN and training set coverage in the graph. Overall, I think the problem this paper studied is interesting and exciting. **The illustration of the paper is clear, especially Fig.2 and 3. **Missing some important references. I would like to see a more comprehensive version in the future considering more justifications on current assumptions and experiments. However, there are couple of work [1,2] study the generalization from non I.I.D training data. See next point for more details. However, as theorem 1 states, the average distance can not guarantee the vertexes closer to $D$ than $D $. I also think experiments on three small benchmark is not sufficient for the significance of the study.<|endoftext|>This paper considers the problem of training set selection for graph neural network training. It shows that the generalization from the training to the test set in a well trained graph neural network is closely tied to the shortest path distance in the graph, which motivates training strategies that "cover" the graph in this sense. The authors apply these ideas in a subset selection program, and verify their results with numerical experiments. I do have some issues that I hope the authors will address, so I will give this paper a weak accept for now. However, I do not find the justification of "the assumption seems to hold empirically" to be a useful explanation of this assumption. And if $M$ is a GNN with distance preservation, do we mean as a map from the set of nodes with the shortest path distance to the embedding space? Experiments: I found the experiments in this paper quite convincing. For instance, there are many better sampling methods one could imagine that would probably fare better in finding a good set of nodes.
Reject; rating score: 3; rating score: 5; rating score: 6; The paper proposes a new notion of ``distributional generalization". It formalizes this through a conjecture (feature calibration conjecture) which says that the output distribution of an interpolating classifier matches the distributions of the labels on a certain class of sub groups of the data. The paper evaluates this conjecture empirically, and also proves it for certain nearest neighbor models. The feature calibration conjecture seems interesting at first sight, but I m not sure it is really saying that much, which is my main concern for the paper. Though generalization in learning theory is often summarized as just train performance $\approx$ test performance, what it really talks about is concentration of measure. There are strengths to the paper too, the definition and subgroup property is interesting, and the experiments are well carried out (though it seems that the authors don t include any code).<|endoftext|>The paper argues about considering entire distribution of classifier behavior rather than a traditional single metric view (test error). It presents a conjecture that when conditioned on "distinguishable features" the distribution on output is similar to that of distribution of true labels. The paper argues that this result is surprising in contrast to Bayes optimal classifier. The theoretical analysis of conjecture stops at nearest neighbors and leaves the characterization of classifiers and distributions which obey conjecture to future work. The paper can benefit with more analysis and at least a rough proof sketch for distributional generalization for more general distributions and classifiers. The conjecture is mainly supported by few experiments and not much theory. The paper stands as a good initial study but needs more analysis.<|endoftext|>The paper reports an interesting phenomenon in deep over parameterized networks which train to zero train error (known as interpolating networks). Not only does the accuracy on test mimic the accuracy on train, but the accuracy measured on certain subsets of the train set also matches that on the corresponding subsets in test set. The paper then proceeds to perform a number of experiments across a bunch of image datasets to exhibit the generality of this phenomenon. It is well written and the main ideas are clearly presented. 2.The proposed empirical observation is novel to the best of my knowledge and a significant observation. 3.The paper proposes a neatly formalized explanation behind the phenomenon and performs a good set of experiments to back their claim. Based on some efforts to reproduce the results on my end, it is not clear how strongly the proposed observation holds which might limit the significance of the contributions of the paper. What would constitute distributional generalization in the setting of regression? If I consider the setting of regression for a moment, the phenomenon appears to be less surprising: a reasonably smooth regression model which interpolates the train data would necessarily exhibit distributional generalization in that setting. This would further stress test the conjecture. It is hard to draw this conclusion from a few test functions on which the outputs match. The paper presents a possible reason behind this behavior and this is formalized in a clean manner. Overall, the paper is very well written and performs a comprehensive set of experiments to back their claims.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; In this paper, the authors study the decentralized empirical risk minimization problem with reproducing kernel hilbert space. The rkhs setting is new and interesting.<|endoftext|>I believe the contribution of this paper is now much clearer and I ve updated my score accordingly. The authors argue (e.g., in Corollary 1) that the proposed approach is more efficient as it requires less communication to achieve min max optimal generalization performance.<|endoftext|>The crux is the development of two different techniques for decentralized computation through a novel introduction of a generalized inner product kernel. Therefore it is suspicious to state this and immediately afterward state that raw data exchange is not required. One of the technical innovations of this work is the use of the generalized inner product kernel. Also, the main boldface question at the top of page 2 is a sentence fragment.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The authors propose a dynamic pruning algorithm by combining multiple methods like early exit and a top k loss function. The authors combine multiples techniques that already exists in the literature. Although the combination is done in a clever way, all techniques are barely explained, making the reader to go to the original papers to properly reproduce their approach. There is no comparison against other dynamic pruning algorithms like [1] or [2]. Finally, the results obtained are not impressive if we compare them against the papers mentioned above. Frequency domain dynamic pruning for convolutional neural networks. In IJCAI (Vol.2, No.7, p. 8).Although the idea is interesting, the novelty is minimal. Besides that, more work has to be done in the experimental section to prove this new algorithm can achieve state of the art results in dynamic pruning.<|endoftext|>The approach uses an early coarse prediction branch that is used to perform a top k classification. This branch is added to the middle section of the neural network. Else, this branch predicts the top k classes for this input and executes the rest of the part of the NN. The authors evaluate their work on CIFAR 10 and CIFAR 100 benchmarks on VGG11&16 and ResNet20 architecture while comparing against a dense benchmark and a simple static pruning technique. This work was evaluated on CIFAR 10 and 100. The one static pruning technique compared against is L1 pruning using a one shot framework. Without comparison against these alternatives, its really hard to evaluate the effectiveness of this technique. The authors also dont motivate the reason for importance of reduction of longest path latency of inference via discussing and showing this element to be a limitation of various prior work. Further, not all dynamic inference techniques suffer from this issue (eg   See [A]). Questions for the author:1. 2.There is a huge body of work in the domain of dynamic filter pruning. Some of these work eliminate the need of optimizing for worst case latency and provide strong benchmarks to compare against[A] https://arxiv.org/abs/1810.05331[B] https://dl.acm.org/doi/10.1145/3417313.3429380[C] https://arxiv.org/abs/2003.03033The motivations in this paper are weakly established. Further the results in this paper have not been compared against relevant benchmarks or on more established benchmarks.<|endoftext|>This paper proposed a new dynamic filter pruning method that utilizes explainable AI along with early coarse prediction in the intermediate layers of a CNN. The early coarse prediction branch is trained using deep top k loss, and the coarse prediction is used to dynamically select CNN filters relevant for those classes. The dynamically pruned model by the proposed method is trainable and easily deployable on the various end hardware. 1 4) Recently, activation functions with negative values such as Swish and Mish are widely used. 1 5) As you know, fine tuning requires a significant training cost, so many filter pruning techniques that achieve excellent performance without fine tuning have been proposed. It is necessary to show whether good performance can be maintained even if fine tuning is removed from the proposed method. To compensate for the fact that the experimental results of this paper are not sufficient to prove the superiority of the proposed method, the motivation of this study needs to be clearly presented at the beginning for enhancing the novelty of this paper.<|endoftext|>The authors propose a set of steps in order to reduce the latency and the flops computation effort of neural networks. They add early exit prediction layers, a top k prediction layer and dynamic pruning based on the set of classes from the top k prediction layer. The pruning uses statistics precomputed from a validation dataset in order to decide which filters are to be used for which class. strengths:good readability except for the XAI section  (which is to brief)Easy to understand conceptfaster on CIFAR 100weaknesses: incremental novelty, it is a combination of existing ideas no speed gains on CIFAR 10 experimental evidence is done using only two networks and only CIFAR datasets. The paper would strongly benefit from more extensive experiments. "Based on the coarse prediction, the XAI dynamic pruner selects the filter kernels of prunedconvolution layers that need to be used." do they use only k 1? How much do you prune ? This sentence can be misread as:The importances are obtained once in the start and during prediction of the sample.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper reports several empirical findings. This shows that each Transformer layer would change the topological structure of the context embeddings. All of the notations and definitions are just for describing their analysis method in a very abstract way. The paper s first two empirical findings are not very interesting to me and the authors also do not explain the impact of those findings. 2.The analysis method is not novel. 3.The paper presents a few very simple experiment results using a very complicated way, which makes the paper very hard to read.<|endoftext|>This paper proposes a framework, using concepts from category theory, to analyze the mechanisms through which Transformer based language models perform contextualize representations. Lastly, the authors perform a range of experiments to better understand various aspects in transformer models including how contexts are represented for [CLS] and the change in context representations by layer. Strengths:  The proposed approach is interesting and suggests an alternative viewpoint to understanding contextualization in language models.<|endoftext|>This paper intends to understand “how does a language representation model represent contexts” which is different from a more popular direction of understanding “how does the context affect the representation of the tokens”. The paper formulates the contextualized language representation problem using category theory. In terms of the story:   The main takeaway of the paper remains unclear to me for most of the paper as the authors were not precise with communicating their intentions. In the abstract, the paper claims that the work would shed light on the improvements in Transformer based language representation models. I provide detailed explanation of my concerns in the main review.<|endoftext|>This paper aims for providing a theory for understanding the context representation in pretrained language models. In this case, I strongly encourage the authors to discuss the definitions of symbols in their equations. However, this paper is severely flawed in multiple aspects: the basic notations and understanding of LMs seem to be wrong; the definitions of concepts are unclear and ambiguous; many symbols are used without definition or explanation; equations are not explained; there is barely any connections between the category theory and the language model; important related work is missed, and the experiments are quite simple and insufficient.<|endoftext|>This paper aims to explore how contexts are represented in a deep language model. Pros:1.The attempt to explain and investigate how contexts are represented in the language model is interesting. 2.The introduction of the category theory to model the connection between contexts and their representations seems novel. My guess is that the "context" of a word is all the remaining words in the sequence, but this is not clearly mentioned in the paper.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper provides an explicit bound relationship an information theoretic analysis, which uncover hidden label marginal biasesThe paper proposes LABEL MARGINAL BIASES which combines CE loss with KL divergence or L1 norm to address class imbalance but without losing generality. Strength: Good motivation, Good organization, Readable Weakness: The paper analyzes Dice and CE losses while there are many recent losses ignored. The proposed loss is tested on Retinal Lesions dataset which groundtruth RoI is labeled as an oval which is not accurate for the segmentation task. Compare to other standard loss, i.e.Dice and CE, the proposed loss does not provide good results and improve with a tiny gap.<|endoftext|>On the hand hand, the cross entropy loss encourages proportions of the predicted segmentation to match the ground truth proportions. While both phenomena are straightforward intuitively, a theoretical perspective from label marginal bias is appreciated. 3.Experiments on both natural and medical image segmentation problems with imbalanced classes were conducted to show that the proposed loss achieves better accuracy than Dice or CE loss themselves. In this paper, only ResNet FPN and ResNet Unet are used in the experiment. 4.I agree with other reviewers on that the "segmentation GT" of the retinal lesion dataset is so poor that it cannot even be called segmentation annotation, but rather just detection annotation. Thus, higher DSC on this dataset is likely due to segmentation shape being smoother/more elliptical. But either way, I think including a detailed comparison of Dice related losses on test set is important, as the whole paper is essentially studying Dice loss and CE loss. Thus, although it is an alternative perspective from label marginal bias, it did not lead to significant new knowledge about both losses. The lack of Dice related losses results on Cityscape dataset test set needs to be addressed.<|endoftext|>The paper presents an analysis of the cross entropy (CE) and dice loss (DSC) for segmentation tasks,  in terms of the label marginal bias. According to the analysis, DSC prefers small regions, while CE encourages a prediction that has a similar proportion to the ground truth. Strengths  The paper presents an interesting analysis for the CE and Dice loss function, indicating their relationships in terms of the called label marginal bias. Base on this, the paper proposes an alternative variation of the CE loss function for segmentation. It might be interesting to discuss results on additional datasets, compared with the DSC (as the paper discussed, it is widely used in the medical domain).<|endoftext|>This work presents a decomposition of two commonly used segmentation losses (cross entropy/CE and Dice losses) into two competing components. Minor comments:   An important point in the paper is the competing nature of the ground truth penalty and label bias term. This work argues that the label marginal penalties in Dice loss favour extreme  class imbalances, and hence their widespread use in medical imaging segmentation tasks which encounter class imbalance more commonly. The label marginal terms in CE is argued to be better as this term matches the label marginals to the ground truth distributions. However, the regularization schemes proposed are not well justified. Experiments on two segmentation tasks show interesting influence of these regularizations. + Experiments with the L1 regularization for the class imbalance dataset (retinal segmentation) show promising results. This is not obvious from the Propositions 2. Perhaps it only serves as a scaling of the label bias term. In that case, how does the L1 regularization influence the scaling of the label bias term. L1 regularization is posited to be better than KLD regularization, and one of the reasons cited is its "gradient properties and stability". Is there a theoretical justification for using the L1 term?
Reject; rating score: 1; rating score: 5; rating score: 6; rating score: 6; There are also some unclear details and missing experiments mentioned in the above section. Experiments are conducted on a mug hanging task. Specifically, there seems to have 3 blocks in Figure 1, but in Section 3.1, the authors stated that the proposed framework consists of two parts. It is unclear how well the method would perform if given data of other categories. The authors did not mention nor providing references in Section 4.1. There are some details missing and some technical errors in the paper.<|endoftext|>The method proposes an implicit field function based representation, which can be directly inferred from the camera images and be used for robot manipulation. The authors propose a new method for capturing object representation directly from images and used for object manipulation. 2.The authors present some experiments (with videos) to illustrate their method. It doesn t capture the relationship between the objects (e.g., whether object A is on top of object B, whether object A will collide with object B). 4.It s not clear what will happen when the query points are out of images. 7.The definition of the "feasible rate" (in table 1) is not clear.<|endoftext|>This paper build on recent progress in implicit object representations and where the representations are trained(or fine tuned) along with a feature head on down stream manipulation tasks. + The related work is well covered and the motivation is setup clearly i.e.we need vision backbones that work for manipulation tasks. Seems like a missed opportunity. Some more algorithmic details would be helpful. How to choose what set of points are used to condition PIFO and what if this choice changes between tasks?<|endoftext|>This is then integrated with a planner based on Logic Geometric Planning (LGP). Strengths:  Neural implicit functions are still quite new, and investigating their use for robotic manipulation is a valuable direction. It is well known that dense features will give better results than a single global vector (or a single vector representation of an object). For example, PIFO predicts graspness scores and hang ness scores, however this is not mentioned in the introduction. The introduction is rather vague. This seems to be a major contribution of the paper, yet it lacks detail.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The results indeed show improved performance on average. However, the results are not statistically significant with the large standard deviation in the metrics. The paper introduces a method for training symmetric locomotion controllers for bipedal characters by leveraging a fatigue model. The idea of using bio inspired model to model fatigues in the motors make a lot of sense and the proposed method seems reasonable.<|endoftext|>This paper presents a Reinforcement Learning (RL) model that uses cumulative effort to improve full body movement generation. The main contribution of this work is the proposed fatigue reward.<|endoftext|>The paper proposes to use a normalized cumulative fatigue (NCF) based reward to learn symmetric locomotion with Deep RL. **Weakness:**  The novelty of the proposed method is limited. Finally, I feel the paper may be more suitable for graphics, animation, or biomechanics conferences than ICLR as its main focus and contribution is on bio inspired reward design rather than learning. Without experiments, it is difficult to tell which design is better.<|endoftext|>This paper proposes to replace the commonly used torque square penalty for RL locomotion task with a fatigue based reward that is more biologically plausible. For example, what are their performance in terms of torque square, the proposed cumulative fatigue, and other task rewards? 3.I am not sure why Spectral Entropy indicates the robustness of the model.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; While this definition is appealing in the rooms domain I am not convinced about its generality. the authors propose an algorithm by which the latent hierarchy is recoverable, but it seems that in order to implement it (phase 2) requires that the agent explore in a reward free way in order to determine the full transition dynamics. And so, in summary, it seems that the key result is established only for a new definition of what constitutes a hierarchy, where the meta RL tasks satisfy a bespoke definition of coverage, and is then applicable only to a small percentage of tasks (even in the single motivating domain). An interesting paper proving regret bounds in a meta learning HRL setting. However, in order to get any traction on the problem, the authors seem to need to make a large number of unprincipled assumptions (and introduce many novel definitions) such that the general interest / applicability of results is in questions.<|endoftext|>The authors develop a theoretical framework to discover the latent hierarchical structures shared across meta training RL tasks, and propose a tractable hierarchy learning algorithm with provable guarantees. The paper seems to be theoretically solid, and is well organized. I believe they can provide researchers with a novel perspective for studying hierarchical RL. However, the current version does not convince me to recommend acceptance due to the following concerns:Q1. Additionally, in many meta learning settings, meta training tasks do not necessarily share a common state space. Q3.The authors make many assumptions, but never explain whether they are reasonable for realistic RL problems. Although the paper seems to be theoretically solid and presents many interesting notions, the current version does not convince me to recommend acceptance.<|endoftext|>This paper proposes a novel formulation  to analyze the provable benefits of hierarchical RL algorithms. The formulation of hierarchical structures (Definition 4.1) is quite interesting and covers many real situations in HRL. From the discussion on the related work, I understand that this formulation is novel enough, and the understanding of HRL is still limited. However, I feel disappointed that there are so many assumptions in both the main text and the appendix. Since most of these assumptions are proposed for the first time and not commonly used in the previous literature, I believe the authors should discuss more on the necessity of each assumptions. In section 5, the authors propose a novel algorithm to learn the transition dynamics and detect exits. Overall, I believe that the formulation and algorithms are quite novel, and the hierarchy formulation covers many real situations in HRL.<|endoftext|>This paper presents a theoretical analysis of hierarchical reinforcement learning in a meta RL setting. Subsequently, for exit coverage, optimistic imagination is introduced. Although I m not the expert in the field of the theoretical analysis of HRL, the regret  guarantee for the meta test setting seems novel. Similarly, it is interesting to see that it is provable that the hierarchical oracle is implementable using the proposed algorithm. The methods for discovering the bottleneck states have been investigated for decades, and there are many ways to define the bottleneck states. To further clarify the contribution of the work, I would like to encourage the authors to deepen the discussion on the relation to the previous studies.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper proposes an $\epsilon$ LDP mechanism, termed $f$ RR, to privatize a real number. The mechanism first maps $v$ into its binary representation (with a sign bit) and then flips each bit with probability $p_i$. The authors then extend their mechanism to privatize a $r$ dimensional vector by sequentially performing the (scalar version) algorithm for each of the $r$ coordinates. Finally, several experimental results compared with other previous methods are provided. 3.The authors claim that the proposed solution resolves the curse of privacy composition; however, from the main utility guarantee Theorem 4, I don t think the problem is solved. In general, I don t think it is possible to improve the state of the art LDP mechanisms (such as [BDFKR 2019]) as performance guarantees of these mechanisms (order wisely) match the information theoretic lower bounds. For instance, upon a quick look at the codes, the "Duchi Mechanism" (which, I believe, should be the privUnit mechanism in [BDFKR 2019]) is indeed implemented as [DJW 2013].<|endoftext|>On significance of the least significant bits for differential privacy. I mean, if the gist of the paper is that bit aware LDP randomiser, why not to also have utility comparisons for that (more) directly ? Other: The figures are quite small and thus a bit hard to read. Some of my concerns regarding the presentation (e.g.why is only the privatisation of the embedded representation studied) seem to be shared also by other reviewers. An interesting LDP randomiser that takes into account the significance of bits in binary encoding of floating point numbers (and vectors). However, I think the presentation should be improved to meet the bar of ICLR. Currently, I think this paper will not be easily understandable to anyone else than those working close to this i.e.practical algorithms for LDP and federated learning.<|endoftext|>There are a lot of clarifying question that I do not understand in this paper. The idea of this paper is to introduce a bit aware randomization technique. I still feel the paper is cluttered and not ready for ICLR and at maximum is a borderline accept mainly due to the strength of the problem studied. That is a randomized response algorithm that flips a bit based on its position in the binary representation of the scalar (or naturally extend to a vector). I have some clarifying questions from the authors:1. It needs to be defined more explicitly. This pretty much means that the privacy bound is in the average case, which is weird!!<|endoftext|>The paper proposes a randomized response algorithm that takes into the bit indices and prioritises higher order bits. As a result, the utility is higher than other competitive algorithms. Additionally, the analysis allows the bit randomization probabilities not to be affected too much by the dimension of the data.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors also prove that the expected error of ANN SNN conversion can be reduced to 0 by using this method. The reported results on CIFAR 10/100 and ImageNet dataset show that this work archives state of the art accuracy with fewer time steps. The authors propose to add a shift term to overcome this problem, which is very interesting and impressive. Weaknesses & suggestions for improvement:(1) My main concern is about unevenness error. Is there such a case, the input is negative during a period as the weight is negative? (2) The accuracy of source ANNs on ImageNet is lower than some compared methods, which decreases the performance of converted SNNs. This paper proposes a novel ANN SNN conversion framework and achieves SOTA accuracy with fewer time steps.<|endoftext|>This research proposes an ANN SNN conversion method based on quantization clip floor shift activation. 1.There are other related works that need to be compared with this method. b.	S. Singh, A. Sarma, S. Lu, A. Sengupta, V. Narayanan and C. R. Das, "Gesture SNN: Co optimizing accuracy, latency and energy of SNNs for neuromorphic vision sensors," 2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 2021, pp. It seems that the legend is missing to explain what is represented by the lines. The achieved experimental results are notable. However, some key points need to be addressed (see the main review section).<|endoftext|>The paper first analyzed the various sources of ANN SNN conversion errors namely clipping error, quantization error and unevenness error and then proposed quantization clip floor shift activation function to replace the ReLU activation function in source ANNs to train them. Please show results for T  1 as that would make the limitations and possibilities much clearer. 4.As the authors claimed the activation difference error is zero if the ANN is trained with the proposed activation non linearity, then is it possible to convert with  T   1? Following are my detailed reviews. The paper is well written and the general motivation of work on efficient ANN SNN conversion is important as it can reduce the SNN training epoch over head. [1]  Spike thrift: Towards energy efficient deep spiking neural networks by limiting spiking activity via attention guided compression, WACV 2021. 2.The organization of the paper and results are good. 2.An important way to reduce spiking activity is through weight sparsity. It would be interesting to see whether such "only conversion" based strategy go hand in hand with state of the art model compression of SNNs [1, 2]. In general we set the max.<|endoftext|>This paper proposes a Quantization neural network to spiking neural network (QNN2SNN) conversion method. The authors first analyze the conversion error between ANN and SNN. Then they construct the ann with quantized activation so that the error can be eliminated. + The empirical results show good improvements. The analysis of conversion error has been proposed in Li et al.2021.The only difference is the unevenness error where Li et al.2021 assume it is 0. Lack of ImageNet conversion experiments. The authors are strongly encouraged to convert more challenging models on the ImageNet dataset. 2.What is energy consumption?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; Why should we only consider adversaries that adhere to these assumptions? They run experiments based on an existing backdoor attack and they show that their technique could be successful. The goal of the paper is to provide a defense against backdoor attacks. However, the assumptions in the paper are very strong which makes the threat model of the paper narrow.<|endoftext|>It appears to me that the proposed method is a robust training method, not a detection or backdoor removal method. 4.Theorem 4.5 (Sets minimizing expansion error are homogeneous) is based on the strong assumption of incompatibility. 9.The biggest concern about the experiment, except for missing many different types of attacks and datasets, is the cherry picking results in the main text, Table 1. The idea of data partitioning, assessing then reassembling is generally new for adversarial defense.<|endoftext|>This paper proposes a new defense to backdoor attacks. This happens in two phases: an ensemble of weak learners identifies distinct homogeneous sub populations in the training se, and a boosting framework aims to exclude poisoned data and recover clean data. The empirical evaluation is conducted only on CIFAR 10. In the empirical evaluation, you only consider other proposed defenses, but do not consider a backdoor attack variant which knows you are looking for these homogeneous sets and then using the ensemble of weak learners.<|endoftext|>This paper tackles backdoor attacks where an adversary performs targeted attack against neural networks by injecting some poisoned data into the training data without sacrificing prediction accuracy on clean data. The defense works by recovering the clean data from poisoned training data. It is very clear and properly motivates the idea of homogeneous set. 2.A more challenging attack case (1 pixel trigger for CIFAR 10) is used in the experiments to validate the effectiveness of proposed defense. The use of a set of weak learners and proposed boosting algorithm are technically sound. The experimental part demonstrates the performance of proposed defense over other existing work.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The longformer is not state of the art   it has yet to be established. This paper attempts to bridge the NLP community and the comp bio community by framing numerous comp bio tasks with analogies to NLP. They are not ground truth. I appreciate how the authors try to make a connection with the ML tasks in biology to NLP. While bridging both communities (NLP and comp bio) is important, this work seems too premature. There is no intuition for the kinds of features that are important for each task.<|endoftext|>Pros:  I applaud the authors for wanting to reach out to the broader ML community with problems in biology and bioinformatics. The selection of tasks could be further refined. The annotations in the paper are DNAse1 hypersensitivity, TF binding, and histone methylation. The accuracy of your algorithm is determined by the performance of GreenGenes chimera calling. While the authors have curated a number of non protein biological datasets, I do not think the objectives of the tasks are refined enough, nor are the methods compelling enough, to justify publication.<|endoftext|>This contribution introduces a benchmark for the evaluation of neuralnetworks for supervised classification of DNA and RNA molecules. In other words, it is not clear to me that one should aimfor a method that predicts well on all these tasks. It could be useful to say a bit more about the longformer model that  is used as a baseline, and about contextual embeddings, and why  either are expected to help on the tasks chosen in the benchmark.<|endoftext|>The authors identify a need for standard ML benchmarks in genomics in order to bridge the gap between the bioinformatics and deep learning communities and to make it easier for ML practitioners to contribute to genomics as they have in computer vision and natural language processing. Machine learning is not necessarily "computationally inexpensive," as the authors claim in the introduction. It appears that they are, but the paper would be much stronger if we had an idea of why they are.<|endoftext|>3) The authors provide reasonable baselines. I thank the authors for tackling this challenging and often overlooked problem of benchmarking, but I would like to see more care taken in the construction of the benchmark before this paper is ready to be accepted. Furthermore, the authors state they draw inspiration from GLUE / SuperGLUE which all deal with a single language (English). This does not feel appropriate for the tasks provided in this paper as there is no single language.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper shows that for the cross entropy loss, when updating both the weight vector and the features by gradient flow, the final solution exhibits the neural collapse behavior, where the neural collapse refers to an observation pointed out by (Papyan 2020), which is that (NC1) the within class features collapse to their class mean, (NC2) the average of the features of each class converges to having the equal length and all pairs of the class means have equal size angles, (NC3) the weight and the class mean are aligned, (NC4) the weight converges to the nearest class center of the features. The optimization problems (or the simplified model) in  the previous works that theoretically show the neural collapse require some constraints or regularization, which are seldom used in practice. Overall, this is a good paper and the result is interesting. The paper first shows a connection between the limiting point of gradient flow on the cross entropy loss and the KKT solution of a minimum norm separation problem. It also shows that every global optimal point of the minimum norm separation problem satisfies the neural collapse condition. Typo: On the second line of the proof of Theorem 3.1, there is a typo regarding the definition of $\tilde{w}_i$: $\frac{1}{n} \rightarrow \frac{1}{ K}$.<|endoftext|>This paper analyzes the phenomenon of neural collapse from the simplified perspective of an unconstrained features models, whereby the only two optimization variables in the model are the last layer classifier and features, which are fit to some labels by minimizing the cross entropy loss. This is an interesting contribution. In addition, the paper is well written and clearly organized. I agree with the authors that this is still relevant and interesting, as it precisely captures the behavior observed in the final stages of training. Could the authors explain why they refer to this analysis as global? I believe this is a valuable contribution, but have some reservations about the presentation of the results.<|endoftext|>In particular, to understand the last layer features and classifiers, the authors study the gradient flow of the unconstrained layer peeled model (ULPM) without any regularizers and show that gradient flow converges to neural collapse classifiers and features. Existing work on the analysis of neural collapse is based on either constraints or regularizers on the features and classifiers. This paper studies the training problem without any regularizers, but it is unclear why this formulation is closer to the practice than the one with regularizers. If the results show the (Riemannian) Hessian has a negative eigenvalue, then I think this is a much stronger result and should be described in Theorem 3.4. 2.As mentioned in the paper, the training dynamics will diverge to infinity where the cross entropy loss archives its minimum.<|endoftext|>The paper studies the recently discovered Neural Collapse (NC) phenomenon for deep neural network training using Cross Entropy (CE) loss. It theoretically analyzes the problem with a so called unconstrained layer peeled model (ULPM), and then it shows that the ULPM with CE loss has a benign landscape. This section really needs to be improved. The "Notations" section in the bottom of page 4 can be moved earlier; otherwise many notations are used widely before without introduction, e.g.[K]Typos:  In the second line after Eq.(1.1), the "y_i" is unbolded while I think it should be bolded?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This authors present a novel method for learning representations for time varying graphs which allows for incorporating information at different time scales using their streaming snapshot model. It is not clear to me how eleven (and not twelve) temporal subgraphs are obtained. The current paper addresses an important problem using a smart approach but requires significant rewriting as well as better empirical evaluation. I do not believe the paper is ready for publication in the present form.<|endoftext|>The key idea of obtaining a higher classification accuracy is to use a bi level meta learning paradigm. In general, it is an interesting paper. The strengths are:1. However, the overall quality is not strong enough for acceptance. The problem considered in this paper is interesting and new. 2.The authors proposed a new method, which includes two parts. al., 2021 have demonstrated the power of time related attention layers. The contribution of the streaming snapshot model is not critically novel. The authors use both two models but may have different time granularity. 2.The technical contributions are limited. The time attention layers are commonly used in temporal graphs which are also proposed in previous work.<|endoftext|>This paper introduces a methodology of graph learning for dynamic graphs, where the dynamics are encoded in the representation to obtain improved results on graph classification tasks. This framework includes a temporal graph encoder that uses attention mechanisms to generate representations, as well as a meta learning component that ensures easy knowledge transfer. The additional meta learning module is a nice addition, and it is useful to see how to interface this module with learning dynamic graph representations. In Section 4.1.1, where is the attention in the "intra snapshot time attention" module? Perhaps this can be clarified in the author response. It seems that the meta learning portion in the current work is new, but is that the only source of novelty?<|endoftext|>The paper proposed a method for metric learning for few short examples, where each example is a temporal graph modeled with two timescales. The method used in the paper is a combination of different standard methods available. Do you mean that a snapshot is also a temporal graph with edge appear and disappear at different times (a)? The problem is not clearly defined and hard to read.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper proposes locality based mini batching (LBMB), a method for extracting batches for GNN training. This method can be applied to both training and inference, which is a strength of this work. Evaluation demonstrates that LBMB spends much less epoch time for training than existing methods, while maintaining comparable accuracy. Inference also shows significant speedups than existing methods (especially than those using the full graph). I am asking because a runtime sampling method can often be fully pipelined with GNN computation if a sufficient number of sub processes are used [R1][R2]. The authors argue that the preprocessing is one time cost, but this overhead can be compounded by hyperparameter tuning for LBMB such as the number auxiliary nodes per primiary node, batch size, and so on. However, I still have several major issues/questions about this work, and would ask the authors to address them in their rebuttal.<|endoftext|>This paper studies the training of graph neural networks and proposes a mini batching scheme that circumvents the expensive cost of traditional sampling approaches due to random memory access. The authors construct deterministic mini batches so that they can be laid out in consecutive memory, which speeds up training. This paper studies an important question. Random memory access is not necessarily the root cause that prevents faster training, but indeed the extra cost to reorganize batch data into consecutive memory is an anchor of improvement. The authors precompute the batches and shift the burden from training to preprocessing; this is an interesting idea to explore. Another concern is the memory cost incurred to store the mini batches. However, the current empirical results are not sufficiently strong. More in depth study, from either the theoretical side or the empirical side, is needed to solidify the proposed work. Minor issues/questions:At the beginning of section 3, it is mentioned that the first step of LBMB obtains the $k$ most important auxiliary nodes for each primary node. Could the authors clarify? Table 1 can benefit from also reporting the number of training epochs to reach the test accuracy listed thereof. There remain concerns regarding too many options (none preferred) in the proposed method and the unclear impact on training convergence.<|endoftext|>Although the experiments appear to have been conducted thoroughly, results are reported for only one set of hyperparameters. The authors propose two procedures for selecting the primary nodes in a minibatch. The proposed method appears to introduce more hyperparameters than some of the other approaches. There are also strategies to prevent the optimizer from processing consecutive batches that are very similar. The paper states in Section 3.3 that “a machine learning model is trained only once”, but later makes the argument that the pre processing overhead involved in constructing the partitions is a one time overhead compared to the training cost. It then becomes questionable how valuable it is to reduce the training time for a dataset, since it would form only a small portion of the overall training overhead. The paper proposes a novel approach to deriving minibatches for training graph neural networks. The envisioned learning pipeline could be better articulated to make the trade offs clearer in terms of time required for pre processing and training time (number of epochs, time per epoch). Related to this are concerns about how hyperparameter tuning factors into an assessment of the practical training time. The batch size has a major impact on the performance of the sampling algorithms (both in terms of accuracy and convergence time), so it is important to explore a range of choices. The impact of the paper may be somewhat limited because of the heuristic nature of the proposed methodology.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; At the current state, I feel like GAL is very straightforward self training with the unlabeled data generation with large LM generation. To overcome this challenge, the paper proposes to "generate" in domain data using large self supervised LMs. The rest of the process, which are annotation and learning, follows typical self training and this paper takes the approach of learning with soft target as (Du et al.) The paper examines the proposed approach (GAL) on GLUE benchmark on KD (table1), SSL on full data (tab2,3), and prompt based few shot (tab4), and also examines GAL in tabular tasks (tab 7). This paper identifies challenges in self training as a lack of in domain data (input x). On KD, GAL usually shows the largest improvement on 1st iteration and shows minor fluctuations of performance during more iterations (Tab2, 7). Has good survey of related work in self training. Extensive experiments on KD, self training, and few shot + tabular task. As *generation from fine tuned large LM* is the main contribution of this paper, I suggest the authors to compare with other possible ways of collecting this in domain data (such as kNN or generation without fine tuning) to contrast and highlight their scientific contribution. How big is one iteration of GAL? I want to first thank the authors for writing a clear paper and conducting extensive experiments. The experimental results show performance gains, however, I am not convinced that this approach is novel as it has been shown in previous work that self training can improve performance and as we know that GPT 2, 3 can generate quality examples.<|endoftext|>More specifically, the authors handle the challenge of lacking task specific unlabeled data and use unconditional language models to synthesize in domain unlabeled data, showing the effectiveness of semi supervised learning, knowledge distillation on NLP, and tabular tasks. The authors study how to handle the challenge of lacking task specific unlabeled data in a semi supervised learning (SSL). The problem is important and well motivated. Strengths:*   The paper introduces a clear flow and comprehensive studies in the experimental section. However, this claim is still validated based on comparison with other state of the art data augmentation methods. * The proposed solution is to leverage data augmentation to generate task specific unlabeled data. * Even though authors review many related works, they did not include other data augmentation works as baselines. Since this area is widely explored, it is important to understand how much benefits can be brought by this method contrasting other data augmentation options. If KD is based on trained labeled data, this seems not a fair comparison. This can only show that using additional LM for data augmentation is useful and such a claim is provided in many existing papers.<|endoftext|>The authors propose a new framework of data augmentation based on generative models. The idea is to generate some new samples and then to classify them in an unsupervised manner to improve a student model. The authors give a very clear formalization & discussion to bridge between data generation and semi supervised learning. The experimental section is very strong, investingating several interseting ablation. The last section (5.5) seems very promising for the future, in particular for data2text applications. ++ Clear & well written++ relevant combination of different language models++ strong & relevant experimental part++ great last experiment on UCI tabular dataset  (probable) high computation cost  anonymization issueThe fact that GAL appears in the GLUE learderbord associated with the name XXX is a major issue if XXX is indeed the author of this article. Few detailed comments:Section 3 about distillation is very clear. The fact that GAL appears in the GLUE learderbord associated with the name XXX [to be discussed privately] is a major issue if XXX is indeed the author of this article. The authors do not discuss the computation cost of combining GPT2 & RoBERTa large modeling. The proposed framework is simple, well described and very powerful. Experiments are numerous and relevant. I propose to accept this paper.<|endoftext|>The paper provides a novel framework for advancing self supervised learning, Knowledge distillation and few shot learning for NLP and Tabular data. The framework focuses on self training and knowledge distillation by generating synthetic data based on an unconditional generative model. They also conduct and extensive literature review for each of the components used in their framework. First, I want to thank the authors for the effort in writing this paper. The model seems solid and justification for decisions made are clear. Improvements are not large but, given they are being compared to very strong baselines, it is fair to say they are relatively solid. 3   For table 1 and 2, some standard deviation for the multiple runs or significance testing might be useful to compare results that are similar to each other. I do think that is an important component for the paper that was not covered as well as the rest. 5   In table 5, you might want to add a baseline comparison without GAL so it is easier to compare the improvements. 6   For tabular tasks, I think that section was more confusing than helpful. There is in depth discussions about limitations and benefits, and about the decisions made for the framework. My opinion is that this is a clear accept.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper proposes a 2 stage sampling method that can find its use in either feature selection or instance (training examples) subsampling. At a high level, the authors frame the problem as a subset selection problem with the goal to minimize the performance difference for a downstream task between using the original set and the downsampled set. The main contribution of this paper is to propose a 2 stage sampling method. The main weakness of this paper is that the core details of the proposed method are missing in the main text, which makes it very difficult for readers to assess its true value. Action learning selects examples to be labeled, but which examples to select is based on the error/loss of the existing model, which in turn depends on the label information. If it s the former, the previous description in Sec.1 indicates that the authors would like to minimize the "performance degradation", which is the difference in losses from the original dataset $D$ and the subset $D_S$. If it s the latter, then why there re expectations here? It s not clear at all. What does a sparse Bernoulli prior look like? Could you give an example? 4) p. 4, Sec.3.4: For the neural networks $\varphi$ and $f$, how are they trained? There s no such guarantee by simply sampling from a VAE. Therefore, this paper in its current form, doesn t seem to meet the bar of ICLR, and requires major improvements.<|endoftext|>This paper proposes a data summarization technique that can be used for arbitrary downstream classification tasks. The authors show that their methods can be used to reduce the number of features required by a model while preserving accuracy. The algorithm proceeds by sampling features according to some softmax distribution defined by the classifier on each feature. The algorithm then learns to sample such that the accuracy of the classifier does not fall significantly when trained on a small subset of features. 1.The method offers performance improvements, although I m skeptical about the empirical significance of the experiments. The experiments are only on celebA faces and MNIST digits, both unchallenging datasets. 1.I do not think the numerical comparisons in Fig 4 are fair. The authors technically get access to the entire dataset and then choose what to show to the classifier, based on evaluations on different subsets. The performance gains over competing methods tell me that the baselines are most likely unfairly weaker. 1.I have concerns about the technical novelty of this work. It essentially boils down to sampling from a softmax distribution over features, then putting a sparsity prior on the distribution to make sure few features and data points are preserved. Additionally, I do not see the point of the experiments where a subset of the data is selected to train the classifier without sacrificing the accuracy   each training technically requires the full dataset, even though the classifier gets to see only a subset, and training a new classifier would require the entire dataset to select a different sample.<|endoftext|>More precisely, the paper proposes to regularize the task loss by a Kullback Leibler divergence between the selected samples/features and a sparse Bernoulli prior, and employs a neural network to learn the weights used for sample/feature selection. The major weakness of the paper is that it lacks theoretical justification for the proposed method. For example, it is unclear if the learned decision function (e.g., regression function and classification boundary) is consistent or not. This paper present an interesting method and promising results for subsample selection used in deep learning, while lacking theoretical justification for the proposed method.<|endoftext|>The paper proposes a Stochastic SubSampling method (SSS) that is helpful in reducing the size of input dataset while preserving reasonable performance on a target task. SSS is two stage and set based, which can be jointly optimized with arbitrary downstream tasks. Extensive experiments verify that SSS do perform well in the sense of efficiency and generalization a variety of tasks and datasets. Strength:  The paper writing is good, and the presentation is very clear. Weaknesses:   In Figure 4 (c): for the task of image reconstruction on CelebA, I observe that LTS performs even worse than random subsampling, which is counter intuitive and is inconsistent with other tests, e.g., Fig 4 (b). Discussions need to be added here. In Section 3.5, I would suggest re order the paragraphs so that they match the order of the illustrations in Figure 3 to increase the readability. Now it starts with Fig 3 (b), then (a), (c), (d). Why not (a), (b), (c) and (d)? Similar things could be improved in Section 4 when discussing each tasks and the experiments. Typo:  In the third to last line of the paragraph after Eq.(2), there is a right parenthesis missed for "Ber(\rho(\bar d_i)"The paper writing is clear, and the proposed method is clearly advantageous over other baselines in terms of generalization and efficiency. I would vote for an acceptance of the paper.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This paper proposes to investigate meta learning to generalize compositionally. This is an important challenge. Literature As the authors note, compositional generalization has been a major topic of research for some time. Or are the stimuli represented differently as inputs to the agents? Even this detail of the experimental evaluation is not clearly stated. Some of the ideas are intriguing.<|endoftext|>In this paper, the authors propose a new benchmark to investigate state of the art artificial agents  abilities to exhibit compositional learning behaviors. The authors propose a Symbolic Continuous Stimulus (SCS) representation and cast the problem of learning compositional behaviors as a meta reinforcement learning problem. The experiments are not sufficient to support the claimed contributions. I think the idea of compositional learning is interesting. The experiments are not sufficient as well.<|endoftext|>This paper studies the problem of learning compositional behaviors through referential games. The authors first introduce the problem of fixed shape representations that are not suited for continual learning with infinitely many tasks. They then define the compositional learning problem using meta reinforcement learning where a set of new tasks are introduced over time and two agents communicate to develop a compositional language. I think the posed problem, using meta RL for continual referential games with different semantic structures, is an interesting problem. Upon first reading the abstract, I thought one of the main contributions of the work is a new benchmark for compositional learning. This is strange to me as using only ~1K updates gives the same performance as using 8K updates.<|endoftext|>The paper presents a new benchmark based on a referential communication game (where a speaker has to communicate the stimuli they observe to the listener and the listener has to find the right stimuli among distractors) to test for compositional learning behaviors. **SCS**The authors propose a new method to represent symbolic stimuli as continuous representations as an alternative to one hot encodings. Similarly, I could not find the value of the dimension of the symbolic space $N_{dim}$ that was used to generate the stimuli. Eg: The following line is exactly the same in the Introduction and Related Works section. Thus, in its current form, I recommend that the paper be rejected.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This manuscript has several issues to be accepted as raised in the weaknesses. The other is a permutation equivariant reduction over the geometric products using an attention mechanism. The authors demonstrate applications in physical science such as crystal structure identification, molecular force regression, and backmapping of coarse graining operators. Strengths:+ It is interesting that the idea of utilizing the theory of geometric algebra to achieve equivariance. The authors should explain it in the main manuscript since this is one of the main contributions. The authors are required to exlicitly describe how it is implemented. Why is the proposed architecture limited to "small point clouds"?<|endoftext|>The paper proposes a method for learning functions that take small point clouds as input, such that they are both permutation and rotation equivariant. Permutation equivariance is guaranteed by the standard attention framework. The setting of the paper is clearly defined as learning functions on small point clouds, where there is an emphasis on higher order interactions and respecting symmetries. Overall, the paper proposes a convenient and principled method to an interesting problem. The empirical evaluation is generally convincing, but could be somewhat more thorough in its analysis, especially given that the paper is a full page under the page limit.<|endoftext|>The paper proposes a geometric algebra attention network for small point clouds. Rotation equivariance based on geometric algebra is novel and impressive. Mathematical proof of rotational equivariance is missing. Is it possible to extend work into relatively larger point clouds? Is that determined by the size of point clouds? If so, what’s the threshold then? The experimental results are also impressive. However, from my viewpoint, the paper still has several weaknesses in terms of clarity and experimental validation.<|endoftext|>This paper proposes a rotation and permutation equivariant geometric deep learning model for problems where the data is represented as small points clouds. The equivariance properties are achieved by leveraging geometric algebra formulations. These models also offer additional features like the analysis of the attention maps produced. # weaknesses:   In table 1, the proposed model performs worse than the baseline GemNet Q and some possible reasons for that are enumerated by the authors like the use of quadruplets atoms, the incorporation of energy, or even better a better architecture. But, I think this paper proposes a novel formulation to provide rotation and permutation equivariance for deep learning models for small point clouds which are useful in many applications.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This yields a practical approach that the authors can apply to MNIST and to another dataset of that level of simplicity. I will admit that I m not familiar with the recent developments when it comes to ICA and to Flow based modeling of densities, but this seems like a good paper. This is a good paper even though I do not have familiarity with the recent developments on that specific topic.<|endoftext|>The authors propose to use volume preserving transformation to solve the disentanglement problem in latent variable models such as ICA. There is a volume preserving transformation from the sources to the mixed signal which represents the generative process. I think there are some merits in terms of the novelty of the paper. However, the setting for ICA appears to be different from the classical setup as it requires labels. Understandably you are trying to solve the same problem but you have labels to train the model. The paper proposes a simple and elegant solution to the disentanglement problem encountered in ICA.<|endoftext|>Overall, my impression is that this work provides some solid contributions to the interesting nonlinear ICA problem. The authors prove the identifiability of the proposed framework and implement the framework by volume preserving Flow based models. In particular, the theoretical results lead to the insight that the main indeterminacy of a nonlinear ICA framework using volume preserving transformations is the rotation of latent variables. This paper is well written, and the motivations and contributions are clearly presented.<|endoftext|>[Sorrenson et al 2020] proposed to do nonlinear ICA via volume preserving mixing functions, when a variable u is observed that makes the latent variables conditionally independent. Strengths: 	The authors provide theoretical support for the use of the pre existing GIN model for ICA. The paper is very clearly written. Could the authors please clarify this? I believe the appendix should be separately provides as supplementary material. The paper provides an interesting theoretical analysis of an existing method.<|endoftext|>The paper proposes a framework for nonlinear ICA with that restriction that the mixing function is a volume preserving transformation. Empirically, not all of the conditions seem to be necessary. Therefore, I tend towards rejecting this article but I am open to enter a discussion with the authors and the other reviewers. The method outperforms iVAE, which is an important baseline in the field.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper is very clearly written right until the actual method is described. One could argue that these are calculated in some form of an equilibrium pattern, but that is not the authors intent or explanation either. In addition, authors tend to miss quite a bit of related work.<|endoftext|>There are multiple issues in the paper, due to which the reviewer feels that the paper is not ready in its current form. In source coding, we have the entropy of message that is communicated through a certain message length.<|endoftext|>The article introduce a new theoretical game called Markov Coding Game (MCG). If we are facing a two players game, is it a special case of Markov game as in (Littman 94) ? According to the authors, this game is supposed to generalize several referential games and channel coding games.<|endoftext|>Therefore, I think additional studies are needed in order to  improve the technical and empirical contribution of the work. This game is related to the referential games, source coding as well as decentralized control. The paper identifies some interesting connections of MCG with several existing problems.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper studies task relabelling in hindsight to increase the efficiency of meta reinforcement learning. I like this paper overall. The motivation is sound: meta RL is almost by definition slow, by using a slower timescale for meta learning than the fast learning or adaptation, and so data efficient methods are key. The particular proposed method seems reasonable, although I have some concerns about the detail of the exposition – I found section 4.1 fairly difficult to follow. It could be I’m missing something simple or these things are all straightforward and clear to a reader with the right context. The implementation of the approach is quite neat. I am persuaded that the optimal meta learned solution will not be biased by the proposed relabelling; and that the derivation is sound. Optimising the relabelling distribution for the immediate post adaptation returns makes sense as a somewhat myopic heuristic to accelerate meta learning.<|endoftext|>The relabeling probability of a trajectory is chosen to be proportional to the exponentiated utility function, which is defined as the expected return after the agent uses that trajectory to adapt. Overall I think this paper presents an interesting idea for sharing data between tasks of a meta RL problem. Pros:1\.I find the main insight of the paper simple and intuitive. The derivation of relabeling according to the exponentiated post adaptation return follows naturally. 3\.The paper is well written. Therefore it is not clear to me why using the learned Q function is a good way to estimate return on that specific task.<|endoftext|>The paper proposes an approach for using data relabelling for meta RL for better sample efficiency and to enable training on sparse reward environments. Small performance gap with HIPI, Simplistic Environments Out of the 8 experimental domains chosen, the performance of the proposed approach (HFR) is significantly better than HIPI on only two domains (ant goal and sawyer push). This is despite the fact that meta RL also considers a multi task distribution, and can benefit from explicitly using data for a different task and relabelling it under the corresponding reward function. The mathematical formulation of the approach closely follows HIPI[2], with the difference that post adaptation trajectory return is considered instead of current trajectory return, to be aligned with the meta learning objective.<|endoftext|>The proposed method is simple and effective. However, I am not quite convinced about the novelty of the proposed idea and I think the experimental settings can be improved to strengthen the paper’s claim. The paper further proposes a meta RL algorithm based on PEARL (Rakelly et al.2019).The experimental results on several sparse reward tasks show that the method outperforms other relabeling methods as well as PEARL. The authors made an interesting point that compared to multi task RL, the objective in meta RL is to learn to learn a new task, so the metric of interest for relabeling trajectories in meta RL is their usefulness for task identification, rather than the returns like in multi task RL (Section 4, page 4). The paper’s main contribution is a trajectory relabeling algorithm in meta RL setting based on this intuition. This makes the meta RL problem setting confusing as the paper also says the tasks share the same dynamics and only differ in the reward function.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; For several knowledge intensive tasks such as question answering, it is common to use a mixture of language models (that encode knowledge implicitly in its parameters) with a graph neural network based model that can encode external knowledge from an knowledge graph (KG). This paper does an analysis of such GNN based QA models that do message passing over an external knowledge graph (KG) to gather external knowledge required to answer a question. Weaknesses /Questions for the authors  The model seems to be designed specifically for multiple choice questions. I think the paper will benefit a lot if this method is shown to be effective for a dataset which does not have multiple choice questions. The context node has all incoming edges and the mechanism in which the neural counter work all the weights of each edge will be added to the context node? Is there a correlation between how dense the context node is for a question and answer choice? Is it only in the subgraph creation process. The paper is definitely very interesting and is fun to read.<|endoftext|>Based on these observations, the paper designs Graph Soft Counter (GSC), a simple graph neural model which basically serves as a counter over the knowledge graph. The analysis of existing GNN modules is interesting. The insights on dissection are novel and significant for building better reasoning modules. The definition of multiple choice question answering and some terms used in related work are not briefly introduced,  making it difficult to accurately understand some of the content. It is inconsistent with the description (2*4+38 46) that follows. Do the observations and hypotheses in the paper apply to other complex reasoning (e.g.multi hop) QA datasets? The observations are novel, the proposed model is simple but efficient, and the experimental results are impressive.<|endoftext|>The paper designs a new GNN+LM model to do question answering and commonsense reasoning over knowledge graphs. The authors propose graph soft counter, a surprisingly simple and effective GNN module that counts edge mentions over the extracted subgraph for a question answer pair. I feel this is a great paper that is somewhat "too good to be true". The proposed GSC achieves better performance than UnifiedQA on the OpenbookQA dataset but using 1/30 number of parameters (actually is this accurate?find the detailed comments below). The authors also perform several analysis on the proposed GSC, e.g., the number of parameters, ablation of GSC and so on. However, in this work the graph_score is only calculated using the KG subgraph. Then the graph_score will also be similar. One such example being a question and its version with negation added. Can the author explain the reason behind this? I think it would make the paper much more impactful if the author can show that GSC works on another QA dataset and knowledge graph. Can it go beyond multiple choice question answering and find the answers over all the nodes on the KG?<|endoftext|>I agree with this finding but how does this imply that GNN modules are merely counting edges in the KG? The motivation behind this study is as follows. This paper’s analysis reveals that knowledge aware GNN modules may only carry out some simple reasoning such as counting and that counting of edges in the graph alone plays a crucial role in knowledge aware reasoning. Based on these insights, this paper proposes a Graph Soft Counter(GSC) which is a simple yet effective neural module as the replacement for existing complex GNN modules. The paper shows that with less than 1% trainable parameters compared to existing GNN modules for QA, the GSC module outperforms those complex GNN modules on two benchmark QA datasets. Weakness   The solution seems to be too good for the simplicity and the performance that it offers. It is hard to believe that such a simple model works so well. Not much novelty in the proposed solution.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper presents a new model for code completion which allows the model to completions with “holes” that are inserted in places where the model is uncertain. The model is evaluated on C# and Python programs and is shown to outperform existing techniques. RegexAcc   a metric for evaluating the fit of predictions with holes. I think this paper should be accepted. The evaluation shows that the approach outperforms existing techniques for code completion.<|endoftext|>Please clarify. While the runtime performance is unclear, the proposed approach can still deeply impact the future of LMC (especially for generative tasks), therefore I recommend this paper for appearing at ICLR in order to get the exposure it deserves. The provided context is not enough to make a reliable prediction of the identifier name, so the model seeks the user input. to my knowledge, Grammformer is the first grammar guided model that can generate code around "holes" that are added to the sequence in order to skip tokens with high uncertainty  the combination of RL and a novel metric (RegexAcc) to overcome the lack of supervised dataWEAKNESSES:  my main concern is about the runtime performance.<|endoftext|>This work proposes, GRAMMAFORMER, a transformer model for generating code with "holes" inserted in places where a model is uncertain. GRAMMAFORMER is trained on code completion task for C# and Python. However, to my knowledge, this is the first work where sketch generation is used in the context of code completion to prevent generation of code with low certainty. * The paper introduces REGEXACC measure. This will improve the results since the model will always generate syntactically correct programs.<|endoftext|>To exploit this observation, this paper presents Grammformer, a transformer based model for generating code completions with "holes" inserted in places where the model is uncertain. Strengths    Explores an interesting and potentially useful variant of code completion  The evaluation shows that the proposed method outperforms simple baselines like the unmodified transformer. I could not find any experimental results showing the overhead. Overall, I find the idea of sketch generation to be potentially useful.
Reject; rating score: 5; rating score: 5; rating score: 6; I d be willing to raise my score if the above points are addressed. Otherwise, I feel the paper proves something interesting, but the translation of these ideas into either experiments or downstream theory is lacking.<|endoftext|>The quality of the presentation of this paper is not good. and there is no simulation result to justify the theorem shown in section 5. 6) Numerical results are limited.<|endoftext|>This paper studies the Q learning in episodic learning from a Lagrangian formulation of the Q form Bellman optimality equation. I expected the authors to explain how T comes into play. The overall presentation of this paper is well organized and follows a concentrated storyline.
Accept (Poster); rating score: 6; rating score: 6; rating score: 3; FairCal does not require the protected attribute labels during training and testing. Additionally, they also report improvement in face verification accuracy. This makes me question the novelty of this work. How are samples in the calibration set chosen? (Minor) IJB C is one of the most widely used datasets for evaluating face recognition algorithms, which contains gender and skin tone labels. So, it is recommended that the authors add results related to skin tone  and gender bias on IJBC. I hope the authors address my concerns (especially about similarities with the BFW paper and add comparison to this paper).<|endoftext|>The paper proposes an approach, called FairCal, for fairly calibrating face verification models. The method does not require access to sensitive attributes for calibrating, instead the paper uses k means algorithm to find clusters of data and applies beta calibrating algorithm on each cluster. 6.I think it would be better to explain the beta calibration method in the main body of the paper as it is a "kernel" of FairCal method.<|endoftext|>This paper introduces a Fairness Calibration method for the task of face verification problem. Thus, I suggest to give the decision of rejection to this paper. The main strengths of this paper is that the propsoed method is a unsupervised method, and does not require retraining. For example, the global accuracy improvements compared with the state of the art methods are very limited  as shown in Table 2.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; rating score: 10; The paper examines the role of SGD noise in the generalization performance of neural networks. The authors show that this is indeed possible for a number of different architectures (ResNet, DenseNet, VGG) trained on CIFAR 10 dataset. By showing that simple full batch gradient descent can indeed achieve results similar to SGD, the paper provides an impactful contribution to the theory community. When going through Tables 1 & 2 and Appendix C.2, it was not clear to me where exactly the improvements are coming from. I strongly suggest reporting metrics such as training loss & accuracy with the validation metrics in order to avoid confusion. Will adding the explicit regularization on top of SGD improve the model? Can we reliably state anything regarding the flatness of the loss landscape by using 1 D projections of a O(1M) dimensional loss manifold? By playing with these normalization parameters, one can change the curvature without changing the predictions of the model. The main conclusions of the paper are supported with strong empirical evidence. There are a number of improvements / clarifications that can be added to the paper (which I have listed above).<|endoftext|>The paper shows what the title says; to paraphrase, the paper shows that GD with a bunch of non stochastic tricks can be as good as SGD with minibatch noise I think the paper reveals an important message to the field. The same effect can be reproduced by a series of deterministic tricks on GDHowever, I feel that the way the authors present the conclusion is a little too negative against SGD. Not to mention that a simpler technique tends to be more robust in more complicated settings. The authors are obliged to show that when the minibatch sampling is added, there is no further improvementUpdate: I am satisfied with the author s reply. Though I actually would like to give a 7, due to ICLR s scoring constraints, I can only give a 6. I do not feel that giving 8 is appropriate because this paper only suggests the possibility of SGD noise being non essential, but does not give a definitive answer. To be specific, the main speculation/conjecture is only validated for a single example, it is completely unclear whether these results are generalizable or not. However, I give a 5 because I feel that the authors missed one crucial conclusion, which might mislead the readers, and missed one crucial control experiment.<|endoftext|>The authors show that replacing implicit bias of SGD with explicit regularization can eliminate the generalization gap. The paper is also well written and easy to follow with a good cover of past efforts on this topic. However, this number is not justified in any way and no further exploration in that regard is made. Some possible questions that arise: (1) can we apply the same number of iterations as in small batch (117K) to close the gap completely with no need for additional regularization? (3) in terms of computational effort? Regularization   the authors choose a specific regularizer to replace the implicit bias offered by SGD. However, several other choices can be offered instead (e.g SAM, preconditioning). I understand that a complete 117K steps training in full batch is too cumbersome, but the current single datapoint of 3000 epochs seems arbitrary and not justified in any way. Regularization on baseline SGD   does it help? will be very interested to see a discussion either way. I still feel some of the choices are arbitrary (or at least not substantiated enough) and may apply to the specific case presented only (dataset, model), so I will raise my score 5 >6.<|endoftext|>This paper shows that it is possible to train CIFAR 10 models with full batches, and still obtain get test accuracy. As an empirical paper, there are a number of regularization techniques discussed in section 3: baseline sgd, stabilizing training, finite difference regularization of the Hessian of the loss equation (7), learning rate schedules, gradient clipping, gradient penalty, data augmentation. However, CIFAR 10 is no longer a challenging dataset. In this study, we seek to achieve high performance on full batch training at all costs. The theoretical implications of the experiments are not significant. So models trained with SGD *may* empirically overfit less than models trained in other ways. However, no one has argued that SGD is *theoretically necessary* for generalization. An empirical theory hypothesis would be something like: perform this type of regularization, and in practise, we obtain similar generalization performance to SGD. However, this would require more substantial empirical evidence, than is presented in the paper. The achievement of the paper is an empirical result: good validation accuracy on CIFAR 10 using full batch training and a number of regularization techniques.<|endoftext|>Paper presents empirical results that show that full batch training of neural networks can still generalize with appropriate explicit regularization. Updated score   I am satisfied by the results of the control experiments, and the explanations of the authors. I have read the other reviewer comments, and I believe the work will be quite valuable for the ICLR community and thus have changed the score to a strong accept. + Do the results hold across wider range of datasets, While I can see how much larger datasets could become more expensive   it would have been very satisfying to verify. Some other comments: Relevant citations that are missing, which hopefully authors can easily address:Second order optimization that seem to work well for Neural Nets:https://arxiv.org/abs/1503.05671 (KFAC)https://arxiv.org/abs/2002.09018 (Shampoo)https://www.bmvc2020 conference.com/assets/papers/0479.pdf (L BFGS)Recent work on curvature and gradient clipping (and other techniques): https://arxiv.org/pdf/2110.04369.pdfThis paper provides convincing contrary evidence that shows stochastic mini batching by itself is not unique, and can be substituted with explicit regularization, and techniques such as gradient clipping.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors theoretically show that the implicit regularization of (noisy) SGD encourages maximizing the inner product between the encoded features of current and next state action pairs, which is responsible for the observed phenomenon. Thus, the authors propose a novel explicit regularizer for offline TD learning to encourage smaller inner products. 2.This paper provides theoretical evidence to explain the feature co adaptation phenomenon, which meanwhile explains why it is severe for out of sample next actions. It is not easy to follow this story, and there are many preliminaries about offline RL that are indispensable for readers to understand the discussions. This is a good paper which studies an important problem from both empirical and theoretical aspects and brings in sufficient novel stuff. I appreciate the contributions made by the authors and recommend this paper.<|endoftext|>This paper provides empirical and theoretical evidence that value basedmethods that optimize TD errors with SGD have an implicit "regularizer" thatincreases the dot product of the representation at successive states. The paper shows empirically that these large dot products are asource of divergence and that training with the DR3 regularizer canstabilize learning. It might be good to further show evidence that the noise    model is well justified in the RL regime, which would justify the    leap from theory to empirics. Empirical analysis is comprehensive, clearly demonstrating the    problem and algorithms stability when the problem is addressed    (through the proposed regularizer). I think there is a leap in reasoning by connecting states aliasing    and high dot products. Is there any way to account for this?<|endoftext|>The paper looks theoretically sound some questions remain regarding the experimental efficacy of the proposed regularizer claimed by the authors. They prove an implicit regularization effect, following previous works focussing on supervised learning, that could explained the co adaptation phenomenon. Overall the paper is well written, pedagogical, and the problem at stake is really interesting. This could help conclude about the training stability the authors claim reporting in the paper (e.g.p. 2 " giving rise to methods that train for longer, reach a better solution, and remain stable at this solution ").<|endoftext|>This paper discusses how the implicit regularization effect of SGD could be harmful in the offline deep RL setting, due to degenerate feature representations, aliasing the representations for state action pairs that appear on either side of the Bellman backup. To address this issue, the paper proposes a simple explicit regularizer (DR3) that counteracts the undesirable effects of this implicit regularizer. The paper is overall well written and clear. The method proposed is simple, quite clearly motivated and the claims are quite well supported by the experiments. It seems that even when data is relatively abundant, it would be interesting to report the performance with and without the regularizer.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes to use six evaluation metrics to evaluate saliency methods for time series classification. 1) **Unclear objective / no novelty or insight**: The objective of this paper is very unclear, and there does not seem to be any novelty in the methods it introduces.<|endoftext|>This study deals with the issue of interpretability in the analysis of time series data using deep learning models. The strength of this paper is that it provides a comprehensive evaluation of nine well known interpretation methods for NNs with multiple architectures using multiple data sets.<|endoftext|>The paper proposes six quantitative metrics for evaluating post hoc visual interpretation methods on time series. It demonstrates the efficacy of nine visual interpretation methods using these six metrics over some common neural network architectures. So, although the paper is not novel, the experiments performed and the results obtained is very useful for researches looking for a way to perform post hoc visualization.<|endoftext|>This paper studies deep neural network (DNN) explainability methods in the context of time series data. However, it is not clear whether these metrics are reliable when applied to DNN explainability methods on time series data. 2.The work is really well motivated. The appendix also has insightful experiments and figures! (I am not penalizing the paper for this).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper discusses model robustness, claiming that the current studies on the concentration of measurement do not consider label uncertainty. The paper is clear and well written. It is an interesting paper.<|endoftext|>This paper focuses on adversarial learning, which is one of the hottest topics in machine/deep learning. The authors are concerned that the standard concentration of measure problem in prior works cannot capture realistic intrinsic robustness well. I am concerned about the availability of soft labels and the effectiveness of confident learning methods for estimating label uncertainty. This paper focuses on an important problem in adversarial learning.<|endoftext|>It suggests that the current formulation of intrinsic robustness based on concentration measures is insufficient because of the exclusion of label information. **Relevant Discussion**:The paper proposes a new formulation of label uncertainty. The paper is mathematically rigorous and well motivated. **Clarity**:The paper is well written.<|endoftext|>The paper improves the previous results on the intrinsic robustness (i.e., an upper bound on the adversarial robustness over a set of classifiers) based on concentration of data distribution, by incorporating the constraint on the label uncertainty of the models. This requires an information of label uncertainty for each data sample, so the paper leveraged CIFAR 10H (CIFAR 10 with human annotated uncertainty) in their experiments. Overall, I found the paper is clearly written, well motivated, and addresses an important problem.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper improves the architecture of deep VAEs using the attention mechanism. The introduction of the layer wise attention mechanism for deep VAEs is novel, and its effectiveness is supported by empirical results  b. d. Literature in deep VAEs seems to be well known by the authors and sufficiently cited## 2. c. Experiments are limited to CIFAR 10, larger scaler experiments (i.e.ImageNet) would be beneficial to the paper. The introduction of the layer wise attention mechanism is a great contribution. The results on MNIST, Omniglot and CIFAR 10 demonstrate the effectiveness of the method, and the ablation study clearly shows the positive effect of the two attention modules (layer wise and spatial). They would give the reader a better understanding of the impact of the attention modules.<|endoftext|>This paper proposes a novel attention based architecture for deep VAEs that facilitates dependencies between non neighbouring layers and reports improvements on the marginal likelihood over recent related methods on MNIST, Omniglot and CIFAR10. **Clarity**Overall clear. Some terminology is used a bit loosely in my view, in particular “local” and “global”. The paper suggests that the proposed method could be combined with autoregressive generative models, adding such an experiment would strengthen the empirical contribution of the paper. **Post rebuttal note**: Increased score to accept.<|endoftext|>This paper identifies a common problem in previous VAE related models: adding more stochastic layers to an already very deep model yields small predictive improvement while substantially increasing the inference and training time. The model is evaluated on standard dataset MNIST and OMNIGLOT, and showed superior performance against a wide range of baseline models. Overall, this is a pretty solid paper. 2.Evaluation showed strong superior performance compared with a wide range of baseline models.<|endoftext|>This paper presented a variational inference with attention mechanism. By explicitly modeling the local and global interactions in latent space, an expressive variational distribution was constructed with probabilistic justification. 2.This paper provided the experimental evidence to show that the previous method on expressive variational model via designing the deep hierarchy of interdependent latent variables would incur the problem of diminishing returns. 2.The experiments did not show the generated samples the authors obtained. The experiments can be furthered strengthened.
Reject; rating score: 10; rating score: 5; rating score: 6; rating score: 8; In general it seems close to a model paper. It then shows that recovery os theoretically possible and also that an obvious current algorithm candidate will fail but offers a substitute algorithm that works well on the examples given. It would be good to see more real world applications (and ones not contrived). The application to situations where one knows there are some correspondences (but not the correspondences themselves, for privacy reasons) is sufficiently of general interest.<|endoftext|>Some concerns:  There is the parameter $\epsilon_3$ in Assumption 3, which appears in the upper bound as the factor $1/\epsilon_3^2$. The authors propose to solve this problem by a nuclear norm minimization problem, and provide theoretical upper bound on the distance between their estimate and the true underlying permutation. If I set $\epsilon_3   O(n^{ 3/2})$ by that argument, it becomes a scale of $n^3$ in the upper bound, which might make the upper bound greater than $n$ and thus trivial (since the distance between two permutations is at most $n$). Overall I like the idea of the paper and the presentation, however I have one concern about the Assumption 3 and the possibility that the main result (upper bound) in the paper might be trivial in some cases, and hope the author could address that in the rebuttal period.<|endoftext|>(iii) The Sinkhorn algorithm seems to use O(n^2) time per iteration, as in (17) there is a term C(hat{M_B}), which needs O(n^2) time to be computed. Hence, in the regime where n << 1000 the algorithm might take much more time than O(n^2) (this is the regime considered in the experiments). (i) In the paper, the authors discussed this connection (i). Could you provide some intuition how it shows up, and in particular give an example? Suggestions: The following experiments might be useful. One would expect that the mean error and variance increases as the quality of initialization decreases. 3.Sensitivity to other hyper parameters. Minor Comments on language usage: (for example)     1.<|endoftext|>The authors also run a fairly detailed set of experiments that demonstrate the performance of their algorithm. They study this in the settings where there are missing entries as well as in the presence of bounded additive Gaussian noise. [1] considers the problem that is solved in this paper as well, but use an extension of the algorithm meant for the regression version of the problem which seems to suffer from the inability to extend to more aggressive noise models. I think this is an interesting problem and the paper is an interesting second step that comes up with an algorithm that more closely respects the structure of the problem than prior work.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper studies the problem of meta imitation learning of robot control policies from only video demonstrations of humans performing the tasks. This paper studies a very challenging problem and presents a novel method, but the experiments are not convincing enough. The method proposed by the authors combines multiple components.<|endoftext|>The paper considers the problem of robot imitation learning from video demonstration, without the need of accessing to an explicit representation of the action of the demonstrator. The method is experimentally assessed on different visual tasks.<|endoftext|>The paper talks about solving an interesting problem of meta imitaion learning from watching human video demonstration.<|endoftext|>This paper studies the meta imitation learning from human videos problem, which aims to learn a new task by watching a few demonstrations of a human performing the task. Since the meta IL component operates on the latent representation and not the generated images themselves, it is an even more indirect measure. It would also be nice to include some qualitative examples of the translated videos.
Reject; rating score: 6; rating score: 6; rating score: 6; The paper analyzes the performative prediction setting (Perdomo 2020)where multiple agents perform gradient descent to converge to a performatively optimal point. In my opinion, the model is novel enough for me to recommend a 6, but the technical results are expected and perhaps maybe that can push it to below the acceptance threshold. The stable point coincides with the optimal point where the sum of the agent s losses are minimized. The authors show that the learning dynamics converges to the multi agent performatively stable pointfor small learning rates. The requirement for convergence is that the hessian of the loss must be positive definite, which is satisfied by existence of a potential function. The authors also show that the dynamics exhibits Li York chaos for large enough learning rates. The strength of the paper is that it extends a relatively new model (2020) to a cooperative multi agent setting. We agree with the authors that this work only scratches the surface of the in multi agent performative prediction. The model is an original and novel contribution (to the best of my knowledge), but the results are confirmatory. So it is unclear to me if the work is significant for this venue. Overall the paper has a central point by presenting a framework of multi agent learning. The model is cooperative, so a potential function exists and learning converges for slow learning rate regime. A discussion section is lacking, in particular with regard to the significance of the work and results. What insights can be provided by the analysis from the paper? The claims in the paper are supported and correct to the best of my knowledge.<|endoftext|>The paper introduces a multiagent extension of the performative prediction framework in which multiple agents try to predict the same outcome, which influences the outcome they want to predict. Strengths:The framework of performative predictions is very interesting, with potential applications that are beyond the standard supervised learning methods. Its extension to a multiagent setting widens the  applicability of the framework in the real world. I am not convinced by some of the derivations:1.1. On the other hand, performative stability is a global solution of the objective problem, and is a much stronger condition than the Performative (Pareto) optimality. 3.2.1.4.In Eq.(4), the authors define the fixed point of (3) as the average gradient. Why is so? The argument at the end of page 5, in which they multiply numerator and denominator by the average gradient seems rather loose, since you could multiply both factors by any other value and still claim that this value is the convergence point. 2.1.The proposed approach reminds me of the replicator dynamics from evolutionary game theory, for which the emergence of chaos has also been studied. 2.3.Conditions on the learning rate for the chaotic behaviour of gradient descent have been studied in the literature. How the current results relate to these previous results? 3.The simulations show oscillatory behaviour, but it is not clear from the experiments that this is actually chaotic behaviour. Minor comment:It is clear that the algorithm will converge for small enough learning rate, and it can oscillate or diverge beyond that threshold. Some motivation on why it is important to study the chaotic behaviour in particular would help to appreciate the impact of this work. Metareview The authors have responded to most of my comments clearing my concerns on the derivations. I think that an experiment showing sensitivity to initial conditions is missing.<|endoftext|>The setting is called performative prediction, which can be viewed as a special case of reinforcement learning (after the model makes a prediction, the environment returns a feedback by changing the data distribution). The proof is based on standard analysis of studying a potential (surrogate/Lyapunov) function for the mean squared error, and arguing that the gradient flow vanishes only if the potential approaches $0$, which implies that the convergence of the flow. Then the discrete gradient update is approximating the flow well if the stepsize is small enough. (ii) if the learning rate is larger than some threshold value, the authors then showed that the exponentiated gradient descent became Li Yorke chaotic, where it can periodically oscillate for infinitely many times. The main idea is to use Eq.(7) to characterize the dynamics, and show that under some conditions Eq.(7) induces a sequence of { x_i }_{ i >  1}, with some recurrent behaviours as shown in Lemma 4.7. I think this paper makes a reasonable contribution. (1a) The first main result of "using small learning rate EGD is stable" is kind of a standard result as expected, and also the techniques are based on standard analysis (potential functions and approximating continuous gradient flows). (1b) The second main result of "using large enough learning rate EGD is Li Yorke chaotic" is also similar to existing work, e.g., the observation of EGD is chaotic in other games (Palaiopanos et al., 2017). However, given there already exist observations and results about exponentiated gradient descent could be chaotic in several game settings, and the performative prediction studied in this paper is also a special game setting, I consider the contributions and technical novelty in this work on an incremental level. My main concerns are addressed and thus I would increase my score.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper focuses on estimating treatment effects under a limited overlap, i.e., subjects with specific characteristics might only belong to one treatment group. To solve this problem, the authors propose a variational autoencoder (VAE) called $\beta$ Intact VAE, which extends the framework of CVAE and iVAE. The authors prove that the identifies the treatment effects. The authors have provided theoretical guarantees for the proposed method.<|endoftext|>The paper considers the setting of limited overlap of covariates and studies identifying and estimating the causal effects. The prognostic score is an appropriate tool for limited overlapping because it can map some non overlapping values to an overlapping value in a space of lower dimensions. 3.The theoretical analyses are novel under the setting of this paper. In the end, the paper compares the model with recent methods in the experiments.<|endoftext|>I believe the paper solves the interesting problem of estimating CATE with limited overlap. The proposed method seems reasonable, and justified by the theoretical analysis in the paper. The examples of this paper, in which we want to adjust a causal estimate using high dimensional covariates has always struck me as a bit unrealistic. The authors present a theoretical analysis using PtS, an adaptation of prognostic scores.<|endoftext|>The present paper proposes to address the issue of limited overlap in treatment effect estimation by investigating the identification assumptions of prognostic scores which, in certain contexts, are less restrictive on overlap than other methods such as propensity score based methods. Propensity score weighting for causal inference with multiple treatments. The authors claim that $\mathbb{M}(X)$ is an effect modifier. The main contributions of this work are presented in Sections 3.2 and 4.2, where they derive identifiability of the treatment effect via prognostic scores and propose a generative prognostic model based on variational auto encoders (VAE).
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; This manuscript describes a method that models multiple time series data (sensors) across different individuals (samples). A graph is used to model the dependency between the sensors, such that the data of one sensor could be used to infer another, which is the key point of this ms. How one sensor affects another is modelled as a message passing problem on the graph. At each time point, an observation embedding is generated, either from a sensor or its neighbours in the graph. Observation embeddings of a sensor is then aggregated into a fixed length sensor embedding, using temporal attention. I think the model design is reasonable and the results are impressive. It is interesting to see that the dependency between different sensors really helps the predictions in Setting 2,3,4, which will be very useful for many applications.<|endoftext|>The core idea of the paper is transforming multivariate time series data into a vector z and using z for classification. What’s more, the dot product between u and v is a dot product with two vectors between them. What’s more, based on equation 3, e is calculated by α. Therefore, it seems there is no need to link u and e in Figure 3 a. 3) The packed observation embedding in Figure 3 b is not mentioned in the paper. I tried to search the word “packed”. The source codes are unavailable, and the main results cannot be confidently reproduced.<|endoftext|>This paper introduces a GNN based method for classifying irregular multivariate time series (MTS). It jointly learns embeddings and dependency graphs through several attentive mechanisms. In this paper, the authors developed a GNN based methods for modeling irregular multivariate time series (MTS) and their classification. The goal is to learn MTS embeddings, which build upon sensor embeddings, which in turn build upon observation embeddings using temporal attention. During training, the embeddings and the dependency graphs were learned jointly, through the GNN related parameters. The dependency graph was inferred through observation level attention across sensors. Encouraging similarity among all samples may be an unreasonable simplification. The concerns of the paper are summarized as below. The following several papers have taken into account of learning correlation parameters between sensors/variables in learning embeddings of irregular MTS. 10.In the experiments, the visualization of the dependency graph cannot support the conclusion that the model learns dependencies for distinguishing classes. In other words, it is obscure on how important message passing is for dealing with the irregularity. The paper is meaningful in exploring the new application of GNNs in irregular MTS related tasks. 5.Since Eq (5) doesn t consider temporal order of MTS by using temporal attention, it is better to discuss if there is any impacts by neglecting the temporal structure during the aggregation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; As the authors also stated, there is an implicit assumption about "other object states do not change." This is a very strong assumption and can make model learning much easier, as in the STRIPS representation. **Model**It will be important to showcase or verify that the modeling of belief is important. The fact that at each step, the observation only contains objects with state changes.<|endoftext|>This domain is particularly challenging, since the text observations received at each time step are variable length, and will only provide a partial description of the world, from which the full state must be reconstructed. sec 4 table 2: why aren t the GATA baselines in the "model based planning category"? I m familiar with one way that evidence lower bound can appear: $\log P(X)   D_{KL}(Q || P)$. 8: figure 4   last two subfigures have same labelOverall, I thought this paper was well written and presents a significant contribution in a novel area: modeling dynamics for text based games. Are there no model based baselines, since this work is the first in the text based games domain? I had some questions about the technical details of their method, especially the formulation of their objective. But these could probably be well answered in follow up discussion or in an appendix. Is there a one to one correspondence between $S$ and $Z$? It sounds like each action is identified by a (possibly lengthy) sentence.<|endoftext|>The motivation of the paper makes sense. The author starts with challenges in text based games:* text observation has variable length of sequence. General Significance: This is a difficult and interesting domain to study. The empirical results give strong evidence that this approach led to reasonable improvement in planning. The general ideas are clearly conveyed. But if the author can use more illustrative examples on explaining the technical details, it would be more readable for broader audience.<|endoftext|>Therefore, the authors can combine the learned model with Dyn Q or MCTS methods to select actions. The success enriches the application of dynamic model learning and planning algorithms. I don t agree that extending OO MDP with partial observability is a significant contribution. 2.In Figure 4, there are two OS OOTD in the title. This violates the logical flow and makes the section less coherent. It is not proper, as z should be a learned latent variable and I think there is no ground truth information about z. In contrast, the authors should talk more about the observation, which is the real given input to the dynamic model.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The current method proposes a scene generation technique that models explicit knowledge by imposing semantic rules on the objects in the scene. Disagree, rules are explicit knowledge about the scenes and can certainly be incorporated in the similar fashion as  [Ref 2] as long as those are simple co occurrence based. To me the adversarial attack is something where we modify the image / scene by a small amount (invisible to the naked eye) to deceive an existing trained model. It would be more convincing if the attack changes only a tiny amount of the 3d point cloud and still could  fool a trained model. * The synthetic scene layouts utilized in this work are rather simple.<|endoftext|>The approach is evaluated on a synthetic dataset and as a way to generate adversarial examples for scene based segmentation models. The ability to inject explicit knowledge into generative models would be very helpful for many tasks. A VAE model is then trained to learn a structured representation of the data. I feel there are many other applications to evaluate the general generation capabilities. I am not experienced in this domain but it feels to me that this should be more closely investigated from the view point of graph based neural networks and graph based image representations and rendering. The tree based representation feels very restrictive to me.<|endoftext|>This paper proposes a method to incorporate domain knowledge into the physical scene generation process. Strength:  The tree structured generative model looks interesting, and they propose a two stage training method to learn such a model. I think the authors should briefly introduce the previous approaches instead of simply citing them, e.g., [49] and [55]. The contribution of this paper is the proposed T VAE and knowledge guided generation. The knowledge part adopts some explicit knowledge rules. The baseline methods are not state of the art methods, and they should consider the more recent works. The T VAE and knowledge guided generation proposed in this paper looks new, and the attack looks interesting.<|endoftext|>This paper proposes the use of tree structured VAE as a mechanism forencoding several forms of constraint knowledge. A more realistic LIDARsegmentation scenario was also explored, where the goal was to generaterealistic but adversarial scenes. I have not seen this approach previously, and the literaturereview supports the claimed originality. In the case of the experiments, there is a lot ofprecise detail that is missing that leads one to wonder if the claims area bit exaggerated. Since the generator iscreated by the authors, it is unclear why the cars could not be aligned withthe lines. The experiments and applications are not very exciting.
Reject; rating score: 3; rating score: 5; rating score: 8; rating score: 8; The authors study data poisoning attacks that aim to degrade the overall (test) accuracy of a model by adding small perturbations to all training samples. They then show that this principle is sufficient to create poisoning attacks by proposing a simple method relying on random, class specific pattern. Finally, the authors propose defending against these such poisoning attacks by relying on a pre trained model and only fine tuning the last (linear) layer. At the same time, the rest of the paper s contributions are rather minor:  The finding that shortcuts are sufficient for poisoning is relatively well known. While the work does contain some interesting findings, the overall contribution is rather minor when taking into account prior work. Thus, I do not find it suitable for the general NeurIPS audience.<|endoftext|>The authors empirically investigate the underlying reasons behind the effectiveness of indiscriminate poisoning attacks. For some experiments, the experimental details are not provided. For example, against which deep neural network have the authors crafted the poisoning samples to generate Table 1? The paper tries to clarify an issue, but more insights need to be developed. This should be clarified in the paper, and more details on the practicality of the threat model should be discussed or acknowledged as specific limitations of this work (and other papers that use similar setups). 1) The authors claim the poisoning samples generated adding imperceptible perturbations to the training data are well separable (accordingly to the class of the original sample) in input space with hyperplanes. They should add a comparison of their computational complexity. 4) The authors claim that using a pre trained model is sufficient to defend against clean label poisoning attacks.<|endoftext|>They exploit the shortcut learning of deep learning. Strengths:  The understanding of this paper is interesting. The understanding applies to a wide range of existing works in this area. At the beginning of section 2.4, you describe shortcut learning as spurious features that are useful for training but do not generalize to test data. This paper made an interesting observation on the linear separability of a wide range of indiscriminate data poisoning attacks.<|endoftext|>Additionally, pre trained feature extractors are suggested as a powerful defense strategy. 2.The criticism from Radiya Dixit & Tramer (2021) may fail to challenge the effectiveness of the indiscriminate attacks considered in this paper, since their criticism was directed only at those targeted attacks such as Fawkes and LowKey. Specifically, not all indiscriminate data poisoning attacks work by adding imperceptible perturbations. 4.The last sentence on page 1 is imprecise. Specifically, it was [4] that pointed out for the first time that "adversarial examples can serve as an indiscriminate poisoning attack".
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This has the potential to make model upgrades more practical for real world large scale applications. The paper is the first to study the impact of such “hot refresh” techniques to the image retrieval domain. S3) The proposed training method achieves improved performance in the hot refresh application compared to previous work, reducing the penalty incurred by negative flips. The proposed uncertainty based backfilling method overall feels simplistic. This is an interesting paper, targeting an important application, proposing a novel observation and method to improve hot refresh image retrieval. However, some aspects of the paper are not well developed (see weaknesses listed above).<|endoftext|>This paper tackles the problem of model upgrades in large scale training of image retrieval models. However, this does not impact the overall readability of the paper too much in my opinion. For the results, I think they are clearly presented and link to the contributions well, as demonstrated in Figures 3 7 which shows that the new losses and uncertainty driven backfilling indeed helping the hot refresh model update process significantly. vanilla, as it ties back to the observation of negative flips causing the model regression problem as a main motivation for this work. Another comment I have is the phrase “Regression free”. Overall, this paper is a good read and does provide useful insights to the proposed problem and potential remedies.<|endoftext|>The paper presents a method to learn compatible features with low negative flips rate. Resampling is based on feature uncertainty. Their dependency seems to be only related because of the final application. Even if sequential multimodal compatibility did not perform well, it would have been a great baseline to compare with hot refresh model update. This could be a strong assumption since the gallery is not a statistically meaningful set like for example the test set in classification tasks. The paper introduces a very nice problem with some technical details to be clarified.<|endoftext|>To solve this the authors propose a compatible training method that takes these flips into account, encouraging the new to old pair positives to be more similar than both new to old and new to new negative pairs (this is nicely illustrated in Figure 2). _Strengths_  The presentation of the paper is very good. The paper is well written and the method has been properly motivated. It addresses an interesting problem that has real world application in industrial image retrieval applications  _Weaknesses_  Although it addresses a novel problem in image retrieval, I find the novelty of the method itself somehow limited. Even though I consider the technical contribution a bit limited I positively acknowledge the fact that they re the fist to study the problem of hot refresh model upgrade in image retrieval in particular and the problem of model regression.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper describes a method for augmenting task selection in meta learning, by interpolating support and query sets between two random tasks from the base dataset. I found the approach to be simple and relatively well explained, including ablations studies on large point questions I had while reading, including its behavior and effectiveness for different sizes and number of classes in the original base dataset, as well as effects of inter  and intra  task interpolations. The theoretical sections corroborate this, but I found them hard to follow. This difference enables between task interpolation which adds additional augmentation particularly in settings where few tasks can be drawn from the base data.<|endoftext|>This paper proposes a task augmentation method via task interpolation for data efficient meta learning. The experimental results on variety of few shot learning dataset show that MLTI is effective when the meta training data for constructing training tasks is not enough, for both gradient based and metric based few shot learner. This paper proposes a novel task augmentation method, which is affected by Manifold Mixup, which can be applied to many existing few shot learning tasks. Comparison with the prior methods in large dataset is missing. However, the proposed method is not restricted to small dataset. However, I believe the idea of this paper is valuable for few shot learning field, and I recommend to accept this paper.<|endoftext|>The interpolation strategy is quite simple   interpolate between a pair of tasks, in contrast to existing methods such as adding label noise or data augmentation on each task individually. strengthsAlthough nifty, the idea of pair wise task interpolation is an incremental change over the existing data augmentation approaches. The theoretical results, highlighting the relationship between task interpolation and the Rademacher complexity, are non trivial extensions of the Zhang et al.ICLR 2021 and Yao et al.ICML2021 to account for pair wise task interpolation. The paper is well written and easy to follow. The theoretical results are non trivial extensions/combinations of existing work. The contributions are strong, albeit limited to the meta learning research community.<|endoftext|>[Summary]This work tackles a scenario where there may not be a large number of training tasks available, which increases the susceptibility of meta learning algorithms to meta level overfitting/memorization problem. The paper reports better performance than other methods on benchmarks that have fewer training tasks. [Strengths]The work provides extensive theoretical analysis to provide theoretical guarantees as to how the proposed MLTI task interpolation method achieves better generalization. [Recommendation]Despite strong experimental results and analysis, at this point, I believe the technical novelties are not significantly different from the work by Ni et al.Thus, I believe the work is marginally below the acceptance threshold. As the authors have addressed most of my concerns, I m happy to increase the score accordingly.<|endoftext|>In this paper the authors propose a meta learning method for few shot learning. The new tasks are generated by interpolating features/labels of two sampled tasks from the training set. Although previous work exists that augment the number of tasks, this is the first approach that augments across tasks (rather than within task). It would be nice to see results in other domains such as RL/NLP/etc tasks. I find the theoretical analysis difficult to follow and potentially not very informative to the rest of the paper (that been said, I am not an expert on generalization theory/Rademacher complexity and cannot properly validate it). The proposed idea is simple and novel, the paper is well written and the empirical evaluation is well executed. **Post rebuttal update**I thank the reviewers for the rebuttal and I keep my rating of 8. Congratulations for the nice work!
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper tackles the dynamic scheduling problem in semiconductor manufacturing using an RL approach. Due to the processing actions taking different amount of time to complete, the problem has a long reward delay issue, which the paper addresses through using predictron to estimate the targets in DQN. Results show that the proposed method outperforms the baselines in terms of cumulative lateness of the parts. The proposed method is basically DQN with the targets estimated by predictron based on rolling out a simulator. Empirical results are presented to show the advantage of the proposed method over common heuristics and vanilla DQN.<|endoftext|> PDQN   A Deep Reinforcement Learning Method for Planning with Long Delays: Optimization of Manufacturing DispatchingIn this paper, the authors applied a reinforcement learning algorithm to the problem of manufacturing dispatching. They used a model based algorithm that is inspired on the algorithm of predictron. Cons:1.It is not clear why the authors decide to make the presented modifications to the original predictron algorithm. The algorithm is only compared with some toy methods and the RL baselines DQN is quite outdated. *Summary Of The ReviewI am worried about the novelty of the algorithm, and the claims are not supported by experiments.<|endoftext|>This paper finds its motivation in scheduling for semiconductor manufacturing systems. The authors introduce an algorithm called Predictron Deep Q Network (PDQN) that plugs a predictron architecture into the loss of the Q network. The efficiency of PDQN is tested on two scheduling problems and challenged against 3 natural baselines, CR, FIFO, and DQN, the first two baselines being standard for scheduling. As such, delay is not formalized, although intrinsic to the problem considered. Although this work is well motivated, I have two main concerns leading me to a « weak reject » notation. First, on the theoretical side, the problem of delayed reward is not tackled frontally, but rather implicitly through the predictron. Another Q learning method for delayed rewards is proposed in [3]... Learning and planning in environments with delayed feedback. In that regard, a review of delayed RL works is missing. This discredits a bit the use of the predictron.<|endoftext|>The authors study a deep reinforcement learning approach to the problem of scheduling machines in a semiconductor factory. The authors  main contribution is a deep Q learning based system that uses a predictron to estimate the value function (instead of max_a Q(s ,a)). The architecture the authors propose seems quite similar to a dueling DQN in the way that the Q function estimation and the value function estimation are split—the difference being that the dueling DQN does not use a predictron and that the dueling DQN has a different aggregation architecture. As an application perspective, there are critical application specific issues that are not addressed.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; Experiments are convincing. The provided code is helpful for researchers who need a fast computation of NTK. Though the ideological (mathematical part) is very simple. The first method, Jacobian contraction is quite straightforward and is not suggested for use. The second one,  NTK vector products is based on reducing the task to the computation of Jacobian vector and vector Jacobian products (which themselves can be computed in a fashion similar to forward pass). The method is suggested for use for networks whose width is smaller than the dimension of the output space (probably for contractive autoencoders). It turns out that this method is preferable for shallow networks or the typical sitution when  width is larger than the dimension of output space. I would not recommend the paper for publication at such a top tier conference as ICLR. The simplicity could be justified, in principle, by the novelty. But this paper is too simple, and the novelty is moderate.<|endoftext|>This paper studies an in depth analysis of runtime and memory requirements for computing the finite width NTK. Finally, they make all their implementations open source based on the JAX library. The novelty of this work is fairly weak. All results come from simple computations of automatic differentiation, and the reduced cost of NTK follows from amortizing simple linear operations, which is not surprising at all. However, these weaknesses might be covered by their open source implementation, as it can be significantly important and useful to future works. The infinite width NTK can be exactly computed without Jacobians (Arora et al., 2019) and it can be much efficient than Jacobian based approaches. Does the finite width NTK have concrete benefits compared to the infinite width NTK? Minor Issue:    It would be great if more details of Automatic Differentiation (AD) operations to derive memory costs of JVP/VJP are provided. It would be great to make larger  font size. The review score is all about the battle of lack of novelty versus the impact of implementations. I judge this work to be under the bar of acceptance for now but am willing to raise it depending on the author s feedback on the importance of computing finite width NTK.<|endoftext|>Strengths:  Computing the NTK is a popular area at the moment and the open source code should make some experiments possible that were not previously for some researchers. For example, 1 second to compute the NTK of a resnet 18 on a v100 for a single pair of imagenet inputs is not ideal. This may seem a bit unfair a criticism but I feel it is valid given that this is a code paper. I think JAX is nice, but wondering if the authors also be implementing their approaches in other frameworks like PyTorch? I realise there are properties with the way that autograd is written in pytorch that makes this more difficult, but I feel like an implementation in just JAX serves to benefit a subset of the research community not the whole, particularly one with a commercial motive. I know it s standard notation but some may not have seen this. You can probably get away with it in the abstract for space, but in eq 1 you should spell these out imo. Nice code which should be used by some researchers. Concerns that calculating NTK is still too expensive (which it is by nature, not fault of this paper) to prohibit some researchers and also that uptake in using the code will be only by a subset of the community.<|endoftext|>This work aims to solve the computation problem of the finite width neural tangent kernel (NTK), which is a central object in deep learning. The authors analyze the computation and memory requirements for finite width NTK and propose two novel algorithms that can improve efficiency. Strengths(1) This work provides a comprehensive study on various ways to compute the finite width NTK. Thus it should be independent with ${\rm W}$ and ${\rm L}$. (2) The dashed lines in Figure 1 are hard to distingish, I suggest the authros could make it clearer. Overall, this article is clearly written and well structured. If the author can solve the problem of unclear dotted line in Figure 1, it will be more perfect. It would be better if the authors discuss why they choose JAX instead of other frameworks.
Reject; rating score: 5; rating score: 5; rating score: 6; It also proposes an algorithm, ParetoSelect, to choose hyperparameter defaults that lie close to the Pareto frontier in a multi objective evaluation (e.g.model accuracy and inference latency). Such a large scale comparison is missing from the literature, and represents a gap in our current understanding of which method best apply to which context. In addition, the paper introduces a method to select hyperparameters and ensembles based on approximation of the Pareto front in a multi objective setting. This approach is interesting and to the best of my knowledge is novel in a time series context. The paper is reasonably well written and easy to follow, although the mathematical notation and writing style could be made more precise, as a few points are listed below. The authors are to be commended for providing full source code to reproduce experiments, and a cursory look suggests that the code is well written and easy to understand. The main concern with the paper is that its focus is unclear as it tries to do both too much and too little:  If viewed as a contribution to the large scale benchmarking of time series forecasting algorithms, it offers a starting point but falls far short in offering substantive analysis of the presented results, e.g.to convey strong conclusions as to the suitability of particular methods for particular datasets, e.g.regarding time series properties such as variability, intermittency, seasonalities, etc., and possibly grouping benchmark datasets by broad area of study (e.g.retail, economics, energy, ...). Section 3.5: The results reported in Table 1 include rank information across an unknown set of models: that s obviously more than 7+6 13 introduced in Section 3.2, and seems to include some form of ensembling. However, how the ensembles are constructed has not yet been introduced by that point in the paper. p.5 [Comparison of classical & deep learning methods]: the paper should explain how the test statistic is constructed exactly. Does the test include only the raw deep learning methods, or it includes ensembles as well? p. 20: "we set each quantile to the forecasted value"  > do you mean that $q_{0.5}$ receives all the probability mass and the others receive zero mass? The paper attempts a valuable contribution to the univariate discrete time forecasting literature, but by attempting to cover too many things quite superficially, does not appear ready to recommend acceptance at ICLR in current form.<|endoftext|>This paper provides comprehensive evaluations of various time series forecasting models on 44 heterogeneous and public datasets. Results have shown that ParetoSelect has outperformed other model selection methods. The result itself is valuable: e.g., comparison between classical and NN based methods, global and local models, etc. 2.Learning models in a multi objective setting is a common problem in many applications. Constructing the model configuration set based on the Pareto front is a natural solution and is proven to be effective over other baselines. The overall structure of this paper can be improved. This can easily cause misunderstanding to readers what is the major focus of this work. 2.Although the empirical study is an important component, there are still too many experimental details in the paper. 3.Since the property of each dataset varies a lot, the authors should also consider providing insights on which type of model is more suitable to datasets with certain properties, while the current paper aggregate the data information too much, and just provide general results on all datasets. Are these layers of MLP or others? ## Update after RebuttalWe thank the authors for their detailed response in addressing questions and updating their manuscripts. The analysis of dataset characteristics in Appendix F is interesting but it does not appear to provide useful information that can serve as an insight in the future: this is fine on my side since it is not easy to discover patterns on massive datasets. Although the authors have also addressed the minor questions, we still think the structure of the paper can be further improved to make the paper more readable. The authors need some non trivial modifications before it can be accepted.<|endoftext|>This paper studies the tradeoff between primary model performance metrics such CRPS and CRPS rank vs other criteria such as inference latency. The surrogate model is used to sequentially evaluate the considered models and optimize the hyper volume of the Pareto front. An interesting observation made in the paper is that classical models do not always beat deep methods even for relatively smaller datasets consisting of few thousand data points. **Strengths**  This is an excellent work comparing a large and diverse set of time series forecasting approach wrt to model performance and prediction latency. The main contribution of this paper is empirical, presenting the latency and performance tradeoffs for various models. The study presented in this paper while not absolutely exhaustive, is still quite valuable to the community. Some ways to improve the study even further are suggested in the weaknesses section. This is an important observation as it breaks a common myth in the time series community. Although this statement may be a little too strong since classical models may still outperform deep models when the number of data points is less than a thousand. The datasets considered in the paper have at least a 1000 points. Were the models tuned in any way? or were default hyper parameters used? This is the most significant weakness of this paper. Although, GPs are not necessarily the best choice for ranking. The latency values in Table 1, do not include confidence intervals / stddevs (only required for the unconstrained models). **Update after rebuttal:**Thanks for the response. However, I do also agree with the other reviewers that the organization of the paper and the presentation of the results can be greatly improved and hence I am keeping the same score. Details below:  Hyper parameters: Thanks for pointing to the appendix. Column 2 shows that different search sets for the context length have been used for different deep learning approaches. For a fair comparison, the same set of context lengths should be used since a larger context may provide an extra advantage  to the models. Ranking for model selection: My concerns have been fully addressed. It is only feasible to rank the performance rather than predict the exact accuracy. Contributions in the paper: I do agree with the other reviewers about the presentation of the contributions and the organization of the paper. Overall this a great piece of work comparing tradeoffs between multiple performance objectives for time series forecasting models, including classical and deep models. Some interesting observations have been shown in the paper which are mostly well justified. The main weakness is that the choice of hyper parameters has not been explained, raising the concern of whether better hyper parameter choices can improve the baseline results.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; rating score: 3; * Related: would it make sense to have some notation for k dimensional persistence diagram? As an example, one can construct a data embedding and force it to be a circle, or a figure 8, or have three clusters. The paper is based on a series of several 2020 2021 papers, with which I am unfamiliar, so it was difficult for me to judge on the novelty. I found the paper interesting, and the method seems to work well. with the topological cost, where the latter forces the embedding to have a particular topological structure as represented by the persistent homology. Wasn t defined. MAJOR ISSUES* All considered datasets are very small. MEDIUM/MINOR ISSUES* I like Figure 1 and appreciate that the authors attempted to explain persistent homology in the main text. The topological loss enforces single cluster but here we see multiple clusters. That s confusing. Would it make sense to annotate them?<|endoftext|>Several novel topological loss functions are presented including k dimensional holes and flares. And the combinations of these basic losses can be applied. From (3), the topological loss of this paper is constructed from the embeddings instead of the input data. 3.The claim that “a topological model that is naturally present in the data should be represented well by many subsets of the data” is not quite convincing. Does it mean top. Authors proposed to joint optimize low dimensional embedding and topological loss, but the proposed method might be thought as a straightforward extension of existing work. Some claims need to be clarified in detail.<|endoftext|>This paper proposes a topological regularization method for incorporating topological prior knowledge for shaping data embeddings. This is achieved by introducing a new family of loss functions based on the characteristics of persistence diagrams. Also, the paper does a poor job in explaining those concepts and I believe many readers in this community will have a hard time understanding it in its present form, due to lack of background. The authors acknowledged this by adding Appendix A, but many points are not sufficiently detailed in there either. Considering that acquiring knowledge about the data topology as well as translating this knowledge into an adequate loss function can both be difficult tasks, I would like to see a more thorough evaluation of how misleading the obtained can be if any of these steps are not performed adequately.<|endoftext|>3) As conceded by the authors, the prior topological information is vital to the method, but it is unclear how it is discovered. The paper accomplishes this by adding a novel topological regularisation term to the standard embedding loss. The following are questions and suggestions to improve the clarity and content of the paper:1) The methodology rests on equation (2) which demonstrates how topological information is used by the regularisation term, but the notation is difficult to parse. Also, what is an example of this summation that has more than one term? This is the number of points in the persistence diagram, but this should be expanded upon. 2) What is the complexity of the method and how does it scale with the number of data points and type of persistence diagram?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The authors also establish the convergence rate of the proposed algorithm. The main optimizer in ProgFed is fedavg. The reviewer still has several concerns about the current submission:(1) It seems that the main benefit of progressive training for federated learning comes from the early stage during training.<|endoftext|>The paper introduces "ProgFed", which is an algorithm designed for federated learning scenarios. Instead of training the neural network from end to end, it first trains the shallow layers, and then gradually train the deep layers, hoping that: 1) training the shallow layers first will learn some simple and meaningful features which will help training the deep layers, 2) and thus reduces the communication cost for the federated learning applications. However, I think that there are several weaknesses in this paper. However, progressive training is an existed method trying to speed up the neural network training in the single machine setting. Thus the first weakness comes from the algorithmic novelty perspective. 2.The second weakness (major weakness) comes from the model and the theoretical analysis.<|endoftext|>This paper introduces the idea of progressive training of deep networks for federated learning, where only some layer blocks are trained initially and the remaining blocks are progressively added, in order to reduce both local computation cost and communication cost in the early stages (as there are fewer weights). Regarding the results, the choice of learning step $\alpha_t$ seems to be hardly usable in practice, as it relies on the knowledge of both the full gradient and the gradient of the sub network. The paper is silent about this aspect there were no ablation studies or investigation on this particular ingredient. In the federated setting, communication gains significant communication gains are also achieved.<|endoftext|>The issue that this work attempts to solve is that of computation and communication efficiency in federated learning. There are some points that I would like some input from the authors which, if turned out positive, would strengthen this work further. As an extra bonus, the authors also show that ProgFed is more amenable to compression / quantization of the federated messages, thus showing that the benefits of these, more traditional, approaches are additive to the ones of ProgFed. This submission is good; the paper is well written, the idea is simple and seems to work well in practice. There is also a nice bonus that the proposed method is orthogonal to more traditional approaches that target communication cost reduction. Does the performance of each head become worse after the end to end fine tuning? Can it be extended to the federated setting? Do you have any indication or intuition as to why progressive training helps with quantization / compression?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a new architecture that adopts the pyramid structure and employs a novel regional to local attention for vision applications. + The performance of this work is promising. And the proposed methods are not novel enough when compare with the previous ones. However, I am concerned with the novelty of this work and the contribution may be limited.<|endoftext|>The paper introduces a new spin on the ViT architecture. I guess this factor might also be different for the Ti/S/B/L models? At the core of the method is a regional to local attention approach, where very coarse regional tokens are processed by a normal multi head attention yielding global attention and each coarse regional patch is additionally represented by a set of smaller local patches, which are also processed together with the corresponding regional token to get finer grained local attention. **Strengths:**  Overall the paper is fairly easy to follow and it s easy to understand how the approach works, including a fair amount of detail to potentially reproduce results. Shifting the focus to this part of the paper would be more interesting in my opinion. Given that one big part of method is the reduced complexity, it is also not surprising that the model is computationally less heavy, but it would be a drawback if there is some performance ceiling it hits due to the specific architecture. Lastly, the writing is not the best in some parts of the paper.<|endoftext|>This paper introduces local inductive bias into vanilla vision transformer by adopting the pyramid structure and employing regional to local attention. The regional to local (R2L) attention is the main contribution of the paper. ### Advantages1) This paper is well written and easy to follow. 2) The experiments are sufficient to evaluate the proposed method. ### DisadvantageSince the R2L attention has two steps, I am concerned about the throughput of RegionViT. It is better to show the throughput comparison for Table 3. The paper is well written and shows efficient experiments to evaluate the proposed RegionViT.<|endoftext|>The paper presents a new ViT architecture by adopting the concepts of pyramid structure and employ regional to local attention instead of global self attention as in standard vision transformers. +ivesThe paper is well written and easy to follow. It is unclear how the regions and patches are novel. The overall accuracy of the model is good but not significantly better than the SotA approaches. The paper is well written and the experimental evaluation is comprehensive. However, the improvement in accuracy is not significant in comparison to the way the novelty of R2L attention is described.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; Authors introduced Model augmented Prioritized Experience Replay (MaPER) as a variation to the canonical prioritized sweeping. The main difference is that they extended the critic network on its last layer to also predict the reward and transition model. Then they changed the prioritization of sample for replay to also include the error in the reward and transition model, in addition to the TD error. Experimental results in various MuJoC domains showed the performance boost introduced by MaPER on top of existing state of the art techniques. Strengths + The paper is well written overall and is easy to digest. Why didn t the proposed approach work? I love to see that inside the paper. Provide a more holistic number across all domains for better sending the message out. For example, normalizing the return between 0 1 and then provide the average across all domains. Add line number to your Algorithm. I did not follow the priority set on line 5 of Algorithm. Can you elaborate ? Figure 2, Table 2, Figure 4 captions: "across five runs with randoms"  > "across five runs with random seeds". Please avoid reintroducing the acronyms. Any insights there? I liked this paper. The experimental results are comprehensive and I think it would be a great value to RL practitioners.<|endoftext|>It proposes Model augmented Prioritized Experience Replay (MaPER), a novel experience replay method, based on the intuition that the model is easier to estimate than Q value. It also proposes a modification to critic network, Model augmented Critic Network (MaCN), by predicting the reward and dynamics model additionally. Experiments on MaPER show that MaPER can be applied to both discrete action space (DQN) and continuous action space (SAC), both model free RL algorithms (SAC) and model based RL algorithms (MBPO), as well as sparse reward tasks, and improve the baseline algorithms a lot. Writing: In general, the writing is very clear. With that being said, I do have a few questions. 1.In Algorithm 1, The index set $\mathcal{I}$ is never used after initialization. 2.When are the priorities in $\mathcal{P_B}$​ updated? 3.In Eq (10), why are all losses weighted equally? 4.How is MaPER combined with MBPO? Experiments: The experiments are quite extensive. For example, we have two separated networks, one of which predicts Q and another predicts the next state and reward. This method has a larger computational overhead than vanilla PER, but can better decompose MaCN and MaPER. Does PER in the experiments (Fig 2, Fig 4) use (4) to stimulate exploration? If not, the comparison is not very fair. The idea of MaCN is pretty natural so it might exist in some prior work but I m not sure. Using model/reward prediction error for prioritized experience replay is indeed novel. Given the novelty of the proposed method and extensive experiments, I like this paper and would recommend acceptance.<|endoftext|>*Summary Of The PaperModel augmented Prioritized Experience ReplayIn this paper, the authors consider using the dynamics prediction error in the priority calculation. The priority is then used to decide the probability for each sample during training. The method is applicable to both model based and model free setting*Main ReviewPros:1. There’s a model free part that considers some of representative algorithms such as SAC, TD3,and there s a model based part that compares against MBPO. And experiments are implemented on both the continuous robotics tasks and atari games. 2.The details of the paper are adequate. 3.Sufficient related work. 4.Compared to the baselines, the authors show that the proposed algorithm has better performance. Adding the dynamics terms in the priority seems pretty random. It is not clear how sensitive the parameters in the algorithm are. For example $\xi_1$, $\xi_2$, $\xi_2$ in equation (5) are three variables i think that could be quite unstable and sensitive. It would be interesting to see if the algorithm only works on 1 set of hyper parameters, or it is in general a very robust algorithm*Summary Of The ReviewI am quite worried about the fact that the algorithm is looking quite simple. But the results are looking good.<|endoftext|>The paper proposes a new method for prioritizing experiences used in the prioritized experience replay (PER) method. The proposed approach is simple: the critic network also learns the reward function and transition function. The two errors (absolute value) are added to the absolute TD error to calculate the priorities in the PER method. The authors provide some intuitions to their method. The paper focuses on an important topic: what should be the sampling distribution for ER? 2.The presentation is reasonably clear. 3.The empirical results show that the proposed method is promising. Although the proposed method is supported by some intuitions, it does not have a solid motivation. In page 1, it is said “learning to predict … is difficult, so sampling based on TD error is far from optimal.” What do you mean by “short term”? And why sampling from TD error is far from optimal? I personally have some experience with it. For example, when your buffer only covers a small subset of the state space. You might compare the sample space coverages during the early learning stage between your algorithm and regular PER. The intuition is not persuasive. They can be quite different. In some environments, learning a model can be much more challenging (e.g., high dimensional), it is possible that the model error term always dominates the priority. Basically, it adds a model error term to the TD error. 4.The experiments are not sufficient. a.What is the parameter sweep for each algorithm? Do you ever try to use a separate NN for model learning (but still use the same priority as you defined)? This can help identify if the auxiliary tasks are useful or not. c. It would be interesting to see how the method works when increasing uncertainty in 1) reward function 2) transition function. I believe there is a category of work along this direction. Please see main review.
Reject; rating score: 3; rating score: 5; rating score: 8; This paper aims to achieve fast and high quality audio synthesis. This experimental setting is unfair. The effectiveness of the proposed method was evaluated on the Strings and VCTK datasets. My primary concern is that the technical novelty is limited or not well described. 3.The improvement of synthesis speed is included in the main contribution. The combination of GAN based waveform synthesis and multi band decomposition was also already introduced in [C]. 4.In Section 2, some related work is discussed. However, the discussion is minimal.<|endoftext|>The authors propose a model for audio generation, trained in two stages: 1) as a changed VAE with a likelihood evaluated by comparing the STFT representations and a temperature (beta) on the KL term in order to avoid posterior collapse, 2) as a GAN optimization scheme to fine tune the model. – Missing references to other literature using VAE and GAN in combination for training. – Eq.2 is only the real ELBO for beta 1. However, these samples are reconstructions. How can you make sure that after fine tuning the model, your KL term remains low so that your sample quality can be high? – In the article, you claim quantitative evaluation of the proposed model.<|endoftext|>This paper proposes an autoencoder for high quality waveform synthesis. The approach is based on multi band decomposition, VAE and adversarial fine tuning. The synthesis speed is substantially faster than existing approaches on both CPU and GPU. Weakness:It would be good to apply the proposed model as a vocoder in text to speech synthesis.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper presents a relabeling scheme for binary and multiclassclassification tasks in which harmful training examples identified byinfluence functions are relabeled to improve performance on a hold outset. The manuscript is well written and clear. The authors address the problem by proposing toreplace influence functions with training loss while retaining theirrelabeling scheme. I wonder how this simple strategy works incomparison to their RDIA approach (the appendix does not reportcomparisons in terms of accuracy), and under which conditions it couldfail (e.g.distribution shift rather than noise). impression that all you need is training loss +relabeling.<|endoftext|>This comment is more of an opening for a discussion with the authors, rather than "something to fix" . Thus, having a mismatch between training and test data. I think this is a good paper with merits to be included as part of the conference proceedings. The results of their experiments show that they are able to reduce the test loss compared to other data resampling approaches. I believe the paper is very well written and structured. All in all I think is a good paper. First, is not until the reader reaches section three that the reader realizes that the authors use influence analysis on the validation set rather than on the test set.<|endoftext|>The authors first derived the influence function for data relabeling as follows. While the target of our approach is to relabel the training bias towards better model performance. This result is coherent with the one previously reported in [Ref1] where a very similar data relabeling approach was studied. The strong aspect of this paper over [Ref1] is the experimental results where the results on DNNs (on MNIST and CIFAR10) are reported, while [Ref1] considered only kernel based models. * *I am not the author of [Ref1]. This is the optimization of the amount of relabeling so that the the validation loss to be minimized. I therefore decided to increase my score with a strong expectation to the authors for **referring [Ref1] appropriately in the main body of the paper**.<|endoftext|>While in the recent times there are papers on influence functions to identify harmful instances, this paper does a very good comprehensive and focused study compared to others. (2) The paper is well written and easy to follow. Related works is well laid out and well covered. This is an improvement from the earlier versions of the paper (from a previous conference). Cons:(1) The technique of using influence function for identifying harmful instances is not new and well known and applied in the recent times.
Reject; rating score: 3; rating score: 6; rating score: 6; This paper is about using BERT for the automatic resolution of merge conflicts. The input sequences are fed to BERT and the results are aggregated and used for classification. The problem of automatic merge resolution is compelling. The experimental evaluation is hard to follow because neither of the existing baselines addresses the exact same kind of merges. Overall, this may be a great software engineering contribution, but I m afraid that there s not a lot here for ICLR. In general, the paper would benefit from a more extensive ablation study. Similar experience reported in CodeBERT? It’s essential for a fair comparison.<|endoftext|>It s not obvious how BLEU 4 is a relevant metric. This paper addresses merge conflict resolution in source code repositories. What kind of solution does BLEU 4 allow you to score positively that would be genuinely acceptable for this task? This is an important problem, and I m excited to see more work in finding a neural solution to it. However, this submission seems more like a refinement of DeepMerge than a novel contribution on its own. It seems excessive to throw out the engineering that went into such non neural tools, if they can be used for something more sophisticated than o, a, b, oa, ob, etc. I don t understand the point of Section 8.2.<|endoftext|>This paper presents MergeBERT, a deep model for resolving program merge conflicts in software development. The model is pre trained on a large corpus of GitHub code. + Pre training can be applied directly. I have more detailed questions below. In that case, I don’t think merging `let` right away is an indisputable decision. 2.For this change from ` num_cores 2` to ` max_length 256  num_cores 2`, what is the result of the two level diff? Overall, the paper presents several new technical contributions and insights, but some parts are not entirely convincing.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; rating score: 6; ## ContributionThe tackled problem of out of distribution generalization for the forecasting of dynamical systems is relevant and valuable to the community, as motivated in the paper. I believe that this paper follows an interesting line of research and that further work could make it ready for publication at a next conference.<|endoftext|>This paper introduces a supervised disentanglement method to learn dynamical systems. pg4: check sentence structure of "being a state of the art model in long term video prediction,"pg5: "on three well studies dynamical systems,"pg9: "prediction is based both on them which"I have some reservations regarding the novelty of this work. However, the authors do not investigate how good the disentanglement is. I believe both quantitative and qualitative analysis of disentanglement would further strengthen the claims made in this paper. I would appreciate it if the authors could back it up with some empirical evidence. It is important to compare results with DSSM (even Kalman VAE) to see the true benefits of supervision. Have the authors tried Gaussian distribution (correspondingly L2 loss) for the decoder?<|endoftext|>The authors conducted experiments on simulated datasets and showed good performance for OOD cases and long term predictions. I think a deeper analysis on disentangle representations is not out of scope and is necessary because the paper heavily relied on VAEs for disentanglement learning. It is difficult to identify the differences between the lines in the current version because they are largely overlapped. If so, please modify the reconstruction loss in page 4 to accurately show the prediction loss. I thus find it difficult to argue for acceptance of the work.<|endoftext|>The paper studies the performance of dynamical systems learned from data with a focus on out of distribution (OOD) evaluations. Authors carry out experiments on several dynamical systems: pendulum, Lotka Volterra system and three body problem. Authors found that additional disentanglement can improve generalization performance of the models and in video prediction setting leads to better long term predictions based on structural and perceptual image metrics. The suggested experimental evaluation is sound, but current results seem a little marginal. With additional results/modifications I believe this work could be useful to a wider audience, but my initial rating is marginally below the threshold.<|endoftext|>This paper identifies two issues for developing dynamical VAEs, i) out of distribution generalisationii) long term trajectory predictionThe main contribution is to address the aforementioned issues using a supervised loss defined between latent variables and domain parameters. Empirical experiments on three problems: LV, video pendulum and the three body problem. Demonstrate long term trajectory prediction and OOD on an easy and a hard task. I find it is an interesting work. However, I strongly feel the authors have not adequately demonstrated the benefit of disentanglement and are missing comparison. It would help if the authors could elaborate the sentence here. If it is OOD generalisation, authors should at least show this as a limitation in existing approaches.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The higher the dimension is, the more accurate the test is, yet the higher the computation costs. 2.It provides the relationship of the reduced dimension with the test accuracy and the computation cost. Could you also include the original chi square test in this case? 4.Theoretically, do we know if the FED chi square test statistics is an unbiased estimation given the original chi square statistics, or systematically larger or smaller? 5.As a reader not in the expertise of stable random projection and geometric mean estimator, I don t fully understand why the lower dimensional $e_k$ gets to the approximation.<|endoftext|>The paper proposed a federated $\chi^{2}$ test protocol. Weakness:1) I could not find any plausible proof or development of the theoretical claims. However, technical concerns remain and the paper does not seem to be well developed and is hard to read.<|endoftext|>Paper proposes chi square test in the federated setting where data sharing is not possible. Paper proposes Fed Chi square,a  secure chi square test in the federated setting. Overall, I like the paper, I have couple questions for the authors:1. 2.Also, how does this compare to standard pooling of test statistics? It is a promising paper, I have couple minor questions.<|endoftext|>This paper introduces a federated analytics technique for computing the Pearson correlation for two random variables. The protocol is argued to be secure in a semi honest security model. Each participant applies a stable random projection (with particular parameters) on their (normalized) frequency distribution. This paper presents a nice new federated analytics technique for computing correlations. I have some concerns about the security of the protocol, but it s possible I m misunderstanding something.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; Dropping the de refinement in AMR algorithm makes it much less competitive compared to state of the art. Authors recognize that the process of AMR, refinement at each step, can be formulated a Markov decision process (MDP) and hence utilize reinforcement learning (RL) to train refinement policies directly from simulation. Three test cases are used for experiments (static, advection and Burger) and authors compare all three architectures with each other as well as some traditional AMR methods to demonstrate the performance of the proposed method. This is a bit strong statement. Discussion on the solution space and mathematical definitions: (major) The finite element requires a bit of discussion on the mathematical formalism. Since, tuning the parameters is also part of RL and all three architectures used, I think this part should be removed, which brings me to another discussion. If this is not necessary, then authors should clarify. Such approaches are also very scalable, which based solely on the results of this paper, it s hard to claim. Discussion on the algorithm:(minor) From Fig 5 d, and the manuscript, I d say authors use non conformal elements, which is fine; however, how to enforce conforming mesh?<|endoftext|>This paper presented an RL based method to perform adaptive mesh refinement to solve PDEs more accurately. The problem of mesh refinement is critical in engineering, and current practices, as the authors stated, are often designed from some heuristics. The application of RL to this mesh refinement, as far as I know, is novel, promising, and may provide new ideas to solving this open problem. The authors compared their policy with some trivial policy but did not compare with the state of the art in adaptive mesh refinement. Hence it is not convincing that the RL based method may outperform the traditional "heuristic" methods on this problem. The paper proposed a promising direction for future research, but it would be more convincing if the authors tested their method on practical scales with more solid validations.<|endoftext|>However, the practical implementation of this simple becomes more convoluted, and is one of the weak points of the paper. Some additional questions and comments:  Even though the authors state it should be simple to implement, the policy networks in this work do not have a "coarsen" or "de refine" action. Would this lead to an overall very fine mesh? It was also unclear to me how the convolutions are applied here. It would be good to improve this description. Overall, given the concerns above, I do not feel like this work is ready for acceptance at this point. This is a work that rests mostly on its empirical results, and yet these are not completely robust at this point, given the issues described previously. As such, I am classifying as marginally below acceptance for now. Nevertheless, I am interested in reading the authors responses to the points raised above.<|endoftext|>The current study formulates adaptive mesh refinement (AMR) as a Markov decision process and applies deep reinforcement learning (RL) to train refinement policies directly from simulation. Q1."Nonetheless, to our knowledge, such variation in state action spaces does not occur in any existing RL application." This comment is not valid. There are many RL formulations and applications that can generalize over a varying dimension of states and actions. For example, many RL approaches seek to solve the solution of various optimization problems formulated as graphs. These approaches train the policy using RL with the small sized problem and then apply trained policy to solve large scale unseen problems. Is there any reason or justification for using this action definition? Q3.Why is the simple policy gradient algorithm used? For example, state representation and the representation learning, specific reward form, or specific learning strategy? Formulating AMR as an MDP sounds reasonable, and structuring meshing policy using the graph neural network is also straightforward.<|endoftext|>The authors suggest to formulate the mesh refinement problem as a MDP and propose different policy architectures for scalable application of reinforcement learning. Dynamical mesh refinement is an important feature in numerical simulation of PDEs. It s hard to understand how transition is computed and defines the dynamics based on only short description. It be helpful if the authors consider providing more description of AMR background. However I could not find where in the paper the policy is designed to capture time dependency and the change of state and action sizes. 4.The experiments are interesting which can show the benefit of the proposed approach. I have only one point that is hard to follow. What is the implication of this baseline? Does it mean the metrics used to define rewards is not optimal? Though I found the proposed idea of using RL for AMR is interesting, the technical contribution is still limited. Therefore I like to keep my original rating.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; I think that the lack of empirical results and experiments is justified by the theoretical contribution. Finally, a result is presented for the case of stochastic mechanisms which give rise to conditional transition distributions $p(z_{t+1}|z_t)$: here, equivariance and imitation are relaxed to hold only in distribution. The emphasis on non iid observations related through some shared latent dynamics shares a common ground with many of the recent results in the literature (discussed in section 5) which makes it conceptually appealing. The lack of independence assumptions is also refreshing and makes this work of potential interest for causal representation learning. The paper does not present any empirical results or experimental analysis.<|endoftext|>Extensions are given to the case of a known mechanism class and to stochastic mechanisms. + The examples that are given, for example, the linear mechanism setting and the equivariances of affine mechanisms, are very helpful for understanding. This aspect is lost here since we either assume a known mechanism or assume a class of possible mechanisms but do not try to identify the mechanism. It was not clear to me if similar results could be obtained if the observations are observed with additive noise. I think this is an exciting direction of work that runs orthogonally to a lot of the recent advances in disentanglement, by assuming knowledge of the underlying mechanism rather than independence and distribution assumptions on the latent variables. Presentation of an algorithm or some basic experiments to justify the feasibility of the new setting would substantially strengthen the paper.<|endoftext|>The paper uses the knowledge of generating mechanisms to identify latent representation for given data. While some of the results are intuitively mentioned in prior empirical work and may be obvious (i.e., the latents are unidentifiable upto scaling constant), I think the paper extends it to provide a strong result when there are multiple mechanisms at play: in that case, identification may become easier (cor 1). My main feedback is on the relevance of this formulation to representation learning tasks:1. What about natural language? Good theoretical insights, can have better connection to practice
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; This work proposes a framework for interpreting model behaviour by generating language descriptions of neurons in the model. The method is trained to maximize mutual information between the description and input examples that activate the neuron. How do image captioning models do on the task of generating descriptions conditioned on inputs?<|endoftext|>The authors introduced MILAN, for mutual information guided linguistic annotation of neurons) that automatically labels neurons with open ended, compositional, natural language descriptions. This is done by searching for descriptions that maximize point wise mutual information with the image regions in which the neurons are active. * Section 4: MILAN obtained higher agreement with human annotations on held out networks than baseline. The paper is about visualization and explainability. If there is more, I would like to have better characterization of the limits of the approach.<|endoftext|>This paper describes a novel procedure (MILAN) to interpret deep learning models for computer vision by generating natural language description that specifies the activation selectivity of a given neuron in the model. For this aim, they first define an exemplar set of input image regions for each neuron by thresholding its activation value. Then they search a natural language description by optimizing the point wise mutual information between descriptions and the exemplar set. The proposed method is concise, straightforward, and well motivated.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 3; This paper concerns the idea of directly learning a model of a belief state MDP from samples from the ground truth dynamics and observation models. One could then use that model for decision time planning (e.g.some form of tree search). In particular, the paper aims to improve upon this idea by fine tuning the belief state model before search using samples from local state region. In particular:  The RLSearch paper has been accepted for presentation at NeurIPS 2021. Original Summary   The paper is well written and considers an important and challenging problem. That said, in my opinion, the empirical results are both novel and significant. All that said, I do have a significant concern about this paper.<|endoftext|>This paper proposes a strategy for improving a trained parametric model for belief state update, by generating and training on new data, online. This fine tuning strategy is shown both to improve the fidelity of belief state updates and to have an ultimate impact in the quality of play in Hanabi, a game with a substantial amount of private information. Not given an observation, it s some sort of expectation over the next belief (this is tricky, I think.) The discussion of the sampling procedure (in that same section) says it produces samples of X^{t+1} but it seems to be important that we are getting samples of the Y_{t+i}, as well, so we can use them to fine tune our estimator. A bit more detail about that would have been helpful in understanding your approach.<|endoftext|>This paper seeks to understand whether finetuning a learned belief state model can enable improvements in performance in partially observable problems, in particular in the Hanabi domain. The problem of learning and optimizing over belief states is an important problem in reinforcement learning, and while the ideas in this paper are not particularly novel, the core idea is implemented and tested well. The paper is easy to read, and exposes the main concepts well. Along the same lines, I think it would be useful to more explicitly outline in the appendix exactly how BFT is used for down stream decision time planning (e.g.with RL Search). How sensitive is BFT to the number of gradient steps that are taken to fine tune the learned model? As a question to the authors, how would these techniques transfer to the setting where these assumptions do not hold (in particular, where the underlying state space $\mathcal{X}$ is not known). I think the point that the method works because it "refocuses" the capacity of the parametric belief state model is interesting, but I don t think it is supported very strongly by the provided data. Along these lines, I think it would be interesting to have some discussion as to whether fine tuning procedures can simply be supplanted by training bigger networks.<|endoftext|>The authors propose a new method for calculating distributions over states in Markov problems in which the true state is not directly observable. The method relies on using an existing model and then fine tuning the results online at each time step. Experiments presented show this gives a more accurate distribution, and that this more accurate distribution leads to statistically significant improvements in performance. While the authors state that they present an algorithm, it might be better to view this as a framework, as the method involves an externally supplied model as an input, and the fine tuning method needs to be tailored to this model. The experimental results are only provided for a single domain, albeit for a HMM, POMDP and FOSG setting in the same model. These criticisms aside, the experiments appear to be fairly conducted and reported, and look at sensible aspects of the problem, with a good choice of baselines/competitors. The authors present an interesting idea well. However, the experimentation is very limited (even for a conference paper) and the results given show a very small effect size.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; It is strange as, for instance, linear models are somewhat expressive, and well trainable, though their variability would be 0. Empirical correlations in the 2D case between depth and that measure (for a given number of free parameters) does not mean the "activation ratio" is a meaningful measure. The parametrization of weight matrices by Householder matrices is interesting, although the experiments are quite preliminary. I do not think this paper should be accepted.<|endoftext|>**Strengths**  Notion of variability is novel. Since these are the basic issues the paper is dealing with, one would expect simple and intuitive definitions in the introduction. As such a trade off exists, and that has not been explored in this work. Is the additional depth necessary? it would be much more costly to evaluate an exponential unit than a ReLU. Is there any explanation for this behaviour?<|endoftext|>It seems that it is understandable but a more accurate definition is needed. That does not specify the form of distribution. The motivation for the proposed _Han layer_ seems to be based only on the nice behavior of equation 4 resented in Figure 2. The motivation (__The purpose of this work is to gain more insights into the behaviors of DNNs and then use them to build new DNN models.__) provided in the work is nice. However,  my feelings are that the paper delivers on the opposite. However, all the parts of the work feel half baked.<|endoftext|>[Strengths]  This paper is very well written and easy to follow. The synthetic experiments are well designed and visualized, and the corresponding results and conclusions are interesting. Moreover, as stated by the authors, it is not the case that the higher variability the better. For example, keeping the number of activations to be the same, then reducing network parameters would improve AR. Currently, Figure 1 shows that HanNet is a solution to this problem. 3.On the image classification datasets  The datasets and models are too small to shed light on the effectiveness of the proposed method in the more practical scenarios.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The learned activations are obtained by a weighted sum of nonlinear activations of hidden layer outputs and the weights are computed from softmax of the projected speaker embeddings such as x vectors. Experimental results show that in terms of SDR improvement, the proposed method achieves comparable performance to the baseline models with a smaller model size. Some details are not well described. mean?Even though the model size is smaller, it comes with additional on the fly computation of several non linearities of hidden activations, which may cause other concerns for on device applications. The paper provides a personalization method by conditioning non linear activations based on the speaker embeddings.<|endoftext|>In literature a common way to condition a neural with an input would be to either concatenate the conditioning vector to the input vector, or inject it before several layers (modulation approach in Figure 1 b). In this paper they instead propose to pass the conditioning vector through weighted sum of output of neural net activation functions. The claim is that this way of training the neural network leads to reduction in neural network parameters without sacrificing performance in speech enhancement / ASR tasks. The authors argue that this is an important advantage for low resource computing. I have several questions on this. I have reviewed this paper before, and I see that some of my concerns from earlier, even though were partially adressed in the rebuttal, now seems not to be addressed in the current version of the manuscript.<|endoftext|>This paper presents a very interesting idea (if I understand it correctly), which is to implement conditioning on e.g.a speaker embedding vector via a parametrization of the layer to layer activation functions of an overall deep neural net architecture. A number of references are given for the concatenation approach, which is good. I am guessing that the formalism is correct but the presentation could be substantially simplified and clarified. That immediately would clarify the fundamental concept   and highlight it s originality.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors show that Sine based (SIREN) non linearity is best suited for the kernel generating MLP, and an experimental evaluation is performed. The paper is well written, and the method s limitations are outlined clearly  Great evaluation of how a SIREN based parametrization is best suited for realizing the MLP. **Questions**:  How was the sequence reduced to a single prediction in the sequence classification tasks (sMNIST, etc.)?<|endoftext|>The submission is also clear and well written. This contribution adapts to sequential data the paradigm of continuousconvolution kernels in convolutional neural networks (CNNs). One point that could be further clarified is the choice of using a single MLPfor modelling the Nin x Nout convolution kernels. I found the concept of continuous kernel interesting.<|endoftext|>I am in favour of the paper mainly due to the compelling empirical performance, which should be of interest to the community, although some additional baselines and experiments could have been included. The paper introduces convolutions with continuously parametrized kernels for sequential data. The main idea, which is to represent the convolution kernel implicitly via parametrizing it by a neural network, is not novel in the literature, but it has not been applied previously for sequential data modelling as far as I know. This is also shown in Table 5, where the performance degrades significantly if there is an order of magnitude difference between training and testing resolutions.<|endoftext|>A potential weakness of the paper is it s limited set of experiments: Compared to the  baseline  paper Bai 2018a, several datasets are missing, and, given the progression of time, several larger scale datasets e.g.from speech or the financial domain could be added. The described idea is a nice transfer from existing work of implicit neural network representations and continuous kernel formulation. This is a nice paper with one clear idea, which is demonstrated to work well. The paper considers convolutional networks for sequence processing and suggests to parameterize convolutional kernels in this setting by small neural networks.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; + Figure 3 shows in the top (deep) layers the augmentations are not dataset specific and therefore justify its motivation. Could the author[s] provide some intuition on why using gradient matching for the search. If it will cause overfitting issues on training data? Showing extra results on some transfer learning experiments on different datasets will be highly appreciated. Experiments can be further expanded to different datasets to show the effectiveness of the proposed method. Can also add more comparisons to the previous methods in Fig.3/4/5.<|endoftext|>The proposed method is able to search for the augmentation policy with multiple layers by searching the next transformation operation conditioned on all previous augmentations. Except for that, based on the experimental results that (1) the proposed method reduces the policy search time by a large margin while maintaining good performance compared to other SoTA augmentation methods, and that (2) the proposed method is applicable to multi layer augmentation, overall the reviewer considers it a good paper. Even though the authors provide the similarity scores after stacking more augmentation layers in Tab6, the comparison of the cosine similarity of gradient between the proposed method and other augmentation methods is missing.<|endoftext|>I tend to give a rejection for now. The formulation of gradient matching brings something new to this area. *After rebuttal: my main concerns are well addressed, hence I recommend to accept this work. The authors should add the comparisons to the paper for correctness. Besides, the manuscript can be improved by providing more discussion and analysis. **C4.** **[unclear experimental details]** I got confused with some details and have some questions regarding them:      **C4.1. If it s true, why not to use more training samples to further boost the policy?<|endoftext|>That could be done by applying RA with different numbers of layers or applying TA multiple times. Very minor: there are some typos here and there. I do believe, though, that the results simply are not strong enough for publication and a few comparisons in the paper are unfair to baselines.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 5; This paper explores the relationship between the low churn problem and distillation. The authors show that there is an equivalence between those two methods and that distillation performs particularly well on low churn dataset tasks. The authors propose a novel churn reduction algorithm based on distillation which involves the training of a classifier by minimizing a distilled loss and solving a convex program. Strengths  The authors provide theoretical guarantees to justify the use of their algorithm compared to comparable approaches  The authors provide empirical justifications of the proposed algorithm using many datasets and baselines Weaknesses 	I was not able to find a major weakness in this paperTypos 	5.2 : « performs the best » or « performs well » probablyThis paper provides strong theoretical and empirical results regarding the effectiveness of the proposed distillation based algorithm for the churn reduction problem.<|endoftext|>Thus for certain class of problems, there is a need to control the churn or the difference in the predictive model due to retraining of the model towards a more robust pipeline. The current paper introduces this technique as a simpler solution for a problem of interest, especially in production settings. The paper can be improved upon by addressing a few points as below:  The authors presented the claim that their process corresponds to `a`  value of $\lambda$ that constrains the churn in the primal setup. The presented results also support the efficacy of the methods. Overall this paper is a good example of an ICLR paper.<|endoftext|>[Main Strengths] This paper s main strength is that the authors  lengthy proof of the performance guarantee (not sure how tight the bound is, though), and the key implementation codes are provided in the Appendix. I encourage authors to make additional improvements, particularly for the experiments described in Section 5, which is to show that how the distillation handles the distribution (or domain) shift issues, which will be highly intriguing and will definitely necessitate more detailed experiment settings. The use of distillation for churn prediction is intriguing, but it does not appear to make a major difference, similar to the many flavors of domain adaptation.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes an approach to learning intrinsically motivated options that is combined with an extrinsically trained policy. W1.2 The final set of intrinsic rewards used in the main method is unclear. For instance [A] can be considered a method for learning intrinsically motivated options and then planning over them to achieve extrinsic rewards. The main paper needs to be clearer about this point. Because the focus of the paper is mainly on demonstrating the empirical utility of the proposed method, significant improvements are needed to make the paper acceptable.<|endoftext|>If semi MDPs are not rich enough to describe these ideas then what is missing? The paper already needs to be more clearer written and framed w.r.t a rich literature on semi MDPs and hierarchical RL. 3.Since the key idea is to learn options, it would have been interesting and important to visually inspect the type of intrinsic options that are learned by this approach. The authors provide a partial answer via Figure 3 in the appendix but this does not clearly indicate the exploration abstract learned by this approach4.<|endoftext|>This paper introduces an extension of Explore Options (EO; Bagot et al.2020) to the non linear function approximation case. Importantly, operating at different time scales seem to be an important ability that we should not prevent our agents to have with a fixed c_switch value. What is the difference to experience, for example? I don’t see data to rule this out. I elaborate on these topics below. First and foremost, I don’t think we have data to quantify the impact of several components discussed in the paper.<|endoftext|>The authors build on previous work in learning exploration options, extending it to the setting of function approximation and learning from image based observations. Thus, I recommend the current version of the paper be rejected. In practice, which should be used when the policy is deployed? It seems counter to the idea that separating IM is important for exploration.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Post rebuttal comments  Thank you for engaging in the process, I still believe that this is a good paper. Experimental results corroborate the proofs and propositions, and highlight the value of the proposed approach. Strengths:This looks to be a strong contribution, attempting to solve an important problem. The paper is well written and relatively easy to follow, and results and proofs are interesting. I would value some discussion on these limitations   would it make more sense to optimise a different distance metric, or to use a different eg.<|endoftext|>Given an imitation domain and an expert domain with example trajectories, a pseudo reward is computed based on the degree to which the distances from a state to its neighbors in the imitation domain,are preserved in the expert domain. It would be  beneficial to see these learning curves, and a wall clock compute time comparison. I had to go elsewhere to obtain the intuition. Most of the readers may simply think thereis an editing mistake and that the same figure was included 8 times. The paper introduces a novel idea for imitation learning.<|endoftext|>More experiment results are preferred to show the effectiveness of the proposed solution. This paper introduces and addresses an important and general cross domain imitation learning problem. It would be better to give a link showing the results in animation. The authors also well justified limitations theoretically.<|endoftext|>This paper presents a method to transfer policies between different MDPs based on the minimization of Gromov Wasserstein distance. This distance provides a pseudo reward that can be used to learn via RL the optimal policy in the target MDP given an optimal policy in the original MDP. The images of the first case (Fig.3) are hard to see. There is no numerical performance (success, reward, some degree of progress). Add more experiments, numerical performance, different metrics…In summary, while the paper presents an interesting turn on previously presented ideas, and the mathematical foundation is well worked out, the experimental evaluation is insufficient to support conclusions about the method.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; The paper studies the linear similarity matching problem, which represents input $x_t$ by a high dimensional vector $y_t$ such that $y_s \cdot y_t \sim f(x_s, x_t)$ for a kernel function $f$. Main contributions of the paper:  The authors propose an online algorithm for linear similarity matching, which can be interpreted as a neural network with Hebbian and anti Hebbian connections. I like the authors  idea to use an online neural algorithm to approximate nonlinear kernel similarity. In the experiments there is no comparison to demonstrate the advantages of the proposed algorithm over other algorithms. There is a comparison with the kernel PCA on the Half Moon dataset, but the finding is that the proposed method s performance is not much worse compared to the kernel PCA. Score after revision  Thanks the authors for adding the performance comparisons with other methods. However, I want to keep my original score, since there is some strange behavior of the proposed method in Fig 5 (as also mentioned by the authors): the error turns to increases with the dimensionality when the dimension is large.<|endoftext|>The paper describes an online stochastic optimization of a neural network to learn an embedding such that the inner product between embedded points approximates a non linear kernel based similarity. This can be a radial basis function network that approximates the embedding of a Gaussian kernel. Experimental results show that the method is able to learn meaningful representations. Strengths: The problem is interesting and follows a long line of kernel methods and scalable approximations. Yet no comparisons are made to baseline the approach in terms of Nyström landmark selection algorithms such as Musco and Musco s approach "Recursive Sampling for the Nyström Method" NIPS 2017. In particular, the statement "we start with a different objective" doesn t make it clear that this paper uses the same cost as the Nyström approach, but that Fu (2014) used a different objective, which is functional approximation in the RKHS. What is meant precisely by "nearly" convex concave, and is it due to the nature of the Legendre transform? page 4, "gaurantee"page 5, "Hebbian rules are been defined" page 5, the number of training iterations could be converted to epochs? Key comparisons versus other scalable techniques Nyström are not made, perhaps to focus on the online nature.<|endoftext|>This paper considers the problem of finding a low rank approximation to a kernel matrix. In the low rank approximation problem, one would want to find a low rank matrix such that the Frobenius distance between the low rank matrix and the original matrix is minimized. The paper shows that this optimization problem can be upper bounded by the cost function of a Hebbian neural network with a recurrent layer. The paper sells the result as an online algorithm for finding a low rank approximation to a kernel matrix which is not quite the right term for their algorithm. Basically, the paper presents an algorithm but it does not analyze the algorithm in terms of convergence, generalization, or runtime. I generally like the idea of the paper and think it is novel to express the low rank approximation problem in terms of a neural network but I have the following concerns about the presentation and contributions of this work.<|endoftext|>This paper studies the problem of feature mapping for reproducing kernels. The focus is when the data size is too large, the SVD for the kernel Gram matrix would be prohibitively difficult. This paper proposes to use the least squares loss to achieve a low rank approximation to the Gram matrix, to use Legendre transformation to disentangle quadratic terms in the least squares loss, and to use a two phase Hebbian parameter update rule to achieve the approximating feature map. Users of the algorithm may not be able to estimate the introduced error. 3.Some minor mistakes in using the Legendre transformation: the sum in t on the right hand side of (4) does not cover (1/2)q^2f(w,w). However, we see that the author(s) made it cover this term in (5) and (6). A.Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev.53 (2011), no. (these algorithms could be plugged into kernel based empirical feature maps to achieve the same purpose as claimed in this paper), the convergence and precision of the proposed algorithm are both difficult to verify.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper proposes a method for generating curriculum for goal conditioned RL policies based on having one agent hierarchically generate sub goals, and have the target agent train from rollouts on each level of this hierarchy, while a third agent adversarially modifies the environment to improve robustness and transfer. The main contributions are introducing a mutual training regime between the path planer and the RL agent, and introducing adversarial environment design to training in each of the sub goals. The one thing that makes this papers approach distinct from this direct combination is that the adversarial design is done independently for every sub goal. One concern is that SAC by itself beats some of the baselines, which makes it seem like the baselines may not be configured properly. In particular, POET has environment dependent parameters which need to be tuned, so it is not obvious to me how this baseline works, or how it was ensured that these parameters were set correctly. In particular, the method used for the environment designer in this work appears to correspond to the "maximin adversary" in the second paper, which was used as a baseline. I will be weakly recommending rejection since it is unclear to what extent the method works because of the integration of HRL and adversarial environment design, and to what extent the same results could be achieved through the simpler direct combination of the existing approaches in both fields.<|endoftext|>To this end, EAT C trains two policies, in addition to the main decision making policy. The authors show empirically that EAT C leads to efficient training of the policy and superior generalization capabilities compared to existing baselines. Thorough ablation studies are also performed. Although the results look good on the two environments provided by the authors, providing additional experiments using more challenging benchmarks, such as 3D continuous control tasks, would strengthen the paper. Second, certain details (e.g.Algorithm pseudocode, model architecture, etc) can be moved to the appendix to make space for a Conclusion and Future work Section, which is noticeably missing. Sub goal trees   a framework for goal based reinforcement learning. However, further experiments on more challenging domains would strengthen the paper.<|endoftext|>I am willing to raise my score if these problems are addressed in the revision. E.g., I could not find information about how the environment generation was implemented (e.g., action space). 3.The conclusion section is missing. It would be good to summarize the main contributions and findings at the end of the paper. In ICLR.(Autocurricula emerged from adversarial multi agent RL training.) However, the writing needs improvement (a lack of details, missing conclusion, etc.). There should also be a discussion on the limit of the current algorithm, which relies on the performance of a specific plan planning method that provides the true structure.<|endoftext|>The paper proposes an RL method targeted at long horizon tasks in environments with perturbed dynamics or configurations. My main questions/critiques revolve around the experimental section. I can imagine that some of them may have been already addressed in the appendix that has been mentioned in the main paper but seems to be missing in the current revision:* As I already mentioned, I think that the authors are in general objective in their assessments of the novelty of their method. Unfortunately, details regarding this parameterization were not included in the main paper. Could the authors clarify on the environment parameterization for the curriculum RL methods? * Another point of confusion is the very poor performance of the hierarchical RL baseline, which is unexpected given the clear benefit that a reader would expected from hierarchical RL in long horizon tasks. Hence it would be important to carefully specify how the environment randomization is perceived by the agent. * A more "realistic" experiment could further improve the paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; However, as $Pr. b) Only PGD 10 $l_\infty$ $8/255$ attacks are evaluated in this paper. I would rate the paper as: score 4 weakly rejectedI will consider to improve the score if the authors can address my concerns. It would also be better if the authors can evaluate SRAT on other types of attacks listed in [a].<|endoftext|>I find the premise of this paper, and the resulting observations about the effect of dataset imbalance on AT, interesting. My main concerns with the paper are as follows:(i) Significance of the findings:   The claims in theorem 3.1 and 3.2 do not seem too surprising to me. While the premise of this paper is interesting, I believe that both the motivation for the SRAT method, and its empirical validation could be substantially improved. ## Post rebuttal UpdateI thank the authors for their response to my comments. I have decided to keep my original score as some of my concerns still hold. However, this seems not to be the case as per Table 1 and 2.<|endoftext|>Two modifications in adversarial training are proposed to ameliorate that: a) a weighted average of the loss per sample (depending on the class this sample belongs into), b) an additional loss that aims in maximizing the class separation. The experimental setup seems rather weak for an empirical paper. The motivation for studying the imbalanced datasets and their adversarial training is clear.<|endoftext|>2.The experiments are conducted comprehensively on various imbalanced datasets and the efficacy of SRAT is justified by the improved performance on overall classes and under represented classes. Further, the authors theoretically analyze the reason for the above observations on a binary classification case. [1] Improving Adversarial Robustness via Channel wise Activation Suppressing, ICLR 2021[2] Large Margin Softmax Loss for Convolutional Neural Networks, ICML 2016Overall, this paper firstly investigates adversarial training in a new scenario under imbalanced datasets and propose to improve feature separability for adversarial training that should enhance the performance on imbalanced datasets.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 1; The paper proposes using separable bijective networks   that is, a two stage network where the first stage is bijective (data flow either way) and the second stage is separable (multiplicative or additive)   to make it practical to include integrals (on the input) as performance goals. The paper is clearly written and relatively easy to follow. The method combines known methodologies in a relatively straightforward but novel manner to achieve the desired goal, which is well motivated. The exposition is clear and the experimental section covers a good number of compelling applications for the method. One potential issue with this paper is that the method described simply combines known approaches.<|endoftext|>This integration technique is applied to OOD detection tasks. The paper is clearly written and well motivated. I think that integration over a continuous region   instead of the more heuristic approach of random perturbation or augmentation   is a novel and more principled strategy that enhances the robustness of models. For the purpose of tractable integration, using neural networks to parametrize separable functions is a natural approach. Based on Section 1 6 of the paper, my impression is that the authors aim to keep the method general purpose. Indeed, when reviewing relevant work on OOD in Section 6, the authors mentioned that "These methods are constructed specifically for OOD detection whereas our method could be applied to a variety of problems based on the continuous regularizers chosen." For this reason, I find it difficult to assess the significance of the contribution of this work purely based on its performance on OOD detection. To summarize, I think the current paper put forward an interesting idea (perform computationally tractable integration using separable functions parametrized by neural nets). Thanks to its generality, the proposed integral method may be useful for other ML applications, which I encourage the authors to demonstrate.<|endoftext|>This paper introduces a hybrid model architecture that makes it possible to integrate a separable loss function across a region of input space. Such integrals can be used as regularizers for robustness near the observed data points (local consistency), and out of distribution (OOD) detection in neighborhoods away from the observations. The method uses a bijective flow network to map dependent input features to a set of independent latent variables. It then applies a separable function on the latent variables to get a loss function (or an approximation). The empirical results are mixed. Note: I was a reviewer to an earlier version of this work. Originality and significance: The combination of flow networks with separable functions to form tractable integrals is a novel idea (to the best of my knowledge), with potential use in other ML domains. The experimental section is lacking details in the main body of the paper.<|endoftext|>For input output pair $(x,y)$, the paper undertakes the task of estimating (*) $E_{x \sim p(x)} \Omega(\hat y(x))$. This paper suggests there is a better way. The main idea is to recognize that if $f$ is a separable function and $h$ is a bijective function, then (**) $E_{x \sim p(x)} f(h(x))$ is easy to evaluate. In other words, it is difficult to discern the relevance of (**) when the goal is to estimate (*). The best we can do with this (\***) assumption is to derive an upper bound on (\*). Give me MC error over a potentially very un tight bound  any day. It is not clear that it’s a worthwhile to restrict a neural network classifier to be of the form (***) only to arrive at the bound in Equation D8 (which is same as Equation 13 in the main text). Although I like the idea that we should pay more attention to integration error, I don t see how we can get around it in ML/DL. The proposed approach of assuming (\***) is certainly not the way. Furthermore, the function $\Omega$ in (\*) that would be encountered in ML/DL cannot be accounted for in the proposed framework unless some type of bound is in D8 is employed.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes an approach to measure to quantify generalization properties (state generalization, observation generalization, action generalization) in single task RL in the context of offline RL. The paper discusses the limitations of several existing approaches for measuring cross environment generalization, then presents their generic measure of generalization (Equation 4), and evaluates generalization when learning from offline data using a DQN and QR DQN in a contextual decision process (CDP) problem created out of MNIST classification. The results suggest that dropout is effective for action generalization, not state generalization, and L2 penalty is effective, and QR DQN can generalize better than DQN in the offline setting, but worse in terms state generalization. However, I think the paper falls short of delivering on and convincing the reader that the empirical analysis is useful and generalizable, and the proposed metric is a good one. It is unclear how these observations transfer to other RL problems. With that in mind, the performance of RL agents is worse, so one argument is that everything is generalizing poorly on an absolute scale. Comparing with such metrics should have been more informative, compared to just DQN and QR DQN, and this is a limitation. 3.In the offline setting, it is known that better algorithms that are more uncertainty aware and do not commit to a single target value, such as ensemble DQN, random ensemble mixture, QR DQN, etc work much better compared to the DQN, because they can be more accurate on OOD predictions. 4.What insights should I take away as to why one algorithm is worse than the other in each of the different settings? A detailed empirical analysis of what about QR DQN and what about DQN makes them perform differently in these different generalization challenges would shed light on this. Similarly, a rigorous empirical study of why dropout helps or why L2 helps, etc, seems necessary to me to take away from this paper. Without presenting any analysis of the **why** question, it seems that the work is not complete in my opinion.<|endoftext|>This paper proposes an empirical evaluation method to measure of the generalization capacity of an RL agent. It relies on CDPs combining a tabular environment with a supervised learning dataset. Generalization is measured across three axis: state space, observation space and action space. I think the ideas in this paper are presented clearly and it is globally well written. I like the distinction the authors made to distinguish generalization across their three axis is interesting and although they have been looked at separately in the literature, their juxtaposition and evaluation procedure in these different regimes are as far as I know novel. “While generalization of state value is similar to regression, generalization with quantities related to action, such as policy or action value, do not have supervised learning analogues and hence require separate consideration. “ could you add more justification for this please? I think the paper would be stronger by providing results for other agents and more complex environments which is why I recommend a weak reject.<|endoftext|>The key contribution of the paper, from my understanding, is that the authors argue that different from generalization in SL, in RL state, observation and action should be considered separately. A measurement (Eq.4) is proposed to evaluate generalization capacity of deep RL within the contectual decision process (CDP) scheme. Experiments were performed on grid world environments with MNIST image as observations and several results were concluded. Strength:  The paper discussed an important problem in RL. The plots are well presented. This assumption restricts the scope of this study to discrete and relatively small state space. Since the results are mostly empirical, it remains unclear how these results generalize to more practical and real world RL tasks. Either the concept of CDP or using performance gap between train and test set as evaluation criteria is not original. Although the experimental results are clear and easy to understand, I expect a more comprehensive empirical investigation, e.g.,  visualizing the learned policy in your envrionments (as in Cobbe et al, 2018), and perform experiments in more tasks. In chap 4, before 4.1, it was written "some have reported that these techniques improve generalization.".<|endoftext|>This paper proposes a new methodology to test the generalisation performance in reinforcement learning. Authors propose a new perspective to evaluate generalisation in RL agents based not on accumulated rewards as most research usually do now, but through a careful evaluation of the generalisation performance of the different elements of an MDP. Still, this work presents a new empirical formulation to better study generalisation. My main concern is that this paper is not the first pointing to change in this manner the way we evaluate generalisation and, the empirical method prosed here is very limited, in the sense that it requires full knowledge of the state space and transition dynamics, e.g., it uses dynamic programming to recover optimal value and policy. It leaves me doubtful if the results obtained here, e.g., that dropout only helps action generalisation, would really translate to more visually rich and complex domains. *Section 2.1 second paragraph, it is not clear to me why rewards cannot be a good way to measure generalisation in the environment, (if you don’t know if your policy is optimal, only way to reduce uncertainty when you don’t have full access to the MDP dynamics is to keep checking new policies to whether one of them produces better reward)*Section 4, a figure of the environments showing how they are used with the MNIST dataset, would help here* Section 4.1, line 4. Authors explore a non standard method to measure generalisation in RL applying it empirically to two deep RL algorithms. Despite of authors multiple points against using rewards as generalisation metric, I am still not convinced that the struggles of RL with respect to supervised learning, which is the absence of the ground truth answer, is tackled by the proposed method. This is not the case in most problems about RL generalisation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; Overall, I like the idea of gauging the robustness to changes in categorical features. The experiments show the effectiveness of such new estimators and corroborates the intuition over model robustness v.s. I feel the paper can be greatly improved by dropping the mutual information part. Therefore, I m giving a score of 3 for now. I hope the authors can address my questions. What is the robustness in Theorem 1 with respect to? Are the robustness measured on the training set or the data distribution? Can we assert that they are close to the theoretical value as samples size increases? The semantics of the original input may have been changed already...<|endoftext|>The paper mainly studies the adversarial vulnerability of a classifier with categorical inputs. It theoretically analyzes the key factors that determine the robustness of discrete input data and uses greedy strategy to solve the problem. Pros: 1.Different from the traditional analysis of continuous data, this paper focuses on the vulnerability of categorical data. 3.The theoretical derivation of the paper is rigorous. In solving NP knapsack problem, provably accuracy is guaranteed. If the categorical input cannot be adapted through gradient calculation due to discrete data, what are the implementation details of gradient attack for Table 1?<|endoftext|>This paper studies the problem of assessing the adversarial robustness of a classifier with categorical inputs, instead of continuous inputs as in the literature. I think it would be great if the authors can provide some discussions on this. > In general I think this paper is clearly written. > It seems to me that the theoretical results in this paper highly depend on the Lipschitz continuity of the model.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This approach is not novel compared to say [1,2] and does not provide any meaningful insight, theoretical or empirical, over these existing papers. Theoretical results are obvious. It would be interesting if the authors could provide some theoretical results suggesting that this approach might be meaningfully better, such as faster convergence. Minor:  Notation for Theorem 1 is unexplained.<|endoftext|>Second, if the value for $ \alpha $ is so small, it is not clear that the proposed algorithm is making a substantial difference at all. Why is such a small value used? 3.The paper is in general clearly written. Also, perhaps some larger scale examples could be created where there still are clear and obvious benefits to the proposed approach. For the idea to be more broadly applicable, there would need to be a way to handle stochastic environments. This is not made particularly clear in the paper, but is an important point since it causes the authors to use SAC instead of a DQN based algorithm on Atari, which is non standard. 4.There are a few weaknesses with the experiments:   1. First, unless I missed something, the paper only lists the value of $ \alpha $ for the Atari games where it was 0.0005.<|endoftext|>* A.1   The math at the end of the proof is so terse that it is very hard to parse. The paper shows that, if all estimators are lower bounds on the true value, then the proposed method converges to the optimal policy. This paper is studying an important problem, but is missing a discussion and comparison to a number of prior works. However, this paper does not compare to these alternative methods, so it is unclear if the proposed method is better than these prior methods. Moreoever, while the proposed method only is guaranteed to work well in deterministic environments, prior work has shown that these alternative approaches do work well in stochastic environments. I would recommend that the paper be revised to discuss and compare to this prior work. **Clarity**: The paper is generally clear. For example, here s a toy example that could be included to illustrate this issue:> Assume a bandit problem with 2 actions, one of which gets reward 1 and the other gets reward $\pm2$. * Sec.2.2   I like that the general idea of value target lower bounding is introduced before the complete method.<|endoftext|>This work proposes to improve learning rate by lower bounding the critic using observed trajectories. Replacing my previous review which clearly was a misunderstanding on my end. The work proposes a low bound on the value by considering the current sampled return. In deterministic domains this provides a lower bound on the optimal value function v^*. I think the work is interesting and the results are nice. Randomness may be small but it is not non existent.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The authors present a class of neural process models that are able to produce correlated predictions while amenable to exact, simple and scalable maximum likelihood optimization supporting multiple outputs. By using invertible transformations (gaussian copula), the model is able to capture non Gaussian output distributions. Experiments with artificial and real data (EEG and climate), highlight the predictive ability of the proposed model. The authors introduce a relatively straightforward extension for Gaussian neural processes in which both mean and (half of the) covariance functions are specified as neural networks, and the covariance function is either explicitly calculated as an inner product in (7) or as a squared exponential covariance function modulated in magnitude by an auxiliary neural network and calculated using the outputs of a neural network rather than the input data itself (x_t, x_c, y_c). Comprehensive experiments on both artificial and real data demonstrate the advantages of the proposed model over the related, but more computationally expensive, fullConvGNP model. Considering that one of the motivations of the proposed approach is how prohibitive existing approaches are, having estimates of computational cost and/or runtime experiments could be a welcome addition to the paper.<|endoftext|>The manuscript proposes variants of Neural process (NP), which can model correlation in the input (and in the output for multiouput regression). The main idea is to directly parameterize the mean and the covariance functions of a Gaussian predictive via neural networks. The authors also propose to use Copulae to handle non Gaussian marginal distributions. The proposed methods are easy to understand and implement. First, the name “Gaussian Neural Process” (and the concept of modeling a Gaussian predictive using neural networks) is already introduced in the reference paper. If this is true, it is required to include proofs. I like that fact that the authors include multiple data sets that can show the effectiveness of modeling dependencies in the input (or the output). The current version of the manuscript is not printable (on Windows 10).<|endoftext|>There is a long line of recent interesting work on neural processes, a scalable and more flexible alternative to GPs for performing prediction at a set of test points (x1, ..., xm) given a conditioning set ((x, y)_1, ..., (x, y)_n). This mapping is learned via meta learning. This paper addresses a core issue of the popular conditional neural process: the predictions at each test point are conditionally independent given the conditioning set. This is an inappropriate modeling assumption for many real world datasets. In response, the authors propose to go beyond a non diagonal Gaussian to describe the joint distribution. The copula model is very close to a full normalizing flow model. I found the discussion of fullconvgp inadequate, as it seems like the most relevant baseline for capturing these dependencies. I m curious how your model behaves when the data truly has diagonal covariance. I feel that the contribution is fairly incremental and I m disappointed that they did not consider full normalizing flow models.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper presents a self supervised learning framework named BEIT, in which the input image can be masked in some regions, and the task is to recover the token of the masked region. Also, I am very interested in the performance of a random tokenizer (i.e.without pre training, just clustering the responses of the randomly initialized networks). OK, I have expressed my opinions that this new direction is promising. 1.The detailed setting of the DALL E tokenizer and ImageNet re trained tokenizer.<|endoftext|>The paper presents a new objective called masked image modeling (MIM) to pre train vision transformers, making the model predict a visual token from the masked patches. In my view, as MRM and MIM are similar enough to be noted, I recommend the authors add the relation with BEiT and VLP models in the related work section. (Though, the authors showed that training discrete VAE with only 1M images (imagenet 1K) is enough to demonstrate the power of the proposed objective.) I think this paper showed rigorous and solid empirical results and well contributes to the community by providing valuable tools.<|endoftext|>This paper presents a novel task called Masked Image Modeling (MIM), inspired by the more famous Masked Language Model task proposed by BERT in NLP. then, BERT task was to replace the token 80% of the time with a [MASK] token, 10% of the time with a random token, and 10% of the time keeping it as it was. BEiT relies on a pre pre trained tokenizer that transforms image patches into discrete tokens, which are then masked and predicted. For this reason, the paper is called BEiT.<|endoftext|>This paper is one of the first to present state of the art results for masked image modeling (BERT style) self supervised learning on images (contrastive approaches held the SOTA before). The whole system is based on a ViT encoder (e.g.with 16x16 pixels patches as input) to produce visual tokens. The pretraining objective consists in matching the visual tokens (one per image patch) from a provided visual tokenizer, with the ViT encoder. (+) The paper is clear and easy to follow. This can potentially be lifted, but the authors made no attempt in this direction. More analysis (e.g.linear probing) could make the paper stronger.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes a Data Dependent GCN framework (D$^2$ GCN) that integrates data dependent dynamic skipping at multiple granularities. Experiments certify the effectiveness and efficiency of D$^2$ GCN. Strengths:(1) This paper is well motivated. (3) Experiments on top of various SOTA GCNs and datasets validate the effectiveness and advantages of D$^2$ GCN. The problem of boosting the efficiency and scalability of GCNs has been fully studied by existing works. The ideas of *Node wise skipping* and *Edge wise skipping* are not novel enough. I suggest the authors add theoretical complexity analysis comparison to better support this. (3) The skipping strategy are confusing:   the node wise skipping parameter $G_l^n$ is 0 or 1. However, the authors claim $G_l^n \in R^{N*1}$. The paper lacks a detailed calculation method. (5) This paper lacks hyperparameters experiments about trade off parameter $\alpha$, which can intuitively provide the trade off between accuracy and efficiency. (6) The writing needs improvements. "Specially, where $X_1$ ∼ $X_4$ are the 4 nodes in the graph."? Overall, this work is well motivated. The technique contributions are limited.<|endoftext|>The authors propose a Data Dependent GCN framework D$^2$ GCN which integrates data dependent node wise, edge wise and bit wise skipping to save the cost for inference while does not sacrifice prediction accuracy. Strength:The writing is easy to understand and the idea is clearly presented. The experiment results show good empirical inference speed compared to the existing methods. …” for equation (1) is not precise. Instead of being viewed as two separated phases, “we no longer need to define an explicit update function, as the update is implicitly defined through the aggregation method”. 2.“we hypothesize that explicit or implicit regularizations during GCN training can improve GCNs’ scalability towards going deeper,”  Randomly dropping being able to help with deeper GCN does not mean all regularization methods can. 5.What is the performance after each stage in your three stage training pipeline. [2] Li Y, Tarlow D, Brockschmidt M, Zemel R. Gated graph sequence neural networks. Some of the technical details need to be provided. The novelty need to be emphasized.<|endoftext|>In this work, the authors propose relatively low cost GCNs in a data dependent way. Their framework has three main components: node wise, edge wise, and bit wise skipping. 2.Edge wise skipping is about the removal of connections between two nodes. It boosts efficiency while achieving comparable performance over benchmark datasets. I think D2 GCN is well designed with simple components and the experimental results are impressive in that many recent graph data are at large scale (especially, web data). The authors focus on the efficiency of computation but I want to ask about the effectiveness of information processing. 1.Authors argue that it can be helpful to alleviate the over smoothing issue. I think it would be interesting to analyze which information is discarded or not.<|endoftext|>To be specific, a so called Data Dependent dynamic GCN framework is proposed, in which node wise skepping, edge wise skipping, an bit wise skipping are integrated via gate function to squeeze out (or reduce) the unimportant neighbor nodes in combinations, unimportant edge connections, and in the bit precision, respectively. Extensive experiments are provided, showing new SOTA results on benchmark datasets. The reviewer is quite interesting how the over smoothing issue is resolved in the proposed D$^2$GCN. More discussions, empirical or theoretical analysis on this point is needed. Furthermore, it is stated that  "such data dependent skipping techniques serve as a regularization effect". Since that the idea of the paper is very simple, it would make the paper stronger if the theory behind could be developed.
Reject; rating score: 3; rating score: 5; rating score: 5; Otherwise, it is difficult to judge the effectiveness of the proposed method. It is good to observe and conclude that KD only works for pruned networks but not for unpruned (original) networks. #### Cons  There are only a few baseline pruning methods compared.<|endoftext|>This paper proposes a hierarchical knowledge distillation method for neural network pruning. 1) The research about channel pruning is insufficient, with only six related works, and the author should summarize the differences with them. In the experiments, the authors should verify the effectiveness of the proposed method by comparing it with various channel pruning and knowledge distillation methods.<|endoftext|>This paper proposes a neural network pruning and fine tuning framework for model compression. The contributions are: 1) A new pruning scheme is proposed by learning the channel importance; 2) The pruning logic is introduced in the pruning scheme. Experimental results show effectiveness of the proposed method. The compared methods are few. Although few authors have attempted to prune EfficientNet, other networks can be compressed in experiments such as ResNet. However, the novelty and the technical details are limited, and the experimental results are not sufficient (see weakness).
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper demonstrates that for the problem of doing predictive maintenance in optical networks, accurate and reliable ML models could be developed in collaborative learning without the disclosure of vendors’ data even in malicious setting. The authors did some experiments to show that 1) federated learning can help to achieve similar prediction accuracy as the centralized approach; 2) malicious behaviors may happen in federated learning, but it can be detected through autoencoder. This paper provides an interesting application of using ML. I think this is just a pure ML application paper.<|endoftext|>This paper designs a maintenance prediction framework for key optical network components based on federated learning. In summary, although the paper tackles a novel problem and designs an FL based framework for the target, it shows significant defects in its motivation, innovation, and experiments. The motivation of this paper is also unclear. On the one hand, secure aggregation is a fundamental task of FL.<|endoftext|>It presents a concept/framework for collaborative learning in predictive maintenance application where a global model is trained based on different vendors training dataset without sharing the data. This is not clear from the paper. •	The description of the dataset should be enhanced in the paper. •	The paper states that a Robust and secure model is developed. While the presented concept of collaborative learning for predictive maintenance can be a key contribution to the domain of PdM, this idea is not well developed by means of related datasets. Then the model can be tested on an unseen different dataset.<|endoftext|>This paper uses federated machine learning for predictive maintenance on optical networks. I think this is valuable work and I liked many parts of the paper. They evaluated privacy and security using robustness as a measure, with different percentages of malicious clients giving motivated "noisty" data as input. The "n out of n" scheme was a nice application to limited model inversion attacks. Only one non public dataset was evaluated, and it was unclear how well that dataset reflected many of "needs" identified in the use case   splitting a monolotic dataset into 10 parts is closer to a 10 fold cross validation scheme than a federated learning scheme. This is a promising, but immature work.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; Strength: This paper considers an important research topic   policy gradient in markov potential games (MPG) and provides interesting new insights on this topic. However, in general, the presented convergence results in MPG are novel. For example, the paper "Stochastic Potential Games" (https://arxiv.org/pdf/2005.13527v1.pdf) also considers potential games with multiple states. I think a more clear and precise interpretation is needed. 3.Section 4 is rather dense and not easy to follow.<|endoftext|>This paper carefully studies the definition of Markov potential games, and establishes the similarities and differences from the stateless case. The definition of Markov potential games is known to be more delicate than that in the matrix (stateless) case. I am not very sure about the accuracy and commonality of the wording. Note that the setting in Daskalakis et al.(2020) is a finite horizon one with random T (not an infinite horizon discounted one). So I am not sure if Lemma C.4 s proof in this paper follows by just citing the results there directly.<|endoftext|>The paper introduces the Markov Potential Game (MPG), which generalizes the classical potential game. The authors then point out several properties of MPG. The paper did a great job introducing the MPG and showing us intuitions about the game with examples and observations. 3.The paper additionally provides the empirical results to verify the convergence. Conference on Learning Theory. The paper is overall well written and established the preliminary studies of MPG problem and multi agent learning algorithm for convergences, though there are still many unsolved questions in MPG such as Price of Anarchy. Therefore, I recommend a weak accept of this paper.<|endoftext|>This paper studies Markov potential games (MPGs) that genralizes the classical normal form potential games. As a result, I think the practical applications of this paper are restricted. Although the theoretical analysis of this paper is well organized and seems to be rigorous, the so defined MPGs seem unrealistic in the MARL setting and are restrictive in practical applications. Specifically, in Prop $3.2$  (C1),  for an MDP to be an MPG, the state transition cannot be affected by the action, which is unrealistic in MARL setting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes a meta learning approach to select the best trade of between memory usage and bit precision for DNN training. As a reviewer mentioned, the idea behand the paper is interesting.<|endoftext|>The paper aims to find Pareto frontiers for the precision given memory budget. The paper formulates the determination of low precision configuration as a multi objective optimization. The key idea seems to be using meta learning and testing to determine these Pareto frontiers. I believe the paper proposes interesting approach in selecting low precision training configurations. I am not an expert on low precision training, but to the best of my knowledge, the overall approach seems reasonable and the evaluation seems okay.<|endoftext|>For both training and testing, the memory matrix is fully computed based on the network architecture and the low precision format. It should be clearly mentioned in the paper. The paper however could be improved especially in terms of clarity, especially in the experimental section. On a related note, how to choose the dateset configuration data points to guarantee this property?<|endoftext|>To achieve the goal of efficiently selecting the best low precision configuration within the memory budget, this paper proposed Pareto Estimation to Pick the Perfect Precision (PEPPP) by using matrix factorization to find non dominated configurations (the Pareto frontier) with a limited number of network evaluations. are possible depending on the computation method/capability. The motivation and contribution need to be made clearer.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies the implicit bias of neural architectures using a Bayesian approach. The paper provides bounds on the average population error of infinite width neural networks that fit the training set under a Gaussian process. The fusion of the different implicit biases brought forth by neural architecture and optimization is an interesting topic. This seems as a suggestion to improve the margin via this constraint rather than an explanation to what happen when using GD in the discussed setup. Additional comments:  Related work does not cover relevant works such as Neural Tangent Kernels and many trajectory based analyses of GD. Interesting paper which suffers slightly from overpromise.<|endoftext|>According to this work, the main reason for this gap is the implicit biased introduced by the gradient descent algorithm. Then, authors show how the implicit biased of SGD is directly related to its ability to control the margin. Strong pointsThe provided PAC Bayes bound is non vacuous, which is something that is remarkable in this context. The analysis of GD is interesting and provides some novel insights. Weak Point:The PAC Bayesian bound does not directly upper bounds the generalization error of a given neural network with null training error. It is an expectation over the generalization error of neural networks with null training error. The analysis of the implicit bias of GD looks a bit arbitrary. Recent literature (Smith et al, 2021, Barret et al.2021) offers much more solid explanations about this implicit biased.<|endoftext|>1.The question of disentangling architecture from the training procedure in the context of generalization is an active and important area of research to which this paper is relevant. When citing NNGP works, please also cite https://arxiv.org/abs/1804.11271, as this work was concurrent with https://arxiv.org/abs/1711.00165. My main concern with the paper is that, based on presented experiments, I believe the central claim about GD providing a boost to generalization through large margin seeking behavior (section 5) is likely very specific to the precise setting considered in the paper. However, I still find it lacking comprehensive experiments to claim that their bound/estimator reflect the implicit bias of the architecture, and the discussion about margin is either lacking rigor or is not completely clear to me at this time (namely, I see why for NNGP concentration on the mean is beneficial, but I struggle to understand the precise analogy with the GD, given that NTK posterior mean can often underperform compared to GD networks). The authors also mention its importance after Equation (14) since it constrains the norms of weights. 1.Section 5.2 does show a very strong advantage of GD ("Nero") over NNGP in an apparently realistic setting.<|endoftext|>Nagarajan et al.**Update**I thank the authors for the response. First, regarding analytical solutions for CNN, the response does not really address the question which is that the technique in its current form does not apply to CNN. This paper explores an important open question in deep learning which is what factors are contributing to the impressive generalization error for deep learning even though no existing theories can adequately explain it. I have read the discussion between the authors and reviewer HuiV and agree with reviewer HuiV that the discussion of margin could use more careful treatment. The paper presents some insightful results on the implicit bias on architecture and gradient descent as an optimization method. What is the reasoning behind this design choice? For example, would this still work if you use cifar 10 or imagenet? In the appendix, there was a discussion about similar problems of PAC Bayes bounds.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The work is quite interesting, and I think fundamental analyses of multitask learning representations like this are important within multitask learning research. This seems like a weird result to me, since a random network should be just outputting noise, and yet the claim here is that the single task network is actually performing worse in terms of the disentanglement metrics. Could authors provide loss curves for the multitask regression tasks and show that the single task networks are at least training? I feel like the weird random vs single results might suggest the single task network is not well tuned or the synthetic tasks are too noisy. I was under the impression that the encoder is a straightforward convnet with no skip connections but am I wrong on that?<|endoftext|>And, it is very clear, and even evident from the results illustrated in Figure 6, the MTL model encodes substantially more information about the input images than the single task models. 2.Another issue with the experimental setup is the experiment with single head vs multi head multi task learning. First of all, it is not discussed that the single head model might perform the tasks significantly worse, as it has much fewer parameters than the multi head model. 3.The experimental setup with VAE based embeddings for MTL is also inconclusive. The paper aims to show that MTL implicitly improves the disentanglement quality of the learned representations, however, the crucial flaws in the experimental setup make all of the major results presented in this paper inconclusive.<|endoftext|>This paper studies the relationship between disentanglement and multi task learning (hard parameter sharing) via empirical study. The authors performed an extensive empirical study and looked at different metrics on a couple of datasets. Major concerns:I think the relationship between multi task learning and disentanglement can be more carefully discussed. I also read the reviews from other fellow reviewers and the authors  responses to them.<|endoftext|>This paper studies the connection between disentanglement and multi task learning. They also show some additional findings, such as that the reverse is not clearly true: make features more disentangled does not robustly help with multi task learning. I think the claim is very plausible, but I think it s hard to interpret this comparison because the reconstructions for the random and single task encoders are much messier than for the multi task encoder, and because it s just a single qualitative example (that s plausibly cherry picked). Weaknesses:  The paper makes some claims about the relationship between disentanglement and multi task learning that I found somewhat uncompelling, or at least confusing.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a representation learning method for reinforcement learning via the bisimulation metric. The only realistic setting is CARLA and I would have liked to see more environments of that nature or more detailed experimentation there. What do the experimental evaluations look like with more environment steps? Are the asymptotic performances of all methods similar or is AMBS just more sample efficient in the beginning?<|endoftext|>While an interesting extension of DBC from Zhang et al.(2021), I have 3 main concerns with this paper:1. **Theoretical issues:** My main concern with this paper is in terms of the theoretical justification for learning $c$. * **Post rebuttal note:** The authors have mostly addressed this in their rebuttal. In equation (5) specify that you are using the closed form of the $W_2$ metric (as in DBC). Please add a reference to the appendix where it is defined. * **Post rebuttal note:** The authors have promised to run more seeds. In the first paragraph of the introduction, consider replacing "based on some RL algorithms" with "based on various RL algorithms"1.<|endoftext|>To implement this intuition, this work proposes to learn a two part representation based off of a bisimulation metric. Learning a state representation based on a bisimulation metric seems quite reasonable, and doing so appears to yield strong empirical results. The experimental evaluation is quite thorough and illustrates several interesting phenomena. The paper is generally fairly clear. Several of the other reviewers also raised concerns about whether the approach is empirically justified. There were also some points of confusion for me in the paper that seem potentially serious, which I included above.<|endoftext|>*Strengths*   The paper studies an important problem in learning visual representations for RL robust to distractions. The reward representation will only capture immediate reward, while its not clear what the dynamics representation will capture since its only objective is future state similarity in the same dynamics representation space. Also, the framing of this learned distance as a "meta learner" seems incorrect. I hope the authors can clarify the above. However in their current form the major components of the method are not clearly motivated and there are some questions I have about the correctness/justification of the approach.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors derive two algorithms based on instrumental variable regression and game theoretic approach for imitation learning. Weaknesses:The paper describes a linear dynamical system expressed in causal language. The DAG is not depicting the mechanism of data formation. Given that the authors are not recovering E[Y|do(X)], the title of the paper "What would the Expert  do(.)?" However, the authors do not seem to be modeling the mechanism of data formation, and they are not estimating the effects of interventions. The authors are performing regression and the causal language is misleading.<|endoftext|>This paper attempts to study imitation learning from a causal inference perspective. More specifically, let S be an observed state, A be action and Z be an instrumental variable. The authors propose that one should perform causal imitation learning by learning a policy function that could induce the same interventional distribution P(A|do(Z)). However, it would be recommended if the authors could build on the existing framework of causal imitation learning. This paper studies an interesting problem in imitation learning with the presence of unobserved confounders. In other words, I am not convinced that the proposed approach in this paper is technically sound.<|endoftext|>The authors proposed a SCM to model latent confounder in the problem of imitation learning. For the instrumental variable, they studied the effect of error in estimation $P(X|z) g(z)$ on Projected Root Mean Squared Error (PRMSE) where $Z$ is the instrumental variable. Moreover, they proposed two algorithms to deal with latent confounders in imitation learning. In overall, some parts of the paper are not well written. Moreover, the model of latent confounder (additive noise with special form in Figure 4) is not justified and the application of the proposed algorithms might be too restrictive.<|endoftext|>This paper proposes two causal imitation learning algorithms to address the confounding issue that causes spurious correlations. The method relies on instrumental variable regression to correct for confounding effects. The main idea is to use the state variable from the last time point as instrumental variable for the state act variables at the current time point in a Markov decision process. States and confounders are marginally independent. Confounders are independent of each other.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; : I am afraid I could not understand this sentence, especialy the part after "applies to ...". First, this paper defined the concept of the Embedding Gap as the measure by which a model can approximate an embedding with low dimensional support. In this respect, the paper is novel. # Post rebuttal comments (11/29/2021)I am pleased by the authors that they considered my comments sincerely and revised the manuscript. The updated paper improved the readability. Still, I feel the organization of this paper would have room for improvement. : I think this sentence is not a complete sentence and needs rewriting. 【Weakness】  [1] I have several questions about the correctness of the statements and proof of the theorems. Theorem 1, Cororally 1      [1 1] If we read the statement literally, the statement trivially holds by setting $F_i   F$ for all $i$. However, Definition 4 does not assume so. [1 3] The output dimension of functions in $\mathcal{T}_{\ell}^{n_\ell}$  looks inconsistent. [1 5] P.17, Proof of Theorem 1: The proof picks $\mu $ such that $W_2(F_{\#}\mu, E_{\#}\mu ) < \epsilon_1 / 2$ . Lemma 7      [1 7] P.18, (62): What is the definition of $\nu$? 22 ... which are not studied here.<|endoftext|>In this paper, the authors studies flow models by a newly developed approximation measure when the data is on a low dimensional manifold. The paper proposes the concept of an embedding measure reflecting the different topologies, then studies an approximation property of the flow models with distributions with low dimensional support under it. On the positive side, the paper is written in detail, with plenty of examples and discussion. One question is that it seems to me that it could be written more clearly what kind of problem was being tackled. The motivation for wanting to deal with different topologies is explained, but it seems unclear to me what difficulties were solved as a result. For example, what conclusions can be drawn compared to simply using the Wasserstein distance? Is it possible to construct a mathematical statement whose universal approximability is compromised when existing measures or topology are used?<|endoftext|>## Strengths* The paper makes good strides in the understanding of injective normalizing flows. This is best exemplified by the class $\mathcal{I}(X, Y)$, which is a very strong assumption to make on our target embedding. This class is **very** close to the core construction in Brehmer and Cranmer 2020, so a result showing universal approximation does not feel particularly illuminating. Finally, the other experiments do not fit the overall narrative and are often underdeveloped. UPDATE Lemma 2 alleviates my main concerns. Furthermore, while the paper claims that it has to account for just topological obstructions, this class of functions limits for more than just that. This is important as these issues significantly hamper the accessibility of the paper. I would suggest that the authors make a thorough pass through the paper to clear up these issues. I included some of the major ones, but there are certainly others. For example, in Theorem 1 the $\mathcal{F}$ is overloaded. * The paper could be better structured. For example, Lemma 1 just proves basic properties of the definition and is unused except for in the appendix; it is not a main result and its purpose can not be understood unless one dives into the details of all proofs. * I was also confused about the previous work section. In particular, it seems very strange to me that the discussion about the core architectures that the paper prove are universal were moved to the appendix, while general inverse problem applications were given several paragraphs.<|endoftext|>This paper analyzes neural networks composed of bijective flows and injective expansive elements and showed that they are universal approximators of a large class of manifolds. A new mathematical notion called the embedding gap is proposed, which measures how far one continuous manifold is from embedding another. The paper is theoretical and mathematically fruitful. The embedding property discussed here is an important geometric property and should be useful in understanding the structure of neural networks. It would be great if the authors can use a more understandable way to present their results in order that the work can reach a larger audience. (1) Good work. Mathematically profound and results may have potential applications;(2) Presentation too mathematical and might reach a larger audience if it can be revised to be more understandable.
Reject; rating score: 5; rating score: 5; rating score: 8; The paper combines the FNO architecture with hypernetworks to learn the solution operator (flow map)of Markovian PDE systems. A hypernetwork whose input is the time domain (\R_+) is trained to output the weights of a FNO which then acts on an initial condition to produce the PDE solution at the time input to the hypernetwork. Numerical experiments showing a modest improvement over the standard FNO are performed on the 1 d Burgers  equation, generalized Burgers equation, and the Chafee–Infante equation as well as on the 2 d Burgers  equation, and the Navier Stokesequation. Overall, the paper is well written and easy to follow. I quite like the idea of using a hyper networkto make the time domain continuous instead of having to add a new dimension to the FNO (the paper does lacka short review of the FNO which I think is needed as it is not a standard architecture quite yet). The paper (https://arxiv.org/abs/2108.08481   Table 2)shows standard FNO results on 1 d Burgers  that are better than those of the presented method and can be improved even more by using a GeLU activation. This casts doubt as to whether the author s hypernetworkapproach is giving the improved results or simply not tuning the FNO. Furthermore considering much longer time domains would be of great interest as simply taking the solution to t 1 is not enough. Even the original FNOpaper considers NS up time time 50. Performing such experiments is crucial if the work is to be published.<|endoftext|>The paper presents a model and a loss function for approximation of the solution map of certain partial differential equations. The models utilize the recently proposed Fourier Neural Operator (FNO) for the mapping model, and a fully connected network for the hypernetwork. Several losses are proposed to ensure consistency of the proposed models, including the composition and reconstruction. The idea of using hypernetworks to model the transition operator is reasonable and of practical interest. The drawback is that the computed solutions are approximate and do not show (at least, this not considered) systematic convergence to the true solution of the PDEs. A classical FEM/FDM solver typically converges as the number of parameters (mesh points) increase. But in practice, such results are never reported. A second question is the methodology of learning the map. As initial condition, a sequence of Gaussian function is used. In PDEs, the errors are measure in the relative norms, which are suitable for the corresponding PDE. Computational speed vs accuracy: the error does not depend on the mesh size (Figure 1) . For finite element, finite volume method there is a systematic convergence of the error with respect to the number of parameters. What is the complexity of solving a PDE compared to the classical solver of the same accuracy? If smaller steps are needed, then it is more close to the classical time stepping (also interesting, but not so, since again, a classical solver, maybe with higher order elements, will outperform the Neural Solver quite significantly).<|endoftext|>In this paper, the authors propose to use hyper network to learn a continuous time dependent evolution operator based on Fourier neural operator. The paper is quite interesting to me. It provides a continuous time representation of FNO using hyper networks. 3.Same as the space time FNO, when working on a chaotic system such as NS, the problem will be very challenging if the time interval t is too large. It will be interesting to study the limit of t for the hyper networks. 4.If remove the batchnorm in the original FNO, its error on 1d Burgers will drop from ~0.01 to ~0.0025, which matches the performance of this hyper networks model, and then the performance becomes consistent among all other 2d 3d PDEs in the paper (check https://github.com/zongyi li/fourier_neural_operator/issues/30). In my opinion, it is totally expected that the proposed FNO model does NOT outperform the original FNO at discrete time steps. In general, discrete models get better errors at discrete time steps compared to continuous models (well, in this case, the continuous model is trained using extra information between steps, so it may not be fair). The main advantage of a continuous model is to do continuous evaluations. I think the problem is significant and the model is novel.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Perhaps some of the assertions made in the article might be relaxed. Overall: good work! +The experimental evaluation design allows the impact of the proposed ideas to be assessed. I like the work the authors have done with the article. However, the ideas are clearly stated, and the experiments are well designed and executed. +As for the results, they look promising. I focus on judging the novelty of the article, dissecting the EPC module, because this is the main contribution. I would like the authors to offer a clear argument about the novelty of their paper for a conference such as ICLR.<|endoftext|>The authors propose a self supervised pre training for the observation encoder using an encoder decoder architecture with the encoded memory as the bottleneck (the decoder is only used for pre training). With that I’m concerned that the proposed masked zone prediction task (which is the core contribution of the paper) is a good choice for handling the pre training. While the approach achieves SoTA on the visual navigation task there are only limited learnings but numerous uncertainties for the reader why this is so. Therefore, the advantage of using zones over frames in the pre training is not at all clear. A definition of zones based on the camera pose (and not necessarily consecutive frames) would be more convincing, as it aims to minimize scene overlap.<|endoftext|>This explicit spatial conditioning encourages learning representations that capture the geometric and semantic regularities of 3D environments. ANS is a very competitive baseline for exploration tasks. More tasks are added, especially the semantic navigation tasks. This will further lift the assumptions on walkthrough videos with pose  The authors should probably adapt ANS to make it work with the stop action, instead of letting it trivially fail. This is a good paper with thorough experimental analysis.<|endoftext|>I strongly agree with the hypothesis that pose information should help with learning representations grounded in 3D space, and the paper does a good job at verifying this hypothesis. I think the approach is well designed. The results are very promising. The authors have picked very good datasets to evaluate on that are challenging and representative of real world applications.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Many recent model based reinforcement learning algorithms exploit the differentiability of the learned model by computing gradients through it. Nonetheless, the standard approach for model learning is to minimize a loss on the prediction of the next environment state which, when the underlying dynamics is complex, does not guarantee that also the resulting gradient information will be accurate. The authors addressed my main concerns. Additionally, the paper proposes to use a model learned with a combination of this new loss and a prediction loss for backpropagation only, while training a traditional model for the generation of trajectories. **Originality:** The method presented is quite original and well positioned among existing methods for model learning. Thus, I now recommend acceptance. **Rigour:** The theoretical results are not of particular benefit for the practical algorithm. **Strengths**  Directly learning the Jacobians of the dynamics is a very appealing goal, given the resurgence in popularity of value gradient methods (i.e., methods which compute the policy gradient by direct differentiation through a learned model). Is the method directly facing this issue? It seems, especially from Figure 4 in Appendix, that a large part of the additional performance of the proposed method comes from the two models component and not from the gradient learning one. Moreover, there is the risk that much of the improvement provided by the use of two models actually comes from the additional capacity. Thus, I would have expected to see an experiment in which the number of parameters used by the two models in total is the same as the one used by the baseline with a model alone. To the best of my knowledge, the only technique for achieving something similar while using stochastic models is the one used in SVG (Heess et al., 2015) to differentiate through trajectories of real experience via reparameterization. This should be clearly explained in the paper. In Figure 3, a more appropriate baseline to show should be MAAC, which seems to be the core algorithm upon which the approach is based, given its backpropagation through the learned model. **Minor Concerns**  I believe it would be more clear to state the assumptions before stating the two theoretical results, e.g., in the background section. Are both of them used to avoid degenerate solutions only, or is there a deeper motivation? There are some issues in the experimental results and in the justification of some moving parts of the algorithm, but the paper proposes a simple and seemingly sound solution for a very important problem in model based reinforcement learning.<|endoftext|>This paper seeks to improve performance of model based policy optimization methods by taking advantage of the world model s differentiability. It starts by showing theoretically that error in estimating the model s gradient contributes to bias in the learned policy. I am recommending to accept this paper because its algorithm construction is well reasoned, the theory appears to be novel and provides appropriate support for the proposed algorithm, and the empirical results provide some minor support for the claims. Overall, I find that this paper contributes an important perspective on the role of using differentiable models, and the need to control the error of first order information (gradient model) instead of only zeroth order information (prediction model) when using differentiable models. **Details for W1:** While the proposed algorithm is the only algorithm to consistently perform well across all 6 benchmark domains, the paper does not provide sufficient information about the baselines to understand if this experiment is fair. How are the hyperparameters selected for the baseline algorithms? The proposed algorithm uses an additional model to encode the world model gradient, suggesting that it has more learnable parameters than any other baseline. Do these extra parameters account for the performance difference alone, or is the performance difference actually due to decreased bias in the policy gradient? The use of only 5 random seeds is also concerning, especially considering the very high variance exhibited in many of the reported results. **Details for W2:**Largely due to the empirical concerns stated above, it seems very challenging to support the claim that the proposed algorithm achieves state of the art results. However, I think the real value provided by this paper is further insight into the need to control the gradient error when using differentiable models. The paper performs two interesting ablation studies, one showing that the proposed algorithm does successfully control this error term, and the other showing the impact of controlling this error on the overall performance. Considering further that two of the six domains were cherry picked for reporting in the main body of the paper, suggests a post hoc maximization bias occurred while selecting these results as well. In order to well support the theory proposed in this paper, I believe the most important set of experiments is actually the ablations. These should be significantly expanded and further insights should be drawn about the role of controlling the gradient error term. In order to make room for this expansion, the SOTA experiments can be largely treated as a demonstration that the proposed method scales well and is competitive, with considerably less focus placed on Figure 1. **Edit during discussion phase:**I have increased my overall score from 6  > 8 based on the ongoing discussion. I have also increased my "empirical novelty" score from 2  > 3 because I believe the new ablations and clarifications yield deeper insight into the impact of using first order information to constrain transition models in RL.<|endoftext|>## UPDATE DURING REBUTTALTo AC & reviewers & authors   I think the authors did make substantial improvements to the paper and it is now easier to understand. Given the prevalence of this problem in MBRL I do think this is a contribution to help the field understand how best to learn dynamics models for control. Though, the empirical questions make my recommendation very borderline. **experimental quality**: the numerical results are impressive, but I don t see enough explanation or intuitions for them. The writing has many typos and the structure is not very clear. If combining the proofs with DDPPO is too difficult, maybe this should be two separate papers. Please cite that so I can learn it / follow the paper. The related works seem dismissive of the past work. It is clear this paper is different, but why are past works important? ### 4 Model based Policy Optimization by Considering the Model Gradient Error#### 4.1 CONVERGENCE RATE FOR MODEL BASED POLICY OPTIMIZATION  in the intro, this sentence makes me think there will be a general explanation of how this can be used for algorithms, but it seems to be lacking "for the policy optimization algorithms in what the policy gradient is calculated using the learned environment model"  This section needs substantially more explanation to support the math. They are big step and hard for me to follow. Can remove some here or in intro. work.Otherwise, a lot of this section is again repetitive. For example, training a model with MSE / MLE is described multiple times and can be done with only a citation. It would be interesting to hear more intuition on why this works. Figure 1 says "trials"   how many each, and how many for other algorithms? which implementation is used for the compared algorithms? Why were Hopper and Ant chosen for Figure 3? Hard to tell if the two model part is needed by the results? Why is there little gain from the gradient trained model when adding the accuracy model? One model is not the most clear. 3  related works "policy gradient to re weighted... re weighted" weird, also missing period at the end of this sentence ... "sample Use the learned"  section 4 "that using the policy gradient" that are using? wrong " in 4.1 "tilde" notation, "hat"  theorem 2 remark: "gradient method converge to its..."  > converges   in 4.2 "between collecting samples from the environment, training the model, and optimize the policy"  > optimizing  in 4.3 "Hence in this paper, we use the error between the estimated directional derivative and the projection value to constraint the learned model’s gradient"  > constrain  "Model Usage" section: "we use the objective function that constructed by following"  > is constructed, " , " space before comma later ## Other comments / ideas (not included in scoring of paper)  in the appendix, the authors include hyper parameters. The environments where the one model approach fails to perform as well seem like they could have a bigger difference in the model errors? I am looking at the Hopper env in figure 4 and 5. Though, the ant does not back this up as much. The paper proposes a novel solution to a well known problem in model based RL. In its current state, I do not recommend acceptance because of its writing / presentation / lack of conveyed intuition, but these are problems that can be fixed in a moderate revision.<|endoftext|>This paper considers learning a model for reinforcement learning and proposes to also learn the gradients of the model in addition to just the predictions. They approximate the model gradient using the nearest samples from the replay buffer in (12) to learn the model gradient (13), which they use to replace the model gradients in the value expansion in (14). They evaluate on MuJoCo environments (Fig 1) and show some key ablations (Fig 3) on the number of samples used for the gradient estimates, and the weight term in (12), and combined/separate models. I like the insight that method can use the model s predictions and gradients and thus we should be aware of model gradient errors. However in practice, we do not know the gradients on the trajectories that we attain and equation (12) requires computing the n nearest points of $x$ in the replay buffer to estimate the model gradient. This seems potentially intractable in practice for large replay buffers, and seems like it will not always coincide with the true model gradient if the buffer is sparsely filled. Is the scalability here the reason why the methods were not run for more timesteps? The most relevant baseline here is to MAAC, as Eq 14 is set up very similarly. One slight concern I have here is that **some of the empirical gain may come from modifying the base model based algorithm rather than adding the gradient information. ** I see the value being optimized in Eq 14 as nearly identical to the one optimized in MAAC (S4.1 there), but some of the baseline results (without the gradient loss) in Figure 4 are better than the MAAC baseline in Figure 1 (especially the cheetah/humanoid). This paper does not provide a video of the learned policy, and furthermore, it is possible for the humanoid agent to remain stationary (i.e.standing still) the entire episode and attain a reward of 5k (because the [base alive bonus is 5 reward/timestep](https://github.com/openai/gym/blob/bb81e141ea7ae67ce339109095841592e8231185/gym/envs/mujoco/humanoid.py#L34) and the episode runs for a maximum length of 1000 timesteps.) ** I request for the authors to clarify this point and provide a video of the agent learned on this task. I am very willing to further discuss this point and increase my score on this paper if this is properly addressed. This is an insightful paper on modeling the gradients that contains an extensive empirical evaluation.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper studies feature variance in a FSL model. Studying this question could potentially guide the how to design an effective few shot learner. + This paper provides code, which makes the paper more reproducible. Why are 1 & 2 & 3 shots selected? It seems that the main novelty of this paper is to use GNN to refine features and make them invariant to hyper parameters such as number of shots. Is it the forward of the GNN module? The evaluation to existing methods is not fair. Most of the methods of comparison use images of 84x84 (or 80x80)  as inputs while this paper resizes all images to 224x224. So I am not able to interpret the Table 1&2&3 and to assess whether the proposed method is effective. Can authors provide the results under the same training protocols with other methods? My assessment is based on the three aspects: motivation, novelty, and results. So I recommend rejection of this paper.<|endoftext|>This work first illustrates a problem in episodic pretraining. Particularly, the paper claims that as we increase the number of shots in the episodic pertaining, the feature space becomes more discriminative because low shot based training is not invariant to the extracted features of the support set. Finally, the paper evaluates the proposed method using miniImageNet, CUB datasets using ResNet10 and ResNet12. Strength:  The problem statement of the paper to build a training strategy with an adapted architecture to increase the generalization looks interesting. Weaknesses:  Unfortunately, I had some difficulties with the illustration of the problem in the beginning section of section 3 (Method). Applying graph neural networks at top of ResNet and testing on the small datasets might increase the generalization due to the over parameterization. Therefore, I recommend the author to discuss the number of parameters in their proposed method and ablate the effect of the applied extra module. For example, what happens if we train the model with a cross entropy loss with a hybrid ResNet12 GNN model. However, large few shot benchmarks are required to measure the effectiveness of the method   MiniImageNet dataset contains images of size 84x84 which is used in few shot literature. Could you please justify the reason for using 224x224? I think the illustration of the problem discussed in the paper is not clear, and part of the evaluation is not clear enough (please see above for the full review).<|endoftext|>Strenghts:This method simultaneously utilizes four FSL modules with different shot partition configurations to train a GNN model that perform joint analysis of the support and query samples. The experiments show the efficiency of the proposed IGFE. Firstly, the two configurations considered in this paper is the shots and the partition, where the partition is the percentage of training samples in the support set, but I do not quite understand the M, the number of sample batches or the selected partition of the support set used for training, which is mentioned in Section 3. Is the M the number of samples in each class that can be used as support set in all episodes (e.g., if M 300 and there are 600 samples in each class, the support set of this class in each episode is selected in these 300 samples)? And whether the query set is selected in these M samples or in all samples of each class? This paper does not show the experimental results with other modules. Therefore, it’s not convictive to show the efficiency of IGFE. Why it is not efficient to directly measure and optimize the distance between the one shot and two shot features? 4.The GNN module is a transductive method. What’s the performance when employing the inductive setting (Constructing each graph with only one query sample)? 5.The number of base classes is 64 in mini ImageNet. 6.There are some typos.<|endoftext|>Pros:1.The experiments on features with different configurations are insightful. Cons:1.The relationship of different shot number has been studied in the former work [1]. This leads to two problems:a)	This procedure seems to be a transductive process since GNN can gather information from multiple query samples instead of just one sample which is the definition of inductive setting. This means it is unfair to some extent to compare the model with former inductive methods. Besides, the proposed method is not comparable with the state of the art transductive methods. The authors can explain about the above questions. In my opinion, it would be better if the authors can provide results without GNN as an ablation study to see if the proposed method is truly effective. IJCAI 2021This paper provides insightful method and strong results. However I have questions about the implementation for now, which makes the proposed method not reliable enough.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This can potentially be useful in many cases. Moreover, the evaluation and comparisons are not entirely convincing. Therefore, I recommend reject. I think similar test time optimization can be easily done with existing methods to obtain higher fidelity results. However, the geometric prior is much more under utilized. Moreover, most of the reconstruction results, including the ones in the supplementary material, are visualized from the same viewpoint as the input image. The paper claims that the proposed method predicts the mesh by deforming a sphere, whereas other methods requires template meshes for training, which I do not agree with.<|endoftext|>Please discuss these limitations in the paper. ### Weaknesses:  The experiments do not sufficiently motivate the necessity of the proposed method:       Does the shape really need to be learned by inversion? This difference should be made more clear. Also, is there a way to modify the baselines such that the comparison becomes more equal? However, the experiments appear not sound enough to support the necessity of the approach and important baselines seem to be missing.<|endoftext|>The paper proposes MeshInversion, a method for single view 3D shape reconstruction. Weakness  One major weakness is the evaluation protocol. However, the same inputs are used as the "ground truth" to evaluate the optimized 3D shapes (Table 2). Another way is to evaluate keypoint transfer accuracy as in CSM [1]. Another weakness is the unfair comparison. I m also not convinced that the proposed chamfer loss is more powerful than the widely adopted differentiable rendering based reconstruction losses. Moreover, how would the proposed chamfer loss deal with occlusion, which should not be a problem for differentiable rendering? How much does the accuracy of the pre trained network affect the test time optimization? However, the experiments and comparisons are not convincing enough to support the major claims.<|endoftext|>The main contribution of this paper is using a GAN, and at test time searching for the latent code that best resembles the object in the input image. Quantitative results are also strong. The code will be released, which further helps comparisons with future works. I think the authors should highlight these differences more clearly. Currently, most results are from the side view of the bird. I am rating this paper with a 6, since I would like the authors to clarify some experimental details, but all in all, I expect to keep my positive score.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The authors propose to use intra class mixup (i.e.linearly interpolating images from the same class) as a form of data augmentation. Furthermore they test their OOD detection performance on many different out distributions. Overall, the paper s use of the angular margin is not well motivated. But even the improved version s performance is worse than, for example outlier exposure [Hendrycks et. I really think this could be clarified explicitly in the Table s caption (especially given that the authors had lots of space left).<|endoftext|>Angle based outlier detection in high dimensional data. This work proposes intra class mixup coupled with an angular mesaure for OOD detection. Strengths  Overall the paper is clearly written, although the experiments and results could be improved. The work, however, misses some key elements in the presentation of the method and has a weak experimental setup. Is this the case? Could the authors motivate why the angular margin needs to be used coupled with another metric (eq.7) and not on its own?<|endoftext|>Angular spread is the standard deviation of the angular margin for a given dataset. With four datasets, they show that intra class mixup increases angular separability in 3 datasets. Table 3: the purpose and analysis of Table 3 could be further discussed. It seems to be in distribution data. The ideas of using intra class mixup to increase angular separability and adding angular margin to the OoD scores are interesting. However, the presentation can be improved.<|endoftext|>The paper proposes to use intra class mixup to train OOD detectors. Adding intra class mixup, the separability between in distribution and out of distribution data is improved. The paper also interestingly shows that the cosine of the angular margin is also a useful measure for OOD detection. The paper proposes an interesting idea of using intra class mixup to improve separability between in  and out of distribution data.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper proposed a novel variational deep survival clustering (VadeSC) method to discover underlying distribution of both the explanatory variables and censored survival times. I also wonder whether it s the same case for tabular data.<|endoftext|>This manuscript proposes an interesting latent mixture survival model that makes a useful contribution to the field.<|endoftext|>In terms of reproducibility and contribution to the community, the authors provide technical details in the appendix.<|endoftext|>This work tackles the problem of clustering in the context of survival data using a generative model. The paper is well written and easy to follow.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 10; The authors describe how existing approaches fit this framework. Right now the paper is claiming more generality than deserved. Minor:    For Figure 2, I feel like the full fine tuning number should come from the original baseline, not the replication. In particular, I liked the section clarifying the connection between Prefix Tuning and Adapters. They conclude that this means existing parameter efficient transfer learning (PETL) approaches are not great for higher resource / more challenging tasks.<|endoftext|>The authors analyse their connections and propose a unified framework which subsumes a number of existing approaches. TACL 2021This paper provides a unified framework for parameter efficient NLP, an important task when serving models at scale. Previous work found large variance in downstream performance across different seeds when performing full fine tuning [1,2,3].<|endoftext|>This paper breaks down the design of state of the art parameter efﬁcient transfer learning methods and presents a uniﬁed framework that establishes connections between them. Experiments on machine translation, text summarization, language understanding, and text classification have been conducted, which indicates that the unified framework enables can instantiate new parameter efficient fine tuning methods that tune fewer parameters. The analysis and conclusion on Adapters, Prefix Tuning, and LoRA are very penetrating.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This work considers the problem of using a non linear classificationfunction as a final layer of a neural network instead of theconventional softmax function. To this end, the authors proposed a newkernelized classification layer which is approximately learned using amodified version of the standard softmax classificationloss. Experimental evidence shows that the proposed kernelizedclassification layer is able to outperform baselines, especiallystandard softmax on synthetic and real world datasets. The paper is generally well written with the ideas explainedclearly. However, the main problem I have is the generality of thisapproximation used to define a functional for evaluating thekernel. The paper truncates higher order terms in the kernel expansionand fixes the number of terms (M) to 10 in all the experiments. Themath however shows that the kernel is indeed sensitive to M, whichisn t surprising. The paper will benefit immenselyfrom a more rigorous treatment of the kernel approximation. The proposed kernel classification layer is a promising idea, howeverI believe it needs more theoretical and empirical analysis to make itgenerally applicable.<|endoftext|>In this paper the authors propose a method that uses nonlinear classification stage at the end of the deep neural network classifiers. The authors argue that deep neural networks learn nonlinear representations by using convolutional layers and activation functions yet a linear classifier is used on the learned embeddings which is suboptimal. Then the proposed method is compared to a baseline deep neural network classifier using softmax loss function. The strengths and the weaknesses of the proposed method can be summarized as follows:Strengths:i) A new method is proposed using radial kernel functions. Weaknesses:i) The main weakness of the paper is that the proposed method is not compared to the related methods well. The authors briefly mention some of the deep neural network classifiers using kernelized classifiers yet there is not any satisfactory argument why the proposed method must be an alternative to these methods. The authors state that the main difference is the use of positive definite kernels (more precisely, they argue that the related methods do not use positive definite kernels whereas they use them). are missing. Please note that nonlinear SVMs use support vectors that are selected from training samples that lie in critical zones where the class samples approach to each other. Therefore, when evaluating a decision function one has to use multiple kernel function valuation against the support vectors. This already restricts to return complex decision boundaries. In fact, the same effect can be obtained by using successive fully connected layers. v) for synthetic experiment, it would be better if the authors show the decision boundaries obtained on a 2d or 3d embedding space returned by deep neural network classifiers as in Center loss paper of Wen et al.References[R1] Lauriola et al.“enhancing deep neural networks via multiple kernel learning” Pattern Recognition, 101, 2020. The advantages of the proposed method over the related methods are not discussed and demonstrated well. There is also limitations of the proposed method as described in may main review in the sense that its capacity for creating highly nonlinear decision boundaries is limited.<|endoftext|>The paper considers replacing the linear softmax layer at the end of a classifier with a kernelized softmax layer to boost the expressiveness of the whole model, especially when using a lightweight feature extractor. The authors have provided theoretical justifications for their layer design, and claim that the designed kernelized softmax layer optimizes over all possible kernel functions on the space of embeddings. The authors evidence the usefulness of this layer in learning more model efficient classifiers in a number of computer vision and natural language processing tasks. # Strengths  The paper is well written and well motivated. # Weaknesses & concerns  Your assumption "this is suboptimal since better nonlinear classifiers could exist in the same embedding vector space" perhaps should only apply to the backbones with limited expressiveness, e.g., the used ResNet CIFAR architectures. As you have not tried more powerful architectures like the wide ResNet family or the ResNet ImageNet family, I hold concerns about the correctness of this assumption. You claim "we theoretically show that our classification layer optimizes over all possible kernel functions on the space of embeddings to learn an optimal nonlinear classifier". But, as detailed in Sec 4, you confine the kernel family as the radial kernels. You can clarify this in the original statement to avoid over claim. Can you provide a discussion on the pros and cons of the two kinds of model choices? I realize the primary con of non parametric kernelized classifiers is that they are not so scalable in the test phase, although they enable analytical inference. If you write the feature extractor as a mapping f and combine f with the radial kernel layers, you are basically defining a deep kernel. The main difference between this work and Wilson et al., 2016 is that you normalize the output of f and learn with parameterized modeling as well as SGD? I am glad to increase my score if the authors are able to resolve my concerns.<|endoftext|>The paper presents a kernelized classification layer as an alternative to the linear classification layer that is widely used in deep networks. The kernel classification layer maps learnt feature vector and weight vector to higher dimensional space to conduct scorer function. The key idea is to represent a kernel into a linear combination of predefined kernels and the combination coefficients are jointly learned with the backbone network.The author showed experiment results on multiple tasks to demonstrated the superiority of the proposed layer. The authors conducted extensive experiments with different tasks, the proposed method is effective by outperforming the compared baselines and other methods. Instead, it could be more convincing if the author conduct experiment on more types of backbones as in Wang et al.2019.Besides, the authors have emphasized that comparing with MKL, the proposed method scales better in large dataset setting. Clearly, this work is inspired by the MKL work and Kervolution neural network. Though the authors have included these work in experiment and show the empirical superiority, the originality and novelty of the idea itself is not very strong. Overall, this paper is well written and the idea is clear.
Reject; rating score: 1; rating score: 3; rating score: 6; This paper proposed Folded Hamiltonian Monte Carlo (FHMC) for posterior sampling over the generator and discriminator parameters in Bayesian GAN. The paper would be strengthened if a formal and more detailed derivation of the proposed methodology is presented. 1.How SGHMC is applied to the S components are not clearly described. The main contribution, the FHMC algorithm, is not presented clearly.<|endoftext|>The paper proposes an extension of BayesGAN (Saatci and Wilson, 2017): BayesGAN learns to sample generator and discriminator parameters from a posterior distribution conditioned on target data using Hamiltonian Monte Carlo sampling; this paper proposes using a modified version dubbed folded Hamiltonian Monte Carlo to speed up the sampling process and make it scalable for high dimensional data. I would be happy to re evaluate if the authors could provide a clearer explanation of the method and results.<|endoftext|>Pros: * The paper proposes a new HMC sampling method for Bayesian based GAN. I find the method novel and effective. * The experiments are nice. * The clarity of the paper could be improved. For example, I am not sure how the equation (8) is derived. Hopefully, the authors can address my concern in the rebuttal period.
Reject; rating score: 5; rating score: 5; rating score: 6; Besides, how (a) can be evolved to (j), how does this work? Strengths:  Significance: This work is related to the exploration issue, which is an important topic in RL and will be intriguing to the community. Weakness:  The exhibition of the paper should be improved and some aspects are hard to follow.<|endoftext|>However, I did not find a proper description of the environment as the corresponding reference is MuCojo and the description in the paper and the appendix do not really offer all details on the environment. al does not work well if the sampling is biased around the starting state and thus, non uniform is not too surprising. An open question is whether approach could be extended to more general environments where the definition of directions is less obvious. The paper s topic is not really specific to Laplacian RL but proposes an exploration method which seems specific to the exmployed evaluation environments of Laplican RL.<|endoftext|>I do not see the particular necessity of hiding position information. * The writing is good and clear. The figures are also informative. Unfortunately, all evidences are empirical. It is just that I feel without theoretic support, the paper is a little bit weak. The conclusion is that TATC can learn a better representation without access to a uniform prior. Temporal Abstraction in Reinforcement Learning with the Successor Representation. My guess is that they are not calculated in the representation space but manually specified instead. * In this paper, the authors use $|\Omega| 8$ unit vectors as the representative directions in a 2 dimensional representation space.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes SAINT, a neural network model for handling tabular data with both continuous and discrete values. Using both InfoNCE and denoising objectives for pre training strategies, SAINT was able to outrank all baseline methods in experiments with 30 different tabular datasets, consisting of binary classification, multi class classification, and regression. The authors claim that inter sample attention is helpful when the number of features is large (i.e.many columns in the table), but this claim is not backed up by data in TAble 5, 6, and 7 in the Appendix. Is there any reason why they are not part of the baselines? Vime: Extending the success of self and semi supervised learning to tabular domain.<|endoftext|>The paper presents a new deep learning network architecture for tabular data classification and regression problems, called SAINT. The new architecture is based on a new attention mechanism, applying attention both between samples (rows) and features (columns). The new model is studied on 30 diverse datasets (10 for binary classification, 10 for multiclass classification and 10 for regression), and compared to 10 previous types of models, including both classical models and recently proposed deep learning models. Additionally, the paper presents a contrastive learning approach for pre training on unlabeled data and fine tuning on a small number of labels, and it show that pre training improves the rank for these small numbers of labels (on average by 1.4 for 50 labels and by 0.5 for 200 labels, not improving the rank when all labels are available).<|endoftext|>The paper introduces SAINT, a deep learning model for structured data that utilizes attention between rows, as well as contrastive pre training. For deep learning based methods, tuning learning rate and the number of hidden units is very important for high accuracy. Explainability capabilities are not presented in a very convincing way. Overall, the novelty of the paper is not high. Overall, the paper has important contributions, especially in improving semi supervised learning for tabular data.<|endoftext|>The paper provides (main contributions) a new deep learning architecture (SAINT) for tabular data that performs attention over both samples and features. While it is understandable that due to the separation in binary/multiclass classification and regression tasks not all datasets from TabNet and TabTransformer could be used it seems a lot of them have been replaced by other datasets from OpenML. Why are the results in the supervised setting (Table 2, last 3 lines, last column) different to the semi supervised setting without missing labels without pre training (Table 3, “middle” 3 rows, last column)? One could argue that by the similarity based approach in intersample attention, SAINT could use the feature information of the unlabeled data during training (where a loss is only backpropagated on data for which labels exist) without the need for pre training.
Reject; rating score: 3; rating score: 3; rating score: 5; This paper proposes a defense method based on Modeling Adversarial Noise. The authors introduce an additional transition network to capture the transition matrix from adversarial predictions to vanilla predictions. The proposed method is novel to me and is empirically validated by the authors under several adaptive attacks. Overall speaking the paper is well written and the method is clearly presented. In addition, supposed that modeling perturbations in low dimensional space is beneficial, it is still not clear that the proposed low dimensional transition matrix actually models the adversarial noise. It would be helpful if the authors can address these concerns. It would be helpful if the questions listed can be addressed.<|endoftext|>This paper proposes an adversarial defense method MAN by learning an instance dependent transition matrix exploiting the transition relationship of adversarial labels and natural labels. The authors should discuss those literatures in the related work and compare the difference between the proposed method and those methods. The authors should well organize this part. Overall, from my point of view, I think the novelty of the methodology and experiments evaluation of the paper in current form is not enough.<|endoftext|>This paper proposes to learn a transition relationship between adversarial labels and natural labels for achieving adversarial defense. The defense strategy that embedding an instance dependent transition matrix into the target model is novel. The authors claim that the proposed framework can be viewed as adversarial training and the transition matrix can be embedded into the target model. Such experiments are necessary to demonstrate the effectiveness of the proposed method, and should not be viewed as the future work. Thus, I think this paper is not enough for publication until these concerns are solved.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper explores and discusses the effects of incorporating prior domain knowledge for modeling fluid dynamics with neural networks. efficiency, scalability, robustness to noise etc.<|endoftext|>The paper develops a learned fluid dynamics simulator based on smoothed particle hydrodynamics approach. This aspect is reflected in the main review section and motivates my rating of "not good enough". The losses compare expectedly, with models that are computationally closer to the reference (ground truth simulator) performing better.<|endoftext|>Neither of these seem to be studied here  the methods are trained and tested on the exact same PDE that is hard coded as "physical knowledge", not even noise is added to the data. I don t think the paper in its current form contributes much new insight for the ML community. The methods range from general (MLP) to physics based (i.e.the actual underlying PDE, with a few learnable parameters).<|endoftext|>Adding SPH related physics information to the model architectures can be a good direction to improve the data driven simulator, but the models used in this paper seem to be simply replacing some parts in the classical SPH solver, which makes me have some concern about the novelty. Based on the smoothed particle hydrodynamics (SPH) method, this paper proposed and explored a fair amount of models with different levels of prior knowledge embedded. Several models with different levels of physics information embedded are proposed.<|endoftext|>The authors propose a hierarchy of physics informed, mostly NN based models of turbulent fluid flow. The NNs are trained to learn various unknown quantities (accelerations or pressures) of the turbulent system’s governing equations (which are derived in the smoothed particle hydrodynamics (SPH) framework) from a synthetic dataset, generated with a fully physics informed model with known parameters. The authors compare each model in the hierarchy in terms of accuracy, generalization, interpretability and data efficiency, and show that the more a model is physics informed, the better it performs on these benchmarks. Weaknesses:  The most physics informed model in the hierarchy is not NN based, but has a few physically interpretable parameters to be inferred, which can be done with standard gradient based optimisation techniques. Comparing the relative strengths and weaknesses of the different models, and arguing the use case for the NN based models would greatly improve the paper.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; However, the authors didn t describe in detail how they mitigate this problem. The only sentence in the paper about this is "However, we avoid this OOD problem by xxx ... revised by generated system response and evaluated reward using offline automatic evaluation". During policy improvement, the sampled system actions and responses with maximum q values are used as labels to update the policy network. The paper mentioned that {a_k}^N is a set of N response candidates generated from the current policy \pi.<|endoftext|>The novelty may be limited by only adding a critic value function on the top of the existing work. 2.Is it possible to add human evaluation for at least of the datasets? (fixed in the rebuttal)The motivation (tackling the response generation diverging issue from human language) of this work is clear, and the solution is also good, by adding a critic on the top of existing pre trained GPT 2 dialogue agent, and shows promising empirical results on the automatic evaluation of two datasets. However, the main concern for this work, is is quite incremental with limited novelty, and also lacks human evaluation to verify its effectiveness, may recommend for a workshop paper.<|endoftext|>p. 8: with following  > with the following;p. 8: the all  > all the;p. 8: whereas original  > whereas the original? The authors have conducted experiments using MultiWOZ and ConvLab and shown that this iterative process improves the performance of the agent. The proposed approach is very simple and should be easy to implement. However, some important details are missing and the novelty is not very clear. Firstly, I think that some important details are missing in the paper. Do you treat the conjunction of a dialogue act and a system response as a single action? The paper does not really describe how the rewards are computed, either. Why is there such a big difference? I am wondering if it causes any overfitting problem.<|endoftext|>The paper claims that it is free from the issue of diverging from human language (a common issue in standard RL), because it learns from the sentences directly sampled from the pre trained language model. However, there are unclear parts to be addressed or clarified. Did the authors try this as another compared baseline? This method should be included in the experiments in order to justify the proposed RL approach is necessary. However, evaluating dialogue policy is also important to justify the learned policy is suitable. However, the paper does not include detailed descriptions about the proposed method, making readers not easy to understand. The paper misses some details when describing the proposed method, making readers difficult to fully understand its idea.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposed an approach to propagate probability distribution through neural networks. In particular, the authors use local linearization to handle nonlinearity and show its optimality in terms of total variation for ReLU. However, the idea of propagating probability distribution through neural networks is not new. Specifically, the introduction section is a bit inaccurate and misleading. The authors claim that they focus on obtaining the propagated distribution in analytical form, or closed form, and that prior work [17] focuses only on Gaussian distributions (note that [17] is very early work from 1999). This is not true. Given the similarity the authors should at least include baselines from [23, 12, 26]. That would make the paper more convincing and maybe highlight what local linearization is a better solution. In terms of technical novelty, since the results for linear transformations are well known, the main novelty rests on the nonlinear part, i.e., the use of local linearization. Here the authors are able to prove that in terms of TV, local linearization is optimal in the Gaussian/ReLU case and Cauchy/ReLU case. In Section 4.2, the authors mentioned that they can compute the exact probability, which is interesting. The trick is to change the softmax into a pariwise Gaussian comparison. It is also unclear what is the setting for Section 5.4. For example, what is the input variance for ‘Pairwise Gaussian (PG)’, does PG produce better uncertainty estimates, etc. Overall, the paper proposed a method for propagating uncertainty through neural network that could potentially be useful. However, it seems some important baselines are missing, the introduction is somehow misleading, the proposed method only works in very limited cases, i.e., Gaussian+ReLU and Cauchy+ReLU (while theoretical results on other activation such as sigmoid/tanh/other ReLU variants are not provided).<|endoftext|>This paper studies the problem of propagating probability distributions through neural networks and applies the results to quantify prediction uncertainties. It proposes a local linearization method to approximate a distribution transformed by a ReLU network, as well as new loss function for learning with distribution valued inputs. It also provides empirical results, showing that the method can quantify two kinds of uncertainty (aleatoric and epistemic) in classification and regression tasks, and training with the new loss function can improve robustness to random and adversarial perturbations. Strengths:   Interesting theoretical result showing that the local linearization is in fact an optimal approximation in terms of total variation for ReLU networks  Nice application of the method to study uncertainty quantification in neural networksWeaknesses:   Only consider the case where the activation function is ReLU (going beyond ReLU, can the linearization method still give reasonable approximations, under some conditions?Would be nice to have some theoretical results in this direction)Comments/questions:  What is the role of depth in the uncertainty quantification and improvement in robustness to random/adversarial perturbations? Can one formulate new loss function for learning with other non Gaussian distributions (other than only Cauchy)? On page 3 (beginning of Section 3.1), is there a typo? Therefore, I am inclined to accept the paper.<|endoftext|>The paper presents a simple new technique to propagate distributions through neural networks. The paper is mostly clear and well written but lacks some details (see below). Or can the total variation distance be used to give guarantees about the predicted uncertainty in these experiments? Regarding activation functions:How well does the proposed method perform with other activation functions. Does the proposed method also produce reasonable results for saturating activation functions such as tanh or the logistic sigmoid? For instance, given a Gaussian input with either a mean very close to zero or a very large variance, the approximation made by the proposed method is not very accurate. How can you explain this? I do not see why this objective is enforcing the model to make a correct prediction. If P(Z_c > Z_e) is close to one for all classes except for a single class where P(Z_c > Z_e) < 0.5, then a wrong prediction will be made but still a high score is assigned by the proposed loss. In my opinion, it would make more sense to define the score by maximizing "min_e P(Z_c > Z_e)". This is similar to the concept of a probabilistic margin, e.g., see [1]. Although I (now) see that this is clearly stated in the introduction, I would really appreciate if a formal definition of the considered setup is made at the beginning of Section 3. If space is an issue, I think Figure 1 is not that important and can be moved into the supplementary material. What architecture is used in Section 5.2? Results on Cifar 10: It would be interesting to see how well prediction uncertainties work for CNNs? How can the complexity be linear if there are quadratically many entries in the covariance matrix? Similarly, how can the complexity be constant if we need to compute a variance entry for each output? I am fine with the experimental results. However, I am not voting for accept right now since in my opinion there are some important details missing (as outlined in my review above) that are required to reproduce results and that would improve the quality of the paper. POST REBUTTAL  I have updated my rating from 5 to 6.<|endoftext|>The paper proposes a simple method that propagates the uncertainty in the data through a neural network with ReLU non linearity. The proposed method simply transforms the mean and covariance of the input Gaussian/Cauchy distribution through linear layers, and applies a local linear approximation for the ReLU activation. The proposed transformation is shown to be optimal under the total variation criterion. The proposed method is very simple, and has theoretical justifications with respect to total variation. The proposed method is only shown to have theoretical justifications for ReLU non linearity. I think to extend it to more general non linearities or give some more empirical analysis about the approximation, at least for the class of piece wise linear functions, is important for this work. Also, it is not clear how to apply the method on max pooling layers. 3.The experimental settings are quite limited. 4.Given the network structure that the method can apply, the network is a piecewise linear model overall. What will be the performance of the method if we propagate the distribution through every local linear model? The main strength of the paper is that the proposed method is simple.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; I would strongly recommend the author to consider this field in the Related Work section. Specifically, this paper compares the landscape of the approximate AUC surrogate loss with that of the cross entropy loss. Overall, the novelty of this paper is rather limited and the relevance with previous work is not clear enough, hence I would vote for rejection. Novelty: The significance and novelty of this paper is rather limited for several reasons.<|endoftext|>The paper studied the AUC loss by investigating its approximates and by comparing it with the cross entropy loss.<|endoftext|>This is an interesting idea for a regime to draw conclusions. The correlation between the loss and the true AUC is not especially strong (section 3.3), which is concerning, considering this is the fundamental assumption made in designing the loss.<|endoftext|>The paper compares using the cross entropy and approximated AUC loss functions in neural network training with AUC evaluation. Further conclusions could be non reliable. However, the evaluation part of the paper is not satisfied standard requirements.<|endoftext|>Authors did empirical studies to visualize and investigate the landscape of AUC (area under curve) loss. [1] Classification vs regression in overparameterized regimes: Does the loss function matter? AUC is mostly used for recommendation and advertising. Using such datasets might be more convincing.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper considers the problem of learning object centric representations from videos. Different from existing methods that mostly model this problem under a pure unsupervised setting by reconstructing video frames, this work introduces two improvements. Previous object centric learning methods are mostly unsupervised. I think this can hardly be well addressed under the pure unsupervised setting. However, this is clearly discussed in the limitation section, and I agree that addressing this problem is out of the scope of this paper. The idea is good and I believe will be beneficial to the object centric learning community.<|endoftext|>The paper proposes an object centric method for video data, inspired from the slot attention architecture. **Pros:**  Adapting Slot Attention to produce coherent object centric representations in video instead of static images is interesting and can lead to further development in the video processing field. It would be nice to see the results mentioned in the last paragraph of the Section 4.2 (Unsupervised settings on MOVi and MOVi++) in a table. I agree with that.<|endoftext|>I am leaning towards acceptance of the paper since it maintains the high bar of the conference quality. The main issue with the Slot attention model is the inability to capture natural texture/background and/or camera movement, and thus I believe this work will not be able to generalize in the real world. ** There are some object centric approaches that use object centric representations for video understanding and might be worth considering citing them, such as:[*] Compositional Video Synthesis with Action Graphs, ICML 2021. I am open to the authors  feedback and other reviewers  opinions. The rebuttal addresses most of my concerns.<|endoftext|>This paper learns the object centric representation for videos by extending the previous static slot attention framework with two new considerations, 1. optical flow for temporal modeling and 2. using simple objects  location cues for better segmentation or tracking. It will be necessary to compare and disucss the PSGNet in the related work. The reviewer has some concerns on the novelty of the paper.
Reject; rating score: 3; rating score: 5; rating score: 6; The paper proposes a direct target adaptation from the source trained model (without the source data). So the comparison in terms of the target domain is not fair enough. Overall due to the missing proper fair comparison, the results do not show the effectiveness of the proposed methods. The paper needs a proper evaluation on the fair baseline.<|endoftext|>Instead of fine tuning *the source model* on the target data, this work proposes to fine tune *a representation learned on the target data alone* using e.g.self supervised contrastive learning. ), there is a lack of uniformity in the evaluation (raising suspicions of cherry picking), and related works are not cited satisfactorily (e.g.black box source free domain adaptation). Are these the combinations where your method achieved the biggest improvement?<|endoftext|>The paper proposes a multi stage approach for source free domain adaptation, namely when source data is not available and one can only use the model pre trained on the source and adapt it based on unlabelled target domain data. is it in table 3? This was just an example.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; + The paper shows the case study for interpreting its prediction by analyzing attention weights to know influential factors. Weaknesses:  Although achieving better results than baseline methods for future lab test prediction, the contribution of this work is negligible in terms of technical perspective. There are some concerns about the experimental settings with respect to the baseline methods. There re many well known models for EHR prediction such as Dipole, TLSTM, HiTaNet, etc., authors should adapt these models to their problem and compare them with their proposed models beside conventional models such as linear regression, k nearest neighbor, and recurrent neural network. In particular, the model encodes information from the patient similarity matrix constructed from their demographic information only without using lab test information from these patients.<|endoftext|>The paper proposes a novel architecture to predict an individual patients lab response and provide supporting evidence about predicted risk for use in a clinical decision support tool. The ablation analysis should be in main paper[1]: https://www.sciencedirect.com/science/article/abs/pii/S0895435618309867[2]: https://www.ijcai.org/proceedings/2021/0486.pdfThe paper targets an interesting problem and the provided results seem to support the efficacy of the model. To properly evaluate such methods, one needs to make sure there are no data leakage across folds. On the other hand, the authors mention that PRIVATE is an outpatient setting   is the data sparse and irregular in that case?<|endoftext|>The paper proposes a representation learning model from EHR data for the prediction of patients  lab test response. Attention weights and the last linear layer are used to define what is called influential factors as to make the model interpretable. Pros:  The proposed model KALP incorporates dosage information which usually neglected in the literature. Cons:   The novelty of the proposed representation is rather limited. It is unclear whether the choice of the representation or the model size is driving the prediction performance. This is not the case in the experimental results of the paper.<|endoftext|>Kudos to the authors! The methods are, for the most part, not new but I appreciate their efforts to incorporate side data to improve predictions a lot, as well as the clarity and thoroughness of the presentation. In this review I will leave aside questions of whether this is truly a clinically useful task and focus on the novelty and effectiveness of the methods presented. This work is does not present any fundamentally new methods, but rather tries to show the value of using largely pre existing methods to incorporate additional information to the task of predicting HbA1c lab test results. This is not new ground, and indeed the strongest baseline used in the work (BEHRT) is essentially the same thing (with a couple notable differences discussed shortly). I would reconsider if this was added to the paper.
Reject; rating score: 3; rating score: 6; rating score: 6; This paper is not a theoretical paper and its value largely lies in its empirical performance. The technical writing of the paper is fairly well. There is no convincing example to show such a generalization can be realized with a constructive scheme. While the proposed method is a minor incremental work built on noise2self. its performance is rather well below recent existing self denoising methods,, in terms of the performance gap to the supervised counterpart. Can the author provide some examples on $g$ for other tasks that goes beyond image denoising?<|endoftext|>This paper proposes a method for unsupervised image denoising. It shows that a better designed operator based on domain knowledge can benefit unsupervised image denoising task. The provided experimental results show the proposed method outperforms existing unsupervised denoising ones and achieves similar performance to supervised methods. However, the second term of Eq.5 is not relevant to f(x). But I still hope the authors address my concerns above.<|endoftext|>This paper proposed a self supervised denoing method using domain knowledge. 1.Examples in Motivation are not convincing. There is no doubt that ground truth clean image can provide better denoising performance, and any other modifications will yield denoising performance decreases. Also in experiments, the results by unbalanced masks are not given, so why give this setting in Fig.2?3.The proposed method is evaluated on their designed settings, which is not consistent with most exisitng self supervised denoising methods. This work proposed an interesing self supervised denoising method and its improvements are significant.
Reject; rating score: 3; rating score: 5; rating score: 6; The paper is not as self contained as it could be, i.e.it is near impossible to understand without reading the core referred literature first. the core idea of the paper is novel as far as I can tell. Already in the Abstract it is claimed that the model learns a discriminative representation, and it is shown in Table 3 to perform in the ballpark of VAE methods on MNIST data.<|endoftext|>The idea of viewing the encoder and the decoder in a close loop auto eoncoder as generator and discriminator is quite interesting. The authors explain that the maximization can avoid that g\circ f is not an auto encoding map. However, it is still not super clear that why such maximization is reasonable to make g(f(x))   x.<|endoftext|>The experiments show that the proposed method achieves competitive performance on reconstruction, generation, and discrimination. Strengths: (1) The proposed adversarial objective function is novel and interesting. As a result, the paper is not self contained, and is not easy to understand. The paper is an extension of MCR.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper is an extension of the Contextual Graph Markov Model, a deep unsupervised probabilistic approach for modeling graph data. The key idea is to leverage Hierarchical Dirichlet Processes, which enables the proposed approach to automatically choose the size of each layer’s latent representation. The authors conduct experiment on graph classification tasks, and the results are quite promising. 2.The proposed model is theoretically sound. The novelty of the paper is insufficient. 2.The improvement over existing methods is not significant.<|endoftext|>In this paper, the authors propose a mechanism to automate the size selection of each latent representation layer of the Contextual Graph Markov model. The paper is well written and easy to follow. This work investigates automatic model size selection without compromising model performance. 3.Experimenting with chemical and social network datasets, the authors show the method perform comparable performance with the state of the art baselines for graph classification problem. Can their proposed method be extended to automate model size selection for those neural models?<|endoftext|>The paper combines the idea from the classical hidden dirichlet processe and CGMM to automate the process of selecting the hyperparameters of CGMM. The authors propose an interesting idea of utilizing the HDP framework to obtain the number of latent variables at each layer. Interesting idea but not very impressive experimental results.<|endoftext|>Authors propose a Bayesian Non Parametric (BNP) method to automatically learn the "structure" of the graph neural network. Specifically, the authors have proposed a Hierarchical Dirichlet Process model for the automatic inference of latent code s size. The proposed method is very interesting, since it s an unsupervised method for automatic selection of network structure, as opposed to the more popular supervised AutoML techniques. Traditional inference mechanisms for BNP based methods are costly to run on larger datasets, hence authors propose a faster Gibbs sampling based inference method.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; NatPN predicts the parameters of the posterior distribution which belongs to the exponential family. # Pros   Utilizing normalizing flows for density estimation for calibration and OOD detection is intuitive   NatPN can be used for both regression and classification tasks. # Cons  Section 2: The authors state that modelling distributions over the weights results in ‘pathological’ behavior. This is vague and hard to understand, what is the pathology? If each component of the model behaves as expected, wont $p( x | \omega )$ remove the need for entropy regularization? (2020).Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. There are a few concerns which are highlighted in the sections above, but overall the paper is a strong submission which solves a relevant problem in a novel and intuitive way.<|endoftext|>NatPN aims to tackle the problem of enabling calibrated uncertainty estimates for in and out of distribution inputs for exponential family likelihoods parameterized by deep neural networks. Could the authors clarify? A mapping from this latent space and its corresponding likelihood under the normalizing flow to the parameters of exponential family update rule is defined. What is the definition of the “predicted evidence” for all models? Also nit: this should be a Table {number} not Figure 4. This is a significant potential advantage over Posterior Networks. However the paper only evaluates on classifiation tasks with 10 classes: CIFAR 10, MNIST, FMNIST. The proposed method NatPN extends the Posterior Network to the exponential family distributions and has the advantage of using a single normalizing flow rather than C flows for classification tasks.<|endoftext|>From which space to which does the NF map the latent variable z? Furthermore, it uses a normalizing flow to account for the epistemic uncertainty. 3.Which experiments show that the normalizing flow contributes meaningfully to the epistemic uncertainty (see b))? Furthermore, I would like to see an explicit discussion of the results. **Update:** changed my score (see comment) from 6 to 8. Firstly, NatPN can be applied to all problems with an exponential family likelihood distribution. The paper provides strong evidence that this is the case. This will be mostly dependent on the estimate of more expert reviewers but I’m also open to arguments by the authors. The paper is well written and the figures are (mostly) of high quality. **Summary:**The paper presents a novel method and provides empirical and theoretical evidence for its effectiveness. All in all, the paper provides a significant contribution to an important problem.<|endoftext|>Towards this, the authors focus on the distributions from the exponential family, which allow for a closed form posterior form whose parameters are predicted via a neural net, together with a flexible density provided by a normalizing flow net for OOD detection. ### Minor Appendix comments  App B: The ELBO loss refers again to the appendix   App B: There appears a discussion on a prior over $y$, while talking about distributions over $\theta$  In the appendix $P(\theta)$ and $P(y|\theta)$ seem to be used interchangeably   The appendix states that the entropy of the posterior is used to estimate the predictive uncertainty, but the posterior predictive uncertainty should actually be computed after marginalization over the posterior. ## Weaknesses  While the paper title is phrased as targeting the predictive uncertainty, the paper s discussion and experiments solely focus on the epistemic/aleatoric decomposed setup (apart from some in distribution calibration results).<|endoftext|>This paper proposes a single pass uncertainty estimation method for neural networks by predicting the update to the natural parameters of the conjugate prior to the likelihood of the data distribution and optimizing a corresponding ‘Bayesian loss’. The paper further proposes an alternative scheme of using a single normalizing flow for density estimation on the features, which is computationally more scalable for a higher number of classes. **Clarity**:Overall the paper is clear regarding the proposed method, although I found it helpful to also read the PostNet paper. Ideally this would be shown for ImageNet (if PostNet can be scaled to 1000 classes), but I understand that this may be too much to ask over the discussion period and would be happy with Cifar100 results   I assume this should be runnable with negligible overhead based off the Cifar10 experiments. **Other notes/questions**:* How sensitive is the method to the value of $\lambda$ (the entropy hyperparameter)? The paper presents a solid and well motivated generalization of prior work.
Reject; rating score: 3; rating score: 3; rating score: 8; Experiments with real world data show that the proposed approach can be plugged in several existing learned index method with improved results. The paper can be improved in terms of both argumentation and design. In particular, Figure 2 presents what appears to be a view of the space error tradeoff. Yet it is not clear what ε values are set for each measured data point in the figure, and it is not clear what querying time they require.<|endoftext|>This paper considers learning based methods for constructing index structures, a fundamental data structure. An empirical study of their algorithm is performed on standard datasets for this area, comparing to several baselines from recent work in this area which use a fixed $\epsilon$. The results show that the proposed $\epsilon$ tuning method can improve several learned index methods from prior work. The main strength of this paper is the empirical study, which shows some interesting results. However, I am not sure that the theoretical justification is sound. Additionally, the overall presentation of the paper could be improved. The paper could benefit from a more clear description of the $\epsilon$ tuning algorithm.<|endoftext|>This paper mainly studies the index structure problem. Existing learned index methods use a fixed value for all the learned segments. The paper is well written and easy to follow. There are some typos, in Table 3, the authors ignore the positive sign in the last column (compared to other columns),2. The author can incorporate more explanations on the experimental results.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; Similar arguments have been put forward for other properties of natural languages, such as ambiguity and vagueness. The effects of population size on the structure of neural emergent communication is less explored and has hitherto not reflected what we know about natural language. # Recommendation updateIn light of the authors  changes and discussion, I have updated my recommendation. In particular, the results do not lend much support to an issue that has already received attention in related (non neural) research on language emergence.<|endoftext|>It has been observed that larger populations produce more structured languages. Furthermore, the authors strive to produce some intuitive rationale to explain why heterogeneity leads to more structured language. I have a generally positive view of the work but, as a non expert, I saw some weaknesses in the paper. What would be the impact on the increasing trends of neg entropy and synchronization (see Figure 4) of having larger heterogeneity in the population? I missed a larger range for his variation.<|endoftext|>* Why did the authors choose the full object reconstruction loss as opposed to a choose among distractors loss (as is slightly more common on the emergent communication literature)? The authors measure various properties of the emergent languages as proxies for how "systematic" a language is, and show several things. Second, in a minimally small population, various measures of "diversity" of the two agents do correlate with their systematicity measures. The paper is interesting and timely, and reports on a large number of experiments.<|endoftext|>The authors refer to prior work in sociolinguistic literature to state that larger communities create more systematic languages. This paper claims that populations explored in machine learning have largely been homogeneous and that population heterogeneity is key to the emergence of structure in artificial agents. Scaling laws for neural language models. However, is this sufficient to claim that the effect is equivalent in the setting you are studying?
Reject; rating score: 3; rating score: 3; rating score: 5; The paper presents a model for learning representations of time series. I understand that the goal is not to evaluate the performances in classification/anomaly detection but the learned representation, but I can not judge the difficulty of the used datasets with the provided baselines. This work is an extension of existing work, the additions being the multivariate aspect and the context invariant aspect.<|endoftext|>The experimental part is a little bit confusing, as the datasets used in the different experiments are not the same. dot should be removed from (2) and bolded font should be used for vectors to make the reading more convenient. This work seems very promising but the interest of the invariant representation is not clearly demonstrated according to me. Some experiments and critical information are missing so that I have to reject this paper. I strongly encourage the author to write & submit an improved version of this article in another conference.<|endoftext|>This paper adapts self supervised learning to the time series analysis domain for the anomaly detection problem. However, some details of the model are not presented clearly and the experimental design could be improved. Especially, why this is important and necessary? However, I also have the following concerns:1.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This work studies personalized federated learning through sparse local masks. The paper proposes and analysis a new method, named FedSpa, which trains sparse local models for all clients reducing communication size, the amount of computation and memory cost. **1.** The idea of using sparse masks to model personalization for federated learning is not novel in this work. Prior works utilize this idea with other techniques (Li et al., 2020) (Vahidian et al., 2021). Moreover, several side benefits such as low communication cost, cheaper computation, and fewer memory requirements should also be attributed to those original works where sparse masks are used, and the same side benefits of sparsity were mentioned. **2.** As far as I can see, the idea of current work that differs from the above two mentioned papers is how the sparse masks are handled throughout the training process. However, there does not seem to be done much progress in this direction neither. The authors nominate two heuristics on how to evolve sparse masks over the iterates, one of which is simply fixing the same mask for all clients and iterations. **3.** It was mentioned in the contributions that the problem of sparse PFL is rigorously formulated (I guess this refers to the last part of section 3, Sparse PFL problem (P4)). However, you do not provide a deeper understating of this formulation, e.g., How well it models personalization compared to other formulations? Why it makes sense to use a small subset of the global model (i.e., sparse masks) as the local, personalized model beyond what was heuristically proposed by (Li et al., 2020) (Vahidian et al., 2021)? Can the formulation (P4) be regarded as a particular case of regularized PFL (P3), say with $l_1$ norm? **5.** Theoretical analysis is very weak. Let us ignore the first two terms in the rate (5) and consider the third non vanishing term $\Upsilon$. Using only Assumption 4 and running any method, one can upper bound the left hand side of (5) by $d B^2$. The level of originality is low given the prior works. A new formulation lacks deeper understanding and theoretical analysis is weak.<|endoftext|>This paper proposes an interesting approach to FL with heterogeneous client data. The main idea is to learn a common weight vector for all clients but allow them to individually mask this global weight vector. ****I appreciate the effort that authors put into adressing my concerns which have been partially covered. In particular, im still not sure about the relation between (P4) and regularized PFL (P3) which is quite generic and also allows for reguarlizers that are indicator functions of sets. My main comments are : * The level or rigour needs to be improved. As a case in point, already the title suggests that the notion of "sparsity" is crucial for the proposed approach. First of all, the sparse FPL problem is a special case of the regularized FPL problem (3) using a specific choice for the regulariser (requiring the non zero entries of the i th weight to agree for all clients.The regularised FPL has been studied in a sparse model context recently e.g.in * M.Yamada, T. Koh, T. Iwata, J. Shawe Taylor and S. Kaski, "Localized Lasso for High Dimensional Regression", Proc. Proc., 2021.it might be useful to compare your approach with the (generalised linear model based) methods proposed in above papers. * The FedSpa algorithm seems to be a straightforward combination of existing techniques (stochastic gradient descent and masking search techniques for sparse deep net training). How did you use Theorem 1 for applying FedSpa (e.g., choice of hyper parameters) in the numerical experiments of Section 5? Im also not sure if (5) can be referred to as "error bound" as it actually bounds the gradient of the objective function along the trajectory of the algorithm. However, the numerical experiments do not use this gradient as the figure of merit but the resulting accuracy. * The clarity or presentation and use of language needs significant improvement:   what are "fully dense PFL models" ? what is "consistently cheap communication"? "The Non IID data distribution makes the training process prohibitively difficult, and the obtained global model may not fit all the local data." "FedSpa does not deploy a single global model, but allows each client to own its unique sparse model..." this is a bit confusing as FedSpa actually tightly couples the sparse model parameters of each client. as i understood, they must have the same non zero weight value for each individual feature. i believe that at least the assumptions on the loss function somehow interact with the particular network architecture. "Experimental results ... also coincides with the theoretical conclusion." "We emphasize that three main progresses are made towards the interpretable and SOTA compression based PFL..." interpretability is a hot topic in machine learning. However, it is not clear how your approach ensures interpretability. The masks appearing in (P4) need more discussion. Are these fixed but unknown (ground truth), or just some estimate. "The framework of FedSpa is extensible ..."  would it be possible to avoid "dist(m1,m2)" by using the ell0 norm (which is already used elsewhere). pls explicitly state any input/hyperparamter used in Algorithm 1,2 3 E.g.<|endoftext|>This paper studies personalized sparse training for federated learning. The proposed method FedSpa is a novel personalized federated learning scheme that employs personalized sparse masks to customize sparse local models on the edge. The authors provide the theoretical result with regard to the error bound and empirical results with several personalized federated learning methods. Strengths:1: This paper proposes a novel personalized federated learning scheme by incorporating sparse to sparse searching techniques. 2: Extensive experiments are conducted to verify the superiority of the proposed algorithm. 3: This paper is well written and easy to follow. The readers can easily understand the paper. Weaknesses:1: The theoretical analysis in Theorem 1 is unclear and weak. It is unclear that what the error bound in Theorem 1 means. The authors need to analyze and compare the theoretical results to other comparable methods. 2: The title is ambiguous and may lead to inappropriate reviewers. 3: I see no code attached to this submission, which makes me a bit concerned about reproducibility. After reading the response, I keep my score at 6.<|endoftext|>This submission proposes federated learning with personalized sparse mask (FedSpa) to apply sparse masks to the local models in training and communication, hence to make the model adaptive to its local data and also to reduce communication cost. Strengths: Addressing the data heterogeneity and reducing communication cost are an important problems in federated learning. Using one architecture for all local models and different sparse masks seems a new idea in the field. Weaknesses: 1) At every iteration, the Algorithm 1 FedSpa requires new masks to be computed, which highly increases the computational time on local machines. The experiments results only report the communication time without the total running time. It is not clear whether the wall clock time can be reduced or the contrary. 2) One merit of using sparse masking is that it reduces the communication memory size and the Table 1 measures the communication cost in terms of GB, so a natural baseline would be FedAvg with compression techniques. A lack of such comparison makes the true effect of sparse masking questionable. This paper has proposed a new method to address the heterogeneously distributed data problem in federated learning. However, the algorithms has some impractical components and the effects over state of the art is not convincing enough due to lack of some baselines.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; The authors present a novel framework for learning different target concepts. Hence, they propose a framework for sampling instances according to only a single target concept and solving this problem with classical ERM and repeating for other concepts. I like the general idea of the paper and appreciate the framework that I consider as novel and well suited for publication in general. Concerns are caused by the baselines in the empirical comparison. The proposition of the framework clearly needs empirical support as the main argument is that a single classifier fails to learn all concepts properly. There need to be more information provided on the baselines to better understand whether they are appropriate or not. Conceptually and theoretically strong but line of argument needs empirical evidence to work out. This problem need to be solved before publication would be OK (could be straight forward to do so)<|endoftext|>From the overall algorithm, it seems to me thatdomains that do not share labels will be assigned to differenthypotheses (even if, in principle, they could be mutually exclusivelabels), because a hypothesis previously trained on a different set oflabels will not predict the new labels. If this is the case, the onlynon conflicting domains are those that share the same label set butmaybe have differences in the input (like in a domain adaptationsetting). In this case the method seems an overkill, as one caneasily match the domains in the episodes by matching their labels (andthe subjective error should be minimized this way). Another unclear part is how the method is applied in testing. Depending on the specific setting (to be clarified), there could beother (possibly simpler) competitors that are more appropriate. Thecross domain fertilization is not evaluated in the experiments as faras I can tell (apart from the matching of domains with the samelabels). The paper is unclear in the description of the method, so much thatsome trivial solutions seem even more appropriate for the setting. AFTER REBUTTAL:The authors did manage to address many of my concerns in their detailed rebuttal and extended experimental evaluation.<|endoftext|>The paper introduces "Open ended Supervised Learning (OSL)", a method that handles data form multiple domains (open ended data). The paper presents the theoretical foundations and empirical results in which it outperforms existing baselines for (generated) regression and classification tasks. Be as comprehensive as possible. (see question)* The proposed concept of the mapping rank seems to not account for label noise explicitly   but I would guess that in the presence of label noise there could be a rank > 1 even though there is only one sensible target function? It would probably be a much stronger paper, if the authors showed that the proposed approach improves on some metric that is not specific to this submission* The paper do not show training and prediction times for OSL and the other baselines. **If the authors can clarify the points below, I vote for accept. The introduced approach OSL solves the problem of allocating data from different domains to their model. OSL could present a path to more efficient ML. * It is still not clear to me whether the dataset X is sampled once or iteratively. **Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. *** The first paragraph after Definition 1 (introduction) is a bit lengthy.<|endoftext|>This paper proposes a framework for open ended data by learning a subjective functions that allows to represent multiple domains without interference. The paper also proposes two new evaluation metrics, one to determine the error of the subjective function and one for calculating the model error in domain prediction. In terms of theoretical analyses, the authors provide some guarantees in terms of learnability and generalisation error. Comparisons to other approaches such as ProbCon, Pseudo L and LabelProp in Colored MNIST, CIFAR 100 and Fashion Product Images datasets demonstrate gains of the proposed approach versus counterparts. Strengths of the paper are:   The paper studies the important problem of learning on multiple domains with open ended data. The proposed approach is novel, simple yet effective, as clearly demonstrated in both the theoretical and experimental results  The paper is very well contextualised in the existing literature   The paper is very well written, organised and the claims are sufficiently justifiedWeaknesses of the paper are:  A few aspect are not entirely clear in the writing of the paper. does this mean that at each time all domains should have corresponding data batches or that each domain is sampled sequentially one after the other? The proposed approach is novel, yet simple, and clearly works well for the problem of learning from open ended data. I would recommend to accept this paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The authors investigate the asymptotic behavior of unrolling applied to dictionary learning. They find that unrolling constitutes a scalable alternative to alternating minimization, where unrolling a relatively small number of iterations or using truncated backpropagation is favorable to ensure stable approximate gradients. From my point of view, this paper is well written and the presented analysis is interesting in the context of a recently growing body of work on unrolling. I only have one minor point which can hopefully be clarified. I think that this paper should be accepted. However, I did not check the proofs in the last detail.<|endoftext|>The paper compares gradient based alternating minimization which uses an analytic gradient (given the code estimate, compute the gradient) to unrolled based dictionary learning which uses backpropagation (automatic differentiation) through an iterative algorithm estimating the code (inner problem) to compute the gradient for update of the dictionary. The authors refer to the dictionary learning problem as non convex. It depends whether you define a global loss function over the whole image or local for each patch. How can you guarantee that invertibility of D^TD on the support stays along the whole iterations of alternating minimization? Given the overcomplete dictionary, very low lambda may result in false positive in support recovery which will affect the dictionary recovery result. Here are some that is recommended to be included in the main. Given this and the additional citations, I have increased my score. Overall, a better organization of the paper is recommended. However, Figure 5 does not show such trend. Is there any insight for this?<|endoftext|>As opposed to alternating minimization (AM) which switches back and forth between dictionary estimation and sparse recovery, the paper writes down the target dictionary as the solution to a bi level optimization, where the "inner" optimization is approximated by unrolling with N steps. The main contribution is an approach (along with careful analysis) of computing the subgradient for the outer optimization, and experiments to show that this method works on synthetic and real datasets. Pros:  The topic is important and timely. It is nice to see some careful analysis for this specific context (dictionary learning). The paper is nicely and clearly written. It may be helpful to illustrate what lessons can be learned here. Bumping my score up to 6. However, the authors could consider contrasting the results with the existing literature and clearly articulating how/where unrolling style training is better or worse than the current of the art.<|endoftext|>The technical writing of the paper is fairly well. 2.The paper presented an quite detailed discussion on unrolling scheme related to deep dictionary learning (DDL) which can be interested to the people working on DDL. if one want to call the proposed scheme for optimization model of dictionary learning2. It seems that  is the cardinality of dictionary. Is this a typo, or they are the same. The scale of such a problem is well handled by the traditional methods such as K SVD, PAML. 4.In general, dictionary learning is not well defined without additional constraints on the dictionary atoms, there is no mention of such constraints in the experiments. For example, proximal alternating method and proximal alternating linearized method. Numerical experiments on MEG signals are also too limited to show its practical advantage
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This work introduces a simple and elegant way to incorporate recency bias for kernelized linear attention mechanism using cos based re weighting mechanism. The authors experiment with LRA, and both language modeling with autoregressive and bidirectional setups to show the efficacy of proposed method. In the table 1, is $\phi_{ReLU}$ linear attention with ReLU as the non linear activation to get positive similary scores? While this is empirically observed, softmax normalization doesn t introduce such inductive biases. This work introduces a simple mechanism to add relative attention / locality bias to linear attention to significantly improve their performance. The results showcase that using the proposed cosine re weighting the linear attention achieves similar performance to vanilla transformers while being significantly faster.<|endoftext|>The paper proposes a variant of the transformer network. The authors base their work on their analysis of softmax attention. They argue that softmax works well because of two reasons (i) it stabilizes training due to reweighing the attention as well as associated connections within the network, and (ii) it forces non negative values in the attention matrix. Utilizing these observations, they propose two modifications (i) a linear projection kernel i.e.ReLU to compute similarity, and (ii) a cosine function for reweighing the attention values. + The idea of reweighing with a cos based function is faily novel. An explaination would help understand the challenges of this particular setting. A few typos (i) Section 3.1 "...the Eq.2 become"  > Eq.2 becomes (ii) Section 3.4, "...such locality bias, ie.,..." > i.e.(note the dots)The paper is generally well written and the problem is motivated clearly.<|endoftext|>Under this context, the authors emperically show that two factors are critical to improve the efficiency of transformers: (1) the self attention matrix should have non negative elements and (2) aggregating negatively correlated information needs the non linear re weighting strategy. **Strengths**  The paper is well written. The proposed cosForm, leveraging ReLU and  the Ptolemy’s theorem (cosine re weighting), is simple yet new, to the best of my knowledge. Glad to see it shows a good accuracy efficiency tradeoff when applying it to replace the softmax operator in calculating self attention for transformers. As for the comparison of runtime speed in training and inference on the long range arena benchmark, did the authors adopt the same transformer architecture for all methods? vs. cosFormer?<|endoftext|>Like Performer and many other previous works, it introduces kernel functions and change the computation order among QKV in the self attention module. Essentially, it replaces softx with ReLU and a cosine operation. Experiments are done on several benchmarks. The proposed method accelerates the inference speed of the model. Though softmax promises a non negative weight, there is no evidence showing that non negative weights is essential. The success of self attention partly attributes to its ability of building long distance dependency. The authors introduce locality into self attention through consine operation but do not show its rationality. In terms of Table 6, the performance gain induced by the cosine operation is limited (~0.13) on some models. Totally, I think the paper is still not well prepared for publication.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper consider the representation learning in low rank MDPs under infinite horizon settings. The authors provide the improved sample complexity and statistical error for the online and offline scenarios correspondingly, based on an observation called the almost optimism/pessimism at the initial state distribution. Does this paper provide new insights on that? There are also some typos in the derivation, most of them are easy to fix. The one step back trick can be conducted at any time step $h$ conditioned on the visiting distribution at $h 1$, and with recursion we can go back to the initial distribution $d_0$, which I now feel is the most significant contribution to the community. The authors don’t properly address the difference between the newly proposed method and Flambe, and I feel the proposed method, although delicate, has so many restrictions that the authors don’t properly address. Meanwhile, the proof is not well polished.<|endoftext|>This paper proposes REP UCB algorithm, which improves the sample complexity of FLAMBE significantly. For the offline setting, the REP LCB algorithm also achieves polynomial sample complexity with partial coverage. This paper is well motivated: representation learning is a very important question in RL, both empirically and theoretically. This paper is also well written, and the technical part of this paper is easy to follow. In terms of the computation complexity, the REP UCB algorithm is oracle efficient, using the same computation oracle as FLAMBE. In Page 14, the second displayed equation in the proof of Lemma 8: it should be ${\hat{\Sigma}_{n,\phi}^{ 1}}$. after rebuttal  The authors  response addressed my concerns. This paper is well motivated, well written, and the results are impressive.<|endoftext|>Theoretical analysis justifies that the proposed algorithms are sample efficient. It also makes sense by comparing the result of the offline algorithm with the policy covered by the offline dataset. The authors calculate the sample complexity by counting $N$, which is the total episodes. Also, [FLAMBE](https://arxiv.org/abs/2006.10814) considered a time inhomogeneous model for finite time horizon setting, while here a time homogeneous, infinite time horizon setting is considered. It would be better if the authors can provide some examples satisfying that assumption. This paper studies the representation learning in linear MDP and significantly improves the previous sample complexity.<|endoftext|>The authors extend the scope of this work to offline low rank MDP where a logged data set is given where an RL algorithm is needed to learn and optimize to come up with a new policy. The paper is well written. However, the paper needs further work to be ready. I doubt anyone would implement this algorithm in practice and the insight is limited. Also, it is not clear how this algorithm might be extended to undiscounted finite horizon MDPs, a subset of episodic MDPs. In that case, is the MLE part efficient? However, there are pieces of this work that are still missing to complete the work.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; The authors study the problem of generalizing out of distribution for RL. They conduct an extensive empirical study, using simulation and real robotics, and observed properties that predict how well an agent will generalize. Similarly, sentences like “A simple supervised metric to evaluate a representation is how well a small downstream model can predict the ground truth factors of variation” and “we use the MLP10000 and GBT10000 metrics (simply MLP and GBT in the following), where MLPs and GBTs are trained to predict the FoVs from 10,000 samples” and “rank correlations between representation metrics and training reward” need to be explained better. Thorough overall results with some fascinating insights. The sim to real experiments were very nice. Regarding other reviewers  concerns:I understand reviewer BEPU s concerns and agree with some of them; but do not think they are deal breakers. The paper has its limitations, but I think it is extremely strong despite these. Robotics papers do tend to have more limitations than papers based exclusively on simulated/game environments, since real world robotics experiments are much more time consuming to perform (but they are also often much more convincing, since they are much closer to real world problems of interests, and not games or simplistic simulations).<|endoftext|>The paper discusses a study of different learned autoencoder based representations in the context of generalisation (in and out of distribution) in reinforcement learning. Comparisons are performed to separately test OOD data for the representation and learned policy as well as generalisation to a real world setting after training in simulation. The analysis shows correlations between various properties and metrics and final downstream RL performanceThe paper is well written and intuitive. The evaluation of generalisation also remains limited. Please make sure that an acronym is explained before using it on its own. Please add an argument for the specific choice of the 2 types of VAEs. The dimensionality of the latent space could have a big impact. I’d recommend at least a minor ablation with the best known parameters and sweeping over the size of the latent space. It does not seem directly intuitive and it would be useful to add a hypothesis. Overall a relevant investigation into the effect of representations on generalisation in RL. While some insights are trivial and the only investigated factor of variation for generalisation is color, there are aspects in the paper worth distributing and discussing (in particular the evaluation of proxy metrics for downstream performance).<|endoftext|>This paper is an analysis paper that uses well known existing pretraining techniques to study the effect of pretrained representations on out of distribution generalization performance. The paper mainly uses the VAEs to train the visual representation network. Weaknesses:* While the paper did a thorough analysis on VAE and in the TriFinger environment, I found these two are limiting. First, there are many different pretraining techniques such as contrastive learning and using auxilliary loss. Therefore, it is possible that the perception network can easily learn to only focus on the object in the image and ignore other things during training. And such a network can be more robust than expected and can transfer to the real world better. Therefore, the conclusions drawn in this paper only apply to VAE pre training and TriFinger environments. It remains unclear whether the conclusions still hold in visually more complex environments such as ProcGen environments. Since the pretraining is mainly about training the perception module, OOD tests should also change the bowl texture, color during the testing. I believe the paper is analyzing an important issue in reinforcement learning. However, more experiments on TriFinger with different visual backgrounds and other visually complex environments such as Procgen, and experiments with different pretraining techniques are needed to make the analysis more thorough and convincing.<|endoftext|>The paper presents an extensive empirical evaluation of the VAE based pretrained representations for OoD generalization of RL agents in a robotic setup. From the reported experiment results, it draws two conclusions:1) agents build on top of the pretrained representations can generalize to some challenging OoD scenarios; 2) the prediction error of the ground truth values of the underlying factors of variations can be used as a proxy metric for selecting representation for OoD agents. The paper provides a systematic and thorough empirical study on the impact of pretrained representations. The correlation analysis between different proxy metrics and the OoD generalization performance is extensive and sound. Weakness  While the key claim of the paper lies in OoD generalization, a large part of experiments are essentially not OoD. Among the four scenarios (IID, OoD1, OoD2, and real world), only the OoD2 presents a clear domain shift to the pre trained encoder. It seems not very clear to which degree the *agents generalize OoD*.
Reject; rating score: 3; rating score: 3; rating score: 3; This paper introduces fair prediction framework which combines graph structure learning into fair prediction to ensure  that unfair pathways are discouraged in the causal graph. This is an interesting paper which tackles an important problem. I noticed that the authors used L_{GL} from Zhang et al.2018 in Eq.(2) directly and added fairness aware loss L_F and label prediction loss L_P. These losses are nothing new (I understand the fairness aware loss is blocking some path, but there is no technical novelty in that aspect). I found the generalisation error analysis is a bit decorative. There is no comment of a wide variety of parameters like $\mathcal{R}_{\cdot}$. Second, sparsity constraint in terms of L_1 regularizer can work well for large graphs. I believe the number of the nodes in the graph is very small  right? In fact, I am not sure how the euclidean distance $||D \bar{D}||$ will enforce causality in reconstructed graph  this is the observed data from the graph but not the graph itself and therefore, I am not sure it will be free from confounding effect. AUC often helps in training with the pairwise surrogate ranking loss. Typo: Page 5, second last line: "linear layer in f_i..":  Please correct the typo on f_iInteresting direction, but lack of novelty.<|endoftext|>This paper aims to improve path specific causal fairness by removing unfair causal pathways. Basically, the framework leverages the existing work by Kyono et al.and adds additional regularization terms. This work is largely relied on the existing work by Kyono et al., including a majority part of the loss function as well as a majority part of the generalization error analysis. Since I am not expert in causality, please clarify if this concern makes sense. If we have a large causal graph, let s say n nodes in the causal graph, then we need n different causal mechanisms. Is there any intuition of setting w_ij as the squared l2 norm? And why are we only using the parameter in the first linear layer? (3) I think it is a bit overclaim about "This observation verifies that our proposedmethod makes the base model to be fair without scarifying too much utility." In appendix about adjacency matrix construction, it is unclear why the entry in adjacency matrix is calculated in this way. As such, I think this paper could be much further improved.<|endoftext|>This paper proposes a framework to integrate graph structure learning and path specific fairness constraints into prediction tasks. This paper targets challenging problems in causal fair machine learning: the causal graph is required, and the computation of PSE is complex. 3.The experimental results show the effectiveness of the proposed method, compared with the baseline approaches. In addition, if there is no causal graph, it is unclear how to distinguish the root node cases and non root node cases. It is expected that the path specific effect is not used as CGF is proposed to address the challenge of complex computation of PSE and simply the PSE calculation. The idea of combining structure learning and path specific effect regularization is very interesting. Based on my understanding, the three challenges are not well addressed:1.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper proposes a new layer TGCL for GNNs which is used to tackle the oversmoothing problem. The proposed layer TGCL is model agnostic. 2.The paper is easy to understand. In this sense, the contrastive loss is not the upper bound. Overall, based on the weakness mentioned above, I think this paper has severe problems with math derivations and the contribution of this paper is also unclear.<|endoftext|>Ref:[1] Xu, Kaidi, et al."Topology attack and defense for graph neural networks: An optimization perspective." For example, the graph sparsity is not studied as discussed in Lemma 1. The proposed TGCL is a flexible plugin to include contrastive information on graphs, and the experimental results somehow evaluate the effectiveness of the proposal. Based on the weaknesses of the paper pointed out above, I tend to reject this paper.<|endoftext|>I cannot find the motivation that why choose PairNorm. The authors analyze the boundary of PairNorm and propose the TGCL. ii)	The paper is well written and easy to follow. Overall, this paper provides a new way to define the oversmoothing problem in GNN.<|endoftext|>In this work, the author analyzed the oversmoothing issue in GNN learning. The author proposed a new method Topology guided Graph Conttrasive Layer which achieved de oversmoothing and maintaining the proposed metrics. Pros:  the manuscript is well organized and easy to follow;  good analysis on K hop study in section 4.3, which explains the intuition of the topology guided method;  the method is generic and easy to add to baseline GNN models which could serve as a contrastive loss extension;Cons:  the three metrics that the author proposed to compare with existing methods are marginal put: most of the baseline model satisfied the third metric(model agnostic) and as for the second metric: the TGCL method is claimed to be easy to adopt but the loss term definition includes extra parameters like temperature, choice of distance method, similarity function, alpha, etc.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper tackles the problem of learning asynchronous multi agent policies with macro actions. At the moment, this section is a little difficult to parse. **Pros**:* Writing: The writing in the paper was largely clear. * Baselines used: I felt the baselines used for comparison were limited to the presented techniques and primitive action based techniques.<|endoftext|>The main contribution of this paper is integrating the macro action value from the Q value based macro action MARL method into multi agent policy gradient.<|endoftext|>The paper presents a method for learning policy gradient based methods on macro actions in environments where multiple agents initiate and terminate actions at different timesteps. Finally, there is a weakness in the experiments in terms of fair comparison. Ah, is it an integer? The experiments comparing macro action based methods are clear and well interpreted with some interesting findings. The explanation of squeezed trajectories in Sections 2.5 and 3 are not clear enough. A second weakness of the paper relates to the ambiguity in some of the descriptions.<|endoftext|>The paper proposes several extensions of the existing multi agent independent actor critic, centralized actor critic, and independent actor with centralized critic to the MacDec POMDP for solving multi agent problems with asynchronous actions. Weakness:The major concern I have is the lack of comparison on other macro action based methods (like those mentioned in the third paragraph in section 1).
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; It is known that fine tuning (FT) outperforms linear probing (LP) ID. This paper suggests that this occurs because fine tuning distorts features in conjunction with the final linear layer. The authors refer to this method as LP FT and show it often outperforms LP and FT both OOD and ID. This paper has many strengths:  It is not well known that FT often outperforms LP OOD   the empirical and theoretical study of this phenomenon may be very important to the community. LP FT outperforms LP and FT on many distribution shifts. In the abstract and introduction, it seems as though LP FT is a method that is being introduced by this paper. For instance, the abstract states "our analysis suggests the simple two step strategy of linear probing then full fine tuning". For instance in the first line of the  "Results." One possible addition to the paper could be discussion of this setting, and in particular if LP FT is still required. 5) The empirical verification of the theory (Section 4.3) is very interesting and could be more thorough.<|endoftext|>This paper explores how different strategies for fine tuning affect in  and out of distribution performance. The authors contrast linear probing (updating only the parameters of the final linear layer), end to end fine tuning (updating all parameters of the model) and a two stage approach, where linear probing is followed by end to end fine tuning. Their experiments on a number of datasets including CIFAR, WILDS FMoW and others, confirm the intuitions from their theory. I believe their results, both theoretical and empirical, would be of interest to many in the community2. Experiments could be more comprehensive. As they stand, it is unclear whether the results from this paper would hold at larger scales. Finally, the authors do not mention the fact that models like CLIP allow the possibility of starting with a good set of initial weights for the final layer, as they can be used in a zero shot setting. 4.Apart from the theoretical results, there is not a lot of novelty introduced by this work.<|endoftext|>This paper studies the problem of how to fine tune a pre trained model and obtain better results for both ID and OOD. Results further verify that LP FT obtains the best performance for ID and OOD tests compared with FT and LP. In fact, the authors could use different models for each ID and OOD pair. This paper discovers that vanilla fine tuning performs worse than linear probing for the OOD tests and then develops a new method combining these two techniques sequentially. Results on several datasets verify its effectiveness. Even there exists some minor problems, this paper is interesting and easy to read. POST REBUTTAL The authors have addressed my concerns.<|endoftext|>This paper discovers an interesting behavior of model fine tuning: the performance is worse compared to linear probing on OOD data (i.e., data from other domains), especially when the distribution shift between inner distribution and out of distribution are big. The authors also provide a simple solution to this issue by fine tuning with a classification head initialized from linear probing and had better results in all the benchmarks they have in the paper. Although the solution proposed in this paper is not fancy, the reasoning, intuition, and experiments are well written.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The critical problems are incomparable results with baseline as well as its incompleteness issues. GAT is computed based on pruned constituency trees, yielding improved performance compared to BERT Large. Unlike previous works (Trautmann et al., 2020), the paper also does not report the complete result (see weaknesses below). For instance, a) In Section 1, many references are missing for argument analysis definition and examples, previous works in par3, and reference to the SPMRL dataset. It needs some references. For instance, a) in Figure 1, why is the depth of the tree capped to a maximal value of three? It sounds to me that this value is determined based on all the data, which potentially harms the methodology.<|endoftext|>The authors present results using syntactic information (primarily through constituency trees) on the task of recognizing argument discourse units (abbreviated to ADU). How often does this happen? The hyperparameters are well documented, and strong empirical results.<|endoftext|>The authors show that the constituency tree representations are relevant since the ADU units are distributed according to the grammatical structure of the sentence which is captured by the tree, and this is supported by empirical results. The results show improvements over a BERT model that does not use the syntactic information nor CRF. The paper lacks many details that, in my opinion, preclude it from publication at this stage. The related work section contains technical details about your experimental setup.<|endoftext|>This paper presents a bert gat crf framework for recognizing argument units in sentences through constituency tree representation. Experiments on small datasets proved the performance of the bert gat crf algorithm. With Roberta liu et al.(2019)?> Roberta (liu et al.2019).4.What are the reasons of not using dependency trees and give a comparison as well for this task? The target questions is rather limited to a specific NLP task of semantic parsing field.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper is overall well written. The second place that I got a bit confused about is how exactly does Koopman pruning provide "new insight on the success (or lack thereof) of magnitude pruning pre convergence," which is one of the main contributions listed in the introduction. I feel that the text and experiments in Section 4 demonstrated that Koopman pruning and magnitude based pruning are *not* equivalent in pre convergence (by epoch 20 or so)   but is that all the authors want to conclude for magnitude pruning at pre convergence? To summarize, I feel that the paper can be a meaningful contribution to the field. However, the "operator theoretic perspective" of this paper   to my feeling as a non expert   is somehow overstated.<|endoftext|>The paper uses a Koopman operator to provide a theoretical basis to explain the success of pruning deep neural networks. The paper is clearly written. 3.Provides a theoretical basis for pruning which is an intuitive and successful procedure. While why this happens is not answered in the paper, it can be considered as a useful by product. Now, the Koopman operator is also sort of hidden and does not provide further insights into the working of the DNN. However, the novelty/significance is not overwhelming and hence the borderline score.<|endoftext|>This paper proposes a new class of pruning algorithms based on Koopman operator theory and shows existing magnitude based pruning methods and gradient based pruning methods can be unified under the proposed framework. This paper tries to explain the success of magnitude based and gradient based methods in compressing neural network models. This paper is well written and easy to follow. For practitioners who are familiar with eigen decomposition, the theory part of this work may not be amazing, however this viewpoint on understanding pruning is very interesting and provides valuable insights into the success of pruning based methods. Although in [1], the layerwise pruning is outperformed by GMP. How much memory is needed? The method is easy to implement and is very practical  (at least for small scale networks).<|endoftext|>This paper attempts to explain why magnitude based and gradient based pruning is effective only after the network has converged, ie it has learned the given task. The aothors attempt to shed some light on this issue via Koopman theory, borrowing from the field of dynamic systems. I guess the main strength of this paper is the perspective the authors use to look at this problem, which to me sounds new. Other merits of this paper is that it is readable and teh authors have made an effort to provide a tutorial on Koopman theory, depspite this subject being quite specialistic. While this paper has the merit of the originality of the approach, it is hard for me to provide a string feedback due to the unfamiliarity with this Koopman theory.
Accept (Poster); rating score: 8; rating score: 6; rating score: 10; Update after rebuttal I like the new result about query complexity of $L_p$ regression problem that the authors have added. Along with the result on $L_\infty$ for (noisy) Vandemonde matrix, I think the paper is above the accept bar, so am changing my score to a 8.<|endoftext|>The paper creates a coreset for solving Lp regression on structured input. For these cases, the paper shows that the coreset size is only polynomial in p. The author defines a notion of rank to Lp regression problem and uses a rounding and grouping technique on the response vector to improve their sample size.<|endoftext|>A subtle issue about the initial experiments is that the gains of biased sampling can only be demonstrated on highly non uniform matrices. Here the incoherence of Vandermonde matrices also provides further motivations for studying such instances. I find the new algorithmic ideas quite interesting. The connections with polynomial regression is quite powerful, and I m only slowly getting a sense of its value.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The main novelty of this paper is the proposed NTK based attack, which attempts to connect the theory (what we understand about the large width limit) and practice (whether a deep neural network is robust). (1) Technical novelty: while the reviewer wasn t aware of any NTK based attacks, the proposed attack is a very straightforward extension of the well known results (essentially, just simply linearizing the model using the kernel and applying FGSM). It is well known that adversarial training is nearly necessary for adversarial robustness   while both the perturbation derived in section 3 and the models evaluated against in section 4 are not adversarially trained. It may be interesting to see whether the analysis in Gao et al.2018 about the limit of adversarial training in the large width regime can shed light on the attack of adv trained models. (2d) Other technical limitations: e.g.all of the models in section 4 are binary classification models; some of the results require the adversary to have access to the training data/labels, etc.<|endoftext|>This paper proposes an adversarial attack that does not require any access to the model under attack or any trained replica of it with the help of NTK. The authors only demonstrate the effectiveness of the proposed method on small data and toy models. (1.b) The method seems to be extremely slow. Although this paper explores an interesting problem, the proposed method is almost infeasible and the contributions are really limited.<|endoftext|>I like the paper in general. this is not correct   the threat model in this paper is in fact incomparable with the usual threat model in white box attacks: The reason is that, for a big part of this paper, we assume that the adversary can access the training data (but not model parameters), but PGD attacks can access model parameters (but not training data). Also, if an adversary can access training data, he can always retrain the model using that training data, and then PGD attack it. However, now to this end when I look at empirical results in Table 4 on test data, then PGD (ce) is much better than NTK attacks. A clear formulation and correct analysis of the threat model is one of the most important things in this kind of work,and this work is somehow very confusing in this regard.<|endoftext|>By using the analytic and empirical NTK versions of common network architectures they are able to generate adversarial samples without requiring white  or black box access to the model under attack. While the authors clearly defer the study of efficacy of test data with pseudo labels for NTK based attacks, it would be good to state why the authors believe this approach to work. As of now there is no reason to believe the attack will work in this setting, putting serious restrictions on the efficacy of the NTK based attack. The approach to NTK theory is novel and intriguing and can lead to interesting new findings. The paper is clearly written and the contributions well put into context of existing research. However, this reviewer is missing some rigor in the theoretical underpinning as well as the experimental ablation. It would be good to understand the theoretical assumptions that need to be made in order to show that the first order correction is always finite and non vanishing  and at the same time dominating the second order corrections. Specific points to improve upon are:1. As a consequence the generalization to first order only input corrections are not obvious. Looking at Figure 1 the differences between NTK based and PGD based attacks are stark.<|endoftext|>The derivation of adversarial perturbation under NTK setting. Contribution wise; applying adversarial attack on NTK at inference time seems a trivial extension of PGD attack, as other reviewers also mentioned. The considered transfer attack does not have a solid demonstration of why the result is significant and does not compare to SOTA transfer attacks in the same setting, like no box attack. While the proposed method is interesting and of practical importance, I have several concerns on performance evaluation on technical novelty compared to previous works, which are detailed as follows. Thought the focus of that paper is on poisoning attack, [R1] also considers data perturbation, and the analysis looks quite similar to the proposed method. I also think the adversarial perturbation method used in [R1] can be a baseline method to be added in the performance evaluation. Technical novelty compared to existing works on existing NTK based generalization attacks. The authors did not discuss the differences at all. 2.Lacking performance comparison to state of the art no box attacks.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper considers an interesting problem, but I am not convinced that the proposed models are warranted in theory. The proposed method leverages conditional variational autoencoder to learn the distribution of scenarios conditioned on the context, where the encoder computes the latent variables (based on the context and scenarios) while the decoder recovers the scenarios. Experiments on two concrete problems are performed. However, I look forward to the authors’ response. Therefore, the rationale behind CVAE SIP is not clear to me. If the goal is not to approximate P but to solve the optimization problem, then having the objective values involved as a predicting goal is reasonable; in this case, having the context involved is justified because they can have an impact on the optimization results. While reducing the scenarios from 200 to 10 is promising, the quality of optimization has decreased a little bit. On the other hand, in Figure 2, using K medoids with K 20 can perfectly recover the original value, which suggests that K medoids is a decent solution and complex learning methods are not necessary for the considered settings. It looks to me that the proposed methods are designed for graph based problems, while two stage integer programming does not have to be graph problems in general.<|endoftext|>This paper explores these problems by using a graph conditional VAE to learn low dimensional representations of scenarios and perform downstream tasks in this space. In my opinion, this is a nice paper. The technical novelty w.r.t.the ML community seems a bit limited, however, the empirical results (in particular for K 20) are very strong, showing that the proposed method can lead to  high quality solutions (< 20% optimality gap) while reducing an optimization solver s runtime by several orders of magnitude. The paper is well written with a smooth introduction to SIPs and the related literature towards machine learning for integer programming. I have two major concerns:1. Further, is there any specific contribution to the GCN literature, or is the proposed method strictly an application of GCNs to a new problem?<|endoftext|>It further demonstrates that using embeddings concatenated with a predicted objective generally improves performance over using the embeddings alone. The objective prediction visualizations address this to some extent, but this only illustrates why the predicted objective is useful, and one could in principle append this prediction to the raw scenario representation as well. **Updated after rebuttal**I think the authors have addressed the concerns I have raised and I have increased my score. The paper demonstrates the use of a conditional VAE in the context of solving stochastic integer programs. The approach seems technically sound and the empirical results show improved performance over both clustering the raw scenarios and over recent baselines. On the other hand (conditional) VAEs have already been used for similar combinatorial optimization tasks before, and their use here towards scenario clustering has small additional significance and novelty.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; However, some claims are not justified and the implementation is not realistic. Are they necessary to be in the bound?<|endoftext|>****************************************I really appreciate the detailed responses from the authors and they did address some confusions regarding the theoretical analysis. However, there are still some confusions in the revised draft. As in their case, I am not very clear about what sample complexity refers to.<|endoftext|>The paper proposes two decentralized AC algorithms, one for actor critic and the other for natural actor critic, which are sample and communication efficient. 2.The decentralized NAC algorithm is probably the first one in the literature. The idea is interesting but not sophisticated. Combining existing tools may lead to new analysis challenges, which needs to be further clarified. Thus I think the paper is on the board line.<|endoftext|>I think the developed decentralized AC and NAC algorithms are good complements to the existing multi agent RL literature, and hence I recommend accepting this work.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The superiority of new sampling probability over the other layer wise sampling method, such as FastGCN and AS GCN, is still unclear. This paper introduced a new layer wise sampling probability as well as a new debiasing sampling algorithm to obtain a better approximation accuracy.<|endoftext|>The reason the paper gives is that a core assumption made by certain GCN schemes such as FastGCN and LADIES do not hold in datasets such as Reddit and OGB. Assumption here is when the assumption in LADIES is not met ( $||\boldsymbol{H}\boldsymbol{W}_{(i)}|| \not\propto$ corresponding $\ell_2$ norm of column $(\boldsymbol{Q}\boldsymbol{P})^{(i)}$), then the proposed method will work well.<|endoftext|>The novelty and difference with existing works need to be emphasized. 3.What is the regression result of your proposed sampling method with regard to $H^{(l)}W^{(l)}$4. The visualization in Figure 3 needs to be improved.<|endoftext|>However, there is no empirical study of the efficiency of the proposed method. (2) Overall, the paper is well written.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 8; After motivating and introducing the method, the authors evaluate it on computer vision datasets with synthetic inserted shortcuts, ie. black pixels in the corners of the images. The paper tackles and illustrates an important issue of not only CNN s but general deep learning models. As the authors describe this is not a novel insight and several approaches were introduced to reduce and even prevent shortcut learning by deep neural networks. This makes it very hard to read. Unfortunately, I do not believe that this paper meets the standard for publication.<|endoftext|>The work proposes a method for reducing the extent to which a model learns to rely on "shortcuts" by introducing a new form of regularization encourages the model to make uniform predictions for white images. The proposed method is evaluated across a range of model architectures and datasets and evaluated in combination with a variety of existing techniques. This conclusion is based on Figure 3 and an inspection of the code. While setup correctly, the example using "black squares" and the CIFAR dataset is a very simplistic (and artificial) example of a "shortcut". However, the paper still needs to be revised to emphasize that it is not focused on "perceptible patterns" as shortcuts (eg, cows are usually in pastures) but rather on "imperceptible patterns" (eg, high frequency signals in an image). I m very familiar with the work on "perceptible patterns" and this paper is not up to par for that area because  it does not identify specific shortcuts and then demonstrate that it has reduced them.<|endoftext|>The paper proposes a method called White Paper Assistance that serves as a data/task agnostic regularization to prevent shortcut learning. 2.Authors conducted a very comprehensive set of empirical experiments and ablation studies to show the usefulness of WPA. The idea is a bit simple   which in of itself is not a true weakness. Would a different white paper reveal different bias in the model? The evidences are 1). This point can simply be illustrated by the experimental results3. However, given the amount of effort that goes into this paper, I m on the fence of whether to recommend accept or reject.<|endoftext|>The paper proposes a novel method, White Paper Assistant (WP), to prevent CNNs from utilizing spurious input out correlations, the so called shortcuts, in classification. The main idea is to intermittently update the CNN to predict uniform distribution over classes for white image inputs. The proposed method is simple and effective, the right combination for wide adoption, and the insights from Section 4 on how WP affects different parts of the CNN classifier can help improve the overall understanding of these models in the community. The experiments are well designed and clear in what they try to illustrate, and sufficiently analyze the performance of the method, on its own and in conjunction with other related regularizations, as well as the contribution of different hyperparameters. In particular, these models can reduce error of classification on data limited regimes (such as the small datasets used in this paper). As such, despite having some concerns listed previously, I think the paper merits acceptance.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; This paper provides a theoretical analysis of both the universal approximation and the approximation rate of the RNN encoder decoder in a linear setting. The summary of the theoretical results in the paper is neat, clear, and important. Detailed proof is provided for all propositions and theorems. This paper gives interesting notions and discoveries on the approximation properties of the recurrent encoder decoder structure, and the statements are rigorous.<|endoftext|>This paper provides theoretical insight for approximation properties of RNN encoder decoder architecture in linear setting. 3.Approximation rate for small size coding vector: the architecture and the following assumptions in the paper give rise to an intrinsic structure to this type of RNN encoder decoders called temporal product structure which can be deconstructed into encoder and decoder parts. The RNN that the paper introduces here? Overall, I think the paper is well written, well motivated, and is a good theoretical insight into one of the first types of DNN structures for sequence to sequence translation.<|endoftext|>The authors first show the universal approximation property of RNN encoder decoders, and subsequently, they show approximation rates of targets for RNN encoder decoders. They introduce a notion of temporal products that can characterize the temporal relationships in the input/output pair. This paper studies the approximation properties of RNN encoder decoders, in the perspective of linear and continuous time. The authors first show the universal approximation property of a linear RNN encoder decoder, then extends to the approximation rates that can be characterized by the rank of temporal products.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposes to use similarity metric to generate pseudo alignments between source/target program pairs. According to the experiments, the word mover s distance works notably well for this purpose, but other metric can also improve the translation accuracy significantly against random selection. The paper also challenged to construct translation systems between arbitrary pairs in 10 programming languages using the proposed framework and observed that the trained system works with certain accuracies. Although I recognized that the main contribution of this paper is not proposing a sophisticated method to align samples, I thought the paper need more attention to design efficient strategy. Also, the algorithm seems a simple greedy method and may be heavily affected by the sampling order. The root cause of this is that the paper did not provide a reasonable criterion about acceptance of this experiment first. This is not critical: this is still acceptable for the first observation. Some results are hard to discuss due to unnormalized data.<|endoftext|>The paper constructs (weak) parallel corpora for code translation tasks using nearest neighbor sampling in the code document embedding space. A variety of similarity metrics / base LMs are evaluated. In its current form, the problem statement and approach seem very trivial. In its current form, it seems like this is preliminary work and with some more sophistication (in method + data sets) it is going to be a good future task. I suggest that this be submitted to an appropriate workshop and a future attempt be made to submit this to ICLR.<|endoftext|>The paper proposes a simple similarity based data augmentation technique to translate code written in one programming language to another. With empirical investigation, they show such apparently simple data augmentation closely matches the performance of the models trained on the manually annotated datasets. + The empirical investigations are solid. + The paper is well written and easy to understand. The main problem with this approach is that the performance suffers significantly with CodeNet dataset. Thus, the dropping of performance for CodeNet is not surprising. However, this raises questions about the usability of this technique. The contrast between performance with noise is useful, but again that shows how sensitive this technique is with noise and for some legacy code (e.g., COBOL   a language authors use to motivate the study) the amount of noise might be high. The authors propose an approach of data augmentation for code translation for document similarities. The approach is also susceptible to noise.<|endoftext|>This paper mines noisy parallel datasets of code by calculating the similarity between two non parallel sets of documents. Based on the two findings, the authors finally apply the proposed method to a large, non parallel code dataset, and observe a performance boost of using a noisy parallel dataset compared to randomly paired datasets. The paper is easy to follow. 2.The proposed method is simple and easy to use. 3.The experiments are relatively thorough, covering different types of document similarity methods, programming languages, model architectures, and evaluation metrics. It would be nice if the authors could add these results in the author response. For example, penalizing the noises (i.e., the targets not belonging to the source) during model training (fine tuning). "Machine translation with weakly paired documents." 2019.**Typos**  Section 3 Proposed Method: curated  > createdThis paper is interesting, but the research is somewhat superficial.
Reject; rating score: 5; rating score: 6; rating score: 8; rating score: 8; The paper focuses on the problem of incorporating symmetries into RL agents’ representations to improve data efficiency. It proposes a model based representation learning method parameterized by a group action that is based on encoding states and state action pairs as elements of the group s representation in latent space. The proposed method is evaluated against a number of baselines in the data limited Atari setting. ### Strengths  The paper is well motivated and addresses an interesting and important problem. The idea of embedding states and actions as matrices representing a group action is intriguing, and may induce interesting properties in the representation beyond those studied in this paper. The paper provides sufficient background to be understood by readers with some but not extensive experience in the topic. ### Weaknesses  **The justification for the proposed method is lacking** — section 4 provides a number of desiderata, but there is no follow up showing empirically or theoretically that the resulting method (especially given the use of target networks and projections) will actually satisfy these desiderata. ** For example, in the discussion of symmetric MDPs the group action is applied to both states and state action pairs, but it is not clear whether these represent two separate actions, or whether $g\cdot \langle s, a \rangle$ has the same effect on states as $g \cdot s$. Section 4 is quite long and in many cases it’s not clear whether the proposed properties are _desiderata_ or guaranteed to be satisfied by the proposed method. ** Adding the group equivariant loss doesn’t provide much improvement over the standard model learning loss — particularly when we look at median performance. This suggests that most of the benefit over the baselines is coming from the model learning component of the objective rather than the incorporation of symmetries. Further, the method without the symmetry learning loss is similar to many existing representation learning methods such as DeepMDP, PSR, PBL, MuZero, LatCo, and DBC. The equivariant loss is therefore critical to the novelty of the proposed algorithm, yet it doesn’t seem to have a significant impact on overall performance. The paper only uses one evaluation scheme and doesn’t provide much insight into whether the learned model really is equivariant to the set of transformations used. Further, the baselines used in the paper don’t seem to be described or cited, which makes it difficult to determine whether a fair comparison is really being made between the different methods. SPR, which does include a learned latent space model, seems to attain a higher median human normalized score than EqR with the equivariant transition loss, suggesting that at least part of the improved performance is coming from the specific parameterization of the model. ### Concrete avenues for improvement  I am very intrigued by the use of matrix embeddings in the representation — typically in RL we treat representations as vectors, and it would be interesting to see whether enforcing linear actions of the transition and symmetry operators on the latent space induces particular structure. The focus of this paper is on equivariant representations for sample efficiency; however, it seems like an arguably more promising application is in improving generalization. Ensuring that symbols and their types are defined before they are used will make the paper much more readable. Section 4 should be rewritten to more clearly express which of the desiderata are definitely satisfied by the proposed method, and which are not guaranteed to be satisfied. With additional work and a more informative evaluation setting, I think the paper will be in a much better position for consideration in future conferences.<|endoftext|>This paper presents a latent variable model for representation learning in RL, taking into consideration both equivariance to an agent’s action and symmetry transformations of the environment. The proposed method (EqR) outperforms several previous methods in data efficient (100k) Atari benchmark. Strengths:* Leveraging invariance and equivariance as inductive bias for representation learning in RL makes sense. The proposed way to encode the equivariance / invariance constraints in the latent representation space is novel and interesting. * The writing is in general clear and not too difficult to follow. Weaknesses:* One of my concerns about this paper is the performance comparison with SPR. Since on Atari median score is often considered as a more reasonable performance measure than mean score, I would not be convinced that EqR outperforms SPR. * Besides, it seems (EqR, $L_R + L_{GET}$) performs worse (in terms of median score) than (EqR, $L_R$) in Table 1 and Figure 3(b). Given that the method is also very similar to SPR in some implementation choices, I am not convinced that the proposed method is better. * To help readers better understand the math in Section 3.2 and Section 4, I suggest the authors give more examples during these 2 sections to provide a context (possibly with the pendulum example). Other comments:* In Equation 5, can the same $g$ act both on element in $\mathbb{S}$ and on element in $\mathbb{S} \times \mathbb{A}$ ? * In the first paragraph of Section 4, $\mathcal{G}_T$ and $\mathcal{G}_R$ are not defined. * I am not quite getting the sense of using $\tau_\theta(s_{t })\tau_\theta(s_t)^{ 1}$ as $\tau_g$. While I do find the idea of this paper novel and interesting, the empirical evidence does not convince me of its superiority over prior methods, in particular SPR. I will not recommend acceptance for its current form.<|endoftext|>This paper presents a representation learning technique for data efficient RL that makes use of invariance/equivariance of the environment wrt both state and agent actions. Such a homomorphism can be useful if the new mapped state action space is easier to work with. The second idea is that we may know that an environment exhibits certain regularities (i.e., may have a group structure) where we know the effect of certain operations. Then building these regularities directly into our models can then make training much more data efficient. In this paper, the authors combine these two ideas (MDP homomorphism and symmetries in MDPs) such that state and actions are mapped to new spaces that satisfy the MDP homomorphism criteria along with the invariance/equivariance relations that result from the structure of the environment. The second (L_GET) encourages the effect of group actions to be equivariant on the learned space. And finally the third one (L_R) encourages the rewards in the learned space to be the same as the original MDP. The authors test their technique (in a Rainbow DQN agent) on Atari levels under data limited setting and show that their technique, albeit slighlty, outperforms previous techniques. Overall, I think this is a quite interesting paper. It presents a very principled approach to learning better representations for RL in a data efficient manner. Perhaps the improvement in performance is not very dramatic (and parts of this were already in earlier publications), but I think the motivation, the technical development, and the final presented technique are all of interest to the community and will be valuable contributions. I don t have many concerns about the paper but I would have liked to see some analysis of the learned representations and the transition models. How does the new learned state/action space look like? Also, it was not clear to me why adding the loss term L_GET led to worse results (in median scores). This might be perhaps the assumed group structure is not really suitable for some of the games. It would be nice to mention what group SE(2) is at some point (perhaps in the Experiments section). L_AET is always included in training but this is only mentioned afterwards.<|endoftext|>The representation are symmetry based, using the state transition symmetry (to create a strong inductive bias robust to symmetries that are not present in the reward) in a latent transition model (to enable learning of the transition model in the latent space, and to allow for linear assumption of the group action through the representation). A decomposed structure is imposed on the representation of the symmetry group, such that it allows to represent the latent embedding space as a direct product of subgroups. I think the idea and method were clearly explained despite the complexity of the overall model, combined with all the moving parts, well done! The author s perspective on equivariance in RL is interesting and novel (to the best of my knowledge). I was convinced by the motivations outlined and the consequently design choices. I think it would benefit the paper to make it clear early on that the choice of the loss depends on a choice of subgroup blocks that depends on prior knowledge the environment (e.g.2d translation, rotation, etc.) as this didn t come clearly to me until Example 1 is presented. I appreciate that the appendix includes a discussion on the choice of group for Atari games, but it would be better to have more general discussion in the main text. I understand that authors wanted to show performance across many different environments to showcase the robustness of the performance, but I would have liked to see more in depth experimental results in one or two tasks including learning curves, specially given that data efficiency is the main desired property. minor comment:I believe the b_{\eta}  above Eq 18 should be g_{\eta}? The paper is well written and introduces a very interesting perspective on equivariant representations in RL. The method proposed makes sense to me, and the results seem promising.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper proposes a new spectral augmentation Robustmix, aiming to improve the robustness of image classifiers. The paper misses a proper ablation study while presenting method is of limited novelty. 1.Strengths    To my mind, the presented idea, while being rather simple for implementation, is well sound with recent advances in the synthesis of signal processing and deep learning. a.First of all, the idea of spectral mixing itself is not particularly novel.<|endoftext|>This paper discusses a new proposal on improving the robustness of image classification/segmentation by a frequency guided data augmentation approach. The proposed technique is based on a DCT transformation to determine frequency bands with high energy in order with respect to their sensitivity. Furthermore, the discussion of the approach based on Stylized ImageNet and ImageNet C is not really convincing to discuss the quality of the approach in general.<|endoftext|>The paper proposes a novel data augmentation method for training classifiers. The training label is based on signal energy in different frequency bands, generally strongly favoring the label of the low frequency content. In particular, the method appears to not improve over standard Mixup in terms of clean accuracy, and only wins convincingly when measuring the corruption error mCE. The paper also does not measure a simpler alternative where low and high frequencies are taken from different images (without mixing) and the label is decided based on spectral energy as in Robustmix.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 8; The authors propose to tune the learning rate of a SGD, by training it by gradient descent. The presented method is already known, the authors do not cite the preceding works in that line of research, and pretend that "[they] are the first to make learning rate in neural network differentiable with the goal of minimizing loss function".<|endoftext|>I believe that this can be an honest case of an independent rediscovery of the technique by the authors combined with a lack of experience in finding related work and writing the paper in a more grounded way (e.g., avoiding vague statements). The paper concerns an adaptive learning rate algorithm based on the derivative of the objective with respect to the learning rate.<|endoftext|>This paper presents a method of adaptive learning rate in training deep neural networks and solving machine learning problems. However, the proposed DSA method is not solving the challenging optimization problem in the field. The way that DSA deals with the sub problem is to either propose a gradient descent or use hyper parameter to perform one step update.<|endoftext|>The authors propose a novel optimizer that makes the learning rate of a deep neural network to be differentiable in a self adaptive manner, which overcomes the problems of existing problems DNN optimizers i.e., insensitive and unstable. Authors propose an optimizer called DSA, which can increase or decrease very sensitively and significantly with the help of the learning rate’s gradient, which overcomes the disadvantages of the existing algorithms e.g., AdaGrad, RMSProp, and Adam. The paper is ready for acceptance.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors propose a two branch attention architecture to handle multi dimensional real valued time series data, where prior  and series attention matrices are computed and utilized in a certain min max competitive learning framework. The final anomaly score is defined as the product between the re construction error and the KL distance between the two attention matrices. Update after the discussion with the authors. The paper now looks very strong. I recommend acceptance. The key idea of using Tranformer s self attention as a measure of anomalousness sounds novel. It is an excellent idea. The proposed architecture featuring a two branch attention mechanism also looks novel. Many researchers in this domain may agree that the baseline model in the present context can be the vector autoregressive model, which naturally realizes a particular type of self attention in the form of the lag dependent covariance matrices. You can find many works that leverage a dependency graph for anomaly or change detection.<|endoftext|>Summary:This paper is looking at anomalies in time series data. In particular they define anomalies as ones that lie outside some learned Gaussian like distribution. They propose a minimax objective function that tries to minimize and maximize the discrepancy between a Gaussian distribution with a learned variance parameter and an “empirical” one learned directly through self attention on the data. They measure discrepancy using the symmetric KL divergence between the two distributions and also use this for their anomaly score. Major Comments:   Is this approach a generalization / extension of change point detection? The prior association is learned as one that minimized the difference with the series association and the series association is one that is learned to maximize the difference with the prior association? But shouldn’t the anomalies be in the tails of the distribution as they are rare? Overall the paper has goods results and seems sufficiently interesting and novel.<|endoftext|>The authors begin by discussing the discrepancy in association between normal and anomalous points and then suggest an anomaly transformer based on an anomaly attention mechanism that is further improved using a minimax technique. On six different datasets, empirical analysis demonstrates that the proposed method outperforms state of the art anomaly detection methods. ### Strength:1) The paper is well written and easy to follow2) Figure 1 and 2 are very intuitive and easy to understand. Experiment/results section need some better explanations. Can you add more details and provide a solid reason on why r 0.5% for SMD, 0.1% for SWAT, 1% for other dataset. [c] Tariq, Shahroz, et al."Detecting anomalies in space using multivariate convolutional LSTM with mixtures of probabilistic PCA."<|endoftext|>This paper introduces an Anomaly Transformer for detecting anomalies in time series with association discrepancy. This paper introduces two discrepancies: the series association (e.g., period and trend) and the prior association (e.g., the local smoothness or continuity). In the model, a two branch strategy is adopted to model the two associations separately. The experimental results demonstrate the effectiveness of the proposed model. The observation that the abnormal points have a strong local association (or prior association) and a weak global association is interesting. 3.Comprehensive evaluation on a variety of anomaly detection datasets demonstrate the effectiveness of the proposed association discrepancy learning strategy. Why the optimization strategy is "minmax"? In general, the observation that abnormal points have a large discrepancy between the prior association and the series association is very interesting. A novel Transformer based model is proposed to model this discrepancy. Besides, there are many ambiguous details that hinder the readability of the paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; Following your intuition to preserve the manifold structure, I wonder why all your best scores are not for the biggest k, as the biggest k would more preserve the structure. This is contradictory with the initial prediction. Finally, if k grows to infinity, the proposed method would just mixup samples between themselves, thus no doing any mixup at all... This is also a concerning point, a study focused on minibatch OT (what authors do in practice) might alleviate this problem. I wonder their impact on the training. Regarding theoretical results, the paper discusses their results properly, however I have some concerns about the validity of the results due to the fact that authors use in practice minibatch optimal transport instead of exact OT (see questions and remarks below). The method has been used on classification problems on a different datasets of different dimensions. It has also been compared to other mixup variants (manifold mixup) and on adversarial attacks. The results show a small increase in the performance. A discussion between k nearest neighbourh is thus lacking. Significance: The idea is interesting but is not too novel as the idea of coupling close samples has already been explored in the original mixup paper through k nearest neighbourh. The difference is the use of optimal transport to determine how to connect samples instead of doing it randomly as done in the original mixup. Authors could have empirically experimented the percentage of connections between different clusters of data to show that their method respects the manifold of data. 8.As the motivation of your method is to respect the manifold of data, an interesting experiment, in my opinion, would be to measure the average percentage of connections between data which belong to different clusters. I have also a concern of the manifold structure preservation in high dimension. Most reviewers also think that the use of theorems in practice is unclear. For all these reasons I keep my score unchanged. While I agree the idea is appealing, it is not too novel and some missing discussions would lead to a huge change in the original paper in my opinion.<|endoftext|>This paper proposed $k$ mixup, which is a generalization of mixup, to better regularize neural networks during training. 2.4 The authors claimed in their paper that the extra computation cost caused by $k$ mixup is small, and it could be better if they could provide numerical evidence for this, e.g., compare the wallclock time of training the same model using regular mixup to that of $k$ mixup where $k$ 32. They also did experiments for various networks and datasets and showed that $k$ mixup can improve the generalization performance and robustness of the models compared to the original mixup. The performance improvements on real datasets seem marginal, and this is also a common concern for most reviewers (iZL9, EGaJ, 8N9h). This cannot be true in practice. Each experiment is also repeated multiple times. Therefore, I think more work needs to be done to provide enough justifications for $k$ mixup (perhaps from the theoretical side) and to further improve the empirical performances. 3.2 The assumptions needed for the theoretical results might lack justifications. The justification for the proposed k mixup method might be unclear, and this is my major concern for this paper. The authors proved in Theorem 2 that $k$ mixup can help the model with smooth interpolation between the clusters, but this model seems somewhat specific and the authors did not talk much about this smoothing effect in this paper. For instance, setting both $k$ and $\alpha$ to be large and setting both to be small should both result in a vicinal dataset. In this case, the two regimes might have a similar local structure for the training data, and it seems confusing why this extra parameter $k$ could help because we can always choose a smaller $\alpha$ to make the training data more local. This may also influence the regularization effect of k mixup and it could be possible that changing the ratio of same class mixup can already improve the performance of mixup. I would like to thank the authors for their very detailed response which addressed some of my concerns (e.g., ratio of same class mixup, wallclock time comparison), but my major concerns (unclear justification, marginal performance gain) about this paper still remain.<|endoftext|>The paper proposes an optimal transport based mixup algorithm, theoretically analyzes the algorithm, and empirically evaluates its performance. This questions the practical gain of the proposed method on high dimensional data. The authors may want to clarify the difference between the proposed approach and the one introduced in this prior work. Performance comparisons with baseline algorithms are missing. This makes it unclear whether or not the performance gain actually comes from the benefits of k mixup. To show that this is not the case, the authors may want to rerun the experiments while adaptively setting the range of alpha values for a different choice of k such that the same (or similar) alpha values can be tested. Minor comments:  "Averaging weights are typically drawn from a beta distribution β(α, α), with parameter α ≪ 1 such that the generated training set is vicinal"  > Not true. See Table 1 and Table 2 in the original mixup paper for the choice of large alpha values. Also, the original paper says "For example, in CIFAR 10 classification we can get very low training error on real data even when α → ∞ (i.e., training only on averages of pairs of real examples)". The theoretical claims look solid, but their implications are unclear. The experimental settings and results could be improved.<|endoftext|>This paper proposes to select pairs of points to mix between two batches of K examples by using the hungarian algorithm to find an L2 optimal matching. There is also theory analyzing how often mixing will interpolate between different clusters when using k mixup. Detailed Notes from reading the paper:    Modification of mixup where batches of k points are perturbed in the direction of k other points using interpolation under a Wasserstein metric. Proof with experiments and theory showing this k mixup preserves cluster and manifold structure. This paper shows some nice improvements on low dimensional toy datasets from using optimal transport to select examples from two different batches to interpolate. This is decent work and it may have some impact, but the small improvements may make the impact fairly limited. Figure 1 is a bit weaker of a result than could be possible   since the solution inside the spiral is still somewhat but only partially blurred after using k mixup. The insight in Theorem 1 is nice, especially that cross cluster mixes will still be selected more as K grows, but as a decreasing fraction of K.     The classification results (Figure 8) are a bit discouraging, and somewhat contradict the introduction which claims that the k mixup technique doesn t hurt results, when several results are 0.1 0.3 basis points below the baseline. FGSM is a very weak attack, so the improvements in Figure 12 are of questionable significance, although it is nice to see slightly better robustness. This paper achieves small improvements on large datasets and significant improvements on either low dimensional data or where the mixing rate alpha is very large. The improvements are small but the idea is simple and logical, so I weakly recommend acceptance.<|endoftext|>This work proposes an improvement on the Mixup regularization for training deep neural networks. Instead of performing weighted averages of randomly chosen pairs of samples, an optimal transport map between two k batches is computed. Then, "new" samples are constructed by interpolating between *coupled* pairs of samples (according to the optimal transport plan). A theoretical study supports this intuition, and extensive experiments are conducted. Theoretical study supports the claim of the paper (as k increases, the vicinal samples better reflect the local structure of the dataset). Isn t k mixup regularization a pre processing step with "fixed cost"? Do you have an idea why increasing k does not always improve your results? This seems to be opposed to the intuition that higher k better reflects the structure of the dataset. More generally, it could be great to have a discussion on how to choose k and alpha for your method. Could this technique be useful in the context of transfer learning (I am not asking for more experiments here)? My only concern is on how to choose the hyper parameters k and alpha, which do not seem to always have the intended effect. Overall, I tend to recommend acceptance.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; However, later on, I got confused to see that it is a structure learning problem from network data (possibly from steady state of ant networked process). To me, strategic behavior or games is very misleading in the current scenario. I would expect the problem to be practically useful if one tries to find the most likely graph structure which would induce the network data i.e.$X^{\ell}$. (3) I did not find the datasets to be very coherent with the main theme of the paper. The paper makes an important promise  but the formulation and experiments are not coherent with this promise.<|endoftext|>I hope the authors improve on the suggestions for submitting their next version. The paper successfully applies Deep graph inference model to the problem of network discovery in network games. With regard to empirical results, I am not convinced of the generalizability and significance of the results.<|endoftext|>Applying transformer like architecture to network inference for network games is an interesting direction, but it is unclear how much is the technical contribution of the paper, and the experimental result is not very strong, as discussed above in the weakness part. The authors use transformer like architecture to conduct the learning task. I would understan ROC AUC when the edges are unweighted, and this it is only a binary classification task on the edges. The experimental evaluation shows that the proposed algorithm performs better, but the margin is not very significant.<|endoftext|>The question the authors ask is the following: assuming rationality of the players and observing a noisy version of their actions x_i, can we reconstruct the original graph? A transformer like architecture is used as encoder to allow for variable length number of players and permutation invariance over set of games. It is difficult to understand if this would work on much larger networks, and whether the improvements of your model over the baselines would still hold in larger networks. This might also suggest that the applicability of this method in actuality might not be straightforward. I m voting for borderline acceptance for this paper but I m not very familiar with this area.<|endoftext|>It would be good if authors discuss about the required no. To justify/motivate their modeling, they show that the PSNE for some well know (utilities in) networks games, optimal actions, is related to a function of network structure (F(A)) and another function of some additional parameters (b). of data samples to infers the (transformer architecture) model parameters.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This work ablates some of the design choices that go into learning a dynamics model for control based environments. The authors study this in a few of the DeepMind control suite environments. Weaknesses:* Takeaways from the paper don’t seem to be conclusive (rf.“No single design choice works significantly better”). Specifically, the study only focuses on four control environments which are not very representative of settings where you would want to do model based RL (e.g.the related works section motivates the use of accurate model prediction for robotics, but only applies it to simulated, deterministic environments). Another thing is that it’s common practice to predict the difference in states. So why should we care about these domains that the authors evaluate on?<|endoftext|>( ) Another concern is about the random seeds. The average and confidence interval based on 5 random seeds may not be accurate enough, therefore it is hard to justify whether some conclusions are valid or not. As a paper empirically compares different model designing choices, I believe showing reliable averaged performance is important. ( ) It is mentioned in the paper that the experimental results obtained in this work are partially inconsistent with previous work (Chua et al.2018), however, there is a lack of discussion on what may cause this inconsistency, which makes it hard to justify whether the result provided in this work is reliable or not. These differences can affect the learning efficiency of the agent. My major concern is that the experiment setting may affect the accuracy of the conclusion provided in the paper.<|endoftext|>The findings provides some useful insights for practitioners as well as for future research directions. The main weakness are that only four benchmarks were used to arrive at these conclusions, and some elements of the methodology are not clear. I believe ablative studies like this are very useful for the research community and practitioners alike. Typos: "rollouts of the provide", "significant more data", " the modeling the "While some design choices made in the study could be clearer, and I would have preferred to have more benchmark environments, the paper is well written and the experiments appear carefully designed. I think it s important to be clear here, there is no connection with what the planner explores? Some data on this would be useful. 4) The multi step results were the most surprising to me, the optimal number of steps also seems to vary considerably between tasks, from 5 40 steps. If you use a fixed data set, I don t see how this could be for the model?<|endoftext|>This paper systematically studies a range of design considerations in model learning: 1) deterministic versus stochastic models, 2) 1 step versus multi step targets, 3) the number of instances in an ensemble model, and 4) the effect of input noise to make the model more robust. * The paper is clearly written, has many results, and good additional videos online. Given that this is a benchmarking paper, I think this is a somewhat narrow range. I think the results partially illustrate this, since on multiple occasions the results are somewhat inconclusive (on one or two it works better, on the other two on parr or worse). * I think you missed some important related work [1], which already extensively studied the effect of multi step losses in RL tasks. * I doubt whether it makes full sense to study stochastic models on deterministic environments only (Sec 4.1). * While is the paper is generally well written, you incidentally have some strange sentences with missing words. For example: “Therefore, the modeling the contact is not the main problem but the model exploitation in the vicinity of strong non linear changes”. On the downside, I kind of doubt whether I learned too much from the results, since 1) the results vary quite a bit over tasks, which I think might be due to the relatively small set of tasks, 2) the results on stochasticity are limited by the set up, which excludes stochastic environments, and 3) some other results were already partially known (like the multi step prediction effects and poor performance of MSE).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; It is not clear how dynamic is the local memory; Figure 1 is not well described in the paper. This paper presents a novel plug and play text decoding method that allows improved text generation for knowledge intensive tasks via reinforcement learning based technique. Strengths:   This method does not require re training or fine tuning LMs with knowledge based objectives. This method can  dynamically  infuse external knowledge into each step of LM decoding.<|endoftext|>This paper presents Knowledge Infused Decoding (KID), an RL based approach for grounded decoding by conditioning on external knowledge. Ablation studies are presented for swapping out the retriever, different hyperparameters for KID and the size of the underlying LM. 3.Would it be possible to add some qualitative examples? Update: Thanks to the authors for their detailed response. What is happening in those cases? Would suggest the authors consider reframing this   it’s fine to include the experimental results, but the argument and claims need to at least be thought out. Updated to weak accept after author response. On a related note, why not also compare against other constrained decoding baselines eg: lexically constrained decoding (https://arxiv.org/abs/2010.12723)? How does this compare to vanilla decoding methods? Some additional comments:  1.<|endoftext|>The paper is interested in improving the performance of natural language generation tasks that require external knowledge (which are often called knowledge intensive tasks). When KID is used in conjunction with GPT2 M or BART L, they show clear improvements, and in some datasets, they achieve higher accuracy than the state of the art. The experiments were done in multiple datasets and the proposed method consistently shows a better result than its baselines. It is especially encouraging because the proposed method is a plug and play module that can be applied to any language model in practice. This might have caused several misunderstandings. The paper does not compare against vanilla (non graph) constrained decoding which limits the output space to the words in the retrieved corpus. I think the paper s result is great and I agree with the overall motivation behind the paper.<|endoftext|>To that end, the authors present Knowledge Infused Decoding (KID), a novel decoding algorithm for generative LMs, which infuses external knowledge into each step of the LM decoding. The proposed method is fairly sound and shows good empirical results. ## Strengths  Proposed method outperforms baselines on various knowledge intensive tasks.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper studies the relationship between Byzantine gradient attacks in distributed optimization and data poisoning. Some theoretical evidence is presented about the equivalence of these. Pros:The paper studies an interesting problem, since both Byzantine attacks and data poisoning attacks have been extensive studied in the literature, but little is known about how these fields relate. For the theoretical analysis a number of recent works, for example [1,2,3,4], have explored (robust) machine learning from multiple datasets from a PAC perspective. For the gradient attack that is designed, there is no comparison to prior attacks as well. It is also unclear what this attack has to do with the rest of the paper   can the authors explain? The paper is also slightly hard to follow, in particular the results of Theorem 4,5 and 6 are quite hard for me to interpret. Further discussion immediately after these results will therefore be helpful.<|endoftext|>The paper studies data and model poisoning attacks in federated learning (FL). It argues that there is an equivalence between data poisoning and model poisoning attacks, by restricting attention to linear and logistic regression. It would be interesting to show an equivalence between model and data poisoning attacks in FL. Another key contribution is to propose a gradient poisoning attack. Therefore, showing an equivalence as the paper argues would be very interesting. Definition 1 relies on ‘true models’ \theta^{perp}. It will be important to elaborate this further. 3.In Definition 2, a loss function is defined to be PAC learnable. PAC learning is usually defined for a class of functions. This seems to be quite restrictive. 6.Theorem 5 requires that a user’s dataset contains at least \mathcal{I} inputs drawn from a certain model. This is quite confusing.<|endoftext|>This paper shows an equivalence between data poisoning attacks and gradient attacks that attack gradient descent on personalized federated learning setting for certain convex loss functions. I was really confused by this section. The notion of PAC* learning is about finding a set of personalized models that are close to the set of true models with respect according to some metric. In order to show this, they need some strong assumptions. For instance, they need the loss function to be strongly convex. I find the topic of this paper extremely interesting. Understanding the relation between gradient attacks and data poisoning attacks are very important. Limitations:  The final equivalence result of the paper only applies to simple settings with strongly convex loss functions and seems to only work for GD optimization. The presentation of the work is not in publication stage. It is really hard to understand the main claims of the paper.<|endoftext|>The paper formalizes the concept of PAC* learnability for a generalized personalized federated learning model and provides a sufficient condition. The paper also proposes the counter gradient attack that is effective and data efficient. 2.The equivalence between data poisoning, model attacks, and gradient attacks are carefully analyzed. In general, I think the assumptions used in this paper are strong. It might be clearer to draw a diagram to show under which conditions the claimed equivalences hold, since the claim in the paper title and abstract is fairly strong. Refined analysis with weaker assumptions and clearer presentation would be appreciated.<|endoftext|>This paper reveals the inherent equivalence between gradient and data poisoning attacks in personalized federated learning settings. The authors built this equivalence by constructing a model attack for personalized federated learning models. Experiments are limited to simple datasets. The attack model considered in this paper is somewhat simplistic in that the authors only considered the single strategic user case. The authors demonstrated the relevance of this PAC framework by showing that linear regression and classifications are PAC learnable under this framework. Having further discussions on this aspect would be very interesting. 3.Most of the experiments in this paper are conducted on MNIST and Fashion MNIST datasets, which are relatively simple.
Reject; rating score: 5; rating score: 5; rating score: 8; The authors performed experiments on the ImageNet 1k dataset, showing that the proposed token pooling can significantly improve the flop accuracy trade off over the existing downsampling methods. The authors analyzed the computational cost of vision transformer components in detail. As far as I m concerned, it has an impressive feature in that it adopts clustering algorithms (such as K Means and K Medoids) to reduce the number of tokens. If token pooling is a general method for vision transformers, the paper should apply it to at least the current SOTA vision transformers. 2.As mentioned in the introduction, max pooling and average pooling are widely used downsampling methods. Why not do an ablation study with max/average pooling? If the authors proved that the proposed token pooling could be applicable to other vision transformers besides DeiT, it would strengthen its value. In summary, this work presents a new token downsampling method for efficient vision transformers.<|endoftext|>The paper proposes a token pooling approach based on the K Means algorithm to improve the efficiency of vision transformers. The method is evaluated on image classification, where it reduces the computational complexity by half while maintaining similar performance. Strength:+ The submission is technically sound. In Section 3.3, the analysis on attention as a low pass filter is interesting and makes sense. Weakness:  The K Means clustering method for efficient transformers has been introduced in [1], which narrows the contribution of the paper. The method is only evaluated on the DeiT. Can it apply to other vision transformers, e.g., PVT and Swin? Reducing the number of queries is unfriendly to apply to downstream tasks. It would be better to report the actual throughput and compare the proposed method with the previous dynamic vision transformers, such as [2] and [3]. ArXiv, abs/2106.02034, 2021. [3] Wang Y, Huang R, Song S, et al.Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length[J].<|endoftext|>In this paper, the authors propose a novel token pooling method to reduce redundancies in tokens for recent vision transformers. They analyze the computation cost distribution of vision transformers and the limitations of grid based & score based token downsampling methods. They further formulate a reconstruction loss and optimize it with the token pooling layer. The experimental results based on DeiT show that token pooling improves accuracy while reducing computation cost by a large margin. This idea can be applied to other vision transformers as well. Ablations in the appendix on different clustering, convolution downsampling, models are well written. + Impact on communityVision transformers have been deployed on multiple vision tasks and are being a mainstream solution. Efficient vision transformers are important to be developed and the token pooling proposed in this paper can be used for other vision transformers without major modifications. In the experiment at Appendix F, it seems token pooling is not affected by normalized or unnormalized Q/K vectors. I recommend an acceptance to this paper.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper proposes to leverage classical adversarial training to defend against data poisoning attacks. can provide a better understanding of the performance of the proposed training method. I reviewed this paper before and the following problems have not yet been addressed. If only a very small portion is injected, then it is possible that the proposed training may eliminate the effect from poisoned data. The two parts are not well organized. The current version of the paper does not seem to study the impact of different poisoning rates on the proposed approach. For instance, use subtitle "poisoning attacks" to indicate the discussion on attacks and "poisoning defenses" to indicate the discussion on defenses. This is useful for $L_\infty$ attacks as it is more like classical adversarial training. The results on the semantic patch is poor by the proposed approach. The best result is achieved using image patch, which is actually the baseline CutMix. 2.The method is only effective against $L_\infty$ but not $L_0$ and it is not clarified in the introduction. 3.A baseline defense method is not evaluated in the paper. 4.The number of evaluated models and datasets is small and limited. Or the authors can adjust their threat model and just focus on $L_\infty$ attacks. The observations and experimental results may not be general and applicable to other cases.<|endoftext|>Specifically, the paper shows how one can effectively apply data poisoning attacks in the training loop to successfully defend against potential data poisons. The introduced method is rigorously evaluated by (adaptive) known poisoning attacks. The paper demonstrates that the new defense outperforms existing data poisoning defenses on CIFAR 10. ### Strengths  The introduced method is sounds and a simple adaptation of adversarial training to make it more effective against data poisoning attacks. The paper is well written and flows smoothly  The authors put decent effort in detailing prior work and replicating their results within the same framework to foster reproducibility. What is the takeaway in this section? The presented empirical results are convincing.<|endoftext|>This paper propose to use the idea of adversarial training to defend data poisoning. The idea is interesting and the arguments are convincing. It would be great if the authors can provide more evidence on the novelty of this algorithm, e.g.what difficulties you met when designing this algorithm. For example, what attack does adversarial training tend to defend, and what is the formula of the attack used in Figure 3?<|endoftext|>The proposed training objective for poison immunity is reasonable and well motivated. Is the proposed defense objective in Eq.(4) a natural generalization of the original adversarial training objective in Eq.(1).In other words, is Eq.(1) a special case of Eq.(4)?The authors are suggested to discuss this. If it is not the case, the wording of some parts of the paper, such as the title of Section 3 (i.e., "Generalizing Adversarial Training to Data Poisoning"), may need to be modified to avoid misunderstandings. 2.There are some closely related papers [1, 2, 3], which the authors may want to discuss. Their procedure shares some similarities with this paper. [2] also extended the use of adversarial training for defending against one type of poisoning attacks (they called delusive attacks). Note that those papers are largely concurrent to this paper, so they would not affect the contribution of this paper.
Reject; rating score: 3; rating score: 3; rating score: 6; This paper proposed an effective and scalable algorithm to train the graph neural network. Pros.1.The writing of this paper is clear. The authors claim in Section 3.2.2 that graphon neural networks can be seen as a generative model for GNN. Can we understand that the graphon neural network is only used to generate a small subgraph from the original graph? 2.Previous work. I think some previous works [1,2,3] were also proposed to address the issue of scalability of training a graph neural network model. How is the proposed method compared to them? The settings for the experiments are not convincing to me. However, the size of datasets in the experiments is only ~1k. There is no evidence that the training of the graph neural network has already converged by this epoch.<|endoftext|>This paper shows that a graph neural network (GNN) trained on an increasingly larger graph behaves like a graphon neural network (WNN), the "limit object of a GNN", which arises when considering a continuous graphon instead of a discrete graph (the graphon can be seen as a generative model from which the discrete graph is sampled). The authors then propose an algorithm to train a GNN by gradually adding nodes to the training data, leading to a computational advantage compared to training the GNN on the full dataset from the start. If one of the advantages of the proposed method is to reduce the computational cost of training the GNN, then this advantage should be quantified in the experiments. There are also some similarities between the two papers, down to things like notation, (sub)section names, the content of some sections, and one of the two experiments. 5.One of the main objectives of the paper is to "address the computational burden of training a GNN on a large graph". 6.Figure 1 seems to indicate that there is no difference in how many nodes are added at each epoch, and that having 200 300 nodes is already more than enough for achieving performance similar to when we have 1000 nodes.<|endoftext|>In this paper, the authors propose a method that learns a large graph neural network (GNN) starting from a relatively smaller GNN which would increase its number of nodes step by step (epoch by epoch). This paper also demonstrates its proposed method on a recommendation system and a decentralized control problem and shows reduced computational cost. Pros:1) Reducing the computational burden of training a GNN is important, as in practical use cases, e.g., graphs like social networks usually are huge, and cost many resources to train and fine tune. 3) In general, the main flow of the paper and ideas are clear. Does it mean that the derived proof and conclusion only apply to the case where the input feature at each node has dimension one? 3) Some comments should be added to clarify how Theorem 1 and 2 connect to GNN, i.e., $\bf{\Phi}(\bf{x}_n; \mathcal{H}, \bf{S}_n)$. Further, please give some intuitions/or suggestions for how to choose the number of nodes added in each step. 6.It could be future work, but to me, the graph sizes in the numerical results are not very large.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper talks about a simplification of transformer architectures. The original transformers employ a dense attention between all tokens in a sequence to focus on sequence comprehension. In this work, however, the focus is on the retrieval of the last item. It therefore only evaluates the attention between the proposed candidate item and the preceding items in user history. Perhaps this is okay for recommender systems, when there are not much complexity in the sequential patterns, but the contribution is rather technical than academic.<|endoftext|>This paper focuses on tackle the sequential recommendation task, where the authors proposed Iterative Memory Network (IMN), an end to  end differentiable framework for long sequential user behaviour modeling. The main contribution of the paper is the IMN framework with efficient in memory and complexity. The paper needs to have deeper analysis in explanation of model architecture and not only performance.<|endoftext|>This paper proposes an "iterative memory network" for long user sequence modeling such as those for ctr prediction in ads and recommender systems. Weakness  The major weakness of this paper is evaluation. Otherwise the experiments look like what a paper would do 2 years go. Second, only offline experiments are done on two datasets. The proposed method is intuitive and easy to understand, but may fit an application oriented conference better.<|endoftext|>This paper proposes the Iterative Memory Network (IMN), an end to end differentiable framework for long sequential user behavior modeling. The technical significance and novelty of the proposed model are good. The reviewer tends to accept the paper. Modeling the long sequence is a hard problem in recommendation applications.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; These are interesting and valid results, but I am not confident they are very insightful. We could argue that uncurated data does not match the proposed generative model, but then that is just a case of model misspecification which is a rather well known problem and not directly connected to the theoretical results of the paper. They cover classifiers based on generative models, but there might still be parallels worth discussing. I find this problematic, as at best this is a bad case of text recycling. ### Questions  In section 4.1., it is not clear how the annotators are simulated. Could the authors give more details on how the data was generated? Were the annotations just sampled from the categorical distribution defined by the  true  neural network? This is not entirely clear in the text.<|endoftext|>In this model, the likelihood of a labeled data obtained with consensus is given by product of the probability that each labeler labels correctly. In toy experiments and Galaxy Zoo experiments, authors show that the test likelihood improves when unlabelled curated data is added, but decreases on the uncurated data. These results are potentially interesting. I am not clear on this connection. It does not say what happens when the data is uncurated. On the experiments, it would be nicer to see the consequences of the theoretical results on modern semi supervised learning approaches and datasets they use. The theoretical and experimental contribution of the work are not enough to justify publication at the current form.<|endoftext|>This paper shows that low density separation semi supervised learning (SSL) objectives can be understood as a lower bound on a log probability that arises from a principled generative model of data curation. Experiments on toy data generated from a known model and on real data from Galaxy Zoo confirm the importance of data curation for SSL. Minor comments (language mistakes, etc.):Figure 1 caption: The first ``annotators" should have its first letter capitalized. Section 2.1: "ensure sure" should be "ensure". The paper is generally very well written, clearly presented, and a pleasure to read.<|endoftext|>### Coments on Figure 3* Why in Figure 3 are points labeled "bus" and "train"? The key result is that if unlabeled data is generated without multi annotator curation, then the unlabeled data does not improve classifier accuracy when included in training using SSL. Other SSL principles (e.g.those based on generative models) aren t applicable. I recommend accepting the paper. In the rebuttal and revisions, I hope the authors can address my questions about experimental results and why the bounds say about when each SSL method might be preferred over alternatives. However, there is little insight here about when each bound might be expected to be "the best".... does FixMatch always dominate? This may seem a bit out of scope, but I think offering some analysis here would help practitioners in thinking about which methods to apply in which circumstances. Perhaps the data augmentation related to FixMatch used here is driving this?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The updates of DQN s parameters follows from the tractable approximate Gaussian inference (TAGI) algorithm. Weaknesses:In general, the algorithm seems largely rely on an existing algorithm, TAGI, and the main contribution is extending this to the deep Q learning settings. In summary, this paper studied how to learn DQN in a fully Bayesian way and is interesting. However too many parts of the paper related to its core contribution is unclear at this time. Unless it s significantly updated then this paper is not ready to published.<|endoftext|>The authors present a method for combining deep Q learning with tractable approximate Gaussian inference to allow for deep Bayesian reinforcement learning without the need for gradient based updates. They demonstrate this method can attain performance comparable to standard backprop trained agents in a number of environments. This makes it a little harder to validate the claims made about the underlying TAGI method, since as far as I can see it does not appear to have been peer reviewed yet. I do not however believe that the paper is ready to be published as is since the experimental validation and comparison needs expanding, and further discussion on why exactly this method is so useful compared to the current literature would be useful.<|endoftext|>This paper combines the TAGI algorithm with Q learning algorithm and demonstrates that TAGI DQN can achieve on par performance with its backpropagation based counterpart with fewer hyperparameters. 3.I think the focus of this paper is a little bit unclear to me. In the related work part, it seems the main contribution of TAGI DQN is to propose an analytically tractable inference approach; whereas in the Discussion part, the main focus is to propose a gradient free method compared with its backpropagation counterpart. The definition of A in eq(4) is not the same as the size of action space. 3.I do not see how eq(10) takes the maximization of the expected value as described one line above.<|endoftext|>The paper describes how to adapt the Q learning algorithm so that it is compatible with Bayesian deep learning, as opposed to the standard (semi )gradient based deep learning implementation. Instead, the paper only claims to achieve “comparable” performance, and only compares against one baseline. 2.Demonstrates an alternative implementation of deep Q learning at the scale of Atari games. Is this a fair comparison? I assume these refer to the Jacobian? (Note: each of the numbered weaknesses below corresponds with the same numbered strength above.) 6.I’m confused about the exploration rates for backprop on Atari. Is it standard DQN? I appreciate the time and effort spent in preparing this submission, and I do see some value in the proposed approach. Unfortunately, in its current form, the paper does not have a clear contribution. I recommend rejection.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This work proposes a method where the optimal temporal alignment is learnt by a CNN. The authors provide a good number of experiments for two different settings, however there is no study showing how well the model is able to predict the optimal alignment between two sequences. While this may not be a major concern per se, it seems a little confusing that the method is trained for sequence alignment but it is not evaluated for that. In fact, the alignment here is proposed as a “proxy distance” to be used for other tasks. There is nothing particularly wrong with that, but again this missing study raises some confusion. The choice of CNN as the main model is a little questionable in my opinion. Regarding supervised representation learning, it is not clear why a simple 1 NN classifier was used. This may be a concern since the very nature of the method, temporal alignment, is not evaluated. All my concerns have been well addressed and the paper is know more polished.<|endoftext|>This paper is interesting because it attempts to solve the alignment problem directly by using CNN. (1) and (2), and what we have to do is to find the optimal alignment matrix T*. These are stacked to Ds and fed to a small network g to generate g(Ds). There are some concerns. In p.4, the CNN g has three layers with kernel sizes of 5,5,3, which indicates that the size of the receptive field is about 10 (because the spatial size is kept). However the datasets used in the experiments have much longer sequence lengths, as illustrated in Figure 2. (note: this applies only for the supervised learning, because few shot learning uses only 8 frames)Second, relating to the above, the CNN g might not work as expected. The residual connection D + g(Ds) works as shown in experiments, and it is not clear how much the residual g(Ds) contributes the results. However the proposed concept has not been well studied.<|endoftext|>This paper proposes a sequence distance metric named temporal alignment prediction (TAP) which leverages a CNN to predict alignment between two sequences, in contrast to other non learnable alignment algorithms in prior work. As those prior work of sequence distances focuses on few shot action recognition, it is hard to convince me that TAP is superior than the prior work. As shown in Table 1, TAP does better in terms of MAP, but not as good as the prior work in terms of 1 NN. Although the proposed way of computing distances between sequences is learnable, flexible, and light weighted, the experiments on few shot action recognition could not support the supremacy of TAP over existing methods. The paper also lacks necessary ablation study on the data augmentation used in TAP experiments. Post rebuttal  The authors have addressed my concerns and questions in the rebuttal.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; In this paper, the authors present a discussion on two main stream meta leaners with attentional classifiers and threshold meta learners under a unique view of polythetic classification. And to address the limitations of both, attention based feature selection is introduced. Improved performance is demonstrated by both synthetic and real world few shot learning tasks. **Strengths**This paper discusses meta learner from a very unique perspective, with the shortcomings of both ProtoNet liked threshold meta learners and MatchingNet liked attentional meta learner discussed solidly with intuitive examples and convincing derivation of misclassification rates. The proposed attentional feature selection is simple yet effective, and can potentially guide and stimulate many follow up improvements to meta learning. There are many other directions of meta learning that are completely overlooked in the discussion.<|endoftext|>An attention based feature refinement technique is proposed and evaluated, beating both baselines on specifically polythetic few shot tasks. Issues stem from the perhaps overly concise writing; many aspects of the paper are in need of elaboration. The proposed method performs well in its intended setting. I have some lingering concerns, shared with reviewer q98z, regarding the lack of _demonstrated_ practical benefit, and the fact that accuracy gains in the more “real world” benchmarks shrink substantially relative to the motivating XOR problem. Contrary to claims in the abstract and conclusion, the paper does not show that the embedding space of a threshold classifier must grow exponentially with the number of features. The embedding space does grow exponentially with task complexity alpha, which is itself O(n), but they are not the same and these claims should be clarified (assuming that this is in fact authors’ intended argument). What is the envisioned use case scenario for this kind of approach? What is the envisioned use case for a polythetic meta learner? This section could use some elaboration and clarification.<|endoftext|>This paper discusses monothetic and polythetic classifications in the context of few shot learning: distinguishing similar classes often require reasoning with combinations of certain features, which can be especially challenging when only a few training examples are available. Practical value isn’t clearly demonstrated; improvements over matching networks for the only non toy dataset (TieredImageNet) is marginal at best. Concepts from the paper are well explained with several simple examples, and toy settings are designed to illustrate them empirically. Overall, this paper reads well, and the analysis leads to some interesting insights into few shot learning. The proposed method strongly outperforms the baselines in the toy settings, but improvements over the Matching Networks baseline on TieredImageNet is marginal at best. As such, as nice as these insights are, it’s not clear if it’s practically useful. Additionally, while I understand the focus was comparing with ProtoNets and Matching Networks as representatives of threshold based and attentional classifiers, it would have been nice to have included more baselines in the experiments. The new perspective on meta learning provided by the authors is an interesting one, but the practical benefits of this approach can still be more concretely demonstrated.<|endoftext|>This paper first considers the limitations of threshold and attentional classifiers. They proposed an attention based method for feature selection to address the problems of threshold classifiers and attentional classifiers. The experiments on  several synthetic and real world few shot learning tasks seem good. (2) The motivation on the polythetic classification is well motivated. The proposed feature selection mechanism is too simple and the only technological innovation. (3) Lack the experiments for comparison with threshold classifiers and attentional classifiers in the main paper. (4)  Self attention also has some parameters for the transformation, why is the proposed method non parametric?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; I think that the authors addressed both the issues in their new version of the paper. In particular, the model can be seen as a 3 level hierarchy. Once the hierarchy has been learned, one can tune it on the same or different tasks. The authors propose to use MPO as a reinforcement learning algorithm. The mixture of movement primitives has also been utilized in reinforcement learning, leading to a very similar probabilistic model (with a discrete variable selecting the skill, and the continuous variable selecting the skill s parameters) [7 10]. S2.1: I think that the proposed method is sound. The idea of incorporating information asymmetries, as also noted by the authors, was empirically shown to be also effective. This work does not suffer from this problem. 2.I think that a large part of related work is missing, especially work prior to 2017. WEAKNESSESW1: The readability of this paper can, in my opinion, be improved.<|endoftext|>This paper proposes a three level hierarchy for learning motor skills. For that reason, this paper is definitely worth reading. Thus you are given more flexibility and are possibly able to learn more complex tasks, with the proposed hierarchy compared to earlier methods based on only two levels. The most interesting aspect of the proposed system is the fact that transfer by retraining only at the discrete level can be so advantageous. The experimental section is quite extensive and includes comparisons to other alternative methods, as well as ablation studies. Other than that, the paper is very easy to read and understand, with clarity in both language and notations.<|endoftext|>This paper introduces a latent variable controller model that allows for reusable skill learning in behaviour cloning and reinforcement learning settings. There is a wealth of research around hierarchical mixture latent variable policies and skill discovery which is missing from this work, and reduces the strength of the architecture as a contribution. Post rebuttal comments  Thank you for your response and for the hard work in addressing the feedback provided. I feel the paper is much stronger for this, so have updated my score. Results show that this approach is effective on a range of tasks, and that skill re use (as expected) out performs learning from scratch. Strengths:Hierarchical policies are a great idea, and the proposed model seems like a sensible approach to integrate both discrete and continuous latent variables into a policy, in contrast to many existing approaches which use discrete latent indicator variables to trigger fixed policies. Results show the proposed architecture seems to work, and I was particularly interested in the comparison with a hierarchical behaviour cloning model.<|endoftext|>This paper presents a method to learn a three leveled hierarchy of skills offline, from a dataset of demonstrations, that can then be applied to accelerate reinforcement learning. The experimental evaluation is rather limited and several questions arise with respect to the benefits over previous approaches, the robustness and generality. It encodes a discrete selection, a continuous contextual variable (dependent on the discrete selection) and a low level policy dependant of the continuous contextual variable. It is nice to read (except for some parts of the method that are rushed)  The method is novel, extending prior work  The results show some improvementsWeaknesses:  Structurally, I like papers that push the related work section to the back because they first present some relevant common theoretical components to understand both the presented method and the related work. This is not the case here. The results are somewhat mixed. Could you provide an intuition of what is the different information represented by the categorical and continuous high level latent codes? What is exactly the benefit of the two levels? The method section is a bit rushed. In summary, the paper presents a novel approach with three hierarchical levels for skill learning. The approach is novel, the results are good, the text is well written, and the problem is relevant.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; ~~~~~~~~~~~~~~This paper considers the offline setting of the contextual bandit with neural network function approximation. The key idea of the proposed NeuraLCB is to use neural network to learn the reward function and use a pessimism principle via a lower confidence bound (LCB) for decision making. After its introduction, it has been applied in various off policy RL and off policy bandit settings. 2.This paper is very well written and is easy to follow. Cons: 1.Related to my previous comments on the pessimism principle, the main contribution of this paper is to incorporate the pessimism principle neural contextual bandit setting.<|endoftext|>The paper studies the problem of offline contextual bandits, where policy learning can only leverage a fixed dataset collected a priori by behavior policies. Using a pessimism principle, the authors propose a new algorithm called NeuralLCB with overparameterized neural networks and provide theoretical regret guarantees based on the analysis framework of the neural tangent kernel. Pros:a) The paper provides concrete theoretical analysis to support the proposed algorithm. I have not gone through all the derivations, but the overall result looks good. c) The paper is generally well written. The required assumptions are discussed clearly. b) It is not very clear how the improvement of \sqrt{d} and O(n) is achieved.<|endoftext|>This paper is the first study considers offline policy learning for contextual bandits with neural networks. The authors proposed NeuraLCB algorithm that used neural network to model the rewards and followed pessimism principle with lower confidence bound in policy learning. It is a very intuitive combination. Overall I think this is a good paper with a straightforward idea of combining pessimism and neural bandits for offline policy learning.<|endoftext|>This paper proposes a neural network based contextual bandit algorithm in the offline setting where a dataset of contexts and rewards are given by a logging policy. The goal of the proposed algorithm is to learn an optimal policy from the offline dataset. I agree that this paper makes good progress in connecting neural contextual bandits and offline settings. The difference is that it uses a lower confidence bound for estimating the reward function instead of an upper confidence bound, and that the optimization procedure for learning the neural network representation is based on the loss on one data point instead of the whole historical data.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; The paper presents a method called neural deep equilibrium solver to increase the efficiency in the inference stage for implicit deep models by initializing the equilibrium states using neural network. The authors start with the traditional Anderson Acceleration scheme for fixed point calculation and extend it using neural network initialization and Anderson steps to improve the inference efficiency. Thorough empirical analysis on the property of the solver is well conducted in the paper, showing the overhead for training the solver is minimal compared with the training of the implicit models. This shows that the application of the method is "free". I think the paper would be a good addition to ICLR if the authors can carefully address them. The notations of the paper has some typos. The use of Hyper Anderson Iterations can be more clearly justified. Implicit graph neural networks. The paper is written clearly and is a solid work overall.<|endoftext|>The authors introduce a neural network approach for solving the fixed point equations arising in deep equilibrium models. This consists of a tiny network that provides an initial guess for the fixed point, as well as a small network that computes coefficients inside an algorithm inspired by Anderson iteration. The approach is intuitive and empirical. It seems like the neural solver is learning to imitate the behaviour of a provided solver (given access to z^\ast, which may not actually be a root, nor does a root necessarily exist, nor is it guaranteed to be unique), rather than solve the problem directly.<|endoftext|>The paper proposes to speed up inference of Deep Equilibrium Models (DEQs) by replacing the classic fixed point solvers (Broyden or Anderson Acceleration) by a learned extension of AA. Their approach operates on a pre trained DEQ, and trains a small neural network to propose an initialization and update scheme based on ground truth fixed points. They show pareto improvements across these tasks as compared to standard DEQs, while only adding a ~1% additional training overhead of the DEQ. Is it enough to enforce that the $\alpha_k$ weights sum to $1$? Thanks to this paper, DEQ models seem to be approaching practicality during inference time. I believe this is a very strong paper that significantly pushes the state of the art of DEQs, and should get accepted without reservation (barring something substantial I may have missed).
Reject; rating score: 3; rating score: 3; rating score: 5; This paper  a new self supervised learning (SSL) paradigm for sequence recommendation by  contrastive learning between positive and negative views of sequences based on model augmentation. The model augmentation methods includes neuron masking, layer dropping and encoder complementing. The authors should reorganize the contributions in the paper. The neuron masking model augmentation has been proposed in CL4SRec, but claimed as a contribution in the paper. The proposed methods look a little straightforward and motivations are not supported by analysis.<|endoftext|>This paper proposes three levels of model augmentation methods: neuron masking, layer dropping, and encoder complementing. This work opens up a novel direction in constructing views for contrastive SSL and does experiments to verify the efficacy of model augmentation for the SSL in the sequential recommendation. But the novelty and contributions are limited. Besides, it fails to explain the motivation of the proposed augmentation methods.<|endoftext|>The paper is trying to address the sequential recommendation problem in which the goal is to predict the next items in user behavior. The paper proposes 3 levels of model augmentation methods: neuron masking, layer dropping and encoder complementing. Have the authors given much thoughts about the differences between model augmentation and model regularization? Are they similar concepts? The paper addresses an important research problem and shows that a few model augmentation techniques can help with the sequential recommendation performances. My main concerns about the paper is the lack of explanation for the differences between model augmentation and regularization techniques.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper studies the problem of PCA with a generative model setup. The algorithm they analyze is a projected power method (PPower), similar to iterative hard thresholding for the case when $w$ is sparse. I think this is an interesting paper and vote to accept it, though it would be nice to see the experiments for the questions I ve asked.<|endoftext|>My major concerns are now addressed, and I am happy to assign an imporved score. This implies that the training of $G$ and its performance is not independent, and this undercuts the theoretical results presented in a major way. The problem is important and the idea of replacing sparsity with a generative model is very interesting. The paper is clearly written.<|endoftext|>This work studies PCA under the contraint that the principal eigenvectors lie in the range of some fixed generative model, e.g., a neural network with fixed weights. Strengths:  The paper studies an important and well motivated problem and provides sample complexity bounds for an interesting setting. The authors show two main results.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; To complete this authors don t provide any reproducibility statement. There s no way to verify the correctness of the result in the paper or to reproduce any of the results if the paper is accepted.<|endoftext|>We suggest that this is at least partly due to a limitation of the Transformer architecture"But later, the paper does "exploit existing Transformer architectures" ... "without modifying the underlying architecture". So, do the authors argue that the Transformer architecture inherently limited or not? I also agree that some of these address learning graph algorithms (and not python programs), but I would also expect a conceptual comparison and discussion. This limits the evaluation mostly to short programs and poses scaling limitations on the approach itself. This is not discussed in the paper. The models with scratchpads are trained with the full program execution trace, while the baselines are not. Is it a fair comparison?<|endoftext|>Do we know how much this affects the results, independent of the extra data from CodeNet? The authors demonstrate improvements compared to direct execution for all of these settings. The CodeNet paragraph says: "We additionally improved our tracing technique to allow tracing programs with errors; when an error is reached, the error message is added to the end of the trace text and tracing is stopped". There could be more systematic analyses of the settings where the idea works well and where it does not, beyond the ones already in the paper.<|endoftext|>The current state of the art in this domain is weak, and the authors show an incredibly simple method here that leads to big performance gains. I am strongly in favor of accepting this work! The main scratchpad idea presented here is new and interesting, not only to people who care about executing programs with LMs but also to the language modeling community at large. I think this opens up a lot of possibilities that were not available before. 3.The ideas presented in this paper are simple and easy to reproduce. So the results are biased towards shorter programs.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper introduces a novel technique for the analysis of biomedical signals. The technique differs from wavelet transform by the fact that instead of using a fixe wavelet function, a dedicated invertible neural network is learnt for each frequency subband. Finally, features from each subbands are fused using a Transformer like model. The proposed techniques has been thoroughly tested on separate datasets and separate tasks, and compared to various state of the art techniques, and the proposed technique has consistently outperformed other techniques on these various tasks. The authors suggested an innovative solution for the analysis of biomedical time series.<|endoftext|>This paper introduces T Wavenet, a fused approach including both classic feature engineering and recent advance of deep learning, in an attempt to improve the state of the art analysis of time series. In this way, some concrete analysis should be added about some unique domain specific properties of such signals, and the author would also justify why the proposed approach would be able to learn the representation of these properties more effectively. Post rebuttal comments:My concerns have been mostly resolved given the update and additional experimental results. Disadvantages:+ The biggest concern I have is that, the author claims "Such a disentangled representation learning method facilitates a more effective extraction of the discriminative features, as demonstrated by our extensive experiments on various real life time series classification datasets."<|endoftext|>The authors proposed T WaveNet, a novel tree structured wavelet neural network for time series signal analysis. It utilizes the dominant frequency range to extract informative representation from raw signals. The tree structure network consists of invertible neural networks as a frequency bisection operator, and of a feature fusion module. This is quite an interesting approach for the real world time series signal analyses, and seems promising in terms of accuracy.<|endoftext|>The authors introduce a deep learning approach to classify time series. In particular, a wavelet based algorithm is carried out using invertible neural networks and a formant based strategy. Finally, different databases are tested, showing an interesting performance for non stationary pattern coding. Overall, the paper is clear and easy to follow. Moreover, the experiments dñemonstrate convincing evidence regarding the time series discrimination results. The loss function in eq 11 depicted an L2 regularization; how to deal with outliers? A good paper is presented founded on a deep learning based representation strategy that can be used as a data driven wavelet representation holding attention mechanism.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper addresses the problem of sound event detection and localization from multi channel raw audio waveforms. The experiments are interesting but not conclusive enough to understand the root cause of improvements. Through ablation studies the authors show that this feature extraction scheme performs better (albeit marginally) compared to some of the traditionally used feature extraction schemes like MFCC, and LFBEs. The premise of the paper that using a more appropriate Filterbank which learns to choose different time frequency resolution tradeoffs based on data is indeed interesting. The authors have made a nice effort in figuring out the details and their experimental evaluation. Personally I think using a significantly larger backbone of transformers on the proposed features to show improvement was not a fair comparison. 2.It appears that the authors have reported single / best model accuracy numbers and it is not clear how reliable these numbers are   perhaps reporting average metrics  + standard deviation across multiple model runs (with random initialization) would be better ? 4.The paper can benefit greatly from re writing the motivations and editing. This is not making sense to me. needs revision. d) In Eq.(5) what is l (ell) ?<|endoftext|>I am sorry, by mistake I posted the incomplete review version! This manuscript proposes a sound source localization method utilizing "synperiodic filter" bank representation as to the input of a deep convolutional neural network. Authors claim that the convolution of the proposed filterbanks with the raw waveform helps to achieve multi scale perception in the time domain which results in performance improvement. NOTE, they have proposed a new combination of feature sets and new model structures. 1  The writing of the manuscript requires improvements, there are many ambiguous sections/ paragraphs that require more elaboration. It is not clear how the training/testing is conducted. It would be more helpful if the authors could elaborate more on some implementation details. However, they have not discussed/supported this claim in the experimental section. I like the main idea in the paper, however, I think this work requires some minor improvements to be in proper shape for this proceedings.<|endoftext|>This submission addresses the sound source detection problem. Compared to previous method, the authors propose a novel synperiodic filerbank compared to syndistance bank, which learns a data dependent time frequency resolution map. Experiments show that the proposed method achieves state of the art performance on several datasets. **Note**: I am not an experienced researcher in the sound source detection. My reviews may miss some published results or technology. ## Strengths+ The paper is clearly written, with sufficient technical details. Overall, I think this submission has solid novelty and state of the art performance on the sound source detection task. However, my comments may not be professional and I would wait for authors response before making the final decision.<|endoftext|>This paper proposes a method for improving the detection and the localization of several sound sources using a 4 channel signal (The addressed configuration, determined or under determined case is not detailed in this paper). The proposed approach is based on a so called "synperiodic" filter bank representation that is used as the input of a deep convolutional neural network. For all the reasons reported below, I cannot recommend to accept this paper in the current form, however I encourage the authors to submit again an improved version of their work. I) the overall organization of the paper should be improved. Despite the presented "good" results, the experiments are not reproducible since relevant information related to the protocol and the implementation are missing. This point is important  since it is a possible new contribution claimed by the authors. The authors should explain rigorously what is "synperiodicity" and "syndistance" with consideration with previously established theoretical results. III) Despite good claimed results, the experiments are not sufficiently detailed to be reproduced. Do the authors have reimplemented the compared state of the art methods or did they only reported results from the literature ? Can please provide more details about the implementation of your method, used language and framework, providing implementation considerations and/or with source code ?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; The proof is elegant, the result is significant (providing a matching upper bound, asymptotically), and the writing is clear. It seems likely that the used proof techniques are useful for the proof of relevant theoretical results on the expression power of neural networks.<|endoftext|>They also prove a lower bound on number of parameters and bit complexity showing that this is optimal. The paper is well written and the proof sketches provide sufficient intuition.<|endoftext|>This paper studies memorization capacity of deep ReLU networks. It also carefully discusses bit complexity and shows that the construction is also optimal in terms of the number of bits required (comment 2).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper focuses on a very particular kind of lossy compression, which they call "cross domain lossy compression". As such, this scenario puts together signal compression and denoising/super resolution, so that I can definitely say it comes with interesting applications. Technically, the authors show that the problem can be nicely formulated as a special kind of optimal transport under entropy constraints, which gives an interesting flavour to their contribution and much theoretical grounding for their proposed method. The authors consider a scenario where an encoder observes some signal X and transmits it to a decoder as a quantized version ^X. At the decoder, this signal is transported into another signal called ^Y, from which the reconstruction Y is computed. In this setup, the very nice contribution of the paper is to express the whole problem as a particular instance of optimal transport, that indeed appears as the right framework to modify X~Py so that it becomes representative of Py. With this in mind, the "compression" part comes into the picture as the introduction of a latent variable ^X with limited entropy. Having some personal background in source coding, I personally found the whole story very appealing and the derivations look very well grounded. The experiment look rather weak in my opinion, and I would definitely have liked a much stronger support for the theory, possibly showing me the applicability of the method on real life problems/scales. Still, the few experiments that are provided are ok and look promising already. Before providing my detailed feedback, I must hence say that I recommend acceptance of this paper. My understanding is that optimal transport in this case is not trivial at all. I see two main ways to circumvent the problem: 1. However, I would be curious at what happens when they are not. It turns out I worked in such cases of "informed coding" where both coder and decoder were sharing information that is dependent on the signal to convey, and I kind of feel that the framework you propose could be compatible with this kind of ideas still. I am asking because that sentence had me confused: do we agree that even if we assume it is random, we do assume that U is available at both coder and decoder (in the shared noise setting) ? Is there no way to relax this to delve into the domains of differential entropy ? I would be curious about what you think would be the next steps in a direction that would relax this discretization assumption* importantly, I can see that "in practice, we do not impose a hard equality constraint" on entropy. So actually, it looks to me that you are *not* implementing your "entropy constrained" OT, but rather some entropy penalty. Although I understand it goes it the same direction, all your previous derivations seem to be violated by these practical choices. I guess it would be very important to at least hint at how this particular choice is under optimal in the rate distortion setups you developed ? I guess that this fact makes previous work about Sinkhorn distances more related to your work than explained in the paper, right ? The connection with optimal transport is definitely interesting. There is much to do regarding validating the proposed approach in real large scale applications.<|endoftext|>The paper explores the problem of learned compression scheme for the specific setup where the source and target distributions are not the same (such as in restoring a degraded image). The authors formulate the problem as an optimal transport problem with entropy constraints to control the compression rate. They further argue that the use of common randomness in the encoder and decoder improves the rate distortion tradeoff and dcoument it on denoising and super resolution experiments. (+) pros / ( ) cons (+) well written and developed paper (+) topic interesting and important, addressed innovativelyFurther questions for clarification/discussion Binary example is hepful but the use case is for more complex (continuous) X and Y distributions. Would it be possible to find an illustrative example in the continuous space? Minor text problems / typos Page 1, 7 lines from bottom: application  > applicationsPage 6, 2 lines from bottom: Q  > Q is a quantizerI find the paper contributes innovatively to the field nds is worth the attention of the community. Hence I recommnend it for acceptance for the conference.<|endoftext|>This paper addresses the problem of lossy compression when the source and target distributions differ. As example domains, the authors look at denoising (the distribution of noisy images and "clean" images are not the same) and super resolution (again, the distribution of low res and high res images are different) both addressed as unsupervised learning problems, i.e.the noisy/clean and low/high res training data are unpaired. The problem is treated as optimal transport (to account for the domain shift) with an entropy bottleneck (to account for the rate constraint needed to achieve compression). The authors derive theoretical bounds for the achievable distortion in two scenarios (with and without a source of shared randomness between the sender and the receiver), and they show that a shared source of randomness strictly improves rate distortion performance. The Wasserstein distance used in a Wasserstein GAN framework accounts for the distribution difference between the input and output domains. I don t see any issues with the Theorems 1 3, but I did not work through the derivation provided in the appendix. The motivation also makes sense to me since there may be benefits to treating restoration (denoising, deblurring, super resolution, etc.) In some sense this seems like the typical case, rather than a rare one, since many sensors will include some kind of signal degradation and only store / transmit a compressed representation of the captured data (e.g.the camera in a mobile phone). That said, it shouldn t be to difficult to get paired data from such a device meaning that a supervised approach can be used. I assume the sender and receiver agree on a pseudo random number generator ahead of time and some kind of seed is transmitted, after which both sides can generate the same U. It was not clear from the paper how the results compare to a baseline method that does *not* model restoration and compression jointly. I think this would look something like: learn to upscale low res mnist images without a rate constraint (in an unsupervised fashion), and then separately learn to compress the high res reconstructions. The paper addresses a difficult and interesting problem, and I see no problems with the math.<|endoftext|>This work considers compression and restoration of degraded input in a joint framework, and formulated this problem as an oprimal transport with a constraint on the rate. The theorems shown in this work look interesting. For example, Theorem 1 indicates that, without common randomness, the optimal solution can be decomposed with compression (quantization and dequantization) and transport. Authors also demonstrate that common randomness is helpful at reducing the rate for a given distortion. Experimental results can corroborate the theorems. I am just a bit confused by the term "cross domain". I was thought it means different data sources with distinct spatial structure and appearance. My only suggestion is that, it seems Wang et al.(2021b) also model the shift in distribution due to degradation as an optimal transport problem. It would be much better if authors can specify more differences with respect to this work. Authors also discussed how to implement compression and restoration jointly by a deep neural network.<|endoftext|>* Characterize the optimal transport under the distortion and the output probability distribution constraints. * Provide DNN methods to evaluate the above quantities. Strength: Extend the conventional concept of the optimal transport to handle perception (constraint on the output probability distribution). (2) Theorems 1 and 3 seem logically inconsistent with each other. Detail of (1): in the footnote of page 3 says that any DISCRETE RV Z can be losslessly compressed to at most H(Z)+1 bits. The pointis Z being DISCRETE. In order for Definitions 2 and 3 to have practical significance, the RVs to be compressed mustbe discrete and finite support, in particular, Z in Definitions 2 and 3 must be finite support. The problem formulations do not seem relevant for practical data compression,Detail of (2): In the problem formulation (Definition 3) and its solution (Theorem 3), there is no condition on the common randomness U. Taking H(U) 0, i.e., U 0 always should reduce Definition 3 to Definition 2 and Theorem 3 to Theorem 2, but it does not. There seems to be some unstated minimally required entropy on U, i.e., H(U) must be larger than some unstated constant number. For example, page 5 states "This is in contrast to the case without common randomness in Theorem 1 where the reconstruction Ymust be generated at the decoder." Is this true even with U of H(U)   1 / 1,000,000,000?? The relevance of the paper to practical data compression is unclear, and it seems that mathematical quantities without practical relevanceare considered. The stated theorems look false.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper describes a white box attack on ASR systems using TTS. Since this component is responsible for prosodic and other non segmental qualities of speech synthesis, the expectation is that the naturalness of the synthesis will not be impacted, but the ASR recognition will be modified. The most interesting component of this paper is the fact that adversarial attacks can be constructed by manipulation of the prosodic characteristics of synthetic speech. However, there are a number of unanswered questions about this adversarial attack. There is no assessment of TTS quality either of the un modified initial utterance (that should be recognized correctly by the target ASR model) or the adversarial utterance. This is a limitation especially in comparison to audio dependent attacks (ADA) which sound like natural speech with a small amount of background noise. For this attack to be completely successful the synthetic audio should be indistinguishable from human speech. However, the contribution here is rather narrow. It is interesting, but not especially consequential, that CVAE manipulation can be used to localize the contribution of an adversarial attack.<|endoftext|>A conditional variational auto encoder (CVAE) based speech synthesiser is trained based on a combined connectionist temporal classification (CTC) and regularization loss. The framework is new, and the results are promising. However, the scientific depth seems a bit shallow. (2) Additional experiments should be conducted to confirm the effectiveness of the proposed approach. On the other hand, the SSA framework directly generates audio samples from the text. Thus, an additional experiment is required: using another ASR system, trained from other datasets or constructed by another model architectures (such as LAS or hybrid CTC/Attention), to recognize the adversarial audio samples synthesized by SSA. Please conduct additional experiments and include the results in the paper. To us, the comparison does not provide useful information. (6) It is very nice that the authors provide demo samples on a website. The quality is worse than the ones of (Carlini & Wagner, 2018). The promising results confirm the effectiveness of the proposed SSA to attack the ASR system. The paper is well written, and the theoretical part should be correct.<|endoftext|>The paper presents a new adversarial attack mode to ASR systems based on speech synthesis. The main idea is to synthesize speech utterances that naturally sound like ground truth transcription y_o, but can fool an ASR system. I wish that the paper is better justified as to when this kind of attack can be a real threat, instead of relying on the reader s own conjecture. The targeted attack results are substantially better than the other models. Weakness:  It is not too clear to me what is the real world attack scenario based on the proposed method. There have been other adversarial models that are based on synthesized audio. I understand the difference between the proposed model and these existing models. The paper might benefit from additional justification. While the reviewer also observed that the samples uploaded on the website are convincing, I believe that a large scale evaluation on (a) sound quality of the synthesized speech (b) human annotator s transcription is necessary. The authors indeed pointed out in the conclusion section that there are unnatural examples they observed. However, it is still not too clear if it is the best way. The proposed method is based on the white box attack.<|endoftext|>To this end, the authors use a conditionalvariational auto encoder as a speech synthesis (TTS) model and develop a adaptive sign gradient decent algorithm in order to solve their SSAoptimization problem. Experiments are provided showing the effectiveness of their approach, along with a detailed analysis of results. The paper is well written, clear, the problem stated concisely, ample referencesare provided, and the experiments are convincing. The quantitative presentation is good and the inclusion of their adaptive signgradient decent algorithm may be useful for further researchers. The authors show the validity of their SSA approach by demonstrating its superiorability to attack an ASR system. The only minor point is that there was no MOS done on samples the authors generated. And, as the authors point out, and as this reviewer was able to listen,some of the audio samples do not sound well.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; Major issues with clarity, algorithm design, and general structure of the writeup make it difficult to endorse this work. The paper touches upon orthogonalization of gradients, which is actually something that has already been explored in the literature   see for example https://arxiv.org/abs/2001.06782. However, the authors  claims do not hold up to serious vetting. (1) The CIFAR baselines are incredibly weak.<|endoftext|>However, many of my concerns remain:* The performance of the baselines is still very low compared to number typically seen in the literature for similar experimental setups, which indicates that something is either missing from the presentation (to explain why they are so different), or that there is a technical problem with the experiments. The work proposes a novel idea, but the presentation, evaluation, and analysis is not yet sufficient to warrant publication. * Perhaps I should have been more clear: I would like to see discussion about *why* orthogonalising gradients will have a different effect to orthogonalising weights. * The motivation for introducing this method is contradicted by the experimental results, so it is unclear why/when one might see benefits from using this approach.<|endoftext|>This paper proposes to orthogonalize the gradients of each neuron (within a layer) in order to improve training dynamics. I recommend authors to use strong baselines, otherwise it is very difficult to convince the reader whether this method is useful in practice. As explained briefly in the summary, orthogonality is widely studied in neural networks. (5) Orthogonal basis is normalized, which affects the magnitude of the gradients; therefore learning rate should be searched for SGD and proposed method separately using a validation set. (6) "...for Orthogonal SGDM we see that the components’ parameters are more similar than they were with SGDM and even include the initial spike.<|endoftext|>The conducted experiments do not support the author s main claim of improved training speed. This should be measured on the training loss. I am recommending a reject. Nevertheless, the proposed idea seems interesting and I strongly urge the authors to study neural network optimization papers that look into the convergence speed of deep networks. The authors should tune the method they compare against as well as their own method since hyperparameters are crucial for training deep nets   since this is not done thoroughly in the paper, interpretations of the conducted experiments are difficult. Experiments seem not to align with questions asked by the authors.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This work presents a new neural network model that can learn some symbolic relations in a specifically designed neural network module, which can later generalize to other unseen data. The proposed method was evaluated on datasets MNIST and BlockStacks to demonstrate its potentials. Specifically, I had significant trouble identifying the relationship between sections 2 4 and section 5. The theory, definition, and discussion in sections 2   4 are interesting and I enjoyed reading it. But it seems to be disconnected from section 5. To make sure my understanding is correct, I would like to ask a few questions  what is the main contribution of sections 2   4? how the theoretical description in these sections can be connected to the neural model proposed in section 5? on the other hand, if there is a connection, to what extent the model proposed in section 5 can realize those concepts discussed. For example, how to make sure an $\epsilon$ consistency of sub structure in the model? Furthermore, in figure 2, if my observation is correct, the proposed model performs much better on the out of domain data (nearly 100% accuracy on the BlockStacks data) than on the in domain dataset (MNIST), how should we understand this difference?<|endoftext|>This paper proposes a loss based on theory for the relation prediction problem, using the sentence of the relationship as the supervision. The paper designs an relation transfer learning experiment. The thesis theoretically and rigorously expressed the relation prediction problem in the framework of universal algebra, which is inspiring. This loss links the different relationships together and is a more general and universal form. It seems interesting. From Equation 8, the loss of a sentence seems to be the product of the loss of each relation in the sentence. For example, isGreater can be represented by the conjunction of several isSuccessors, but it is not feasible to list all these sentences when the domain is larger. 3.About Definition of Equation 9: I don’t understand why we need to calculate expectation over s. The two terms in the formula seem to have been summed over s, and have nothing to do with s.4. The paper lacks an explanation for the motivation of DC. Why is DC better? 5.The experiment aims to show that by supervising one relation, other relations can be transferred. First,  the motivation of the work is not clear. Second, the proposed method is straightforward and short of novelty.<|endoftext|>This work introduces a more general definition of structures namely soft structures that can be used to learn structure over richer domains in a data driven manner. Further, they introduce suitable definitions of coherence and consistency for the proposed soft structures. The key argument of the paper is that transfer is to be expected across shared relations despite their overloaded usage across domains — for example, greater than (GE) can have completely different interpretations in the source / target domains. Strengths:  The paper makes multiple simple yet interesting contributions. The experimental results, at least with the DC model, are consistent with the hypothesis of the work. The paper is generally well written and is easy to follow. Weaknesses:   While intuitive and backed by experimental results, the experiments feel nascent and it’s not clear whether the same trends will hold for more complex datasets / relations. Further, it introduces a neural architecture that jointly learns element and relation representations   the experimental results do back the hypothesis of the paper. However, the experiments have a limited scope and therefore, I am inclined but not very strongly towards acceptance.<|endoftext|>I.e., train on the BlockStack domain and then evaluate on MNIST? Section 3:  "Z is a latent variable, itself draw from marginal p_z"  > "draw" should be "drawn". Strength  * The paper is very well written and the organization is clear. * I find the way in which the authors develop a formal framework, and then provide a quantitative account for this in terms of soft structure, which are then used to train a Variational Auto Encoder very appealing. Section 4:  Overall I found this section quite difficult to follow and I wasn t sure the details were relevant (but I could be wrong here). You could consider removing some technical details here (and moving them to the appendix), so that you have more space for possible additional experiments. * The topic is also interesting and relevant to AI. It is not explained what an "atomic subformula" is. "Coherence" seems to be an important topic of the paper. The grounding $s_{ijk}$ is used but only in the next paragraph it is explained what it means, so please define it beforehand. However, in the empirical results this didn t seem to be mentioned anymore. The main weakness is that the empirical results felt somewhat underdeveloped, so I would recommend swapping out some of the technical details (e..g, Section 4) for more analysis on the empirical studies (see main review for more details). Maybe it did, but then it wasn t clear to me, so in any case I think the relation between the experiments and Section 3.1 can be made clearer. It was difficult for me to understand what it did. More importantly: it seems to outperform all other models very convincingly, but I could not find any discussion on why you believe this is the case. 3.The empirical studies by itself are also somewhat one sided (MNIST  > BlockStack).
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper is concerned with fairness in recommendations. Previous work has modelled fairness as the constraint of all user groups having the same accuracy or as the prediction probability being independent of the item group or the user group. In this work, the notion is generalized so that the prediction is independent of both the item and user group. Optimization algorithms are shown to solve this problem along with experimental results. If there were significant technical issues it does not seem to me that they were well emphasized in the paper. Have the authors thought about including any theoretical guarantees. While the paper introduces a new meaningful notion of fairness.<|endoftext|>The paper defines a mutual information based mathematical expression to measure unfairness, called equal experience metric, and then optimize it along with a matrix factorization based collaborative filtering. In the spirit of the above question, I would like the authors to elaborate on their criticism of fairness notions based on the difference in recommendation accuracies (Paragraph 2 of Introduction). Weaknesses: The normative goal of the fairness notion is not well motivated in the paper. The paper tackles an important problem of fairness in recommender systems, and it defines a fairness notion that contains both the user groups and items groups.<|endoftext|>The presented study argued that a fair recommendation should be independent of both user and item. Therefore, the study introduced a new fairness notion, i.e., equal experience, and further incorporated this fairness notion as a regularisation term in the matrix completion framework to construct a fair recommender system. The presented study did not analyse the diversity of the recommended items in the experimental results, which the new proposed fairness notion aims to improve. The new fairness notion introduced by the presented study can provide certain new knowledge and insights on how to construct a fair recommender system, and the effectiveness of the proposed recommendation method seemed to be supported by the experimental results.<|endoftext|>This paper proposes a new notion “equal experience” to measure the fairness among groups in recommender systems, and provides an method to optimize this new notion based on matrix completion. Experiments demonstrate the effective of the proposed optimization framework. Strengths:+ The paper is well written and easy to follow. + It is interesting to utilize mutual information for fairness problems.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper proposes a joint trajectory prediction framework. This means that the model produces multi modal future predictions at the scene level (i.e., for all actors jointly). I identify two main contributions in the paper: (i) hierarchical and sparse heatmap output representation and (ii) a recombination method that is pluggable in any multi modal marginal prediction model. The paper criticizes generative models in the second paragraph of the related work for not providing a probability value for each prediction, which makes me expect the method to provide such scores. However, there is no mention about it in the approach or experiments sections. 2021.I think this paper has significant contributions in terms of the presented method.<|endoftext|>The paper proposes a multi modal trajectory prediction pipeline. With those end points a trajectory can be inferred for each agent in a collisionfree way by leveraging all heat  maps jointly. With those trajectories, multiple, consistent scene level trajectories are predicted. The main contribution of the paper is on the reasoning about the end pointsgiven by the predicted heat map. The difference between this method and GOHOME was  not clear to me in this sense. #### A single dataset evalution might not be enough for the community to access the value of this paper. Motion prediction is currently a very competitive scene.<|endoftext|>This paper addresses the task of multi agent multi modal trajectory prediction in the context of autonomous driving. For end point estimation, this paper proposes hierarchical iterative refinement from a probability heatmap, with collision aware greedy sampling to generate collision free multi agent trajectories. To further produce scene consistent multi agent multi modal trajectories, the paper also proposes a modality combination ranking module that re orders the modality of each agent. The method largely follows previous works, with little technical contributions. In the end of introduction section, the authors summarize three main contributions:i) an efficient graph based model. Despite good results on the leaderboard, given the limited technical novelty without solid supporting evidences, I do not recommend this paper for acceptance at current status.<|endoftext|>This work proposes THOMAS, a hierarchical heatmap refinement scheme for multi agent trajectory forecasting. I share the same overall sentiment as Reviewer PgCY, the paper may not have a single "major" novelty, but the set of presented incremental contributions are sufficient to convince me of the performance and utility of this work over prior approaches, and I believe this paper is now more suitable for publication.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; Also, key results and presentation of experiments is not clear. The main idea is that models trained on data from some distribution should be robust to distribution shift. The paper empirically evaluates the connection between the generalization ability of a model and its privacy (using the MI metric). For example in Figure 1 it’s not easy to see the correlated between OOD accuracy and MI attack accuracy.<|endoftext|>Section 4 contains more empirical results to explain the empirical results in section 3. The paper studies the correlation between MI attack accuracy and OOD accuracy on synthetic and real world datasets. For example, the in domain generalization accuracy is not shown.<|endoftext|>The authors performed extensive experiments to study their connections. The empirical results also shows that robustness to MI attacks can be a good indicator of whether the learned features are stable. This work studies the connection between OOD generalization, stable features and privacy (particularly robustness against membership attacks) through extensive experiments. However, it provides evidence on the relationship between the three factors. The main concern is that the experiments do not control the factor of the algorithms being used.<|endoftext|>This paper presents an extensive empirical study to show the connection between out of distribution (OOD) generalization, privacy and stable features using SOTA domain generalization methods. The results shows that there is no direct relationship between better OOD generalization and privacy. The authors evaluated SOTA methods on various datasets and found some patterns in the correlation between MI attack accuracy and OOD accuracy. It is important for privacy community, but I m not sure if it is complete without any theoretical claim.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The authors just give the proof for the very simple edge cases like linear and non linear sine models. How about the classification case that this paper is targeted for? These simple cases are only true when the feature extraction network is kept unchanged during training. 2.Empirically, the experiments are conducted in a limited and counter intuitive: (a) When testing on CIFAR 10, it is quite unintuitive that the authors train on the test set and evaluate on the training set. To address the lack of training examples since the test set is used for training, they use the pretrained model on ImageNet; (b) When testing on the synthetic dataset relabeled from ImageNet, the information of the probability of the label assigned by a pretrained model is used as a proxy for the confidence of a human annotator. My suggestion is that the authors should collect a real world large scale dataset such that the PI is insignificantly expensive to obtain along with the main annotations to prove their claim empirically. However, the authors have not given any proof for the general case (not the edge cases) theoretically or empirically successfully.<|endoftext|>This paper proposes a method, TRAM, that integrates the privileged information into the learned network weight through weight sharing at training time and approximately marginalizes over the privileged information at test time. 3. the proposed method can (in principle) apply to any neural network model and has zero overhead at prediction time. However, the latter investigations only focus on the accuracy improvement without an in depth discussion about how and how much the label noise can be "explained away." 2. there lacks quantitative (or even qualitative) evidence about how, how much, and what kind of privileged information is transferred through weight sharing in realistic deep neural network models. Experiments on both realistic and synthetic datasets show that TRAM can help improve the model accuracy on noisy data.<|endoftext|>The manuscript proposes classification methods utilizing privileged information (PI, which is available only at training time). Another main point is to learn the representation (\pi(x)) of the features with access to PI at the training time. To do this, the proposed methods employ knowledge transfer by weight sharing. The proposed methods include several variants by adopting heteroscedastic classification (Het TRAM) or distillation (Distilled TRAM). The proposed methods have simple architectures (not requiring specific modules, e.g., Gaussian dropout [Lambert et al., 2018], for the marginalization). 2.Making prediction for a test point (PI is not available) has the same cost with a standard network trained without PI. It seems that the analysis just supports that PI can be useful. In the introduction, it is stated “We provide empirical evidence suggesting that the representations learned with access to PI are more robust against label noise”.<|endoftext|>This paper develops a simple and efficient method for training with privileged information and testing without privileged information. The training with privileged information focuses on transferring via sharing the weights of the encoder, which is the knowledge learned with privileged information. The corresponding solution is to marginalize over privileged information at test time. The proposed framework uses privileged information in deep neural networks efficiently. 2.The method is well demonstrated with both theoretical analyses and numerical experiments. Thus this work could contribute a new idea to learning with label noise. For example, in Page 2, in addition to saying "Denoting by $\prod_x$ the orthogonal projector associated with $X$", it would be better to show the equation as well. In Page 3, $u$ in $q(y|x,a;u)$ is not clearly defined.It is also confusing why $a \cdot u$ is the noise term. An ablation study that compares the performance of fine tuning the last layer using clean data with fixed feature extractors (trained with PI or not) would be helpful. The paper proposes a new perspective on the noisy learning literature.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper is well written, presents a marked improvement over the baselines provided (I’m not sufficiently familiar with the text adventure game literature to be certain those represent state of the art, but I will assume they do unless corrected), and provides an interesting approach to the exploration problem through the two policy architecture. All in all it seems that the proposed strategy here could work on a wider range of environments than addressed in the paper. Can the authors comment on why they chose the self imitation approach instead?<|endoftext|>Pros:The paper is generally well written and easy to follow. The method itself is novel and the empirical finding in this paper might be particularly interesting for the audience of text based RL.<|endoftext|>However, if other co reviewers are fine with it, I m fine too. 2, the authors mentioned that "Note that the action distribution over actions $a$ induced by $\pi_{inv dy}$ is conditioned only on the current observation $o$". As a consequence, to my understanding, the contribution of this paper is the two phase pipeline and the sampling strategies in Section 3.1.2.<|endoftext|>I think the results of these algorithms should also be included in the main table, and I think this can further support the main arguments of the paper. Finally, the authors demonstrated the outperforming results in the Jericho environment. This paper is well motivated and most parts are well written, but the main method section is written to be difficult to follow. The results demonstrate empirical gains in the Jericho environment.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposes a representation analysis technique to particularly understand what information is contained in self supervised models. The authors propose a visualization technique based on conditional diffusion models that seeks to synthesize realistic looking inputs whose representation matches that of a target image. 2.Visualizations generated using the proposed technique look promising in addressing the question laid out in the study. is going to rank better on the fidelity scale..? And the perceptual quality of image generation is not likely a good reflection of its faithfulness to the representation. Further, the conclusions that are drawn using this conditional image syntheses approach could just as well have been drawn by analyzing the representations directly (for instance, by seeing how well the scale of images can be decoded from the representation). I enjoyed reading the paper. However, I m uncertain about how much value this paper adds in efforts towards understanding the nature of self supervised representations and the results are not very interesting.<|endoftext|>This work introduces develops a method for sampling different natural images that a pretrained encoder maps to the same or similar representation. To do this, the authors propose a modification to the diffusion model of Dhariwal & Nichol for sampling conditioned on a given encoded representation $h f(x)$ that is well suited to conditioning on high dimensional vectors (i.e., representations). This new method is used to qualitatively compare encoders trained via a number of methods (supervised, DINO, SimCLR, Barlow Twins, VICReg etc.). This paper considers a very worthy problem, that of analyzing encoders via visualizations, and clearly develops a high quality generative modeling approach with which to do this. I think this paper may be on its way to being a vey nice paper, however I have concerns with it’s current state. Current strengths include:  high quality of generative models used (the generated samples are high fidelity). But I do think it is therefore important to demonstrate the method s usefulness in analyzing and understanding pretrained models. This is not a problem in itself. Consequently, I am currently not in favor of acceptance. I am unlikely to raise the score to an accept without significant updates to the work, or strong arguments in favor from other reviewers. All results presented are qualitative and visual in nature (with the exception of a sanity check that the generative models are producing photorealistic samples). A lack of demonstration of useful applications for the proposed method. E.g.specifically, could you develop some metric based on your visualization method that correlates well with (e.g.,) segmentation performance.<|endoftext|>This paper proposes a technique for visualizing the representations of fixed pretrained self supervised neural networks. More specifically, by using a conditional denoising diffusion probabilistic model (DDPM, coined _RCDM_ here), a learned neural network representation $h$ can be mapped (back) to image space (i.e., the space of "natural images") and, by drawing multiple samples conditioned on the same $h$, the invariances learned by the (self supervised) network can be visualized. Experiments with different pre trained models show that the method is indeed able to synthesize high quality natural images corresponding to the learned representations. By demonstrating that gradient based methods do not suffice (Sec.2) to reconstruct _natural_ images, the authors show the need for a probabilistic approach and, by opting for a conditional diffusion model, provide a method capable of generating high quality images corresponding to the representations, as demonstrated in the experimental section. Following on from this: the paper should demonstrate the need for a probabilistic approach by comparing, for example, with a simple regression baseline $x   f_{\theta}(h)$. The proposed approach to do this by synthesizing "natural" images generated by a generative model conditioned on these learned representations (here a diffusion model) is intuitive and reasonable.<|endoftext|>This paper proposes a conditional diffusion model that can be used to visualize representations learned by SSL or supervised models. The proposed Representation conditioned Diffusion Model (RCDM) can generate images that are both close in the representation space to a given image, and looking realistic. **Strength**: This work provides a visualization tool for understanding NN representations. The proposed RCDM is not technically challenging (it is adapted from ADM in the prior work), but it works well for probing the information in NN representations without sacrificing generation quality. The experiments cover different SSL methods and the comparison with supervised learning, showing results on 1) in/out of distribution generation, 2) interpolation, 3) super resolution, 4) unconditioned generation, 5) algebraic manipulation. The relation and comparison with prior work is discussed adequately. The method in this paper is unsurprising but gives good results. I find the experiments thorough and insightful, and would recommend an accept.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; The authors introduce an abstraction of self supervised learning (SSL) algorithms in the federated setting, which they term FedSSL. Algorithm 1 suggests the answer, but I encourage to make this more clear in the text. My major issue with the paper is that its empirical scale is quite small in terms of what makes the FL scenario challenging. Post Rebuttal  The authors have addressed my concerns sufficiently and I have updated my score accordingly. I would encourage the authors to explore that angle or clearly position their work in terms of which niche of the heterogeneous  FL space they target. I encourage them to evaluate some experimental insights on larger data sets such as Imagenet. A such, it is a very general statement. E.g.can you do centralised performance with small batch sizes?<|endoftext|>It seems some baselines such as FedMoCoV1 and FedMoCoV2 were missing from Table 7. This is an empirical paper. The fine tuned version is shown to have significant performance improvement. Here, the main contribution of the paper is a series of ablation studies (Table 1, Figure 2, Table 2, Figure 3) that measure the isolated impact of several fundamental components of the aforementioned SSL blocks (e.g., SimCLR, SimSiam, MoCo and BYOL). For Tables 3 4, how would it impact performance in the IID setting when lambda is set to 1 instead? Could the authors comment more on this? EXPERIMENTThe experiment is quite extensive & interesting. But I do have a few follow up questions:In Tables 3 and 4, what are FedEMA s performance on IID setting when lambda   1.<|endoftext|>FedEMA is built onto of FedSSL which is a framework for self supervised learning in a Federated Learning context. The authors have demonstrated that the approach is superior by a quite significant margin emperically on CIFAR 10 and CIFAR 100. But W itself has not been defined in the paper. Overall an interesting idea. For example more experiments could have been run and it would be good to theoretically show why this works.<|endoftext|>This paper combines self supervised learning framework BOYL and FedAvg to improve the performance of unlabeled non IID datasets. Unsupervised FL is a well motivated and timely topic. This paper is a simple combination of BYOL and FedAvg. I cannot see significant novelty in both BYOL and FedAvg like algorithm. 2.Why is SimSaim [2] not used as the SSL framework? According to Section 3.3, the authors use CIFAR 10 and CIFAR 100 to partition the dataset in an unbalanced manner with respect to the number of classes in each client. data" is required in this work. 4.Note that FL clients should not be stateful. The newly sampled client do not have any cached states to conduct adaptive optimizer or moving averaging like stateful algorithms [4]. 6.The source code is not provided for reproducibility. 7.It would be better to discuss the privacy/security concern of the proposed FedSSL framework.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This work proposes a previous knowledge channel attention module (PKCAM) that captures channel relations across the different layers to help enhance feature representation. However, the authors claim that Y is a feature vector with dimension of C_0. However, I can not find the definition of f_2(). There are many reference errors throughout the paper, e.g., fig. The experiments are not sufficient and convincing.<|endoftext|>This paper computes channel attention by considering feature maps across different layers, namely previous knowledge channel attention module (PKCAM). The technical contribution of this work seems limited.<|endoftext|>The proposed PKCAM mainly aggregates the feature maps of earlier CNN blocks for aggregating previous knowledge. This paper is not well prepared for review with many incomplete and inconsistent expressions, e.g., the Fig.?? Besides, the implementation of PKCAM is not special and actually the commonly used spatial and channel attention. Thus the paper has limited technical novelty. The paper is not ready for review, also the contribution and analysis are limited. Thus, I vote for rejecting.<|endoftext|>The proposed method is called previous knowledge channel attention module, whose aim is to make use of the knowledge from the previous layers. This paper proposes an attention module with the knowledge from the previous layers also included. I have two concerns about this idea. This idea is too intuitive. Try to polish the paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; At its heart, the paper explores which inductive biases (and image representations) enable CNNs to generalize better across 2D transformations including rotations, translations and scale variations. Same is true for rotation. PMLR.6.Chaman, A. and Dokmanic, I., 2021. While the paper proposes some interesting ideas (iteration, log polar convolutions) and attempts to understand generalization across 2D transformations in a rigorous manner, it is currently not mature enough for publication. The reason behind training a CNN capable of these transformations might be that such models would then have an understanding of this transformation just as humans have, and then we can hope that the representations learned by these models can generalize on downstream tasks. However, this has not been explored, or sufficiently discussed in the paper currently. 3.Paper is very well written. It is easy to follow and understand. Placing this work in the context of existing literature in empirical analysis of invariances is essential to make its contributions clear. All experiments are conducted on a very simple toy dataset, despite several more complex datasets existing: The paper suggests the use of this dataset generation pipeline because it provides them complete control over the training and test distributions, interpretable manipulations and computational speed. In is unclear how well these findings extend to more complex data, which significantly reduces the utility. This presents a problem, as implementing this approach on any other dataset would first require segmenting the objects to obtain the boundary control points.<|endoftext|>The paper proposed POLARAE, an auto encoder that operates in log polar space. The paper demonstrated the model, together with iterative learning, is better than simple baselines in o.o.d generalization for a set of affine transformations. 2.Experiments shows that operating in log polar space does have in o.o.d generalization for affine transformations. I suggest authors to use at least 2 or 3 datasets (such as the Shape dataset) to prove that their method s effectiveness is generalisable to other datasets. There are many more VAE variations that can be compared. 3.The paper has limited novelty. 4.Many design choices seem arbitrary without proper explanations. 3.Kim, Jinpyo, et al."CyCNN: a rotation invariant CNN using polar mapping and cylindrical convolution layers." More baselines should be compared against on more datasets.<|endoftext|>Though the authors start the paper with psychology literature and discussions, they fail to really establish connections between their proposed methods and the literature. The key contribution of this paper is to propose to utilize convolution, high training diversity, iterative training, and log polar space for improving the generalization ability of AE models for OOD settings. The authors demonstrate its usefulness on a toy task. For example, the authors write in the paper "...we hypothesize that performing convolutions in log polar space would help in the OOD generalization of rotation and scaling", but they never provide any, not even distantly related, support from psychology. I don t know why the authors do not use the 3D dataset used in [https://arxiv.org/abs/1709.0188](https://arxiv.org/abs/1709.01889](https://arxiv.org/abs/1709.01889),), which seems more difficult than the one used in this work. The writing of this paper seems sloppy and needs to be improved significantly. The observations in this paper are not likely to generalize to realistic settings that we really care about.<|endoftext|>The authors trained autoencoders and variational autoencoders in cartesian space, as well as autoencoders in log polar space, with generated data representing a range of interesting transformations. In particular, their data generation strategy is flexible and can produce a large variety of images with different rotations, scales, and translations. The authors test the ability of the models to extrapolate beyond the transformations present in training, and identify 4 ways to improve generalization ability (iterative training, diversity, convolutions, log polar coordinates) The authors show that some strategies are effective for translation, but more is needed to address rotation and scaling. The paper is well written and interesting to read. Weaknesses: Though the data generation setup and test environment is interesting and unique, it s not clear how effective the techniques will be on more complicated realistic datasets. There s also prior work showing iterative training is useful for addressing domain adaptation https://arxiv.org/abs/2002.11361. It would be interesting to test if log polar coordinates can help OOD generalization on ImageNet. My score reflects my view that the data generation process and test environment that the authors propose are important and novel contributions.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors employ ResNet family and DEIT family as the counterparts of CNNs and ViTs. This paper is working on an important question on the capacities of ViT and CNN models for medical image tasks2. As we all know, there are some major differences between medical images and natural images, in terms of the graphic patterns, scenarios, densities of labels, etc.. As a result, replacing CNNs with ViTs will have different impacts on medical and natural image tasks in different ways. We would like to see more information or rational on this. 4.The methodology of this paper lacks of novelty.<|endoftext|>Overall, the authors should re evaluate their conclusion. I provide suggestions for what might be more appropriate below. In addition, I think ICLR is not the most fitting venue for this sort of medical imaging experimentation paper, but that may well be debatable and I am happy to be convinced otherwise. However, I believe the authors would have a much bigger impact with such analyses at CVPR or MICCAI. As the authors say, a lot of the work in medical image analysis is on segmentation and similar tasks (registration, super resolution, synthesis, etc), unlike CV where classification is by far the dominant task. Similarly, there are a slew of very popular segmentation tasks, like the BRATS challenge (menze 2014) or the Medical Decathelon (Antonelli 2021).<|endoftext|>Use of more challenging datasets for segmentation: brain segmentation would be a good benchmark since it has more challenging structures and a lot more classes than the ones used. I think this paper is not ready for publication in a top venue like ICLR. This paper does not present any technical novelty since it is an empirical paper. I agree that empirical papers can contribute a lot to the community as well, but it requires deep analysis of the results. The experiments are very coarse only comparing the average performance of two models in classification and segmentation of medical imaging.<|endoftext|>This work studies the feasibility of replacing CNNs with ViTs for medical imaging analysis on the task of classification and segmentation. It sheds light on the feasibility of replacing CNNs by the recent vision trends of using ViTs for medical imaging task. The implication that ViTs outperform CNNs in this segmentation task cannot be validly drawn from an 0.2% difference with larger variance. Overall, it is a good starting point to explore the vision transformers in the medical imaging task. However, some preliminary conclusions are not well supported. So I would like to recommend a weak reject as of now.<|endoftext|>Experiments are conducted on a number of medical image benchmark datasets with both CNNs and transformers for both classification and segmentation. It would be interesting if the authors could shed some light on this observation. I think it is important to see if nnU Net outperforms DeiT in similar settings. If not, the authors should give a good reason why that comparison is not necessary. I am also satisfied with most of their responses to other reviewers as well. As the main contribution of the paper is empirical, this becomes a main issue.