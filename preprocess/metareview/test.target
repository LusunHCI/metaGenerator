The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it s clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term  UCB , as mentioned in an anonymous comment, is somewhat misleading. "Approximate Confidence Interval" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >  0 or 1). 
All reviewers believed that the novelty of the contribution was limited.
The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising. There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper. I also ask the authors to the result figures easier to read. The chosen colors are not ideal and the use of log scale x axis is not standard. Finally, including DAgger in the same plot is confusing assuming that DAgger user more information.
This is an interesting application area, but the quality of the presentation and experimental work here is not sufficient for acceptance. The numerical ratings from reviewers are just not high enough to warrant acceptance. 
The authors have proposed a method for imitating a given control trajectory even if it is sparsely sampled. The method relies on a parametrized skill function and uses a triplet loss for learning a stopping metric and for a dynamics consistency loss. The method is demonstrated with real robots on a navigation task and a knot tying task. The reviewers agree that it is a novel and interesting alternative to pure RL which should inspire good discussion at the conference.
This is a borderline paper.  The reviewers are happy with the simplicity of the proposed method and the fact that it can be applied after training; but are concerned by the lack of theory explaining the results.  I will recommend accepting, but I would ask the authors add the additional experiments they have promised, and would also suggest experiments on imagenet.
Pros   Shows alternative strategies to train low rank factored weight matrices for recurrent nets.  Cons   Minor modifications (and gains) over other forms of regularization like L2.   Results are only on an ASR task, so it’s not entirely clear how they’ll work on other tasks.  As pointed out by the reviewers, unless the authors show that the techniques generalize well to other tasks, and larger datasets it hard to accept it to the main conference. The AC, therefore, recommends that the paper be rejected. 
This paper proposes a method for performing stochastic variational inference for bidirectional LSTMs through introducing an additional latent variable that induces a dependence between the forward and backward directions.  The authors demonstrate that their method achieves very strong empirical performance (log likelihood on test data) on the benchmark TIMIT and BLIZZARD datasets.  The paper is borderline in terms of scores with a 7, 6 and 4.  Unfortunately the highest rating also corresponds to the least thorough review and that review seems to indicate that the reviewer found the technical exposition confusing.  AnonReviewer2 also found the writing confusing and discovered mistakes in the technical aspects of the paper (e.g. in Eq 1).  Unfortunately, the reviewer who seemed to find the paper most easy to understand also gave the lowest score.  A trend among the reviewers and anonymous comments was that the paper didn t do a good enough job of placing itself in the context of related work (Goyal et. al, "Z forcing") in particular.  The authors seem to have addressed this (curiously in an anonymous link and not in an updated manuscript) but the manuscript itself has not been updated.  In general, this paper presents an interesting idea with strong empirical results.   The paper itself is not well composed, however, and can be improved upon significantly.  Taking the reviews into account and including a better treatment of related work in writing and empirically will make this a much stronger paper.  Pros:   Strong empirical performance (log likelihood on test data)   A neat idea   Deep generative models are of great interest to the community  Cons:   Incremental in relation to Goyal et al., 2017   Needs better treatment of related work   The writing is confusing and the technical exposition is not clear enough
All reviewers agree that the proposed method is novel and experiments do a good job in establishing its value for few shot learning. Most the concerns raised by the reviewers on experimental protocols have been addressed in the author response and revised version.
The reviewers agree that the manuscript is below the acceptance threshold at ICLR.  Many points of criticism were evident in the reviewer comments, including small artificial test domain, no new methods introduced, poor writing in some places, and dubious need for DeepRL in this domain.  The reviews mentioned a number of constructive comments to improve the paper, and we hope this will provide useful guidance for the authors to rewrite and resubmit to a future venue.
The authors make an experimental study of the relative merits of RNN type approaches and graph neural network approaches to solving node labeling problems on graphs.   They discuss various improvements in gnn constructions, such as residual connections.  This is a borderline paper.  On one hand, the reviewers feel that there is a place for this kind of empirical study, but on the other, there is agreement amongst the reviewers that the paper is not as well written as it could be.  Furthermore, some reviewers are worried about the degree of novelty (of adding residual connections to X).  I will recommend  rejection, but urge the authors to clarify the writing and expand on the empirical  study and resubmit. 
 Pro:    Interesting approach to tie together reinforcement Q learning with CNN for prediction and reward function learning in predicting downstream effects of chemical structures, while providing relevant areas for decision making.  Con:   Datasets are small, generalizability not clear.   Performance is not high (although performance wasn t the goal necessarily)   Sometimes test performance is higher than training performance, making results questionable.   Should include comparison to other wrapper based combinatorial approaches.   Too targeted an appeal/audience (better for chemical journal)
The paper has some potentially interesting ideas but it feels very preliminary. The experimental section in particular needs a lot more work.
The reviewers agree that the idea of utilizing covariance information in the few shot setting is interesting. There are concerns with the novelty of the paper, as well as the correctness in terms of ensuring the covariance matrix is PSD in all cases. There are some concerns with the experimental evaluation as well. In this area, Omniglot is a good sanity check, but other baseline datasets like miniImagenet are necessary to determine if this approach is truly useful.
This paper presents a bias/variance decomposition for Boltzmann machines using the generalized Pythagorean Theorem from information geometry. The main conclusion is that counterintuitively, the variance may decrease as the model is made larger. There are probably some interesting ideas here, but there isn t a clear take away message, and it s not clear how far this goes beyond previous work on estimation of exponential families (which is a well studied topic).  Some of the reviewers caught mathematical errors in the original draft; the revised version fixed these, but did so partly by removing a substantial part of the paper about hidden variables. The analysis, then, is limited to fully observed Boltzmann machines, which have less practical interest to the field of deep learning. 
This work introduces a new type of structured attention network that learn latent structured alignments between sentences in a fully differentiable manner, which allows the network to learn not only the target task, but also the latent relationships. Reviewers seem partial to the idea of the work, and it s originality, but have issues with the contributions. In particular:    The reviewers note that the gains in performance from using this approach are quite small and do not outperform previous structured approaches.
The paper received mostly positive comments from experts. To summarize:  Pros:   The paper provides complex counterparts for typical architectures / optimization strategies used by real valued networks. Cons:   Although the authors include plots explaining how nonlinearities transform phase, intuition about how phase gets processed can be improved.   Improving evaluations: Wisdom et al. computes log magnitude; real valued networks may not be suited for computing real / complex numbers which have a large dynamic range, like the complex spectra. So please compare performance by estimating magnitude as in Wisdom et al.   Please add computational cost, in terms of the number of multiplies and adds, to the final version of the paper.  I am recommending that the paper be accepted based on these reviews. 
The reviewers agree that while the presented result looks interesting, it is but one result. Further, one of the reviewer finds this to be a weak comparison as well. The novelty of the approach over the paper by Ba et. al. also is in question   good results on multiple tasks might have made it worth exploring, but the authors did not establish this to be the case convincingly.
This paper tries to establish that LSTMs are suitable for modeling neural signals from the brain.  However, the committee and most reviewers find that results are inconclusive.  Results are mixed across subjects.  We think it would have been far more interesting to compare other types of sequence models for this task other than the few simple baselines implemented here.  It is also unclear what is the LSTM learning extra in contrast with the other models presented in the paper.
This paper proposes a new generative model that has the stability of variational autoencoders (VAE) while producing better samples. The authors clearly compare their work to previous efforts that combine VAEs and Generative Adversarial Networks with similar goals.  Authors show that the proposed algorithm is a generalization of Adversarial Autoencoder (AAE) and minimizes Wasserstein distance between model and target distribution. The paper is well written with convincing results. Reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights.  Overall, the paper is strong and I recommend acceptance.
This paper addresses the question of how to solve image super resolution, building on a connection between sparse regularization and neural networks. Reviewers agreed that this paper needs to be rewritten, taking into account recent work in the area and significantly improving the grammar. The AC thus recommends rejection at this time. 
Overall I agree with the assessment of R1 that the paper touches on many interesting issues (deep learning for time series, privacy respecting ML, simulated to real world adaptation) but does not make a strong contribution to any of these. Especially with respect to the privacy respecting aspect, there needs to be more analysis showing that the generative procedure does not leak private information (noting R1 and R3’s comments). I appreciate the authors clarifying the focus of the work, and revising the manuscript to respond to the reviews. Overall it’s a good paper on an important topic but I think there are too many issues outstanding for accept at this point.
The authors propose a distillation based approach that is applied to transfer knowledge from a classification network to non classification tasks (face alignment and verification). The writing is very imprecise   for instance repeatedly referring to a  simple trick  rather than actually defining the procedure   and the method is described in very task specific ways that make it hard to understand how or whether it would generalize to other problems.
Three reviewers recommended rejection and there was no rebuttal to overturn their recommendation.
This paper introduces a new dataset and method for a "semantic parsing" problem of generating logical sql queries from text.  Reviews generally seemed to be very impressed by the dataset portion of the work saying "the creation of a large scale semantic parsing dataset is fantastic," but were less compelled by the modeling aspects that were introduced and by the empirical justification for the work.  In particular:    Several reviewers pointed out that the use of RL in particularly this style felt like it was "unjustified", and that the authors should have used simpler baselines as a way of assessing the performance of the system, e.g. "There are far simpler solutions that would achieve the same result, such as optimizing the marginal likelihood or even simply including all orderings as training examples"    The reviewers were not completely convinced that the authors  backed up their claims about the role of this dataset as a novel contribution. In particular there were questions about its structure, e.g. "dataset only covers simple queries in form of aggregate where select structure" and about comparisons with other smaller but similar datasets, e.g.  "how well does the proposed model work when evaluated on an existing dataset containing full SQL queries, such as ATIS"  There was an additional anonymous discussion about the work not citing previous semantic parsing datasets. The authors noted that this discussion inappropriately brought in previous private reviews. However it seems like the main reviewers issues were orthogonal to this point, and so it was not a major aspect of this decision. 
This paper combines multiple existing ideas in Bayesian optimization (continuous fidelity, use of gradient information and knowledge gradient) to develop their proposed cfKG method.  While the methodology seems neat and effective, the reviewers (and AC) found that the presented approach was not quite novel enough in light of existing work to justify acceptance to ICLR.  Continuous fidelity Bayesian optimization is well studied and knowledge gradient + derivative information was presented at NIPS.  The combination of these things seems quite sensible but not sufficiently novel (unless the empirical results were *really* compelling).  Pros:   The paper is clear and writing is of high quality   Bayesian optimization is interesting to the community and compelling methods are potentially practically impactful   Outperforms existing methods on the chosen benchmarks  Cons:   Is an incremental combination of existing methods   The paper claims too much
The paper presents an interesting extension of the SkipThought idea by modeling sentence embeddings using several document structure related information.  Out of the various kinds of evaluations presented, the coreference results are interesting   but, they fall short by a bit (as noted by Reviewer 2) because they don t compare with recent work by Kenton Lee et al.  In summary, the idea provides an interesting bit on building sentence embeddings, but the experimental results could have been stronger.
This paper extends last year s paper on PATE to large scale, real world datasets.  The model works by training multiple "teacher" models   one per dataset, where a dataset might be for example, one user s data   and then distilling those models into a student model. The teachers are all trained on disjoint data. Differential privacy is guaranteed by aggregating the teacher responses with added noise.  The paper shows improved teacher consensus by adding more concentrated noise and allowing the teacher to simply not respond to a student query.  The new results beat the old results convincingly on a variety of measures.  Quality and Clarity: The reviewers and I thought the paper was well written.  Originality: In some sense this work is incremental, extending and improving the existing PATE framework.  However, the extensions and new analysis are non trivial and the results are good.  PROS: 1. Well written though difficult in places for somebody like myself who is not involved in this area. 2. Much improved scalability to real datasets 3. Good theoretical analysis supporting the extensions. 4. Comparison to related work (with a new comparison to UCI medical datasets used in the original paper and better results)  CONS: 1. Perhaps a little dense for the non expert   
While novelty is not the main strength of this paper, there is consensus that presentation is clear and the experimental results are convincing. Given the practical importance of designing and benchmarking methods to compactify deep nets, the paper deserves to be presented at ICLR 2018.
The reviewers agree that the method is simple, the results are quite good, and the paper is well written. The issues the reviewers brought up have been adequately addressed. There is a slight concern about novelty, however the approach will likely be quite useful in practice.
Paper presents a way in which linear RNNs can be computed (fprop, bprop) using parallel scan. They show big improvements in speedups and show application on really long sequences. Reviews were generally favorable.
In this work reviewers use structured attention as a way to induce grammatical structure in NMT models. Reviewers liked th motivation of the work and found experiments mostly well done. However reviewers found the paper a bit difficult to follow, with several commenting that distinctions made between the different sub types of attention were not clear. Mainly the reviewers were not overwhelmed by the results of the work, saying that these gains, while clearly isolated to the use of structure were not significantly large. Additionally there were some concerns about the claimed novelty of the work, particularly compared to Liu and Lapata and other use of syntax in translation, and also which aspects were new or necessary. 
The paper proposes a method for learning convolutional networks with dynamic input conditioned filters. There are several prior work along this idea, but there is no comparison agaist them. Overall, experimental results are not convincing enough.
The paper addresses the problem of learning a teacher model which selects the training samples for the next mini batch used by the student model. The proposed solution is to learn the teacher model using policy gradient. It is an interesting training setting, and the evaluation demonstrates that the method outperforms the baseline. However, it remains unclear how the method would scale to larger datasets, e.g. ImageNet. I would strongly encourage the authors to extend their evaluation to larger datasets and state of the art models, as well as include better baselines, e.g. from Graves et al.
The proposed neural tree transduction framework is basically a combination of tree encoding and tree decoding. The tree encoding component is simply reused from previous work (TreeLSTM) whereas the decoding components is somewhat different from the previous work. They key problems (acknowledge also by at least 2 reviewers):  Pros:   generating trees input under explored direction (note that it is more general than parsing as nodes may not directly correspond to input symbols)  Cons:   no comparison with previous tree decoding work   only artificial experiments   the paper is hard too read (confusing) / mathematical notation and terminology is confusing and seems sometimes inaccurate (see R3)    
 + interesting novel extension of equilibrium propagation, as a biologically more plausible alternative to  backpropagation, with encouraging initial experimental validation.    currently lacks theoretical guarantees regarding convergence of the algorithm to a meaningful result    experimental study should be more extensive to support the claims
Pros:   Use of Bloomier filters for lossy compression of nets is novel and well motivated, with interesting compression performance. Cons:   Does lossy compression for transmission, doesn’t address FLOPS required for runtime execution. A lot of times, client devices do not have enough cpu to run large networks (title should be udpated to mean compression and transmission)   Missing results for full network, larger deeper network.  Overall, the content is novel and interesting, so I would encourage the authors to submit to the workshop track. 
All three reviewers recommended rejection and there was no rebuttal.
Pros: + Clearly written paper.  Cons:   Limited empirical evaluation: paper should compare to first order methods with well tuned hyperparameters, since the block Hessian free hyperparameters likely were well tuned, and plots of convergence as a function of time need to be included.   Somewhat limited novelty in that block diagonal curvature approximations have been used before, though the application to Hessian free optimization is new.  The reviewers liked the clear description of the proposed algorithm and well structured paper, but after discussion were not prepared to accept it primarily because (1) they wanted to see algorithmic comparisons in terms of convergence vs. time in addition to the convergence vs. updates that were provided; (2) they wanted more assurance that the baseline first order optimizers had been carefully tuned; and (3) they wanted results on larger scale tasks. 
Pros: + Builds in important ways on the work of Sagun et al., 2016.  Cons:   The reviewers were very concerned that the assumption in the paper that the second term of Equation (6) is negligible was insufficiently supported, and this concern remained after the discussion and the revision.   The paper needs to be more precise in its language about the Hessian, particularly in distinguishing between ill conditioning and degeneracy.   The reviewers did not find the experiment very convincing because it relied on initializing the small batch optimization from the end point of the large batch optimization.  Again, this concern remained following the discussion and revision.  The area chair agrees with the authors  comments in their OpenReview post of 08 Jan. 2018 "A remark on relative evaluation," and has discounted the reviewers  comments about the relative novelty of the work.  It is important not to penalize authors for submitting their papers to conferences with an open review process, especially when that process is still being refined.  However, even discounting the remarks about novelty, there are key issues in the paper that need to be addressed to strengthen it (the 3 "cons" above), so this paper does not quite meet the threshold for ICLR Conference acceptance.  However, because it raises really interesting questions and is likely to provoke useful discussions in the community, it might be a good workshop track paper. 
I think the model itself is not very novel, as pointed by the reviewers and the analysis is not very insightful either. However, the results themselves are interesting and quite good (on the copy task and pMnist, but not so much the other datasets presented (timit etc) where it not clear that long term dependencies would lead to better results). Since the method itself is not very novel, the onus is upon the authors to make a strong case for the merits of the paper    It would be worth exploring these architectures further to see if there are useful elements for real world tasks   more so than is demonstrated in the paper    for example showing it on tasks such as machine translation or language modelling tasks requiring long term propagation of information or even real speech recognition, not just basic TIMIT phone frame classification rate.  As a result, while I think the paper could make for an interesting contribution, in its present form, I have settled on recommending the paper for the workshop track.   As a side note, paper is related to paper 874 in that an attention model is used to look at the past. The difference is in how the past is connected to the current model. 
The pros and cons of this paper cited by the reviewers (with a small amount of my personal opinion) can be summarized below:  Pros: * The method itself seems to be tackling an interesting problem, which is feature matching between encoders within a generative model  Cons: * The paper is sloppily written and symbols are not defined clearly * The paper overclaims its contributions in the introduction, which are not supported by experimental results * It mis represents the task of decipherment and fails to cite relevant work * The experimental setting is not well thought out in many places (see Reviewer 1 s comments in particular)  As a result, I do not think this is up to the standards of ICLR at this time, although it may have potential in the future.
This paper seeks to integrate tensor based models from physics into machine learning architectures. The two main objections to this paper are first that, despite honest (I assume) efforts from the authors, it remains somewhat hard to understand without substantial background knowledge of physics. Second, that the experiments focus on MNIST and CIFAR image classification tasks, two datasets where linear models perform with high accuracy, and as such are unsuitable for properly evaluating the claims made about the models in this paper. Unfortunately, it does not seem there is sufficient enthusiasm for this paper amongst the reviewers to justify its inclusion in the conference.
The paper proposes learning the prior for AAEs by training a code generator that is seeded by the standard Gaussian distribution and whose output is taken as the prior. The code generator is trained by minimizing the GAN loss b/w the distribution coming out of the decoder and the real image distribution. The paper also modifies the AAE by replacing the L2 loss in pixel domain with "learned similarity metric" loss inspired by the earlier work (Larsen et al., 2015).  The contribution of the paper is specific to AAE which makes the scope narrow. Even there, the benefits of learning the prior using the proposed method are not clear. Experiments make two claims: (i) improved image generation over AAE, (ii) improved "disentanglement".  Towards (i), the paper compares images generated by AAE with those generated by their model. However, it is not clear if the improved generation quality is due to the use of decoder loss on the learned similarity metric (Larsen at al, 2015), or due to the use of GAN loss in the image space (ie, just having GAN loss over decoder s output w/o having a code generator), or due to learning the prior which is the main contribution of the paper. This has also been hinted at by AnonReviewer1. Hence, it s not clear if the sharper generated images are really due to the learned prior.  Towards (ii), the paper uses InfoGAN inspired objective to generate class conditional images. It shows the class conditional generated images for AAE and the proposed method. Here AAE is also trained on "learned similarity metric" and augmented with similar InfoGAN type objective so the only difference is in the prior. Authors say the performance of both models is similar on MNIST and SVHN but on CIFAR their model with "learned prior" generates images that match the conditioned upon labels better. However this claim is also subjective/qualitative and even if true, it is not clear if this is due to learned prior or due to the extra GAN discriminator loss in the image space   in other words, how do the results look for AAE + a discriminator in the image space, just like in the proposed model but without a code generator?  The t SNE plots for the learned prior are also shown but they are only shown when InfoGAN loss is added. The same plots are not shown for AAE with added InfoGAN loss so it is difficult to know the benefits of learning the code generator as proposed.  Overall, I feel the scope of the paper is narrow and the benefits of learning the prior using the method proposed in the paper are not clearly established by the reported experiments. I am hesitant to recommend acceptance to the main conference in its current form.  
This work is incremental compared to previous work, solving very specific challenges, and would probably appeal to only a very limited fraction of ICLR s audience. 
This paper proposes to combine Depthwise separable convolutions developed for 2d grids with recent graph convolutional architectures. The resulting architecture can be seen as learning both node and edge features, the latter encoding node similarities with learnt weights. Reviewers agreed that this is an interesting line of work, but that further work is needed in both the presentation and the experimental front before publication. In particular, the paper should also compare against recent models (such as the MPNN from Gilmer et al) that also propose edge feature learning. THerefore, the AC recommends rejection at this time. 
The paper presents Simple Recurrent Unit, which is characterised by the lack of state to gates connections as used in conventional recurrent architectures. This allows for efficient implementation, and leads to results competitive with the recurrent baselines, as shown on several benchmarks.  The submission lacks novelty, as the proposed method is essentially a special case of Quasi RNN [Bradbury et al.], published at ICLR 2017. The comparison in Appendix A confirms that, as well as similar results of SRU and Quasi RNN in Figures 4 and 5. Quasi RNN has already been demonstrated to be amenable to efficient implementation and perform on a par with the recurrent baselines, so this submission doesn’t add much to that.
 + Empirically convincing and clearly explained application: a novel deep learning architecture and approach is shown to significantly outperform state of the art in unsupervised anomaly detection.    No clear theoretical foundation and justification is provided for the approach    Connexion and differentiation from prior work on simulataneous learning representation and fitting a Gaussian mixture to it would deserve a much more thorough discussion / treatment. 
The idea of universal perturbation is definitely interesting and well carried out in that paper.
General consensus among reviewers that paper does not meet criteria for publication.  Pro:   Improvement over the original IDP proposal.   Some promising preliminary results.  Con:   Insufficient comparison to other methods of network compression,   Insufficient comparison to other datasets (such as ImageNet)   Insufficient evaluation on variety of other models   Writing could be more clear
All three reviewers were positive about the paper, finding it to be on an interesting topic and with broad applicability. The results were compelling and thus the paper is accepted. 
All reviewers are unanimous that the paper is below threshold for acceptance.  The authors have not provided rebuttals, but merely perfunctory generic responses.  I think the most important criticism is that the approach is "very ad hoc."  I would encourage the authors to consider more principled ways of automatically designing reward functions, like for example, Inverse Reinforcement Learning, in which you start with a good agent behavior policy, and then estimate a reward function for which the behavior policy maximizes the reward function.
The reviewers were quite unanimous in their assessment of this paper.  PROS: 1. The paper is relatively clear and the approach makes sense 2. The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks. 3. Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation. 4. The multi stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.  CONS 1. Lack of novelty e.g. wrt to Finn et al. in "Deep Spatial Autoencoders for Visuomotor Learning" 2. The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem. 3. The contribution on reward shaping would benefit from a more detailed description and investigation. 4. There is concern that results may be specific to the chosen task. 5. Experiments using real robots are needed for practical evaluation.
The paper presents a way of training 1bit wide resnet to reduce the model footprint while maintaining good performance. The revisions added more comparisons and discussions, which make it much better. Overall, the committee feels this work will bring value to the conference.
The paper reports experiments where a LSTM language model is pretrained on a large corpus of reviews, and then the produced representation is used within a classifier on a number of sentiment classification datasets.  The relative success of the method is not surprising. The novelty is very questionable, the writing quality is mixed (e.g., typos, the model is not even properly described). There are many gaps in evaluation (e.g., from the intro it seems that the main focus is showing that byte level modeling is preferable to more standard set ups   characters / BPE / words). However, there are (almost) no experiments supporting this claim. The same is true for the  sentiment neuron : its effectiveness is also not properly demonstrated. In general, the results are somewhat mixed.  Pros:   good results on some datasets Cons:   limited novelty   some claims are not tested / issues with evaluation   writing quality is not sufficient / clarity issues   Overall, the reviewers are in agreement that the paper does not meet ICLR standards.  
While the reviewers agree that this paper does provide a contribution, it is small and does overlap with several concurrent works. it is a bit hand engineered. The authors have provided a lengthy rebuttal, but the final reviews are not strong enough. 
The authors present a toy stacking task where the goal is to stack blocks to match a given configuration, and a method that is a slightly modified DQN algorithm where the target configuration is observed by the network as well as the current state. There are a few problems with this paper. First, the method lacks novelty   it is very similar to DQN. Second, the claims of learning physical intuitions is not borne out by the method or experimental results. Third, the tasks are very simple and there is no held out test set of target configurations. 
This paper provides a game based interface to have Turkers compete to analyze data for a learning task over multiple rounds. Reviewers found the work interesting and clear written, saying "the paper is easy to follow and the evaluation is meaningful." They also note that there is clear empirical benefit "the results seem to suggest that MTD provides an improvement over non HITL methods." They also like the task compared to synthetic grounding experiments. There was some concern about the methodology of the experiments but the authors provide reasonable explanations and clarification.  One final concern that I hope the readers take into account. While the reviewers were convinced by the work and did not require it, I feel like the work does not engage enough with the literature of crowd sourcing in other disciplines. While there are likely some unique aspects to ML use of crowdsourcing, there are many papers about encouraging crowd workers to produce more useful data. 
The paper got mixed scores of 4 (R1), 6 (R3), 8 (R2). R1 initially gave up after a few pages of reading, due to clarity problems. But looking over the revised version was much happier, so raised their score to 7. R2, who is knowledge about the area, was very positive about the paper, feeling it is a very interesting idea. R3 was also cautiously positive. The authors have absorbed the comments by the reviewers to make significant changes to the paper. The AC feels the idea is interesting, even if the experimental results aren t that compelling, so feels the paper can be accepted. 
This paper provides an important discussion about the relationship between training efficiency and label redundancy. The updates to the paper will improve the paper further. Reviewers found the paper interesting, well written, and addresses and important problem.
This paper introduces a method for making synchronous SGD more resistant to failed or slow workers. The idea seems plausible, but as the reviewers point out, the novelty and the experimental validation are somewhat limited. For a contribution such as this, it would be good to see some experiments on a wider range of tasks, and experiments with real rather than simulated workloads. I don t think this work is ready for publication at ICLR. 
This paper proposes a simple idea for using expert data to improve a deep RL agent s performance. Its main flaw is the lack of justification for the specific techniques used. The empirical evaluation is also fairly limited. 
the idea is interesting, but as pointed out the reviewers (and also agreed by the authors), the current manuscript lacks clear motivations, reasons underlying specific design choices and convincing empirical evaluation. 
The paper presents a layered image generation model  (e.g., foreground vs background) using GANs. The high level idea is interesting, but novelty is somewhat limited. For example, layered generation with VAE/GAN has been explored in Yan et al. 2016 (VAEs) and Vondrick et al. 2016 (GANs). In addition, there are earlier works for unsupervised learning of foreground/background generative models (e.g., Le Roux et al., Sohn et al.). Another critical problem is that only qualitative results on relatively simple datasets (e.g., MNIST, SVHN, CelebA) are provided as experimental results. More quantitative evaluations and additional experiments on more challenging datasets will strengthen the paper.  * N. Le Roux, N. Heess, J. Shotton, J. Winn; Learning a generative model of images by factoring appearance and shape; Neural Computation 23(3): 593 650, 2011. ** Sohn, K., Zhou, G., Lee, C., & Lee, H. Learning and selecting features jointly with point wise gated Boltzmann machines. ICML 2013. 
The paper performs an ablation analysis on LSTM, showing that the gating component is the most important. There is little novelty in the analysis, and in its current form, its impact is rather limited.
Thank you for submitting you paper to ICLR. The reviewers and authors have engaged well and the revision has improved the paper. The reviewers are all in agreement that the paper substantially expands the prior work in this area,  e.g. by Balle et al. (2016, 2017), and is therefore suitable for publication. Although I understand that the authors have not optimised their compression method for runtime yet, a comment about this prospect in the main text would be a sensible addition.
This paper applies a form of recurrent autoencoder for a specific type of industrial sensor signal analysis.  The application is very narrow and the data set is proprietary.  The approach is not clearly described, but seems very straightforward and is not placed in context of prior work.  It is therefore not clear how to evaluate the contribution of the method.  The authors have revised the paper to include more details and prior work, but it still needs a lot more work on all of the above dimensions before it can make a significant contribution to the ICLR community.
This paper is generally very strong. I do find myself agreeing with the last reviewer though, that tuning hyperparameters on the test set should not be done, even if others have done it in the past. (I say this having worked on similar problems myself.) I would strongly encourage the authors to re do their experiments with a better tuning regime.
The idea studied here is fairly incremental and the empirical evaluation could be improved.
The authors show evidence that an RL agent with a new neural architecture with an external memory is superior on a version of the concentration game to a baseline.   However, other works have proposed neural architectures with episodic memories, and the reviewers feel that the proposed model was not adequately compared to these.  Furthermore, there are concerns about the novelty of the proposed model.
The paper proposes a BNP topic model that uses a stick breaking prior over document topics and performs VAE style inference over them. Unfortunately, the novelty of this work is limited, as VAE like inference for LDA like models, inference with stick breaking priors for VAEs, and placing a prior on the concentration parameter in a non parametric topic model have all been done before (see e.g. Srivastava & Sutton (2017), Nalisnick & Smyth (2017), and Teh, Kurihara & Welling (2007) respectively). There are also concerns about the correctness of treating topics as parameters (as opposed to random variables) in the proposed model. The authors  clarification regarding this point was helpful but not sufficient to show the validity of the approach.
The paper is hard to follow at times. The heuristic reward has little justification   not clear how this would extend to other domains. Lack of empirical comparisons (see e.g. Hester et al., Deep Q Learning from Demonstrations, 2017). 
This paper presents a simple yet effective method for weight dropping for an LSTM that requires no modification of an RNN cell s formulation.  Experimental results shows good perplexity results on benchmarks compared to many baselines.  All reviewers agree that the paper will bring good contribution to the conference.
High quality paper, appreciated by reviewers, likely to be of substantial interest to the community. It s worth an oral to facilitate a group discussion.
The paper presents a method for forward prediction in videos. The paper insufficiently motivates the proposed method and presents very limited empirical evaluations (no ablation studies, etc.) to backup its claims. This makes it difficult for the reader to put the work into  the context of the broader research around learning from unsupervised video data; leading reviewers to complete about perceived lack of novelty and clarity.
All the reviews like the theoretical result presented in the paper which relates the gating mechanism of LSTMS (and GRUs) to time invariance / warping. The theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known. The experiments are not mind boggling, but none of the reviewers seem to think that s a show stopper. 
This paper proposes a general regularization algorithm which builds on the dropout idea. This is a very significant topic. The overall motivation is good, but the specific design choices are less well motivated over, for example, ad hoc choices. Some concerns remain after the post rebuttal discussion with the reviewers: the improvement is incremental in terms of concepts and methodology, the clarity needs to be improved and the experiments are somehow weak. In summary, the main idea and research direction is interesting, but the attempted generality of the algorithm and the significance of the area call for a more clear and convincing presentation. 
This paper was reviewed by 3 expert reviewers. All three recommend rejection citing significant concerns (e.g. missing baselines).
A well written paper proposing some reasonable approaches to counter adversarial images. Proposed approaches include non differentiable and randomized methods. Anonymous commentators pushed upon and cleared up some important issues regarding white, black and gray "box" settings. The approach appears to be a plausible defence strategy. One reviewers is a hold out on acceptance, but is open to the idea. The authors responded to the points of this reviewer sufficiently. The AC recommends accept.
This paper presents a very cool setup for multi task learning for learning fixed length representations for sentences.  Although the authors accept the fact that fixed length representations may not be suitable for complex, long pieces of text (often, sentences), such representations may be useful for several tasks.  They use a significantly large scale setup with six interesting tasks and show that learning generic representations for sentences across tasks is useful than learning in isolation.  Two out of three reviewers presented extensive critique of the paper and there s thorough back and forth between the reviewers and the authors.  The committee believes that this paper will add positive value to the conference.
The paper presents yet another approach for modeling words based on their characters. Unfortunately the authors do not compare properly to previous approaches and the idea is very incremental.
Dear authors,  I agree with the reviewers that the paper tries to do several things at once and the results are not that convincing. Overall, this work is mostly incremental, which is fine if there is no issue in the execution. Thus, I regret to inform you that this paper will not be accepted to ICLR.
This paper describes a new library for forward propagation of binary CNNs. R1 for clarification on the contributions and novelty, which the authors provided. They subsequently updated their score. I think that optimized code with permissive licensing (as R2 points out) benefits the community. The paper will benefit those who decide to work with the library.
All the reviewers and myself have concerns about the potentially incremental nature of this work. While I do understand that the proposed method goes beyond crafting minibatch losses, and instead parametrizes things via a neural network, ultimately it s roughly very similar to simply combining MMD and minibatch discrimination and "learning  the kernel". The theoretical justifications are interesting, but the results are somewhat underwhelming (as an example, DANN s are by no means the state of the art on MNIST >MNIST_M, and this task is rather contrived; the books dataset is not even clearly used by anyone else).  The interesting analysis may make it a good candidate for the workshop track, so I am recommending that.
This paper studies to what extent adversarial training affects the properties of adversarial examples in object classification.  Reviewers found the work going in the right direction, but agreed that it needs further evidence/focus in order to constitute a significant contribution to the ICLR community. In particular, the AC encourages authors to relate their work to the growing body of (mostly concurrent) work on robust optimization and adversarial learning. For the above reasons, the AC recommends rejection at this time. 
The concerns raised by AnonReviewer3 point out that, despite the effort of the authors to bridge the SM / ML divide, there is still some work to be done. The gulf between thermodynamic limits and finite effects is oft cited in the author response. This seems to be a catch all. This gap needs to be addressed early. The authors might even suggest some open (empirical) questions looking for these phase transitions in finite systems in cases where they think engineering has placed us "not too close".  
This paper does not seem completely appropriate for ICLR.
This paper presents a sampling inference method for learning in multi modal demonstration scenarios. Reference to imitation learning causes some confusion with the IRL domain, where this terminology is usually encountered. Providing a real application to robot reaching, while a relatively simple task in robotics, increases the difficulty and complexity of the demonstration. That makes it impressive, but also difficult to unpick the contributions and reproduce even the first demonstration. It s understandable at a meeting on learning representations that the reviewers wanted to understand why existing methods for learning multi modal distributions would not work, and get a better understanding of the tradeoffs and limitations of the proposed method. The CVAE comparison added to the appendix during the rebuttal period just pushed this paper over the bar. The demonstration is simplified, so much easier to reproduce, making it more feasible others will attempt to reproduce the claims made here. 
The submission proposes a loss surrogate for top k classification, as in the official imagenet evaluation.  The approach is well motivated, and the paper is very well organized with thorough technical proofs in the appendix, and a well presented main text.  The main results are: 1) a theoretically motivated surrogate, 2) that gives up to a couple percent improvement over cross entropy loss in the presence of label noise or smaller datasets.  It is a bit disappointing that performance is limited in the ideal case and that it does not more gracefully degrade to epsilon better than cross entropy loss.  Rather, it seems to give performance epsilon worse than cross entropy loss in an ideal case with clean labels and lots of data.  Nevertheless, it is a step in the right direction for optimizing the error measure to be used during evaluation.  The reviewers uniformly recommended acceptance.
This paper presents a new multi document summarization task of trying to write a wikipedia article based on its sources. Reviewers found the paper and the task clear to understand and well explained. The modeling aspects are clear as well, although lacking justification. Reviewers are split on the originality of the task, saying that it is certainly big, but wondering if that makes it difficult to compare with. The main split was the feeling that "the paper presents strong quantitative results and qualitative examples. " versus a frustration that the experimental results did not take into account extractive baselines or analysis. However the authors provide a significantly updated version of the work targeting many of these concerns, which does alleviate some of the main issues. For these reasons, despite one low review, my recommendation is that this work be accepted as a very interesting contribution.  
This paper is a timely application of linear algebra to propose a method for reducing catastrophic interference by training a new task in a subspace of the parameter space using conceptors. The conceptors are deployed in the backprop, making this a valuable alternative to recent continual learning methods such as EWC. The paper is clearly written and the results give a clear validation of the method. The reviewers agree as to the merits of the paper.
Thank you for submitting you paper to ICLR. This paper provides an informative analysis of the approximation contributions from the various assumptions made in variational auto encoders. The revision has demonstrated the robustness of the paper’s conclusions, however these conclusions are arguably unsurprising. Although the work provides a thorough and interesting piece of detective work, the significance of the findings is not quite great enough to warrant publication.  Reviewer 1 was searching for a reference for work in similar vein to section 5.4: The second problem identified in the reference below shows examples where using an approximating distribution of a particular form biases the model parameter estimates to settings that mean the true posterior is closer to that form.  R. E. Turner and M. Sahani. (2011) Two problems with variational Expectation Maximisation for time series models. Inference and Learning in Dynamic Models. Eds. D. Barber, T. Cemgil and S. Chiappa, Cambridge University Press, 104–123, 2011.
There is a very nice discussion with one of the reviewers on the experiments, that I think would need to be battened down in an ideal setting. I m also a bit surprised at the lack of discussion or comparison to two seemingly highly related papers:  1. T. G. Dietterich and G. Bakiri (1995) Solving Multiclass via Error Correcting Output Codes. 2. Hsu, Kakade, Langford and Zhang (2009) Multi Label Prediction via Compressed Sensing. 
The paper appears unfinished in many ways: the experiments are preliminary, the paper completely ignored a large body of prior work on the subject, and the presentation needs substantial improvements. The authors did not provide a rebuttal.  I encourage the authors to refrain from submitting unfinished papers such as this one in the future, as it unnecessarily increases the load on a review system that is already strained.
Meta score: 4  The paper presents a manually constructed cloze style fill in the missing word dataset, with baseline language modelling experiments that aim to show that  this dataset is difficult for machines relative to human performance.  The dataset is interesting but the fact that the experiments are confined to baseline language models Pros:    interesting dataset    clear and well written    attempt to move the field forward in an important area Cons:    limited experimentation    language modelling approaches not appropriate baseline  
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * The motivation of the problem is presented well * The architecture is simple and potentially applicable to real world applications  Cons: * The novel methodological contribution is limited to non existant * Comparison against other relevant baselines is missing, and the baseline is not strong * The evaluation methodology does not follows standard practice in IR, and thus it is difficult to analyze and compare results * Paper is hard to read and requires proofreading  Considering these pros and cons, my conclusion is that this paper is not up to the standards of ICLR.
The paper presents a method for learning from expert state trajectories using a similarity metric in a learned feature space. The approach uses only the states, not the actions of the expert. The reviewers were variously dissatisfied with the novelty, the theoretical presentation, and the robustness of the approach. Though it empirically works better than the baselines (without expert demos) this is not surprising, especially since thousands of expert demonstrations were used. This would have been more impressive with fewer demonstrations, or more novelty in the method, or more evidence of robustness when the agent s state is far from the demonstrations.
This paper introduces a student teacher method for learning from labels of varying quality (i.e. varying fidelity data). This is an interesting idea which shows promising results.  Some further connections to various kinds of semi supervised and multi fidelity learning would strengthen the paper, although understandably it is not easy to cover the vast literature, which also spans different scientific domains. One reviewer had a concern about some design decisions that seemed ad hoc, but at least the authors have intuitively and experimentally justified them.
While this paper has some very interesting ideas the majority view of the reviewers and their aggregate numerical ratings are just too low to warrant acceptance.
This paper is intersting but has a few flaws that still need to be addressed. As one reviewer noted, "the authors seems to have simply applied the method of Andrychowicz et al. If they added some discussion and experiments clearly showing why this is a better way to improve the existing inference methods, the paper might have more impact.". Overall, this work builds on existing work, but does not really dig deep enough for answers to these questions raised by the reviewers. The committee still feels this paper will be of great value at ICLR and recommends it for a workshop paper. 
The reviewers have various reservations. While the paper has interesting suggestions, it is slightly incremental and the results are not sufficiently compared to other techniques. We not that one reviewer revised his opinion 
This clearly written paper extends the Kronecker factored approximate curvature optimizer to recurrent networks.  Experiments on Penn Treebank language modeling and training of differentiable neural computers on a repeated copy task show that the proposed K FAC optimizers are stronger than SGD, Adam, and Adam with layer normalization. The most negative reviewer objected to a lack of theoretical error bounds on the approximations made, but the authors successfully argue that obtaining such bounds would require making assumptions that are likely to be violated in practice, and that strong empirical performance on real tasks is sufficient justification for the approximations.  Pros: + "Completes" K FAC training by extending it to recurrent models. + Experiments show effects of different K FAC approximations.  Cons:   The algorithm is rather complex to implement. 
This paper describes active vision for object recognition learned in an RL framework. Reviewers think the paper is not of sufficient quality: Insufficient detail, and insufficient evaluation. While the authors have provided a lengthy rebuttal, the shortcomings have not yet been addressed in the paper. 
This paper presents a sensible, but somewhat incremental, generalization of neural architecture search.  However, the experiments are only done in a single artificial setting (albeit composed of real, large scale subtasks).  It s also not clear that such an expensive meta learning based approach is even necessary, compared to more traditional approaches.  If this paper was less about proposing a single new extension, and more about putting that extension in a larger context, (either conceptually or experimentally), it would be above the bar. 
This paper presents an interesting idea: employ GANs in a manner that guarantees the generation of differentially private data.  The reviewers liked the motivation but identified various issues. Also, the authors themselves discovered some problems in their formulation; on behalf of the community, thanks for letting the readers know.  The discovered issues will need to be reviewed in a future submission.
The paper presents a variational Bayesian approach for quantising neural network weights and makes interesting and useful steps in this increasingly popular area of deep learning.
An empirical study of weight sharing for neural networks is interesting, but all of the reviewers found the experiments insufficient without enough baseline comparisons.
Learn to complete an equation by filling the blank with a missing function or numeral, and also to evaluate an expression.  Along the way learn to determine if an identity holds (e.g. sin^2(x) + cos^2(x)   1).  They use a TreeNN with a separate node for each expression in the grammar.  PROS: 1. They ve put together a new dataset of equational expressions for learning to complete an equation by filling in the blank of a missing function (or value) and function evaluation.   They ve done this in a nice way with a generator and will release it.  2. They ve got two interesting ideas here and they seem to work.  First, they train the network to jointly learn to manipulate symbols and to evaluate them.  This helps ground the symbolic manipulations in the validity of their evaluations.  They do this by using a common tree net for both processes with both a symbol node and a number node.  They train on identities (sin^2(x) + cos^2(x)   1) and also on ground expressions (+(1,2)   3).  The second idea is to help the system learn the interpretation map for the numerals like the "2" in "cos^2(x) with the actual number 2.  They do this by including equations which relate decimals with their base 10 expansion.  For example 2.5   2*10^0 + 5*10^ 1.  The "2.5" is (I think) treated as a number and handled by the number node in the network.  The RHS leaves are treated as symbols and handled by the symbol node of the network. This lets them learn to represent decimals using just the 10 digits in their grammar and ties the interpretation of the symbols to what is required for a correct evaluation (in terms of their model this means "aligning" the node for symbol with the node for number).  3. Results are good over what seem to us reasonable baselines  CONS:  1. The architecture isn t new and the idea of representing expression trees in a hierarchical network isn t new either.  2. The writing, to me, is a bit unclear in places and I think they still have some work to do follow the reviewers  advice in this area.  I really wrestled with this one, and I appreciate the arguments that say it s not novel enough but I feel that there is something interesting in here and if the authors do a clean up before final submission it will be ok.
The authors have proposed a  soft  version of VIN which is differentiable, where the cost function is trained by behavior cloning / imitation learning from expert/computer trajectories. The method is applied to a toy problem and to real historical data from mars rovers. The paper does not acknowledge nor compare against other methods, and the contribution is unclear, as is the justification for some of the aspects of the method.  Additionally it is difficult to interpret the relevance or significance of the results (45% correct).
The authors analyze the IWAE bound as an estimator of the marginal log likelihood and show how to reduce its bias by using the jackknife. They then evaluate the effect of using the resulting estimator (JVI) for training and evaluating VAEs on MNIST. This is an interesting and well written paper. It could be improved by including a convincing explanation of the relatively poor performance of the JVI trained, JVI evaluated models.
The reviewers agree that the results are promising and there are some interesting and novel aspect to the formulation. However, two of the reviews have raised concerns regarding the exposition and the discussion of previous work. The paper benefits from a detailed description of soft Q learning, PCL, and off policy actor critic algorithms, and how SAC is different from those. Instead of differentiating against previous work by saying soft Q learning and PCL are not actor critic algorithms, discuss the similarities and differences and present empirical evaluation.
The presented method essentially builds a model that remaps features into a new space that optimizes nearest neighbor classification. The model is a neural network, and the optimization is carried out through a genetic algorithm.  Pros:    One major issue with neural network classification is that of a lack of explainability. Many networks are currently "black box" approaches. By moving to the optimization problem to that of building a feature space for nearest neighbor classification, one can, to a degree, alleviate the "black box" issue by providing the discovered nearest neighbor instances as "evidence" of the decision.   Authors use established datasets.  Cons:   Authors do not properly cite previous work, as brought up by reviewers. There is much literature on optimization of feature spaces (such as the entire field of metric learning), as well as prior approaches using genetic optimization. The originality and significance here is therefore not clear. 
The reviewers generally thought the proposed algorithm was a straightforward extension of Yin et al., 2017, and not enough for a new paper.  They also objected to a lack of test results (to show generalization), but the authors did provide these in their revision.  Pros: + Adaptive batch sizing is useful, especially if the larger batches license parallelization.  Cons:   Small, incremental change to the algorithm from Yin et al., 2017   Test performance did not improve over well tuned momentum optimization, which limits the appeal of the method. 
The reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack.   There were missing relevant references in the original submission, but these have been added.  I would suggest the authors follow the reviewer suggestions on claims of generality beyond CNN; although there may not be anything obvious stopping this method from working more generally, it hasn t been tested in this work.   Even if you keep the title you might be more careful to frame the body in the context of CNN s.
The paper proposes various improvements to Wasserstein distance based GAN training. Reviewers agree that the method produces good quality samples and are impressed by the state of the art results in several semi supervised learning benchmarks. The paper is well written and the authors have further improved the empirical analysis in the paper based on reviewer comments.
meta score: 4  The paper has been extensively edited during the review process   the edits are so extensive that I think the paper requires a re review, which is not possible for ICLR 2018  Pros:    potentially interesting and novel approach to prefix encoding for character level CNN text classification    some experimental comparisons Cons:    lacks good comparison with the state of the art, which makes it difficult to determine conclusions    writing style lacks clarity.  I would recommend that the authors continue to improve the paper and submit it to a later conference. 
The paper proposes a method to learn and explain simultaneously. The explanations are generated as part of the learning and in some sense come for free. It also goes the other way in that the explanations also help performance in simpler settings. Reviewers found the paper easy to follow and the idea has some value, however, the related work is sparse and consequently comparison to existing state of the art explanation methods is also sparse. These are nontrivial concerns which should have been addressed in the main article not hidden away in the supplement.
The reviewers found a number of short comings in this work that would prevent it from being accepted at ICLR in its current form, both in terms of writing (not specifying the loss function),  experiments that are too limited, and inconclusive comparisons with existing regularization techniques. I recommend the authors take into account the feedback from reviewers in any follow up submissions.
Pros and cons of the paper can be summarized as follows:  Pros: * The underlying idea may be interesting * Results are reasonably strong on the test set used  Cons: * Testing on the single dataset indicates that the model may be of limited applicability * As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns * There is little mathematical notation, which compounds the problems of clarity  After reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here. As a result, I do cannot recommend that this paper be accepted to ICLR in its current form. I would suggest the authors define their method precisely in mathematical notation in a future submission.
This paper investigates the performance of various second order optimization methods for training neural networks. Comparing different optimizers is worthwhile, but as this is an empirical paper which doesn t present novel techniques, the bar is very high for the experimental methodology. Unfortunately, I don t think this paper clears the bar: as pointed out by the reviewers, the comparisons miss several important methods, and the experiments miss out on important aspects of the comparison (e.g. wall clock time, generalization). I don t think there is enough of a contribution here to merit publication at ICLR, though it could become a strong submission if the reviewers  points were adequately addressed. 
This paper proposes to use data driven deep convolutional architectures for modeling advection diffusion. It is well motivated and comes with convincing numerical experiments. Reviewers agreed that this is a worthy contribution to ICLR with the potential to trigger further research in the interplay between deep learning and physics. 
The paper proposes a new way of learning image representations from unlabeled data by predicting the image rotations. The problem formulation implicitly encourages the learned representation to be informative about the (foreground) object and its rotation. The idea is simple, but it turns out to be very effective. The authors demonstrate strong performance in multiple transfer learning scenarios, such as  ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR 10 classification.
This paper offers a theoretical and empirical analysis of the expressivity of RNNs, in particular in comparison to TT decomposition. The reviewers argued the results was interesting and important, although there were issues with clarity of some of the explanations. More critical reviewers argued the comparison basis with CP networks was not "fair" in that their shallowness restricted their expressivity w.r.t. TT. The experiments could be strengthened by making the explanations surrounding the set up clearer. This paper is borderline acceptable, and would have benefited from a more active discussion between the reviewers and the author. From reading the reviews and the author responses, I am leaning towards recommending acceptance to the main conference rather than the workshop track, as it is important to have theoretical work of this nature discussed at ICLR.
The paper proposes to study the impact of normalizing the gradient for each layer before applying existing techniques such as SG + momentum, Adam or AdaGrad. The study is done on a reasonable number of datasets and, after the reviewers  comments, confidence intervals have been added,  although Table 1 puts results in bold but many of these results are not statistically significant.  The paper, however, lacks a proper analysis of the results. Two main things could be improved:   Normalization does not always have the same effect but the reasons for it are not discussed. This needs not be done theoretically but a more thorough analysis would have been appreciated.   There is no hyperparameter tuning, which means that the results are heavily dependent on which hyperparameters were chosen. Thus, it is hard to draw any conclusion.  Regarding the seemingly conflicting remarks of the two reviewers, it all depends on what the paper is trying to achieve. If it tries to show that is it state of the art, then comparing to state of the art algorithms on every dataset is crucial. If it tries to study the impact of one specific change, in this case layer normalization, on the optimization, then comparing to the vanilla version is fine. The paper seems to try to address the latter so it is OK if it is not compared to all the state of the art algorithms. However, proper tuning of existing methods is still required.  Ultimately, a better understanding of layer normalization could be useful but the paper is not convincing enough to provide that understanding. There is no need to increase the number of datasets but it should rather focus on designing setups to test and validate hypotheses.
Understanding global optimality conditions for deep nets even in the restricted case of linear layers is a valuable contribution. Please add clarifications to ways in which the paper goes beyond the results of Kawaguchi 16, which was the main concern expressed by the reviewers.
Dear authors,  After carefully reading the reviews, the rebuttal, and going through the paper, I regret to inform you that this paper does not meet the requirements for publication at ICLR.  While the variance analysis is definitely of interest, the reality of the algorithm does not match the claims. The theoretical rate is worse than that of SG but this could be an artefact of the analysis. Sadly, the experimental setup lacks in several ways:   It is not yet clear whether escaping the saddle points is really an issue in deep learning as the loss function is still poorly understood.   This analysis is done in the noiseless setting despite your argument being based around the variance of the gradients.   You report the test error on CIFAR 10. While interesting and required for an ML paper, you introduce an optimization algorithm and so the quantity that matters the most is the speed at which you achieve a given training accuracy. Also, your table lists the value of the test accuracy rather than the speed of increase. Thus, you test the generalization ability of your algorithm while making claims about the optimization performance.
The paper presents a reasonable idea, probably an improved version of method (combination of GAN and SSL for semantic segmentation) over the existing works. Novelty is not ground breaking (e.g., discriminator network taking only pixel labeling predictions, application of self training for semantic segmentation each of this component is not highly novel by itself). It looks like a well engineered model that manages to get a small improvement with a semi supervised learning setting. However, given that the focus of the paper is on semi supervised learning, the improvement from the proposed loss (L_semi) is fairly small (0.4 0.8%).
Nice language modeling paper with consistently high scores. The model structure is neat and the results are solid. Good ICLR type paper with contributions mostly on the ML side and experiments on a (simple) NLP task.
The paper creates a dataset for exploration of RL for molecular design and I think this makes it a strong contribution to the community at the intersection of the two. For a methods focussed conference such as ICLR however, it may not be the best fit. Hence I would recommend submitting to a workshop track or targeting a more focussed venue such as a bioinformatics conference. 
The paper proposes an extension to the reverse curriculum RL approach which uses a discriminator to label states as being on a goal trajectory or off the goal trajectory. The paper is well written, with good empirical results on a number of task domains. However, the method relies on a number of assumptions on the ability of the agent to reset itself and the environment which are unrealistic and limiting, and beg the question as to why use the given method at all if this capability is assumed to exist. Overall, the method lacks significance and quality, and the motivation is not clear enough. 
This method has a lot of strong points, but the reviewers had concerns about baselines, comparisons, and hand engineered aspects of the method. The authors gave a strong rebuttal and made substantial updates to the paper to address the concerns. I think that this has saved the submission and tipped the balance towards acceptance. 
The paper provides an interesting take on GAN training based on Coulomb dynamics. The proposed formulation is theoretically well motivated and authors provide guarantees for convergence. Reviewers agree that the theoretical analysis is interesting but are not completely impressed by the results. The method addresses mode collapse issue but still lacks in sample quality. Nevertheless, reviewers agree that this is a good step towards the understanding of GAN training. 
Authors apply dense nets and LSTM to model dependencies among labels and demonstrate new state of art performance on an X Ray dataset.  Pros:   Well written.   New improvement to state of art  Cons:   Novelties are not strong. One combination of existing approaches are used to achieve state of art on what is still a relatively new dataset. (All Reviewers)    Using LSTM to model dependencies would be affected by the selected order of the disease states. In this sense, LSTM seems like the wrong architecture to use to model dependencies among labels. This may be a drawback in comparison to other methods of modeling dependencies, but this is not thoroughly discussed or evaluated. (Reviewer 1 & 3)    There is a large body of work on multi task learning with shared information, which have not been evaluated for comparison. Because of this, the contribution of the LSTM to model dependencies between labels in comparison to other available approaches cannot be verified. (Reviewer 1 & 3)    Top AUC performance on this dataset does not carry much significance on its own, as the dataset is new (CVPR 2017), and few approaches have been tested against it.    Medical literature not cited to justify with evidence the discovered dependencies among disease states. (Reviewer 1)  
pros: * novel explanation: skip connections < > singualrities * thorough analysis * significant topic in understanding deep nets  cons: * more rigorous theoretical analysis would be better  overall, the committee feels this paper would be interesting to have at ICLR. 
Important problem (navigation in unseen 3D environments, Doom in this case), interesting hybrid approach (mixing neural networks and path planning). Initially, there were concerns about evaluation (proper baselines, ambiguous environments, etc). The authors have responded with updated experiments that are convincing to the reviewers. R1 did not participate in the discussion and their review has been ignored. I am supportive of this paper. 
Split opinions on paper: 6 (R1), 3 (R2), 6 (R3). Much of the debate centered on the novelty of the algorithm. R2 felt that the paper was a straight forward combination of CycleGAN with S+U, while R3 felt it made a significant contribution. The AC has looked at the paper and the reviews and discussion. The topic is very interesting and topical. The experiments are ok, but would be helped a lot by including the real/synth car data currently in appendix B: seeing the method work on natural images is much more compelling. The approach still seems a bit incremental: yes, it s not a straight combination but the extra stuff isn t so profound. The AC is inclined to accept, just because this is an interesting problem.
The paper presents a simple but surprisingly effective data augmentation technique which is thoroughly evaluated on a variety of classification tasks, leading to improvement over state of the art baselines. The paper is somewhat lacking a theoretical justification beyond intuitions, but extensive evaluation makes up for that.
The paper presents a boosting method and uses it to train an ensemble of convnets for image classification. The paper lacks conceptual and empirical comparisons with alternative boosting and ensembling methods. In fact, it is not even clear from the experimental results whether or not the proposed method outperforms a simple baseline model that averages the predictions of T independently trained convolutional networks.
This paper is not aimed at introducing new methodologies (and does not claim to do so), but instead it aims at presenting a well executed empirical study. The presentation and outcomes of this study are quite instructive, and with the ever growing list of academic papers, this kind of studies are a useful regularizer. 
We encourage the authors to improve the mentioned aspects of their work in the reviews. 
This method makes a connection between evolutionary and variational methods in a particular model.  This is a good contribution, but there has been little effort to position it in comparison to standard methods that do the same thing, showing relative strengths and weaknesses.  Also, please shorten the abstract.
The paper proposes a new sparse matrix representation based on Viterbi algorithm with high and fixed index compression ratio regardless of the pruning rate.  The method allows for faster parallel decoding and achieves improved compression of index data storage requirement over existing methods (e.g., magnitude based pruning) while maintaining the pruning rate. The quality of paper seems solid and of interest to a subset of the ICLR audience.
The consensus among the reviewers is that this paper is not quite ready for publication for reasons I will summarize in more detail below. However, I think there are some things that are really nice about this approach, and worth calling out:  PROS:  1.  the idea of tackling tasks broadly all the way from perception through symbolic reasoning is an important direction.  2. It certainly would be useful to have a "plug and play" framework in which various knowledge sources or skills can be assembled behind a simple interface designed by the ML practioner to solve a given problem or class of problems.  3. Clearly finding ways to increase sample efficiency   especially in a deep net approach   is of great importance practically.  4. The writing  is good.  CONS:  1. The comparison to feedforward networks needs to be made fair in order to disentangle the benefit of the architecture from the benefit of pre training the modules.  2. Using the very limited 2x2 grid was too low a bar for the reviewers.  The authors aim at a  more general, efficient architecture useful for a variety of tasks, and perhaps you didn t want to devote too much time to this particular task, but I think having a slam dunk example of the power of the approach is really necessary to be convincing.  3. Given the similarity, I think more has to be done to show the intellectual contribution over Zaremba et al, the difference in motivation notwithstanding.  One way to do this is to really prove out the increased sample efficiency claim.
This paper deals with the important topic of learning better graph representations and shows promise in helping to detect critical substructures of graph that would help with the interpretability of representations. Unfortunately, this work fails to accurately portray how it relates to previous work (in particular, Niepert et al, Kipf et al, Duvenaud et al) and falls short of providing clear and convincing explanations of what it can do that these models can t, without including all of them in experimental comparisons. 
The experimental work was seen as one of the main weaknesses.
Reviewers are unanimous in scoring this paper below threshold for acceptance.  The authors did not submit any rebuttals of the reviews.  Pros: Paper is generally clear. Hardware results are valuable.  Cons: Limited simulation results. Proposed method is not really novel. Insufficient empirical validation of the approach.
This is an interesting paper, but was quite difficult to follow. As they stand, the empirical results are not altogether convincing nor warrant acceptance.
The paper studies subsampling techniques necessary to handle large graphs with graph convolutional networks.  The paper introduces two ideas: (1) preprocessing for GCNs (basically replacing dropout followed by linear transformation with linear transformation followed by drop out); (2) adding control variates based on historical activations.  Both ideas seem useful (but (1) is more empirically useful than (2), Figure 4*). The paper contains a fair bit of math (analysis / justification of the method).  Overall, the ideas are interesting and can be useful in practice. However, not all reviewers are convinced that the methods constitute a significant contribution.  There is also a question whether the math has much value (strong assumptions   also, from interpretation, may be too specific to the formulation of Kipf & Welling making it a bit narrow?).  Though I share these feelings and recommend rejection, I think that the reviewers 2 and 3 were a bit too harsh, and the scores do not reflect the quality of the paper.  *Potential typo: Figure 4   should it be CV +PP rather than CV?  + an important problem + can be useful in practical applications + generally solid and sufficiently well written   significance not sufficient   math seems not terribly useful  
quality: interesting idea to train an end to end attention together with CNNs and solid experiments to justify the benefits of using such attentions. clarity: the presentation has been updated according to review comments and improved a lot significance: highly relevant topic, good improvements over other methods
This paper proposes methods for replacing parts of neural networks with tensors, the values of which are efficiently estimated through factorisation methods. The paper is well written and clear, but the two main objections from reviewers surround the novelty and evaluation of the method proposed. I am conscious that the authors have responded to reviewers on the topic of novelty, but the case could be made more strongly in the paper, perhaps by showing significant improvements over alternatives. The evaluation was considered weak by reviewers, in particular due to the lack of comparable baselines.  Interesting work, but I m afraid on the basis of the reviews, I must recommend rejection.
Although paper has been improved with new quantitative results and additional clarity, the reviewers agree though that larger scale experiments would better highlight the utility of the method. There are some concerns with computational cost, despite the fact that the two networks are trained asynchronously. A baseline against a single, asynchronously trained network (multiple GPUs) would help strengthen this point. Some reviewers expressed concerns with novelty.
here, yet another sentence representation method is proposed. i agree with R1 and R3 that this does not contribute significantly to be a full length conference paper.
The paper proposes the use of GANs to match the joint distribution of features to the product of their marginals for ICA. The approach is totally plausible but reviewers have complaints about lack of rigor and analysis in terms of (i) mixing conditions under which the proposed GAN based approach will work, given that ICA is ill posed for general nonlinear mixing  (ii) comparison with prior work on linear and PNL ICA.  Further, in most scenarios where GANs are used, one of the distributions is fixed (say, the real distribution) and the other is dynamic (fake distribution) trying to come close to the fixed distribution during optimization. In the proposed method, the discriminator encodes the distance b/w joint and product of marginals which are both dynamic during the learning. It might be useful to comment whether or not it has any implications wrt increased instability of training, etc. 
The paper characterizes the latent space of adversarial examples and introduces the concept of local intrinsic dimenstionality (LID). LID  can be used to detect adversaries as well build better attacks as it characterizes the space in which DNNs might be vulnerable. The experiments strongly support their claim.
Important problem (modular continual RL) and novel contributions. The initial submission was judged to be a little dense and hard to read, but the authors have been responsive in responding and updating the paper. I support accepting this paper. 
At least two of the reviewers found the proposed approach novel and interesting and worthy of publication at ICLR. The reviewers raised concerns regarding the paper s terminology, which may lead to some misunderstanding. I agree that upon a quick skim, a reader may think that the paper performs the crossover operation outlined at the bottom right of Figure 1. Please consider improving the figure and the caption to prevent such a misunderstanding. You can even slightly change the title to reflect the policy distillation operation rather than naive crossover. Finally, including some more complex baselines benefits the paper. I am curious whether performing policy gradient on an ensemble of 8 policies + periodic removal of the bottom half of the policies will provide similar gains.
The main contribution of the paper is a technique for training GANs which consists in progressively increasing the resolution of generated images by gradually enabling layers in the generator and the discriminator. The method is novel, and outperforms the state of the art in adversarial image generation both quantitatively and qualitatively. The evaluation is carried out on several datasets; it also contains an ablation study showing the effect of contributions (I recommend that the authors follow the suggestions of AnonReviewer2 and further improve it). Finally, the source code is released which should facilitate the reproducibility of the results and further progress in the field.  AnonReviewer1 has noted that the authors have revealed their names through GitHub, thus violating the double blind submission requirement of ICLR; if not for this issue, the reviewer’s rating would have been 8. While these concerns should be taken very seriously, I believe that in this particular case the paper should still be accepted for the following reasons: 1) the double blind rule is new for ICLR this year, and posting the paper on arxiv is allowed; 2) the author list has been revealed through the supplementary material (Github page) rather than the paper itself; 3) all reviewers agree on the high impact of the paper, so having it presented and discussed at the conference would be very useful for the community.
The paper proposes a novel approach for DNN inversion mainly targeted towards semi supervised learning. However the semi supervised learning results are not competitive enough. Although the authors mention in the author response that semi supervised learning is not the main goal of the paper, the experiments and claims of the paper are mainly targeted towards semi supervised learning. As the approach for inversion is novel, the paper could be motivated from a different angle with appropriate supporting experiments. In its current form it s not suitable for publication. 
This paper introduces an algorithm for optimization of discrete hyperparameters based on compressed sensing, and compares against standard gradient free optimization approaches.  As the reviewers point out, the provable guarantees (as is usually the case) don t quite make it to the main results section, but are still refreshing to see in hyperparameter optimization.  The method itself is relatively simple compared to full featured Bayesopt (spearmint), although not as widely applicable. 
The paper is a well written review of regularization approaches in deep learning. It does not offer novel approaches or novel insight with empirically demonstrated usefulness  > ICLR is not the appropriate venue for it.
This is a thought provoking paper that places GANs and VAEs in a single framework and, motivated by this perspective, proposes several novel extensions to them. The reviewers made several good suggestions for improving the paper and the authors are expected to make the revisions they promised. The current title of the paper is too general and should be changed to something more directly descriptive of the contents.
This paper proposes a method for quantitatively evaluating GANs. Better quantitative metrics for GANs are badly needed, as the field is being held back by excessive focus on generated samples. This paper proposes to estimate the Wasserstein distance to the data distribution. A paper which does this well would be a significant contribution, but unfortunately (as the reviewers point out) the experimental validation in this paper seems insufficient.  To be convincing, a paper would first need to demonstrate the ability to accurately estimate Wasserstein distance   not an easy task, but one which receives little mention in this paper. Then it would need to validate that the method can either quantitatively confirm known results about GANs or uncover previously unknown phenomena. As it stands, I don t think this submission is ready for publication in ICLR, but I d encourage resubmission after more careful experimental validation along the lines suggested by the reviewers.  
This paper clearly surveys a set of methods related to using generative models to produce samples with desired characteristics.  It explores several approaches and extensions to the standard recipe to try to address some weaknesses.  It also demonstrates a wide variety of tasks.  The exposition and figures are well done.
 PROS: 1. well written and clear 2. added extra comparison to dagger which shows success 3. SOTA results on open ai benchmark problem and comparison to relevant related work (Shi 2017) 4. practical applications 5. created new dataset to test harder aspects of the problem  CONS: 1. the algorithmic novelty is somewhat limited 2. some indication of scalability to real world tasks is provided but it is limited
This paper proposes a regularizer for recurrent neural networks, based on injecting random noise into the hidden unit activations.  In general the reviewers thought that the paper was well written and easy to understand.  However, the major concern among the reviewers was a lack of empirical evidence that the method works consistently.  Essentially, the reviewers were not compelled by the presented experiments and demanded more rigorous empirical validation of the approach.  Pros:   Well written and easy to follow   An interesting idea   Regularizing RNNs is an interesting and active area of research in the community  Cons:   The experiments are not compelling and are questioned by all the reviewers   The writing does not cite relevant related work   The work seems underexplored (empirically and methodologically)
meta score: 4  This paper is primarily an application paper applying known RL techniques to dialogue.    Very little reference to the extensive literature in this area.  Pros:    interesting application (digital search)    revised version contains subjective evaluation of experiments  Cons:    limited technical novelty    very weak links to the state of the art, missing many key aspects of the research domain 
Thank you for submitting you paper to ICLR. ICLR. Although there revision has improved the paper, the consensus from the reviewers is that this is not quite ready for publication.
This paper presents a theoretical justification for the Adam optimizer in terms of decoupling the signs and magnitudes of the gradients. The overall analysis seems reasonable, though there s been much back and forth with the reviewers about particular claims and assumptions. Overall, the contributions don t feel quite substantial enough for an ICLR publication. The interpretation in terms of signs is interesting, but it s very similar to the motivation for RMSprop, of which Adam is an extension. The performance result on diagonally dominant noisy quadratics is interesting, but it feels unsurprising that a diagonal curvature approximation would work well in this setting. I don t recommend acceptance at this point, though these ideas could potentially be developed further into a strong submission. 
The paper proposes a new convolutional network architecture, called CrescendoNet. Whilst achieving competitive performance on CIFAR 10 and SVHN, the accuracy of the proposed model on CIFAR 100 is substantially lower than that of state of the art models with fewer parameters; the paper presents no experimental results on ImageNet. The proposed architecture does not provide clear new insights or successful new design principles. This makes it unlikely the current manuscript will have a lot of impact.
Authors present a method for modeling neurodegenerative diseases using a multitask learning framework that considers "censored regression" problems (to model where the outputs have discrete values and ranges). Given the pros/cons, the committee feels this paper is not ready for acceptance in its current state.   Pro:   This approach to modeling discrete regression problems is interesting and may hold potential, but the evaluation is not in a state where strong meaningful conclusions can be made.  Con:   Reviewers raise multiple concerns regarding evaluation and comparison standards for tasks. While authors have added some model comparisons in response, in other areas comparisons don t appear complete. For example, when using MRI data, networks compared all use features derived from images, rather than systems that may learn from images themselves. Authors claim dataset is too small to learn directly from pixels in this data (in comments), but transfer learning and data augmentation have been successfully applied to learn from datasets of this size. In addition, new multitask techniques in the imaging domain have also been presented that dynamically learn the network structure, rather than relying on a hand crafted neural network design. How this approach would compare is not addressed.   
The paper proposes iterative training strategies for learning teacher and student models. They show how iterative training can lead to interpretable strategies over joint training on multiple datasets. All the reviewers felt the idea was interesting, although, one of the reviewers had concerns about the experimentation.  However, there is a BIG problem with this submission. The author names appear in the manuscript thus disregarding anonymity.
This paper is novel, but relatively incremental and relatively niche; the reviewers (despite discussion) are still unsure why this approach is needed.
Though the general direction is interesting and relevant to ICLR, the novelty is limited. As reviewers point out it is very similar to Le & Zuidema (2015), with few modifications (using LSTM word representations, a different type of pooling). However, it is not clear if they are necessary  as there is no direct comparison (e.g., using a different type of pooling). Overall, though the submission is generally solid,  it does not seem appropriate for ICLR.  + solid + well written   novelty limited   relation to Le & Zuidema is underplayed
This paper implements Group convolutions on inputs defined over hexagonal lattices instead of square lattices, using the roto translation group. The internal symmetries of the hexagonal grid allow for a larger discrete rotation group than when using square pixels, leading to improved performance on CIFAR and aerial datasets.  The paper is well written and the reviewers were positive about its results. That said, the AC wonders what is the main contribution of this work relative to existing related works (such as Group Equivarant CNNS, Cohen & Welling 16, or steerable CNNs, Cohen & Welling 17). While it is true that extending GCNNs to hexagonal lattices is a non trivial implementation task, the contribution lacks significance in the mathematical/learning fronts, which are perhaps the ones ICLR audience will care more about. Besides, the numerical results, while improved versus their square lattice counterparts, are not a major improvement over the state of the art.  In summary, the AC believes this is a borderline paper. The unanimous favorable reviews tilt the decision towards acceptance. 
All of the reviewers have found some aspects of the formulation interesting, but they raised concerns regarding the practical use of the experimental setup. 
This paper presents a method for learning more flexible prior distributions for GANs by learning another distribution on top of the latent codes for training examples. It s reminiscent of layerwise training of deep generative models. This seems like a reasonable thing to do, but it s probably not a substantial enough contribution given that similar things have been done for various other generative models. Experiments show improvement in samples compared with a regular GAN, but don t compare against various other techniques that have been proposed for fixing mode dropping. For these reasons, as well as various issues pointed out by the reviewers, I don t recommend acceptance. 
To summarize the pros and cons:  Pro: * Interesting application * Impressive results on a difficult task * Nice discussion of results and informative examples * Clear presentation, easy to read.  Con: * The method appears to be highly specialized to the four bug types. It is not clear how generalizable it will be to more complex bugs, and to the real application scenarios where we are dealing with open world classification and there is not fixed set of possible bugs.  There were additional reviewer complaints that comparison to the simple seq to seq baseline may not be fair, but I believe that these have been addressed appropriately by the author s response noting that all other reasonable baselines require test cases, which is an extra data requirement that is not available in many real world applications of interest.  This paper is somewhat on the borderline, and given the competitive nature of a top conference like ICLR I feel that it does not quite make the cut. It is definitely a good candidate for presentation at the workshop however.
Authors present a method for disease classification and localization in histopathology images. Standard image processing techniques are used to extract and normalize tiles of tissue, after which features are extracted from pertained networks. A 1 D convolutional filter is applied to the bag of features from the tiles (along the tile dimension, kernel filter size equal to dimensionality of feature vector). The max R and min R values are kept as input into a neural network for classification, and thresholding of these values provides localization for disease / non disease.  Pro:    Potential to reduce annotation complexity of datasets while producing predictions and localization  Con:   Results are not great. If anything, results re affirm why strong annotations are necessary.   Several reviewer concerns regarding novelty of proposed method. While authors have made clear the distinctions from prior art, the significance of those changes are debated.  Given the current pros/cons, the committee feels the paper is not ready for acceptance in its current form.
The paper proposes using a set of orthogonal bases that combine to form convolution kernels for CNNs leading to a significant reduction of memory usage. The main concerns raised by the reviewers were 1) clarity; 2) issues with writing and presentation of results; 3) some missing experiments. The authors released a revised version of the paper and a short summary of the enhancements. None of the reviewers changed scores following the author response. The reviews were detailed and came from those familiar with CNNs. I have decided to go with reviewer consensus. 
The paper aims to address the mode collapse issue in GANs by training multiple generators and forcing them to be diverse.  Reviewers agree that the proposed solution is not novel and has disadvantages such as increased parameters due to multiple generator models. The authors do not provide convincing arguments as to why the proposed approach should work well. The experiments presented also fail to demonstrate this. The results are limited to smaller MNIST and CIFAR10 datasets. Comparisons with approaches that directly address the mode collapse problem are missing.
The authors propose an architecture that uses a curriculum and multi task distillation to gain higher performance without forgetting. The paper is largely a smart composition of known methods, and it requires keeping data from all tasks to do the distillation, so it is not truly a scalable continual learning approach. There were a lot of concerns about clarity in the manuscript, but many of these have been assuaged by an update to the paper. This is a borderline paper, but the author s rebuttal and update probably tip it towards acceptance. 
The reviewers have unanimously expressed strong concerns about the technical correctness of the theoretical results in the paper. The paper should be carefully revised and checked for technical errors. In its current form, the paper is not suitable for acceptance at ICLR 2018. 