This paper presents a state representation learning technique aiming at extracting only state features that are relevant to solve a given task. It combines several ideas, in particular (1) keeping only features that are relevant to take actions from an information theoretic point of view, (2) model based learning, and (3) sparsity inducing constraints. Experiments on CarRacing and VizDoom show that the proposed method outperforms existing baselines.

Although authors did a thorough job trying to address reviewers  comments during the discussion period, in the end most reviewers remained unconvinced by the submission in its current state, the main remaining concerns being:
* Unclear description of the methodology and informal maths
* A somewhat complex optimization objective that may require tuning many hyper parameters, and whose entire relevance isn t clearly demonstrated empirically

Overall I agree with these concerns, in particular the general feeling that the theoretical part is difficult to follow, with some apparent typos / mistakes (clearly the original submission had a lot of issues, given the original reviews that required the authors to fix several points). To give a concrete example, while reading the final revision I first ran into potential issues when defining the objective $H(a_t | ...)$ on p.4:
* Although the left hand side is associated to a single timestep $t$, the right hand side is a sum over all values of $t$
* The definition of $q_{\phi}$ seems weird to me, in particular the fact that it takes $o_t$ as input (through $y_{1:t}$)  > this seems like a typo (?) and otherwise I don t really understand how this defines a proper definition over states
* As at least one reviewer pointed out, authors start from the mutual information $I$ but drop the entropy term $H(a_t | R_{t+1})$ by claiming it doesn t matter since the goal is to learn the ASR. However, in that objective the distribution over actions $p_{\alpha}$ seems to be learnable (through the $\alpha$ parameters), so if we try to minimize the mutual information $I$ including over $\alpha$ it would have been important to retain the entropy over actions as well.

In terms of the relevance of the results, they look pretty good but:
* The proposed algorithm ends up being somewhat complex, with a lot of terms in the loss (eq. 4), and a lack of empirical validation of what actually matters. I see a single ablation study in the Appendix (Fig. 10), and possibly also the comparison to VRL (but it isn t entirely clear to me what this baseline is implementing as it lacks details). I would have appreciated a more thorough empirical analysis of how each term in eq. 4 matters.
* CarRacing experiments consistently use 21 dimensions "for a fair comparison", but this dimensionality was chosen specifically for and by ASR. As a result, it doesn t really look "fair" to me: a fairer comparison would have either selected the optimal dimensionality for each method, or shown results across a range of different dimensionalities.

I also have some concerns regarding the applicability of the algorithm:
1. Relying on random actions to build a world model only works if random actions allow sufficient enough exploration of the state space. There are many situations where this isn t a realistic assumption (also alluded to by at least one reviewer).
2. Minor: in the setup of eq. 1 the reward $r_t$ doesn t directly depend on $s_t$. I m not sure to which extent this matters for the proposed algorithm, but if this is a necessary condition for it to work properly, it may cause issues in many stochastic environments.

As a result, I am recommending rejection as I believe the paper is not quite ready for publication. I would encourage the authors to try and simplify the presentation (the paper is very notation heavy and not an easy read), focusing on showing convincing theoretical and empirical justification for all components of the proposed technique.
This paper proposes a framework for learning disentangled representations of content and style in an unsupervised way, using a permutation invariant network. It adopts VQ network for content encoding, and Cross Attention for Style and Linking Attention at decoder. It is shown to be domain agonistic, working well in image and audio domain. Experiments are conducted on speech and image datasets.

The paper is recommended as an accept (weak) to ICLR. The reviewers have given detailed feedback and suggestions   please address them in the next revision of the paper.
This paper presents a meta learning method where the standard ReLU activation units are replaced by the stochastic LWTA units to learn data driven sparse representation.  Most of reviewers like the idea of embedding the stochastic LWTA into a MAML framework. Initial assessment pointed out the lack of clarity in various places in the paper. Authors did a good job in the author’s rebuttal period, to clarify the paper. Experiments demonstrated the competitive performance over existing meta learning methods. Two of reviewers raised their overall score to 5 (from 3). However, all reviewers have concerns in the incremental technical novelty and feel that presentation should be improved to make the paper more clear and friendly to readers. While the idea is interesting, which is backed up by experiments, the paper is not ready for the publication at the current stage. I encourage to resubmit the paper after addressing these concerns.
This paper presents a decentralized version of the CEM technique, where an ensemble of CEM instances run independently from one another and each performs a local improvement of its own sampling distribution. The paper shows that the proposed technique can alleviate the problem of centralized CEM related to converging to a local optimum. The paper includes a theoretical analysis and simulation experiments that show some benefits of the proposed technique over centralized CEM.

The key criticisms from the reviewers include the straightforward nature of the proposed idea, which limits the technical contribution of the paper, as well as the limited improvements over centralized CEM in the simulation experiments. 

In summary, this is a borderline paper. While the paper is well written and the proposed approach is clearly explained, the lack of strong empirical results that show a pronounced improvement of decentralized CEM coupled with the incremental nature of the idea of decentralized CEM makes me lean toward a rejection.
The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design.
This submission received 4 final ratings above the acceptance threshold: 6, 6, 6, 8. The reviewers mentioned limited novelty, but acknowledged practical importance of this work, and particularly appreciated thorough analysis provided by the authors. After a strong rebuttal, most of remaining concerns have been addressed.
The final recommendation is therefore to accept this submission as a poster.
Based on the previously observed neural collapse phenomenon that the features learned by over parameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for few shot downstream tasks. Both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well.

The problem that this paper delves into is important. The paper is well motivated, and well structured with a good flow. Both theoretical and empirical analyses of the paper are solid. Preliminary ratings are mixed, but during rebuttal, multi round responses and in depth discussions were carried out between authors and reviewers, and the final scores are all positive with major concerns well addressed. AC considers the paper itself and all relevant threads, and recommends the paper for acceptance. Authors shall incorporate all response materials into the future version.
This is a borderline paper. The scores were initially below the bar. The novelty of the work is limited and there are strong claims in the paper that should be revised. The authors can also do a better job in positioning their work with respect to the existing results. However, the authors managed to address several questions/concerns of the reviewers and convince them to raise their scores. I would strongly recommend the authors to address the rest of the reviewers  comments, especially those related to strong claims and connection to related work, and further improve their work in preparing its final draft.
This paper proposes a method to improve the robust accuracy of classifiers using test time training.  The reviewers all agree that the method is interesting, and many reviewers had a positive view of the method.  However, two main criticisms remain: (i) the method increases the runtime of inference, and (ii) comparisons to other related methods were lacking.  The authors responded to (i) by reporting runtimes for their method in the rebuttal.  Some reviewers were concerned that the runtime increase of the method is not acceptable, however I am not very concerned with this issue since I think the paper contains an interesting methodology even if it’s not ready for deployment at the industrial scale.  However, issue (ii) does not seem to have been adequately addressed.  The comparison to SOAP is a welcome addition the reviewers acknowledge, but a number of other methods, for example masking and cleansing, are closely related (but different) and so comparisons should be provided.
The reviewers and AC all agree that the paper considers an important problem but that several concerns remain which makes the present submission of limited novelty. 
We strongly encourage the authors to revise their manuscript to incorporate the reviewers comments as this will significantly strengthen the significance of their work. 
In particular it will be important to strengthen the theoretical analysis and expand the empirical evaluation, including incorporating an ablation study and considering settings of various difficulty, noise level, etc.
This is a borderline paper. While reviewers believe the findings from this paper may be of potential interest, they are fully convinced. For instance, if the authors want to claim the proposed mechanism is general for UDA, then they should demonstrate its effectiveness to other application domain(s), such as the NLP domain, where the pretrain finetuning strategy is widely adopted for transfer learning. However, the authors did not provide correspondingly additional experiments as requested by a reviewer but claimed they only focused on the CV domain. If the focus is on the CV domain, then the authors need to explain in detail why in the CV domain, the proposed mechanism works well (while in other domains, it may not). There are many other concerns about the assumptions, experimental settings, etc. 

In summary, this is a borderline paper below the acceptance bar of ICLR.
This paper observes that a fully convolutional model in the style of recent MLP Mixer and ViT variants can have surprisingly good initial performance. As this paper attracts certain amount of attentions, three expert reviewers have provided very detailed and serious comments, and two actively engaged with author discussions. AC also carefully read the paper as well as all discussion threads.

AC agrees the authors should not be penalized by not achieving the best performance, nor not comparing with very recent work. The main legitimate critiques, however, focus on three aspects: (1) over claimed contribution; (2) experiment solidness/competitiveness; and (3) writing completeness/clarity.

First, this paper established an interesting ablation experiment that a very simple model, that uses only standard convolutions to achieve the mixing steps, can roughly "do the work". However, AC disagrees this is a very "surprisingly new" result, on top of MLP mixer: given convolutions are increasingly re injected into ViTs to gain the vision inductive bias, their similar role in MLP mixer should be expected too. Moreover, as in general agreement by reviewers, the paper title might have over claimed   the authors cannot directly prove this concept "patch is the most critical component" yet. The authors later also agreed and changed some confusing wording, which is a good move (but also, making their contribution now even less obvious). 

Second, this method does not achieve noteworthy competitive results compared to others, in order to justify its merit (simplicity alone is good to have, but insufficient to justify a strong work). Importantly, it has been pointed out by two reviewers that the model throughput is much worse than the competitors. AC also noticed that the comparison was not very rigorous, e.g., comparing ConvMixer patch size 7 with DeiT B 16 patch size 16 doesn t help draw much fair informative conclusion. The cifar 10 results alone did not provide strong support and were later de emphaszied by authors too. 

Third, while NOT being the main reason of rejection, AC personally suggests the authors to responsibly enrich their main text, and to remove the  “A note on paper length” paragraph. The authors intentionally kept the paper length unusually short. Reviewers generally dislike this idea. Being an innovative writer is good, but very relevant details and discussions were left in the supplemental as a result. Especially, AC agrees the whole section A and part of section B of the supplemental should have been in the main paper at very least.

In summary, the authors strive to tell an interesting story, but it is not yet a well settled story. The experiments are not solid enough to support their bold claims. The authors are suggested to improve their work further by taking into account reviewer comments.
Thank you for your submission to ICLR.

This paper presents a technique for image synthesis based on stochastic differential equations and a diffusion model.  This looks to be a very nice idea with good results.  After discussion, the reviewers converged and all agreed that the paper is ready for publication the most negative reviewer raised their score after the author rebuttal, from a weak reject to weak accept.  The rebuttal clearly and concisely addressed several concerns of the reviewers.

I m happy to recommend accepting the paper.
The paper develops a method for decomposing 3D scenes into objects by coupling NeRF decoders to representations produced by a slot based encoder.  After the discussion phase, reviewer ratings are mixed with three on either side of above/below threshold, and one higher (but low confidence) accept score.

Drawbacks include limited novelty, as stated by Reviewer 8UAh: "the unsupervised decomposition of the scene, which is somehow incremental, given that it s achieved by applying the slot based approach of [Locatello et al. 2020] to NeRFs".  Reviewer bAmB likewise mentions this issue.  Reviewer VrrK: "some of its contributions are on improving NeRFs while the decomposition part is rather marginal". Reviewer VrrK also raises concerns about lack of experiments on real data: "Since the proposed method is based on NeRF, how well does it work with real photographs?"

The AC agrees with the marginal rating of the reviewers and is particularly concerned with overall novelty of the proposed pipeline and question of applicability beyond simulated data.  More work seems required to solidify an experimental case on real images.
This paper proposes new analysis on Variance Collapse of SVGD in High Dimensions. The analysis provides some new insights despite of some limitations.
This paper presents an explanation based learning approach that learns from both observations (examples) and explanations paired with examples. It proposes to learn an interpreter that can map from natural language sentences to examples. The authors also develop an evaluation environment and protocols for the tasks.

Strengths:
  The proposed idea is intuitive and seems general
  The benchmark dataset is useful resource for future research

Weakness:
  The motivation of the present problem setup needs more justification
  The close connection to the line of work on explanation based learning (especially recent ones in modeling natural language explanations) are not thoroughly discussed and compared.
  Experiments beyond the game like datasets will help validate the claims better and justifies that the problem setup has real world applications
As pointed out by reviewers, the presentation needs to be improved to clarify the algorithmic and theoretical contributions.
The reviewers agree that test time model adaptation is an interesting problem, providing a new perspective to improve model robustness. The proposed method builds on intuitive assumptions that are easy to understand. However, there are mainly two concerns regarding novelty and effectiveness. The paper can improve in these two aspects to meet ICLR standard.
This paper studies the problem called "Predict, then Optimize (PTO)", where the goal is to predict some unknown parameters in the objective function and then optimize the function. The paper focus on special case of PTO where the goal is to learn the unknown parameters corresponding to some combinatorial ordering of the data. The paper argues that traditional methods which perform the ranking after minimizing the objective can be very unstable, and proposes a new alternative loss called ATGP loss. The goal of the loss is to ensure that the total group preorder of the learned solution is as consistent with the observation as possible after fitting the data. 

The paper shows some property of the new loss and claims that it can be better than naive regression then ranking approach on learning to rank with linear regression problem, however, the theorem only shows that the robustness bar of TGP loss is larger or EQUAL to that of the naive approach, not exactly higher   Thus, such an argument is not really convincing.  The authors also proposed a heuristic algorithm to minimize a smoothed version of the proposed loss. 

However, the proposed objective is still interesting on its own, and the experimental result looks promising. Thus, this paper is a borderline accept paper at this conference.
Strengths:
* Strong empirical study across multiple datasets. However, the gains are not as impressive as for other pretraining domains, such as text or images.
* Interesting formulation of pseudo homophily as an objective to optimize in the self supervision stage
* Well written paper

Weaknesses:
* Novelty may be limited by the fact that the method is essentially learning (or searching) for a weighted average of self supervised training objectives
* In that case, while the pseudo homophily angle is interesting, there may be other appropriate baselines for yielding this weighted combination of tasks that are not explored
* There is concern about the degree of empirical improvements on certain datasets
This work tackles video generation using implicit representations, and demonstrates that using these representations enables improvements to long term coherence of the generated videos.

Reviewers praised the writing, the thorough experimental evaluation, and the strong quantitative results. Some concerns were raised about a lack of discussion of relevant related work, novelty/significance, model architecture, and a lack of qualitative examples, many of which the authors have tried to address during the discussion phase. Several reviewers raised their ratings as a result.

Personally I certainly believe that exploring implicit representations for video is important, and I know of no published prior work in this direction, which amplifies the potential significance of this work. Even if results are qualitatively worse than previous work in some ways, this exploration is still valuable and worth publishing.

While the paper ultimately received one reject rating, another reviewer chose to champion this work and award it the highest possible rating. Combined with the other positive reviews, this provides plenty of convincing evidence for me to recommend acceptance. That said, given the rating spread, I would like to encourage the authors to consider the reviewers  comments further as they prepare the final version of the manuscript. Especially providing more qualitative results would be a welcome addition.
As the public post indicates, significant deliberation went into this decision. However, the core criticism remains: the primary contribution of this paper, Theorem 1, is somewhat incremental. It is acknowledged that MI is an important problem and understanding its intricacies is worthwhile, but the present paper s contributions in this space remains narrow. A more thorough exploration of the points brought up in the latest discussion and author response might help strengthen the paper. ​In particular, more careful discussion and systematic discussion and exploration of relationships between various MI attack efficacy measures (accuracy vs positive accuracy) and privacy notions (pure vs approx DP   it wasn t totally clear how broad the result in Section 5.2.1 is without a precise theorem statement) would strengthen the paper. Additionally, while it indeed seems that the positive accuracy bound given also suffices to protect against the type of attack mentioned (where 1% of the datapoints are highly vulnerable), it is unclear if this is necessary. This feeds into the previous point: it would be valuable to get a more systematic understanding of the various MI efficacy measures and how they interact with DP. Finally, it is now appreciated that the Sablayrolles et al (2019) result worked under an unusual model restriction, though deficiencies of their result does not necessarily make this result stronger (as an aside, I believe their restriction is so that they can get a tight understanding of behavior in other settings, and DP protections were somewhat of an afterthought). The authors are encouraged to further build on this work, potentially in the directions suggested, to get a more thorough understanding of the relationships between DP and MI attacks
This paper proposes a very simple procedure to accelerate the inference time of graph structured Neural Networks, by distilling knowledge of a GNN into a node wise MLP. 
Despite some concerns about the novelty of the methodology (which borrows heavily from previous KD works), reviewers generally found this empirical work well executed and providing a potentially useful baseline for large scale applications. Therefore, the AC recommends acceptance.
This paper investigates fast adversarial training methods as a bilevel optimization problem. The proposed algorithm compares well with the existing techniques in overall runtime (obtaining better clean test accuracy, which is not the goal, and) matching the robust accuracy of existing adversarial training methods. The proposed framework, however, is more general and flexible and is theoretically grounded. The problem studied here is exciting and the approach the authors take is interesting. 

The current version, unfortunately, has some serious shortcomings. The empirical comparisons are a bit lacking — in general, the wall clock time is not a very good measure, it depends heavily on the implementation and various optimizations therein. A more suitable comparison would be in terms of floating point operations, or in terms of iteration complexity. 

The paper reports other interesting findings such as how the proposed method avoids robust overfitting. However, there is little theoretical evidence or insight for how the proposed method avoids it. 

The writing can be improved with more emphasis on the novelty and significance of the contributions — some of the statements regarding improvements over prior work are somewhat misleading given the incremental gains (e.g., see Table 1). I believe the comments from the reviewers have already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission.
In this paper, the authors studied algorithmic stability of batch reinforcement learning algorithms, as well as its connection to certain generalization bounds (motivated by the prior work Hardt et al developed for SGD on nonconvex optimization problems). While understanding the stability and generalization of batch RL is certainly an interesting and important direction, the paper in its current form is not yet ready to be published. As the reviewers pointed out, both the analyses and the claims need to be polished (in fact, important details and definitions are missing); and the theoretical contributions are only made in a limited setting.
This paper has several issues:
(1) The empirical results were incomplete and hard to interpret.
1.a The paper uses non standard benchmark domains making comparisons with results in the literature very difficult. The paper does not use the same environments as related baselines they build on. Some progress on this last point was made during the discussion period well done authors!
1.b The experiments did not sweep key hyperparameters of the TrajeDi baseline, and generally did not comment on nor ablate several other potentially important hyperparameters and design choices
(2) the proposed method is very similar to another called TrajeDi and the paper and results don t clearly show why the modification of TrajeDi is significant (see #1). The paper initially claimed the TrajeDi was a concurrent submission but one reviewer pointed out the work was published in May 
(3) writing and structure could be improved. In addition some inaccurate statements could be cleaned up
(4) The algorithm is more generally applicable beyond human AI coordination and the reviewers found it odd the paper did not focus on this

In addition, the authors did not respond to several of the reviewers responses. This made it difficult for the reviewers to increase their scores. Several reviewers found the work intriguing, but its not ready yet
The paper proposes an explanation method based on message flows, and shows better performance than the state of the art methods.
The authors addressed most of the reviewer s comments but the reviewers are not enthusiastic.  So I give my evaluation (some concerns are shared with reviewers and were not well addressed in the rebuttal)

Pros:
  State of the art results on edge scoring.

Concerns: 
  The main claim is not supported.  The authors say "we argue that message flows are more natural for performing explainability.  To this end, we propose..."  But I see no such argument after the proposed method is introduced.  Also, no advantage of the flow based approach is shown.  The experiments only show edge scoring, which ignores the layer wise edge scoring.  For this task, many existing methods are similar to the proposed approach in the sense that they measure how much information goes through subgraphs.  Although the proposed method shows good performance in edge scoring, this is not necessarily because the proposed method is flow based, and cannot be evidence of the superiority of flow based methods.  Fine details of the algorithm can contribute to the performance.  In the rebuttal, the authors mentioned a virus infection dataset as a situation where the flow based method can do beyond what existing methods can do.  This kind of experiment should be shown in the paper to support the main claim.  
  Difference between flows and walks is unclear.  The authors imply that this paper is the first paper based on the flows, and reviewers understood so.  The authors say a walk is "similar" to a flow but the difference is not explained.  (the authors only talk about the difference in how to compute the score in Schnake et al.)  Essential difference between walks and flows should be explained. 
  The reason why the proposed method performs better than the existing methods is not analyzed.  The authors say they "believe" that this is because the proposed method is based on flows, but what readers want to see is evidence.
  Presentation should be improved.  Some formulations are unclear, e.g., I have no idea what F_?{t} means.  If this would be the best notation the authors think of, it should be explained with a figure.  Use another character if t is not the layer id.  Notation is not consistent, e.g., edges are denoted by e in Section 2.1 while they are denoted by a later.  
  Marginal technical contributions.
  High complexity.  The proposed method seems not scalable even with the crude Monte Carlo approximation with a small number of samples.

With my concerns above and the reviewer s evaluations, I would not recommend acceptance.
Understanding neural networks once they have been trained is a big open problem for machine learning. This manuscript designed graph theoretic and information theoretic measures aimed at helping us understand community structure and function in trained networks. In particular, they measure community structure (modularity) and entropy for trained networks and related these to the performance of the networks. The manuscript runs experiments with fully connected networks on problems such as MNIST and CIFAR. Both community structure and entropy measures are shown to correlate (Spearman and Pearson correlation coefficients) with performance metrics in the networks studied. 
Reviewers tended to agree that the paper was well written and motivated by an interesting and timely question (understanding trained networks). However, on the whole, most of the reviewers believe that the manuscript is too preliminary for publication at ICLR and I agree. A central issue cited by most of the reviewers is that the experiments are performed on small/toy models for small tasks and under particular hyperparameter regimes. It is therefore unclear to what extent the results would generalize to other situations. E.g. would the results hold for larger dataset or for convolutional neural networks? Connected to this complaint, reviewers worry that there is not enough connection to the literature and baseline methods that could be used to predict performance given measures of trained network activity. Even allowing that the observed correlations are true and generalizable, are these measures better than those covered elsewhere in the literature? Additionally problematic, the measures are not theoretically justified either. Thus, we are missing both reasoned arguments for the metrics and robust quantification beyond a limitted experimental setting. One reviewer, Xmnm, is compelled by the work and recommends acceptance. However, they do not present a compelling case for acceptance, and even repeat several of the concerns raised by other reviewers. 
In sum, the work is on an interesting subject and timely, but needs further work to be ready for publication.
This paper studies the loss landscape of domain adversarial neural networks for domain adaptation. First, the authors show that smooth minima with respect to adversarial loss leads to sub optimal generalization on the target domain. Then, they suggest to enforce smoothness only with respect to the task loss. 3 reviewers are on negative sides, and 1 reviewer is on a positive side. All negative reviewers interacted with authors in the discussion period. After the discussion period, even the positive reviewer agreed negative comments of other reviewers and declined to champion this paper.  

AC thinks that this is a borderline paper; the proposed claims (both theoretic and empirical) are interesting and there is no critical weakness of this paper. However, the results are not super excited, as evidenced by 3 negative reviewers. In particular, AC agrees with negative comments of reviewers on limited novelty and lack of strong theoretical motivation for the proposed scheme. AC also thinks the performance improvements in experiments are not that significant. Furthermore, the problem scope is narrow, i.e., the authors study a certain property (smoothness) of a special algorithm (adversarial training) for domain adaptation that is a particular way for domain generalization. Hence, the impact to the community can be not significant as well. Considering all aspects, AC is a bit toward to suggesting rejection.
This paper received 5 quality reviews, with 3 of them rated 8, 1 rated 6, and 1 rated 5. In general, while there are minor concerns, the reviewers acknowledge the contribution of applying Knowledge distillation to the problem of monocular 3D object detection, and appreciate the SOTA performance on the KITTI validation and test sets. The AC concurs with these important contributions and recommends acceptance.
The main contribution of this paper lies in the novel setting that is being considered: offline data without rewards is combined with meta training tasks to quickly adapt to new long horizon tasks at meta test time. Within this setting, it is shown that the combination of SPiRL and PEARL outperforms the individual algorithms. The technical contribution is limited, as now new methods are introduced. Nevertheless, the setting considered is interesting and the empirical evaluation is solid. For these reasons, I recommend acceptance.
This paper received 4 unanimous accept (including 1 marginal accept). This well written and clear paper clarifies the relationship between transformers and a recent exciting model of the medial temporal lobe in neuroscience. There was some clarifications requested by the reviewers that were addressed during the revision. This paper will make a great computational neuroscience contribution to this year ICLR!
The paper presents a method for collaborative task solving via an attention mechanism. The method is evaluated on manipulation task in simulation. 

The reviewers agree that the paper is well written and the idea is novel and intuitive. They also share concerns about the limited applicability (too many assumptions and only two robots) of the method and that it contains unjustified claims, and therefore does not meet the ICLR bar.  

Constructive feedback for the next version of the manuscript:

  The authors should decide if this is a robot learning (a learning based method that advances robotics, specifically robot manipulation) or a machine learning paper (a method that advances cooperative multi agent learning). The decision should drive the publication venue and the baselines. If the target is robot learning, the paper should consider adding on robot experiments. If the target is ML method, that more baselines and benchmarks from the MARL community should be added to the evaluation section. 
  The authors should be careful not to confuse safety guarantees, which have theoretical and analytical implications, with empirical evaluation without collisions.
  Evaluate the learning on more that 3 seeds.
This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis.

3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the main contribution interesting and significant enough for acceptance. Although I think this is a reasonable objection, it is not shared by the other 3 reviewers. Since the negative reviewer does not point out any critical flaws in the paper, I think the positive opinions should outweight the negative one in this case. I therefore recommend accepting the paper.
The paper introduces an object detection method that integrates vision and detection transformers through a novel Reconfigured Attention Module (RAM). Among other questions, the reviewers raised concerns about fair comparison with baselines, limited novelty of the RAM module, completeness of experiments, and missing details. The rebuttal adequately addressed these concerns with clarifications and additional experiments. R1 remained unconvinced that a simple modification to YOLOS could not be devised to improve the speed similar to the proposed method, but stated he/she wouldn’t argue strongly for rejection. While this is a legitimate concern, the AC agrees with R2 and R3 that the paper has enough merits to be accepted at ICLR, as the results are strong and are likely to have significant practical value.
This paper considers the idea of meta learning the loss function for domain generalization. It s a simple idea, that seems to work reasonably well. Although, as pointed out by the reviewers, the margin is actually quite modest when compared to the strongest baselines (not ERM).

On a positive note, many reviewers agree that the idea was simple, novel, and interesting. The insight that cross entropy can be improved for domain generalization is interesting. On the other hand, many reviewers pointed out that the, despite some careful empirical work, it s not clear why this idea works. I read the paper myself, and I agree that the paper could use a bit more work before it is ready for publication. Specifically, I agree with Reviewer eZ71, who asked for a clear justification of the proposed idea. The idea seems sensible, but there is some burden on the paper to provide insight, and not simply present an idea.  Here are some specific suggestions that came up during discussion, which could strengthen the paper:
  A more comprehensive discussion of the limitations of this approach.
  It would be good to understand how critical was the specific choice of parametric loss family. Here are some questions that would be good to address: does the parametric family interact with the type of domain shift in the datasets? Why are Taylor polynomials preferable or beneficial for domain generalization compared to, e.g., a linear combination of standard loss functions?
  Is the dataset on which you learn your ITL loss critical? I.e., how critical was the choice of rotated MNIST for learning the ITL loss? Does it generalize to very different and more diverse domain shift tasks, like those in the WILDS benchmark? It would be particularly interesting to see if loss functions meta trained on distinct datasets learn similar parameters.
  More broadly, evaluation on larger and more diverse domain shift tasks, like those in the WILDS benchmark, would further strengthen the conclusions in the paper.
This paper tries to improve the training of adversarial deep neural networks by avoiding fitting the “harmful” atypical samples and fitting more “benign” atypical samples. 

Overall, the main concerns are

1. The current presentation can easily cause some misunderstandings on the observations made in Section 3, especially [1] and [3] mentioned by the reviewer iXiX. 
  The authors may consider moving "related work" to the first half of the submission, and organize existing findings with rare/hard/atypical in a more principle manner. 
  Besides, as author mentioned in Section 3.1: "it is equivalent to a classification task based on an extremely small dataset, with one or a few training samples given". Such findings are natural and not novel to the deep learning community. Authors may consider shorten Section 3.1 and elaborate more in Section 3.2.

2. Theorem 1 and 2 do not help much. 
  It does not talk about the training algorithm and models, which over simplifies the learning problem. 
  Besides, the authors can consider some theoretical results how BAT improve the performance of typical samples, but still preserve the ability to fit those "useful" atypical samples. 
This helps to bridge the gap between motivation behind BAT and its algorithm design (raise by reviewer  ytJj and sm19).

3. It is also suggested to make observations more convincing. 
  Since authors want to claim their findings are universal, it is better to consider more adv training methods and datasets; it is also better to change the ratio of "normal samples" v.s. "atypical samples". In this way, the effect of atypical samples in adversarial training can be more carefully quantized.
The paper proposed learning partial and full equivariances from data in an end to end way. The problem studied is an important issue of existing equivariant neural networks which always assume full equivariance. However, the paper only got 3 "marginally below threshold" and 1 "marginally above threshold" even after rebuttal. The major challenges include technical parts being hard to follow due to multiple reasons, unsatisfactory paper writing (the updated version has undergone restructuring), the issue of "breaking equivariance" after multiple layers, some important experiments (such as comparing with Steerable G CNNs) missing, etc. After rebuttal only reviewer RivC raised his/her score to "marginally above threshold". Such scores are difficult to justify acceptance. The AC appreciated the authors for making great efforts on rebuttal and revising the manuscript. However, from the review comments it is clear that the paper still needs further revision (many of those clarified for the reviewers, e.g., explaining distribution for the discrete groups and adding more experiments, should be included in the manuscript). So the AC deemed that the paper is not ready for publication.
While the reviewers were somewhat split on this paper, they all found some strengths, and pointed out some weaknesses. Among these the main seems to be the somewhat incremental nature of the work, in particular with respect to PCL. As the authors point out, the differences w.r.t. PCL are meaningful and include the main thrust of the paper (removal of false negatives), and the results do indicate usefulness of the proposed approach. Given the wide interest in self supervision I think the paper is above bar for acceptance.
None of the reviewers recommended this paper. There were concerns that it is hard to draw meaningful conclusions from the experimental work due to the comparisons provided.  While the design of the block masking + contrastive learning proposed in this paper was rated as potentially being quite important, there remained some concern that subsequent tokenization steps could be problematic for "spatial heavy" datasets.

The AC recommends rejection.
The reviewers have the following concerns:
1. The theoretical results for the proposed method are weak. Theorem 4.2 cannot be considered as a convergence result, because the bound depends on some random variables $r_{T,i}$. The reviewers agree that a proper analysis would require some knowledge on the lower bound of these variables. Although there is some empirical explanation for this, the lower bounded assumption of  $r_{T,i}$ is not theoretically justified. The authors acknowledge that this is the main challenge for the present algorithm. In addition, the analysis requires bounded gradient and bounded function value, which is also strong for nonconvex settings. 
2. The empirical performance is not strong. In most experiments, the proposed method is not better than the baseline AEGD. The novelty and contribution of SGEM over AEGD is quite limited, since the idea of adding momentum is not new. 

The suggestions to improve this paper are as follows
1. Since the lower bounded assumption on $r_{T,i}$ is not standard and hard to verify, the authors might consider analyzing a theoretical guarantee for it. On the other hand, they could verify more experiments with various data sets to have some sense whether this assumption may be true or not. Next, please try to relax the strong assumptions as discussed. 
2. It is better if the authors can show the performance of SGEM for convex settings, and for other deep learning tasks (e.g. NLP) as suggested by the reviewers.

The authors should consider to improve the paper based on the reviewers  comments and suggestions and resubmit this paper in the future venues.
The authors propose a new methods for graph shortest distance embedding method called BCDR based on betweenness centrality. Then they show that the method is competitive both theoretically than experimentally with existing work.

After a discussion with the reviewers and after considering the nice changes in the paper and explanation in the rebuttal we agree that the paper contains some very interesting ideas but it is not probably ready for publication. The comparison with previous works is, in fact, still a bit limited and it should be extended. In addition the algorithm should also be tested on larger datasets.
The paper proposes a multi scale network that uses DEQ models to incorporate samples at multiple resolutions. The authors also propose a training strategy to improve the performance of the model. The authors investigate the interest of the approach through ablation and explainability, weighing the value of hierarchical heritage, diversity modules, perturbation size, and regularization penalties. 

The reviewers appreciated that the authors tackled the problem of incorporating multiple scales and the “impressive results” on CIFAR 10, CIFAR 100. The reviewers also expressed concerns regarding the computational assessment, in particular the additional computational/memory overhead of unrolling and what the authors mean by “explainability” in their experimental evaluation. The reviewers also made suggestions to organize the paper better. 

The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers who took part in the discussion considered that they are “satisfied by the response” and the “major concerns have been addressed”.  The feedback provided was already fruitful and the final version should be already improved. The ablative analysis and comparison to baseline is careful and thorough.

Accept. Poster.
This manuscript was the object of a rich and lengthy discussion. The AC also felt compelled to read the paper in details and discussed it further with the SAC.

The authors did a thorough job at addressing some of the reviewers points. The added results on cross entropy loss and additional discussion, as well as the points made in "Further Discussion on the Numerical Experiments" are very much appreciated. 

However, significant concerns remain on establishing connections with prior work, including related ideas on invariance from the causality literature, so as to gain deeper understanding of the implications of the proposed objective. We also strongly encourage the authors to further work on strengthening their theoretical analysis to clearly demonstrate the value of the proposed approach.

The proposed formulation is certainly thought provoking and we urge the authors to pursue their work in view of the above comments.
The authors propose a novel operator splitting method for solving convex relaxations of neural network verification problems, and develop and validate an optimized implementation of the same on large scale networks, focusing on the problem of verifying robustness to norm bounded adversarial perturbations.

The reviewers agree that the paper contains interesting ideas that are worthy of further development and that these ideas may prove useful eventually in pushing the envelope of what is possible in neural network verification. However, in its current form, the paper misses some key experimental evidence to rigorously evaluate the value of the contributions made:
1) Comparison against SOTA incomplete verifiers: The authors do not provide detailed and rigorous comparisons against well known baselines (for example the incomplete verifiers from Fast and complete (Xu et al., 2021), Beta CROWN(Wang et al. 2021)) 
2) Incorporating tighter relaxations: It would be valuable for the community to understand whether the proposed algorithm is compatible with tighter relaxations like those of (Tjandraatmadja et al., 2020). Even if they are not, it would be interesting to understand the comparison against standard solvers for these tighter relaxations compared against the advanced solver developed by the authors applied to the weaker relaxation.
3) Showing performance in the context of complete verification: While this is not a requirement, it would be great to see how the method performs in the conjunction with a branch and bound search, as this sometimes reveals surprising tradeoffs or weaknesses of incomplete verifiers (as observed in the results of Beta CROWN(Wang et al. 2021)).

I encourage the authors to strengthen the paper adding these experiments and resubmit to a future venue.
This paper proposes a few shot (untargeted) backdoor attack (FSBA) against siamese network based visual object tracking. Contributions can be summarized as follows: First, this paper treats the attack task as an instance of multi task learning and can be regarded as the first backdoor attack against VOT. Besides, a simple yet effective few shot untargeted backdoor attack is proposed and achieves significant effectiveness in both digital and physical world scenarios. This paper reveals the vulnerability of VOT to backdoor attacks caused by outsourced training or using third party pre trained models. One weakness is that threat model requires a very strong attacker with ability to modify the training algorithm, but only very simple defenses are considered. Overall, this is a good first attempt at showing vulnerability of VOT approaches.
This paper proposes SimpleBits, for simplifying input images to remove irrelevant details but keep relevant details for classification. This idea can be applied during/after training. Authors have significantly revised the draft to address reviewer concerns, to improve the readability and clarify concerns on complexity analysis, for which reviewers have raised scores post rebuttal. However, even with score changes, there are commonly expressed concerns, that manuscript still needs some more improvements to be ready for publication in their post rebuttal comments: findings are not very nontrivial or significant (reviewer eYVm), still incomplete (RfmX) or optimization algorithm is yet to be found (reviewer agcx) .
This paper focuses on how to improve video text retrieval via using additional user comments, and uses an attention mechanism to filter out the irrelevant comments. The main contribution is a context adapter module that allows learning from the auxiliary modality through an attention mechanism. The reviewers appreciated the overall idea s intuition and well written paper, but they also felt that the technical novelty is incremental, and that the treatment of user comments should be more intuitive via the dialogue thread structure. There were also concerns about the applicability of the context adapter module to more realistic scenarios with much longer videos, where the number of comments is very large, and where number of distractor comments is larger than the non distractor ones.
This paper set out to show that increasing task diversity during meta training process does not boost performance. The reviewers mostly  agreed (only reviewer wVFn dissented) that the empirical set up of the paper was convincing, but they also felt it over emphasized empirics over a deeper understanding of the phenomena observed. In turn, this resulted in discussions around how the experiments and the explanations didn t fully prove that increasing task diversity does not help. Overall, the discussion and the additional analysis tools provided by the authors (such as the diversity metric) will greatly improve the paper.
The paper presents a RL planning algorithms where a policy selects a reachable state. The empirical evaluation shows promising results in some environments. While all the reviewers agreed that the state planning RL is a relevant and promising direction, the reviewers expressed concerns with the rigor, significance of the results, and incremental novelty.

To improve them paper the authors should:

  Bring the theoretical foundation in the main text, and add more rigorous analysis, including the limitations of the method.
  The readability of the figures needs to be improved. The legend on the figures is too small and colors are too similar that renders the figures unreadable and confusing.
  If the authors  goal is to develop a method for interpretable RL, then some results and analysis need to address the interpretability of method.
The paper proposes a new method to combine global and local image features, targeted at image retrieval applications. The main idea is a model branch where both spatial and channel attention are used. The local feature branch undergoes supervision directly and this branch’s output is concatenated to the global feature branch’s output in order to eventually produce the final image embedding. 

The reviewers appreciated the care in the evaluation (ablative analysis) and the promise of the approach compared to existing baselines. The reviewers also expressed concerns about several claims, for instance that the proposed approach is able to learn homography transformations, the quality of the exposition, and missing baselines. The reviewers also pointed out that several parts of the paper were hard to follow and important details were missing. 

The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers considered that ‘the concatenation of local features with global ones works does not mean at all that some geometric transformation is learned’ and the justification provided for omitting baselines (suggested by the reviewers) were unconvincing. The feedback provided was already fruitful, yet major issues still remain.

We encourage the paper to pursue their approach further taking into account the reviewers  comments, encouragements, and suggestions. The detailed feedback lays out a clear path to generate a stronger submission to a future venue.

Reject.
Two of the initial reviews of the paper were mildly positive (2 scores of 6), and one was very positive (score of 8). However, these reviews failed to notice some severe issues with the paper, which were detailed by the Area Chair in an Extra Review which was provided late. The severe issues include: clarity of exposition (undefined notation in many places) and theory (vacuous or meaningless theorems and assumptions). I apologize to the authors for not having had the chance to defend against this late review. However, the issues are indeed severe.
This submission proposes a new loss function for facial attribute GAN editing and transfer via text inputs. 
A latent mapping mechanism based on StyleCLIP is used to disentangle the semantic attributes of human face. 
The resulting semantic directional decomposition network (SDD Net) transfers attributes from reference image to a target guided with text descriptions. Experiments show on CelebA HQ dataset some qualitative results and ablations  for the « smile » attribute.

The main contribution is essentially a loss term that measures latent similarity in CLIP latent space.
Most of the reviewers are not convinced by the approach and have raised several issues. One can question the relevancy of the way that text features are used (as a semantic direction). The role of the reference image in attribute transfer is is also questionable in the proposed framework.
Additionally, evaluation is not sufficient, in particular to investigate whether the proposed method works on a wide range of attributes.
The paper only conducts experiments on CelebA HQ dataset. It would be interesting to have experiments on other  datasets.
The comparison to StyleCLIP is also insufficient, and there are no quantitative experiments to support the authors  claims. 
We encourage the authors to take into account all these remarks and Rs  comments in order to get an improved proposition for a future conference.
The authors study the problem of open ended knowledge grounded natural language generation, in the context of free form QA or knowledge grounded dialogue, focusing on improving the retrieval component of the retrieval augmented system. By retrieving more relevant passages, the generations are more grounded in retrieved passages. 

Pros:
+ The paper is clearly written and motivated.
+ Presents a straightforward approach that shows improvement over a  strong baseline.  
+ A strong paper focuses on a rather under explored problem of knowledge grounded open ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics.
+ The authors included human evolution results to support their findings. 
+ The authors did a good job addressing several questions raised during review period and added several new experiment results and discussions to strengthen their findings. The reviewer team was generally satisfied.

Cons:

+ Several related work on knowledge guided dialog response generation is missing in the paper. Although the paper s focus is on retrieval based QA systems, the main focus is on open domain generation, which has overlaps with dialog response generation research. So the authors should cite the following papers in their paper: [1] Dinan, Emily, et al. "Wizard of wikipedia: Knowledge powered conversational agents." arXiv preprint arXiv:1811.01241 (2018). [2] Zhou, Kangyan, Shrimai Prabhumoye, and Alan W. Black. "A dataset for document grounded conversations." arXiv preprint arXiv:1809.07358 (2018). [3]Zhan, Haolan, et al. "CoLV: A Collaborative Latent Variable Model for Knowledge Grounded Dialogue Generation." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021. [4]Zhao, Xueliang, et al. "Knowledge grounded dialogue generation with pre trained language models." arXiv preprint arXiv:2010.08824 (2020). 
+ There are several related work concerning with generation of a response given a relatively small set of evidence text such as the following ones:  [5]Lian, Rongzhong, et al. "Learning to select knowledge for response generation in dialog systems." arXiv preprint arXiv:1902.04911 (2019). [6]Kim, Byeongchang, Jaewoo Ahn, and Gunhee Kim. "Sequential latent knowledge selection for knowledge grounded dialogue." arXiv preprint arXiv:2002.07510 (2020). Although these work do not include a retrieval part, the authors should cite and discuss similarities and differences to [5] & [6] in their paper.
The paper addresses the question of skill discovery in reinforcement learning: can we (without supervision) discover behaviors so that later (when supervision is available via a reward signal) we can learn faster? The paper proposes a new contrastive loss that an agent can optimize for this purpose, based on a decomposition of mutual information between skills and transitions. The reviewers praised the extensive experimental evaluation and good empirical results, as well as the analysis of failure modes of related algorithms.

Unfortunately, there appeared to be errors in the derivation and implementation. (These include typos in derivations that made them difficult to follow, as well as uploaded code that didn t match the experimental results.) While the authors claim to have fixed all of them, the reviewers were not all completely convinced by the end of the discussion period. In any case, these errors caused confusion during review; so, whether the errors are fixed or not, it seems clear that there hasn t been time for a full evaluation of the corrected derivations and code. For this reason, it seems wise to ask that this paper be reviewed again from scratch before being published.
Authors propose an autoencoding echo state machine for a one shot one class time series classification task. Their approach feeds a (one dimensional) error signal over time relative to a reference training datum to SVMs. Training is very fast by design. OFC signal analysis has practical value in neuroscience. But only one benchmark (seq MNIST) was used to evaluate their method. While the performance seem impressive, no explanation of why the internal representation learned by the proposed system is superior and robust to noise was provided. No sequential autoencoders or latent neural trajectory inference methods were compared. Although the manuscript has greatly improved through the review rebuttal process, there are missing key details (e.g. length of E(t) used for classification important for real time application, initial state for the reservoir, choice of W_in  important since it seems to be a chaotic network that s driven by strong input). While there is novelty in the approach, there is a general lack of enthusiasm among the reviewers for the manuscript as is. The reviewers and AC strongly encourage the authors to further developed these ideas and add thorough analyses for another conference.

(BTW, perhaps it s worth citing https://doi.org/10.1109/IJCNN.2016.7727309, since autoencoder combined with reservoir computing has been used for anomaly detection.)
This paper seeks to find an answer to some quite interesting research question: can deep vanilla networks without skip connections or normalization layers be trained as fast and accurately as ResNets? In this regard, the authors extend Deep Kernel Shaping and show that a vanilla network with leaky RELU family activations can match the performance of a deep residual network.

Four reviewers unanimously suggested acceptance of the paper. There were concerns about the clarity or marginal performance improvement. However, they all including myself agree: achieving the competitive performance with the vanilla deep model itself can be seen as a big contribution and the clarity has been improved to some extent through revision.
The authors proposed a new loss function for end to end edge detection to overcome the label imbalance and edge thickness problems. Overall, the proposed VT appears to be a useful representation for boundary detection. Though similar to DT, VT outperforms DT by a large margin and is more robust to noise. One reviewer recommends acceptance, two others recommend marginal acceptance. The main issues have been adequately addressed in the rebuttal.
This paper proposes monotonic graph neural networks (MGNNs) for the transformation of knowledge graphs. Specifically, MGNNs transform a knowledge graph into a colored graph where each node is represented by a numeric feature vector and each edge encodes the node relationship with different colors. The authors provide theoretical analysis showing that monotonic constraint can enable the model to derive logical inference rules in Datalog, and thus the trained model is explainable. 


The authors addressed most of the concerns raised by the reviewers, such as motivation, runtime, and comparison with existing baselines. Three of the four reviewers are positive (with the scores of 6 or above) towards acceptance after rebuttal discussions, and the remaining reviewer gives a score of 5 (below acceptance threshold) thinks that this work still lacks novelty, but he/she is not against acceptance if other reviewers choose so. Considering this work makes a good exploration on explainable graph neural networks, which is an interesting and important research direction, we recommend for acceptance. We thank the reviewers and the authors for their active discussion.
The paper proposed an imputation free method to handle missing data by learning an input encoding matrix using RL with the prediction error as reward/penalty signal. Reviewers appreciate the interesting setup where RL is used to deal with missing data, and the method being imputation free. Three out of four reviewers (reviewer he3p, azSY, and 4Cb5) have raised concerns on the complexity of the proposed method, but it seems like all the reviewers see the strength of the work outweigh the weakness.
There are many discussions among the reviewers for this paper and eventually none of the reviewers (including the one who gave most positive score) would like to support the publication of this paper. 

Some concerns from the reviewers are as follows:
1. Missing the discussion on storage cost. 
2. The improvement is limited. $G_0$ must be small and independent of $n$, hence it is not clear if it is possible to give a fair comparison between the current complexity and previous best complexity. 
3. Missing the discussion on the case when $n \leq \mathcal{O}(\varepsilon^{ 4})$ of the state the art results. 
4. For the complexity results in terms of $\varepsilon$, it requires $\varepsilon$ to be arbitrarily small. The authors should also discuss this point for comparing with their result. 
5. Some other statements in the papers are overclaimed. 

Please take the comments and suggestions from the reviewers carefully to revise the paper for the future venues since they raised valid points.
The paper proposes a modulation layer to address the problem of missing data.

The results do not show that the approach outperforms existing sota approaches.
The results do not demonstrate that the proposed modulation layer is an improvement over attention layer.
Many smaller errors (spelling, etc.) where found in the manuscript.
Experimental details are insufficient to make the results reproducible.
The authors have not provided a response to the reviewers.
This paper theoretically studies the convergence of memory based continual learning with stochastic gradient descent, and suggested several methods based on adaptive learning rates.

The reviewers appreciated the novelty of the direction, and some of them thought the experimental results are promising.

However, most reviewers (3/4) were negative. I think the main reason was the paper presentation and clarity, which they found lacking (and I agree). One reviewer thought the experimental evaluation should be improved, but there might have been some misunderstanding there. Lastly, even the positive reviewer thought the results were somewhat incremental and non surprising.

I hope the authors improve their paper and re submit.
The paper performs an empirical evaluation of deterministic methods for the quantification of epistemic uncertainty.  There is no new algorithm.  The main contribution is the empirical evaluation.  This empirical evaluation will be useful for the community. It is an independent evaluation that casts some doubts on the calibration of several existing deterministic techniques, which will spur additional research. However, the paper is not well written. As pointed out by the reviewers, the paper does not provide much background. It refers to many  concepts without defining them. The concepts are not new (references are provided for each concept), but since the paper does not describe any new technique it should do a good job at explaining those concepts. The authors added some explanations in the supplementary material, but some of those explanations should really be in the main paper.  The most important issue with the paper is that it does not explain why the deterministic techniques do not seem to be well calibrated. The authors added a "theoretical justification" in section 6.1, but it amounts to saying that deterministic methods make a point estimate, which is too general to explain much. An important factor for proper generalization and calibration is the inductive bias of the model. At the end of the day, if we generate data from a model, then that model will be better calibrated than the other models. So a discussion of the inductive bias of each model and how this inductive bias relates to the properties of each dataset would have been much more insightful.
In this paper, in order to theoretically investigate the relationship between graph structure and labels in GNNs, interaction probabity and frequency indicators are introduced and analyzed, and a new family of GNNs with multiple filters is proposed based on the insights from the theoretical analysis,
In the discussion, there was an opinion that the theoretical analysis is interesting, but its novelty and clarity are limited. Although certain contributions are acknowledged, the impact is marginal and the audience for which this paper will matter is rather limited.
In general, the reviewers were lukewarm about the paper. They all acknowledged the strength of the paper: it is well written, HCCL showed (somewhat) improvements over previous methods, and it is easy to implement. However, it still feels incremental, and the improvement over the full training setting is small due to the natural limitation of consistency assumption. The AC feels that while there is merit of the proposed method, the impact seems to be limited to specific scenarios such as limited epochs.
The authors propose a reference less metric for evaluating NLG systems by training a discriminator which distinguishes between human generated and machine generated text. 

The main concerns raised by the reviewers were (i) lack of clarity in certain portions of the paper (ii) lack of demonstration of the "universal" applicability of the proposed metric (only evaluated for poetry generation) (iii) lack of clear guidelines on how to use the proposed metric in a reproducible manner (iv) lack of details about what exactly does the proposed metric capture and look for in the generated text.

The authors did not respond to the specific queries of the reviewer and agreed that more work is needed on their part.
In this paper, the authors propose a method to generate sets, which are order invariant, with a sequence to sequence model. The main idea is to order the elements of the sets, and then treat them as regular sequences. The authors propose to use PMI and conditional probability to obtain a partial order on the elements of sets. Overall, while the reviewers note that the proposed method is simple and intuitive, they also raised concerns about the paper: one of the main concerns is about missing baselines, such as non seq2seq models for set generation, such as binary classification (to predict whether an element should be included or not). For this reason, I recommend to reject the paper.
This paper proposed a new approach to jointly model text and stock price information and fuse them for stock market forecasting. It encodes text and stock price information in parallel and then fuses them using a co attention transformer. According to the reviewers, the design of the model is not very well justified and seems to be a little ad hoc. The authors spent quite a few pages introducing background knowledge and the novelty of the proposed model is not sufficiently described. Some details in the experiments are missing, and it is not clear whether the results could be easily reproduced.  There are many writing issues too. As a result, we do not think the paper is ready for publication at ICLR in its current form. BTW, after the reviewers posted their comments, the authors did not submit their rebuttals.
The paper proposes a data augmentation approach called SpanDrop to help to distill supervision signals from a long sequence prediction problem. The reviewers generally agree on two major drawbacks of the paper. First, the novelty of this approach. Second, the experiment results are not very convincing.

After reading the responses from the authors, I don’t think the authors convinced me of the novelty of the work, especially when comparing it to the word dropout. No matter if you treat the data or the model as a black box, it’s effectively doing the same thing. Apart from that, the model can only be used in the setting of “underspecification” long sequence tasks, which diminishes its value in real applications.

On the experiment side, there are three issues. One, many tasks considered are not long sequence tasks. Second, the improvement is marginal in many cases. Three, more related methods should get considered as baselines. Besides these three points raised by the reviewers, I also want to raise the point that it is not (and should not) be acceptable to report ALL your language experiment results on dev sets. I understand it is more time consuming to get the test results on tasks where the test has to be done online, e.g. SQuAD. However, it is not a good practice in reaching a conclusion merely on dev sets in general.

Based on the reviewers  comments and the reasons listed above, I recommend rejection of this paper.
The paper presents an analysis of the benefit of unsupervised contrastive learning for downstream classification tasks using the cross entropy loss. Building on prior work, the authors show that the contrastive loss can be bounded in terms of the cross entropy term and an “intercept” term which depends logarithmically on the number of negative samples per positive sample (for contrastive learning) rather than polynomially as in the prior work. 

There are several differences between the setting here and that of the prior work by Arora et al. (2019). First, the work here focuses only on cross entropy loss and leverages the similarity of the loss structure between the contrastive loss and the cross entropy loss. Second, the assumptions here are different, e.g., boundedness of the representation. Finally, the assumption that latent classes are the same as the label classes (which is not the case in the prior work) is significantly restrictive. 

The writing is poor and the presentation is not clear. Despite the title and various references to learning bounds in the abstract and the main text, there are no learning bounds in the paper. The main result is to bound the contrastive loss in terms of the cross entropy loss under the assumption that the latent classes and the label classes coincide. Authors state that getting generalization bounds is routine and, therefore, they chose not to give them — I do not see how generalization bounds follow in a straightforward manner here, and even if they do, it is important to write them for completeness. 

The main contribution here is that the bounds depend logarithmically in K — the number of negative samples per positive sample — compared to sqrt{K} in the previous work. The previous bound however holds for Lipschitz losses as well, for e.g., hinge loss. So the question remains whether this improvement is only for the cross entropy loss. Regardless, K is typically small in practical applications. Even the experiments in the paper (Figure 7) suggest that the performance degrades for larger K even on simple tasks. So, the improvement is really somewhat insignificant. 
 
The reviewers were generally positive and appreciated the paper. However, in the light of comments above (of which I am quite certain), unfortunately, I am unable to accept the paper at this point. I believe the comments above (and from the other reviewers) will help improve the overall quality of the paper. I encourage the authors to incorporate the feedback and work towards a stronger submission.
Authors present a method to disentangle epistemic from aleatoric uncertainty for avoiding the noisy TV problem during self driven exploration. This is an important area where we need more ideas and experiments. The authors present a biologically inspired approach and through experiments. Although it doesn t present the state of the art exploration in well known RL environments, I acknowledge that new solutions to problems that were previously intractable often would face such an issue. The prediction to discriminate neuroscientific modulations that directly encode epistemic and aleatoric uncertainty is bold but not very specific. Unfortunately, as the reviewers noted, the manuscript in the current form doesn t quite meet the bar yet. I suggest comparing methods for directly estimating uncertainty. I also suggest adding discussion on the estimation bias for the epistemic uncertainty for the proposed method. I strongly encourage the authosr to continue this interesting line of work.
This paper develops a Python library for geospatial data based on Pytorch, TorchGeo. TorchGeo is a useful tool for applying deep learning methods to geospatial data. The reviewers agrees the contribution of this library. It will help machine learning researchers to use geospatial data and help geospatial researchers to apply machine learnig methods. However, the technical contribution is low, and the novelty is not high enough since the results can be achieved by a combination of existing packages.
The paper presents an approach for spatio temporal representation learning using Transformers. It introduces a particular architecture design, which shows an impressive computational efficiency. The reviewers agree that the experimental results are strong, and unanimously recommend the paper for acceptance. The reviewers find their concerns regarding the details of the approach/setting address after the authors  response.

We recommend accepting the paper.
The paper proposes a strategy for incrementally pruning deep learning models based on activation values. The approach can satisfy different kinds of requirements, trading off between accuracy and sparsity.

The approach seems promising and seems to have competitive performance. However, the method is described by reviewers as a combination of ideas that have been proposed in the literature, and the experimental evaluation relies too much on a dataset considered too small to be reliable in such experiments   CIFAR10. We do not expect substantial experiments within the rebuttal period: such comparisons with relevant SOTA methods should have been present in the submission. Moreover, the strategy proposed for selecting a threshold seems to rely on some doubtful assumptions, and there are no benchmarks on actual runtime.

The writing has improved based on reviewer input, and the reviewers are satisfied with this aspect. I would still add that I would prefer some clarity in the method presentation: is there a quantity being optimized? is there a value we can monitor to ensure our reimplementation is correct? etc. In addition I would like to ask authors in the next revision to be mindful to the difference between `\citet` and `\citep` in author year citations   see e.g. the first two ones in 3.1.
There is consensus in the reviews that this paper convincingly demonstrates strong acceleration of policy learning using differentiable simulation in tasks involving contact rich dynamics. The authors are encouraged to explore where the smoothness assumptions made in the simulator actually transfer to real robots. The paper may be further strengthened through more complex benchmarks involving contact rich manipulation.
The paper receives mixed ratings. All reviewers agree that the paper is well motivated and the updated draft is clear. However, the experiment results are not fully convincing especially given the added complexity. In addition, it is hard to fully understand where the gain comes from. We hope the reviews can help improve the draft for a strong publication in the future. 
Four experts reviewed this paper and rated the paper below the acceptance threshold. The reviewers raised many concerns regarding the paper, mainly the lack of empirical studies and clarity. Some reviewers also suggested the authors better positioning the paper in the literature by discussing more related works. The rebuttal did not address all concerns. Considering the reviewers  concerns, we regret that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper introduces the maximum n times coverage, a new NP hard (and non submodular) optimization problem. It is shown that the problem can naturally arise in ML based vaccine design, and two heuristics are given to solve the problem. The results are used to produce a pan strain COVID vaccine. 

The reviewers and I think that this is an interesting paper with a compelling application. There were some concerns about theoretical novelty and biological accuracy but these were addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews in the final version of the paper.
The authors theoretically analyze learning of two layer neural
networks by gradient descent with respect to a data distribution that
exposes how useful features are learned during training.

Overall, the reviewers felt that the analysis yielded useful insight,
and was original.

During the discussion period, a reviewer recommended that the authors
look at papers providing lower bounds on statistical query learning of two layer networks,
and consider comparing the lower bound technique of this paper with that earlier work.
This paper proposes an alternative approach to epsilon greedy exploration by instead generating multi step plans from an RNN, and then stochastically determining whether to continue with the plan or re plan. The reviewers agreed that this idea is novel and interesting, that the paper is well written, and that the evaluations are convincing, showing large improvements over epsilon greedy exploration and more consistently strong performance than other baselines. While the original reviews contained some questions around discussion of related work and the simplicity of the evaluation environments, the reviewers felt these concerns were adequately addressed by the rebuttal. I agree the paper explores a very interesting idea and convincingly demonstrates its potential, and should be of wide interest to the deep RL community especially as it touches on many different subfields of RL: MBRL/planning, exploration, HRL, navigation, etc. I recommend acceptance as a spotlight presentation.
The manuscript proposes an experience replay method that supports two time scales of memory, as in complementary learning systems from the cognitive sciences literature. The authors demonstrate that their method on a wide range of benchmarks and after the rebuttal demonstrate it on one additional benchmark.
The reviewers raised a number of questions and concerns around additional experiments (benchmarks and ablations), online training, the cost of training, number of hyperparameters, further analysis, clarifications, and citations. The authors address the majority of these concerns during the rebuttal period, and overall the reviewers were in favour of acceptance. Therefore, I recommend acceptance of this work.
This paper is on the right track to be accepted after a revision but it is not ready yet. The reviewers were mostly puzzled by different details in the evaluation process, however, as far I can see, most of them should be possible to address. My impression is also that the authors might be more used to a slightly different style of paper writing, more similar to what is typically accepted at MICCAI, ML4H, MIDL etc. I think that a lot can be improved in this respect just by carefully analyzing the reviews.
This paper proposes a new method for understanding the role and importance of individual units in convolutional neural networks. The reviewers were in agreement that the technique is novel and provides potentially valuable insights into neural network behavior. The reviewers were less certain about the utility or significance of this idea; however, the authors partially addressed this concern by adding studies of using this technique as a pruning heuristic, and future researchers will be the best judge of the paper s eventual significance. With that in mind, I recommend acceptance so that this intriguing idea can become part of the research literature and future researchers will have this opportunity.
The paper points out an interesting and, to me unexpected, problem when learning Q functions to do with spectral bias. Figures 1 and 2 are quite striking. The diagnosis and proposed solution elegantly combines ideas from NTKs and NeRFs. The proposed random Fourier actor critic performs well in practice. The main problem reviewers had in the end is that the authors added substantial new empirical results too late to review thoroughly.
This paper studies the problem of producing distribution free prediction sets using conformal prediction that are robust to test time adversarial perturbations of the input data. The authors point out that these perturbations could be label and covariate dependent, and hence different from covariate shift handled in Tibshirani et al 19, the label shift handled in Podkopaev and Ramdas 21, and the f divergence shifts of Cauchois et al 2021. 

The authors propose a relatively simple idea that has appeared in other literatures like optimization but appears to be new to the conformal literature: (i) use a smoothed (using Gaussian noise on X, and inverse Gaussian CDF) nonconformity score function, in order to control its Lipschitz constant, (ii) utilize a larger score cutoff than the standard 1 alpha quantile of calibration scores employed in conformal prediction. The observation that point (i) alone lends some robustness to adversarial perturbations of the data is interesting. As several experiments in the paper and responses to reviewers show, this comes at the (apparently necessary) price of larger prediction sets. 

I read through all the comments and also the supplement. The authors have responded very well to all the reviewers questions/concerns, adding significant sections to their supplement as a result. Three reviewers are convinced, but one remaining reviewer requested additional experiments to compare with Cauchois et al (in addition to all the others already produced by the authors originally and in response to reviewers). However, the authors point out that the code in the aforementioned paper was not public, but they were able to privately get the code from the authors during the rebuttal period. At this point, I recommend acceptance of the paper even without those additional experiments, since it is not the authors  fault that the original code was not public. Nevertheless, I suggest to the authors that, if possible, they could add some comparisons to the camera ready since they now have the code.

I congratulate the authors on a nice work, a very solid rebuttal, and also the astute reviewers on pointing out various aspects that could be improved. 

Minor point for the authors (for the camera ready): I would like to comment on the Rebuttal point 4.4 in the supplement, which then got further discussed in the thread. The reviewer points out four references [R1 R4]. I will add one more to the list [R5] https://arxiv.org/pdf/1905.10634.pdf (Kivaranovic et al, appeared in 2019, published in 2020). I think the literature reviews in this area are starting to be messy, and all authors need to do a better job. Clearly, the original paper of Vovk et al already establishes various types of conditional validity (and calls it PAC style guarantee), produces guarantees that others in this area produce, and it appears that much recreation of the wheel is occurring. For eg, [R2, R4] do not cite [R5], despite [R5] appearing earlier and being published earlier, and having PAC style guarantees and experiments with neural nets, etc. However, in turn, [R5] do not cite Vovk [R1], but [R2, R4] do cite [R1]. (And [R3] does not seem to be relevant to this discussion of conditional validity?) In any case, I am not sure any of these papers need citing since the current paper does not deal with conditional validity. If at all, just one sentence like "Conditional validity guarantees, of the styles suggested by Vovk [2012], would be an interesting avenue for future work".
The paper studies self training for a one hidden layer architecture, showing that with proper initialization self training will improve over standard supervision. The reviewers appreciated the analysis and thought the results make sense. However, they did comment that the paper does not provide sufficient insight about the effectiveness of self training and this should be discussed in the final version. There were additionally comments about the architecture choice (e.g., fixed output weights), and the authors should also address this.
Summary of the paper and the reviews:
The authors propose a method to explain the GNN predictions in a task agnostic setting, meaning that the method can be applied to a new downstream task without fine tuning. The task is formulated as to predict the important subgraph given the input graph and the ground truth label. The learning algorithm optimizes the mutual information between the embedding of the subgraph and the original graph. The experiment shows the quantitative improvement measured by the fidelity score, the qualitative visualization of highlighted subgraphs and comparison of the cost w.r.t the baseline GNN explainer models. 
Strength: 
1) The task agnostic setting is novel.
2) The proposed method shows improvement in the fidelity score with a reduced training cost over the baseline models
Weakness: 
1) The proposed objective requires additional justification. To optimize the intractable mutual information objective, the authors propose JSE and InfoNCE as upper bound estimations of mutual information, but the negative sampling technique in the proposed method is not fully justified.
2) During training, the authors simulate the task specific importance vector by sampling masks from a Laplace distribution. During testing, the importance vector is obtained by gradient based approach. Further analysis is needed to quantify the effect of this training/testing discrepancy.  
3) In the empirical experiment, the proposed task agnostic model outperforms the task specific baselines. Why should such an outcome happen? The reason needs additional analysis but is not provided in the current paper. Moreover, the qualitative results of a few examples are not sufficiently convincing for the reported empirical success.

Summary of the discussions and the decision by reviewers: One reviewer asked for a justification of the negative sampling approaches used to approximate the mutual information objective. While the authors described their implementation design in their rebuttal, the theoretical justification of the method was not enough. Two reviewers raised the question about how randomly sampling importance masks during training could affect the downstream tasks performance, which was not fully addressed in the rebuttal. Other than that, the experimental concerns about new baselines and datasets were well addressed by the authors.

Recommendation: The paper has received borderline review scores (5, 5, 5, 6). Although the authors addressed some of the concerns in their rebuttal on the experimental design and added important baselines, more convincing justifications/analysis for the proposed method are still missing. Therefore, the reviewers didn’t raise their scores. Based on the above concerns, the recommendation is to reject.
Although reviews were initially a little polarized, they trend toward accepting the paper after rebuttal and discussion.

The most negative review raised issues of datasets, baselines, and experiments, and various details that they find confusing. These concerns were not shared by the other reviewers for the most part. Following a detailed rebuttal the most negative reviewer ended up siding with the more positive reviewers.
The paper introduces GANGSTR, an agent that performs goal directed exploration both individually and "socially", with suggestions from a partner. It builds a graph of different configurations of a 5 block manipulation domain, and navigates this graph. The theoretical motivations for this algorithm are solid, and the direction is interesting. However, the results are less than convincing. In particular, as was mentioned in the discussion, it is not clear how this algorithm would generalize beyond the very simple 5 block manipulation domain. While having a simple benchmark has the advantage that you can explore it in depth, it also might obscure problems with the algorithm, unless complemented by a set of other benchmarks. It therefore seems that the paper is not ready for publication yet.
First, I would like to thank all the reviewers for their efforts in reading and understanding this paper. I tried to read the paper as well and I also find it s really difficult (if possible) for me to understand the ideas presented here. The most important task in writing a paper (as Reviewer Svha also suggested in his/her review) in the field of machine learning is to explain to your peers what is the problem you are trying to solve and how you solve (or partially solve) that problem. I think there is a consensus among the reviewers that the paper did not do a great job of that. I am not questioning the quality of the idea or the research here, but I think the paper here will need to do a significantly better job here in explaining the idea before it can be a good ICLR publication.
The reviewers all generally find the paper both well motivated in addressing an important challenge as well as well written. However, there s quite a bit of hesitation around whether the proposed metric is convincing enough as an approach to measure local calibration.

Reviewer 76PS and 784d s concerns around the choice of feature map and associated hyperparameters remain unaddressed, and I agree with their concern. There is no clear understanding of what constitutes a "good" feature map, which makes the metric quite difficult to use whether as a benchmark of ML methods or for general application.  I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue.
This paper presents a new approach for learning binary latent variable models using evolutionary optimization.

Pros:
* A new perspective to learning binary latent variables is proposed using evolutionary algorithms.
* The proposed method works well on auxiliary tasks such as zero shot denoising and inpainting. 

Cons:
* The proposed evolutionary optimization performs poorly on the binary VAE problem. 
* It has a high computational cost that will limit its application in real world problems.
* An in depth comparison with prior work on learning discrete latent variables is missing. This may include MCMC based approaches, REINFORCE based techniques, REBAR or RELAX. 

This paper presents an interesting direction for learning binary latent variables using evolutionary algorithms. However, the proposed method performs poorly on the binary VAE problem which is the core problem, targetted in this paper (See likelihood values for binary VAEs in Fig. 15). The reviewers have raised concerns regarding the computational complexity of the evolutionary method in practice. They have also criticized the missing baselines for the binary VAE experiments. 

The authors have argued that the proposed method excels at auxiliary problems such as zero shot image denoising and inpainting. However, these problems are not the central problem of this submission, and naturally, they have not been discussed, reviewed, and evaluated thoroughly. They can be also addressed with non binary VAEs and other forms of generative models which are not discussed in the paper. 

Given these concerns, we don t believe that this submission in its current form is ready for publication at ICLR.
This paper studies the challenging problem of object centric generation of visual scenes. While the paper has some novel ideas that make it interesting, its (quantitative and qualitative) comparison with existing methods is currently premature to allow drawing conclusions with sufficient evidence.

Instead of claiming that existing models cannot do well for the more realistic datasets mentioned by reviewer dAqW, it would be more convincing to conduct a comprehensive experimental study by comparing the proposed method with existing methods on a range of datasets, from simple ones to more realistic ones. The synthetic Fishbowl dataset introduced in this paper can be one of them.

Moreover, the clarity of the paper could be improved to make it appeal better to the readers.

All three reviewers engaged actively in discussions (both including and not including the authors). Although one reviewer recommends 6 (weak accept), the reviewer also shares some of the concerns of the other reviewers. As it stands, the paper is not ready for acceptance. If the comments and suggestions are incorporated to revise the paper, it will have potential to be a good paper for future submission.
The work proposes a simple neural network framework for detecting anomalies on sequential data. The manuscript is quite rough. The paper needs significant editing. The authors should take the reviewers  recommendations to heart and make deeper changes to the paper.
The paper provides two new generalization bounds for non linear metric learning with deep neural networks, by extending results of Bartlett et al. 2017 to the metric learning setting. The main contribution of the paper is by extending the techniques of Bartlett et al. from a classification setting to the metric learning setting (which has very different objectives) and consider two regimes. In the first regime the techniques are fairly similar but the second regime is more novel. However, the current version of the paper does not highlight the similarity and differences between the results and techniques with Bartlett et al. 2017; it also does not give sufficient intuition on how the metric learning setting is fundamentally different from the classification setting and how the paper leverage the difference to get improved bounds. All the reviewers had some confusions to different degrees, and the paper would be much stronger if it can explain the intuition and make more explicit comparisons.
While the reviewers found several interesting points about the paper, they raised several issues, which prevents me from recommending acceptance of the paper. In particular, the paper is not positioned properly in the literature, hence the novelty and the contributions are not properly clarified. The approach of the paper is reasonably simple (which would be a good thing by itself), but there seem to be natural avenues along which more complete results could be obtained, as mentioned in the reviews. Finally, the experiments should be improved (e.g., comparing with other algorithms from the literature). In summary, this is a promising work, but it requires some improvements before it can be published.
This paper explores large scale supervised multi task training across 107 NLP tasks combined with self supervised C4 masked span infilling, using the T5 sequence to sequence model.  The results improve over prior strong T5 baselines on several NLP benchmarks such as SuperGLUE, GEM, and Rainbow.

The paper s main strengths are the scale and large number of tasks, the release of the trained models and data, as well as the clarity and presentation.  Reviewers had concerns with the novelty, limitations in the evaluation (to just T5, and to just SuperGLUE in portions of the paper), and the potential impact of hyperparameters on the results.  During the discussion period, the authors noted that it is not obvious a priori that their approach would work, and that their evaluations on other tasks made it unlikely to be overfitting to SuperGLUE.  They also noted that running the additional hyperparameter experiments suggested during the reviews were computationally prohibitive.

Overall, despite the drawbacks and relative lack of novelty, the extensive experiments and released models provide significant value and will be of interest to the research community.
This paper trains a neural network to predict expert strategies (described in natural language) in the game of Angry Birds. While the reviewers agreed this was potentially interesting, there was also a consensus that the scope of the paper was too narrow, that the writing was imprecise, and that the evaluations too few and too qualitative. I agree the paper does not seem thorough enough for ICLR, and recommend rejection.
The paper proposes a language modeling architecture based on the RNN cells leveraging Legendre memory units. The proposal is interesting, but as all the reviewers notice, the paper is not ready for the presentation in the top ML conference for several reasons: comparison with weak baselines, shallow or weak analysis of the presented results, insufficient discussion of the related work, etc. Looking forward for all the comments to be addressed by the authors.

In the rebuttal the authors addressed some of the questions but all the reviewers think that the paper is not ready for acceptance and careful rewriting is needed. Recent research on the improved RNN mechanisms suggests that Legendre memory units and related mechanisms might be a gateway to solving several standard issues of training regular RNNs so the topic is definitely of great importance. Thus the authors are highly encouraged to resubmit the paper after making all suggested corrections.
This paper introduces a control based approach to sampling. All of the reviewers found the idea interesting. There were serious concerns by some of the reviewers regarding how the paper positioned itself relative to the literature, how it designed baselines for experiments, and how it compared itself to existing methods. There was vigorous rebuttal phase. The authors submitted a slightly late revision based on a procedural misunderstanding, and I decided to incorporate their late revision.

Based on the revision, the majority of reviewers felt that the paper was at least above the bar for acceptance and some of the more positive reviewers stood strongly by the paper. I believe that this paper is of value to the community, so I will recommend that it is accepted, but I want to be very clear about something: the authors **must** incorporate the late revision as the basis of their camera ready and I **strongly recommend** that they address the concerns of reviewer d7Mk, including but not limited to:

  "Our approach avoid long mixing time theoretically and is more efficient": this claim is too strong.
  Figure 1 is not particularly informative and the authors should reconsider it.
  The presentation of Eq (2), Algorithm 1, and Algorithm 2 should be simplified. 
  Section F.1 should incorporate the comments of Reviewer d7Mk.
  Citing standard references mentioned by Reviewer d7Mk.

The reason I highlight these recommendations is that I believe they will greatly improve the quality, longevity, and impact of this paper. Slightly overselling ideas feels like a good strategy, but it is a bad long term strategy. I believe addressing these points is in the interest of the authors.
The paper proposes a stochastic network, named Ensebmle in One (EIO), to increase adversarial robustness. EIO replaces the layers in a given architecture by so called random gated blocks (RGBs) in which a random gate switches between multiple copies of the original layers. By sampling from the random gates different subnetworks can be sampled which can be arranged to form an ensemble. During training non robust feature distillation (as proposed in previous work) between models is applied. For inference in the experiments a single subnetwork is sampled, and the robustness of that subnetwork is compared against several ensemble methods and adversarial training.  

One reviewer was worried about model capacity and recommended to perform experiments on image net to demonstrate scalability to large datasets. In turn, authors added experiments on CIFAR 100 during rebuttal period. Another critique was that the model does not show a significant advantage over vanilla adversarial training (AT), which can easily tuned with different perturbation strengths and only takes half of the training time. While other ensemble techniques, like DVERGE, can be combined with AT to improve their robustness, combing EIO with AT does not lead to improvements as shown by experiments performed during the rebuttal period. Two reviewers stated that adding a theoretical analysis will improve the paper. Another suggestion for improving the paper was to add a comparison to stochastic path networks, which is related work, and to investigate model performance when results from several sub networks are aggregated. 

Overall, the paper can not be accepted in its current state, but I would recommend the authors to continue the direction of work and to incorporate reviewers suggestions in a future version of the manuscript.
The paper addresses two important aspects of deep learning: model transferability and authorization for use. It presents original solutions for both of these problems. All of the reviewers agree that the paper is a valuable contributions. Minor concerns and critical remarks have been addressed by the authors during the discussion.
This paper introduces a perceptual similarity on top on the commonly used perceptual loss in the literature (LPIPS). The authors draw experiments highlighting that human perceptual similarity is invariant to small shifts, whereas standard metrics are not. The paper studies several factors (anti aliasing, pooling, striding, padding, skip connection) in order to propose a measure on top of LPIPS achieving shift invariance. 

This paper initially received mixed reviews. RLHuY was positive about the submission, pointing out the relevance of the real human data and the studied factors for measuring the impact on shift invariance. RGQvy was slightly positive, but also raised several concerns on justification of the claimed properties, human perception experiments, and positioning with respect to data augmentation (PIM). RLHuY, an expert on the field, recommended clear rejection, pointing out missing references (including DISTS), the limited scope of the paper (shift invariance and tiny shifts). After rebuttal, RLHuY and RLHuY stuck to their positions ; RGQvy were inclined to borderline reject  because of unconvincing answers on comparison to data augmentation techniques. 

The AC s own readings confirmed the concerns raised by RGQvy and RLHuY, and points the following limitations: 

  The submission includes limited contribution and expected results: the studied modifications on neural networks  architecture, although meaningful, directly follow ideas borrowed from the literature. They are not supported by stronger theoretical analysis, and several insights related to accuracy or robustness remain unclear. 
  Experimental results are contrasted, e.g. compared to data augmentation: although these approaches are more demanding at train time, they do not induce any overhead at test time   in contrast to the proposed approach.

Therefore, the AC recommends rejection.
This paper addresses a continuous time formulation of gradient based meta learning (COMLN) where the adaptation is the solution of a differential equations. In general, outer loop optimization requires backpropagating over trajectories involving gradient updates in the inner loop optimization. It is claimed that one of main advantages of COMLN is able to compute the exact meta gradients in a memory efficient way, regardless of the length of adaptation trajectory. To this end, the forward mode differentiation is used, with exploiting the Jacobian matrix decomposition. All the reviewers agree that the derivation of memory efficient forward mode differentiation is a significant contribution in the few shot learning. The paper is well written and has interesting contributions. Authors did a good job in responding to reviewers’ comments during the discussion period. What is missing in this paper is the discussion of some limitations of the proposed method.  This can be improved in the final version. All reviewers agree to champion this paper. Congratulations on a nice work.
This paper proposes the use of the determinantal point process to introduce the diversity in the prosodic features, including intonation, stress, and rhythm, in text to speech synthesis.  The proposed approach is certainly new, but the experimental support is of critical importance for this work.  One of the major points of discussion was the reliability of the experimental results.  In the original submission, the mean opinion score (MOS) of the proposed approach was inferior to the baseline.  The authors updated the experiments, which significantly (more than the confidence interval) lowers the MOS of a baseline.  This however makes the experimental results questionable.
This submission proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks.  Anti aliasing is achieved by parametrization with anisotropic Gabor filters.  The reviewers were unanimous in their opinion that the paper is suitable for acceptance to ICLR.  The authors are encouraged to make use of the extensive reviewer discussion in improving the final version of the paper.
The paper provides a way for explaining the reasoning of a neural network to humans in the form of a class specific structural concept graph (c SCG). The c SCG can be modified by humans. The modified c SCG can be incorporated in training a new student model. Experiments show that the new model performs better on classes that their corresponding c SCG have been modified. While all the reviewers agree that the paper puts forth an interesting idea, some concerns have been raised by reviewers about the scale of experiments and the lack of theoretical guarantee on the fidelity of the SCG. The authors have added two large scale experiments which confirm their previous results as part of their rebuttal. This paper is borderline and needs to be discussed further.
The paper proposes a new neural network architecture for hyperspectral image reconstruction. The paper received borderline/negative reviews. Significant concerns were raised about the novelty and significance of the contribution. Unfortunately, the authors did not upload a rebuttal, preventing the reviewers from changing their opinion about the paper. There is therefore no reason to overturn their recommendation.
This paper proposes Decoupled Kernel Neural Processes (DKNPs), a new neural stochastic process, which learns a separate mean and kernel function to directly model the covariance between output variables. Numerical experiments on 1 D regression and 2 D image completion are provided.

There  was a concern that the original version of the proposed model was not a valid stochastic process, since the consistency condition of Kolmogorov Extension Theorem might not be satisfied. The authors fixed this by replacing multihead mixed attention (MMA) in the covariance path with multihead cross attention.
Overall, reviewers find the work interesting, but there remain concerns that the novelty is limited and that  the current work lacks sufficient experimental evaluation.
This paper addresses the problem of learning representation of 3D point clouds and introduces an interesting approach of concentric spherical GNN with the property of rotationally equivariant. It shows some promising results on point cloud classification under SO(3) transformations and on predicting electronic state density of graphene allotropes. The reviews suggest that, while it does not suffer from any major flaws, the paper has a fairly large number of minor issues that add up to make it subpar for publication. The proposed approach have several hyperparameters, but the authors do not seem to be up front about how the parameters are selected except for stating that they use "standard tuning techniques"   this is not a satisfactory answer and appears to be dodging the question. Many technical details and specific choices could use more thorough explanation and analysis. The distinction of the proposed approach in relation to the large body of existing literature could be more clearly spelled out. Collectively, these issues made the contribution of this paper less clear.
This paper proposes a new loss function for molecular conformation comparison to be used in generation tasks. All reviewers found the research topic is interesting, but the work lacks in multiple aspects. Major concerns include limited contributions and novelty, lack of comparison with prior methods, limited improvements, writing and clarity, etc. The authors did not provide any response during discussion. Given the consistency and extent of concerns, and lack of response, I recommend this paper be rejected at this time.
This paper presents a simple approach called PDM for composing non linear and complex normalizing flows with score based generative models. Since score based models can be considered as a special form of continuous time normalizing flows, PDM corresponds to a composition of different classes of normalizing flows. 

Pros: 
* Combining generic normalizing flows with score based models is an interesting direction as they have different characteristics and can be complementary to each other.
* Using Ito s lemma to show that the model learns a non linear SDE in data space is valuable. 
* The authors show that the variational gap can be reduced using normalizing flows.

Cons: 
* The proposed method does not exhibit a clear advantage compared to the diffusion baseline without the normalizing flow component. On the CIFAR10 dataset, the best NLL and FID results are obtained by the diffusion baseline.

* Theorem 2 makes a very unrealistic assumption that a flow network is flexible enough to transform $p_r$ to any arbitrary distribution. If this holds, we wouldn t need the score based generation model anymore. We could simply train the normalizing flow to map the input data distribution to a Normal distribution.

* This submission chooses to discuss differences with the recent LSGM framework. However, in doing so, several inaccurate claims are made. The lack of inference data diffusion in LSGM is mentioned as one of its drawbacks. However, it is not clear what is the value of having such a mechanism and what implications it may have on the expressivity of the model. Note that mapping from data space to latent space in VAEs can be considered as a stochastic inversion rather than an exact inversion. Ito s lemma does not require invertibility and it can be easily applied to the forward and generative diffusion in LSGM. The authors argue that applying it to the forward diffusion in LSGM will result in $\hat{p_{r}}\ne p_{r}$. But, $\hat{p_{r}}$ would be only considered for visualization of the forward diffusion and it is not used for training or any other purposes. LSGM, the proposed PDM, and score based models are all trained with a reweighting of ELBO (see [here](https://arxiv.org/abs/2106.02808)). It is not clear if the drawback mentioned above has an impact on the training or expressivity of the model.

* The presentation in the paper requires improvement. The motivation on why invertibility plays a key role is not clear beyond generating the visualization in Figure 2. 

In summary, the paper proposes an interesting idea and explores directions very relevant to the current focus in generative learning. However, given the concerns above, we don t believe that the paper in its current form is ready for presentation at ICLR.
Despite a lively discussion and author explanation and revision, this paper remains below the bar for publication at ICLR. The technical exposition and goals remain poorly explained. The technical contribution is not sufficient. And the utility of the empirical results remain in question. The strong consensus among the reviewers who submitted reviews in a timely manner is that the paper is not suitable for publication.  The 5th and final review, was submitted too late, well beyond the end of the discussion period, and hence was not considered in this decision.
The manuscript discusses weaknesses in previous sample selection criteria in learning with noisy labels, and proposes a new selection criterion by incorporating the uncertainty of losses, together with theoretical justification. To select samples, the manuscript uses the lower bounds of the confidence intervals derived from concentration inequalities instead of using point estimation of losses. By incorporating uncertainty about large loss samples, the method is able to distinguish: truly mislabeled samples, or clean yet underrepresented samples that are less frequently selected or learned by the model so far. Experiments are performed on four benchmark datasets (MNIST, F MNIST, CIFAR 10, CIFAR 100) and use a diverse set of possible noise functions. 

Reviewers agreed on several positive aspects of the manuscript, including:
1. This manuscript addresses weaknesses in previous sample selection methods and potentially impacts various applications (e.g., worst group generalization and fairness);
2. The technical steps are clearly explained with theoretical justification;
3. Many datasets and comparison methods used and thus the proposed approach is also validated empirically.

Reviewers also highlighted several major concerns, including:
1. The space complexity of the proposed method; to implement the proposed method, it seems that naively one will need to keep track of the history of losses of every sample in the training set;
2. The validity of Markov process assumption on the training losses when momentum is used in optimization. Also, the position of the parameter in parameter space depends on the starting points and search paths, but not merely the last positions.
3. The experiments mostly are designed with relatively low noise (20% and 40%);
4. The evaluation matric as well as the analysis should be diverse. The performance of the proposed approach appears to have larger variances, which should be further explained.

Many of the major concerns have been addressed during the rebuttal including: experiments with 50%, 60%, and 70% symmetric noise, extension to NLP data by showing results on the NEWS dataset, and further discussion on the space complexity and Markov process assumption.
This is an interesting paper, aiming to separate the generalization properties of SGD and GD.  Unfortunately, the reviewers had many significant concerns, primarily on the topic of the relationship to prior work by Wu et al. (which has a similar setting and similar proof techniques), but also regarding presentation and interpretation of results in general.  As such, I recommend the authors continue with this line of valuable work, aiming in particular to further separate it from existing results.
The authors focus on large scale out of distribution (OOD) detection for which they propose three benchmarks with multiclass and multi label 
high resolution images. In these settings, they find that a simple extension, using maximum logits (MaxLogit), of a common baseline  maximum softmax probability (MSP), is surprisingly competitive to prior methods.

Five knowledgeable reviewers found the idea of having these novel benchmarks potentially interesting, but highlighted some issues that needs to be taken into account before the paper can be publishable. 

First, reviewers highlighted how the presentation can be better organized (more structure on and stronger overall motivation for the three different contributions) as to present the three ideas in a more cohesive way and more formal in introducing methods (e.g. the MaxLogic and MSP) as to clearly highlight the technical contributions and the differences with other models. 

Second, more baselines need to be introduced and therefore experiments extended. For example, the comparison with another detector, LogSumExpLogit, a relaxation of MaxLogit already used for (small scale) OOD in the context of generative models. Authors provided in the rebuttal some preliminary experimental results but promised more (for a camera ready) that could not be evaluated by the reviewers.

Third, the scope of the proposed benchmarks raised some concerns by some reviewers. If not in the motivation behind the task of treating whole objects as anomalies, additional care shall be put into the provided annotations. As one reviewer highlighted a certain percentage of images are mis annotated. While this percentage is somehow low (3.9%) and should not change the empirical conclusions drawn in the paper, it highlights that the core contributions of the paper might have been rushed.
This paper describes means of incorporating boundary conditions into graph neural networks used for simulation.  Assorted techniques dynamically adjust computations near a boundary.  Reviewers agreed this work is well written but disagreed regarding whether the scope of the contribution (and application, which focuses mainly on granular flow despite the title of the paper) merits publication at a top conference like ICLR.

The authors rebut the claims of limited scope strongly, but the experiments in the paper somewhat belie the claims of broad scope.  Some comparisons are also missing to Sanchez Gonzalez et al. 2020, which appears to be closely related.

The AC agrees that the scope of the work and contribution here have not met the bar for publication.  Reviewer QRXr has some thoughtful suggestions for ways to improve this work in future submissions, or sharing with an audience that can better appreciate the application oriented contributions might be a reasonable direction to take.
Manipulating deformable objects is an up and coming topic in robotics and machine learning, and it creates many interesting scientific and real world challenges. The paper looks into long horizon tasks of manipulation of deformable objects, using an interesting mix of more local trajectory optimization and differentiable physics. The reviewers agree on the interesting significance of the suggest work, all above acceptance threshold, but also a bit bimodal in terms of “just above” vs. “solidly good”. Thus, the paper appears an useful and discussion provoking accept for ICLR.
This paper has conflicting reviews with no strong advocate.  One of the positive reviewers states the caveat that paper is "very dense to read and needs to be improved".  Having looked at the paper myself I would agree with this criticism.  One of the negative reviewers states that the paper gives "an incremental variant of the NLM model".  I am less confident in this judgement.  However, I find the density of the paper and the use of synthetic data to be significant drawbacks. With the lack of any real champions for the paper I do not see a path to acceptance.
The paper studies interpretability in multi instance learning (where model is trained with a label provided for a bag of instances). The author proposes model agnostic weight sampling strategy to improve sampling in prior methods such as (SHAP), and evaluate their performance on three datasets (and authors provided results on more datasets during rebuttal). 

All reviewers agree the paper is well written and well motivated. The paper presents a simple but meaningful extensions to existing interpretability study and will be helpful for the community. Reviewers had some concerns with the comprehensiveness of the evaluation, the strength of their proposed results, and the originality/novelty of the paper. The authors have provided further experimental results on new datasets as well as additional baselines. Given the study of MIL setting in interpretability is scarce, I am leaning towards the acceptance.
The authors propose a new framework of population learning that optimizes a single conditional model to learn and represent multiple diverse policies in real world games. All reviewers agree the ideas are interesting and the empirical results are strong. The meta reviewer agrees and recommends acceptance.
The authors define the task of solving a family of differential equations as a task of gradient based meta learning generalizing the gradient based model agnostic meta learning to problems with differentiable solvers. According to the reviews, there were some concerns regarding the practical value of the paper, for example, (1) the proposed technology is restricted to linear systems, and relatively easy problems (2) there is no demonstration of practical application utility (3) It lacks systematic comparison with other methods (4) some technical details are missing. There were quite a lot of discussions on the paper among the reviewers, and the consensus is that the paper is not solid enough for publication at ICLR in its current form (the reviewer who gave the highest score is less confident and does not want to champion the paper).
Overall, the reviewers were insufficiently enthused by this paper.  There was no rebuttal, and the authors did not engage or answer questions raised.  I concur with the reviewers, and encourage the authors to carefully consider the provided feedback.
The paper proposes a framework for object detection on lidar scans, with query of scene feature extracted offline from previous traversals. Overall there is good agreement among reviewers, with three recommending accepting the paper and one marginally accepting it   to me the authors satisfactorily addressed most aspect raised in reviewing.
The paper proposes a method for inferring which of a set of pretrained neural networks, once fine tuned on a transfer task, will generalize the best. This is accomplished by deriving a quantity based on a mean field approximation of a dynamical system defined on the adjacency matrix of the weights of a neural network, known as the "neural capacitance". The model selection procedure involves attaching a fixed, randomly initialized network onto the outputs of the pretrained network and fine tuning for a small number of iterations, and computing the metric; the fixed network is called the "neural capacitance probe" (NCP).

Reviews, though low confidence, awarded borderline scores, and a central concern was clarity and motivation, in particular the role of the NCP. acZh, the highest confidence and most verbose reviewer, echoed these concerns along with specific criticisms, for example about the heavy reliance on Gao et al (2016) without elaboration. The authors have responded in considerable depth but unfortunately the reviewer has not acknowledged these responses. On the NCP, the authors note that this is an approximation to the ideal metric that they have empirically validated.

Reading the updated draft, I find myself still concurring with reviewer acZh in large degree. The draft has improved with the noted additions, such as Appendix G devoted to an explanation of Gao et al (2016), but the presentation is still quite challenging to follow. I am left with fundamental questions about the soundness of the approximation being made, its wider applicability, and the many arbitrary decisions regarding the architecture of the NCP that appear out of nowhere. How sensitive is the procedure to these choices? Did the authors tune these architectural hyperparameters? Using what data? The table of results does not include units, and for a paper proposing a general purpose metric I d ideally want to see a a robust rationale for hyperparameter selection of method specific hyperparameters as well as a rigorous statistical treatment of the method s performance. Since it involves an approximation, a comparison to the "ideal" or "exact" procedure on a toy problem where the latter is feasible would strengthen the paper considerably. I do appreciate the breadth of architectures and datasets examined, but I believe the central focus of the paper should be explaining the mathematical motivation (perhaps at a higher level and deferring more detail to the appendix), why precisely it makes sense in the context of neural networks (also raised by acZh, with an answer provided that I believe partially addresses this) and justifying the concrete, approximate instantiation of the method involving the NCP and the hyperparameter selection and evaluation protocol that led you to the particular NCP employed.

At a higher level, this is a very mathematically dense paper that relies considerably on concepts outside of what might be considered typical expertise in the ICLR community, reflected in the confidence scores of the reviewers. While I feel that the issues described above already preclude acceptance at this time, I believe it may be difficult to do the proposed method justice in the short conference paper format, and would suggest to the authors to consider a journal submission instead, where a didactic presentation can be given the full attention it deserves without the difficulty created by length constraints.

Finally, I d like to apologize to the authors for the non responsiveness of the Area Chair. The original Area Chair was not able to complete their duty and I have been belatedly assigned this paper to evaluate it, and it is clear that not as much discussion took place as would have been ideal.
This paper proposes a pseudo labeled data selection method for semi supervised pose estimation. The investigated task in this paper is practical and useful. The framework is well designed and reasonable, and extensive ablation studies are conducted to test the efficacy of the method. After discussion, all the reviewers recommend accept of this paper.
The paper suggests a new aggregation rule for federated learning in order to mitigate Byzantine attacks. However, as the reviewers pointed out, the theoreticial results of the paper are weak and incremental and the experiments are not solid.
The premise is an exciting observation: Differential privacy in federated
learning might imply being certified against poisoning attacks. While
this may be considered not surprising by some, the connection between
differential privacy and robustness is interesting to many. The
relationship was characterized both theoretically and empirically.

The reviewers discussed the paper extensively with the authors, and
while many issues were clarified, issues on correctness still
remained: it is unclear if the proposed DP mechanism actually is DP,
and subsampling amplification also had issues. Clarity needs to be
added in the writing, and the extensive comments by the reviewers
hopefully help the authors in that.
This work studies the question of increasing the expressive power of GNNs by adding positional encodings while preserving equivariance and stability to graph perturbations. 
Reviewers were generally positive about this work, highlighting its judicious problem setup, identifying the right notion of stability and how it should drive the design of positional encodings. Despite some concerns about the discrepancy between the theoretical results and the empirical evaluation, the consensus was ultimately that this work is an interesting contribution, and therefore the AC recommends acceptance.
This paper proposes a labeling trick for subgraph representation learning with GNNs. The proposed method, GLASS, improves on subgraph level tasks. The topic of subgraph representation learning is relatively new, and this paper makes progress in that community which would be appreciated by other researchers interested in the same problem. 

The paper in the original submission state raised some concerns from the reviewers about unclear writing of the motivation and potential applications, technical novelty, and comparisons with existing approaches (even one that are not specifically designed for subgraph representation learning). It is good that the authors conducted additional experiments to show the effect of SSL (that the approach makes improvements without SSL). This and other clarifications from the authors convinced the reviewers to recommend acceptance.
The paper provides a procedure for certifying L2 robustness in image classification. The paper shows that the technique indeed works in practice by demonstrating it s accuracy on CIFAR 10 and CIFAR 100 datasets. 

The reviewers are positive about the paper. Please do incorporate feedback, especially around experimental setup to ensure that the work compares various methods fairly and provides a clear picture to the reader.
This paper proposes a federated learning method called FedProf that adaptively selects different subsets of the clients  data in training the global model. There were several concerns brought up in the reviews and discussion. The multivariate Gaussian (with identity covariance) assumption on the neural network representation is limited. The paper also claimed to provide privacy preservation, but there is no formal statement of the actual privacy guarantees. (The fact that it s running federated learning does not guarantee privacy protection.) The presentation could use improvement. The reviewers had issues trying to understand the main theorem. Overall, there is not sufficient support for acceptance.
This paper reveals that popular data poisoning systems, Fawkes and LowKey, fail to effectively protect user privacy in facial recognition. The methods to defend against poisoning attacks are quite simple you can either adaptively tune the face recognition models or just wait for more advanced facial recognition systems. Given these “disappointed” findings from the technical solution side, this paper further argues that legislation may be the only viable solution to prevent abuses of facial recognition.

Overall, all the reviewers highly appreciate the comprehensive and rigorous evaluations provided in this paper and enjoy reading it. The biggest concern is raised by the Reviewer 6s7m, given this work fails to discuss/compare to previous works on Facial identity anonymizing and the technical contribution is incremental. During the discussion period, all other reviewers reach a consensus that 1) facial identity anonymizing is not relevant; and 2) this work make enough contributions and is worthy to be heard by the general community; the Reviewer 6s7m still hold the opposite opinion, but is okay for accepting this paper anyway. 

In the final version, the authors should include all the clarification provided in the discussion period.
This paper made a solid contribution studying the convergence rate of a simple distributed gradient clipping algorithm. The proposed algorithm simply clips the gradients on each local machine and then do simple distributed update of the parameters. 

The result, if correct, is quite strong and significant: The proposed algorithm is simple, and shows some benefit comparing to previously proposed algorithms   The strongest part of the paper is that it comes with a convergence rate bound (which is typically hard to prove for gradient clipping methods).


However, during the rebuttal period it was discovered that a number of places in the proofs are not well supported, the paper has to go through major revision in order to meet the publication standard.
The reviewers agree that the proposed method of joint model policy optimization using a lower bound is novel and interesting and worthwhile pursuing. But all reviewers find a variety of issues in the paper, such that ratings are just above borderline or below. Given all the mixed feedback, it appears that the paper is still a bit premature for publication and could greatly benefit from improvements in a future submission.
The paper proposes a new offline RL technique to generalize across domains. The paper was initially confusing (i.e., MDP vs POMDP) and weak empirically.  The authors greatly improved the paper.  However, a the end of the day, it is still not clear why the proposed approach performs better than existing techniques.  We can think of the cumulant function with the discrete labels as essentially computing some statistics of future actions, observations and rewards.  This is what every self supervised technique does.  They differ in terms of their particular choice of statistics and architecture.  The paper does not sufficiently motivate the particular architecture.  Interestingly, in the experiments, the best statistics are cumulative rewards, which are closely related to the Q values. In that case, it is even less clear why the approach should be beneficial since RL techniques that generalize across domains by learning state representations to predict Q values seem very closely related.  

Despite the updates to the paper, the POMDP references are still confusing.  The issue is that the paper embeds observations as if they were sufficient to predict future observations and rewards.  This corresponds to the memoryless approach where a policy is optimized based on the last observation instead of the history of past actions and observations.  Memoryless strategies are effective only when the last observation is a sufficient statistic, meaning that we really have a (near) fully observable MDP.  The paper should discuss this and acknowledge that the approach will suffer in domains where memory of past actions and observations is critical.
The paper addresses the problem of generating images by combining visual components. These components are learned during pretraining, forming a dictionary of visual concpets which plays the role of text in DALLE. The technique is based on DALLE and slot attention approach to generate VQ codes in a way that is consistent.

Reviewers had various concerns, including (1) that using synthetic images makes it easier to combine visual components  (2) that the novelty and relation to literature was not clear enough (3) missing ablations.  The authors provided a detailed rebuttal which addressed reviewer concerns in a convincing way. 

One remaining issue of the paper is the writing. The paper fails to clearly explain the workflow (what are input and output during pretraining, training and inference), and how compositionality is controlled (what can be used for conditioning). As a consequence, it requires substantial effort to understand the idea of the paper, and what real problems can be solved with the proposed approach . 

The paper can be accepted to ICLR, but it is expected that the writing would be improved. The abstract and introduction should make concrete statements about what the approach does, what problems it solves and how it can be used for the various tasks as disucussed in the experiments
The paper addresses various improvements in visual continuous RL, based on a previous RL algorithm (DrQ). As the reviewers point out, the main contribution of the paper is of empirical nature, demonstrating how several different choices relative to DrQ significantly improve data efficiency and wall clock computation, such that several control problems of the DeepMind control suite can be solved more efficiently. The average rating for the paper is above the acceptance threshold, and some reviewers increased their rating after there rebuttal. While a mostly empirically motivated papers is always a bit more controversial, the paper may nevertheless stimulated an interesting discussion at ICLR that will be beneficial for the community, and should thus be accepted.
This paper develops a variational auto transformer model (VAT), a VAE based on the transformer (encoder decoder) architecture designed to provide isotropic representations by adding a token level loss for isotropy. All the reviewers agree that this is a novel architecture with a valid and interesting goal behind it.

Reviewers varied somewhat on their impressions of the paper, but none were strongly positive on accepting it. I think the strongest and most aligned concerns were from reviewers ZoL1 and pcez. They both feel that the experiments do not convincingly demonstrate what is required. It would be good to better establish the success of variational sampling and the usefulness of isotropic representations. I would think that even a page of examples in the appendix, contrasting sampling by various methods, would add a lot of information to what is presented here. It would be even better to have experiments showing the relation between improved isotropy and improved task performance (suggested by j72L). Both reviewers are concerned about the small model and weak results and whether these results would extend to larger models that people actually use. While on the one hand, controlled comparisons are valuable, it is also true that people in NLP routinely like to see results on models of a reasonably competitive size. In practice, for 2019 2021, it seems that people regard having models of BERT base size as the "reasonable" small size that they will accept and for which there is reasonably good performance and lots of available empirical results. Transformers directly trained with very few layers do not perform that well. Reviewer pcez is also concerned about the change of the data set in the MiniBERT comparison, which seems valid, and reviewer 5v5U is concerned about what s fair in terms of parameter counts.

This paper needs further work with larger and more careful experimental comparisons to meet the needed level of experimental rigor to be convincing. The authors were not able to iterate sufficiently quickly to achieve this during the ICLR reviewing period, so it seems best that the paper be rejected for now, and the authors look to subsequently submit a more developed version of this work.
This paper presents Yformer to perform long sequence time series forecasting based on a Y shaped encoder decoder architecture. Inspired by the U Net architecture, the key idea of this paper is to improve the prediction resolution by employing skip connection and to stabilize the encoder and decoder by reconstructing the recent past. The experiment results on two datasets named ETT and ECL partially showed the effectiveness of the proposed method.

Reviewers have common concerns about the overall technical novelty, presentation quality, and experiment details. The authors only provided a rebuttal to one reviewer and most concerns from the other three reviewers were not addressed in the rebuttal and discussion phase. The final scores were unanimously below the acceptance bar. 

AC read the paper and agreed that, while the paper has some merit such as an effective Yformer model for the particular problem setup, the reviewers  concerns are reasonable and need to be addressed in a more convincing way. The weaknesses are quite obvious and will be questioned again by the next set of reviewers, so the authors are required to substantially revise their work before resubmitting.
This paper proposed a method for adaptive network compression at inference time. However, the paper contains various issues raised by the reviewers that needs to be addressed.
This work introduces/applies the mirror descent optimization technique to adversarial inverse reinforcement learning (AIRL). As a result, the proposed algorithm (MD AIRL) incrementally learns a parameterized reward function in an associated reward space. The two issues of standard adversarial imitation learning algorithms are 1) current "divergence" based updates may not lead to updates that better match the expert (due to geometry) 2) "divergence" based updates may suffer when only small number of demonstrations are provided. Thus the goal of this work is to (presumably)  to "robustify" the learning of reward function especially by addressing these issues. The proposed algorithm is evaluated on a bandits problem, a multi goal toy example and standard mujoco benchmark.

**Strengths**
This work attempts to address the important problem of understanding and improving the updates of IRL algorithms
A theoretical analysis is provided

**Weaknesses**
The major concern is clarity of the manuscript. Even after updating clarity remains a concern
While a lot of experiments were performed, evaluation is not entirely convincing. One reason for this that it is hard to tie the results back to the original motivation/claims of this algorithm. As one reviewer notes "it s unclear how the new algorithm affects reward functions". Furthermore, reviewers find the experimental results not entirely convincing 

**Summary**
After rebuttal and revision, the clarity and experimental analysis remain a concern. My recommendation is that the authors are encouraged to take the reviewers feedback and improve the manuscript. In its current form it s not quite ready yet for publication.
The paper proposes to use the Huber and absolute loss for value function estimation in reinforcement learning, and optimizes it by leveraging a recent primal dual formulation by Dai et al. 

This is a controversial paper. On one hand, it is a well motivated idea to apply robust loss on RL; the paper implemented the idea well by leveraging the saddle point formulation, and empirically demonstrate its advantages in practice. 

On the other hand, the technical novelty of this paper is limited. The idea of Huber and standard conjugate formulation are straightforward application of existing techniques (despite being well motivated). 

The authors seem to think that there has been no application of Huber loss on RL. But existing implementations of RL already uses Huber loss. For example, in the openAI baselines (https://openai.com/blog/openai baselines dqn/), they said the following: 

"Double check your interpretations of papers: In the DQN Nature paper the authors write: “We also found it helpful to clip the error term from the update [...] to be between  1 and 1.”. There are two ways to interpret this statement — clip the objective, or clip the multiplicative term when computing gradient. The former seems more natural, but it causes the gradient to be zero on transitions with high error, which leads to suboptimal performance, as found in one DQN implementation. The latter is correct and has a simple mathematical interpretation — Huber Loss. You can spot bugs like these by checking that the gradients appear as you expect — this can be easily done within TensorFlow by using compute_gradients." 

The authors discussed the first approach above on in the rebuttal, but I am not sure if the authors have considered the second method. If not, it would be worthwhile to discuss and compare with it.  

See also "Agarwal et al. An Optimistic Perspective on Offline Reinforcement Learning" and "Dabney et al. Distributional Reinforcement Learning with Quantile Regression."

On the other hand, I have not seen the application of saddle point approach by primal dual method of Dai on Huber specially. 

It seems that the proposed algorithm is in the end equivalent to MSBE+primal dual+ (h with softmax output). If it is that simple, I think it would help the readers to explicitly point this out upfront in the beginning (which is an interesting conceptual connection).  Because the primal dual approach need to be approximate h with a neural network, the difference of the two methods is vague in the primal dual space. 

A side mark:  when we say "an objective for which we can obtain *unbiased* sample gradients", i think that the gradient estimator of the augmented Lagrange is unbiased; the gradient estimates of MHBE and MABE are still biased.  

Overall, it is a paper with a well motivated and valuable contribution, but limited in terms of technical depth and novelty.
Summary: This paper studies an inverse (linear) contextual bandits (ICB) problem, where, given a $T$ round realization of a bandit policy’s actions and observed rewards, the goal is to design an algorithm to estimate the underlying environment parameter, along with the “belief trajectory” of the bandit policy. A particular emphasis is placed on the belief trajectory being “interpretable” and capturing changes in the policy’s “knowledge of the world” over time.

The paper’s main contributions are (i) formalizing the inverse contextual bandits problem, (ii) designing two algorithms for this problem based on two different ways of modelling beliefs of the bandit policy, and (iii) providing empirical illustrations of how their algorithm can be used to investigate and explain changes in medical decision making over time

Discussion: This paper has received high quality, long and detailed reviews that highlighted some flaws, in particular in the well posedness of the problem and the clarity of the writing. The authors  response was long and detailed as well, and its quality was recognized by the committee. 
However, the consensus is that this work would require a full pass allowing to include most of the feedback received in the main text rather than in appendices, to discuss related problems in the literature in more depth and perhaps to refocus the exposition on the problem considered.

Recommendation: Reject.
All five reviewers unanimously agree that the paper needs to be rejected. One of the main concerns is the lack of technical novelty/originality. The reviewers also point out lacking citation and comparison to prior work, and missing experiments. The authors have not provided any rebuttal.This paper describes an approach for zero shot detection of seen and unseen objects in scenarios. All five reviewers unanimously agree that the paper needs to be rejected. One of the main concerns is the lack of technical novelty/originality. The reviewers also point out lacking citation and comparison to prior work, and underwhelming experiments. The authors have not provided any rebuttal.

We recommend rejecting the paper.
This paper provides an interesting method to address the CTDE problem in MARL. While the experiments are promising, the theory is either insufficient or not rigorous. One of the reviewers believe that there is a flaw in the paper. There was an extensive discussion among the authors and the reviewer. The authors could not convince the reviewer for the apparent flaw.
This paper proposes a hypergraph representation learning based on multiset encoding, which  covers most existing propagation methods for hypergraph neural networks. The authors provide theoretical proofs that both CE based and tensor based propagation rules can be represented as a composition of two multiset functions, and propose two different multiset encoding functions, based on DeepSets and SetTransformer. The authors validate their method for its semi supervised node classification performance on multiple benchmark datasets, showing that it is superior or comparable to a large number of existing works on hypergraph representation learning. 

The following is the summary of the pros and cons of the paper mentioned by the reviewers:

Pros
  The proposed framework generalizes existing message passing methods for hypergraphs, and authors provide theoretical proofs on how it can generalize to two different types of propagation rules for hypergraph representation learning. 
  The paper is well organized and is clearly written, and the code is provided for reproduction. 
  The experiments consider a wide range of hypergraph datasets and baselines, and the proposed method either outperforms them or at least achieves comparable performance.

Cons
  It is still unclear where the benefits come from, due to lack of ablation studies and deeper analysis. 
  Experiments are only restricted to the semi supervised node classification task. 

While the initial reviews were split due to these negative points, all reviewers unanimously recommended for acceptance after the discussion period, as they found the responses from the authors satisfactory.  

In summary, this is a well written paper that proposes a neat framework for hypergraph representation learning that generalizes to most existing methods, backed up by compelling performance on benchmark datasets, which will make it a strong addition to ICLR. However, as mentioned by the reviewers there should be more ablation studies and in–depth analysis of what makes the proposed multiset function more effective, as this is lacking in the current version of the paper. It would be worthwhile to also validate the proposed framework on other tasks (e.g. graph classification tasks). 
 
One minor thing that I want to point out is regarding the claim that this is the first attempt to connect the problem of learning multiset functions with hypergraph neural networks.  [Jo et al. 21], which is a hypergraph based framework for edge representation learning, utilized GMT [Baek et al. 21], which performs multiset encoding using SetTransformer for hypergraph representation learning, and thus I suggest the authors to tone down on the claim that this is the first work that connects multiset encoding with hypergraph neural networks, and properly acknowledge this. 

[Jo et al. 21] Edge Representation Learning with Hypergraphs, NeurIPS 2021
This paper was reviewed by four experts in the field and received mixed scores (1 borderline accept, 3 borderline reject). The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper. AC feels that this work has great potential, but needs more work to better clarify the contribution and include additional ablated study. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper extends Lowe et al. 2020 to discover causal relations from nonstationary time series by assuming conditionally stationarity of the times series. Based on the assumption, a deep learning method based on VAE is proposed to learn the causal relations from data. 
The paper is well motivated and well organized.

However, there are some concerns from the reviewers. 1) The presentation needs significant improvement, e.g., clarification of the notations.  2) The identifiability of the causal graph is not given. It is unclear under what conditions the proposed method can discover the true causal graph. 3) The capacity of the discrete states might not be able to handle complex real situations. 4) The experiments are limited to synthetic and low complexity cases. This further weakens the significance of the proposed method given that there are also no theoretical guarantees of the proposed method. 5) Discussions about some important relevant works are missing.

Overall, the paper studies an interesting problem. However, given the above concerns, the novelty and significance of the paper will degenerate. Both theoretical and empirical analysis of the proposed method need further improvement. Addressing the concerns needs a significant amount of work. Thus, I do not recommend acceptance of this paper.
This paper improves on the efficiency of prior work that uses homomorphic
encryption to perform privacy preserving inference. There are two main
concerns raised by the reviewers. First, multiple reviewers (and I) found
this paper difficult to read. Multiple pieces of the problem are not
clearly presented especially with respect to the technical contributions.
This was fixed in part in the rebuttal but more could still be done here.
But more importantly, three reviewers raise concerns about the evaluation
methodology, especially with respect to comparisons to prior work. On top
of this, there are valid criticisms raised by the reviewers about if the
contribution here is that significant when compared to prior work. (This
is something that both more clear writing and more careful experiments
could hep address.) Taken together I do not believe this paper is yet ready
for publication.
The paper proposes a novel black box attack aiming to fool a particular detector model. All reviewers see problems in the claims, the experiments etc and all argue for rejection. The authors did not provide a rebuttal to clarify any of the questions of the reviewers. Thus this is a clear reject.
The paper proposes TOME, which extends Transformer by attending to entity mention memory. Experiments are conducted on claim verification and QA.

Reviewers generally found the paper is solid. However, the novelty appears to be limited and is mainly in the combination of existing models.
The paper tackles the problem of covariate shift in adaptive curriculum learning.  Unfortunately, the paper lacks clarity and the experiments are insufficient.  The author response clarified the notation and corrected many typos, however, the paper remains conceptually unclear as pointed out by the reviewers.  Hence this work is not ready for publication.
The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. The proposed method looks quite simple and clear, while the results show strong gains against baseline methods on 3 different datasets. Ablation studies show that the model achieves good performance on more complex questions. While the reviewers raise some concerns, e.g., on the sensitivity of the proposed method, the technical novelty against prior works, they see values in this paper in general. And the authors did a good job in their rebuttal. After several rounds of interactions, some reviewers were convinced to raise their scores by a little bit. As a result, we think the paper is in a good shape and ICLR audience should be interested in it.
Three reviewers recommend Reject. Two reviewers recommend Accept although do not champion the paper. I believe the paper develops an interesting idea to better estimate the location of the inducing inputs in sparse GP models. However, I still think the paper would benefit from another careful revision and therefore I do not recommend Acceptance at this stage. I agree with reviewers that 1) currently the method is unable to estimate the covariance between two data points. This is important in applications of GPs for uncertainty quantification such as Bayesian optimisation. For example, including a BayesOpt example would clearly strengthen the paper. 2) empirical evaluation lacks simple baselines, e.g. Titsias (2009). The authors claim that Titsias (2009) does not scale and that s why they don t care for it. Even if this is true, including an example that helps to better compare against this method at a different scale might strengthen the model proposed here.
This paper is proposed to improve base CNN models by dual multi scale attention module. To achieve a better feature representational ability, authors consider the multi scale mechanism from both channel dimension and spatial dimension. The proposed method has been verified on several benchmarks, including ImageNet and MS COCO. However, all reviewers consider rejecting this paper because this work lacks novelty, the results are suspicious, and the writing is poor. No responses are submitted by authors to address the reviewers  concerns.
I recommend this paper to be accepted. All reviewers are in agreement that this paper is above the bar.
This paper investigates how well properties invariant to changes such as lightening and background learned in the major class can be transferred to the minor class. In this paper, the authors reveal that invariances do not transfer well to small classes, and suggest that resolving this phenomenon can help increase the performance on imbalanced datasets. From this point of view, the authors propose a generative model based augmentation technique.

Three reviewers suggested acceptance, and one reviewer judged borderline reject. It seems true that the method is not novel enough, but it is solid and well motivated. In particular, the finding of the paper is interesting and the design of the experiment is well done, so I think that it will have a great influence on research in this field in the future. As the negative reviewer mentioned, the lack of large scale experiments is a major weakness of this paper. I strongly encourage the final version to supplement the promises made to the reviewer, including adding iNaturalist experiments.
This paper proposes to synthetize virtual outliers by sampling from low likelihood regions of the feature space of a class conditional distribution, in order to make more robust predictions via a regularization loss term.

In the reviewing phase certain criticisms were raised by reviewers: namely that i) the paper was not clear w.r.t. its goal, motivation and position in the literature of OOD detection for bounding boxes; ii) details about the energy based formulation and covariance definitions and iii) experimental setting and metric used were missing. During rebuttal the authors answered to all the above criticisms up to a satisfying extent and were able to increase two reviewers  scores.

The paper is accepted conditioned on the fact that the camera ready includes the additional details and discussions that arose in the comments with a specific emphasis on properly framing (and limiting) the motivation of OOD detection for open set object detection and it is expected to properly cite the literature of the more general OOD detection task as discussed in the comments.
This paper develops a ``preference conditioned” approach to approximate the Pareto frontier for Multi Objective Combinatorial Optimization (MOCO) problems with a single model (thus dealing with the thorny problem that there can be exponentially many Pareto optimal solutions). It appears to provide  flexibility for users to obtain various preferred tradeoffs between the objectives without extra search. The basic idea is to use end to end RL to train the single model for all different preferences simultaneously. 

The technical soundness and practical performance are strong. This work s approximation guarantee depends on the ability to approximately solve several (weighted) single objective problems. This may be challenging due to the NP hardness of the latter. However, this limitation seems to also apply to other end to end learning based approaches. 

One area where the novelty is somewhat limited is that the paper borrows some number of ideas from neural single objective optimization. The contribution overall seems noteworthy for hard multi objective problems.
The paper presents a new problem: open set single domain generalization, where only one source domain is available and unknown classes and unseen target domains increase the difficulty of the task. To tackle this challenging problem, this paper designs a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. Then, the paper proposes a cross classifier consistency regularization that minimizes the multi binary classifier s output and one vs all multi class classifier s output. 

The proposed OS SDG is an interesting and realistic problem. However, since it is way more challenging, the optimal solution to it remains elusive. Some reviewers think the method might be heuristic and lack theoretical guarantees. Nevertheless, the results are promising and the paper makes a first step toward the challenging OS SDG problem. Another concern is that the CCR loss needs more ablation studies to further analyze its role. Though the authors have added more explanation of this part, I suggest the authors put more ablation studies in the final supplementary document. 

Overall, the paper is novel and interesting.  I would recommend acceptance of this paper given its novelty and impressive performance, but I highly suggest the authors add more ablation studies in the final supplementary, as suggested by the reviewers.
The authors study the degradation problem observed in KD for large teacher networks and propose to address it by quantifying and adapting to a *sharpness gap* between the student and the teacher. The reviewers generally appreciated the proposed approach in handling larger teachers and found it effective within the scope of the numerical results provided in the paper. That said, the reviewers raised several critical issues concerning the writing and the presentation of several crucial parts of the paper, in particular those related to the sharpness measure and the proposed training method ATKD. Thus, given this, and the exchanges between the reviewers and the authors, in its present form, the paper cannot be recommended for acceptance. The authors are encouraged to incorporate the valuable feedback provided by the knowledgeable reviewers.
There is a consensus that the contribution is not strong enough to effectively
argue for an important novel lead which would justify publication at ICLR. 

Authors have also not engaged with the reviewers.

For these rejections, this paper cannot be endorsed for publication at ICLR 2022.
It seems that the reviewers reached out a consensus that the paper is not ready for publication at ICLR. The reviewers raised concerns including “The empirical observations are not supported by theoretical analysis” , “The proposed algorithm is a simple modification to an existing algorithm”, concerns with “with the novelty of the paper”, “The message of the paper is not new. “ Please see the reviews for more detailed discussions about the paper.
Even though reviewers found some responses by the authors satisfactory, several concerns regarding the paper still remain. The authors are strongly encouraged to:

1) Explore how dataset size impacts accuracy.
2) Reason about annotation costs via empirical experiments.
3) Including benchmark datasets in experimental evaluations.
The paper studies a robust GNN against adversarial attacks on both graph structure and node features. 
The reviewers agree that the paper need to improve in terms of novelty and more technical details to meet ICLR standard.
This paper presents a method to turn a pretrained unconditional VAE into a conditional VAE by training an encoder to predict the unconditional VAE latents given conditional input. On a variety of image tasks, the method is shown to perform competitively with GANs, yielding good sample quality and diversity, and resulting in training time that improves on direct conditional generation approaches. While the technical novelty is limited, the strong empirical results and relevance given the growing availability of pretrained unconditional models lead me to recommend accepting this paper.

Ethics concerns have been raised for this paper. In particular, there were concerns with respect to the application of generative models, which inherit biases from the dataset, to guide medical imaging. It would be good to discuss this issue in more depth. A second point that was raised by the ethics committee is the fact that chest X rays are usually not taken in a sequential manner. We ask the authors to either provide evidence that X rays can be taken sequentially (one can think of situations where that s the case, e.g., X rays of teeth in the mouth), preferably in the context of chest X rays; if that s not possible, please highlight that the application, as described in the paper, is unrealistic (at the moment), and that it only serves as an illustration.

The key point we therefore ask the authors to address is to ensure that the paper clearly states how realistic the application is and what potential problems may arise when using generative models in this particular domain.
The paper makes some novel and interesting observation pertaining the relationship between data heterogeneity and personalization. Reviewers like the paper and ideas in general but raised several concerns. The rebuttal rectified several confusions and provided more clarification which convinced the reviewers that the paper is above bar for publication.
The SAC wrote a very good meta review and I just copy and paste it here. I completely agree with the SAC that the contribution of the paper due to the similarity to MME and MCD. Hopefully adding data augmentation to MCD and providing empirical results on new tasks can shed some lights to the community.

 
Based on a request from the authors, the SAC read the paper and the reviews and engaged two additional expert reviewers to provide an additional assessment.

The paper addresses semi supervised learning (SSL) and makes two contributions: 1) a method called χ model, which combines data augmentation with a two headed network that has an adversarial loss between the heads and the feature backbone; 2) an empirical evaluation on classification and regression SSL tasks.

As pointed out by reviewer 6Dgh, the proposed model is very similar to two existing methods: MME and MCD. These related works were not mentioned in the submission. During the rebuttal the authors compared to MME but not MCD.

Similarity to MME: The core idea in MME (Min Max Entropy) is to introduce a min max game between the head and backbone to regularize the model. The authors point out that one difference is that X model uses two heads and regression loss, while MME uses a single head and entropy loss. They also have other components like data consistency regularization, which are borrowed from prior work (FixMatch). They further point out that MME was evaluated and motivated for unsupervised domain adaptation whereas their paper evaluates on the same distribution (regular SSL). In general, there is a lot of overlap and borrowed techniques between UDA and SSL methods, but the experimental benchmarks tend to be different.

Similarity to MCD: A larger concern is that the proposed method is actually more similar to MCD than MME. In Eq.5, the data is augmented into two ways and two different output heads are applied. If the data is not augmented this way, the formulation is the same as MCD. The authors did not discuss this point, even though reviewers pointed out the similarity. From the perspective of the technical approach, the novelty wrt MCD appears to be mainly in adding the data augmentation. They do show in the paper that this type of min max regularization is effective in various regression tasks, which is a good empirical contribution. In the ablation studies of the data augmentation part, the difference over MCD is not very large, but the augmentation provides good gains in some experiments (although MCD is not mentioned, in Table 2 and 5, the "χ model (w/o data aug.)" is likely essentially MCD.)

Overall, the strength of the paper appears to be in adding data augmentation to MCD and providing empirical results on new tasks showing that it works on same distribution test sets and on regression tasks. The technical contribution seems somewhat limited, if accepted, the paper should include a clear discussion w.r.t MCD in the method and experiment sections. Furthermore, some baselines may potentially be missing, e.g. an MDD based (similar to MCD) UDA method for regression can be used as a baseline (Regressive Domain Adaptation for Unsupervised Keypoint Detection, CVPR 21).
The reviews are of good quality. The responses by the authors are commendable, but ICLR is selective and reviewers still believe that the research would be better as two separate papers: one about the problem and solution from an ML perspective, and the other about the application to surgery. Papers that provide a new method in the context of a single application domain run the risk of making a contribution to neither, and of being evaluated by reviewers who are not experts in both.
This paper has been independently reviewed by four expert reviewers. Two of them recommended straight acceptance, one of them assesses this work as marginally acceptable after increasing their score as a result of the author s rebuttal, and the last reviewer considers this paper marginally below the acceptance threshold. While the reviewers agree on the importance of the  targeted problem and relative novelty of the presented work, the main points of criticism involve empirical evaluations   its methodology, experimental design, missing relevant and important comparisons. Since the authors have addressed most of those concerns in their rebuttal, I am leaning towards recommending acceptance of this work for ICLR.
The paper proposes to incorporate unsupervisedly extracted emotion related tokens/embeddings to improve sentiment classifiers trained on top of BERT. The strengths of the paper, as identified by reviewers, are in the importance of the problem, a relatively easy to reproduce method, and a clear write up. However, all the reviewers identify several major weaknesses, including the lack of a clear research question, unclear contribution, limited novelty of the method, missing baselines, and relatively small gains in the downstream task of sentiment analysis. In sum, all the reviewers agree that the draft is not yet ready for publication.
All reviewers except one agreed that this paper should be accepted because of the strong author response during the rebuttal phase. Specifically the reviewers appreciated the new ablation study showing that improvements are not due to minor architectural changes, the new experiment on the number of time steps required for experiments, the agreement to change language around "neural energy minimization", the improvements to the related work, the novelty of the unrolled optimization approach, and the nice experimental results. Given this, I vote to accept. Authors: please carefully revise the manuscript based on the suggestions by the reviewers: they made many careful suggestions to improve the work and stressed that the paper should only be accepted once these changes are implemented. Once these are done the paper will be a nice addition to the conference!
This paper proposes a new approach to enforce monotonicity in the context of risk minimization, or to promote it as an inductive bias.  This improves upon existing point wise gradient based methods by expanding the region where monotonicity is enforced.  Group monotonicity is found valuable as a regularization for convolutional models, and multiple applications were shown where the approach appears effective.

The paper is well written, and received detailed discussion. Despite the rebuttal, some major concerns remain, such as drop in accuracy, and empirical estimate of the probability that Definition 1 would not hold over the distribution in question.  Overall, revisions are needed to make the paper publishable.
### Description 
The paper demonstrates that efficient architectures such as transformers and MLP mixers, which do not utilize translational equivariance in the design, when regularized with SAM (sharpness aware minimization) can achieve same or better performance as convolutional networks, in the vision problems where the convolutional networks were traditionally superior (with data augmentation or not, regularized or not). The paper demonstrates it very thoroughly through many experiments and analysis of the loss surfaces.

### Decision + Discussion

I find the paper to be very timely in its context. It has a remarkable coverage of experimental studies and different use cases: SAM + augmentation, +contrastive, +adversarial, +transfer learning; as well as ablation studies such as keeping first layers convolutional. The reviewers have asked further questions, and the authors were able to conduct respective experiments within the discussion period fully addressing all concerns and making the findings of the paper even more comprehensive and convincing.

After the rebuttal 3 reviewers were for "accept", one "marginally above" and one "marginally below". In the latter case the concern was that the paper is an experimental study of a known method, SAM. While I understand that many researchers are expecting theoretical and innovative results from ICLR papers, I find that it does not prevent acceptance. Indeed, the experimental findings in this paper are on a "hot" topic, could be of wide interest and could lead to a change of paradigm in designing models towards more generic ones. On the other hand, it could just indicate that CNNs are not fully exploiting their potential, e.g. not exploiting the context well enough in the hidden layers?

To get more insight, I am still wondering, how the predictions behave if the input is shifted by a few pixels in CNN and Transformers? It seems counterintuitive that making the first layers in ViT just an MLP of image patches is a good design. Furthermore, fully convolutional models allow to take input of an arbitrary size and average the predictions on the output if it happened to be larger than 1x1. 

Since convolutions are also used for e.g. semantic segmentation and generative models, one should not (and the authors do not in the paper) discard them too fast. See also a recent work combining transformers and convolutional networks,
Chen et al. (ICCV 2021) Visformer: The Vision friendly Transformer.
This paper tackles an open set setting where new classes (with few labeled examples) are introduced after the initial pre training on different categories. A simple approach is proposed based on a normalized softmax classifier and feature averaging to generate a classifier for the new categories. Results are shown on a few standard datasets as well as the Pl@ntnet dataset. 

While reviewers found the topic and setting (as well as Pl@ntnet dataset) interesting, they had significant concerns on the novelty (t3Uk, Tp5p, AHvJ), contribution, and rigor of the empirical evaluation. Since the method is simple and largely leverages from prior works, the latter is especially important; reviewers pointed out that some of the latest in metric learning is ignored (e.g. Proxy Anchor, Tp5p and AHvJ), and no comparison is made to other classes of methods that (by the authors  admission) are very close to the setting such as open set recognition (especially those that seek to classify new categories) and incremental learning.

Unfortunately, no rebuttal was provided by the authors, so these significant concerns remain and the paper cannot be accepted as is. Since the reviewers did appreciate the setting and dataset, I recommend refining the paper and significantly beefing up the empirical evaluation for future resubmissions.
The authors propose a method called Hybrid Memoised Wake Sleep (HMWS) for training models with both discrete and continuous latent variables efficiently using amortized inference. They extend Memoised Wake Sleep (MWS), which can only handle discrete latent variables, to discrete continuous systems by using importance sampling to approximately marginalize out the continuous variables and then applying MWS to the discrete variables.

This is well motivated and well written paper. The method is novel, clearly described, and evaluated in two fairly different interesting settings. However, while the empirical evaluation was considerably strengthened by the ablation studies and other experiments included in response to the reviewers, it is still on the weak side. The main issues are the relatively low dimensional latent spaces and insufficiently tuned baselines. As one reviewer pointed out, importance sampling is unlikely to scale well to high dimensions, so exploring this aspect experimentally would strengthen the paper. The use of default Adam parameters for all methods and models substantially undermines the results and might at least in part explain the underwhelming baseline performance. For example, VIMCO has been successfully used to train a discrete/continuous model of seemingly comparable complexity in [1] and yet here it seems to fail completely. To shed some light on this, the authors might want to explain how they used VIMCO and whether their approach was substantially different from the one from [1].

Finally, it at least somewhat misleading to claim, as is done e.g. in Sec. 4, that HMWS is more memory efficient than the baselines, when unlike them, it needs to store M discrete latent configurations per training case. Please make this claim more precise to avoid potential confusion.

[1] Variational Memory Addressing in Generative Models, Bornschein et al., NIPS 2017
This paper proposes a novel model based Bayesian meta learning approach that combines a novel conditional dropout posterior a new variational prior for the data efficient learning and adaptation of deep neural networks. It is applied to tasks such as 1D stochastic regression, image inpainting, and classification.

Overall, this paper received positive reviews: Reviewers thought that the "the paper makes a nice connection between meta learning and variational dropout, resulting in an overall elegant approach" and that the "reasoning of the novel prior is clear and understandable" while the "experiments are comprehensive".

Given the agreement of the reviewers and the novel use of dropout for adaptation of a model to different tasks, I recommend accepting this paper.
This paper proposes to use LOO to characterize the generalization error of neural networks via the connection between NN and kernel learning. The reviewers find the new results interesting. The meta reviewer agrees and thus recommend acceptance.
Canonical correlation analysis is a method for studying associations between two sets of variables. However these methods lose their effectiveness when the number of variables is larger than the number of samples. This paper proposes a method, based on stochastic gating, for solving a $\ell_0$ CCA problem where the goal is to learn correlated representations based on sparse subsets of variables. Essentially, this paper combines ideas from Yamada et al. and Suo et al. who introduced Gaussian based relaxations of Bernoulli random variables, and sparse CCA respectively. They also extend their methods to work with nonlinear functions by integrating deep neural networks into the $\ell_0$ CCA model. They gave experimental results on various synthetic and real examples, including to feature selection on biological data. The author response addressed a number of the reviewers  concerns, including by providing additional experiments and analyzing the genes selected by their model on the METABRIC dataset. Overall this is a solid contribution both from a theoretical and experimental standpoint.
The authors propose Variational Inference for Concept Embeddings (VICE), a method to learn representations such that an odd object can be detected given a triplet (i.e. the odd one out task). The authors build on Sparse Positive object Similarity Embedding (SPoSE) which learns sparse, non negative embeddings for images by placing a zero mean Laplace prior. Claimed contributions include replacing it with a spike and slab Gaussian mixture prior, and a principled approach to choosing the subset of the dimensions of the learned embeddings. The empirical results show improvements over the SPoSE baseline.

The reviewers appreciated the empirical improvements over SPoSE and accept that a more informative prior might lead to improved results. However, the **motivation, novelty and significance** of the proposed method doesn’t meet the acceptance criteria for ICLR. After the rebuttal and the discussion phase the reviewers felt that the work necessitates a major revision (notwithstanding the remaining issue with limited novelty), and raised the following as the main improvement points: 
  Clarifying the motivation and significance.
  Stronger empirical validation and generalization beyond the THINGS dataset.
  Address the discrepancy with analyzing GMM priors, but using unimodal Gaussians in the implementation.
  Comparing the chosen prior to other prior distributions and justifying the design choices.
This paper proposes to study Auto induced Distribution Shift (ADS), the phenomenon that models can create a feedback loop: the predictions of a model influence user behaviors when it is deployed, which, in turn, affects the accuracy measure of the model. The paper empirically shows that a meta learning algorithm called PBT causes a distribution shift instead of maximizing accuracy. While the premise of this paper is interesting, the proposed frameworks are very similar to the idea of strategic behavior in machine learning, and of "Performative Prediction" (Juan C. Perdomo, Tijana Zrnic, Celestine Mendler Dünner, Moritz Hardt). However, this line of work is neither cited nor discussed in this paper. In addition, the paper is hard to read in certain parts. We encourage the authors to compare their work with performative prediction. We hope the authors find the reviews helpful.
This paper proposes to analyze the generalization error of deep learning models and GANs using the Lipschitz coefficient of the model.

There was significant discrepancies in the evaluation of the paper among reviewers. While all reviewers acknowledged the interesting theoretical approach to understand generalization and the relevance to ICLR of the problem, they disagreed about the readiness level of the paper. Some concerns were expressed in terms of clarity (and the AC agrees with these), but most importantly, reviewer wKt9 pointed an important flaw in the current analysis that was not properly responded to by the reviewers (see below). In discussion, other reviewers were also concerned by this flaw, and so the AC decided to recommend a major revision of the paper taking the reviewers comments in consideration.

## Important flaw in the paper analysis (from wKt9)

Basically, Theorem 1 assumes that a loss $f(h,x)$ is $L$ Lipschitz w.r.t. input $x$ in some compact set of diameter $B$ for any $h$. The author shows that the:
$\sup_{h \in H} |E_{P} f(h,X)   E_{\hat{P}} f(h,X)|$ is upper bounded by $L B + C \sqrt{\text{stuff}/m}$.

The concern of wKt9 is that the LHS is upper bounded *trivially* and deterministically by the tighter $L B$ [see proof sketch next] for any distribution $P$ and $\hat{P}$ just because of the compactness of the input set and that $f$ is $L$ Lipschitz; one does not even need to include the number of samples $m$ in the analysis (thanks to the very strong assumption on $f$). The reviewer also was concerned that later (Theorem 3), the authors study ways that we can make $L$ exponentially small (which is interesting), but this has both the issues that:
1) it tells you nothing about the absolute performance of your network, as this only bounds the variation between any two distributions (indeed including the empirical and true distribution; but the fact that it also contains all distributions should indicate how loose this bound is!), and so perhaps the best empirical error one can obtained is still big
2) the current version of Theorem 3 uses a loose bound with a dependence on $m$ which was not even needed (as per the result above).

While it s true that empirically one can observe small empirical error, and thus combining this with a small Lipschitz constant would indicate good absolute performance; but the current presentation of the theory is rendered quite problematic by the above refinement, and should be corrected in a revision.

### Proof sketch:
For simplicity, I ll prove it for $P$ being a discrete distribution and $\hat{P}$ being the empirical; but I m pretty sure you can extend it to continuous distributions as well.

Note that we have $|f(h,x)   f(h,x )| \leq L B$ for all $x, x $ in the compact set of diameter $B$ and for all $h$.

Now $$E_{P} f(h,X)   E_{\hat{P}} f(h,X)   \sum_j \pi_j f(h, x_j )   \frac{1}{m} \sum_i f(h,x_i)$$

For each $x_i$, associate several $x_j$ s so that the total sum of their probabilities is $1/m$ (split some $\pi_j$ in multiple pieces if necessary)   we can augment the index set for these new pieces, to obtain new probabilities $\pi_j $ and call $I_i$ the set of associated indices to $x_i$. We have $\sum_{j \in I_i} \pi _j   1/m$

We thus have:
$$E_{P} f(h,X)   E_{\hat{P}} f(h,X)   \sum_i \sum_{j \in I_i} \pi _j \left[ f(h, x _j)   f(h,x_i) \right]$$

Thus:
$$|E_{P} f(h,X)   E_{\hat{P}} f(h,X)| \leq  \sum_i \sum_{j \in I_i} \pi _j \left| f(h, x _j)   f(h,x_i) \right| \leq L B$$

This is true for any $h$, so this is also true for the $\sup$, *deterministically*! QED
This paper proposes an unsupervised learning method for GANs, called SLOGAN, which allows conditional generation of samples, by utilizing clustering structures of training data in a latent space. The main significance of the proposal over existing unconditional conditional GANs is that it is capable of dealing with training data with imbalance in the latent space. The proposal consists of the use of implicit reparameterization based on the generalized Stein lemma, which makes learning of the mixing coefficient parameters possible, as well as introduction of the U2C loss.

The initial review score distribution is such that two of them are just above the acceptance threshold, and two others are just below it. Upon reading the review comments and the author responses, as well as the paper itself, I think that the evaluations of the reviewers are more or less coherent with each other:

1. The proposed method is moderately, if not significantly, novel: The differences from DeLiGAN are the use of implicit reparameterization based on the generalized Stein lemma, learning of the mixing coefficient parameters, and introduction of the U2C loss.
2. The experimental results, while demonstrating effectiveness of the proposed method to some extent, were not convincing enough.

As for the item 2, the authors have provided results of additional experiments in their responses, as suggested by the reviewers, and two reviewers have revised their scores upward accordingly.

Yet another point I would like to mention is that in some numerical results summarized in Tables 1 and 2, as well as in several other places, one can notice somewhat large errors, so that one might be able to question the statistical significance of the claimed best performing methods, shown in bold. (If my guess would be correct, the authors regarded the *best in the mean* as the best, ignoring the standard error, and did not perform any statistical testing to confirm the significance.) I would therefore appreciate additional assessment of significance of the numerical results via proper statistical testing.

Because of the above, I would like to recommend acceptance of this paper.
The authors analyze linear regression with gaussian covariates in an
asymptoptic setting, where the number of examples and the number of
covariates go to infinity together.  They identify conditions
on the covariance under which "multiple descent" occurs, and
conditions under which a regularization removes this effect.

Concerns were raised that the overlap between this paper and previous
research was too substantial for it to be published in ICLR.  These
persisted after the authors  response and the discussion period.
The authors focus on the conditional generation of molecular conformations (i.e. 3D cartesian atom positions) from a given molecular graph. They formulate the generation via diffusion probabilistic models.  Conformations are generated by a reverse diffusion process from isotropic Gaussian noise to molecular conformations. This diffusion process is learned from data using a SE(3) invariant formulation of the diffusion process. The authors work directly with atomic positions (i.e. a point cloud) instead of interatomic distances or an intermediate bond geometry representation. Experimental evaluations show state of the art results according to COV/MAT metrics on GEOM Drugs and GEOM QM9 datasets.

Strengths:

  High technical novelty: first generative model for molecular conformation generation based on a diffusion framework
  Very clearly written paper.
  Impressive empirical results with state of the art results on GEOM Drugs and GEOM QM9 datasets.

Weaknesses:

  Most of the weaknesses reported by the reviewers seem to have been addressed in the rebuttal.

The idea of the work is highly novel. The authors propose the first generative model for molecular conformation generation based on a diffusion framework. This paper brings together recent ideas and methods (e.g. diffusion, SE(3) equivariance) to the established task of molecular conformation generation with impressive empirical results. All the reviewers agree on acceptance with high scores. I recommend the authors to look at the reviewers  comments to improve the paper for the camera ready version
The reviewers recommended a rejection. The authors of the paper did not respond.
The paper aims to devise a distributed multi task privacy preserving framework for image processing. In this regard, author propose partitioning neural network models into task specific heads/tails and a common task agnostic feature backbone (body). A training procedure is designed which is claimed to be privacy preserving wherein the head and tail is trained locally on the client or using federated learning when multiple clients share a task, while the main backbone/body is trained in a centralized manner by collecting appropriate gradients from the clients. Making easy to follow code is also highly appreciated. We thank the reviewers and authors for engaging in an active discussion and also updating the paper. While the new version is definitely resolves some of the concerns of the reviewers, some still remain. Privacy preserving in title and in main body of the paper seems misleading. Proposed method doesn t provide any guarantees for privacy (also pointed out by many reviewers). The author response doesn t seem to be convincing and other federated learning papers do not claim privacy unless having some specific mechanism like adding noise, secure aggregate, etc. Also, the reviewers are in consensus that novelty as well as large scale empirical evaluation is limited.
Standard algorithms for deep hypergraph learning have not been designed for hypergraphs with edge dependent vertex weights (EDVWs), where the weight of a vertex can depend on the edge of which it isa member. This paper develops a connection between EDVW hypergraphs and undirected simple graphs, thus enabling the use of existing undirected graph neural networks as subroutines. This is done via a unified random walk framework.

(Two typos: ``equivalency" should be ``equivalence", and ``undigraphs" should be ``undirected graphs".) 

The theory of equivalence between EDVW hypergraphs and undirected graphs via random walks is a good contribution. The experimentation across different domains is laudable. 

However, there are concerns over the lack of key baselines in the experiments. The author rebuttal has presented additional results with some baselines: sensitive hyperparameters (e.g., learning rate) are not tuned for the baselines. The clarity of the paper is mixed. The map from hypergraphs to graphs is not injective, so there could be ambiguity issues (different hypergraphs mapped to the same graph, thus having the same representations). 

Also, the contributions of Section 3 (designed for simple undirected graphs alone) do not appear significantly novel.
The paper proposes a sampling technique for unnormalized distributions. The main idea is to gradually transform particles by following the gradient flow of the relative entropy in the Wasserstein space of probability distributions. The paper tackles an important problem and provides an interesting new perspective. However, even putting aside the concerns on the theoretical analysis raised by the reviewers, the experimental evaluations does not seem sufficient to demonstrate the benefits of the proposed approach.
The meta learning framework based on learning the loss function for time series forecasting is an interesting and important topic. However, the reviewers think the literature, baselines, and experimental results need significant improvement.
The reviewers were split about this paper: on one hand they appreciated the clarity and the experimental improvments in the paper, on the other they were concerned about the novelty of the work. After going through it and the discussion I have decided to vote to accept this paper for the following reasons: (a) the potential impact of the work, (b) the simplicity of the idea, and (c) promise of release of open source code. I think these things make the paper a strong contribution to ICLR. The only thing I would like to see added, apart from the suggestions detailed by the reviewers, is a small discussion on the carbon footprint of training such largescale graph networks. The authors motivated the work by saying it could have a beneficial impact for modelling energy which is needed to combat climate change. However, we know from recent results that such large scale models also have a non trivial emission footprint. So I d like to see the authors specifically calculate the carbon footprints of the models they trained. There are tools to help with this such as: https://mlco2.github.io/impact/  With this addition I think this paper will not only make a large impact on graph network training but also start a discussion of how to responsibly decide training, taking environmental impact into account.
The paper proposes a method for structured representation learning using autoencoders. The method has two primary ingredients: (i) encourage independence in latent blocks by feeding different blocks of the latent representation to different depths of the decoder by injecting noise in an Ada IN inspired block, (ii) a so called hybrid sampling, that samples each block from a fixed learned set of k latent vectors, similar to the codebook used in VQ VAE (Oord et al 2017). The method is claimed to result in higher fidelity reconstruction and generation while also learning representations that are more disentangled compared to VAE and $\beta$ VAE. 

Some limitations that came up in the reviews and later in the discussion among the reviewers are (i) lack of comparison with more advanced disentangled VAEs, which would be helpful in establishing the claim of the paper on better reconstruction but comparable disentanglement performance to regularization based methods (ii) high level similarity to VLAE and other methods that also use hierarchical latent variables that limits the claims on novelty. Current draft also emphasizes disentanglement which the reviewers found lacking in justification and rigor. The paper is currently not suitable for publication at ICLR but taking into account the comments from reviewers on the presentation aspects will help improve the paper.
Reviewers agreed that taking into account the secondary structure in addition to the amino acid sequence, although not new in bioinformatics, may be a good idea in the context of deep generative models of peptides. On the other hand, all reviewers also agreed that the experimental results do not allow concluding about the potential benefit of the method, i.e., whether it is likely to produce potent AMPs (and whether it does it better than existing methods). Indeed, the proposed computational criteria can not replace a proper experimental validation, and it is not clear whether a "better method" on the computational criteria will be "better" in the real world. Second, the results on the computational criteria are not convincing: regarding the physical properties, it remains debatable to claim that a method is good if it outputs many AMPs that fulfill the criterion, while less than 7% of the true AMPs do; and regarding the computational prediction of being an AMP, the proposed method is outperformed by existing ones. In conclusion, we consider that the paper is not ready for publication at ICLR, since there is no significant methodological novelty nor significant experimental results if this is an application paper, and we encourage the authors to consider a publication with wet lab experiments to demonstrate the relevance of the method.
This paper uses prototype memories for learning generative models. Inspired by the finding that there is sparse activity and complex selectivity in the supragranular layers of every cortical region, even primary visual cortex, the authors propose to use prototype memories at each level of the hierarchy, which marks their work as novel. They show superior performance in few shot image generation tasks.

The reviewers  scores were borderline (5,5,8), making this a case that required some AC consideration. The reviewers generally agreed that the paper was relevant and interesting, though the two more negative reviewers had some concerns about (1) the tests used, (2) the interpretation relative to neuroscience data, and (3) the novelty. After reading through the paper, the reviews, and the rebuttal s, the AC felt that the authors had made a decent attempt at addressing items (1) and (2), and item (3) was ultimately a subjective question. The authors were reasonably clear about what marks their work as novel, and it is certainly not *exactly* the same as previous work. Altogether, given these considerations, the AC felt that this paper deserved to be accepted, given the reasonable attempts from the authors to respond to the reviewers  concerns and an average score above acceptance threshold (though the scores did not change post rebuttal, it should be noted).
While the reviewers acknowledge the broad experimental work done in this paper, they all find several issues, which in their combination show that the paper is simply not in a good enough shape. This impression has not changed during the rebuttal phase and as a result, this is a clear reject.
This paper proposes a new semiretro algorithm by combining the two major approaches of retrysynthesis, the template based method and the template free method   breaking a full template into several semi templates and embedding them into the two step template free framework.  They also obtained state of the art performance in this task based on the recent GNN architecture. Although all reviewers were satisfied with the idea and excellent performance of this paper, the possibility of information leakage in their experiments was raised through the discussion period, and the authors seem to agree to this to some extent.

In conclusion, it is difficult to say that accurate and rigorous experimental verification of the proposed method has been made yet. I encourage the authors to resubmit the paper after correcting errors in their experiments.
This is an interesting paper discussing differential privacy for multi label classification. The initial reviews rated the paper with rather extreme scores, therefore I have invited an additional reviewer. This review did not clarify the issues raised by the most critical reviewer, but pointed out that the goal of showing how DP can be enforced in MLC is not fully obtained as there is a lack of the discussion concerning the MLC performance. This is also a problem raised in my comments. Taking this into account, I need to state that the paper is not ready for publication.
This paper proposed a strategy to train EBMs according to the length of MCMC trajectories required. The paper covers three settings with the different length of MCMC: image synthesis, adversarial defense, and density estimation. The reviewers generally find that there are interesting ideas and promising results in the paper, but the paper is not ready to publish at its current stage. The argument regarding density estimation and FID evaluation is not convincing. The proposed method is also more complicated than the baseline methods (CoopNets and PCD), and we would need a stronger argument for the added complexity.
Reviewers agreed that this work is well motivated and presents a novel approach for data augmentation around the adaptive augmentation policies. There were some concerns around the lack of ablation studies and unclear performance improvements, which were addressed well by the authors’ responses. Thus, I recommend an acceptance.
The reviewers found this work well motivated and the additional experiments conducted during the response phase were greatly appreciated. Anderson s acceleration appears to be a simple device that may be of great value to this field, and therefore this work is a timely contribution. The presented theoretical results justify the authors  modifications, although at times it felt more comparisons would be welcome: (a) Section 3.2 could have compared to a lot of three term recurrences that lead to the optimal dependence on the condition number, including Chebyshev s polynomial and conjugate gradient, as well as the results in Brezinski et al. (2018); (b) Section 3.3 would benefit from some comparison with "Evans, Claire, Sara Pollock, Leo G. Rebholz, and Mengying Xiao (2020). “A Proof That Anderson Acceleration Improves the Convergence Rate in Linearly Converging Fixed Point Methods (But Not in Those Converging Quadratically)”. SIAM Journal on Numerical Analysis, vol. 58, no. 1, pp. 788–810." (c) the results in Section 3.4 seem to be a bit preliminary and it would be great if the authors could compare to standard rates of SGD. 

Overall we believe this work will generate more interest on memory based optimization techniques in deep learning, and we encourage the authors to thoroughly polish their draft by incorporating the reviewers  comments and the responses during the discussion phase.
This paper proposes a new method for multi label classification, which leverages the advantage of the emery based model. However, one reviewer and the area chair have two serious concerns on the experiments: (1) The proposed method is only evaluated on low dimensional datasets; (2) Some important baselines methods are missing, which makes the comparison not convincing. I suggest the authors to evaluate their methods on more datasets, and add the results from well known multi label classification method for comparison.
The paper addresses the problem of generating captions for ECG signals where they extend the task in the literature from monolingual to multilingual captions. The model proposed in this work is a variant of mask language model where they augment the target by switching the words from one language to another. In addition to predicting the actual words, they also predict the language associated of the words.

Pros
+ The problem is motivated by real world application and need. 
+ The presentation is clear and the authors compare the performance of the model with appropriate recent models.

Cons
  The model has higher complexity in both training and parameters, yet achieves only comparable performance to simpler models.
  Empirical evaluation of the multi lingual output is inadequate since the ​ground truth is derived from Google Translate.
  There are also a number of other specific concerns raised by the reviewers (e.g., VuBG, HBxE).

Reviewers raised questions about several shortcomings. In response the authors updated the paper and were very engaged in the discussion with the reviewers. However, at the end of the day, the paper in its current form has serious shortcomings and left the reviewers unconvinced. I suggest the authors take advantage of the feedback from the reviewers and address them fully in their next iteration.
This paper proposes a new paradigm   called in sample Q learning   to tackle offline reinforcement learning. Based on the novel idea of using expectile regression, the proposed algorithm enjoys stable performance by focusing on in sample actions and avoiding querying the values of unseen actions. The empirical performance of the proposed algorithm is appealing, outperforming existing baselines on several tasks. The paper is also well written.
The paper considers the saddle point problem of finding non convex/non concave minimax solutions. Building onEG+ of Diakonikolas et al., 2021 that works under weak MVI conditions, the work presents a new algorithm CurvatureEG+ that works for a larger range of weak MVI condition compared to previous work and also works for the constrained and composite cases. The authors show cases where this algorithm converges while the previous algorithms can be shown to reach limit cycles. Overall, this theoretical work seems strong. Most reviewers seem to agree that the contribution is good enough for publication. Compared to EG+ the additional contribution is to expand the range of weak MVI condition. While this seems like a slight improvement, looking beyond just the final convergence rate, the paper has some nice insights that provides a unifying view that captures past algorithms (like EG+ as special case). I recommend acceptance.
This paper studies the effect of data quality on adversarial robustness. It focuses on a single measure of data quality (number of times there is a perturbation that is misclassified across training iterations). The authors study the effect of data quality on robust overfitting, robustness accuracy tradeoffs and "robustness overestimation" (gap between strong and weak attacks). The main conclusions reported are that data quality as measured by their metric plays an important role in all three aspects, and a takeaway is that data of higher quality may improve robustness. While the reviewers appreciated the premise of this work, some concerns remain post rebuttal. For example, few reviewers remain skeptical of the universality of the notion of "data quality" as measured in the paper because different training methods behave differently and the data quality measured in the paper is tailored to a particular training algorithm. Some reviewers also opined that at least one of the practical implications discussed in the rebuttal should be systematically investigated and that it is important to study the effectiveness of different data quality measures, especially for the extra data. Given all this, we are unable to recommend acceptance at this time. We hope the authors find the reviewer feedback helpful.
This work considers one shot pruning in deep neural networks. The main departure from previous work is to consider stochastic Frank Wolfe. The reported results are convincing although a number of baselines were missing from the initial submission. The authors provide a balanced account of the strengths and weaknesses of the proposed approach.

The authors adequately addressed the concerns of the reviewers. For instance they ran additional experiments to compare to missing pruning baselines. I would encourage the authors to revise the manuscript by including the missing related work, the additional clarification discussions (e.g., motivation for K sparse constraints, follow up analysis, and cost per iteration) and to include the additional experiments that were conducted (e.g., pruning with training).
This work proposes a system for generating piano music (in the symbolic domain) using a learned reward function. Reviewers raised concerns about the organisation of the paper, clarity of writing, a lack of experimental comparison with previously published approaches (and the quality of the baseline), several unsubstantiated claims, and some missing related work. Unfortunately no attempt was made to address these issues.
This paper focuses on improving the efficiency of sharpness aware minimization method for training neural networks. The proposals are stochastic weight perturbation, namely selecting subset of the parameters at any step, and sharpness sensitive data selection. The philosophy behind sounds quite interesting to me, namely, sharpness aware minimizer can be approximated properly with fewer computations after analyzing the min max procedure. This philosophy leads to a novel algorithm design I have never seen.

The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please include the additional experimental results in the next version.
This paper tackles an interesting problem: distribution shift generalization often requires parameter identification but this is not possible for over parameterized neural networks. This paper shows for quadratic neural networks, it is possible to identify the function without identifying the parameter. 

This is an interesting result. However, reviewers raise concerns about the assumption and technical details. The meta reviewer agrees with these concerns.
This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.
This paper examines conditional GANs, which are found to lead to model collapse in low data settings. The paper proposes what appears to be a simple but effective method that addresses the issue. Reviewers were generally happy with the experiments and the utility of the observations and analysis. Code for the method was provided during the author response period. Only one reviewer did not vote to accept this paper, but they did acknowledge that the authors had addressed their concerns during the discussions with the authors. All others rated the paper as an accept.
The AC recommends accepting this paper.
The paper takes a creative step in the theory of tournaments, and it seems plausible that this could lead to interesting follow ups. The reviewers made many excellent comments and I highly encourage the authors to take ALL of them into account in the revision, it will make the paper much stronger.
Two trust region constrained optimization for policy gradient RL, where the second trust region is based on a virtual policy built from a memory buffer and using an attention mechanism to combine prior policies. The reviewers agree that the paper is well written, the idea is novel, and the paper is extensively evaluated. The authors are commended for running the additional baselines during the rebuttal period.

However, the paper still contains some shortcomings, specifically, the results are somewhat inconclusive even after the rebuttal. While it is not expected that the method wins across the board, it is important to provide an analysis of the limitations of the method. When is the algorithm appropriate to use, and when is it not? 

To make the paper stronger, in the next version of the paper should:
  move the theory in the main text (Appendix C).
  provide the analysis of the algorithm and its limitations.
This paper presents work on video scene segmentation.  The reviewers appreciated the introduction of a boundary aware pre training method.  However, concerns were raised regarding limited novelty, empirical effectiveness, and generic applicability.  The reviewers engaged in significant discussion based on the other reviews and authors  responses.  Based on these discussions the reviewers concluded that while the proposed method does have differences with respect to BSP, the overall contributions were not sufficient for inclusion in ICLR.
The main consensus among the reviewers was that although the approach is interesting, this submission suffers from two main weaknesses:

  The methodology is not very novel, and the proposed parts of the method not well justified (in particular regarding the interplay of a differentiable sorting approach and of the random choice of k)

  The results, compared to a standard cross entropy loss are not very convincing: there does not seem to be a statistically significant advantage.
After going over the reviews and the rebuttal, and skimming the paper, I feel like unfortunately this paper is not ready to be accepted.

My reasoning is as follows. I feel the comparison with A2C and PPO is not and should not be the main target of the work. Of course they are good to have as reference points, and they should be in the paper. But the work is not trying to claim that the distilled symbolic policy is more data efficient (or outperforms these methods). If that would be the point, that one has questions similar to reviewer cXsw about these baselines maybe underperforming (compared to other published work). Maybe this is due to a change in setup as argued by the rebuttal, nevertheless this makes comparison and understanding the results difficult. The other argument is that  A2C / PPO are not the most efficient DRL methods for atari.  Lastly, is the question of distilled symbolic policy having access to an expert, making this not an apples to apples comparison. 
But as I said, and I think this is the point of the authors as well, this is not the point of the paper. But then I find the results not being sufficiently contextualized either by comparing to other methods in this space, or various ablation studies to motivate the choices taken by the authors. Similar points were raised by other reviewers (wezQ, cXsw). Some of these ablations have been brought forward in the rebuttal, but I think they should be a more central part of the work and implies considerable edits to the paper. 
I think the stance that the object identification is decoupled from the symbolic policy is also a bit dangerous. I.e. a learned object identifier (particularly in a visual more complex setting) will have different failure modes, which will affect the policy. I think having a paragraph discussing the issues raised by reviewer AL2N would actually strengthen the paper, and being open about open questions/weaknesses. Alternatively additional ablation or experiments in either other kind of environments (e.g. 3D or environments with occlusion) or just assuming some form of failure at segmentation the visual stream into objects to show robustness would be of interest. 

Overall I urge the authors to resubmit their work after properly integrating some of the feedback. In particular focusing on ablation studies or having baselines that are more similar in spirit or at least being more explicit of how it compares with existing work and what aspect of that existing work is trying to fix. For e.g. part of the approach is that it relies on distillation rather than dealing with the RL objective (as other methods might try to do). Now if you take those methods, but you phrase them in a distillation process how would they do? I don’t know if all of this needs to be done, but it just feel as a work to be less grounded and sufficiently far to other existing methods to trivially understand the relationship, while directly only comparing to non symbolic methods in a way that is not in some sense in the advantage of the non symbolic methods. 
 Additionally,  being more explicit about the potential weaknesses of the method, maybe empirically showing what happens with imperfect segmentation. The work is interesting, and I agree that this is a young field and the goal is *not* to produce state of the art results or outperform DRL methods. And is *not* to solve all the problems with symbolic methods at once, but to improve our understanding in this space. But I think the framing is not the right one in the current manuscript.
This paper proposes a new dataset, called RainNet, obtained from gridded precipitation data, for training precipitation downscaling methods, as well as a new neural network based architecture for that task, which estimates the underlying dynamics of the local weather system, and new metrics for evaluating precipitation downscaling methods.

Reviewers praised the large, novel and useful dataset (D3tQ, szBD, ggKX) and novel metrics for evaluating statistical downscaling methods (D3tQ), along with evaluation on 14 baselines (szBD, ggKX).

There were however many issues highlighted by the reviewers. First, reviewer D3tQ raised concerns about the paper being resubmitted after rejection from NeurIPS (/pdf?id VVZZJiQB51l), with minimal changes (/pdf?id 6p8D4V_Wmyp), and noticed that the authors did not follow up on most reviewer recommendations. D3tQ noticed however that in the ICLR resubmission, the cross validation results were presented to provide a more robust comparison between models, and that the discussion of metrics in section 4 was much more thorough than in the previous version.

Other themes in the negative reviews included concerns about missing standard errors in the cross validation results (D3tQ, 5pVg) or measures of uncertainty in the upscaling (ggKX), lack of information about hyperparameter tuning (D3tQ), inadequate literature review about statistical downscaling (D3tQ), lack of information about the dataset (5pVg), missing discussion about applications (ggKX) and insufficient proofreading (D3tQ, 5pVg).

I will not take into consideration the criticism from szBD who "don t feel that ICLR is the right venue for this work" as I do not find such opinions to be much helpful.

The authors did not provide a rebuttal to the initial reviews and there was no discussion about this paper among the reviewers. Given the issues raised by the reviewers and the scores of 3, 3, 5 and 6, I believe that this paper does not meet the acceptance bar in its current form.

Sincerely,
AC
The paper extends the original work on flooding to individual instance level to prevent overfitting. Even though the technique is a intuitive extension, the reviewers appreciate its simplicity and effectiveness, and consider the extension necessary. Most reviewers  concerns were addressed through rebuttal.
The paper under review provides a theoretical analysis for contrastive representation learning. The paper proposes a guarantee on the performance (specifically upper and lower bounds) without resorting to previously used conditional independence assumptions. Throughout, the theoretical results and assumptions are supported by experiments. 

After a lively discussion, and after changes made to the paper in the revision stage, all four reviewers recommend this paper for acceptance. 
  Reviewer tWSB appreciates that the paper makes weaker assumptions than prior work (i.e., not assuming conditional independence), but raises a number of serious concerns on the theoretical results: The review questions whether assumption 4.6 used in the theory can be true, and whether the bound is vacuousness. The authors argue that this assumption was used in prior work, point out that only some of their results rely on this assumption, and that the assumption is compatible with the theory. The response of the authors partly resolved the reviewers concern and the reviewer raised their score. 
  Reviewer bTLa finds the idea of understanding contrastive learning for intra class samples interesting, but finds some key assumptions too strong, a critique similar to that raised by reviewer tWSB. The authors responded and the reviewer increased their score, and mentioned that most concerns were addressed. The response partially resolved the reviewers concern, and the reviewer now also recommends acceptance. 

I recommend to accept the paper. Understanding contrastive learning better is an important problem, and based on my own reading, I agree with the reviewers that the paper contributes to the understanding of contrastive learning. Two reviewers had concerns about unrealistic assumptions, but those have been largely resolved in the discussion.
The paper proposes a novel approach to graph representation learning. In particular, a graph auto encoder is proposed that aims to better capture the topological structure by utilising a neighbourhood reconstruction and a degree reconstruction objective. An optimal transport based objective is proposed for the neighbourhood reconstruction that optimises the 2 Wasserstein distance between the decoded distribution and an empirical estimate of the neighbourhood distribution. An extensive experimental analysis is performed, highlighting the benefits of the proposed approach on a range of synthetic datasets to capture structure information. The experimental results also highlight its robustness across 9 different real world graph datasets (ranging from proximity oriented to structure oriented datasets).

Strengths:
  The problem studied is well motivated and the method proposed is well placed in the literature.
  The method is intuitive and the way that the neighbourhood information is reconstructed appears novel.
  The empirical comparisons are extensive.


Weaknesses: 
  Some of the choices in matching neighborhoods seem a bit arbitrary and not sufficiently justified. 
  The scalability of the proposed method is questionable. The method has a high complexity of O(Nd^3) (where N is the number of nodes and d is the average node degree). The authors address this problem by resorting to the neighborhood sampling method (without citing the prior art), which is only very briefly discussed in the paper. 
  The reviewers have also expressed concerns about the fixed sample size q. The question of how the neighbour sampling is handled when a node has less than q neighbours remains unanswered.
The authors introduce a method that improves goal conditioned supervised learning (GCSL) by iteratively re weighting the experience by a variable that correlates with the number of steps till the desired goal. The reviewers mention that the authors focus on an important problem, their method is simple and the empirical results are significant. However, they do point several flaws of the paper, the main ones being questionable theoretical claims and the clarity of the presentation. After an extensive discussion, most reviewers agree that the paper should be accepted but I do encourage the authors to take into account the comments by the reviewers for the final version of the paper and make the theory more clear.
This paper proposes to encode positions of nodes in graphs by anchor based GNN with customized message passing steps. All reviewers raised significant concerns on this paper, including novelty of the message passing steps, experiments, writing and clarity, etc. The authors have actively responded to reviewer comments, but many of the concerns are still not addressed. Thus, the paper needs some work in order to be competitive.
The authors introduce a method for offline imitation learning in the presence of optimal and non optimal data. In particular, they propose to learn a discriminator that can be then further used to modify the behavior cloning loss which leads to performance improvements over baselines. The reviews mention that the idea is novel and most sections of the paper are well written and self explanatory. They do point out, however, several flaws such as the clarity of the derivation and  the thoroughness of experimental evaluation. While the paper has significantly improved during the rebuttal, its significant changes warrant another round of reviews. I encourage the authors to continue improving the paper, addressing the reviewers  feedback and resubmitting it as it has a potential to be a strong submission.
The problem considered in this paper is of general interest to all reviewers. However, while the reviewers in general appreciate the authors’ effort in providing theoretical analysis for a seemingly effective algorithm, they are unconvinced that the key technical claims are well justified (i.e. separation between theoretical analysis and the algorithm, which ultimately relies on the OOD score), the propositions are clear (e.g., key claims in the quality of kNN density estimator as an OOD detector not well supported by analysis/experiments), or that the experimental results are sufficiently compelling (e.g., lack of controlled experiments/ ablation study) to merit acceptance for the proposed solution.
This work presents a new sentiment representation method with the use of affect control theory and BERT. Reviewers pointed out several major concerns towards the insufficient experiments and results, as well as the lack of ablation studies and related work discussion. I would like to encourage the authors to take into account the comments from reviewers to further improve their work for a stronger version for future submissions.
This paper presents a GNN based attention mechanism and tests it on a robotic stacking task.

While all the reviewers agree that this work is novel and interesting, they also are unanimous (even after the rebuttal) in pointing to the insufficient experimental evaluation of the proposed method.

I encourage the authors to incorporate the feedback of all the reviewers.
This paper proposes improving human interpretability and manipulability of neural representations by obtaining syntactic roles (here, subject, object, prepositional object, and main verb) without supervision by means of them becoming linked to latent variables in a novel proposed attention driven VAE (ADVAE) model, which provides cross attention between a language transformer and latent variables. The paper argues that syntactic roles are quite central to meaning interpretation and that the ADVAE recovers them better than LSTM or Transformer (with mean pooling) VAEs.

This is a quite interesting direction and paper. There was active discussion with the reviewers, one of whom (9pDc) moved their rating from reject to quite strong support, while the other reviewers either sat on the fence or raised from reject to borderline. Nevertheless, I overall tend to agree that the paper is still lacking in empirical support, a view clearly shared by reviewers WuPD and 7uFL. The SNLI data is very simple descriptive sentences, nearly all in the form of S V O or S V PP. Would this work on more complex data, in other languages, or with more word order variation? There isn t very much investigation, but the new results added during reviewing based on Yelp data seem to offer more concerns than confidence. These are also very short sentences but with more varied structure and some complementation. It seems like D_{dec} is now very low (much lower than for the sequence VAE), the ability to distinguish out grammatical roles seems limited to {subj} vs. {dobj, pobj} in the encoder and none at all in the decoder (Figure 6/7). And then for the examples in Appendix D, the disentanglement abilities barely seem stronger than being able to pick out subjects, though when there are sentences with subordinate clauses, it is perhaps random which subject you get. The resampled realizations in appendix H also seem to show limited disentanglement: resampling the subject usually seems to change the object as well, often markedly. No convincing downstream applications are shown. As such, while I agree that disentanglement is at the heart of representation learning, I can t get on board with reviewer 9pDc feeling that this paper now has convincing results. Reviewer 7uFL also emphasizes that there is no strong reason that the latent variables have to align with syntactic roles. In particular, the motivation in NMT whereby constituents clump and reorder together does not exist here. It may only work for the very simple and regular sentences of SNLI.

Hence, overall, I feel that this method needs more extensive validation on harder, more varied data sets before it becomes a convincing contribution, and so I propose rejecting the paper at this point in time. Nevertheless, I do think the topic is interesting and this approach has the potential to be good.
The paper proposes a weakly supervised contrastive learning, using auxiliary cluster information, for representation learning. Their method generates similar representations for the intra cluster samples and dissimilar representations for inter cluster samples via a clustering InfoNCE objective. Their approach is evaluated thoroughly on three image classification task.

The reviewers agree that the paper is well written, presenting interesting theoretical analysis (Reviewer h3zd,  a8kw) and solid experimetal results (Reviewer RhYi, 1ziy). The core idea of the paper is relatively simple and well motivated (Reviewer h3zd). While the focus is using the clustering with auxiliary labels, the method can be applied without auxiliary labels with K means. 
There were some concerns from the reviewers:  the overlap with a concurrent work [1]. The authors have provided detailed discussions on conceptual (concurrent work focuses on unsupervised cases where this work focuses on weakly supervised setting) and emprical comparisons. Accordingly, reviewer a8kw and 1ziy had some issues with the novelty of the paper, as it can be interpreted as slight modification from previously explored idea (vanilla InfoNCE loss). 

Despite some overlap with existing approaches, the paper presents an interesting and well conducted study of integrating clustering information for learning representation, so I vote for acceptance. 

[1] Weakly Supervised Contrastive Learning. ICCV 2021.
Although sharing data between tasks benefits multitask RL, this requires that rewards be relabeled across tasks. This paper shows that, for binary rewards, directly reusing data from other tasks with constant reward relabels is effective, and the paper develops a method around this idea that is highly effective.  The reviewers found that the idea and execution were impressive, that the paper was well written, and that the empirical analysis was convincing. 

In response to concerns in the preliminary reviews about certain shortcomings in the empirical analysis and some lack of theoretical analysis, the authors provided substantial revisions to the paper. Due to some lack of reviewer response to the discussion, this meta reviewer examined whether those revisions were sufficient to address the reviewers  concerns. The authors did a good job in providing the requested improvements and the analysis is stronger, but remaining similarities to existing methods (CDS) means that this paper still remains borderline. These same concerns were also shared by reviewers that continued to engage in discussion with the authors. To remedy this, the authors are encouraged to better and more substantially address differences with prior work in the writing and motivation throughout the entire paper. In addition, although space is a concern, it would be beneficial to integrate the high level takeaways from the new analyses in the appendices into the main paper.
This paper applies deep learning to a problem from OR, namely multistage stochastic optimization (MSSO). The main contribution is a method for learning a neural mapping from MSSO problem instances to value functions, which can be used to warm start the SDDP solver, a state of the art method for solving MSSO. The method is tested on two typical OR problems, inventory control and portfolio management. The reviewers think that the idea is interesting, the empirical results are impressive, and the paper is well written. However, there are reservations on its relevance to the ICLR community.
The proposed method, Differentiable Symbolic Execution (DSE), addresses the safety of learned navigation and control programs. The approach samples code paths using a softened probabilistic version of symbolic execution,  constructing gradients of a "safety loss" along these paths, and then backpropagating these gradients through program operations using RL. 

Pros
   The paper is well written and sound
   The issue of safety is underexplored
   The method improves over a strong baseline on benchmarks

Cons
   The benchmarks are relatively small scale and artificial
The authors propose a new MLP Mixer like architecture called Cycle MLP which has two main advantages with respect to MLP Mixer: (i) it’s applicable to varying input image sizes, and (ii) linear computational complexity. The authors present competitive results on image classification, object detection and segmentation.

The reviewers felt that both (i) and (ii) are key issues in the current MLP Mixer based models. The reviewers also appreciated the simplicity of the idea and the execution of the empirical evaluation. During the rebuttal and discussion phase the authors provided compelling evidence for the issues pointed out in the initial review. 

Given that MLP Mixer based architectures are becoming increasingly popular, I believe that these contributions will be of great interest to the ICLR community and I will recommend acceptance.
Due to the delayed rebuttal made it very hard for reviewers to react.

The paper proposes a new sub type of POMDPs dubbed AFA POMDP. The proposed approach first learns a sequential VAE, then an RL approach learns control and feature acquisition policies jointly. The approach is evaluated on two tasks and shows very promising results compared to baselines. Overall the setting and the approach are very interesting.

The replies and revised paper managed to address some of the concerns of the reviewers. However, there remain a few open questions and doubts (see updated reviews), in particular as some of the arguments of the authors remain in the hypothetical, and the reviewers are still not entirely convinced by the choice of the experimental tasks.
The manuscript proposes a method to adjust a biased model without requiring explicit annotations of biases. The main hypothesis of the manuscript is that there are differences in the direction and magnitude of the loss gradients for underrepresented samples compared to majority patterns in the training data. Based on this hypothesis, the manuscript proposes a rejection sampling method that tries to balance samples in a minibatch. However, a sample with a noisy label can appear to be an underrepresented sample with a correct label which can affect the proposed method. To tackle this, the manuscript also proposes a denoising module that successfully eliminates the effects of noisy labels on the debiasing algorithm proposed. Experiments are performed on various synthetic and real world biased sets. 

Positive aspects of the manuscript includes:
1. The results for varying levels of "bias" as well as the success of the proposed "denoising" setup is remarkable for the datasets tested;
2. An interesting hypothesis about the differences between gradient magnitude and direction (as measured by its proximity to an "average" gradient direction for all samples) look different for underrepresented samples as compared to "regular" data sample.

There are also several major concerns, including:
1. Lack of motivation and analysis on the connections between per sample gradients and the majority/minority splits in more complex datasets;
2. The key assumption which motivates the proposed method, namely that minority samples have different gradient distributions than majority ones, deserves a more rigorous validation;
3. The "scalability" of the proposed method. One common theme across these datasets is that they can be "learned" (at least the biased version) with a much smaller amount of data than is present in the training set. Hence, a rejection sampling based method can work even when the minority set diminishes; 
4. Assumption about known bias. The proposed method assumes knowledge about which of the factors were biased so that a suitable "bias only" model can be trained by leveraging only the "bias". 

Post rebuttal, reviewers stayed with borderline ratings, and they have suggested further improvements: more details about Biased MNIST numbers (to address concerns about known bias), and ablation studies on real datasets (e.g. compare to results without denoising, or denoised using FINE) to fully justify the practical importance of proposed denoising module.
Overall the paper present the idea of caching and using stale information to update instead of sub sampling for speeding up graph convolution neural network. Reviewers liked the idea but also there were concerns about experimental comparisons.  In the rebuttal the authors did provide more evidence of comparison with other caching based and other relevant baselines. Overall the importance of scaling up GCNN and empirical results helped the paper cross the high bar.
The reviewers unanimously appreciated the clarity of the work as well as the framing of the proposed method. Congratulations.
This submission received four high quality reviews and there are a lot of meaningful discussions during the author response period. After the discussions, all four reviewers agreed that the submission can be strengthened in a number of ways, including more solid experimental results and a justification for the correctness of the ELBO.  The AC agrees. The authors are encouraged to revise the paper based on the reviews for the next venue.
All reviewers recommend rejection, and I m following this recommendation.
This submission proposes a simple way to improve the stability of training GPT 2: Increase the sequence length of examples over the course of training. It is shown that this simple heuristic can result in using larger learning rates, therefore significantly speeding up convergence. Reviewers agreed that this was a simple and effective approach, but shared various concerns about the paper:
  The paper focuses on GPT 2, while stability issues can arise in a much wider range of models. Additional experiments with other models (and ideally other codebases/training setups) would help verify that the proposed method is broadly applicable.
  Better analysis of why using the sequence length as the difficulty metric would be helpful. What other criteria would be possible? Why is sequence length the best?

I would suggest that the authors significantly expand the submission based on the above suggestions and resubmit.
While the reviewers appreciated the method s ability to replace transformer models and SMILES data augmentation their main concerns were with (a) the experimental section, and (b) the technical innovation over prior work, which updated drafts of the paper did not fully resolve. Specifically for (a) this work performs very similarly to prior work: for reaction outcome prediction the proposed method improves top 1/3/5 for USPTO_STEREO_mixed but is outperformed by prior work for top 1/5/10 for USPTO_460k_mixed; for retrosynthesis the model is outperformed for USPTO_full and only outperforms prior work that does not use templates/atom mapping/augmentation for top 1 on USPTO_50k. The authors argue that their method should be preferred because their method does not require templates, atom mapping, and data augmentation. The reviewers agree that template free and atom mapping free methods are more widely applicable. However, the benefits of being augmentation free is not convincingly stated by the authors who only state that their approach is beneficial by "simplifying data preprocessing and potentially saving training time." The authors should have empirically verified these claim by reporting training time, because it is not obvious that their model which requires pairwise shortest path lengths is actually faster to train.
For (b) the reviewers believed that the paper lacked technical novelty given recent work (e.g., NERF). The authors should more clearly distinguish this work from past work (e.g., graphical depictions and finer past work categorization may help with this).
Given the similar performance to prior work, the lack of evidence to support training time claims, and the limited technical novelty, I believe this work should be rejected at this time. Once these things are clarified this paper will be improved.
Most of the reviewers thought this paper has issues where it could be improved.  There was a range of concerns. Most importantly, several reviewers felt the novelty in the paper was unclear as well as the requirement for more details in the experimental evaluations.
This paper presents an interesting analysis of mixup, discussing when it works and when it fails. The theory is further illustrated with small but intuitive examples, which facilitates understanding the underlying phenomena and verifies correctness of the predictions made by the theory. The submission has received three reviews with high variance ranging from 3 to 8: mn55 favoring rejection while eGEK recommending accept. I read all the reviews and authors  response. Unfortunately, mn55 did not follow up to express how convinced they are with author s reply, but I do find the responses to mn55 very solid and convincing. In concordance with eGEK, I do find the provided analysis important and helpful, and the presentation of the theory through concrete examples very compelling.
This paper looks at stochastic and Markov potential games. Its different results, including the sample complexity ones, are overall interesting and relevant for the community.

This said, we had an intense discussion as several of the aforementioned results    actually, closely related results, not the exact same one   already appeared elsewhere (in a ArXiv preprint, that has been publicly submitted at a previous conference). We do believe that there is no ethical/plagiarism issue here, however, it remained the question of "paternity" of these results.

We have decided to give the paternity of the sample complexity to the first paper (the ArXiv preprint) that proved it. We can therefore only credit to this paper the improvements in the sample complexity results (as they are not exactly similar).

However, this had an impact on the reviewers (and also mine and the SAC one) evaluation of the paper, when some substantial parts of the paper are "discarded".

Nonetheless, we think that this paper has its merits, although it does not reach the ICLR bar in its current form. We strongly encourage the authors to work on a revised version   incorporating the different comments of the reviewers   and to resubmit it at a further venue.
This paper makes a contribution in the literature of cooperative multi agent reinforcement learning by proposing a decentralized and communication efficient training framework under a fully observable setting. The paper first defines the homogeneous or permutation invariant subclass of Markov games (homogeneous MG), where it is proved that sharing policy parameters does not loose optimality. The paper then proposes an actor critic algorithm for the homogeneous MG. The proposed approach is empirically supported. The reviewers had originally raised concerns or confusions, but no major concerns remain after discussion. 

Overall, the paper studies an interesting and practically relevant setting, providing new insights and solid basis for policy sharing that has been used in the literature without much understanding.
All reviewers found that the paper offers interesting contributions for multi agent RL and favour acceptance of the paper. The strengths of the paper are summarized below:
  Good algorithmic contribution
  Offers a new set of benchmark tasks for coordination in MARL settings
  Exhaustive experiments on complex tasks with a reasonable number of agents
  All the issues raised by the reviewers (missing references, missing discussion of limitations...) have been satisfactorliy addressed.

I therefore join the reviewers in the recommendation to accept the paper.
This paper experimentally shows that the block structure of similarities between layers typically appears for different models and such a structure is mainly induced by small set of dominant datapoints. Moreover, the dominant datapoints are not just noisy artifacts but represent some common image patterns such as background colors. The authors also found that the block structure can easily disappear by removing  the dominant datapoints, and the authors also proposed a method to suppress the block structure by regularizing PCs, Shake Shake regularization, and transfer learning.

This paper gives thorough experiments that clarify the mechanism of appearance of a block structure. However, its significance is a bit minor. Indeed, the block structure does not affect the generalization ability very much, and it can be removed without changing the predictive performance. I agree that investigating the behavior of the internal representation is of scientific interest as the authors pointed out, but on the other hand, its significance would not be convincing. Indeed, this concern was pointed out by several reviewers. Next, the main focus of this study is about the setting of large model with small data size. It is not clear whether it is universal across different model size relative to the dataset size. There is no theoretical investigation (for example, the block structure phenomenon could be explained by a high dimensional random matrix theory). 

In summary, this paper investigates a somehow interesting phenomenon but its significance is not convincing. Thus, it would be a bit below the threshold of the acceptance.
The paper presents a meta algorithm for learning a posterior inference algorithm for restricted probabilistic programs. While the reviews agree that this is a very interesting research direction, they also reveal that there are several questions still open. One reviewer points out that there learning to infer should take both the time for learning+inference and the generalization to other programs into account, i.e., what happens if the program is too different from the training set? Is benefit than vanishing? Moreover, as pointed out by another review, recursion as well as while loops are not yet supported. Also, the relation to IC needs some further clarification. These issues show that the paper is not yet ready for publication at ICLR. However we would like to encourage the authors to improve the work and submit it to one of the next AI venues.
This paper presents a gradient alignment approach to alleviate negative transfer and catastrophic forgetting for multitask and cross lingual learning. Experiments on many domains and datasets demonstrate the efficacy of the proposed approach.

All reviewers agree that the simplicity of the proposed method is a strength of the paper and the experiments are promising. They have suggestions to improve the experiments section, which I believe the authors have addressed in their rebuttal by adding GLUE, image classification, and statistical significance tests, among others. 

I recommend accepting this paper for ICLR.
To improve the data augmentation for an existing self supervised learning framework, the paper presents identity disentangled adversarial augmentation (IDAA) that utilizes a pretrained VAE and adversarial perturbation in the VAE latent space to generate identity preserving hard negative/positive samples. The proposed method has been clearly described and is of interest to the community. A wide variety of experiments have been done to illustrate the effectiveness of IDAA. Given the concerns on the validity and generalizability of the experimental results presented in the paper, the AC does not consider the paper of its current form to be ready for publication, as discussed below.

The AC appreciates the amount of effort that the authors have spent responding to the reviewers. However, thees very detailed rebuttals do not appear to be that effective in directly answering several key concerns of the reviewers.

1. While many experiments have been added to improve the paper, most of the settings are still considered by the reviewers to be unconvincing due to the use of a small number of epochs, unusual network backbones, and/or non standard datasets (e.g., downsampled 64*64 ImageNet with about 30% accuracy).

2. The comparison to the literature is considered to be insufficient. While the basic idea is still how to generate effective views or hard negative/positive views, the paper chooses to narrowly focus its comparison with CLAE that relies on adversarial perturbation in the pixel space, ignoring a careful comparison with many existing methods that boost the performance by mining hard negatives/positives and/or using a memory bank to accommodate a large number of negatives. 

3. Related to point 2, a main justification of IDAA is that it outperforms CLAE, which, however, is shown by the authors to be a weak baseline that barely improves an existing self supervised learning (SSL) framework in a variety of different settings. For example, the results in Table 1 show CLAE hurts SimCLR s performance. The weak performance of CLAE makes it even more important to include stronger baselines.

4. A reviewer also pointed out that the use of a pretrained VAE introduces additional model parameters and increases the computation cost, and hence a careful discussion on how to ensure a fair comparison with the baselines is desired. 

5. The presentation could be somewhat misleading in giving the impression that IDAA is a stand alone domain agnostic data augmentation technique. While it is totally fine that IDAA is a complementary data augmentation technique, the authors need to carefully revise the presentation to minimize the risk of giving the reader a wrong impression.  

6. The authors dismissed some of the negative reviews by arguing them to be “mostly incorrect, wrong, makes no sense”. These strong arguments, if not supported with clear evidence asked by the reviewers, may convince neither the reviewers nor the AC and lead to unnecessarily tedious lengthy discussions. From the AC s reading of the paper and reviews, it appears that there are several cases where a reviewer was asking the answer to Question A, but the authors were providing an answer to Question B and pushing the reviewer to accept that as a satisfactory answer to Question A. For example, while many experiments have been done on several different datasets, there is a legitimate concern on why these computing resource has not been spent on getting results on ImageNet with ResNet 50. When the experiments are only being done on ImageNet 100 or the downsampled full ImageNet with a small network, there could be clear concerns on whether the observed performance gains are only applicable to relatively small/low resolution datasets or small networks. For example, the VAE and/or the adversarial attack may not work that well on large scale/high resolution images and hence IDAA may break down when scaling it to a larger dataset/model. That type of concern can only be addressed if the authors have taken the time to follow a standard setting, such as ResNet50 on ImageNet, and make comparisons with a wide variety of baselines whose results on these standard settings are readily available.
This paper proposes an efficient training free NAS method, NASI, which exploits Neural Tangent Kernels (NTK)’s ability to estimate the performance of candidate architectures. Specifically, the authors provide a theoretical analysis showing that NAS can be realizable at initialization, and propose an efficient approximation of the trace norm of NTK that has a similar form to gradient flow, to alleviate the prohibitive cost of computing NTK. Since the method is training free, NASI is also label  and data agnostic. The experimental validation shows that NASI either outperforms or performs comparably to existing training based and training free NAS methods, while being significantly more efficient.

The below is the summary of pros and cons of the paper, after :

Pros
  The idea of using NTK to predict the performance of candidate neural architectures is both novel and promising, and the proposed analysis and efficient approximation are non trivial.
  The paper provides sufficient theoretical proof of its claims, including the assumptions made.
  The method is highly efficient in terms of search cost, and the searched architectures obtain good performance on benchmark datasets.
  The method is data/label free and thus allows transfer architectures across tasks.
  The paper is well written.

Cons
  There is no result on ImageNet obtained by directly applying NASI on it. 

The initial reviews were split, due to other concerns regarding whether the proposed method finds good architectures, missing comparison against certain training free baselines, and some unclear descriptions. However, they were addressed away by the authors during the rebuttal period which led to a consensus to accept the paper. 

In sum, this is a strong paper that proposes a novel idea for training free NAS, and the proposed method seems to be both effective, efficient, and generalizes well across tasks. One remaining concern is the computational cost of running the method on larger datasets, such as ImageNet, and I suggest the authors report the results and the running time in the final paper.

Another suggestion is to include discussion of, or comparison to other efficient NAS methods based on meta learning, such as MetaD2A [Lee et al. 21], which is not training free but is more efficient than the proposed NASI. 

[Lee et al. 21] Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets, ICLR 2021
The paper seeks to improve straight through estimators by combining them with the ideas for correcting the step direction to be closer to a natural gradient.
 
While some (modest) improvements are demonstrated experimentally, the paper critically lacks technical correctness and has quite some gaps when trying to derive the algorithm from the natural gradient and Rao Cramer bound. See public comments by reviewers and AC. The algorithm ends up to be a mirror descent with a mirror map, which is cheap to compute but not particularly well motivated. Moreover application of mirror descent to the activations (unlike the weights) is not well justified. The paper is rather unclear and hard to read also language wise. Please proofread _before_ submitting.
The authors argue in favor of task aware continued pretraining and demonstrate through experiments that using objectives based on the end task during continued pretraining help in improving downstream performance. 

The reviewers generally appreciated the motivation, the formal treatment of the topic and the thoroughness in the experiments. There were some concerns about (i) positioning of the paper (pretraining as opposed to continued pre training) (ii) thorough comparison with other MTL frameworks (iii) evaluating on more datasets (iv) cost of continued pretraining for each task v) the benefit of META TARTAN over MT TARTAN only in specific  settings and (vi) lack of surprise/novelty in the results. 

IMO, the authors have adequately addressed ALL the above concerns raised by the reviewers. Further, despite the above concerns, all reviewers agree that the problem is well motivated and of interest to the community and most aspects of this work are thorough. The findings will be useful and may spawn other work in this area.
Ultimately somewhat below the threshold based on the scores. The reviewers raise issues of the overall contribution, as well as issues with the design/structure of the model/paper and issues with the experiments. While there are some positive aspects, collectively the issues put the paper below the bar for acceptance.
The reviewers consider the authors  approach to pruning of convolutional networks reasonable; but neither sufficiently novel nor sufficiently well explored for inclusion in the conference.  In particular, the reviewers would like to see a more explicit discussion of the effect on training time of the authors  method, and more discussion and comparison against previous probabilistic pruning methods.
This paper proposes a novel method for training neural rough differential equations, a recent model for processing very long time series data. The method involves a lower dimensional embedding of the log signature, which is obtained via pretrained autoencoder to reduce overhead. The results show significant and consistent improvements over previous methods on long time series data.

Overall, the reviewers and I all agree that this paper offers a novel and impactful contribution leading to significant improvements over previous state of the art methods for training neural rough differential equations. I recommend acceptance.
In this paper, the authors studied reinforcement learning applications that have access to both online and offline data (with limited online interaction though).  In order to handle the mixture of online and offline data efficiently, the authors proposed a new paradigm called adaptive Q learning, which treats offline and online data differently (as reflected by whether pessimism is implemented or not). The effectiveness of the proposed paradigm has been tested empirically. The reviewers have raised concerns about the sufficiency and significance of the experiments conducted in the paper, and pointed out that the proposed algorithmic idea is a somewhat incremental change over existing ones. The changes the authors promised to make will make the paper stronger.
This paper investigates the scaling laws of neural networks with respect to the number of training samples $D$ and parameters $P$ for some estimators in two regimes: the variance limited regime and the resolution limited regime. The theoretical results are supported by some numerical experiments.

Unfortunately, the paper has several critical issues, in particular, in its novelty and technical correctness. 
1. The theoretical analyses lack much of their rigor. The assumptions and problem setups are not precisely introduced. Accordingly, the statement of each theorem is presented in an inaccurate way. Moreover, some theoretical consequences contain technical flaws (e.g., $1/P$ should be replaced by $1/\sqrt{P}$ without an appropriate assumption on the loss function such as strong convexity and smoothness).  
2. Many of the presented results are already known in the literatures. It is unfortunate that the authors did no cite relevant existing literatures and did not discuss its novelty compared with the existing work. 

For those reasons, this paper lacks its novelty and the quality of the paper is not sufficient to be accepted.  
I recommend the authors to thoroughly survey the literature of the statistical learning theory from classic nonparametric regression analyses to recent advances on overparameterization.
The paper proposes NAFS (Node Adaptive Feature Smoothing), which constructs node representations by only using smoothing without parameter learning.  The authors first provide a formulation for the smoothing operator. They then define over smoothing distance to assess how much a node is close to the stationary state. Finally, they use the over smoothing distance to calculate a smoothing weight for each node. Experiments are conducted to verify the efficacy.

Strength
* The paper tackles the problem of over smoothing, which is a well known issue in GNN.
* The solution appears to be effective. 
* The paper is generally clearly written.

Weakness
* The novelty and significance of the work might not be enough. Aspects of the contributions exist in prior work.

 

Additional experiments have been conducted during the rebuttal. The reviewers appreciate the efforts.

After rebuttal：

Reviewer SHxg increased the score accordingly.

Reviewer w2Qg says “Given the concerns proposed by the other reviewers, I adjusted my score.”

Reviewer YM4P says “I read the rebuttal and slightly increased my score.”
This paper is a solid contribution to researchers in this field, as it provides a new idea for the basic problem of determining the direction of causality between two variables, using the functional causal model as a dynamical system and optimal transport.
This paper presents a framework to test the accuracy and robustness of different machine learning algorithms in classifying the COVID 19 spike sequences. After reading the paper and taking into consideration the reviewing process, here are my comments:
  The work is aligned to the efforts on understanding the COVID 19 pandemic.
  Many concepts are not novel.
  Sequences errors are not modeled in a realistic way.
  The benchmark is very limited and nonlinear machine learning approaches are presented. 
  Many typos are presented.
From the above, the paper is not suitable for aacceptance in ICLR.
This paper introduces sparse modeling inspired regularizations to improve deep neural network based image generators. Experimental results on both (low resolution) image synthesis and deep image prior based inverse problems are used to validate the proposed method.
The majority of the reviewers were against the acceptance of the paper. As summarized by Reviewer tsoA: "There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes." The AC agrees with that summarization and recommends rejection.
This submission proposes a few small changes to the (PreLN) Transformer architecture that enable training with higher learning rates (and therefore can result in faster convergence). The changes include the addition of two layer norm operations as well as a learnable head scaling operation in multi headed attention. The proposed operations add only a small computational overhead and should be simple to implement. Experiments are conducted on language modeling and masked language modeling, with improved results demonstrated at various scales and according to various evaluation procedures. The paper also includes a good amount of ablation study as well as some analysis. Reviews on the paper were mixed, and a great deal of changes were made to the paper during the rebuttal period. To summarize the concerns and recommendations, reviewers requested
  better connection between the proposed changes and the purported issue (gradient scale mismatch between early/late layers)
  better analysis of why gradient scale mismatch is a major issue and investigation of where it comes from
  better comparison to existing techniques that allow for higher learning rate training of Transformers
  additional experiments on different model types and ideally different codebases/implementations

I think overall this is a solid submission, since it proposes a simple change that is reasonably likely to be helpful (or at least not harmful). However, I think that there are enough concerns with the current draft and there were enough changes made during rebuttal that this paper should be resubmitted to a future conference. I would suggest the authors take the final updated form from this round, add additional motivation/analysis/experiments, and resubmit, and I suspect a positive outcome.
This paper proposes to create an explanation space to describe the relationships between input data and prototypes (and also between the prototypes themselves). It constructs such a space suing VAEs and conducts experiments to validate the effectiveness and interpretability of the method.

Strengths:
  The proposed method is interesting and intuitive

Weakness:
  Novelty of the idea is limited
  Missing experiment comparison with some important previous work
  Some claims are not well supported by the empirical results
Four reviewers acknowledged the author s response and did not change their largely negative scores.  The one enthusiastic reviewer did not respond to the more negative reviewers and has not worked in the theorem proving area. The main problem with the paper seems to be that the reviewers were not convinced by the empirical results.  They felt that results should have been presented on more widely used benchmark datasets.
The paper provides the theoretical justification for the "label trick" (using labels in graph based semisupervised learning tasks). The authors performed a thorough evaluation of their analysis, which constitutes an experimental contribution. The authors provided a rebuttal that the AC finds to have reasonably addressed the reviewers  concerns. We recommend acceptance.
This paper deals with a problem of significant practical relevance: memory efficient neural networks. The authors propose some pruning methods for binary networks. However, several weaknesses were identified by the reviewers (novelty, lack of extensive experiments, problems with the presentation of the paper), and several valid points of concern were raised. These points of criticism were not adequately addressed, hence the paper in its current form cannot be recommended for publication.
The paper studies the effect of manifold geometry on the complexity of the function implemented by a random ReLU network, as measured through its decomposition into linear / affine regions. In particular, it provides bounds on a surrogate for the number of such regions and the distance of a fixed point to the boundary of its region. These bounds follow from an extension of an argument of Hanin and Rolnick for Euclidean space. The bounds hold at random initialization, and are complemented with experiments in which they remain valid through training. 

Initial reviews of the paper were mixed. All reviewers recognized the extension to structured / non euclidean data as an important direction, and the results as extending the argument of Hanin and Rolnick to this setting. At the same time, there were questions about the novelty, clarity, and implications of the paper. One issue concerns the implications of the results and the amount of insight they offer into the data complexity   network complexity relationship. In particular, the paper would be stronger with a more explicit accounting for the constant C_{M,\kappa} and intuitive explanations of how manifold properties such as curvature and reach affect the number of linear regions. There were also concerns regarding the statement and proof of Theorem 3, the initial version of which only held for small \epsilon. The review also raised other smaller issues regarding the paper s clarity and implications. After considering the authors feedback and revisions, reviewers retained their mixed evaluation of the paper. This appears to be a promising direction, but a paper that could benefit from further refinement.
This paper receives positive reviews. The authors provide additional results and justifications during the rebuttal phase. All reviewers find this paper interesting and the contributions are sufficient for this conference. The area chair agrees with the reviewers and recommends it be accepted for presentation.
This work presents the Neural Simulated Annealing (NSA) approach as a heuristic for general combinatorial optimization problems. After revising the paper and reading the comments from the reviewers, here are the general comments: 

  In general, the paper is clear enough. The contributions are stated in a proper way.
  The novelty is rather limited, but the key idea of using neural networks in SA, and training it with RL, has merit.
  This approach has merit but the novelty is very limited.
  The NSA improves the vanilla SA, but the benchmark reveals that NSA is not enough competitive with other state of the art methods. 
  The benchmark does not reveal enough information about the NSA against the SOTA methods.
  The work needs technical improvements and validation is required before accepting the work.
This paper takes inspiration from Global Workspace Theory to propose a modification for attention based network architectures. This is exemplified both in transformer models and in recurrent models (RIMs). The key idea is to replace the quadratic, pairwise communication between "specialist" units (which in transformers corresponding to the positions) by a higher order communication model which consists in a competitive, sparse writing step into a shared workspace, followed by a reading step where information is broadcasted from the global workspace to all specialists. The competitive writing step establishes a limited bandwidth channel for this communication which encourages specialization.

The reviewers agree that this is an interesting and very well written paper which unifies several existing ideas. The main contribution of this paper is in establishing a connection to GWT which may inspire future research to keep developing these ideas. The experiments on relatively small tasks (but challenging ones) provide a good proof of concept. Some concerns pointed out by some of the reviewers include a certain overstatement of the capabilities of the proposed model, as well as lack of experiments that scale up the model to larger and unstructured datasets. The authors replied with additional experiments included in the appendix, which in my opinion address these concerns convincingly. 

Overall, this is a strong paper and I recommend acceptance. I encourage the authors to take into account the reviewer s suggestions in the final version. I also think that the connection to related work could be improved, as there is several related works [1, 2, 3] which asks/investigates similar questions to this paper and should probably be acknowledged:
  The "shared global workspace" of this paper (Transformer + SW) is reminiscent of the Star Transformer [1], as well as other more recent works which use special units (e.g. CLS tokens) to encode "global" representations. While that work does not include the competitive component (the "bottleneck"), I think it should be acknowledged.
  Variants of transformers with competition among specialists via sparsity have also been proposed, e.g. adaptively sparse transformers [2]. That framework is an alternative to top k softmax used in this paper.
  Empirical studies which analyze the redundancy among specialists (in this case attention heads) and propose strategies to prune them have also been made by [3]. 

[1] https://arxiv.org/abs/1902.09113
[2] https://arxiv.org/abs/1909.00015
[3] https://arxiv.org/abs/1905.09418 

Minor point: "Hence unlike pairwise interaction, messages passed among neural modules in the shared workspace setting also include HO interaction terms"   I believe higher order interaction happens too every two layers with pairwise interaction. Perhaps this should be clarified.
The reviewers were fairly consistent in agreeing that this is a reasonable paper with an interesting idea.  However, the use case is fairly narrow, as the main benefit is less intermediate storage (and only significant for very rectangular matrices) but compared to alternatives it require many passes over the data (usually 5 or so). So it s a narrow use case and many of the comparisons are not apples to apples since the accuracy, time, space complexity and number of passes differ from algorithm to algorithm.

So while acknowledging the potential benefits of the method, there are downsides too, and thus a clear presentation is very essential. The reviewers mention that presentation (listing the algorithm, clear experiments) could be improved.

On my own reading, I noted that the choice of SketchySVD as the dominant baseline is misleading. SketchySVD is for streaming data (more restrictive than single pass) so this is an unfair comparison. The appendix does a better job of including other baselines (block Lanczos), though it mischaracterizes them (it says "BlockLanczos requires persistence presence of the data matrix X in memory", but this is not true, the method could easily be implemented in a matrix free fashion). Another method to compare with is the single pass algorithm randSVD in Yu et al., who show how to implement one of the Halko et al. 2011 2 pass methods in just one pass.  Other reviewers mention baseline algorithm issues too.  I do acknowledge the improved accuracy of your method over all these baselines for some matrices, in terms of the Frobenius norm (or tail error); however, I m not sure the differences in spectral norm are are great, and see Remark 2.1 in Martinsson and Tropp  20 for arguments about why Frobenius norm guarantees are often not as desirable as spectral norm guarantees.

Another issue is related to the left vs right singular vectors. A reviewer noted: "It is not fair to compare RangeNet with SketchSVD, RangeNet just produces the right singular vectors while SketchSVD produces both left and right singular vectors." The authors respond "Range Net computes both left and right singular vectors but does not consume main memory to store left singular vectors at run time". However, if we allow another pass over the matrix to find the left singular vectors, this post processing can be applied to *any* technique that approximates the singular values and right singular vectors, hence PCA methods are applicable, including deterministic methods like the "Frequent Directions" method (Ghashami et al.  16).  

In summary, this method is high accuracy and low memory, yet it also has downsides compared to other methods, and the paper could use some improvement.  I don t think the paper is ready at this time for acceptance, but given the advantages of the method, I encourage the authors to make changes and resubmit an improved version to ICLR next year or other similar venue.


References:

Yu, Gu, Li, Liu, Li, "Single Pass PCA of Large High Dimensional Data". IJCAI  17, https://doi.org/10.24963/ijcai.2017/468

Ghashami, Liberty, Phillips, Woodruff, "Frequent directions: Simple and deterministic matrix sketching". SIAM Journal on Computing. 2016;45(5):1762 92.

Martinsson, Tropp. "Randomized numerical linear algebra: Foundations and algorithms". Acta Numerica. 2020 May;29:403 572.
In this paper they adapt unsupervised contrastive learning to the problem of representation learning for proteins from 3D structure, using sub structure sampling for the data transformation. The reviewers have concerns that the application tasks used for evaluation are not particularly impactful tasks, and that additionally, they are likely to not require protein representations that require more nuanced information. There are also concerns about the clarity of the manuscript, and novelty of the technical approach.
This paper proposes a new method for learning symmetric representations for equivariant world models. All reviewers recognized the interesting results in the paper. The reviewers have raised some concerns, which were not addressed well yet after the rebuttal. For example, Reviewers LG1G and Uu3z mentioned about the limitation of using the group for a task and the generality of the approach. Reviewer 5ro3 mentioned about the lack of novelty. Though they gave 6, they were quite neutral about the paper acceptance. Eventually, after a second round of deiscussions, we had to make this difficult decision: The current form of this paper is not ready for publications.
All reviewers vote for rejecting this paper. The main points of criticism shared by the reviewers are missing novelty and missing/unclear significance of the contribution. There was no rebuttal, so this is a clear reject.
All of the reviewers believe the paper should not be accepted, and I concur with their recommendation for the reasons they mention.

Four of the reviewers (vEBH, idrP, KoFV, 5k4c) believe the technique proposed in this paper is not particularly novel. Rather, the novelty is that it is being used on a BERT model rather than the computer vision models that are typically the starting point for pruning work. They also argue that the paper was not particularly thorough in its comparison to other pruning techniques (specifically dynamic sparsity techniques), which is essential for pruning work given how crowded and noisy the space is. Finally, they rightfully note that the paper does not look at the real world speedups attainable on conventional hardware (GPUs and TPUs), the latter of which has no support for sparsity and the former of which (NVIDIA Ampere) has limited support for specific kinds of sparsity and especially limited support for sparse training.

The reviewers also raised several more specific methodological issues with evaluation (e.g., using the MLM loss rather than fine tuning as a basis for evaluation), but the above concerns alone were enough to convince me that the paper does not merit acceptance at this time.
This paper received 4 quality reviews, with the final rating of 8 by 2 reviewers, and 6 by the other 2 reviewers. All reviews recognize the contributions of this work, especially its superior performance. The AC concurs with these contributions and recommends acceptance.
The paper introduces a compression method for distributed Split Learning for better communication efficiency, by compressing the intermediate output between client and server model by vector quantization. Convergence analysis and experimental results are provided. 
Unfortunately consensus among the reviewers remained that it remains slightly below the bar after the discussion phase. Main remaining concerns were the variety of baselines and benefits from split learning setup in experiments, compared to other FL approaches, quantization approaches, architecture splits. Reviewers also missed a discussion of latency requirements of model parallel training in FL as opposed to data parallel which allows less frequent communication compared to here (e.g. discussing the split layer size vs latency trade off, here of quantized intermediate layers compared to regular FL). The newly added Figure 6 does not specify or vary the number of local steps (or batch size) in FedAvg.
We hope the detailed feedback helps to strengthen the paper for a future occasion.
The paper focuses on zero shot capability of BERT like models. The key contribution boils down to a novel prompting techniques that effectively ensembles predictions made for [MASK] tokens inserted at different places.

Reviewers B5Rv and 9k3X voted for rejecting mostly on the grounds that the contribution is not significant enough for ICLR. In particular, there are already existing works show that null prompting works, and other works that suggest that using multiple prompts works. While these insights have not been combined before, it is to some extend incrementally.

On the positive side, the multi null prompting strategy is a genuinely useful tool. I think it is likely to find applications in different NLP applications as an effective way to generate ensemble predictions. The paper has also many carefully carried out experiments that will likely help guide future efforts in designing effective prompting strategies.

On the whole, I am recommending rejection. I know this is a disappointing result. Thank you for your submission, and I hope the reviews will help improve your paper.
The reviews are split. The most significant concern seems to be the narrow focus of the paper: insensitivity of a very specific architecture, ViT to some patch based transformations of the image. The paper aims to "understand and improve" the behavior of ViTs in this respect, but as the reviewers point out, the understanding (what exactly is the mechanism for this insensitivity) is lacking. Furthermore, there is a good reason to believe that other transformer architectures might not have a similar behavior. Ultimately both the lack of depth and the lack of breadth of the investigation suggest that the impact may be limited. I think this is not a good fit for ICLR.
Overall the paper makes good contributions to the area of robust deep reinforcement learning. The presentation needs to be improved to avoid any confusions. Please take all the reviews into account and revise the paper accordingly.
This paper tackles the problem of exploration using intrinsic rewards in RL in states that have never been encountered before. The authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement, which estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples. The intrinsic reward resulting from the so called DISDAIN (discriminator disagreement intrinsic reward) exploration bonus is more tailored to the true objective compared to pseudocount based methods.

Reviewers agree that the paper is well motivated and well written, that the proposed DISDAIN exploration method is simple and practical, and that experiments are convincing. Experiments on continuous control tasks such as MuJoCo locomotion environments could have strengthened the paper further.
This work extends the successor feature framework by focusing on the question of which policies should be learned in order to get the best generalization performance. The reviewers all agree that the question being addressed is interesting and important. One concern raised by two of the reviewers is that the work is rather incremental, providing a relatively small extension from the work of Barreto et al. Nevertheless, the authors have provided a convincing rebuttal, resulting in an increase in score of two of the reviewers. Hence, I recommend acceptance. I do want to ask the authors to carefully read the post rebuttal point mentioned by reviewer 3QcK about clarifying the unsupervised RL setting.
The authors propose a method—"Proto Trex"—that incorporates prototype networks into text classification architectures to facilitate model explanations via presentation of similar training examples. There was agreement that the direction here is promising and the work contains some nice ideas and a good initial set of evaluations. However, the presentation can be improved substantially to better situate the contribution with respect to related work (and clarify the specific contributions on offer here), and to clarify the key technical details of the proposed approach.
This work takes ReQuest, an approach for safe deep reinforcement learning utilizing human feedback, and studies it s feasibility in pixel based 3D environments (previously it was only shown to work in simple 2D environments). In order to apply ReQuest in this much more challenging settings, this novel instantiation of ReQuest learns a pixel based dynamics model from a lot of human demonstration data, and a different (as compared to the "base" ReQuest) reward sketching approach to infer the reward function from human feedback. 

**Strengths**
globally a well motivated and well written/structured manuscript
Adresses an important problem, and shows promising results

**Weaknesses**
on the more detailed level, there are some clarity concerns (even with the lengthy appendix)
evaluation was missing some more relevant comparison (partially fixed after rebuttal/revision)
lack of technical novelty, and lack of in depth analysis of results
motivation of algorithmic choices : why did you choose the reward sketching approach that you chose? How is it different, and does it improve performance?

**Rebuttal**
The authors addressed most questions/things that were unclear and updated the paper to include an additional baseline.  

Additional baseline: First, it s great that you added this additional baseline! Yet, to me it s unclear what that additional baseline really represents (Request + sparse rewards). The original ReQuest paper also learns reward from human feedback, is that what you did for this paper? If yes then what does the sparse reward mean? Why does this version of request perform worse than your proposed version?

**Summary**
I agree with the reviewers and authors that this is a promising direction. However, in it s current form this manuscript is not ready yet for publication. My  concern are centered around motivation of algorithmic choices: The reward sketching part (while not novel in itself) is a novel component of the ReQuest pipeline, but you do not evaluate what it adds, and neither do you motivate that choice. Furthermore, the additional baseline is not clearly described, it s unclear how it s different for your proposed approach and why we see the performance improvement of your ReQuest version vs the baseline ReQuest version.
The paper proposes a new approach called pessimistic model selection (PMS)  for model selection in offline RL and tests it in 6 different environments. Under certain assumptions this allows theoretical results that the best model is recovered with high probability.

Several points were raised by the reviewers and maintained after the rebuttal:
  Theoretical results were considered weak as they only hold asymptotically.
  Experimental results limited (potentially different regret scales, no sufficient comparison to other baselines).
  Exposition of the paper that needs to be improved.

Given the strong consensus among the reviewer I recommend rejecting this paper.
This paper studies deep non linear infinite width neural networks that go beyond the NTK and learn features. This paper extends the prior result on shallow neural networks to deep neural networks and empirically evaluates the deep inf wide nn. The reviewers find the contributions in the paper valuable. The meta reviewer agrees and thus recommends acceptance.
The paper introduces a simple yet effective technique for supervised pre training based on kNN lookup from a MoCo memory queue . Initially, the reviewers raised concerns about limited novelty with respect to neighborhood component analysis, baseline results lower than the original papers, and several other questions such as how many positive samples fall in and out of kNN. The author response was strong, adequately addressing the reviewer’s comments with additional experiments and clarifications. After the discussion period, three reviewers recommended borderline acceptance. One reviewer maintained score 5, suggesting a more exhaustive search for hyper parameters, but indicated he/she was on the fence and would be ok if the paper is accepted. The AC considers the response of the authors regarding hyper parameter search (and the small gap from other reported results) is reasonable, and agrees with the majority that the paper passes the acceptance bar of ICLR.
*Summary:* Investigate the NTK of PNNs and enhanced bias towards higher frequencies. 

*Strengths:* 
  Spectral bias is a contemporary topic. 
  Some reviewers found the paper well written.

*Weaknesses:* 
  Restricted setting (two layers / no bias / infinite width), particularly in view of the objective to provide architecture design guidance. Restricted experiments (Introduction indicates learning spherical harmonics). 
  Sparse discussion of related works, particularly on spectral bias. 

*Discussion:* 

During the discussion period authors made efforts to address some of the concerns of the reviewers. A late new experiment prompted KnZp to raise score. TQnp found the paper good but also expected a more profound theorem addressing broader PNN families given the existing work. They found that experiments and discussion of prior work could be improved. The authors added discussion of prior works and provided an explanation for their choices, but left extensions and further analysis for future work. nFMY expressed concerns about applicability of the analysis and evidence in experiments. Author responses addresses this in part. cEcf points out that the main theoretical contributions have straight forward proofs based on previous works and asks about extensions. Authors agree that the paper does not introduce novel techniques and that extending the analysis is an important direction, but leave this for future work. FuRi finds the paper provides an interesting viewpoint and raised score from 3 to 5 following the discussion (improving presentation, rigor, clarity), but considers that the paper has several drawbacks (oversimplification, lack of technical novelty) that need to be addressed. 

*Conclusion:* 

One reviewer found this work marginally below the acceptance threshold, three marginally above, and one good. I find that the paper considers an interesting problem and makes some interesting observations and some valuable advances. I appreciate the authors’ efforts during the reviewing period. Hence I am recommending accept. At the same time, I find that, clarity, technical and experimental contributions still can be improved and encourage the authors to carefully consider the reviewers comments when preparing the final version of the paper.
The paper received two accepts and 1 marginally above acceptance recommendations. The authors provided satisfactory answers, mostly on clarifying the unsupervised learning methodology, in conjunction with the MAA recommendation. I recommend the paper be accepted as poster.
Overall, this paper receives negative reviews due to limited technical novelty and contributions. The reviewers discuss extensively on the merits of this work after the rebuttal phase. However, the authors  rebuttal does not address all the raised concerns. As such, the area chair agrees with the reviewers and does not recommend it be accepted at this conference
The paper observes that the number of redundant parameters is a function of the training procedure and proposes a training strategy that encourages all parameters in the model to be trained sufficiently and become useful. The method adaptively adjusts the learning rate for each individual parameter according to its sensitivity (a proxy for the parameter s contribution to the model performance). The approach encourages the use of under fitted parameters while preventing overfitting in the well fitted ones. Experimental results are presented covering a wide range of tasks and in combination with several optimisers, showing improvements in model generalization.

The paper is very well written and easy to follow (as mentioned by Reviewers NSqH, 4pzE and sSHP). 

The authors provided a strong rebuttal including new experiments, like training using CNN based architectures (as requested by Reviewers sSHP and MzBV). Reviewer sSHP requested these results to be reported with STD, the AC encourages the authors to do so for the camera ready.

Reviewer MzBV points out that the paper could be improved by giving a motivation of the update rule and proving convergence. However, still recommends accepting the paper due to the novelty in the idea of not taking redundant parameters as something inevitable and devising an effective strategy to improve it. This idea was also appreciated by the other reviewers. While the AC agrees that adding these points would improve the work, it takes as valid the point made by the authors. Namely, that the intuition behind the update rule is quite clear, and many other reasonable variants were ablated (in Appendix A.4.4). Furthermore, the empirical evidence shows that the method improves generalization.

Reviewer NSqH points out that while SAGE improves the model’s generalization performance for lightly compressed models, its performance becomes more susceptible to pruning when the model is compressed heavier. While the authors responded with good points, the AC encourages them to follow the reviewer’s advice and incorporate further experiments studying this issue (e.g. other datasets).

In sum, the paper proposes a simple and effective method that is able to improve generalization of large scale models. All four reviewers recommend accepting the paper. The AC agrees and encourages the authors to incorporate the requests mentioned above.
The reviewers agree that the paper is addressing an interesting problem. However, the authors analyze the effect of heterophily on GNN for node classification. The authors simplify the analysis by removing the nonlinearity in the GNN model and derive some theoretical results. However, the analysis is very specific to the simplified version of GNN, and the link to later proposed solution is also weak. Furthermore, a more significant improvement in experiments will also make the paper more convincing.
Heterophily is known to degrade the performance of graph neural networks. This paper explores whether, for graph convolutional networks (GCNs), this is a general phenomenon, or if there are some circumstances under which a GCN can still perform well in a heterophilous setting. This paper characterizes one such setting under a contextual stochastic block model (CSBM) distribution with two classes (generalized in the appendix to multiple classes). The main takeaway is that there are indeed scenarios where a GCN can be expected to perform well, even under heterophilic neighborhoods.

There are limitations, and the reviewers have been fairly thorough in pointing these out: the analysis is specific to GCNs under CSBM, and there are a number of assumptions on the node label/feature/neighborhood distributions. The non linear operations in the GCN have also been dropped. Even still, the reviewers were generally satisfied that the experiments backed up the claims in this specific scenario.

There is still quite a bit more to do in order to make this a more general result. Essentially, this paper shows that heterophily is not always a problem. One reviewer has stated that it is not always considered a problem anyway, but at least this paper outlines a specific scenario in which this is theoretically true. However, there is still a large space of “bad” heterophily, and this paper leaves open what these are, and how to deal with them. It is also possible that there are other “good” scenarios as well that are unexplored.

Still, in the narrow scope under which the analysis lies, the paper is clear and accomplishes what it sets out to do. I would encourage the authors to ensure that the paper incorporates the suggestions of the reviewers, particularly with regard to scope, to ensure that the paper is properly grounded in its claims.

All reviewers leaned towards the side of acceptance, except one who did not engage in post review discussion. After reading over their review, and the subsequent response, I am satisfied that their concerns have been adequately addressed.
The authors propose a graph multi domain splitting framework, called GMDS, to detect anomalies in datasets with temporal information. The reviewers agree that the paper studies an important and interesting problem but they think that the paper should be improved significantly before being accepted.

In particular, the reviewers feel that the authors should provide more technical details and insights on the design of the solution proposed and the proposed method should be compared with other(even simple) baselines for the same problem.
This paper has conducted extensive experiments to examine the scaling and transferring laws of LMs for machine translation and has concluded several interesting findings which could be inspiring to the future work.

The main concerns from reviewers are that the novelty of this paper is not enough. In addition, the experiments are not well designed and the clarity of this paper can be further improved. We hope the reviews can help authors improve their paper.
The paper provides an algorithmic framework to accelerate RL through Behavioral Priors, while having some notion of safety incorporated. The reviewers are divided about this paper:

On the positive side, some of the reviewers consider the problem important, and the experimental results reasonable and promising.

On the negative side, reviewers raised issues such as
1) The paper is on a heuristic side.
2) No formal guarantee on the safety is provided.
3) The paper is not as self contained as it should be, as it relies much on Singh et al. (2021).
4) The algorithm requires access to unsafe offline data.

I do not give the same weights to all these concerns. For example, even though (d) is an issue in some applications, it is alright for others. What concern me most are (1) and (2). 

A method for safety that is only evaluated empirically and does not have any formal guarantee cannot be used for safety critical tasks. I realize that some other published papers may have the same issue. But given that this is a real concern, and that two out of four reviewers believe that the paper should not be accepted, unfortunately I cannot recommend acceptance of this paper, especially given the competitiveness of this conference.

P.S: I also noticed that in the proof of Proposition 3.1, an expectation term $E[p_\phi(a|s,c)]$ in Eq. (9) is replaced by a $\log p_\phi(a|s,c)$. This requires more justifications.
Thank you for your submission to ICLR.  The reviewers ultimately have mixed opinions on this paper, but reading in a bit more depth I don t feel that the critical comments raised by the sole negative reviewer really raise valid points.  Specifically, the fact that this reviewer directly asks e.g. for comparisons to Levine and Feiz 2019, when the paper (before its revisions) contains an entire section devoted to exactly this comparison, strikes me as not sufficient for a thorough review.

However, while I m thus going to recommend the paper for acceptance (it does present a notable, if somewhat minor, advance upon the state of the art in randomized smoothing), I also feel the paper is generally rather borderline for more straightforward reasons.  Specifically, given the _very_ narrow focus of the proposed improvements (improvements to the bounds of randomized smoothing, for L0 perturbations, for Top k accuracy), I ultimately don t think the paper presents that significant an advance in the field.  The paper could go other way, thought definitely not doing so due to the issues that the sole critical reviewer takes.
This paper shows gradient flow of ReLU activated implicit networks converges to a global minimum at a linear rate for the square loss when the implicit neural network is over parameterized. While the analyses follow the existing NTK type analyses and there are disagreements among reviewers on the novelty of this paper, the meta reviewer values new theoretical results on new, emerging settings (implicit neural networks), and thus decides to recommend acceptance
The authors propose a simple and effective technique for task specific pruning of transformer models that identifies which model components to prune by minimizing validation loss. Weaknesses of the paper include (1) related work reads more like a list and doesn’t compare and contrast the proposed approach with related work, (2) authors don’t compare to other structured pruning methods (that use different objectives) (3) lack of novelty — main difference with existing work is using validation loss to optimize and (4) one reviewer was unconvinced that the results should be possible given the approach. I share these concerns, and, in particular, I think they might be related. Given that the models are pruned using the development set (essentially equivalent to training on the development set), it seems infeasible that this approach could have been developed without looking at the testing data, and I’m concerned that this explains the unprecedentedly high accuracy compared to previous pruning approaches. At the very least, comparing to a baseline that trains on development data would be prudent in order to understand the result.
The paper discusses new RL algorithms for solving large. TSP instances. The algorithm is novel and the problem is important however certain technical questions regarding the soundness of the algorithm were raised. Furthermore, it seems that despite much larger computational time, the algorithm provides only very moderate gains over previous baselines. Finally, it is not clear how the proposed methods (e.g. equivariance) can be applied outside of the TSP problem scope. Thus the concern is the limited impact of the method on the field.

The authors addressed some of the concerns of the reviewers in the rebuttal however it is still not clear:

(1) how the presented mechanism can be applied for other combinatorial problems beyond TSP and therefore how useful it can be for the machine learning community,

(2) how novel the paper is (the use of equivariance is as direct as in the regular graph neural network setup).

Furthermore, the experiments show that the deep learning approach to TSP is still not competitive with standard non machine baselines. Thus it is not clear whether the proposed algorithm is a right approach to solve this problem, even though it beats other deep learning techniques. The paper is very well written though and the presented method is definitely of value to the research community working on the TSP. Therefore it seems that at this point the paper is more suited for one of the mathematical journals on combinatorics and graph theory.
Thank you for your submission to ICLR.  The reviewers were split on this paper, with more favoring acceptance but with relatively low confidence.  After reading through the paper and reviews, I tend to agree slightly more with the more critical comments.  The paper is very much on the borderline, but ultimately 1) the rather incremental nature of the work compared to [Bhagoji, 2021], and 2) the rather small scale evaluations in the current version, which the field has largely moved on from as they often give overly optimistic impressions of certified robustness.  Ultimately a lot of the extensions (which at this point are fairly standard in most methods for deep network verification), seem like they should really be taken into account in the current paper.  For these reasons I lean slightly towards not accepting the paper in its current state.
The reviewers in general did not seem to be strongly impressed by the contribution of the paper. As the authors noted, some reviewers seemed to misinterpret the claims of the paper   the paper is not to design new MORL algorithms that are significantly better on standard MORL benchmarks but is to apply MORL on offline RL and fine tuning. On the other hand, the AC suspects that the paper s exposition could be more centered around the applications, e.g., arguing why offline RL can be benefited from better MO training, and why the challenge of offline RL is to balance some given notions of risk and return computationally (instead of, e.g., developing the right notion/formula for quantifying the risk and return.) Moreover, I think the paper would be stronger if the evaluation for offline RL setting can be made stronger, e.g., including more tasks and algorithms on the D4RL dataset. If the paper s claim is that MORL is a great tool for offline RL, perhaps it s useful to demonstrate that MORL can achieve SOTA reliably when used on top of existing offline RL algorithms (which almost always have two parts in the objective). In summary, in the AC s opinion, the paper has a valuable contribution to the community but is somewhat boardline for ICLR in the current form, and the AC encourages the authors to resubmit to a top venue conference after addressing some of the reviewers  comments.
This paper presents a new technique for constrained offline RL. The proposed method is based on reducing a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. The proposed algorithm is tested against several baselines on both random grid worlds and continuous environments. Results clearly show that the proposed algorithm outperforms baselines while keep the provided constraints satisfied. 

The reviewers agree that the paper is well written, the proposed algorithm is novel and technically sound, and the empirical evaluation clearly supports the claims of the paper. There were some concerns regarding the novelty of this idea, but these concerns were properly addressed by the authors in the discussion.
The paper addresses the problem of recovering a graph structure from empirical observations. The proposed approach consists of formulating the problem as an inverse problem, and then unrolling a proximal gradient descent algorithm to generate a solution. 

Whereas the paper has definitely some merit, it received borderline reviews, with three borderline rejects and one borderline accept. The reviewers have appreciated the clarifications and discussions provided by the rebuttal, and one reviewer went up from reject to borderline reject. More precisely, this reviewer agrees that the paper has become stronger, but he/she believes that the paper requires additional experimental work (see section "After rebuttal" from his/her review). Another active reviewer during the rebuttal/discussion stage was not convinced by the rebuttal, after raising issues about identifiability. The area chair agrees that solving the identifiability issue is not a key requirement for this paper; however, this raises legitimate questions about the guarantees/properties of the returned solutions.

Overall, this is a borderline paper, which introduces an interesting idea, but which requires additional experimental work and discussions about the properties of the solutions. Unfortunately, the area chair agrees with the majority of the reviewers and follows their recommendation. The two previous points should be addressed if the paper is resubmitted elsewhere.
While the paper has merits, I generally agree with negative reviewers. Among other issues, there were concerns about the theoretical contribution overlaps with prior work. While the authors argued the current work is not an extension, but rather designing ADML is. If this is the case, the paper should be rewritten to deemphasize the less novel contribution and focus more on what the authors believe to be the novel contribution. I don t believe in the practice of putting different messages (some novel and some not) into a paper with the hope that this makes the overall result "more novel". I d suggest the author rewrite the paper and more clear about the message.
The reviewers unisono do not accept the paper, because it is (a) not well written; (b) experimentally not convincing, but addresses a nice problem.  I suggest that the authors address the issues in a subsequent paper, and resubmit to one of the main conferences.
This paper examined physics inspired inductive biases in neural networks, in particular Hamiltonian and Lagrangian dynamics. The work separated the benefits arising from incorporating energy conservation, the symplectic bias, the coordinate systems, and second order dynamics.  Through a set of experiments, the paper showed the most important factor for improved performance in the test domains was the second order dynamics, and not the more common explanation of energy conservation or the other factors. The increased generality of this approach was demonstrated with better predictions on Mujoco tasks that did not conserve energy.

All reviewers liked the insights provided by the paper.  They agreed that the paper clearly laid out several hypotheses and systematically tested them.  The reviewers found the experiments thoughtful and the results compelling.  The reviewers also pointed out several aspects of the document that could be improved, including additional formalism clarifications (reviewer nLbj), baseline algorithms (reviewer wu5x), and domains (reviewers 7KKB,SW9u).  The reviewers found the author s response satisfactory but were disappointed that a revised paper was not ready to read. The reviewers want the final paper to include the modifications that were promised in the author response.

All four reviewers indicated to accept this paper which contributes novel insights that simplify and generalize physics inspired neural networks. The paper is therefore accepted.
The authors propose a deep multi agent RL framework to compute equilibria in a economics problem. Several reviewers raised issues with the presentation, as well as issues with evaluating the impact of the work, partly because the novelty of the approach is made insufficiently clear. While the authors have resolved some of the confusions arising from the presentation in their rebuttal, resulting in 2 out of the 4 reviewers to increase their score, the concerns regarding novelty mostly remain. For these reasons, I don’t think this work is ready for publication at ICLR at the moment and recommend rejection.
This paper introduces a new method for jointly training a dense bi encoder retriever with a cross encoder ranker. More precisely, the proposed method is iteratively training the retriever and the ranker, using an objective function inspired by adversarial training. In addition, the authors propose to use a distillation loss from the ranker to the retriever as a regularization term. The proposed method, called AR2, is evaluated on three retrieval benchmarks from question answering: NaturalQuestions, TriviaQA and MS MARCO. The method obtains state of the art retrieval performance on these three datasets.

Overall, the reviewers agree that the strong performance obtained by the proposed method is a strength of the paper. Regarding novelty, some reviewers argue that the method is a combination of existing techniques, hence lacking novelty, while the others believe that combining these different techniques is novel enough. Regarding the experimental section, some concerns were raised about comparisons with previous work (eg, BERT vs ERNIE) or the fact that it was a bit hard to determine where the improvements come from. I believe that these concerns were well addressed by the authors, and I tend to believe that combining existing techniques to obtain a strong system is novel enough. I thus lean towards accepting this paper to the ICLR conference.
In this paper, authors study adversarial robustness against the union of Lp threat models. Reviewers had some concerns about this work. They mentioned the paper is not well organized and the explanations of the novel components should be clearer.  In particular, they suggested authors to study the effects of different components of E AT and motivate their combinations with fine tuning. The lack of novelty was another concern. I suggest authors to focus on the fine tuning part in their revised draft which has more novelty. Given all, I think the paper needs a bit more work before being accepted.
This is a pretty nice paper, but it suffers a bit from being in an  uncanny valley  between application and research.  The approach clearly has been made, and derives from, the application under consideration.  However, the application is not a real application, and rather is a simplified simulation. That s okay, but it means that the application here is not the real goal.  So, our attention should go to the solution technique.  Unfortunately, this seems rather specific, exploiting known structure for the specific problem at hand, and lacking other reasonable baselines one could imagine.

So, this is not really an application paper, as the application in the paper is a proxy.  But this is also not really an algorithm paper, as the algorithm is not clearly shown to be generalisable to other settings.  And this is also not a theory paper that tells us something general and meaningful.

These are just observations   this is not criticism per se.  But it means I struggle a little to find something meaningful to learn from this paper, that could be applied elsewhere.

This, in addition to the overall recommendations by the reviewers, unfortunately lead me to reject the paper in its current form.  I want to thank the authors for engaging with the discussion, and hope they have found it interesting and rewarding, despite the outcome this time around.
## A Brief Summary
This paper proposes two critical modifications to the original RUDDER algorithm:
1. Proposes the Align RUDDER method that assumes that the episodes with high rewards can be used as demonstrations.
2. Uses a profile model from the Multiple sequence alignment approach to align the demonstrations and redistribute the rewards according to how frequently events in the demos are shared across different demonstrators. MSA is being used as a profile model instead of LSTM.

The paper uses successor features to represent state action pairs, which is then used to compute the similarity matrix used for MSA afterward. 
The paper shows promising results in the Minecraft environment (ObtainDiamond task,) as well as synthetic grid world environments. 

## Reviewer bJbP
*Strengths:* 
  Empirical evaluation is well done.
*Weaknesses:*
  The writing requires more work.
  Limited experiments: Mostly on toy grid world/navigation environments, it is not clear if the results will generalize to the control problems.

## Reviewer mK3T
*Strengths:* 
  Simple and effective technique for identifying sub goals.
  Large improvements over original RUDDER.
  Impressive results on Minecraft.
*Weaknesses:*
  More through ablations on the importance of different elements of Align RUDDER.
  Presentation and writing need more improvements.
  Assumption of a single underlying successful strategy is an important limitation.
  Figure 1 is problematic and confusing because of the way it explains the RUDDER algorithm. 

## Reviewer nk2L
*Strengths:* 
  Impressive results on Minecraft.
*Weaknesses:*
  Poor justification and motivation.
  RUDDER vs Align RUDDER comparisons are only done on two grid world environments.
  More ablations are required to justify the approach.
  Writing requires more work, some important concepts require more clarity. Some undefined concepts...
  Incorrect claims such as:
> Q function of an optimal policy resembles a step function

## Reviewer YcqX
*Strengths:* 
  Strong motivation.
  MSA for demos is novel.
  Strong experimental results.
*Weaknesses:* 
  Several grammatical errors.
  The method is not explained well in the paper, the writing needs more work to improve the clarity.
  Lack of sufficient analysis and ablations on the Align RUDDER approach.

## Key Takeaways and Thoughts
Overall, the result provided in this paper in the Minecraft environment is impressive. The motivation for the Align RUDDER is clear for me. I like the paper; in particular, the application of the MSA for the alignments across the demos is novel. However, as all the reviewers of this paper agreed that the paper is unclear, especially the method description requires more work. The paper needs to present more ablations and analysis to justify which components of Align RUDDER algorithm are essential. I agree with both insights, the authors have made improvements in the paper to improve the exposition of the algorithms, but still, the paper feels a bit rushed. I would recommend the authors reconsider the paper s current structure and improve the writing further, especially the description of the method can be further improved. I would recommend that the authors fix those essential issues with the paper and the other comments reviewers made in a future resubmission.
The reviewers were split about this paper: on one hand they would have liked to see more experiments on different problem settings on the other they appreciated the elegance of graph encoding methods and current results. After going through the paper and discussion I have voted to accept for the following reason:  the additional experiments and discussion posted during the rebuttal phase have addressed many of the main concerns of the reviewers (i.e., training time, message passing figure, discussion on encoding and SAT solvers). The only remaining one I see is the request for additional experiments which I don t think is grounds for rejection: current results are comprehensive and an additional experiment I think would not alter the main conclusions. I urge the authors to take all of the reviewers changes into account (if not already done so).
This paper proposes a method to improve the transfer of visual control policies between robots.  The method extends a visual foresight approach using a learned robot agnostic world dynamics model and a (potentially analytic) robot specific robot dynamics module.  A key aspect of the method is to form a blocky mask over the robot s body in the visual image, thus allowing the learned dynamics to depend less on appearance attributes of the robot.  Planning with these dynamics models operates in the visual observation space.  The method is tested for zero shot transfer on multiple physical robots and also with simulated robots, with positive results across multiple experiments.

The reviews raised multiple concerns on the details of the method and the clarity of the presentation that were largely addressed in the author response.  The authors refined their claims to a demonstration of zero shot transfer of visual skills across real world robots.  The evaluation of the proposed transfer method on real world robots is a notable strength of the paper.  The core limitation, raised by one reviewer, is that the generality of the invariance is only provided by a visual mask on the color and appearance of the robot.  This serves as a limited form of invariance to the robot s dynamics.  

The discussion phase did not yield a consensus on the merits of the paper, with the proposed method seen as useful but still limited.  Three knowledgeable reviewers indicate to accept the paper and one indicates to reject.  The formal description does not make the world dynamics explicitly robot agnostic, as the learned model $(P_w(o_w |o_w, r, r , a)$ in Equation 2) still explicitly depends on the actions of the robot ($a$) and thus the robot s dynamics.  Despite some limitations in the formalism, the practical utility of the method is convincingly demonstrated in multiple robot experiments.  This is a clear contribution to the literature, which is supported by three reviewers.  The paper is therefore accepted.
Good premise:  What unsupervised training supports IR?  This is a key question for IR and is a focus for papers in TREC 2019 Deep Learning Track, for instance.  Also, historically, empirical work in the IR community is a very high standard. 

One reviewer says the contrastive loss for learning Siamese Transformers is not new, and prior experimental work was listed.  Several reviewers suggested extensions to the empirical work, some of which was subsequently done.  Results are "promising" according to one reviewer, but not strong enough.  Another reviewer says a different use context is needed since its hard to compete with efficient BM2t in its own terms.  The authors made some good changes to their paper: updated intro and related work, extended results and discussion, but the 4 reviewers remained in agreement, a reject.  However, some reviewers felt this was a good contribution, so with further empirical work and polish it should be good.
Description of paper content:

The paper provides a framework to develop a family of algorithms that decompose rewards into linear combinations of several reward channels. The value functions per channel are estimated in a new space using an invertible function transformation, f. The framework encompasses several previously published algorithms, including Log Q Learning. Conditions are provided for acceptable choices of f. Convergence to the optimal Q function in the tabular case is proven for a special learning update.

Summary of paper discussion:

All review scores were above the acceptance threshold. Overall, the reviewers found the idea interesting, the theoretical results satisfying, and the writing and presentation clear. Initial concern about the directedness of the experiments in showing the usefulness of this particular theoretical framework to explain performance improvements was allayed when some of the results in the paper (e.g. reward density in Atari Skiing) were re emphasized. Generally, all reviewers felt that this was a nice, thorough contribution with the demerit that the framework lacked “a killer application” experimentally.
After the author response multiple reviewers remained concerned over the degree to which the current manuscript makes the case for the proposed hyper network approach to text to image generation. It was felt that this was mainly an empirical paper for which the reviewers remain unconvinced that the proposed hyper network based modulation was better than simple channel wise StyleGAN2 style modulation. While the authors have shown that their approach beats a StyleGAN2 baseline with sentence conditioning on CLIP R the reviewers felt that the comparisons with StyleGAN2 baseline needed fairer word conditioning. Only one reviewer recommended accepting this paper.

The AC recommends rejection.
The authors have addressed several of the issues raised by the reviewers, and they are strongly encouraged in include the additional experiments, and sections, that they propose, in a revised submission. The reviewers also recognized the novelty and extend of applications the proposed methodology has. Nevertheless, the paper would significantly benefit from a rigorous and thorough comparison to related work, placing it well within the context of the literature brought up by reviewers. Experimental comparisons to competitors, even if the latter address more restrictive settings, would strengthen the paper. Most importantly, the authors should consider including a comprehensive related work section, that convincingly discusses and compares to related/adjacent methods.
Summary: The paper introduces and studies the expected improvement (EI) technique as a way to balance exploration and exploitation for the contextual bandit problem. The authors propose two EI based algorithms for linear bandits and for neural bandits for a general class of reward functions. The paper presents regret bounds for both methods and shows the experimental results to support their theoretical claims.

Discussion: The reviewers have identified technical issues in the regret bound of LinEL which has been now corrected. Similarly, reviewers have had difficulty assessing the correctness of the paper due to typos and unclear exposition, and raise concerns regarding the amount of corrections that were necessary to reach the current stage. There is no consensus between the reviewers, and some would feel more comfortable if the paper could go through another round of review after major revision.
Reviewer UDHj points that after corrections, "the regret has an additional $O(\sqrt{\ln T})$ dependency compared with the regret of LinUCB and Thompson Sampling." and this should be discussed in the updated version.
Reviewer co1L suggests to compare to the bounds in "The End of Optimism? An Asymptotic Analysis of Finite Armed Linear Bandits". The authors responded that " To our knowledge, the analysis of the optimal asymptotic regret for contextual bandits as we consider (where the context may be controlled by an adversary) is still an open problem.". In fact, this is the topic of several recent works including:
* "Asymptotically Optimal Information Directed Sampling" COLT 2021
* "An asymptotically optimal primal dual incremental algorithm for contextual linear bandits", NeurIPS 2021

The connections of the present work with these two references are strong and should be discussed in more depth. I believe it is a more important discussion than the comparison with the regret bound of LinTS which is yet another problem.  

The reviewers have appreciated the originality of the ideas and for that reason we encourage the authors to revise their draft and submit to a future venue.

Recommendation: Reject.
The paper provides a unique contribution to the scalability of Bayesian inference to Pyramidal Bayesian Models with application to neuroimaging. The major point of concern by the reviewers is around how close is the inference approach to the more classical Mean Field VI. However, in my opinion, the authors have addressed these concerns in the rebuttal. Therefore, I recommend Accept.
The paper presents an approach to semantic segmentation based on text embedding of class labels. This enables zero shot semantic segmentation with class labels that were not seen during training. I appreciate the new ablation against a ResNet 101 backbone. I don t find the similarity with CLIP substantial, and I recommend that the paper is accepted.
This paper aims at improving the data efficiency of pretraining in CLIP. This is a practically meaningful research direction. The proposed method is simple, even kind of straightforward and has limited innovations. It combines self supervision within each modality, multi view supervision across modalities, and nearest neighbor supervision from other similar pairs. Such a combination showed strong empirical results: achieved better performance using seven times fewer data. The rebuttals resolved most critical concerns on experiments, such as fair comparisons with the original CLIP work.
The paper proposes a methodology for alternatively growing and pruning a subset of layers within a network in order to eventually produce a trained, sparse model.  After discussion, all reviewers favor accept.  Empirical performance of the sparse models appears strong, but requires significant computational expense during training to achieve.
This paper is about an unsupervised method to learn new skills in non stationary environments by maximizing an intrinsic reward function. Experimental evaluations on OpenAI gym environments show that the proposed approach improves the diversity of the learned skills and is able to adapt to continuously changing environments.

This paper is borderline. After reading each other s reviews and the authors  feedback, the reviewers discussed the pros and cons of this work. Even if the reviewers have pointed out that the paper has some limitations, they agree that the paper represents a valuable contribution and have appreciated the improvements implemented by the authors during the rebuttal, thus reaching a consensus towards acceptance. 
The authors need to update their paper according to what they have proposed in their response and they have to take into serious considerations all the reviewers  suggestions while they will prepare the camera ready version of their paper.
This paper has been reviewed with four expert reviewers. The reviewers have reached the consensus that the paper is not yet ready for publication. The main concerns are related to novelty. All reviewers gave substantial and constructive feedback. Following the recommendation of the reviewers, the meta reviewer recommends rejection.
This paper demonstrates the hypothesis that a very small word piece vocabulary (giving a "quasi character level" model) outperforms current methods of neural MT in truly low resource scenarios, and provides some auxiliary studies around word piece frequency and domain transfer. It considers LSTM, CNN, and Transformer NMT models. This is useful information for people working in low resource scenarios to know.

The paper got 3 reviews by people with very strong machine translation expertise. There was a general consensus that the paper was insufficiently aware of prior work on this topic and the paper had problems in experiment construction which raised issues about the comprehensiveness of the result. That is, while this paper adopts a more extremely small vocabulary, Sennrich and Zhang (2017) already showed that a much smaller subword vocabulary can give much stronger results for low resource MT (while Araabi and Monz questioned whether this was as true for Transformer NMT. Meanwhile Cherry et al. (2018) and Kreutzer and Sokolov (2018) argued already the benefits of (almost) character level NMT. On the experimental side, both not having results on genuinely low resource scenarios and the commented of Reviewer FBrF that the problem with larger subword vocals here may be mainly due to the small corpus size used for constructing the subword vocabulary are both quite important. Moreover, as mainly an MT experimental study, this paper seems better suited to a more specialized audience of MT researchers at an ACL, WMT, AMTA, etc. venue.

I recommend rejecting this paper as not sufficiently novel, with experiments that need further work, and lacking strong interest to a broader representation learning audience.
The manuscript investigates common adversarial attacks on event based data for spiking neural networks.
They conclude that also in this setup adversarial attacks can strongly harm SNN performance.

Although the reviewers agree that the paper presents some solid results and is well written, there was also substantial criticism.

The main points were:
  It is not very clear how the usual attacks are applied to event based data, and in general experimental setups are unclear.
  The methodological contribution of the paper seems limited.
  The novelty is limited, in particular Marchisio et al. 2021 investigates a very similar question and goes somewhat further. The author noted that their attacks are not deployed on neuromorphic hardware. A number of other important prior work is not discussed.
  The impact of adversarial defences was not considered.
  A more detailed comparison of event based attacks to standard ANN attacks would be desired.

After the reviews, the authors have invested substantial efforts to improve the paper. These efforts were appreciated by the reviewers. In particular, the authors ran additional experiments using the defence method TRADES. The results showed that TRADES is effective, but the attack has still a large success.

In summary, the reviewers agree that this is a solid manuscript and an interesting direction, however, they see it finally slightly below acceptance threshold for ICLR.
The initial reviews for this paper were somewhat diverging, however the paper did not receive any significant negative criticism to push it towards below the acceptance threshold. The reviewers have found some minor issues about the paper. Following the reviewer recommendations, the meta reviewer recommends acceptance.
This paper considers the problem of on device training for federated learning. This is an important problem since, in real world settings, the clients have limited compute and memory, and local training needs to be efficient. The paper shows that the standard sparsity based speed up techniques that consider top K weights/activations during forward and/or backward pass do not work well in the federated setting and proposes several solutions to mitigate this issue. The proposed solutions are demonstrated to work well on several datasets.

In their initial assessment, given that this is largely an empirical insights driven paper, the reviewers mainly expressed concerns about the experimental evaluation (e.g., only one dataset CIFAR10 and one architecture ResNet18) and lack of more baselines (e.g., Federated Dropout). The authors responded in detail to the reviews and also conducted additional experiments and the reviewers and authors engaged in discussion. As the discussion converged, the reviewers agreed that the revised manuscript addresses their key concerns and their assessment, on an average, are now learning largely towards a borderline accept.

I also read the reviews, the discussion, and read the paper. I think the paper is a good initial attempt at providing a general approach to enable on device federated learning when the clients are lightweight devices (e.g. edge devices). Even though the study is somewhat preliminary, the current manuscript, after the revision during the discussion phase, is significantly improved version of the original submission and does address the key concerns from the reviewers. Overall, I would rate the paper for a borderline acceptance.
This paper presents an approach for using prior knowledge to constrain transitions for consecutive time steps and aims to replace conditional random fields for sequence tagging tasks in sequence labeling. However, the paper seems incomplete with no experimental results and analysis to validate the proposed ideas.
Overall, reviewers are positive. The majority praised the approach as novel and viewed the results as quite strong. Further, reviewers valued the provided ablations and analysis that helped motivate the proposed method. A few concerns were raised about overall clarity (though some reviewers praised the clarity of presentation), the use of hand crafted filters, and certain experimental comparisons. The majority of these concerns have been adequately addressed in author response.
The paper  builds  fast and high quality SMILES based molecular embeddings  by distilling  state of the art graph based models teachers.
This has the advantage of speeding inference time w.rt to graph based methods. 

The reviews were split regarding the motivation of the work, in the sense of why not train directly on SMILES instead of distilling graph based methods that are in some tasks behind SMILES transformer. Authors provided clarifications in the rebuttal showing that on Knowledge distillation of graph models  surpasses  SMILES only model training. 

I think given the experimental nature of the paper the main motivation of the paper should be better clarified and supported with more experimentation and downstream tasks.
This paper provides some empirical investigation of the choice of the prior distribution for the weights in Bayesian neural networks. It shows empirically that, when trained via SGD, weights in feedforward neural networks exhibit heavy tails, while weights in convolutional neural networks are spatially correlated. From this observation they show that the use of such priors leads to some improved performances compared to the iid Gaussian prior in some experimental settings.

Reviewers have conflicting views on this paper, that have not been reconcilied after the author s response and the discussion.
On the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice of the prior in Bayesian neural networks, which could lead to further developments. 
On the negative side, the claims made in the introduction are not fully supported by the experiments (the claims have been slightly amended in the revised version), and the take home message is not so clear. In particular, Bayesian approaches with the proposed priors still underperform compared to SGD without tempering. The authors could also have considered a broader sets of experiments.

Overall, I think the contributions outweight the limitations of this paper, and I would recommend acceptance.
This paper introduces a novel quality diversity algorithm, "Evolutionary Diversity Optimization with Clustering based Selection (EDO CS)", and applies it to reinforcement learning. A bandit approach (UCB) is used to select which cluster to sample parents from. The QD algorithm can be evaluated on its own, outside of the RL context, and if so it should be compared to the several approaches to niching and other standard diversity preservation approaches in evolutionary computation that rely on clustering. (And the authors should make an effort to connect to the niching literature in particular.) However, the use of the algorithm for RL makes it possible to use behavioral features as the space in which to cluster, separating it from standard diversity preservation methods. The resulting algorithm is relatively simple and the empirical results are good.

Some of the main concerns for reviewers included the bibliography, which the authors promptly acted on by citing several suggested papers and comparing their approach where relevant. There was also discussion about the exact novelty of the paper, for example as compared to the CVT MAP Elites algorithm, but this was clarified by the authors. Reviewers agree that the paper is easily to follow and well written.

Based on this, it seems that the paper makes a clear contribution to QD methods for RL, and is worth accepting.
This paper provides an overview of evaluating graph generative models (GGMs). It systematically evaluates one of the more popular metrics, maximum mean discrepancy (MMD). It highlights some challenges and pitfalls for practitioners and suggests some ways to mitigate them. The reviewers found the paper practically relevant and several reviewers upgraded their scores through the discussion process. The authors acknowledged there are still some remaining issues regarding (i) considering other metrics & descriptor functions; ii) evaluating node/edge attributes and iii) addressing molecule generation. I am satisfied that these areas are beyond the scope of the current work and that the clarification improvements in the paper are adequate. It stands well enough on its own to accept in its present form.
One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay.  This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay.  The paper proposes novel information theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers).  They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance.

Pros:  The paper is well written and organized.  It was easy to follow. The formulation is novel and technically sound. The idea of taking learnability into account is novel and interesting.  It provides a nice way of avoiding outliers and balancing surprising information. The authors presented the motivation for each part of the framework well.  

Cons: To understand the contribution of each component of the formulation and competing criteria, an ablation study is needed.
Reviewers had several detailed suggestions and questions, including sensitivity to hyperparameters, additional citations, additional data sets beyond MNIST and CIFAR10, etc.  

In the rebuttal, the authors have addressed several of these concerns.  Please make sure to include and incorporate reviewer suggestions in the final revised version.
In this paper, the authors proposed an offline policy optimization algorithm, motivated by an analysis of the upper bound error of importance sampling policy value estimator. Specifically, by the decomposition of the error in a particularly way, the authors identified some error which does not converge. Then, the authors introduce the contraints over feasible actions to avoid the overfitting induced by such errors. Finally, the authors tested the proposed algorithm empirically.

The paper is well motivated and the authors addressed some of the questions in their rebuttals. However, there are still several issues need to be addressed, 

  The alternative practical estimator with plug in behavior distribution would perfectly avoid the over fitting, which is, however, ignored. This is an important and easy to implemented competitor.

  The pessimistic principle in the face of uncertainty (PFU) has been exploited extensively in offline policy optimization problem. How the proposed algorithm is connected to the PFU has not been discussed carefully, especially in terms of non asymptotic sample complexity, which makes the paper is not well positioned. 

  While the motivation is derived from the unbiased importance sampling estimator, the counterfactual risk minimization in Equation 7 is introduced suddently, without clear justification. 

  In my opinion, for a better clarification of the paper, the expressiveness of the policy family should not be discussed in this way. I understand the authors would like to avoid any possible degeneration, and explain the asymptotic lossless in terms of policy flexibility. However, the whole point of the paper is trying to introduce some mechanism to avoid the possible overfitting by regularizing the policy family. In other words, the restriction is on purpose and beneficial. I think the argument of policy family expressiveness should be re considered and re discussed. 


Minor: 

  Markovian vs. non Markovian baseline comparison is not fair, and more comparison on well known benchmarks, e.g., OpenAI gym, should be conducted. 
  The \sigma upper bound should be explicitly provided and verified in practice.

In sum, the paper is well motivated, however, need further improvement to be pulished.
This paper studies model based RL in the setting where the model can be misspecified. In this case, MLE of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. The authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account.

This paper studies an important problem and this was recognized by all reviewers. Its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. The rebuttal was comprehensive and exemplary. For instance, one concern of this paper was limited empirical evaluation. The authors added 5 new benchmarks and also included stabilizing improvements in their original baselines. I strongly support acceptance of this paper.
The paper proposes multiresolution and equivariant generative models.  Experimental results for several applications are shown.

Pros:
  A first hierarchical generative model with multiresolution and equivariance.
  Extensive experiments

Cons:
  Marginal novelty (multiresolution and permutation equivalence each is not novel for graph neural networks.
  State of the art methods are not compared as baselines.
  Some standard metrics are not evaluated, and the used metrics are questionable (some generated molecules might not be stable although the chemical validity is 100%).
  Time/space complexity evaluation is missing.

The authors did not address some of the serious concerns in the rebuttal.
This paper proposes a global model agnostic explanation method. The method relies on a neural model that learns to predict which input features are important for the original model’s predictions. Using experimentation, the authors demonstrate that their approach outperforms LIME and Integrated Gradients. They compare the explanation methods in terms of the faithfulness of explanations and computational complexity. While the premise of the work is interesting, reviewers have suggested several areas for improvement: i) writing can use more clarity. writing seems verbose and hard to understand especially in intro and methods section ii) several points have been raised about empirical evaluation including lack of user studies, a more extensive set of baselines and data modalities. Given this, we are unable to recommend an acceptance at this time. We hope the authors find the reviews helpful.
The paper studies the problem of multi class calibration, proposing new notion of "top label calibration", and presenting and comparing new algorithms for multi class calibration. Reviewers generally found the paper to be well written, and tackling a foundational problem. There were some questions regarding the experiments:

(1) _Lack of explanation of why unnormalised beats normalised._ One of the paper s main empirical findings is that using an unnormalised predictor with histogram binning (CW HB) can significantly outperform a normalised one (N HB). There is however limited discussion prior to this of why such behaviour is expected.

(2) _Lack of comparison to isotonic regression._ One can apply isotonic regression in conjunction with one of the M2B algorithms in Sec 3. It is of interest whether isotonic regression + a suitable M2B wrapper compares to HB + an M2B wrapper.

(3) _Comparison to OVA calibration_. One reviewer raised the concern that the new algorithms proposed in this work are not too surprising; e.g., using a binary calibrator of one label vs everyone else appears natural.

(4) _Overloaded appendices_. One reviewer pointed out that some material in the Appendix is not referenced in the body, thus making the work not self contained.

For point (1), the response indicates that the unnormalised model can obtain distribution free guarantees. The lack of such a guarantee for the normalised model does not suggest that such a guarantee is impossible, however. Certainly the present work need not solve this issue in entirety, but if this is intended to be a main takeaway of the experiments, a little more discussion in the body seems advisable. 

On this note, from my reading, the experiments seek to demonstrate that suitable M2B reductions can dramatically improve the performance of HB. However, with the use of multiple evaluation metrics (Top ECE, Top MCE, Classwise ECE) it is not clear if the authors intend to promote one specific M2B reduction as generally favourable; further, the text in Sec 3.2 suggests that N HB is the method considered in prior works, which then seems to do better on top label ECE than prior works. Which suggests that for this particular metric, one does not gain much from other M2B reductions?

For point (2), the response argued that "the main message of the paper is not about HB vs other binary calibrators, but proposing a single agnostic framework for achieving multiple notions of multiclass calibration using any binary calibrator". From my reading of the paper, I think this claim is accurate, and agree that the conceptual advance is the generic M2B framework itself

For point (3), the authors responded to suggest that while Algorithm 2 performs a natural one versus all calibration, this is different from Algorithm 1. Further, the latter is shown to be useful in Table 2 (bottom panel).

For point (4), the revision involved referencing relevant material in the Appendix. These seem better, though I would prefer if theorem statements (e.g., Theorem 1) appear in full or as sketches in the body.

Further to the above, I have a couple of minor suggestions:
  consider removing most hline s from Tables 1   3

  keep the ordering of Algorithms 1   4 the same as that in which methods are presented in Table 1.

**Summary**. The paper considers a foundational problem. It makes one simple yet interesting contribution in its definition of top label calibration. Detailing the various multi class calibrators in Section 3 is another contribution: albeit simple, it does illuminate the subtle issue of the role of normalisation in post hoc calibration, which empirically is shown to have non trivial impact. There are certainly avenues where the paper could be further strengthened, but overall I do see it as potentially being of broad interest to the community, and inspiring future work. My recommendation is thus for the authors to further incorporate the reviewers  detailed suggestions and the comments above, which can broaden the clarity, scope and impact of the work.
The paper considers quantization issues for learned neural network based image compression methods. 
Many works on the topic incorporate quantization into the training of the method. The paper provides evidence that post training quantization is effective. Specifically, the paper demonstrates that state of the art learned image compression methods can be quantized post training and retain a very similar level of compression performance. The paper argues that this is important in particular for cross platform applications, where an image is decompressed on different architectures. Finally, the paper proposes an approach to discretize entropy parameters.

The reviewers raised the following concerns. 
  Reviewer 2XDr is concerned about the application of post training quantization being a contribution, since post training quantization has already been studied in [1] (and in the recent paper [2] that can be considered as concurrent work). The authors response is that the methods in [1] has extra overhead and clarify how the prior work is in fact different. This addressed the reviewer s concern, and the reviewer raised their score.

  Reviewer eyVf finds the comparisons with previous methods to be insufficient, and in general find the value of the research unclear, as the goals are not sufficiently specified. The authors clarified, and the reviewer was satisfied with the response and raised the rating to marginal above the threshold. 

  Reviewer L7dn tends towards acceptance, but has concerns about the technical novelty, that are unspecified unfortunately.

  Reviewer oV3R argues that the solution is marginal relative to prior work, and votes to reject the paper. The authors responded why they think it isn t, and also wrote a private letter to the area chair in which they explain why they think that reviewer s oV3R should not be taken into account. I agree with the authors that the paper under review provides a step relative to Balle et al (2019), and that the writing of the paper is not an issue; however, the reviewer s overarching point is that the overall contribution is marginally significant when taking the prior work by Balle et al (2019) into account and this is the sentiment of other reviewers as well.

  Reviewer GrpS, an expert on image compression, leans towards acceptance and argues that the results are strong as they show little to no loss due to the quantization technique, but also rates the contribution to only be marginally significant and novel, and raises a few questions and issues, to which the reviewers responded. 

This paper is really borderline. Four out of five reviewers rate this paper as marginally above the acceptance threshold. The consensus is that while the experiments and claims are correct, the contribution is only marginally significant or novel, in particular, relative to prior work, and therefore I recommend rejecting the paper. I would, however, not be upset if it would be accepted.
This paper proposes a new knowledge distillation (KD) method for adversarial training. The key observation is inspiring: soft labels provided by the teacher gradually becomes less and less reliable during the adversarial training of student model. Based on that,  they propose to partially trust the soft labels provided by the teacher in adversarial distillation. 

Reviewers unanimously agree that this paper has clear motivation, well sorted logic, and neat writing. While some reviewers initially posed concerns on evaluation completeness and detail clarification, they were well addressed during the rebuttal. AC reads the paper/discussion thread and agrees this is a worthy work to get accepted.
The paper claims to present actionable visual representations for manipulating 3D articulated objects. Specifically, the approach learns to estimate the spatial affordance map as well as the trajectories and their scores. After checking the rebuttal from the authors, all reviewers agree that the paper adds value to the research area. In the end, it got three borderline accept ratings. The initial criticism included lacking (experimental) comparison to baselines, and the authors successfully corresponded to the request from the reviewer. One reviewer commented that the proposed approach is a combination of Where2Act and curiosity guidance for RL Policy for Interactive Trajectory Exploration, which we believe is a valid point. Still, the paper extends the previous Where2Act and successfully demonstrates its success on difficult tasks.

We recommend accepting the paper.
This paper received scores of 6,6,6 after the reviewers succeeded in making two authors raise their scores from 5 to 6. However, even after this, none of the reviewers actively argued for the paper. The only positive point raised in the private discussion was that the results are strong. (However, there is still the question of how much of this was due to the different design space used.)
Negative points raised in the private discussion included that 
  despite the authors clarification on the differences to Zen NAS, the difference is perceived not to be large.
  there is no theoretical foundation behind the selection of a critical parameter, and this directly limits the applicability of ZenDet in searching for FPN connections. 
  as a paper focused on detection NAS, a limitation to only search for the backbone may not be novel enough for publication at ICLR. 

Overall, I agree with this criticism and weakly recommend rejection.
Reviewers are in agreement that the paper is below the acceptance threshold. Main concerns focus around novelty, experiments, and justification of the paper s main claims.
This paper deals with the important topic of active transfer learning. All reviewers agree that
while the paper presents some shortcomings , it is considered to be a worth contribution.
The authors show that it is possible to overcome the script barrier in MLLMs by using transliteration. In effect, they show that transliterating all text to a single script improves the performance for low resource languages. They also provide additional analysis in the form of statistical tests and crosslingual representation analysis to substantiate their claims.

The main concerns raised by the reviewers are: 
(i) lack of novelty: the idea of using transliteration has been extensively studied in the context of NMT, Speech. It has also been studied in the context of MLLMs by some recent work (which can be considered to be contemporary). IMO, this is a concern.
(ii) focus on Indic languages: there are some concerns raised about the broader applicability of the techniques presented in the paper (personally, I disagree with this concern as Indic languages are important   for example, there are numerous papers which only report results on En De, En Ru translation)
(iii) limited evaluation: the technique is evaluated only using the ALBERT model and other configurations (such as ROBERTA, XLM, etc) are not considered. IMO, it would have helped if the authors presented results on these models also (at least we would know if transliteration only helps in the case of small/compact models or even in the case of large models)
(iv) missing references: there is a large body of related work on NMT, speech, etch which the authors had missed in their initial draft. This has been rectified in the updated version.

The reviewers did participate in the discussion with the meta reviewer (not with the authors though) and even after looking at the revised draft mentioned that the novelty is limited. 

To summarise my views, I think the initial draft of the paper did need improvements and the final draft is a significantly improved version of the initial draft. However, I still feel the novelty is missing. Even the empirical novelty claimed by the authors is ;lacking due to the use of a single model (ALBERT).
This paper proposes a conditional quantile generative model using optimal transport. Although the problem addressed in this paper is interesting and important, the proposed convex potential quantile (CPQ) approach is highly relevant to a recent work (Carlier et al. 2017). Due to the lack of clear explanations of the contributions compared to the existing work, none of the reviewers suggested acceptance of this paper.
Dear Authors,

The paper was received nicely and discussed during the rebuttal period. However, the current consensus suggests the paper requires another round of revisions before it gets accepted.

In particular:

  There were still some gray areas regarding comparison to simple techniques. E.g., one reviewer raised the question how it compares to simply stopping based on validation accuracy for example. The reviewer was missing the justification why stopping at the loss of Ramanujan graph property is preferable in comparison to other criteria. 
  Several reviewers found the general idea interesting, but all felt that more reasonings about the impact/insights/relationship of Ramanujan graph property with pruning need to be found to get accepted.
  Reviewers appreciate that the authors corrected many parts of the submission (see increased scores). However, reviewers felt that the paper requires more data/evidence to get accepted at this level, based on the discussions made during the rebuttal period. 

Best AC
This paper proposes techniques for improving the scalability of set to hypergraph models.
The main issue with the submission is that all reviewers found the clarity of the paper to be problematic including the proofs, the experimental conditions, and many other parts.
The authors responded but some reviewers explicitly state that their questions have only partially been answered and some reviewers did not respond to the authors. Unfortunately, given the number of clarity issues raised by the reviewers it makes more sense to re submit this paper after re writing based on all the suggestions from the reviewers.
This paper proposes that the superior performance of modern convolutional networks is partly due to a phase collapse mechanism that eliminates spatial variability while ensuring linear class separation. To support their hypothesis, authors introduce a complex valued convolutional network (called  Learned Scattering network) which includes a phase collapse on the output of its wavelet filters and show that such network has comparable performance to ResNets but its performance degrades if the phase collapse is replaced by a threshold operator.

Reviewers are all in agreement about the novelty and significance of the work. They also find the empirical results compelling. The main weakness of this work which was highlighted by all reviewers is clarity. The paper can be significantly improved in terms of the writing. While I am recommending acceptance, I strongly recommend authors to take reviewers  feedback into account and improve the writing significantly for the final version so that more people would benefit from this paper and build on it in the future.
The paper examines the advantage of using models in RL.  The authors  rebuttals convinced us of the value of the paper.
This paper investigates the stability plasticity dilemma in the class incremental learning context. It investigates which model components are eligible to be “reused, added, fixed, or updated” to achieve a good balance. Initially the paper had one supporter (xDnv) who liked the motivation and extensiveness of the ablation. NFc9 and UadE also echoed some similar points about motivation and liked that the work was easy to follow. Reviewers expressed concerns such as incrementality w.r.t. spaceNet, too many hyper parameters, unclear performance benefit, lack of comparison to SOTA, fixed sequence of classes specified by the authors, not clear how much forgetting is happening in each method (echoed by multiple reviewers), and limited datasets used for evaluations. The authors responded to the critical reviews and provided a revised version of the paper with additional comparisons to rehearsal free methods and with more datasets (MNIST/FashionMNIST). 

Following the author response, NFc9 stated that they thought the paper was in better shape with the revisions and upgraded their score claiming it was “closer to acceptance”. Yet, they still had concerns with the practical implications of having too many hyper parameters. UadE engaged further with the authors but claimed that they avoided the reviewer’s direct concerns. UadE maintained their concerns with the manual ordering of classes and older baselines. I agree that there are several rehearsal or pseudo rehearsal methods to which they could have compared. Reviewer o6Js did not engage further. Overall this is a borderline paper. I appreciate the authors engaging in the discussion period, though my assessment is that the key issues still remain. This paper could use further development so my recommendation is reject.
Four experts reviewed this paper and all recommended rejection. There was no rebuttal. The reviewers raised many concerns regarding the paper, such as missed citations, lack of comparison with related methods, and some presentation issues. Considering the reviewers  concerns, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
Although all reviewers had many positive comments on the paper, and the authors engaged nicely in the discussion period, at the moment there is a consensus among the reviewers that the central claims of the paper (related to minimal representations / information bottleneck) are not adequately supported by the current experiments. In particular, there were concerns that performance gains could be due to diversity of predictors, rather than minimal representations, which would need to be addressed. It s suggested that the reviewers take all of these comments and discussion into account when preparing a revised version of the paper.
All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods.  I believe this paper will be of broad interest to different communities at ICLR.
This paper proposes a diversity loss and a topological prior to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. These loss terms significantly improve the efficiency in finding trojaned triggers. The experiments results show that the proposed method performs substantially better than the baselines on the Trojaned MNIST/CIFAR10 and TrojAI datasets, respectively. This paper shows detailed ablation study with great empirical performance. Some reviewers have doubts about the experimental comparisons and some of the assumptions made in the algorithm. Overall, the thorough experimental investigation fo the proposed method makes this paper worthy of publication and widely being shared.
PAPER: This paper describes a method to generate visual gestures by learning an intermediate representation based on gesture sequences. This proposed method builds from previous work on VAE and vector quantized VAE.
DISCUSSION: The reviewers wrote some detailed reviews about the paper, bringing some valid concerns and asking some questions to the authors. Unfortunately, no responses were posted from the authors.
SUMMARY: After looking at all reviews, there was a general consensus among the reviewers that this paper was not ready for publication. We hope that the reviews will be helpful for the authors in revising their work for future submission.
This paper considers the recent line of work on algorithms with predictions.  They give new results on the online facility location problem. Overall, the reviewers felt the topic was of interest to the community.  There were some concerns about the error metric used and the overall framework. However, the majority of reviewers still felt the paper was interesting and I think the paper can be accepted.
This paper studies the convergence of Adam type algorithms (two variants of AMSGrad in particular) in min max problems that satisfy a one sided "Minty variational inequality" condition.

The reviewers identified several weaknesses in the paper and the authors did not provide a rebuttal to these concerns so there was consensus to reject the paper.
The paper proposes a plug and play method for solving imaging problems. Plug and play methods use a denoiser to solve linear inverse problems. The paper proposes a plug and play method and uses convex optimization tools from analyzing proximal gradient methods to provide convergence guarantees. The algorithm is applied to a variety of inverse problems showing that the method works well. 

After the discussion period, all four reviewers recommend acceptance. 
  Reviewer QQES provided a detailed review and raised a few concerns including a clear motivation for and description of the denoiser, and unsupported claims, in particular related to a proof in the paper. The authors revised the paper and responded in length to the claims, in particular they detailed steps and assumptions related to the theorem in their response. As a response, the reviewer changed their score to accept.
  Reviewer xYLt strongly supports acceptance based on the strong theoretical results and a very good exposition.
  Reviewer GZzY likes the overall idea of the paper and raised a few minor concerns and questions, which were addressed by the authors. 
  Finally, reviewer E8QG also appreciates the method, convergence analysis, and extensive validation. The reviewer also raised a few minor concerns and asked for clarification, and the response of the authors resolved those concerns. 

Based on my own reading and based on the reviews, I recommend acceptance. The paper provides a variant of a plug and play method, proves interesting convergence results for the method, and has a strong experimental evaluation of the method. I encourage the authors to take the feedback of the reviewers into account, which they have done for the most part already, and it would also be interesting to see the performance of the method for compressive sensing problems.
The paper proposes a simple approach to quantify uncertainty in "deterministic" neural networks, not unlike the works of SNGP, DDU, and DUE, where one only performs one forward pass rather than in an ensemble or Monte Carlo sample. In particular, they propose a kernel based method on a network s logits to estimate uncertainty, obtaining data and model uncertainty estimates separately using a bound on Bayes risk.

While I agree with the relevance of the problem, there s a shared concern among reviewers across both technical novelty and experimental validation particularly compared to prior work that can be difficult to understand the key distinguishing factor. I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue.
In this paper, the problem of identifying a low dimensional latent space for high dimensional Bayesian optimization (BO) is considered. In particular, the authors focus on the problem of collision, where different points in the original space become identical in the latent space, and propose a regularization method to avoid this problem. Latent space identification for high dimensional Bayesian optimization is an interesting and the authors  approach sounds reasonable. However, many reviewers pointed out that the discussion and results in the paper do not provide sufficient evidence for the authors  claims. Therefore, we have to conclude that the paper cannot be accepted at this time.
In standard message passing GNNs (MPNNs), one step at any node u involves receiving state/embedding information from all of u’s neighbors, and then updating u’s state as a function of these messages and of u’s own current state. Thus, the communication pattern at every step is that of a star topology (a graph with u at its “center”, and with u connected to all its neighbors, and with no other edges). However, it is well known that the expressive power here is bounded by that of the 1st order Weisfeiler Leman isomorphism test (1 WL). This paper then takes the natural step of generalizing the star topology to more general ones (e.g., k hop egonets, the subgraph induced by the nodes of distance at most k from u). It is shown that this framework is strictly more powerful than 1 WL and 2 WL (however, as pointed out by a referee, this is actually a weaker version of 2 WL that is equivalent to 1 WL), and is at least as powerful as 3 WL. Subgraph sampling approaches that improve efficiency are also introduced. It is shown that this method beats the SOTA for some number of well known graph ML problems.

It looks like this paper has a very strong overlap with the NeurIPS 2021 paper "Nested Graph Neural Networks" (https://openreview.net/forum?id 7_eLEvFjCi3). Both papers use rooted subgraphs (k hop ego nets) to replace the k hop rooted subtrees in traditional GNNs, and both use a base GNN over the rooted subgraph to compute a subgraph representation as the node representation while pooling the node representations into a graph representation. Both papers claim to outperform 1 WL in expressive power; both use distance to center node in order to enhance subgraph node features. The authors are urged to compare and contrast the two papers in the camera ready version.
This paper studies the problem of how to use 3D molecular geometry information during training to improve performance during prediction time when 3D information is not available. This is a highly interesting problem as obtaining 3D molecular geometry information requires expensive calculations and such information is usually not available in practice during prediction, while there are some training data with both 2D and 3D information. The work proposes to use self supervised, predictive and generative approaches to make use of such information. The reviewers overall expressed mixed recommendations. One of the reviewers who scored 5 did not provide further feedback after author response even being prompted multiple times. The other reviewer who scored 5 actively participated in discussion and it seems most of the concerns have been addressed. Given the importance of this problem, and this work seems to be among the first to address this problem, I lean toward accept.
This paper provides a learning theoretic account of domain generalization in which domains themselves are treated as data, generated from some domain generating distribution. All of the reviewers were positive about this approach and found it interesting. There were, however, a couple of critiques raised by reviewers that lead me to recommend that it is rejected:

  the theory provided in this paper does not remotely apply to the datasets that are used in the experiments. While, I agree with one of the author responses that DG benchmarks exist with many domains, DomainBed has very few domains, and it s not clear that their theory is a remotely satisfactory account of the experimental results presented in the paper.
  Despite some back and forth on the wording and positioning of the paper, I think the writing still does not give enough credit to worst case analyses of DG.
The paper uses a transformer model to generate CNN models and use it for few shot learning.

Although the reviewers appreciate the ideas and the good benchmarking results presented in the paper they are find the paper somewhat incremental compared to previous work in the hyper network literature. This also despite the authors thorough rebuttal with additional results. This shows that the authors could have done a better job in presenting their work.

Rejection is therefore recommended with a strong encouragement to rework the paper to counter future reviewers having similar reservations.
This work proposes a concept called Populated Region Set (PRS) as a measure of robustness of deep neural networks (DNNs). The paper provides a suit of empirical results to demonstrate the strong correlation between the PRS ratio and adversarial robustness of DNNs. The authors made great efforts on addressing reviewers  concern, which is greatly appreciated. However, the theory of the work is a bit thin, and it leaves a number of outstanding issues unaddressed. For example, it is not clear the practical advantage of calculating PRS over the direct measure of robust accuracy. What new and better computational procedure can be constructed based on PRS? We encourage the authors keep improving their work for future submission.
While several reviewers acknowledge that the paper contains potentially useful ideas related to multi modal self training applied to genomic data, they also point out a number of weaknesses and room for improvement that the discussion with authors did not fully address. This includes in particular the need to better explain the details of what is done in the paper; the choice of experiments which is not relevant (eg, predicting promoter regions) or complete (eg, showing results on only one transcription factor); the lack of comparison with existing methods, etc... We therefore consider that the paper is not ready for publication in its current form, but hope that the reviews will help the authors work on a revision addressing the issues.
This paper analyzes analyze the fairness of Integrated Gradient based attribution methods. The authors exploit SHAP and BShap, two approaches based on the theory of Shapley Values, as the reference of "fair" methods. Specifically, they present an "attribution transfer" phenomenon in which the Integrated Gradients are affected by some sharply fluctuated area across the integration path, thereby deviating from the  fair  attribution methods. To avoid the attribution transfer issue, they further propose Integrated Certainty Gradients (ICG) method, where the integration path does not pass through the original fluctuated input space. Experiments are performed to demonstrate the advantages of ICG in avoiding attribution transfer. While the basic premise of the work is interesting, many conceptual details remain unclear and experimental evaluation can also be improved (please see detailed reviewer comments below). Given this, we are unable to recommend acceptance at this time. We hope the authors find the reviews helpful.
### Summary

The paper demonstrates the applicability of pruning to tabular datasets, which aren t typically explored in the literature on pruning. The work identifies that yes, pruning can indeed be applied to this domain with some success.


### Discussion

#### Strengths
  
An unconventional domain that, nonetheless, should be studied. 

#### Weaknesses

The empirical setup does not include comparisons to baselines or ablations (e.g., different importance metrics).

### Decision

I recommend Reject. Reviewer k3Jq provides a precise and constructive set of criticisms that if solved would make for an interesting and significant piece of work.
This paper has been reviewed by three reviewers, two scoring it borderline leaning towards an accept, and one scoring it as accept. The key criticism from reviewers is the lack of novelty (*the technical novelty is limited due to the adoption of existing methods without substantial changes (pseudolabeling for latent distribution, adversarial learning for domain generalization)*) and limited technical analysis (*Theoretical analysis is weak due to the use of existing conclusion from Sicilia et al., 2021*). On the other hand, authors argue that the problem they address is new (*learn the generalized representation for time series data, which is a new and challenging problem*).

As it stands, AC sees this paper as borderline leaning towards reject for the above reasons even though the evaluations are interesting.
Introducing an adversarial agent that re configures the rendered scenes of CLEVR to demonstrate that models that appear achieve super human performance are actually easily fooled due to their lack of ability to reason, provides a nice insight into limitations with existing approaches and correspondingly how we evaluate on some benchmarks.  There is a persistent concern that the results are only on CLEVR, and that the adversarial examples are not really disproving reasoning but rather issues with vision.  However, overall reviewers were generally positive about the aims the work.
The paper proposes a new method for subgraph similarity search by learning embeddings via a GNN based approach to reflect the edit distance between subgraphs. Reviewers highlighted that the paper proposes an intuitive and promising approach to an interesting problem and provides a good balance between theoretical and empirical results. However, reviewers raised also concerns regarding the significance of technical contributions, limited analysis (e.g, performance on large scale graphs, baselines, evaluation) and comparison to related work. After author response and discussion, reviewers did not come to a full agreement with two reviewers indicating weak acceptance and two reviewers indicating (weak) reject. Taking rebuttal and discussion into account, I agree with the viewpoint that the paper is not yet ready for acceptance at ICLR as it would require an additional revision to fully address the raised concerns. However, I encourage the authors to revise and resubmit their manuscript based on the feedback from this reviewing round.
Three knowledgeable referees recommend Accept. Reviewer eyrZ s concerns have been addressed by the authors in the rebuttal, in my opinion.  Therefore I recommend Accept. I ask the authors to 1) rename the title of their paper and their model to the more specific name Multi task Neural Processes (MTNP). I agree with both reviewers F6YH and ACBa that the name "Multi task Processes" does not make justice to the many other models out there that also provide ways to model several stochastic processes simultaneously. Make sure you propagate the name of the new model through the paper. 2) include a discussion in the main paper about the variability of the new results provided in the rebuttal. Only mean NLL and MSE are provided which can be misleading without standard deviations and potential tests for statistical significance.
The paper has received 5 reviews with 4 advocating for rejection (marginal or clear cut) and one borderline leaning towards a weak accept. The key concerns voiced by the reviewers are the lack of novelty (*the novelty of the proposed multi derivative architecture is limited*), the lack of comparisons with specific architectures in appropriate setting (rPPGNet without the STVEN module, DeeprPPG, RhythmNet, CVD), and concerns about the use of synthetic data (although authors provide some justifications to that end). It appears that the key to reviewers  scores is that higher order dynamics did not constitute a sufficient novelty.

Given the post rebuttal scores and discussions, AC has no option but to recommend a reject at this point.
The paper studies and compares different notions of robustness. However, reviewers found there are many unjustified claims in the analysis, and the paper does not provide novel findings nor useful approaches.
This paper presents work on multi task learning.  The reviewers appreciated the method based on SVD of loss gradients.  However, concerns were raised regarding empirical effectiveness and overall impact.  The reviewers considered the authors  response in their subsequent discussions.  While the methods are interesting, the concerns over their effectiveness would need to be more thoroughly addressed in order to improve the impact of the paper.  As such, it is encouraged that the authors take these suggestions into account in preparing a new version of the paper for a future submission.
This paper suggests using a conditional prior in conditional diffusion based generative models. Typically, only the score function estimator is provided with the conditioning signal, and the prior is an unconditional standard Gaussian distribution. It is shown that making the prior conditional improves results on speech generation tasks.

Several reviewers initially recommended rejection, but after extensive discussion and interaction with the authors, all reviewers have given this work a "borderline accept" rating. 

Criticisms included that the idea is too simple or obvious to warrant an ICLR paper. I am inclined to disagree: simple ideas that work are often the ones that persist and see rapid adoption (dropout regularisation is my favourite example). Like the authors, I believe simplicity is an advantage in this respect, rather than a disadvantage. Of course, simple ideas do require extensive and convincing empirical validation to be worth publishing at ICLR. After the authors  updates, I believe the work meets this bar.

Another issue raised by several reviewers is the limited theoretical justification for the approach. However, combined with the simplicity of the method, I believe the empirical results of the revised version sufficiently justify the approach on their own. Nevertheless, I would recommend that the authors consider further how they could address this issue in the final version of their manuscript, as they have already begun to do during the discussion phase.

Another way to strengthen the paper further would be to demonstrate how the generic approach can be applied in a different domain (e.g. conditional image generation), but I do not consider this addition necessary for the work to warrant publication.

In light of this, I am recommending acceptance.
The paper proposes an unconditional GAN that learns a set of structured keypoints as the intermediate representation. It was shown that these learned keypoints may be used to control the image synthesis output. The paper received a mixed rating before the rebuttal, with one reviewer rating the paper marginally above the bar and three reviewers rating it marginally below the bar. While a couple of reviewers commented that the keypoint idea was interesting, several concerns were raised, including the seemingly challenging tuning requirement and the usability of the proposed method. Several missing related works were also pointed out. The rebuttal addressed some of the raised concerns but not fully. While Reviewer Jmzu raised the score from marginally below the bar to marginally above the bar, Jmzu still expressed concerns about the quality of the paper. Reviewer kpkc kept marginally above the bar rating but was not impressed with the contribution. Consolidating the reviews and rebuttals, the meta reviewer found the raised concerns valid and would not recommend acceptance of the paper. The authors are encouraged to incorporate the feedback to strengthen the contribution.
This paper proposes to use deep ensembles to parameterize a variational Gaussian process posterior, and uses an additional L2 penalty on parameters of the neural networks, and an (MC) NN GP prior (although the prior is a free design choice). Reviewers appreciated aspects of the paper, finding there to be a minor improvement in uncertainty calibration over regularized deep ensembles, and nice results for the contextual bandit experiments. Ultimately, however, after a healthy and active exchange between reviewers and authors, four out of five reviewers are voting to reject the paper. There is a belief that the paper can be substantially improved from its current form, by carefully accommodating reviewer feedback, but it is not currently at a stage ready for publication.

There were common themes in the concerns expressed by several reviewers. Many reviewers found the technical contributions incremental. Parametrizing a GP using deep ensembles, or adding L2 regularization, is not itself a major technical contribution, and the variational framework leans heavily on Sun et. al (2019) and work that came before it from Titsias (2009). Similarly, the theoretical contributions were found to be incremental. 

These concerns about the technical contributions may have been counterbalanced if the experimental results had been outstanding or the framing of the paper perceived to be very clear and well justified. However, the experimental results had a mixed reception, with several reviewers noting accuracy was not in fact much better than the simpler regularized deep ensembles, despite some improvements in uncertainty calibration. One reviewer liked the bandit experiments, but wished there was a deeper exploration of this application domain. The current experimental results do not seem to warrant the relative complexity of the approach over simple regularized deep ensembles. 

Additionally, several reviewers found the framing and presentation of the paper needing significant work. The introduction of the L2 regularization terms, for example, was perceived to be overly complex, involving several steps that were not well motivated.  

Several reviewers also found the motivation about making deep ensembles Bayesian unconvincing. A procedure being sensitive to initialization, or unreliable in certain settings, does not mean it does not perform approximate Bayesian inference. For example, variational methods and Laplace approximations can depend on initialization, and could get stuck in poor local optima. Quoting papers referring to deep ensembles as non Bayesian is also not an argument in itself. The blog post linked by a reviewer is clearly pushing back against these claims, and does address points raised in the discussion, such as unimodal approximations and theoretical guarantees. As reviewers have also noted, several papers have now provided plain deep ensembles with a Bayesian justification, and these papers should be acknowledged. It could be reasonable to argue that your paper makes deep ensembles _more_ Bayesian, and you could potentially try to measure this claim in a concrete way. Or you could simply argue that your approach helps reduce sensitivity to initialization, and represents solutions with lower posterior density, which can be helpful practical contributions and don t need to be tied to claims about the method being Bayesian.

Please thoughtfully reflect on the reviewer comments in updated versions of the paper. The reviewers put a lot of effort into providing feedback and engaging during the rebuttal period. While the paper has some nice features, there is significant room for improvement on several fronts: technical innovation, experimental investigation, and framing. Improving the framing will help, but working further to also address other concerns will likely be needed to sway reviewers.
The topic and ambition of this paper has been judged as important by all reviewers. Yet there is
a consensus that the theoretical and experimental contribution is not strong enough to effectively
argue for an important novel lead which would justify publication at ICLR. For these rejections,
this paper cannot be endorsed for publication at ICLR 2022.
Description of paper content:

The paper proposes a strategy to train a “transition policy” that can connect two pre trained policies. The transition policy tries to reach state action pairs that are within the occupancy distribution of the second policy using Inverse RL. The technique was evaluated on robot manipulation and locomotion problems. 

Summary of paper discussion:

The discussion was not lengthy. The reviewers felt the paper was quite well written, instructive, and novel, yet also implied the experimental results were less systematic than might be desired. All reviewers were weakly supportive of publication and made few critical comments. The salient ones concerned the experimental domains, the number of baselines, and the question of the generality of the approach.
This authors seek to improve upon previous work on randomized smoothing for certifiably robust models. They develop loss functions inspired by the notion of distinguishing hard and easy samples while training the base classifier that is randomly smoothed and conduct experiments evaluating their proposed losses on benchmark datasets.

While the reviewers agree that the paper contains interesting ideas, the paper in its current form is unacceptable for publication because:
1) Missing large scale experiments: All prior work on randomized smoothing report results on ImageNet, and this was seen as one of the main advantages of randomized smoothing. Since the authors do not report this, it brings into question the robustness and scalability of improvements obtained.
2) Computational complexity and improvements: The authors  approach has significant computational complexity and the final improvements obtained are marginal. This makes it difficult to justify the use of a more expensive method.
The paper investigates the effect of temperature in the loss function for graph contrastive learning and proposes a novel method to dynamically adjust it during learning (GLATE). The paper is concerned with an interesting problem, is timely, and relevant for the ICLR community. 

After author response, reviewers did not come to a full agreement on the paper, with two reviewers indicating (weak) reject and two reviewers indicating (weak) accept. Reviewers highlighted the potential impact of improvements in graph contrastive learning as well as the theoretical analysis as strengths of the paper. However, reviewers raised also concerns regarding scope, novelty of the contributions, and clarity of presentation (method, evaluation, etc.). While the authors  response addressed some concerns regarding aspects of the experimental evaluation, it did not change the overall evaluation of reviewers. 

Taking author response and reviewer feedback into account, I narrowly agree that the manuscript is not ready yet for acceptance at ICLR due to the aforementioned concerns. However, I encourage the authors to revise and resubmit their work based on the feedback of this reviewing round.
The paper considers the problem of learning to carry out novel, multi task instructions specified via temporal logic using deep reinforcement learning. A specific focus of the paper is improving generalization to test time instructions that differ from those encountered during training. To facilitate this generalization, the proposed architecture encodes a latent specification of the goal according to the given instruction and environment state that is then combined with a task agnostic environment embedding. Experiments on grid like domains demonstrate that the proposed framework outperforms recent deep RL approaches to satisfying temporal logic based instructions.

The instruction following problem has long been of interest in the robotics, ML, and broader AI communities dating back several decades. The problem has received renewed attention in the last few years, largely as a target for neural network based multi view and RL learning architectures. The primary contribution of this paper is the proposed extension of existing deep RL approaches to reason over a learned, latent goal specification as a means of improving generalization to novel test time utterances. The approach is sound and several reviewers agree that the ablation studies together with comparisons to contemporary deep RL architectures support the advantage of these inductive biases. The reviewers raised initial concerns regarding the statistical significance of the results and the clarity of the presentation. The authors provided detailed feedback to the reviewers and updated the paper to address many of these concerns, largely satisfying two of the reviewers.

However, concerns remain that the paper doesn t adequately position this work in the context of the decades worth of research in instruction following. Early work in this area focused on interpreting highly structured instructions (e.g., formal logic based), first using rule based methods, and then parsers trained via supervised learning. Over the past decade, however, the field has largely moved towards learning to follow instructions conveyed in "natural" language, which brings with it a significant number of challenges, including the assumption that test time instructions will inherently be out of distribution. That is not to say that the contributions of the paper aren t interesting they are, but in the relatively narrow scope of deep RL based approaches to following structured, temporal logic instructions.
The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The proposed method is inspired by domain adaptation approaches that require a model to extract domain invariant/agnostic features from two domains. Although the experimental results are solid and technically sound, the novelty of the methodology is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation such as "unsupervised domain adaptation by backpropagation". On the other hand, more recent SOTA methods are missing and only smaller scale datasets are used for evaluation. During the discussions, the major concerns from three reviewers are novelty. 

I totally agree that the simplicity of the method should be a virtue. However, the idea of domain invariant representation learning is already established well, and its application to adversarial training is quite intuitive to the community. Also, the similar methodology already exists in domain adaptation. According to the top tier conference culture in the ML community, what most valuable is the novelty and insight, not the performance. In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper.
Discussions and additional baseline experiments added during the author response period were enough to motivate multiple reviewers to change their recommendation to an accept during the author response. Multiple reviewers felt that the technical novelty of the work was limited, but the rebuttal cleared up their concerns enough to motivate them to switch their assessments to accept.

The claim of this work is that it provides a simpler, sparser, and faster algorithms for differentially private fine tuning of LLMs, yielding SOTA privacy results vs. utility on a number of standard NLP tasks. The work proposes a meta framework. 

In the end, all reviewers rated this paper as an accept and the AC also recommends acceptance.
*Summary:* Low rank bias in nonlinear architectures. 

*Strengths:*  
  Significant theoretical contribution. 
  Well written; detailed sketch of proofs. 

*Weaknesses:* 
  More intuitions desired. 
  Restrictive assumptions. 

*Discussion:* 

Authors made efforts to improve the discussion in response to 6P7z. Authors agree with eeoo about Assumption 2 being relatively restrictive but point out that main results do not need it. They discuss Assumption 1 and revised it formulation. Reviewer eeoo was satisfied with this. Following the discussion udhX raised their score (after authors acknowledged an early problems and improved them) and found the paper well written with novel and significant results. 

*Conclusion:* 

Three reviewers consider this a good paper that should be accepted. A fourth reviewer rated it marginally above the acceptance threshold but following the discussion period explicitly recommended acceptance. I find the topic interesting, timely, relevant. In view of unanimously favorable feedback from four reviewers I am recommending accept.
The paper proposes the use of a state distribution estimation objective with a classic behavioral cloning objective for imitation learning. 
The submission also proposes the use of a continuous normalizing flow training technique coined "denoising normalizing flow" to learn the state distribution. The authors experimentally validate their method on several MuJoCo continuous control benchmarks.
The theorem 4.1 does validate the fact that this proposed objective is can be maximized by the target policy.
However, the technical contributions (proposal of new objective and the denoising normalizing flow method) are marginal compared to previous work (e.g., SoftFlow or Energy Based Imitation Learning). 
The empirical validation is lacking more extensive comparison with PWIL or NDI, which are more recent methods attempting to address the challenges described in the submission. 
I m recommending this paper for rejecting for this conference.
Four reviewers have reviewed this manuscript and two found it borderline leaning towards acceptance, two other reviewers scored it below the acceptance threshold. While the authors *identify the key challenges and bottlenecks in 3D point cloud model*, the most positive two reviewers notice that the depthwise kernel and the attention mechanism (and similar tools) are well known in the literature and that this work is *more of an engineering improvement than a technical contribution*, which erodes the novelty of the proposed idea on that front. While authors noticed some discrepancies in the numbers quoted by Rev. 3, the model gains are also nonetheless modest compared to other models. Overall, the feeling amongst the reviewers was that  the presentation of NK attention could be further improved and that the paper uses very heavy machinery to achieve results comparable with SOTA.

On this occasion, the manuscript is below the acceptance threshold with even the borderline positive reviewers having their doubts about clear cut technical novelty.
This paper demonstrates that deep networks can memorize adversarial examples of training data with completely random labels, which motivates some analyses on the convergence and generalization of adversarial training (AT). The authors identify a significant drawback of memorization in AT that could result in robust overfitting and propose a new algorithm to mitigate this drawback. Experiments on benchmark datasets validate the effectiveness of the proposed algorithm. One of the reviewers is concerned about (1) the validity of stability analysis where 80% of the data labels are noisy, and the perturbation (64/255) is large, (2) the gap between theory and practice, and (3) novelty. The authors have made a great effort to address these concerns. Although there is still no consensus after the author s response, the majority of the reviewers are in strong support. I, therefore, recommend acceptance.
The paper extends the previously established connection between adversarial training (AT) and Wasserstein distributional robustness (WDR) to other adversarial defense methods such as PGD AT, TRADES and MART, and connects them to WDR. While this connection itself is not surprising given earlier works connecting AT and WDR, the paper makes contributions in establishing it formally and proposing algorithmic variations (eg, softball projection) that show clear empirical gains on standard benchmarks of MNIST/CIFAR10/CIFAR100 over point wise adversarial defense methods.
The paper studies the problem of how to construct orthogonal convolutional layers. It is known that a convolution layer is orthogonal if and only if its filters are obtained by certain Fourier operations on an orthogonal matrix. Previous work proposes to learn this orthogonal matrix, parameterized either through Cayley transform, or the exponential of a skew symmetric matrix. This requires spectral computations with large matrices. The idea of this submission is to reduce the computational cost associated with this construction by letting this “core” matrix P be a periodic extension of a smaller orthogonal matrix P_0. Because of cancelations in the inverse DFT, this leads to sparse filters which can be implemented by dilated convolution. 

The review process generated a very detailed discussion between authors and reviewers, with several important clarifications. Reviewers generally found that the paper contributes a novel construction of orthogonal convolution layers, with better efficiency at test time. Remaining concerns held by some reviewers include the limitations vis previous constructions of orthogonal convolution layers, questions about the efficacy of use of a Taylor expansion, certain minor limitations of the experiments. After detailed interaction with the authors, the reviewers converged to a decision to accept, motivated by the novelties of the construction and its advantages for test time efficiency.
This article introduces an interesting variant of the work of Nakkiran & Bansal (2020). It shows empirically that the test error of deep models can be approximated from the disagreement on the unlabelled test data between two different trainings on the same data. The authors then show theoretically that a calibration property can explain such behaviour, and they report experiments showing that the relationship does exist in practical situations.  All reviewers agree on the practical and theoretical value of the article, which is very well organised and written. The ideas developed here are likely to lead to further work in the future, and they clearly deserve to be published at ICLR.

I agree with one of the reviewers that the title is somewhat misleading, as the reader expects an analysis based on SGD. The title could be shortened to "Assessing Generalization via Disagreement", and the experimental restriction to SGD could be mentioned in the abstract.
The authors introduce a modification to CQL to use a weighting based on density estimates. In an idealized setting, they show that the estimate Q values bound the true Q values. Finally, they evaluate their proposed approach on a few benchmark offline RL tasks.

Generally, all reviewers felt that the results were too incremental. The theoretical result follows with light modifications from the CQL paper and even then, the implications of the result are unclear. The experimental results showed small improvements or comparable performance while requiring training a density estimator and introducing an additional hyperparameter. Furthermore, the set of tasks evaluated was limited and no comparisons to other methods than CQL were shown. 

While I appreciate the effort the authors took to investigate this improvement, at this time, the paper falls below the bar and I recommend rejection.
The paper proposes an approach to performing label smoothing, with the amount of smoothing being sample dependent and guided by the model s prediction (similar to self distillation). While the reviewers find the studied problem relevant and important, they find the contributions (in their current state) to be borderline, mainly on the basis of lack of novelty and missing discussion with some related papers. While authors  response was able to partially resolve these concerns, at the end none of the reviewers was a strong advocate for accepting the paper and all scores remained at the borderline (although on the positive side). In concordance with the reviewers, I believe this submission can be made much stronger by digging a bit deeper into the problem, and also making broader connections with the existing literature.

As a concrete example/suggestion (among many other possibilities for strengthening this work), the authors may want to go a bit deeper into the theoretical analysis. Currently, their analysis shows the approach is able to reduce model s confidence, which is what happens in label smoothing and self distillation. However, self distillation is more than confidence reduction, and the information contained in the "dark knowledge" can provide a much stronger regularization than a sole confidence reduction argument. There are already some papers in the literature on the regularization/generalization effects of self distillation, which the authors might want to use as a stepping stone.
This paper examines the evolution of densities of initial conditions under the multiplicative weights update rule for learning in two player zero sum games. Specifically, the authors estimate the differential entropy (DE) of a density of initial conditions as it evolves over time (what they call "uncertainty"), and they show that (a) as long as the density of states assigns sufficient mass to all strategies, its DE will increase; and (b) the density of states will get arbitrarily close to the boundary of the state space infinitely often (i.e., at least one pure strategy will be employed with arbitrarily small probability infinitely often). The authors also apply these results to a population like model of learning as well as an optimistic variant of the MWU protocol (the latter in the supplement).

The paper was extensively discussed during the review/rebuttal phase. While the reviewers appreciated the conceptual contributions of the paper, they also identified certain technical shortcomings that were only partially addressed by the authors. One of these issues concerned the possibility that the density of initial conditions may exhibit singularities, in which case the DE may fail to be well defined. As a result, one of the reviewers indicated an intent to downgrade their score from "8" to "3" due to concerns on the correctness of the results presented in the paper.

After discussing with both the authors and the reviewers, my view is that the merits of the paper outweigh its flaws, so I am making an "accept" recommendation. At the same time, there is a number of revisions that the authors will have to undertake in the camera ready version of their paper:

1. The authors need to be more careful with their assumptions and notation. The reviewers already indicated a number of glitches, most of them easily fixable (so they are not of particular concern). On the other hand, the issue of whether the initial density of states becomes singular or not is more subtle and led one of the reviewers to drastically change their evaluation of the paper.

  The problem here is that the authors are not being precise in their assumptions for $g^1$ and its support, and this confusion remained throughout the discussion: the authors are looking at distributions that are "smooth with bounded support", but this does not exclude singularities. The counterexample given to the authors was a random variable $X$ supported on $\mathcal{X}   (0,1)$ with density $g(x)   1/(2\sqrt{x})$; this density has bounded support and it is smooth on its support, but it is not itself bounded. [There is an ambiguity here in whether the authors are considering the support to be closed or not.]

  The issue for the initial density can be trivially fixed by asking that $g^1$ be itself bounded (or smooth over the closed support, or any other similar statement). However, even if this is assumed for $g^1$, the density at some later time $t$ could, a priori, become singular (incidentally, this is a problem that arises frequently in the study of densities that evolve over time, e.g., as in optimal transport). Thus, even an explicit assumption for $g^1$ does not suffice to ensure that $g^t$ does not develop singularities in future stages. [Incidentally, the authors  reply that the singularity has measure zero and therefore does not contribute to the integral misses the heart of the matter (and raises concerns about the authors  overall treatment of this question): the function $g(x)   (\log 2) \big/ (x \log^2x)$ has infinite differential entropy over $(0,1/2)$ even though it is a smooth density over $(0,1/2)$.]

  To be clear, I do not believe that blow ups actuall occur in the authors  model, but there is still something that needs to be shown here. However, since it is impossible to check an argument or proof at this stage (and I do not think it would be fair to let this stand in the way of accepting the paper), the authors should instead revise their paper to add as an **explicit** assumption that $g^t$ has bounded support and is bounded over its support (or clarify whether they take the support to be closed or not).

1. Another concern revolves around the use of the word "uncertainty" to describe the basic premise of the paper. In the authors  model, this does not refer to uncertainty among the learners (all their observations are perfectly certain and deterministic), so it is not used in the sense that is standard in game theory and learning (cf. the classic works of Bertsekas, Dekel, Fudenberg, Tsitsiklis, and many others). Instead, the authors  use of the word seems to refer to some "outside spectator" who can only partially guess the players  initial conditions, and tries to guess the evolution of the players  mixed strategies (but still has full information about the learning model that players use, its parameters, etc.). However, this model is not fleshed out in sufficient detail by the authors, so the term "uncertainty" does not seem appropriate here.

  During the rebuttal phase, the authors argued that the goal of their paper is "bringing the notion of DE to machine learning audience s attention as a measure of uncertainty, explaining how the change of DE is related to the Jacobian of the underlying dynamical systems" and they asked "that [the paper s] title remains as is". While I am sympathetic to the authors  request, the fact remains that the current title (and part of the discussion in the abstract) is not representative of the paper.

  Given the authors  stated objective, the simplest solution would be to frame the paper as the "evolution of differential entropy under..." or the "evolution of spectator/observer uncertainty" or something of the sort. Both titles carry more information and, based on the authors  input, are more appropriate for the range of ideas the authors wish to convey – but simply saying "uncertainty" goes against the established terminology of the field.

Overall, I would urge the authors to avoid vague/ambiguous terminology and statements, and focus instead on exact mathematical definitions that are not open to interpretation. The ideas presented in the paper are interesting and fresh, so they deserve a likewise sharp and precise treatment.
The authors propose a random perturbation on top of a soft top k operator that builds upon entropic regularized optimal transport (when applied to a 1D problem). The motivation of the paper is built around an approximation bound (proposed in the Xie et al  20 paper) that compares the true OT matrix from the regularized OT matrix in the case where some of the 1D entries from which one wishes to extract top k values are very close (eg. x_{t} ~ x_{t+1}). The authors argue that this bound, with inverse dependencies in the closest element in the list, diverges.

The authors state that this possible divergence is an issue, because values to be sorted/top ked can be very close in practice. To solve this issue, the authors introduce instead a Gumbel noise mechanism that no longer makes the bound diverge, through a fairly long theoretical analysis. The approach now requires the recomputation for several noisy inputs of the same regularized OT estimator. The authors propose then to use these soft top k approaches to solve a combinatorial problem using gradient descent, namely a capacity constrained problem and clustering, including some tricks on controlling both entropy regularization and Gumbel noise magnitude.

The paper has generated a long discussion among the AC and reviewers. While the paper has a few strong points that were appreciated (interest of empirical validation which seems to suggest some improvements over commercial solvers on considered setups), there remain a few issues. 

The theoretical side of the paper is bit blurry. The idea of introducing Gumbel noise on top of an already soft operator is not completely clear, since these perturbations are there to add differentiability to something (reg OT) that was introduced itself to be differentiable. The theoretical motivation is unclear: the noise is introduced because the _upper bound_ diverges (and not the gap between the "true" OT and entropic OT, since it is always bounded). The perturbation mechanism is only motivated to improve the limitations of an upper bound, not of the original algorithm itself. What s more, it s not entirely clear why that gap should be decreased (between true and regularized OT) since it has to exist to obtain some differentiability. While the study of the gap itself was added during the discussion phase in Fig. 1"A toy example to explain Lemma 2", one would expect better foundations for this idea.

With a somewhat unclear theoretical motivation, the experiments should be very convincing. Reviewers have noted some issues related to comparing CPU/GPU times. While I am sympathetic to the problems encountered by the authors when running such comparisons, these issues should be properly reflected in their initial claims, and not appear in the rebuttal only. I also think experiments are still lacking in diversity. For instance, the k means problem is studied in 2D (begging the natural question of whether such an improvement would remain in higher dimensions). I could not find a clear statement on the number of repeats carried out to obtain error bars. Since I don t envision either of the max covering problem nor k means to become the "killer app" of this paper, I would encourage the authors to consider problems that are less synthetic.
The paper addresses open set DA, where samples from novel classes in the target domain get clustered 
into new (unlabeled) classes. A key novelty in the learning setup is that it is assumed that one 
has access to a knowledge graph over classes (both source and target). That KG is used for grouping 
target samples into novel classes. 

Reviewers were concerned that the method is not explained with sufficient 
details and the experiments lacked comparisons with openset DA baselines. 
No rebuttal was submitted. 

The paper cannot be accepted to ICLR.
In this paper, the authors established interesting theoretical results regarding the behavior Graph Neural Tangent Kernel (GNTK). They also provide sufficient evidence (some of which during rebuttal) that their approach is valid. We have had many discussions and I suggest that the authors apply reviewers  comments to the final version of their paper.
This paper introduces a multi domain self supervised representation learning method. Its objective consists of three terms: the first two terms are identical to SimCLR and the last one is to minimize the similarity of pairs across different datasets which is similar to the second term of SimCLR. In the experiment, it tests the methods across multiple common datasets. The method is simple but results are pretty good at the multi domain setting. It seems to demonstrate the importance of domain clustering and moving the domains apart. However, there are several important questions the paper may need more clarification on: 
1. What is the definition of the domain? How to determine the pair of data is from different domains? What is the motivation/theory that you used to choose those datasets as different domains in your experiment? 
2. Is there any of the public datasets that would cover multiple domains? 
Without solving these questions, I think it would constrain the future research/adoption of the method.
## Description

The paper applies ideas from contrastive representation learning to train binary neural networks. Namely, the algorithm promotes binary representations to be similar to the full precision representations while at the same time it promotes binary representations to be dissimilar from full precision representations corresponding to other input images. This is enforced for activations in all layers by the added contrastive loss (9).

## Decision

The main weakness of the paper pointed by reviewers were 1) overlap of the large part of derivation with the prior work [25] Tian et al. "Contrastive representation distillation", ICLR 2020; and 2) the meaning of the derivation when applied in the setting of the paper to binary and full precision weights and its soundedness. The authors proposed their arguments for 1). The reviewers board considered these arguments and did not agree (see below). Point 2) was not addressed by authors (no paper revision, justifications, proofs corresponding to the missing supplementary). It was discussed further and was found critical (see below), such that it is a clear reason for rejection regardless of 1). Overall, the idea is interesting and the method appears to be helpful experimentally, however the paper needs a major revision that would address the two points.

## Details

### Overlap with CRD

Reviewers were in a consensus on this issue, disagreeing with authors. Since the whole derivation chain of the contrastive loss already exists in the CRD work [25], it is redundant to repeat this derivation if not raising ethical concerns. Instead an original work should review or just refer to the existing derivation and only discuss the new context and e.g. change the critic function $\hat h$. 

### Meaning of the derivation 

The reviewers have questioned the soundness of the initial criterion of MI between binary and full precision activations, as it reduces to just the entropy of binary activations. In particular, it seems very different in meaning to the contrastive loss the paper optimizes in the end. Here is additional feedback from the discussion.

1. Maximizing the entropy of binary activations with respect to the data distribution makes some sense. If a single binary activation was considered, its entropy is maximized when it is in the state 1 exactly for 50% of the data. Which makes it discriminative of the input. A similar centering can be achieved by Batch Normalization put in front of the activation   if the preactivation distribution was symmetric, then BN would achieve the max entropy for the sign of preactivation. Such network design is not uncommon.
Maximizing the entropy of the full vector of binary activations appears more difficult. However we can also understand it as the mutual information between the input image and the layer of binary activations. Thus the criterion is to retain as much information about the input as possible. This makes sense as a regularization (often neural networks are regularized by adding data reconstruction capabilities / loss), and is aligned well with goals such as re using the features for other tasks (as in Sec .3.5) but contradicts to some other principles proposed in the literature, e.g. the information bottleneck (that the maximum information about the target rather than the input should be preserved).
Amongst methods that study the direction of maximizing the entropy in binary networks, reviewers mention IR Net and Regularizing Activation Distribution for Training Binarized Deep Networks. The architecture with BN before activation is used in the latter work and some more recent works, e.g. BoolNet.

2. It is not clear whether optimizing the contrastive loss retains the same meaning as maximizing MI. The derivation from CRD paper used here applies several lower bounding steps. Maybe the strongest one is that the critic is chosen to be of a specific function rather than a universal approximator. However there is no obvious gap. In fact knowing that binary activations are just a sign mapping of full precision ones, should allow one to estimate $p(i j| a^i_B, a^j_F)$ in a simple way.

3. In the estimator $h$ in (8) the authors make a mistake (applying their and CRD theory incorrectly):
$h$ should be the probability of a conditional Bernoulli variable estimating $p(i j| a^i_B, a^j_F)$. It should not depend on $a^j_F$ for other values of $j$ than the given one. However in the denominator in (8) it does. Therefore this estimator, and as a result the specific NCE loss proposed, appear unjustified. If the critic from CRD eq. (19) is adopted, it is not clear whether it makes sense for a pair of binary and full precision descriptors (note that for $i j$ the scalar product between the two is just $\|a_F\|_1$).  It seems that the design of a meaningful critic is a serious gap the authors should address. Observing that the initial objective, the MI criterion, was in fact independent of full precision states (as it is the entropy of binary states), one can propose that an appropriate critique should use binary states only, such as
$$
h(a_B^i,a_B^j)   \sigma(\left<a_B^i, a_B^j\right>  + c ).
$$
When fixing $\hat h$ the result in (10) that the maximum likelihood estimator for $p(i j | a_B^i, a_F^j)$ with a generic neural network can approximate this distribution arbitrary well becomes irrelevant.

When the paper speaks of randomness, e.g. "binary and full precision activations as random variables, considering "i j" as a random variable, it is needed to specify the source of randomness or the distribution, i.e. to add "for a network input drawn from the data distribution" in the first case and "under i and j picked at random uniformly in the batch" in the second.

Theoretically, the paper would become more convincing, if the the entropy of binary activations was measured by independent tools from the literature after training with and without NCE loss and it was shown that indeed the method achieves an improvement in this objective, reconfirming that the principle and the derivation were sound. An ablation study on other modifications such as weight decay may be helpful to convince researchers that the main source of improvements in experiments is the new contrastive loss. Note that not all reviewers were convinced by current experimental results due to lack of descriptions / code to fully reproduce and or lack of such ablation studies.
While the main idea of the paper (using a Max Ent objective on the states of an MDP) was considered interesting, all reviewers raised the problem of clarity of the paper which needs to be drastically improved. While the writing could be improved by the revsion, these concerns could also not be fully alleviated by the rebuttal of the authors. The reviewers agreed that the paper needs rewritting in order to clarify the contribution before the paper can be published.
This paper proposes an extension to learning a representation: it motivates, proposes and evaluates a new regularizer term that promotes smoothness via enforcing the representation to be geometry preserving (isometry, conformal mapping of degree k). Comparisons with a standard VAE and FMVAE (Chen et al. 2020) are shown and experiments are provided on CelebA with several different attributes as target classification tasks. 

The paper has received extensive reviews and the authors have successfully answered most of the concerns raised, mostly regarding comparisons to other techniques that try to introduce a regularization based on the properties of the Jacobian of the decoder network. 
The appendix has been extended as a result of the rebuttal and the paper could be accepted.

Notes: 
I find the formulation based on the notion of the isometric decoder somewhat surprising as the encoder is a key object of interest that controls the nature of the representation. The authors should clarify the assumption 3 in 3.3 better by the consideration of potentially $dim(z) << dim(x)$, how the isometry of the decoder effects the encoder, 

Additionally, for the latent space flattening an ablation using SVD (merely a linear mapping for $i(\cdot)$) could be considered.

Reviewer ZGHS has noted that they raise their grade to 6 in their comment, but this is still not currently reflected.
All of the reviewers recommended rejecting this paper.
There were concerns that the underlying research questions being probed were not expressed clearly enough.
Reviewers were concerned that the experimental work was not sufficient to warrant acceptance.
Other concerns included the technical depth of the paper, the degree to which related work was discussed, placed in context and compared with empirically.
The AC recommends rejecting this paper.
The authors provide an interesting improvement on privacy attacks in federated learning, demonstrating the ability to extract individual points even over large batches. While there were some concerns about the technical difficulty of the approach, reviewers were broadly in support of the work. As I tend to agree, this is an interesting strengthening beyond what it appears we were able to do before. This is yet another piece of evidence against the canard in FL that only sharing gradient updates provides privacy guarantees.
This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back translation setup, and it puts together together a number of pieces in an interesting way: text to text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a bit beyond what experiments support, e.g., in the response to zd7L about applicability to COBOL.

All in all, though, it s a good implementation of an idea that should have a lasting place in this line of work, so it s worth accepting.
The authors develop a technique for unsupervised learning of neurosymbolic encoders. Some of the difficulty with the paper came from the accessibility to a broader machine learning audience, though there is related work such as Shah 2020 in machine learning. The other difficulty came from the experiments: there was both a question about the metrics and the task. Quoting a reviewer

"Current evaluation seems not very convincing to me. The authors only show that with the help of symbolic program, the method could get representations with better cluster quality (program helps representation learning). But I think a more intersting perspective is to see whether the learned program itself is helpful. For example, whether it could be used to predict future trajectory (such as 3 body problem), or even help solving some high level reasoning tasks."

and another reviewer 

"Maybe something comparing the programs of experts to what the latent representation learned?"

Making the paper more accessible and improving the experiments will improve its quality.
This paper introduces the concept of classifier orthogonalization. This is a generalization of orthogonality of linear classifiers (linear classifiers with orthogonal weights) to the non linear setting. It introduces the notion of a full and principal classifier, where the full classifier is one that minimizes the empirical risk, and the principal classifier is one that uses only partial information. The orthogonalization procedure assumes that the input domain, X can be divided into two sets of latent random variables Z1 and Z2 via a bijective mapping. The random variables Z1 are the principal random variables, and Z2 contains all other information. Z1 and Z2 are assumed to be conditionally independent given the target label. The paper outlines two approaches to construct orthogonal classifiers that operate only on Z2. The approach is highlighted in three applications: controlled style transfer, domain adaptation, and fair classification.

The reviewers all found the proposed method to be principled and compelling. Beyond clarification questions and some discussion on related work, the reviewers raised a few issues that were subsequently addressed: 1) Additional baselines for domain adaptation and fairness. 2) Controlled style transfer being a new task with no established baselines, and 3) The feasibility of training a proper “full classifier” that minimizes the empirical risk, and its necessity in the approach. The authors addressed these concerns and updated the paper, to the satisfaction of the reviewers. All of them unanimously recommend acceptance.
In this paper, authors introduce two properties of feature representations, namely local alignment and local congregation, and show how these properties can be predictive of downstream performance. The paper has a heavier focus on providing theoretical statements using these properties but authors also empirically evaluate their suggested method.

**Strong Points**:
  The paper is well written and easy to follow.


  The proposed concepts (local alignment and local congregation) are intuitive.


  The theoretical statements and their proofs are correct.


  The proposed metric shows some advantage against a few baselines.


  Prior work on feature representations and transferability are discussed.


**Weak Points**:


  **The connections to prior work on K nearest neighbors and linear classifiers are not properly discussed.** This is very important because authors assume that the network that outputs the feature representations is trained on a different data and they reduced the analysis to that of a binary linear classifier. Hence, all classical learning theory results on binary classifiers apply in this setting. Furthermore, KNN methods and analysis can be simply applied on the features as well. In light of this and the lack of discussion on this matter, the significance of the theoretical and empirical results are not clear.


  **The main proposed properties could be improved further**. It looks like the defined properties (local alignment and local congregation) could be improved by merging them into one property about separability of data? The current properties are sensitive to scaling which is undesirable given that classification performance is invariant to scaling of the features. It seems like local congregation is mostly capturing the scale so some normalized version of local alignment might be able to capture the main property of interest.


  **The theoretical results in their current form are not very significant.** One limiting factor on the theoretical results is that since the analysis is done only on the classification layer, it does not say anything about the relationship of the upstream and downstream tasks. But perhaps the most important limitation is that the properties are defined based on the downstream task distribution as opposed to downstream training data. That makes it difficult to measure them in practical settings where we have a limited number of data points. Classical results on learning theory avoid this and only use measures that depend on the given training set.


  **The empirical evaluation could benefit from stronger baselines** Authors mentioned "We therefore consider only baselines that make minimal assumptions about the pre trained feature representation and the target task" and hence avoided comparing to many prior methods. However, I think the appropriate approach would be to compare the performance of the proposed method to strong baselines but then explain how they differ in terms of their assumptions, etc. Moreover, there are other simple heuristic baselines to consider, eg. K NN (which is not computationally expensive in the few shot settings) or a classifier that is trained by initializing it to be the sum of feature vectors in the first class (assuming binary classification) minus sum of feature vectors in the second class and doing a few SGD updates on it. Therefore, I believe authors could improve the empirical section significantly by taking these suggestions into account.


**Final Decision Rationale**:

This is a borderline paper. While the paper has a nice combination of theoretical and empirical contributions, both theoretical and empirical contributions have a lot of room for improvement (and a clear path to get there) as pointed above. In particular, I believe having either strong theoretical contributions or strong empirical contributions would have been enough for acceptance and I hope authors would take the above suggestions into account and submit the improved version of this work again!
This submission received four high quality reviews. After the discussion period, all reviewers agreed that this submission is not strong enough to be accepted. Concerns include the novelty of the proposed method wrt related work and the limited experiments. The AC agrees. The AC also finds it disappointing that the authors didn t address the concerns on novelty or even discuss the related papers suggested by the reviewers in the revision. 

The recommendation is reject.
This paper propose a way to make minibatch Optimal transport (m OT) more efficient by computing an optimal assignment (in the OT sens) and us this assignment to compute instead a hierarchical OT loss (bomb OT ) that can be used instead of the m OT loss. The authors discuss how the equivalent OT plan with bomb OT is much more sparse, and how the proposed approach is actually not biased when the number of mini batches $k\rightarrow \infty$ . Numerical experiments  show that the proposed method allows a gain in performances in applications such as generative modeling, domain adaptation, color transfer and approximate Bayesian computation.

The paper originally got borderline negative scores from the reviewers. While the reviewers acknowledged that the idea is interesting, they had some concerns about the theoretical results strength, some missing baselines and discussions in the numerical experiments. The authors did a detailed reply that clarified some problems. the new numerical experiments with m UOT were also greatly appreciated by the reviewers but they also raised some questions about the paper. Some concerns detailed below about the comparison with m OT appeared during the reviewers discussion. Despite the new information,  the reviewers reached an agreement that this paper is interesting but needs more work and another round of reviews before acceptance. For theses reasons  the AC recommends a rejection for this paper.

More details and suggestions below:

  While it is clearly not the objective of the paper a discussion about the proximity of the average plan to the exact OT plan is interested. Also a short numerical experiments showing that the bomb OT average plan is closer to the exact plan than m OT would be a good illustration of the better performance of bomb OT. This seems more important for the paper than the color transfer experiments that is kind of a toy problem.

  After checking the definition in the paper and discussion between reviewers it appeared that the comparison with m OT is a bit unfair due to the reformulation of the problem in (1). indeed in the usual formulation, k pairs of independent  minibatches are used and the OT is done on those pairs (a sum of k OT) not on all the possible pairwise permutation as in definition of m OT in equation (1). In other  words in m OT the batches are supposed to be independent which is not the case  in the proposed formulation (it is equivalent in the population case though).  It means that in practical application, for the same computational complexity  (k^2 OT computed), m OT actually uses $k^2m$ independent samples on each distribution  whereas the bomb OD (and the m OT defined in equation (1) ) use $km$ samples . By implementing m OT as  in (1) they actually prevent m OT to explore the dataset as its original  formulation does. This means that all the experiments should be done either with the original m OT implementation of both the original and (1) in addition to bomb OT. The proposed method will proably work better but the current experiment do not allow this fair comparison. 

  The theoretical result need more discussion and justification.  For instance  m OT converges to its population value in 
 $O(m^{1/2}n^{ 1/2}+k^{ 1/2})$ that is independent from the dimensionality  $d$, but the authors prove the  concentration of bomb OT in  of $O(m^{1/2}n^{ 1/d})$  which is  clearly a problem for large $d$. Also the dependence on  $k$ of the convergence would be important since  bomb OT is well defined is true only in the population case where $k$ is  large. Note that the claim that it is well defined and hence better is also a  bit dubious because it is well defined for $k \infty$, which is also the case for m OT when $m \infty$. Both $m$ and $k$ large will lead to not practical optimization problems so they are comparable except that m OT converged to the true OT plan when $m\rightarrow \infty$ which is not the case for bomb OT.

  While the contribution of the paper in indeed a methodological method and does not require to be state of the art on all applications the numerical experiments should be improved. First as discussed above the comparison with m OT is actually unfair an do not correspond to what in done in practice (where all mini batches are independent). m OT should be implemented with  $k^2$ truly independent minibatches.

  Second , the authors use approximate W2 on two of  the GAN dataset and FID on the third. This is  problem because approximate W2 is not defined in the paper. FID is the standard performance measure and should be used for all dataset. 

  Third the novel experiments comparing also raises a lot of questions. m UOT is far better than BoMb OT suggesting that Unbalanced OT can compensate for the limits of m OT far better than bomb OT itself. Yes there is a slight increase in performance  for ebomb UOT over m UOT but is is so small (0.08 %) that it is hard to find them significant, especially since we have no variance. This result that is provided only for DA application actually  suggest that the competitor of bomb OT is m UOT and not m OT so it should also be part of the comparison in the other experiments. The authors talk in their replay about the limits of m UOT but stating that the experiences are not done in the paper is not an excuse for evaluating this clear competitor on other problems and showing numerically these limits.

  Finally in the current version of the paper puts a lot of things in the annex that make the paper clearly not self content. Some experiments could go in annex/supp for instance the color transfer to make place for more details in the main paper. 

Note that it is not one of those comments above that lead the the reject decision but the sum of them that clearly show that the paper needs more work.
The submission considers a stochastic variant of the projective splitting algorithm, with a focus on monotone inclusion problems, and it proposes a novel separable algorithm with the ability to handle multiple constraints and non smooth regularizers.  All reviewers felt that there were merits to the submission and that the submission was borderline.  Public and non public discussion concluded that the paper would be of greater value to the community if the suggestions of the reviewers and related issues were addressed.
The paper contributes to the understanding of out of distribution detection by showing that binary discrimination between in  and out distribution examples  is equivalent to several different formulations of the out of distribution detection problem . The paper shows this in an asymptotic setup based on studying likelihood ratios for distinguishing in distribution examples from out of distribution examples. The paper also provides numerical results showing that a simple baseline based on binary classification works well.

The paper got very mixed responses ranging from strong accept to reject:
  Reviewer YhZ7 (recommending 3: reject) raises several important concerns, specifically that the paper doesn t explain the significance of its contributions adequately, that experiments are not thorough enough (for example that only one out of distribution dataset is considered), and that to train a binary classifier one needs to have sufficiently many out of distribution examples. 
The authors argued in response that the purpose of the paper is to provide an understanding of existing methods that are often empirically driven, made revisions to the exposition, and point out that they actually evaluate on six/seven out of distribution test sets. 
After discussion, the reviewer is still concerned that the paper states  We show that when training the binary discriminator between in  and out distribution together with a standard classifier on the in distribution in a shared fashion, the binary discriminator reaches state of the art OOD detection performance  as a contribution and that this claim is not supported by the results in the paper. The authors say they are happy to drop this particular statement and emphasize that their contribution is that that a binary classifier can be a useful tool for OOD detection. The reviewer is not satisfied by this response, as the reviewer feels that this makes the contribution much less impactful. 

  Reviewer iH61 (recommending 6: marginally above, initially reject) pointed out that the significance of one of the contributions is limited, since the claims resemble the ones by Thulasidasan et al. [2021] and Mohseni et al. [2020], and initially recommends to reject. The authors respond that those two papers only aim at good performance, but do not unify existing approaches, as the paper under review does. The reviewer slightly raised their score, but again points out that the previous works already show that a binary discriminator performs well. 

  Reviewer Lwwq (recommending 10: strong accept) appreciates the unification of different methods and votes for strong acceptance. The reviewer also points out that he/she is not an expert in the field, and thus this reviewer s rating should be taken with care. 

  Reviewer YRfA (recommending 8: accept) points out that the authors make notable progress towards a better understanding of OOD methods, but is concerned about what problem the authors are trying to solve and its significance, and states that he/she cannot judge the importance of the paper.

  Reviewer vYWv (recommending 6: marginally above, initially recommending reject) finds that the paper provides helpful insights to connect methods for OOD detection tasks, and weakly recommends acceptance.  

The reviewer s opinions on this paper vary significantly. Initially, a major selling point of the paper was that  the binary discriminator reaches state of the art OOD detection performance , but after discussion, the authors and reviewers agree that this statement is not supported by experiments, and the idea of using a binary discriminator is also not new, and thus everyone agrees that this statement should be removed. 
This leaves as the major contribution an improved understanding of a variety of methods, and casting them as versions of a binary classifier. 
This by itself would be sufficient to carry a paper, however the stated equivalence is rather weak as it is based on an asymptotic analysis, and in the asymptotic regime, out of distribution detection is rather trivial because the distributions are given. This also explains why in the paper s experiments all the methods that are asymptotically related behave quite differently in experiments. 

I do not recommend this paper for acceptance. I ve read the paper and I ve thought quite a while it and its reviews. I have also discussed the paper with a colleague who works actively on out of distribution detection, since I m not an expert on this topic myself. While in general I find it very valuable to unify and to understand existing out of distribution algorithms better, I don t see how the particular interpretation provided by the paper is impactful, since it is unclear how the connection drawn in an asymptotic setup for Bayes classifiers actually extend to concrete OOD detection algorithms, which operate in the finite sample regime.
This paper provides well written and thorough analysis demonstrating that closed set recognition performance correlates with open set recognition performance, and that simply making the close set model strong via augmentation, label smoothing, etc. along with small scoring changes (using logits rather than softmax probabilities) can get close to (or better than in some cases) performance than much more complicated methods. The authors also propose a large scale benchmark that varies the semantic similarity across classes, allowing for a more fine grained analysis of this problem. 

Overall, all of the reviewers thoughts that the paper provides very thorough validation of an insight that would be very interesting to the community. Reviewer HAFU had some concerns about novelty, since a number of papers have shown closed set classifier improvements (and therefore better embeddings) benefit related problems such as few shot learning and generalization to novel domains, as well as proposed large scale experiments. The rebuttal convinced this reviewer, however, that some of the contributions and findings are unique and provide additional evidence to the community, and the new setting provides more fine grained analysis. Reviewer dw7J had a number of suggestions in terms of additional evaluations, and the rebuttal either clarified why it is not possible or added them. As a result, after the discussion the reviewers all supported acceptance of this paper. 

Given the above discussion, and rebuttal/changes to the paper, I recommend acceptance. It is a very well done empirical paper, provides interesting findings, stronger baselines, and thorough experimentation. Further, some of the smaller findings (ViT correlation experiment) as well as larger relationship between open set recognition and out of distribution detection are valuable contributions to the community. Finally, I would recommend this paper as oral, given that it may garner a good discussion of these contributions.
The paper proposes a GAN based method for synthesizing various types of defects as foreground on different product images (background). The method builds upon StarGANv2, and adds the cycle/content consistency loss and classification loss between foreground and background. While the paper considers an important problem/application, the reviewers found it lacking sufficient novelty for publication. The paper will be more suited for publication at an application oriented venue.
Unfortunately, reviewers unanimously agreed that this paper does not meet the ICLR acceptance standards, citing generally unpolished experiments. I would recommend substantially expanding the experimental results in the future.
This paper presents two novel approaches to provide explanations for the similarity between two samples based on 1) the importance measure of individual features and 2) some of the other pairs of examples used as analogies.  The proposed approach to explain similarity prediction is a relatively less explored area, which makes the problem addressed and the proposed method unique. However, reviewers expressed concerns about evaluation methods and there were some concerns about the design choices that were not well motivated. The major issue is, as pointed out by the majority of the reviewers, the evaluation methods. Given the paper, reviews, and responses of the authors and the reviewers, it appears that there is certainly room for improvement for more convincing evaluation methodologies to convince a cross section of machine learning researchers that the proposed approach advances the field. Overall, this is a good paper, but appears to be borderline to marginally below the threshold for the acceptance.
The paper tackles the problem of generalizing to a new environment by learning a small set up anchor policies (even just 2 for the final approach) which span a sub space that can be searched efficiently in a new environment.
The discussion and additional experiments managed to convince most reviewers that the method indeed works as the authors had hypothesized (especially regarding functional diversity). At the moment the analysis is mainly based on empirical observations, it would be good to also have a thorough theoretical analysis of the method.
This paper presents a novel methodology for performing meta learning for gradient based hyperparameter optimization.  The approach overcomes limitations (scaling, e.g.) of previous methods through distilling the gradients of the hyperparameters.  The paper received 4 reviews, of which all were positive (6, 6, 8, 8).  The reviewers appreciated the technical clarity of the paper and found the proposed approach sensible, novel, technically sophisticated and effective.  The main concerns were regarding the comprehensiveness of the experiments and technical presentation of the dataset distillation.  It seems that the reviewers found the author response (lots of results were added) satisfactory regarding these points.  Thus the recommendation is to accept.
The paper studies the introduction of a variant of batch normalization (BN) to train deep neural network. The underlying idea is a two step approach for per sample based normalization, relying on augmenting the computational graph to handle "several samples" nodes.

The reviewers have mentioned that the idea of altering the computational graph is interesting and potentially novel.
Yet, the numerical experiments were not enough precise or solid to back up the claims by the authors, that their proposed BN alternative is of practical interest.
It was also raised that the paper lacks theoretical supports: no formal analysis, most explanations are ad hoc, etc.
This paper applies and evaluates the use of Q learning for the control of microscopic collectives of Volvox algae. 

While the application is indeed very cool and potentially impactful, the paper has no theoretical contribution to the field of machine learning as it consists of an empirical evaluation of an existing (and well established) algorithm.

The reviewers agree on the importance of the application, reported concerns about the current manuscript. In particular:
  Reviewers QBsR and GPp7 suggested including additional comparisons to other learning algorithms
  Reviewers QBsR and BtTc also suggested improving the writing

Overall, I agree with the reviewers that the current manuscript has a lot of potentials, but it could benefit from additional work. 
Please carefully consider and incorporate the feedback received from the reviewers. Personally, I think that presenting a more sharp message and clearer insights would further increase the quality of exposition and help to make a stronger case for why this manuscript is relevant to the larger ML community.
This paper introduces an algorithm for making a meta model from an ensemble of models by learning model embedding.

All reviewers appreciated the originality and potential usefulness of the paper. However, they also all think that the work is not completely ready for publication. Both the presentation and quality of the results can be improved. 

There are a lot of good feedback in the discussion that can be used to make an important updated version for the next conference.
This work investigates a metacognition model for object detection. Reviewer RHiF wrote the best summary for this work:

The paper proposes a new method to incorporate Spelke s principles of object perception as constraints to improve the performance of an out of the box object detector. This is done via defining a hierarchical generative model which defines "metacognitive" priors over the a set of observations. Through joint inference over these metacognitive priors and new unobserved states, the method outputs better object detections. The authors show improved performance on a synthetic dataset which contains scenes rendered in a virtual environment.

All reviewers agree that this is a really novel and interesting approach of enforcing consistency constraints in object detection, but had various issues with the experiments. At its current state, I believe it would make a very strong workshop paper, but not read for the ICLR 2020 conference. The authors found the reviews to be helpful, in particular, advice about the dataset construction and metric definitions, and I believe that future versions of this work will be significantly improved. We look forward to reading a revised version of this work in a high impact journal or future ML conference, good luck!
The paper introduces a meta learning approach for re weighting samples for better adversarial robustness. Specifically, they parameterize the weights using an additional module and learn it with the MAML objective. I have read the paper and reviews carefully by myself and found that the paper has several weaknesses that are not well addressed in the rebuttal. 

1) limited novelty. The proposed approach is a direct adaptation of the classical MAML algorithm to adversarial training, which is of limited technical novelty as pointed by serveral reviewers. 

2) Adaptive attack experiments are incomplete. The proposed BiLAW relies an additional reweighting module in the training stage, though it will not be used in the test stage. But we can still use an independently learned reweighting head for adaptive attack, which is should be considered in the white box attacks. We do not want to see the new proposed defense will be defeated by other attacks quickly. 

3) The true performance for BiLAW is problematic. Table 1 on MNIST is not representative for current development in adversarial community. On Table 2, comparing BiLAW with TRADES, for AA (0.031), 45.3% vs 51.7% on small CNN and 51.4% vs 52.1% on WRN 32 10. The performance of BiLAW is lower than TRADES, while when combine the two together, the results is very natural higher than TRADES and BiLAW but we are sure which component benefit the gains. For example, we can say TRADES benefits BiLAW because BiLAW+TRADES (52.6%) is much higher than BiLAW (45.3%). Also, the author did not show the results of single BiLAW on CIFAR 100. Considering the around two times running time, this performance is not acceptable in adversarial training methods. 

Due to the above reasons, I cannot recommend acceptance in the current verison to ICLR.
All reviewers give acceptance scores. 
One reviewer also commented that they would like to increase their score from 6 to 7 (which isn t possible in the system).
I encourage the authors to add the substantial new results generated during the rebuttal into the paper.
Confidence boosting via aggregating multiple run of algorithms has been used before. The main result of the paper relies on a generic confidence boosting trick. The authors for instance cite Shalev Schwartz et al 2010 theorem 26 in remark 4 of their paper and correctly point out that for deterministic algorithms like ERM one can use that for confidence boosting. While that theorem there is proved for excess risk and for deterministic algorithms, the main idea there to me seems like what is used in the authors paper as well. 

The basic idea: 
Property A holds in expectation, Hence use Markov inequality to get a low grade probability version of it in each of the K pieces
Now probability that at least one of the pieces is good is high since each piece is independent of the other
Finally aggregate with simple concentration with union bound.

In Shalev Schwartz et al 2010 this is done with property being excess risk, here it is done with generalization error.

(Oh and I should add, the fact that the algorithm is randomized does not affect this line of reasoning as long as we use fresh randomness for each of the K blocks).

Now the missing piece covered is that on average stability implies generalization in expectation. But isn’t this already known to be true in the stability literature? 

To me it seems like the main technical contribution of the paper is not as novel. Further, as one of the reviewers points out, the main goal should be to prove high probability guarantee for the algorithm popularly used like SGD not the confidence boosted version of it.

None the less, it seems like the application of the result to SGD seems interesting and somewhat new. 

I am reluctant to propose an accept here.
The paper proposes monotonic splines as an improvement on current approaches to parametrising quantiles in distributional RL. The idea is an obvious, natural improvement on what exists, and yields improved experimental results.
The authors design a framework for active learning on time series data. The framework, called Temporal Coherence based Label Propagation (TCLP) leverages temporal coherence to propagate expert labels to nearby points by a plateau model. In addition to describing the framework clearly with simple pseudocode, several experiments are carried out with careful analysis to validate the effectiveness of the framework.

The reviewers are mostly positive on the simple algorithm with strong empirical performance as well as the solid analysis of experimental results. They are also satisfied with most of the rebuttal feedback from the authors. Somehow there are joint concerns on the weaker theoretical results, especially in terms of their correctness. In particular, the unrealistic assumptions and over simplification make it hard to connect the theoretical results with the actual algorithm. Several reviewers suggest the authors to move theoretical analysis section to a supplementary section as a hypothesis, and the authors are also encouraged to clearly discuss what the theory can and cannot cover.
Dear Authors,

This paper eventually received mostly negative reviews (scores 5, 3, 5), with one mildly positive review (score 6). All reviews were particularly informative, offering detailed and expert feedback. I was hoping for author engagement, but unfortunately, no rebuttal was submitted. 

In general, the reviewers and me found the paper well written, on a timely topic, but of a very limited theoretical novelty. Well articulated details of this can be found in the reviews and I would recommend the authors to consider them carefully in their revision. I have no option but to reject this work. 

The main reason for rejection in this case is therefore limited theoretical novelty. However, this is a solid paper that is of publishable quality, albeit perhaps in a somehow lesser venue, at least in its current form. 

Kind regards,

Area Chair
The provides a complexity theoretic look at GANs. The exposition is multi disciplinary, and in my personal opinion, it is an interesting look at the GANs in the context of random number generators.
The key contribution of the paper is identifying that datasets have the so called DDD property. In short, datasets are predominantly composed of examples that are either consistently trivial or challenging (often misclassified) for neural networks.

Reviewer MrqK pointed out that it is well known (and provided four references) that many examples are consistently very hard or very easy for neural networks. This is true. It is somewhat novel how the authors attribute this phenomenon to datasets. Here, I would like to note that I slightly disagree with the attribution of the phenomenon to dataset alone. While it is a property of datasets, it is not self evident that deep nets trained with SGD have to learn these trivial examples. Attributing it either only to dataset or to model/optimization seems to be oversimplistic.

The second key issue of the paper is that it is somewhat inconclusive. Many datasets have the DDD property, but the Authors provide a somewhat unclear motivation for why it matters. In particular, the fact the two models make correlated errors on a dataset does not mean we cannot distinguish them. In fact, we have been able to distinguish models using IID and OOD datasets. They make correlated errors, but one makes, with a significant margin, less errors than the other. Having said that, I agree that we without the DDD property we would be able to more easily distinguish models. This is a useful perspective.

Reviewers appreciated added experiments that help better characterize what are these trivial and impossible examples. 

Despite the issues with novelty and framing, I think it is a useful perspective and hopefully will encourage more research into understanding the interaction between data and training. It is my pleasure to recommend acceptance and thank you for submitting the paper.

In the camera ready, please: (a) describe much more clearly and openly relation to prior work; (b) bring to the main text more data from the psychophysical experiments; and (c) address any other remarks made by reviewers.
The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose.
The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing. 

The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation.

Overall, reviewers all agreed that the paper is interesting, well motivated and deserves acceptance.
We hope the authors will incorporate the open points as mentioned by the reviewers.
This paper proposed a weight sharing method to speed up the pretraining of large language models. Basically, during the training, it first share weights across all the layers with the same architecture, and then untie the shared weights at some point later. The main advantage of weight sharing is that it can reduce the memory load. Our reviewers have many concerns on this work. The method is not well motivated or explained, and many experimental details are missing. In particular, there is no downstream task result presented for the so called 10T parameter model. The claim highlighted in the title remains unsupported. In addition, one of our reviewers pointed out that the proposed method is fairly similar to the method in a previous ICLR submission: https://openreview.net/forum?id jz7tDvX6XYR.
the paper proposed a novel idea of  requiring users to complete a proof of work before they can read the model s prediction to prevent model extraction attacks. Reviewers were excited about the paper and ideas. Some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal.
## A Brief Summary
This paper uses offline algorithms that can see the entire time series to approximate the online algorithms that can only view the past time series. The way this is done is basically, the offline algorithm is used to provide discrete class targets to train the online algorithm. The paper presents results on synthetic and historical stock market data.

## Reviewer s1H9
**Strengths:**
  Practical problem.
  Novel approach.
  Clear presentation.
**Weaknesses:**
  No other baselines.
  No theoretical guarantees behind the approach.
  Writing could be improved.

## Reviewer EgW9
**Strengths:**
  Clear writing.
  Interesting research direction.
**Weaknesses:**
  The primary claim seems incorrect and unclear. 
  Due to the unclarity about the primary claim of this paper, it is difficult to evaluate the paper. 
  Lack of baselines.
  The lack of discussions of the related works.

## Reviewer gii5
**Strengths:**
  Interesting and novel approach.
**Weaknesses:**
  Difficult to evaluate, with no empirical baselines or theoretical evidence.
  The datasets used in the paper are not used in the literature before. Authors should provide experimental results on datasets from the literature as well.
  The paper needs to compare against the other baselines discussed in the related works.
  More ablations and analysis on the proposed algorithm is required.
  Unsubstantiated claims regarding being SOTA on the task, since the paper doesn t compare against any other baselines on these datasets.
  The paper can be restructured to improve the flow and clarity.

## Reviewer zoKR
**Strengths:**
  Novel and interesting research topic.
  Bridging classical algorithms and ML.
  Clearly written.
 
**Weaknesses:**
  Lack of motivation for the problem.
  The approach only works with offline algorithms that work on time segmented data.

## Reviewer aaFn
**Strengths:**
  Novel algorithm.

**Weaknesses:**
  Potentially overfitting to the offline data.
  Data hungry approach.
  Confusion related to the occurrence moments of predicted future actions.
  Section 2 is difficult to understand.

## Key Takeaways and Thoughts
Overall, I think the problem setup is very interesting. However, as pointed out by reviewers gii5 and EgW5, due to the lack of baselines, it is tough to compare the proposed algorithm against other approaches, and this paper s evaluation is challenging. I would recommend the authors include more ablations in the future version of the paper and baselines and address the other issues pointed out above by the reviewers.
This paper addresses audio visual navigation tasks where a reinforcement learning agent perceives visual RGB and binaural audio inputs, rendered in a first person perspective 3D environment, and is tasked to navigate to the audio source. The authors propose to make the RL navigation policy robust, by training the agent with additional adversarial audio perturbations. These perturbations consist of an adversarial "ghost" agent (attacker) that emits noise perturbations volume, position and category determined by policies that are trained to maximise the negative rewards for the navigation agent in zero sum game. The agent is then evaluated on the simulated Replica and MatterPort3D environments and compared to a few baselines. The authors conduct a large number of ablation experiments.

The three reviewers were globally positive about the paper, regarding the motivation, joint training of the agent and attacker, and experimental evaluation. Reviewer TLMn had questions about specific results and ablations of existing baselines, whereas reviewer i5Vv had questions about random noise ablations   the authors provided responses for these questions. Outstanding requests were about proofreading.

After rebuttal and discussion, the scores for this paper are 6, 8 and a weak 8 (or 7), i.e., an average of 7, and thus I believe that the paper meets the conference acceptance bar.
The paper studies the Bayesian persuasion model in a more realistic setting where the sender does not know the receiver’s utility but can interact with the receiver repeatedly to learn the utility. The paper proposes a learning based framework to optimize the sender’s strategy, then analyze the theoretical properties of the proposed framework, and perform extensive experiments. The reviewers acknowledged that the paper investigates an important problem of relaxing the practical shortcomings of the Bayesian persuasion model. However, the reviewers pointed out several weaknesses in the paper, and there was a clear consensus that the work is not ready for publication. The reviewers have provided very detailed and constructive feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
This work provides a theoretical analysis of Prioritized Experience Replay (PER ) in a supervised learning setting, points out limitations of PER and proposes a model based approach to address these shortcomings for continuous control problems.  

Strengths:
 
The overall problem was motivated well
Reviewers agree that this proposed algorithm has promise
Overall the paper is well written
a diverse set of experiments is provided

Weaknesses:
 
reviewers point out some clarity issues
The theoretical analysis is performed in a supervised learning setting, and it is unclear how the resulting analysis transfers to the RL setting
There are some concerns (theoretical/technical) wrt to the proposed algorithm. 
The analysis of the experiments is lacking in depth. For instance, no analysis of why the proposed algorithm outperforms very related baselines. Furthermore, it s unclear why for the autonomous driving experiment the algorithms achieve the same return, but the proposed method leads to less crashes. 

Rebuttal:
 
The authors have addressed many of the clarity issues. However, I agree with the reviewers theoretical concerns and deeper analysis requests were not addressed in a significant manner. 

Summary:
 
Overall this manuscript investigates an important problem and provides a promising algorithm. However, some theoretical/technical concerns remain and a deeper analysis of results is required. Hence my recommendation is that in it s current form the manuscript is not quite ready yet for publication.
This paper analyses interpretation methods that use probes to evaluate the information in individual neurons of a deep network and shows that it confounds probe quality and ranking quality, and encoded information and used information. The paper proposes a new method which does not suffer from the same drawbacks. The reviewers were positive about this paper, and the discussion between the reviewers and authors resulted in the authors adding multiple clarifications. I ask the authors to try to optimize the paper for clarity further. I recommend acceptance.
This paper undertakes an empirical investigation of overparameterized neural networks, studying the last hidden representation and identifying "representation mitosis," a cloning effect whereby neurons split into groups that carry the same information. The effect is observed for a variety of architectural configurations/datasets, and a detailed set of experiments are performed to investigate the behavior.

The reviewers had split opinions about this paper, with most reviewers appreciating the novelty and salience of the observations, but with some reviewers expressing skepticism about the generality of the effect. While the experiments are thorough and revealing, the practical importance of representation mitosis remains somewhat unconvincing.

A primary motivating factor for the analysis is the search for an explanation of the unexpectedly good generalization behavior of oveparameterized networks and the origin of "benign overfitting." As highlighted in the reviews, the sensitivity of the mitosis effect to (1) training to zero loss and (2) optimal regularization suggests that it cannot be the sole explanation for benign overfitting, since the latter can and does occur without these conditions. The authors acknowledge this situation, and respond that their focus is on state of the art models used by the community, rather than on toy settings. For this to be a persuasive response, more compelling results in these state of the art situations should be evidenced   in particular, as several reviewers pointed out, the negative results on ImageNet undermine this point to some extent.

Overall, representation mitosis does seem like an interesting and potentially important phenomenon, but further work is needed to develop persuasive evidence in support of the interpretations and implications. While this is a borderline submission, I believe it falls just short of the mark, and cannot recommend acceptance.
The paper was seen positively by all reviewers. The strength of the paper are:
  Intuitive and interesting combination of Koopman Operators and Optimal Control for Reinforcement Learning
  Convincing experiments on challenging benchmark tasks
  All of the issues of the reviewers (advantages to SAC, gaps in the theory and missing references) have been properly addressed in the rebuttal.

I therefore recommend acceptance of the paper.
This paper present a model for reconstructing images from fMRI recordings, based on an encoder and decoder used in a loop. The reviewers were unanimous in their opinion that this paper is not ready for publication at this stage. They raised concerns ranging from the quality of the result and how to compare them to previous methods, to the justification behind different modeling choices. The authors were gracious in their responses to the reviewers. I do not recommend acceptance at this stage,
In this paper the authors demonstrate the use of meta learning in plastic recurrent neural networks with an evolutionary approach, avoiding gradients. They show that this approach can be used to develop networks that can solve problems like sequence prediction and simple navigation.

The reviews for this paper all had scores below the acceptance threshold (3,5,3,3). The principal concerns were:

(1) The lack of novelty. Other papers have taken very similar approaches (e.g. Najarro & Risi, 2020 or Miconi et al., 2019), and fundamentally this paper simply ties together different elements in one package.

(2) Lack of demonstration of the approach beyond some very simple tasks.

(3) Lack of connection to the related literature on neuro evolution and ML. 

(4) General clarity and style of writing issues.

The authors responded to the reviewers, but the responses did not convince the reviewers enough to increase their scores past threshold. Given this, a reject decision was reached.
This paper proposes a framework for novel object captioning by combining BERT and CLIP.  The model improves fluency, fidelity, and adequacy of generated captions. However, as reviewers mentioned, the novelty is limited, combining large models and big data to solve a downstream task does not make useful insights at this moment.
This paper received a split of scores, from 3 to 10. Among the reviewers, there are both strong advocates and strong rejects. All reviewers agree that finding a policy that is not only improving value but also has lowered variance is an interesting ideas. However, many reviewers point out that are major clarity issues that might hide fundamental problems. The proved guarantees seem to require strong assumptions that are unlikely to hold in practice, and experimental comparisons also have some subtleties. Taking together, although this could be a very interesting work, it will require a major revision and another round of review+discussions before it can be shaped into an acceptable paper.
The reviewers found the work interesting but have concerns about the correctness of some of the claims in the paper. Also some reviewers would like to see more experiments and some have concerns about the theoretical results. Overall, I see the work promising but it requires a major revision and some improvements to pass the bar. I would recommend the authors to use the reviewers  comments and prepare the paper for future venues.
It seems the reviewers are in an agreement that the work seems interesting, well motivated, and results are meaningful. The main complaints or issues that remain is the amount of rewriting involved, which might be hard for the reviewers to track, and maybe question regarding the results given for e.g. the choice of architectures for CIFAR 10 making numbers harder to interpret. 

However, I do see that the quality of the manuscript is quite good (and multiple reviewers commented on this), and the idea seems natural to me. I think the results, especially with the addition of CIFAR 100 and IMDB seem sufficient, and given the overall positive feeling of the reviewers over the work with no major concerns, I am happy to recommend this paper for acceptance.
This paper presents a new DDPM model based on solving differential equations on a manifold.  The resulting numerics appear to be favorable, with faster performance than past models.

Most of the reviews thought the main result was of interest and were impressed with the performance.  Reviewer c9bY points out some challenging issues and analytical questions that remain unanswered in the text; they also have some simpler textual revisions that seem less important.

In general, this paper has the misfortune of receiving reviews whose confidence appears to be low.  While partially this is a byproduct of the noisy machine learning review system, the difficulty of the text itself is substantial and made the paper less than approachable; the authors are encouraged to continue to revise their text based on feedback from as many readers as possible.  That said, the authors were quite responsive to reviewer comments during the rebuttal phase, which significantly improved the text.

Overall this is a borderline case, and the AC also had some difficulty following details of this technically dense paper.  Given the positive *technical* assessments of the work and at least one reviewer defending the paper s clarity, the AC is willing to give this paper the benefit of the doubt.
The paper considers the problem of solving time constrained multi robot task allocation (MRTA) problems. Formulating the problem as a Markov decision process (MDP), the paper proposes Covariant Attention based Mechanism (CAM), a graph neural network based policy that can be trained to solve MRTA problems via standard RL methods. The encoder adapts the covariant compositional network to improve generalizability, while the decoder extends a recent combinatorial optimization architecture to the multi agent optimization domain. Experimental results demonstrate that CAM outperforms an encoder decoder baseline in terms of task completion, generalizability, and scalability, while also providing greater computational efficiency than non learning baselines.

The paper considers an important topic multi agent task allocation is an interesting and challenging combinatorial optimization problem. The proposed CAM architecture adapts existing components in an interesting way and seems sensible for the MRTA domain. The reviewers initially raised concerns regarding the conclusions that can be drawn from the experimental evaluation, the significance of the algorithmic contributions, as well as the motivation for the proposed approach. The authors made a concerted effort to address these concerns through the addition of new experimental evaluations (e.g., comparisons to a myopic baseline and ablation studies), updates to the text, and detailed responses to each reviewer. Unfortunately, only one reviewer responded and updated their review (increasing their score). In light of this, the AC also reviewed the paper. The AC agrees with the strengths identified by the reviewers (including those noted above) and with the contributions provided by the additional evaluations. However, the paper remains unnecessarily dense, while at the same time not being self contained (e.g., the new experimental results are relegated to the appendix rather than appearing in the main text). The paper would also benefit from a more concise motivation for learning based solutions to MRTA and a clearer discussion of the paper s contributions.
This paper presents SimVLM, a simpler generative VLP framework with billion scale web image text data, which has good zero shot potential while remaining competitive on standard VL benchmarks. SimVLM achieves SotA on several tasks and shows promising zero shot capacity in certain tasks. Most of the reviewers liked the work; they had concerns about data scaling, but the authors showed that SimVLM_small with the smaller cc3m dataset does not drop performance too much (although the large data scaling with their large scale weakly aligned data is still important to achieve good zero shot learning). All reviewers also mentioned the strong concern of reproducibility and data accessibility, so we encourage the authors to address this as clearly as possible via releasing models and safely cleaned/anonymized data subsets, etc.
This work studies a number of feature representations for few shot classification, including representations learned from MAML, supervised classification, and some self supervised tasks. The main conclusion of the study is that learning from more complex tasks result in better representations for few shot classification. As a practical solution, then, the authors suggest using representations learned from multiple tasks for few shot classification.

The paper studies an important problem in machine learning, and reviewers all appreciate that. However, they raised concerns about the draft in its current state. Authors replied to these comments, and while reviewers acknowledged and appreciated the responses and the revision of the draft, unfortunately that did not convince the reviewers. Several major concerns remained unresolved at the end. Specifically, EEwV believes that the paper is a case study, which though useful, does not bring deep new insights or findings. 63j7 believes that even after revisions made to the paper, there are additional experiments required to understand and examine the claims. Tk8a finds the submission unready for publication due to weak experimental analysis and suggests running additional experiments to examine the hypotheses made by the authors (e.g, relating to spurious features, the need of input harmonization, the benefit of voting) and better tie in the findings of this work to related work. Tk8a provides a list of concrete suggestion along these lines. Similar to EEwV, 28ox also thinks that the paper lack novelty or does not really bring new insights on the way to train the backbone.

Based on these comments, and the ratings, I encourage authors to address these issues and resubmit.
This paper considers GNNs for link prediction (predicting which links are likely to appear next). An idea that has been used before is to add virtual nodes to improve the ``under reaching” problem in shallow GNNs; this paper considers this systematically in the context of link prediction. Specifically, one approach developed is to cluster the graph into clusters C(i), I   1, 2, …, k for some k and to add a virtual node u(i) for each index i, which is made adjacent to each node in C(i). This can ease information exchange, particularly in message passing GNNs. 

Link prediction is an important problem. However, there seem to be at least three issues with this work: the performance gains obtained are not strong enough, it is not conceptually clear why virtual nodes should help with link prediction, and the analysis is quite a bit about repeating existing analyses on nodes alone. I recommend the authors to address these issues thoroughly in the next version of the paper.
The paper gives a new method for code generation from natural language queries using pretrained models. The approach follows two steps: (1) given a query, it selects a set of similar training examples using a method called Target Similarity Tuning, and (2) it then uses a method called Constrained Semantic Decoding (built on top a frozen language model) to adapt these examples into syntactically/semantically correct code.

The reviewers found the paper interesting. There were some concerns about the method s scope and its relationship to prior work but these were mostly addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews (in particular, the review by vLDx) in the final version of the paper.
This paper generalize the idea of Mixup based data augmentation for regression. Compared to classification for which Mixup was used, the paper argues that in regression the linearity assumption only holds within specific data or label distances. The paper thus proposes MixRL to select suitable pairs using k nearest neighbor in a batch for mixup. The selection policy is trained with meta learning by minimizing the validation set loss. The approach provides consistent but small improvement over mixup on several datasets. Reviewers have also suggested discussion and comparison with more baselines, such as respective method using other (lower variant) gradient estimators (e.g., gumbel softmax), and using local input/output kernels for data selection, etc.
This paper advances the long running thread of sequence modelling research focussed on differentiable instantiations of stack based models. In particular it builds upon recent work on the Nondeterministic Stack RNN (NS RNN) by introducing three extensions. The first is to relax the need for a normalised distribution over the state and action distribution and allow unnormalised weights, this mostly serves to facilitate gradient flow and thus easier training. The second extension allows the RNN to condition on the top stack state as well as the symbol, improving expressiveness. The third improvement introduces a method for limiting the memory required to run the proposed model on long sequences, thus allowing its application to practical language modelling tasks. Each of these requires substantial algorithmic innovations.

The reviewers all agree that this is a strong paper worthy of publication. The paper includes a useful review of previous differentiable stack models which nicely sets up the rest of the paper where the contributions are well motivated and clearly presented. The reviewers had a number of clarification questions, partly due to the author s use of overly concise citations for key algorithms rather than inline descriptions. This situation has been improved by updates made to the paper.
The evaluation includes a series of synthetic experiments which are clear and provide a good elucidation of the various stack models properties. The practical evaluation on language modelling is more limited and serves mostly to demonstrate that the nondeterministic model can be scaled to a basic language modelling task.

Overall this is a strong paper with a well motivated and clear hypothesis. It provides a substantial extension to the prior work on nondeterministic stack models and progresses this line of research toward practical applications.
The work proposed multi view learning framework that combines diversity and consistency objectives for semi supervised learning. While reviewers appreciated that simplicity of the proposed method, they raised concerns on the limited contribution on top of the original Bayesian Co Training work. Although authors provided detailed rebuttals that addressed some of the reviewers  concerns, and one reviewer did raise their score, the other reviewers  scores remained unchanged. Given the work is closely based off the BCT work, I would like to see more detailed analyses on the importance of the changes brought in this work, such as changing the base learners and introduction of diversity objectives as pointed out by the authors.
This submission presents a technique to improve generalization of urban scenes segmentation.  Based on a pre trained deep net on synthetic data, the approach aims at adapting statistics on real target domain such as Cityscapes, BDD or IDD datasets using an Instance adaptive Batch Normalization (IaBN) at test time. Results are reported on several synthetic to real scenarios.

Most of the reviewers were not convinced by the approach and have raised several issues. After rebuttal and discussion, no one really changed her/his mind. The novelty of the proposed method is limited to the use of the existing IaBN in this context except the one sample adaptation.  Although the proposed method is effective on some benchmarks, the extra processing time may be a significant limitation. Additional comparisons are necessary. We encourage the authors to consider the reviewers feedbacks for future publication.
The paper proposes a novel explanation for the ineffectiveness of active learning (AL), namely, that AL will select unlearnable collective outliers. 

Reviewers generally find the finding is interesting, but the paper lacks in depth analyses. There re additional concerns on the experimental setups.
The paper describes a new method to improve the generalization of model based RL by means of interventional data augmentation. The key idea is to intervene the value of a particular variable (e.g., object property) in the learned dynamic model for episode simulations. Experimental results show that it improves (i) the generalization ablity in the OoD scenarios with respect to the intervened variable, (ii) sample efficiency in the presence of unbalanced training distribution.

Strengths:

  connects data augmentation to counterfactual property generation
  clearly written
  novel about applying counterfactual data augmentation to DYNA, as opposed to standard data augmentation techniques in other areas of machine learning
  The paper well demonstrates the benefits of counterfactual data augmentation for model based RL

Weaknesses:

  a lack of explanation for how the model is supervised to be equivariant to different data augmentations.
  empirical results seem to suggest that the proposed data augmentation does not have much of an effect on performance
  The claimed connection between the SCM and the proposed dynamic model seems vague
  The technical contribution seems limited and involves very strong assumptions.
  The structural causal model it introduces does not appear to be used by the method at all.
  the presentation does not cleanly separate counterfactual reasoning from intervention
  he greatest weakness of the method, acknowledged by the authors, is that there is no way to train the model on altered data. Thus, the performance of the policy on these altered data hinges on the extent to which the model, trained without such data, happens to make accurate predictions

All the reviewers voted for rejection. I recommend the authors to use the reviewrs  comments to improve the paper and resubmit to another venue.
The paper proposes several modifications to vision transformers: multiscale features, a variant of factorized attention, and "dynamic position bias". The proposed architecture with these modifications achieves strong results on classification, detection, and segmentation.

After considering the authors  responses, all reviewers are positive about the paper (reviewer K7wS mentioned upgrading to weak accept, but apparently forgot to do so). Main pros include clean architecture and strong empirical results. The main con is the somewhat limited novelty.

Overall, I recommend acceptance. While each of the proposed modifications might not be that unique, they are reasonably new in the context of transformers and their combination makes for a clean architecture that performs very well in practice.
The authors propose in this manuscript to use spiking neural networks (SNNs) as an efficient alternative to dilated temporal convolutions. They propose to utilize the membrane time constant of neurons instead of synaptic delays for memory efficiency. Training such networks with BPTT achieves better performance than other SNN based methods and achieve close to SOTA compared to ANN solutions for keyword spotting.

Pros:
  The manuscript addresses an interesting problem.
  Performance is good

Cons:
  Limited evaluations regarding efficiency, although this is a main point of the paper.
  The technical novelty is limited.
  One reviewer noted that the model is not actually an SNN, due to the use of multiple spikes per time step.
  Benchmarking is weak. Little comparison with previous work.
  Structure and writing of the paper needs improvement.

The authors did not reply to any of these critical points. In summary, although the idea seems interesting, the manuscript is not ready for publication.
This paper attempts to rationalize data augmentation techniques for compositional generalization by evoking the principle of meaningful
learning which posits that learning new concepts builds on previously learned concepts (which learners already understand). So for compositional generalization, this means that a model exposed to some new concepts in the test set, should link them to known concepts which have been already attested in the training set. The links between concepts are presumed to be semantic, e.g., hyponyms, hypernyms, or lexical variants. Ideally, a model should perform semantic linking on its own, however the authors do not propose a linking mechanism. Rather they investigate data augmentation as a way of exposing a model to semantic links and then explore whether different operationalizations of semantic linking enable the model to generalize better. Inductive learning is a bottom up approach, where links are created from general to specific concepts, whereas deductive learning is a top down approach where links are created from specific to general concepts. Experimental results indicate that inductive learning works better. 

The reviewers had the following issues with the submission (a) the technical contribution is not very strong (the idea of data augmentation is not new, although the authors  meaningful learning perspective is) (b) semantic linking seems to be able to handle only cases pertaining to lexical generalization (even though the authors include examples with structural generalization in their splits, there is no reason why semantic linking could handle these cases); (c) it would be more interesting/useful  to learn the linking than assume it is given. The authors did their best to respond to the criticism, but ultimately addressing the criticism is future work. I would also recommend to take a look at this dataset which might be useful for machine learning experiments: https://arxiv.org/abs/2105.14802
The paper proposes a new curriculum learning framework by parameterizing data partitioning and weighting schemes. Extensive experiments are performed on three different datasets to demonstrate the effectiveness of the proposed framework. The reviewers acknowledged that the proposed framework is interesting as it encompasses several existing curriculum learning methods. However, the reviewers pointed out several weaknesses in the paper and shared concerns, including the scalability of the framework to larger datasets and the significance of the improvements over baselines. I want to thank the authors for their detailed responses. Based on the reviewers’ concerns and follow up discussions, there was a consensus that the work is not ready for publication. The reviewers have provided detailed feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
The authors provide a convexification for the GAN training via integral probability metrics induced by two layer neural networks. The exposition relies on the convexification tools recently proposed by the Pilanci et al., and provides interesting insights to follow up in the future.
The paper received borderline reviews. While the reviewers acknowledged good motivation, good number of experiments and good numeral results that demonstrated the proposed method outperforms the existing state of the art, there are shared concerns: the experimental setup is not really a "low data" regime, generative models jointly trained with the multi task model only led to marginal improvements, and the prediction quality is quite low for all methods. In addition, it s unclear why the images generated by MGM have a lot of artifacts, and how the artifacts affect the performance. Overall, the reviewers were not convinced after the rebuttal.
The paper proposes a method to predict protein functions from Gene Ontology (GO) and protein sequences. The protein sequences are embedded with a pretrained protein language model (SeqVec) and the GO network is modelled with a graph convolutional neural network.

Reviewers found the paper well written and structured. At the same time, they found the novelty of the paper limited. Two reviewers pointed out that the paper is very similar to DeepGOA, which the authors cite but don t compare against. Overall, there is consensus among the reviewers that the paper is not suitable for ICLR.

The authors didn t submit a rebuttal.

We encourage the authors to take into account reviewer comments to improve the paper. Since it is more on the application side, perhaps a computational biology conference / workshop would be more appropriate for this paper.
This paper aims to use pre training to bridge the gap in performance between 2D GNN and 3D GNN. Specifically, during pretraining, it trains both 2D GNNs and 3D GNNs on data equipped with 3D geometry to maximize the mutual information between the 2D GNN representation with the 3D GNN representation. The proposed approach is interesting and novel and the paper presents some promising results showing that the pre training does provide some benefits for downstream tasks where 3D geometry information is not available in comparison to several other baseline pretraining methods. While the reviewers agree that property prediction without only 2D graph is a practically important setting for high throughput screening, there are concerns about whether the current set of results paint a clear picture on the benefits and superiority of the proposed methods to alternatives (e.g., vs conf gen) even after the revision. This is not due to lacking of results, but more of a presentation issue where results are not organized and discussed clearly to provide a coherent story.  We do see clear and strong potential for this paper but it needs a careful rewrite/re organization to tease out the key messages and how the experiments support them.
The paper studies the problem of learning fiber distributions associated with a machine learning task, in which the goal is to predict Y, given X. One chooses a fiber space / distribution Z / D_Z, and learns a trivialization \varphi : (Y,Z)  > X. The proposed architecture first clusters the label space Y. Within each cluster i it fixes a fiber space and distribution, and then learns a mapping \Phi_i, parameterized by an invertible neural network, by minimizing the discrepancy between the generated distribution and the distribution of the training data. The paper performs experiments on the wine dataset and a dataset coming from an aerospace application, as well as synthetic data with fiber bundle structure. Since the task here is generative modeling, the paper compares to standard GAN architectures (WGAN and conditional GAN) and argues that they are not fiber learners, in the sense of this paper. 

Initial reviews were split, with reviewers appreciating the novelty of the fiber learning task, while also raising questions about the paper’s relationship to conditional GANs, some points of clarity, and limitations of the experiments. After interaction in the response period, the reviewers converged to a decision to accept. The paper’s primary strength is its clear formulation — the paper provides useful language for describing conditional generative models (in particular, for discussing when a factorization of the distribution over Y and Z is appropriate), a valuable contribution to the discussion in this area.
Three experts reviewed the paper. Two reviewers recommended acceptance as they liked that the work identified a legacy design in object detection networks and resolved it by a new module. All reviewers found the empirical results strong. Reviewer MDN5 recommended rejection main for the concern that this newly designed module is a standard exploitation of network architectures. AC sided with the positive reviewers because of the paper s identification of a legacy design in object detection and the strong experimental results. Hence, the decision is to recommend the paper for acceptance. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
The paper demonstrates that transformer architectures can be trained to compute solutions of linear algebra problems with high accuracy. This is an interesting direction and, as the the reviews and the discussion show it is a "good data point and insightful", as one reviewer puts it. I fully agree with this but also agree with one other reviewer in that this is "yet another" application of a known transformer architecture. The author should place the model into context and provide some perspective. Without, the motivation behind solving the specific set of linear algebra problems considered is a bit unclear. For instance, could a transformer now learn to solve corresponding ML problems? Moreover, the dimensions of the considered matrices are rather small, and the generalization to larger dimension appear to be tricky.
Meta Review of Cross Trajectory Representation Learning for Zero Shot Generalization in RL

This work investigates a zero shot generalization method for RL based on an online clustering adapting it to RL. The intuition of this approach (called Cross Trajectory Representation Learning, CTRL) is that the self supervised objective used will encourage the encoder to map behaviorally similar observations to similar representations without the use of reward signals. The authors performed experiments on the 16 procgen tasks, and compared it against several baselines (DBC, PSE, CURL, Proto RL and DIAYN). The performance is generally better against baselines, but what I like about it is that a new approach to achieve such performance is proposed.

The scores were generally good (6, 6, 6, 3), and the 6 s are overall positive with the work (both in the writing, breadth of experiments). Reviewer Kekc, who gave a score of 3, maintained their score, despite acknowledging the authors  responses. The main outstanding issue from Kekc is that they believe the paper should stick with the original 25M step protocol (with a larger training budget), rather than 8M steps. If that s the main issue for this paper to not be accepted into ICLR 2022, I feel this can be adequately addressed for the camera ready version. (Please note that while I disagree with the final score of 3 that Kekc gave, I find their review to be highly informative and useful, and would like to acknowledge Kekc for their input and discussion).

Based on the discussion and the reviews, and with the context behind the score of 3, I would like to be on the side of recommending this paper for acceptance, and urge strongly for the authors to conduct the 25M experiments as reviewer Kekc suggested (as Kekc also noted, the training curves are still going up, so just train them for a longer time). Even if the final results are not as good as the 8M, that s fine, just include them in the final camera ready version, since I believe this work to meet the bar, offers good insights into RL generalization, has a good breadth of experiments and baselines, and will be of great interest to the broader RL community.
This paper develops a modular system named FILM, for egocentric instruction execution task in the ALFRED environment, which uses structured representations that build a semantic map of the scene, perform exploration with a semantic search policy, to achieve the natural language goal. They achieve strong performance while avoiding both expert trajectories and low level instructions. The reviewers all reasonably liked the paper (all reviewers gave  marginally above the acceptance threshold  score) and appreciated the planner ideas + strong results; but many of them also had concerns about the use of templated mappings from 7 high level goal types to low level instruction sequences, and whether this will make the system specific to ALFRED. The authors did provide some new results in the response period to show that results drop without the templates but not by a large margin. Some reviewers also had concerns about the novelty of the work and said that the semantic map building module and sub goal deterministic policy are motivated by previous work, but their incorporation into the FILM system is novel. Lastly, there was some concerns/debates on whether the system assumes/uses too much domain knowledge / task type taxonomy which might reduce the ability to generalize to other domains / data types, versus on the other hand the results may also serve to highlight the need for improvements in high level planning/control in these types of visual language navigation tasks.
Overall it was decided to reject this paper mainly because it seemed to be a minor extension of known constructions. The reviewers agreed that it certainly is valuable that the paper presents the best known results for kernel k means, but the paper was viewed by the reviewers as more of an observation and primarily an off the shelf application of techniques in the coreset literature. Because of this, the novelty was thought to be a bit below the bar. 

One suggestion for improving the presentation is that in the thesis of Melanie Schmidt, there seems to be such a construction for kernel k means which is exponential in 1/eps, and so much worse than what is in this submission. While that s great and certainly something to add and discuss in the paper, the reviewers still felt the technical novelty here was not quite enough to merit acceptance.
This paper develops a new mechanism SubMix that provides next token prediction under a variation of the differential privacy constraint. There is disagreement among the reviewers when assessing the quality of this work. Even though the study of private predictions in large language models is new, the reviewers raised several issues in the proposed approach. First, the formulation of partition level DP created confusion about the privacy guarantees provided by the mechanism. Given the similarity to PATE, it might be useful to articulate if there is any difference between the privacy guarantee in this paper and the one of PATE. Second, the authors might want to further clarify the reason for having two sub parts, which has also created some confusion. Even after reading the author s response and the updated revision, the AC still could not understand the relevant privacy argument. In summary, the paper may require further clarification and revision before it is ready for publication.
This paper presents a domain generalization method for semantic segmentation. The model is trained on synthetic data (source) and is tested on unseen real datasets (target). The authors propose a simple data augmentation method, AdvStyle, generating unconstrained adversarial examples for the training on the source domain.

There was no consensus on the method among the reviewers. Several issues have been raised. After rebuttal and discussion, no one really changed her/his mind. The motivation of why focus just on driving scenes is still questionable. Definitively, it could be interesting to investigate further why it is not straightforward to have gains on other kinds of scenes. Finally, we encourage the authors to address the raised concerns regarding the discussion with previous works and the comparisons for future publication.
This manuscript presents an approach to handling abstract visual analogy tasks where panels of drawings are shown with a missing entry. One of a number of candidate drawings must be chosen to complete the panel. Reviewers brought up several concerns:

1. The task performed was made considerably easier by providing additional annotations at training time. This was not the case in the original task in prior work that the manuscript builds on. No convincing explanation was provided as to why this change is critical to accommodate the manuscript s contributions.

2. A key feature of the approach, the adaptive modular design, does not seem to contribute much. The authors rightly point out this may be a limitation of current benchmarks. Reviewers were sympathetic to this view but that leaves the manuscript in a tough spot: a central contribution cannot be evaluated. What is even worrisome is that without evaluating the effectiveness of the adaptive design we cannot know if it is working at all. What if adaptivity is required for some future analogy tasks but it turns out that this approach, despite seeming to be adaptive, falls short?

3. Another contribution, the multi task encoder does not seem to provide much value in ablation experiments. The manuscript would be improved if this feature was removed or its usefulness was demonstrated.

A number of smaller issues were also brought up by reviewers.

Throughout the responses to reviewers the authors highlight that their central contribution is incorporating a structure mapping prior "The central contribution of our method is introducing a structure mapping prior ...". I would like to draw the author s attention to the fact that they had to remind 3 of the 4 reviewers to focus on this rather than another aspect of the work. That clearly indicates that the manuscript and work needs a shift in focus. I would suggest that authors double down on their structural mapping prior, eliminate all other features which turned out to be controversial or impossible to evaluate, and demonstrate the utility of their idea in two domains, i.e., including another domain. This would really highlight the core contribution.

Unfortunately, what may turn out to be a good idea, the structural mapping prior, is lost among many other complexities. I hope the authors are not discouraged and that we see this line of work again in the future.
This paper proposed a training free method to detect noisy labels based a given pretrained representation. The author suggested two methods based on either voting or ranking to filter noisy instances with corrupted labels without training. Reviewers generally agree that the technical novelty and contributions are only limited or marginally significant. Also experiments are not very convincing as there lacks of extensive comparisons with many existing methods for either noisy label removal or learning with noisy labels and the datasets are somewhat simple and not complex enough.
This was a very borderline decision. Here are the major factors involved in the decision. 

1. The concurrent works by Li et al and Yu et al. It is unclear about the relationship/strength between those results and the ones in the present paper. However, in accordance with the ICLR policy on simultaneous work, we ignore them to the extent possible.
2. The novelty of the approach. All reviewers agreed that the modifications to the PATE approach are fairly minor or incremental, and this appears to be largely an application of this method. That said, methods for the natural language setting are important.
3. The strength of the findings. Reviewers were mixed on the strength of the results: they appeared significant in some cases but rather lackluster in others. 
4. The poor quality of the writing. I personally read the original version and found the writing to make it impossible to understand in parts. The writing is still subpar in many places, which I would have expected the authors to fully polish given the substantial time during the response period. 

While in isolation, any of 2, 3, and 4 may be acceptable, their combination makes it difficult to recommend this paper for acceptance at this time.
This paper exposes a method to reduce the training cost of once for all networks.
Overall this paper is well written and easy to follow, and the experimental section shows a clear reduction of training time on the examples used.
However, the reviewers point out that the experimental section could benefit from adding more design spaces, and have a better explanation of the results. More importantly, three out of four reviewers agree that the novelty of this work is too low for the submission to be accepted, with the fourth one only giving a score of 6 (and also noting the lack of novelty). I therefore recommend reject for this paper.
The authors propose a framework for for the certification of reinforcement learning agents against adversarial observation/state perturbations based on randomized smoothing. They develop the theory of the framework, demonstrating that the framework can be used to certify lower bounds on the worst case cumulative reward of an agent. They validate their theoretical bounds experimentally.

The paper is well written and reviewers were mostly in agreement that the contributions are worthy of acceptance. The technical concerns from reviewer zGtv were addressed during the discussion phase, but I strongly encourage the authors to revise the manuscript to address the points raised in the discussion.
The paper proposes a novel approach for estimating the high dimensional intensity function of a Poisson process. The proposed approach builds on generalized additive models, using lower dimensional projections. 

The reviewers noted that, although the paper is well written, the position of this paper compared to earlier related work is unclear, and the empirical evaluation of the method should be strenghtened. The authors clarified some points in their response, but the paper would still require some more modifications to be ready for publication. I therefore recommend this paper to be rejected.
This paper proposes a new method for the important problem of semi supervised learning. This method relies on an auxiliary task, label observability prediction, to weight the examples according to the confidence in their pseudo labels, so as to avoid the propagation of errors encountered in self training. Limited experiments show that the proposed method can compete with other methods in terms of performance or training time. On the positive side, all evaluators agree on the potential value of the proposed approach, which is generic in nature. On the negative side, the experimental evaluation, although strengthened during the discussion, is not yet strong enough to have really convinced of the real merits of the method. In particular, comparisons with the state of the art still need to be improved. In addition, the paper would benefit from some rewriting, in particular of the mathematics (e.g. the d notation for task B should be avoided as suggested by one reviewer, there is a misplaced partial derivative in equation 6). The authors could also simplify their derivation by using the envelope theorem. I therefore recommend rejection, with an encouragement to strengthen the experimental part, and to improve the derivation of the proposed method.
This paper introduces Autoregressive Diffusion Models (ARDMs), which generalises order agnostic autoregressive models and absorbing discrete diffusion.

All reviewers appreciated the paper with a few also finding it very dense. The experimental section is a bit lacking in detail. This has to some degree been answered in the discussion and should also be included in the final version of the paper. 

Acceptance is recommended.
This paper considers augmenting LSTM language models with a form of residual connection that adds and additional feed forward layer before the softmax that integrates the output of the recurrent cell with the input embedding. This architectural variation is evaluated on the standard Penn Treebank and Wikitext 2 language modelling tasks and shown to lead to lower perplexities on the test sets, particularly when dynamic evaluation is used. 

The reviewers agree that the proposed addition is well motivated, however they also observe that there has been substantial work in language modelling on various forms of residual and skip connections and it is not clear how this work relates to that body of work. The authors have provided some additional comparisons during the discussion, however the reviewers feel that further evaluation and analysis is needed. There was also some additional confusion about the varying hyperparamter tuning protocols employed in the different evaluations. The author’s have clarified this in their response so that it is clearer how the different results were obtained.

Overall this paper presents an promising initial result, but it would benefit from more complete evaluation, analysis, and hyperparameter tuning. This could include ablation studies and analysis to shed more light on what the proposed architectural addition is contributing, how this relates to other varieties of residual connection, and it’s positive interaction with dynamic evaluation. It would also be useful to include a tuned model with a comparison to previously reported Wikitext 2 results.
Thanks for your submission to ICLR.

Three of the four reviewers are ultimately (particularly after discussion) very enthusiastic about the paper, and feel that their concerns have been adequately addressed.  The fourth reviewer has not updated his/her score but has indicated that their concerns were at least somewhat addressed.  I took a look at their review and agree that the authors have addressed these concerns sufficiently.  I am happy to recommend this paper for acceptance at this point.  Note that I really appreciate the time and effort that the authors went into adding additional results and clarifications for the reviewers.
The paper introduces the problem of continual knowledge (language) learning. The authors point out the interesting duality between continual learning and knowledge learning where: in knowledge learning one must avoid forgetting time invariant knowledge (avoid forgetting in CL), be able to acquire new knowledge (learn new tasks in CL), and replace outdated knowledge (a form of forgetting and re learning or adaptation). In their paper, the authors develop an initial benchmark for the task along with a set of baselines and provide empirical studies.

The initial reviews were quite mixed. The reviewers seem to agree this work studies an interesting and fairly novel direction for continual learning of language. However, the reviewers did not agree on whether this initial stab at the problem was "enough." In particular, reviewer U9Hk argues that the formulation is "oversimplified" and the current experiments are limiting.

After the discussion, the reviewers remained split with one high score (8), two borderline accepts (3), and one reject. So three reviewers believe that this manuscript is already a good contribution. The fourth reviewer disagrees, but the authors provided clear and convincing responses to many of their comments (and point to results already available in the appendix).

Overall, this is a clear and reasonable first step considering this paper proposes a new CL problem. The reviewers and I believe that this is interesting and rigorous enough to be impactful and to warrant follow up works. As a result, I m happy to recommend acceptance. I imagine that if the community demonstrates interest in this line of work, there will be work both on methodologies to improve the proposed baselines, but also work proposing extensions to the problem in line with some of the comments of reviewer U9Hk.

In preparing their camera ready version I strongly encourage the authors to take into account the suggestions of the reviewers and your replies. In particular, your discussion regarding encoder decoder and decoder only LMs and the associated results would be good to discuss in the main text (even if the full results are in the appendix).
The authors made substantial  improvements to the originally submitted manuscript; however, reviewers initially remained reluctant to support the paper for acceptance based on the degree to which they were confident in the underlying arguments / position taken by the authors and the evidence provided to support their position and arguments. There are also concerns about the significance of the gains in performance afforded by the proposed approach.

During the author response period two reviewers became satisfied with the additions and modifications leading to an increase in the final score. It will be critical for the authors to try to add ImageNet results if possible in addition to other promises made to reviewers.

The AC recommends accepting this paper.
This paper proposes augmenting standard forward prediction techniques used for representation learning with backward prediction as well, termed "learning via retracing". The paper implements this idea in a Cycle Consistency World Model (CCWM) and demonstrates that CCWM improves performance of a Dreamer agent across a number of Control Suite tasks. The paper also proposes a way to detect "irreversible" transitions and exclude them from the backwards prediction step.

This paper generated mixed opinions, and the reviewers did not come to a consensus on whether it should be accepted or rejected. In particular, Reviewer VSAG maintained it should be accepted, while Reviewer NEVM maintained it should be rejected. The other reviewers did not reply; I thought the authors  responses to their questions were reasonable so I assume their concerns were addressed (additionally, I don t believe a comparison to PlayVirtual is a justifiable request per the [reviewing guidelines](https://iclr.cc/Conferences/2022/ReviewerGuide), as it is concurrent work).

The reviewers generally agreed that the cycle consistency idea proposed by the paper is interesting, well motivated, and borne out by the experimental results. I agree with these points. The main weakness of the paper, brought up by multiple reviewers, was the justification/motivation for the method for irreversibility detection. The authors clarified in the rebuttal that the motivation comes from the idea that temporally adjacent states with very different values will tend to be far apart in representation space. While I believe that is true, it s not at all clear to me that this necessarily *entails* irreversible transitions between those states. That said, my feeling is that this is approach is (1) not the main contribution of the paper and (2) empirically seems to work, based on the experiments, even if the motivation is unclear/unjustified. Therefore, I do not think the concern about irreversibility detection is grounds on its own for rejection.

Overall, I find this is a sensible approach to better representation learning in MBRL. I recommend acceptance as a poster.
This paper proposes an algorithm for learning linear SEMs via the Cholesky factorization and provides a detailed theoretical analysis of the algorithm. After an extensive discussion and clarification from the authors, there was a consensus that the theoretical results are incremental compared to existing work and many of the claims need additional context in light of existing work. In particular, I recommend the authors pay careful attention to the presentation of the sample complexity bounds, which were revealed to be substantially weaker than initially claimed, and to validate these bounds with careful experiments.
This paper introduces an objective for representation learning that captures "controllable elements" in the environment (i.e., things that are affected by the agent s actions). In their reviews and discussion, the reviewers agreed this idea was intuitive, well motivated, and the paper well written. However, multiple reviewers raised concerns about the evaluation and the extent to which LCER is truly an improvement over PI SAC. Although many of the reviewer s concerns were addressed in the rebuttal period, at the end of the discussion they were still unconvinced or confused about how much LCER really helps over PI SAC. Based on this, my assessment is that this paper is a promising piece of work, and that with some more controlled comparisons (see suggestion below) it would be a useful contribution to the literature. However, given that the claims are not fully supported as it currently stands, I recommend rejection.

Specific suggestion to improve the paper: based on reading the paper and the discussion, it seems to me (as per the authors  own statement in response to Reviewer uWv6) that the most valid/controlled comparison between LCER and PI SAC is in Figure 4, where LCER w/ $\beta 0.1$ "can be seen as a variant of PI SAC with the same embedding choices as LCER" (author s words). However, when taking into account the error bars of the training curves, other values of $\beta$ are only clearly better than $\beta 0.1$ in 1/3 environments (D.walker walk). This does not make for a particularly convincing result that LCER is better than PI SAC. To improve the paper, I d encourage the authors to run further well controlled comparisons such as this in a larger number of environments. If they can show via such controlled comparisons that LCER is generally better than PI SAC (i.e. LCER w/ $\beta 0.1$) then that would be a much more compelling demonstration of LCER s superiority.
The paper proposes some interesting ideas about decomposing the global symmetries of multi agent MDP to local symmetries using a method called the Homomorphic Networks. The paper is well writing and can be followed easily. However, there are a number of weaknesses of the paper. Below, we list some of the outstanding ones.

(1) Both some of the reviewers and the AC find there are some clarity issues in the paper, for instance, it is hard to see why using keeping relative communication and local transformation would guarantee symmetry (it would be good to show this with a theorem); it is hard to understand the algorithmic structure due to the use of codes instead of diagram in Appendix D, especially when some definitions in the code are not defined in this paper; the AC also finds it is hard to understand how communication is performed during both training and executing. After the rebuttal, the structure of the paper has been improved but we encourage the authors to keep improving the presentation.

(2) Another weakness is the experiment implementations, which the reviewers found were a bit too simple and contrived   the traffic light example may not be symmetric in the real world. It might be good to demonstrate the effectiveness of the setting in a more realistic setting   perhaps show that the method also works with a slight violation of symmetry.

That being said, the contribution of the paper remains significant, and the AC recommends borderline but slightly leaning toward acceptance.
This paper considers a domain adaptation setting where a source domain model trained on a server is adapted on a client using target domain dataset. The paper considers the setting where the client only has a modest memory footprint (e.g., an edge device) and uses a recently proposed technique "TinyTL" (NeurIPS 2020) which is based on freezing the network weights but only updating the biases and adding a lightweight residual module. The basic idea of the paper is also based on SHOT (ICML 2020).

While the reviewers appreciate the problem setting and the basic idea, there were several concerns, some of which included:

  Limited novelty: The paper s key ideas are largely based on SHOT and TinyTL and a simple combination of these with not such significant challenges or insights offered.
  Federated setting not considered adequately: Although the paper title and the abstract/introduction talk about the federated setting, the paper largely focuses on a single source and single client setting.
  Inadequate baselines and experiments: The federated learning baselines used in the paper are fairly basic ones (e.g., FedAvg). Some of the experimental results are not convincing enough.

The paper received mixed scores and the reviewers engaged in discussions with the authors. However, the concerns still linger. Based on the reviews, discussion, and my own reading and assessment of the paper, I think the paper falls short of the acceptance threshold. The authors are advised to consider the reviewers  concerns to improve the manuscript for a future submission.
All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution.

Some suggestions from the area chair:
  "in causality" is not a standard technical term and also not non technical idiomatic English, so it should be explained the first time it is used.
  The authors should briefly cite and discuss research on so called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name.
  The authors could also mention the obvious but surprising point that if data are generated by two clusters, then a classifier can be learned using exactly one labeled example not even one from each class.
  I have read the reference EJ A’Court Smith. Discovery of remains of plants and insects. _Nature_, 1874 and I fail to see its relevance. It is only one paragraph. Work from the 1800s should not be cited merely to suggest a veneer of scholarliness.
  The writing uses italics for emphasis much too often.
The authors propose a method for training agents in FPS games, and achieve good results in a VizDoom setting. The method combines a number of different components and ideas, and it is not clear which of these are crucial to the success. In particular, ablations of the method are missing, as well as more runs to test variability and diversity. In addition, the paper is not all that easy to read. Reviewers had a number of partly overlapping concerns, of which I ve tried to distil the main ones above. While the empirical results are promising, it is clear that much more work is needed to distil this method into generalizable knowledge.
In the end, all reviewers agreed that this is a solid piece of work. However, there were also some doubts regarding the relevance of the block diagonal design and the underlying assumptions about the p/n ratio. The majority of the reviewers, on the other hand, had the impression that the positive aspects dominate the potential problems, and I also share this viewpoint. However, I d like to encourage the authors to carefully address the points of criticism raised by the reviewers in their final version.
This paper proposes a new algorithm for private ERM, when given access to public data, with a dimension independent risk guarantee if  (A) the public and private datasets are of the same distribution, (B) public dataset size exceeds the dimensionality (or, rather, the squared Gaussian width of an appropriate set), and (C) the public and private loss functions share a minimizer (and the gradients at the shared minimizer must satisfy some variance bounds). The algorithm uses the public data as the Bregman mirror map within private mirror descent (where Gaussian noise is added to the gradients), thus implicitly affecting the geometry, as opposed to explicitly learning the geometry as done in earlier works.

One reviewer was very positive, but two hovered around the borderline and expressed some reservations about the theory and experiments. Regarding the experiments, they did not compare to the ICML 21 paper by Asi et al   however the authors of that paper have (surprisingly) still not released their code, so I think this is forgivable. Since the paper was on the borderline, I read it myself, with a focus on the theoretical aspects. I find myself agreeing with the second reviewer that the assumptions are strong, and their justification is weak and unrealistic. 

Regardless of whether the paper, is accepted or not, I strongly recommend the authors to add condition (C) to their abstract (just the part about the shared minimizer)   currently the abstract mentions two of the above but not the critical third one. I think (A) is already a strong assumption   their justification that some users opt in to reveal their data does not justify this, because the opt in will not be random (if the opt in depends on covariates like gender/age/..., the datasets will not be identically distributed). On top of that, (C) is also a strong assumption   indeed usually the loss functions would be different (for eg, the private one would be clipped, and clipping will rarely preserve the population minimizer, as well as regularized)   their justification that for a linear model with symmetric noise, clipping does not change the minimizer may be true (though not proved), but we would never expect the linear model to be true in practice even if we employ it as a working model. Last, assumption (B) restricts its use in many common high dimensional data problems. Overall, I am pressed into a corner to find situations in which all three assumptions would be true.

Nevertheless, supposing that these assumptions hold, the algorithm is indeed clean, and the empirics appear reasonable. Overall, the paper remains on the borderline. Whether accepted or rejected, I expect the authors to do a much better job of carefully justifying their assumptions, with realistic and not far fetched examples (as suggested by the second reviewer).
This paper includes an interesting idea of pushing towards good, and away from bad, trajectories, in a natural clean way.

The main problem of the paper is one of clarity.  The paper could be written to be more concise and clear, which would allow, for instance, for sufficient space for the figures (which are currently sometimes rather tiny) as well as not having to fiddle with the margins and spaces quite as much as the current submission seems to do (which would be strictly disallowed at most conferences).  The issue of clarity was also clear during discussion, where sometimes multiple rounds of clarifications were needed to allow the reviewers to correctly interpret parts.

For these reasons, I recommend that the authors resubmit a new, cleaned up, version of the work, with all the changes neatly incorporated.  Then I think this could make for a nice addition to the literature.

I appreciate this will be a disappointment to the authors, but I think ultimately it will make their work more impactful, and longer lasting.
The paper proposes a new pseudometric, DARD, for comparing reward functions that avoid policy optimization. DARD builds on a recent work by Gleave et al. 2020 where the pseudometric EPIC was proposed. In contrast to EPIC, DARD operates on an approximate transition model and evaluates reward functions only on transitions close to their training distribution. Empirical experiments in different domains demonstrate the effectiveness of the proposed pseudometric. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers  questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.
This paper takes advantage of a well known fact in the OT literature: that relaxing either of the marginals of OT problems results in nearest neighbor assignments (as e.g. in k means) or soft assignments when using an entropic regularizer. The authors take advantage of this simple property (used e.g. in the first iterations of the word mover s distance) to speed up the inner iterations of the GW problem. As a result, theirs is a very simplified divergence that drops an important piece of info (the weights of the second measure) but which is illustrated on a few tasks dealing with graphs. Overall the paper has been appreciated by most reviewers, some criticizing the paper for its incremental nature but being overall pleased with the experimental validation.
The paper studies stability issues of GNN training when data are limited. The key contribution of this work is to use reweighted self training and negative sampling to stabilize GNN. Multiple reviewers raised major concerns on the technical novelty, experimental setup, comparison, and results. No response was provided during discussion. I recommend this submission be rejected.
The paper introduces As ViT, an interesting framework for searching and scaling ViTs without training. Overall, the paper received positive reviews. On the other hand, R1 rated the paper as marginally below the threshold, raising concerns about search on small datasets and issues regarding the comparison in terms of FLOPS/accuracy with other methods. The authors adequately addressed these concerns in the rebuttal, and helped clarify other questions by R2 and R3. R1 did not participate in the discussion after the author response nor updated his/her review. The AC agrees with R2 and R3 that the paper passes the acceptance bar of ICLR, as the unified approach for efficient search/scaling/training is novel and should be interesting to the ICLR audience.
This paper extends prototypical classification networks to handle class hierarchies and fairness. New neural architecture is proposed and experimental results in support of it are presented.

Unfortunately, reviewers found that paper in its current for is not sufficiently strong to be accepted at ICLR. Authors have made a significant attempt to clarify and improve the paper in their response. However, reviewers believe that contributions and motivation can be clarified further. We encourage authors to improve their work according to the specific suggestions made by the reviewers and resubmit.
In this paper, the authors investigate a multi task RL actor critic technique, where a single actor is used while multiple critics are trained (one per task, where each task corresponds to a different reward function). Experiments on several environments demonstrate that this method works quite well in practice.

All reviewers found the proposed approach sensible and effective, in spite of its simplicity. The main concerns were:
  Lack of novelty: although this is indeed not a particularly original idea, the specific instantiation in the actor critic setup is novel and well motivated
  Some confusing / unconvincing experimental results: after receiving this feedback, the authors were able to upload a new revision that addressed the main concerns
  Focusing on the "multi style" aspect when this is essentially a multi task algorithm: although I agree that framing it as a specific case of multi task learning would make sense and would probably make more appealing to multi task RL researchers, I do not consider this to be a major issue

In spite of being a relatively straightforward paper, I believe it is good to have strong empirical evaluation of such basic techniques disseminated to the research community, and I thus recommend acceptance, in accordance with reviewers  recommendations after the discussion period.
The authors provide in this manuscript a theoretical analysis to explain why deep neural networks become linear in the neighbourhood of the initial optimisation point as their width tends to infinity. They approach this question by viewing the network as a multi level assembly model.

All reviewers agree that this is an interesting, novel, and relevant study. The paper is very well written.

Initially, a weak point raised by a reviewer was that an empirical evaluation of the theory was missing. The authors addressed this issue in a satisfactory manner in their response.

In conclusion, this is a strong contribution worth publication.
The paper addresses the interesting  many to many assignement problem between a set of images and a set of text. Most reviewers, (and I agree with them) think that the  idea and its application worth being published although the performance improvement
is marginal. I request the authors to update the paper based on the discussions.
The paper proposes an energy consumption attack to neural ODE models. There are two complains from the reviewers: 
  Although this is a new application to energy consumption attack, most of the attack techniques are simple extensions to the previous attack papers, so the novelty is questioned by some of the reviewers. 
  The paper is poorly written. 
We therefore decide to reject the paper and encourage the authors to address the concerns in their next submission. Reviewers also think a careful discussion about the defense or detection mechanism against the proposed attack will be a good thing to add.
This paper presents the use of Simulated Annealing (SA) for pruning and optimizing the architecture of a neural network. After reviewing the paper and taking into consideration of the reviewing process, here are my comments:
  The contribution of the paper and the novelty is limited and not well presented
  The related work is very sparse. It requires a major improvement.
  The main concern is about the simplistic experiments and the lack of comparison between the results of the proposal and the SOTA methods.
  Conclusions are not well supported by the results.
From the above, the paper does not fulfill the standards of the ICLR.
This paper proposes an approach for 8 bit fixed point training of NNs, based on a careful analysis of quantization error in fixed point methods. They present convincing and thorough empirical results in addition to a detailed analysis providing insights about their method. Reviews for this paper were quite split. One reviewer was a strong advocate, asserting that the paper will have substantial impact in the area, and that the authors’ approach of minimizing quantization error for fixed point training is of substantial practical interest. Other reviewers were concerned that the proposed method was not novel enough, and that the proposed approach was not practical enough to work in realistic hardware use cases. The authors provided substantial detailed responses addressing the majority of reviewers’ concerns, and after following the discussion in detail I agree with the reviewer advocating for the paper, that the paper presents a practical, novel approach with valuable insights for the field from their analysis and results. 

I indicated I am certain about this decision, but I would be ok with the paper being bumped down from oral to poster.
The authors tackle the problem of cost sensitive hierarchical classification. They decompose the problem into level wise learning to abstain sub problems, and apply the distributionally robust learning (DRL) technique to minimize the abstaining loss. The proposed approach is compared with a few competitors on several data sets. The reviewers find the key idea in the work, namely leveraging DRL as the key technique to solve the decomposed problem, to be somewhat interesting and new. Some of the reviewers find the motivations clear, while others believe that the paper could use a better positioning to connect the motivation with the significance of the technical contributions. 

While the authors have extended the discussions on related works and added some additional experiment results during the rebuttal, the reviewers generally agree that the improvements were not sufficient to warrant acceptance. Most importantly, the few baseline competitors and the small scale data sets make it hard to convince the readers about the validity of the proposed approach. In particular, the scalability of the approach to larger scale data sets remains questionable, and the spectrum of baseline competitors, both in terms of breadth and recency, is not sufficient. Some reviewers suggest the authors include time/efficiency/convergence analysis of the proposed approach. Furthermore, the authors are encouraged to clarify the significance of contributions, explain the choice of DRL, and deepen the discussions on related works in future revisions.
This paper studies margin maximization in linear and ReLu networks. The reviewers appreciate the technical contributions of this paper, especially the simple counterexamples. However, reviewers also found the new results seem not to give enough conceptual insights or an important "main result". The meta reviewer agrees and thus decides to reject this paper.
The paper presents a new algorithm for data augmentation in graph neural networks. The algorithm works by learning a conditional model of a node s neighbor features, and augment the neighborhood representation using the generative model.

 In response to the reviews, the authors provided long answers and clarified much of the text. Nonetheless, after the discussion, two main concerns remained. First, the presention still felt subpar, too notationally heavy for what was presented. Second, the gains with respect to the baselines were assessed as not sufficiently significant to justify the approach which is substantially more complex than a baseline such as GRAND.
This paper is concerned with the problem of distribution shift, and develops techniques for detecting when the risk of a deployed model performs significantly worse on a testing distribution than on the training distribution.

The reviews for this paper were extremely consistent: after the discussion period, all five reviewers unanimously recommended acceptance, and several praised the authors for significantly improving their paper in response to reviewer criticism. Outstanding issues are (i) motivating the importance of the setting, and (ii) comparing with prior work. None of the reviewers seemed to think that these issues should be barriers to acceptance, but please seriously consider them, and all reviewer concerns.
This paper presents an analysis of the robustness of self supervised learning (SSL) features to noisy labels in downstream supervised learning, and provides empirical verification of the results (mostly in the symmetric noise setup); a SSL regularization scheme is also analyzed (section 4). While the paper contains plausible insights, the reviews share similar concerns that the analysis is mainly based on the noise being symmetric, and that the SSL features already have good class separation and Gaussian clusters, which are strong assumptions. Given that the assumptions are not theoretically verified, and that there is not sufficient empirical results in heavy non symmetric noise scenario on large benchmark datasets, the reviewers think the paper does not provide practical guidance for noise label learning in its current form.
This work proposed to detect backdoor in a black box manner, where only the model output is accessible. 

Most reviewers think it is a valuable task, and this work provides a novel perspective of using adversarial perturbation to diagnosis the backdoor. Some theoretical analysis for linear models and kernel models are provided. There is still huge gap to analyze the DNN model. But on the other side, it provides some insight to understand the proposed method and could inspire further studies. 

Besides, since there have been many advanced backdoor attack methods, and many more are coming out, I am not sure that the proposed detection criteria is well generalizable, considering only some typical attack methods are tested. However, I think the studied problem is valuable, and the presented analysis is inspired for future works. Thus, I recommend for accept.
After discussion, all reviewers are convinced about the novelty of the proposed method, and adjusted scores to recommend acceptance. They all appreciate the attempt to attack COVID 19 using machine learning.
The paper provides a uniform generalization bound for overparameterized neural networks using the notion of maximal information gain. The analysis relies on the eigendecay of the eigenvalues of the NTK, which has recently been the object of a lot of work in the literature, including the work of Bietti and Bach (the proof actually uses one of their key lemma).

The paper originally received a set of reviews with a large disagreement between the reviewers (including two reviewers with a negative opinion and three reviewers being more positive). After the discussion period, two reviewers kept a very negative opinion, while other reviewers slightly lowered their score. Some of the problems raised by the reviewers include the restrictions imposed on the data, a missing proof (which was eventually added by the authors), the discussion of prior work being inadequate (including for instance the differences with more classical generalization bounds), and the novelty of the analysis.

Overall, the paper clearly has some merits but some of the concerns above are too important at this stage to accept the paper. I recommend the authors address the concerns mentioned in the reviews before re submission.
The manuscript performs an empirical analysis of existing bias mitigation methods on two large datasets CelebA and ImageNet People Subtree where there are multiple sensitive attributes and some unavailable sensitive attribute labels. The results show that existing methods can mitigate intersectional bias at scale but unlabeled mitigation methods generalize poorly. The manuscript further proposes a knowledge distillation approach which can augment other labeled mitigation approaches.

On the positive aspect, the manuscript studies an important problem: intersectional subgroups on deep learning methods. Reviewers acknowledged that an empirical study on this problem is as an opportunity to make a contribution as it can highlight previously unknown issues.

There are however several major concerns including:
1. Methodological contribution (knowledge distillation) is under developed, while empirical investigation is interesting but can be further developed;
2. The fairness metrics adopted in this manuscript need to be clarified;
3. A discussion on the hyperparameter tuning, maybe involving a fairness accuracy tradeoff;
4. The claimed O(1) complexity for the knowledge distillation approach is implausible because it assumes the availability of G group specific models. This has been clarified in the rebuttal that the claim is only for the inference complexity, and the approach does not improve the training complexity. 

Reviewers also concluded that while the empirical analysis is interesting, the results on CelebA to be of limited use because the sensitive attributes are "purely illustrative." It s not clear that the insights from these illustrative intersectional groups (e.g. big nose & attractive) will hold for groups that are meaningful in a fairness sense.
The paper addresses the problem of domain generalization for learning spatio temporal dynamics. It proposes a solution where an encoder captures some characteristics of a given environment, and a forecaster autoregressively predicts future dynamics conditioned on the characteristics learned by the encoder. Said otherwise, the forecaster learns the general form of dynamics parameterized by an environment representation extracted by the encoder. The conditioning is implemented via an adaptive instance normalization mechanism. A form of padding is also introduced in order to take into account boundary conditions. The two components encoder and forecaster are trained sequentially. This approach is casted in a meta learning framework. Theoretical results inspired by multi task learning and domain adaptation are also demonstrated. The model is evaluated and compared to different baselines on three problems, and for two different settings: varying initial conditions with a given dynamics, and dynamics with varying parameters.

This is a borderline paper. It targets a timely and important problem of domain generalization for dynamic environments. The proposed solution is original and compares well experimentally to several baselines. It allows for better generalization performance for the two test settings considered. In the current version, the paper however suffers from different weaknesses. First there is the imprecision of the arguments and the description of the experiments. Some of the arguments and claims are vague and sometimes abusive, not backed up by evidence. For example, a central claim is that the encoder learns time invariant quantities characterizing the environment when the learned representations indeed change with a time shift in the input for any environment. The same goes for the argument developed for the padding construction. It is claimed to model boundary conditions, but this is not supported by any theoretical or empirical evidence.
As noted by the reviewers, the theoretical analysis is disconnected from the algorithmic and experimental developments and does not bring much additional value to the paper. What is more embarrassing is that some of the claims in this section are overstated and induce incorrect conclusions.  From Theorem 3.1 and proposition 3.3, the authors suggest that multitask learning leads to better generalization than learning independently, while this is not formally guaranteed by the results (this is acknowledged by the authors in a later comment). Besides, the conditions of validity are not discussed while they seem to only cover situations for which the train and the test distributions are the same. The same holds for the second theoretical results (theorem 3.4). It is claimed that this result supports the authors’ idea of training encoder and forecaster sequentially, while it does not. Besides, the bounds in this result cannot be controlled as noted by the reviewers and are not useful in practice.

Overall, the paper addresses an important topic and proposes new solutions. The results are promising and it is indeed an interesting contribution. However, inaccuracies and incorrect or exaggerated claims make it difficult to accept the current version of the article. The article would make a strong and innovative contribution if it were written as a purely experimental article with a detailed description of the experiments and comparisons.
This paper presents work on a dataset for object concept learning.  The main contributions include causal relations in the dataset and a method (OCRN).  The initial reviews pointed to concerns over the nature of the causal relations, the presentation of the paper, and the OCRN method and its motivations / use of the do operator.  The reviewers engaged in significant discussions after considering the authors  responses and the others  reviews.  After this delibration, the concerns over the dataset, its annotations, and the presentation of the methods were deemed to be better served by a full revision and reconsideration of the paper.  As such, the paper is not recommended for acceptance at this time.
This paper received a majority voting of rejection. In the internal discussion, no reviewer would like to change the score according to the author response. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.

**Motivation**

The motivation of this paper is not strong. In this paper, the authors claimed that the fairness level of deep clustering methods is relatively poorly compared with the traditional fair clustering methods. The traditional fair clustering methods employ the hard constraints to achieve fairness by scarifying the cluster utility. Instead, deep fair clustering methods seek the trade off balance between fairness level and cluster utility; therefore, the deep fair clustering can be regarded to use the soft constraints. There is no necessary to compare two different fairness constraints. Even the proposed method is a trade off balance between fairness level and cluster utility.

**Self augmented Training**

The relationship between self augmented learning and fairness learning is unclear. I guess that the authors added this modular simply to enhance the cluster utility. However, such a loss or an operator can be also applied to other (fair) clustering algorithms. The experimental comparisons in Section 5 is unfair. No ablation study on this is provided.

**Novelty**

One reviewer pointed out there existed some work that plugs integer linear programming into a probabilistic discriminative clustering model proposed in 2017. 

**Experiments**

(1) ScFC and DFCV release their codes; no results of these two methods were reported on HAR. (2) No standard deviation. (3) The Initial ILP Results (Ours) and Ours Result in Table 1 on HAR dataset is 0.653 and 0.468, both higher than the Ground Truth (Optimal) 0.458.

**Presentation**

 A few statements are not well supported, or require small changes to be made correct.

No objection from reviewers was raised to again this recommendation.
This paper exposes a method for video compression based on multi head models.
The reviewers seem to agree that the results are interesting, and worth publishing.
On the other hand, there are many concerns raised on the quality of the writing, with grammatical mistakes and confusing parts. The motivation for the multi head models, as well as its novelty, has been questioned in all reviews. Although the authors rebuttal has lead some reviewers to increase their score, it s still very concerning that authors needed to explain the main point of the paper to each reviewers. I think that the authors should polish this paper, taking into account the reviewers feedback, which would make a stronger paper, and submit it again in a future venue. I therefore recommend reject for this paper.
*Summary:* Compare neural networks and tasks using TDA, particularly persistence diagrams. 

*Strengths:* 
  Some reviewers found this a fresh perspective. 
  Distance calculation using TDA can offer advantages and a theoretical basis. 

*Weaknesses:* 
  Insufficient motivation and experimental evidence for utility of the proposed approach. 
  Computational cost and hyperparameter choices in PD computation. 
  Difficulty of interpreting proposed distance matrices. 

*Discussion:* 

ZGgm found the paper interesting and that it offered a fresh perspective, but that the purpose of the comparison was not sufficiently well motivated. The authors provide some explanations, particularly about the method allowing to compare networks of different sizes, but ZGgm found their comments were not adequately addressed. rtBj found that even though the authors made efforts to address their comments, the paper still requires substantial improvements. HwgX appreciated the authors’ responses but considers that the paper needs to be improved with additional validation. They expressed doubts about the adequacy of the approach and found that although it improves upon certain methods, it is insufficiently verified. 


*Conclusion:* 

All reviewers agree that this work has some strengths but also significant weaknesses and does not reach the acceptance bar for this conference. Main weaknesses are insufficient motivation and experimental evidence. The reviewers made several suggestions on how the paper could be improved. I agree with the reviewers and hence I must reject this article.
The paper proposed a novel one pass efficient streaming algorithm for estimating the number of triangles and four cycles. The concerns raised by reviewers were nicely addressed in the rebuttal and all the reviewers agree that the paper is above bar for publication.
The paper analyzes the behavior of VAEs in modeling data lying on a low dimensional manifold. It formally proves some of the conjectures/informal statements in an earlier work by Dai and Wipf (2019) in the case of linear VAE and linear manifold, and disproves the same for the nonlinear case. In particular, it proves, by analyzing the objective and its gradient flow dynamics, that VAE captures the intrinsic dimension of data distribution correctly. For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution. 

Two of the reviewers had raised following specific concerns   (i) Theory only considers linear encoders and linear/1 hidden layer nonlinear decoders, (ii) The convergence behavior during training is only provided for linear VAEs, (iii) Some statements in the introduction/abstract misrepresent the results in (Dai & Wipf; 2019). However the authors have adequately addressed (i) and (ii) in their response   the paper shows that the correct manifold will only be recovered in the linear case; in the nonlinear case, even for simple manifolds (1 hidden layer) the correct manifold is not recovered as shown by the counterexamples. Authors have also promised to modify the statements in the abstract and introduction to address the concern in (iii). Other two reviewers are largely positive about the paper. The paper makes an important contribution to the VAE literature in further clarifying VAEs  behavior while modeling low dimensional manifolds, and will be a good addition to the conference program.