All the reviewers recommend accept, and the found the paper interesting and novel. 
This paper proposes a differentiable version of CEM, allowing CEM to be used as an operator within end to end training settings. The reviewers all like the idea   it is simple and should be of interest to the community. Unfortunately, the reviewers also are in consensus that the experiments are not sufficiently convincing. We encourage the authors to expand the empirical analysis, based on the reviewer s specific comments, and resubmit the paper to a future venue.
This paper describes how multi agent reinforcement learning at scale leads to the evolution of complex behaviors. Actually, "at scale" may be an understatement   a lot of computing power was used here. But the amount of compute used is not the point, rather the point is that complex and fascinating behavior can emerge from a long co evolutionary process (though gradient based RL is used here, the principle is the same) where the arms race forms an implicit curriculum. This is the existence proof that people in artificial life and adaptive behavior have been looking for for so long.   Two reviewers were positive about the paper, with a third being negative because the paper does not give any new insights about how to do RL at scale. But that was not the stated aim of the paper, as the authors clarify in a response.  This paper will draw quite some attention and deserves an oral presentation.
This paper examines learning problems where the network outputs are intended to be invariant to permutations of the network inputs.  Some past approaches for this problem setting have enforced permutation invariance by construction.  This paper takes a different approach, using a recurrent neural network that passes over the data. The paper proves the network will be permutation invariant when the internal state transition function is associative and commutative.  The paper then focuses on the commutative property by describing a regularization objective that pushes the recurrent network towards becoming commutative.  Experimental results with this regularizer show potentially better performance than DeepSet, another architecture that is designed for permutation invariance.  The subsequent discussion of the paper raised several concerns with the current version of the paper. The theoretical contributions for full permutation invariance follow quickly from the prior DeepSet results.  The paper s focus on commutative regularization in the absence of associative regularization is not compelling if the objective is really for permutation invariance.  The experimental results were limited in scope.  These results lacked error bars and an examination of the relevance of associativity. The reviewers also identified several related lines of work which could provide additional context for the results that were missing from the paper.  This paper is not ready for publication due to the multiple concerns raised by the reviewers.  The paper would become stronger by addressing these concerns, particularly the associativity of the transition function, empirical results, and related work. 
This paper presents a method to learn a pruned convolutional network during conventional training.  Pruning the network has advantages (in deployment) of reducing the final model size and reducing the required FLOPS for compute.  The method adds a pruning mask on each layer with an additional sparsity loss on the mask variables. The method avoids the cost of a train prune retrain optimization process that has been used in several earlier papers.  The method is evaluated on CIFAR 10 and ImageNet with three standard convolutional network architectures.  The results show comparable performance to the original networks with the learned sparse networks.   The reviewers made many substantial comments on the paper and most of these were addressed in the author response and subsequent discussion.  For example, Reviewer1 mentioned two other papers that promote sparsity implicitly during training (Q3), and the authors acknowledged the omission and described how those methods had less flexibility on a target metric (FLOPS) that is not parameter size.  Many of the author responses described changes to an updated paper that would clarify the claims and results (R1: Q2 7, R2:Q3).  However, the reviewers raised many concerns for the original paper and they did not see an updated paper that contains the proposed revisions.  Given the numerous concerns with the original submission, the reviewers wanted to see the revised paper to assess whether their concerns had been addressed adequately. Additionally, the paper does not have a comparison experiment with state of the art results, and the current results were not sufficiently convincing for the reviewers.  Reviewer1 and author response to questions 13 15 suggest that the experimental results with ResNet 34 are inadequate to show the benefits of the approach, but results for the proposed method with the larger ResNet 50 (which could show benefits) are not yet ready.   The current paper is not ready for publication. 
This paper presents an interesting and novel idea that is likely to be of interest to the community. The most negative reviewer did not acknowledge the author response. The AC recommends acceptance.
This was a contentious paper, with quite a large variance in the ratings, and ultimately a lack of consensus. After reading the paper myself, I found it to be a valuable synthesis of common usage of saliency maps and a critique of their improper interpretation. Further, the demonstration of more rigorous methods of evaluating agents based on salience maps using case studies is quite illustrative and compelling. I think we as a field can agree that we’d like to gain better understanding our deep RL models. This is not possible if we don’t have a good understanding of the analysis tools we’re using.  R2 rightly pointed out a need for quantitative justification for their results, in the form of statistical tests, which the authors were able to provide, leading the reviewer to revise their score to the highest value of 8. I thank them for instigating the discussion.  R1 continues to feel that the lack of a methodological contribution (in the form of improving learning within an agent) is a weakness. However, I don’t believe that all papers at deep learning conferences have to have the goal of empirically “learning better” on some benchmark task or dataset, and that there’s room at ICLR for more analysis papers. Indeed, it’d be nice to see more papers like this.   For this reason, I’m inclined to recommend accept for this paper. However this paper does have weaknesses, in that the framework proposed could be made more rigorous and formal. Currently it seems rather adhoc and on a task by task basis (ie we need to have access to game states or define them ourselves for the task). It’s also disappointing that it doesn’t work for recurrent agents, which limits its applicability for analyzing current SOTA deep RL agents. I wonder if authors can comment on possible extensions that would allow for this. 
This paper extends previous work on searching for good neural architectures by iteratively growing a network, including energy aware metrics during the process. There was discussion about the extent of the novelty of this work and how well it was evaluated, and in the end the reviewers felt it was not quite ready for publicaiton.
The authors agree after reading the rebuttal that attacks on MOT are novel.  While the datasets used are small, and the attacks are generated in digital simulation rather than the physical world, this paper still demonstrates an interesting attack on a realistic system.
The paper proposed a non autoregressive attention based encoder decoder model for text to sepectrogram using attention distillation. It is shown to bring good speedup to conventional autoregressive ones. The paper further adopted VAE for the vocoder training which trains from scratch although performs worse than existing method (e.g. ClariNet).   The main concerns for this paper come from the unclear presentation: * As the reviewer pointed out, there re some misleading claims that the speedup gains was obtained without the consideration of the full context (i.e. not including the whole inference time). * The paper failed to clear present the architectures developed/used in the paper and the differences from those used in the literature. The reviewers suggested the use of diagram to aid the presentation. * The two contributions are unbalanced presented. Due to the complexities involved, it s better to explain things in more details.  The authors acknowledged the reviewers comments during rebuttal, but did not make any changes to the paper.
The reviewers generally found the paper s contribution to be valuable and informative, and I believe that this paper should be accepted for publication and a poster presentation. I would strongly recommend to the authors to carefully read over the reviews and address any comments or concerns that were not yet addressed in the rebuttal.
main summary:  method for quantizing GAN  discussion: reviewer 1: well written paper, but reviewer questions novelty reviewer 2: well written, but some details are missing in the paper as well as comparisons to related work reviewer 3: well written and interesting topic, related work section and clarity of results could be improved recommendation: all reviewers agree paper could be improved by better comparison to related work and better clarity of presentation. Marking paper as reject.
The proposed method has very weak novelty.
I have read the paper and the reviews carefully. Despite the numerical scores, I think this paper is above the bar for ICLR, and recommend acceptance.  This paper addresses the now well known problem that generative models often assign higher likelihoods to out of distribution examples, rendering likelihoods useless for OOD detection. They diagnose this as resulting from differences in compressibility of the input, and propose to compensate for this by comparing the log likelihood to the description length from a strong image compressor. They show this performs well against a variety of OOD detection methods.  The idea is a natural one, and certainly should have been one of the first things tried in addressing this phenomenon. I m a little surprised it hasn t been done before, but none of the reviewers or I are aware of a prior reference, so AFAIK it s novel. One reviewer believes the contribution is small; while it s simple, I think the field will benefit from a careful implementation and testing of this approach.  Multiple reviewers raise the concern of whether generative models  bias towards low complexity inputs is just a matter of needing better generative models. I don t think so: even arbitrarily good generative models will still be limited by the inherent compressibility of an input (e.g. as measured by Kolmogorov complexity).  I m also not concerned about the lack of an explicit threshold; if one has proposed a good score function, there are many ways one could choose a threshold, depending on the task. 
The reviewers had several concerns with the paper related to novelty and comparisons with other approaches. During the discussion phase, these concerns were adequately addressed.
This work presents a "shadow attack" that fools certifiably robust networks by producing imperceptible adversarial examples by search outside of the certified radius. The reviewers are generally positive on the novelty and contribution of the work. 
This paper caused a lot of discussions before and after the rebuttal. The concerns are related to the novelty of this paper, which seems to be relatively limited. Since we do not have a champion among positive reviewers, and the overall score is not high enough, I cannot recommend its acceptance at this stage. 
There is a consensus among reviewers that the paper should not be accepted. No rebuttal was provided, so the paper is rejected. 
The paper presents a method for increasing the "model compatibility" of Generative Adversarial Networks by adding a term to the loss function relating to classification boundaries. The reviewers recognized the importance of the problem, but several concerns were raised about the clarity of the paper, as well as the significance of the experimental results.
This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and "pair" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self attentive architectures.   Unfortunately, the paper needs work in presentation (in particular, in Section 3) before it is ready to be published.
This paper implements a novel architecture for inferring loop invariants in verification (though the paper bridges to compilers).  The idea is novel and the paper is well executed.  It is not the usual topic for ICLR, but not presents an important application of deep learning done well, and it has interesting implications for program synthesis.  Therefore, I recommend acceptance.
The submission tackles the problem of data efficiency in RL by building a graph on top of the replay memory and propagate values based on this representation of states and transitions. The method is evaluated on Atari games and is shown to outperform other episodic RL methods.  The reviews were mixed initially but have been brought up by the revisions to the paper and the authors  rebuttal. In particular, there was a concern about theoretical support and the authors added a proof of convergence. They have also added additional experiments and explanations. Given the positive reviews and discussion, the recommendation is to accept this paper.
This paper proposes a novel method for class supervised disentangled representation learning. The method augments an autoencoder with asymmetric noise regularisation and is able to disentangled content (class) and style information from each other. The reviewers agree that the method achieves impressive empirical results and significantly outperforms the baselines. Furthermore, the authors were able to alleviate some of the initial concerns raised by the reviewers during the discussion stage by providing further experimental results and modifying the paper text. By the end of the discussion period some of the reviewers raised their scores and everyone agreed that the paper should be accepted. Hence, I am happy to recommend acceptance.
This paper studies spread divergence between distributions, which may exist in settings where the divergence between said distributions does not. The reviewers feel this work does not have sufficient technical novelty to merit acceptance at this time.
While there was some support for the ideas presented in this paper, it was on the borderline, and ultimately did not make the cut for publication at ICLR.  Concerns were raised as to the significance of the contribution, beyond that of past work.
All reviewers agree that the paper is to be rejected, provided strong claims that were not answered. In this form (especially with such a title) it could not be published (it is more of a technical/engineering interest).
The paper is interested in Chinese Name Entity Recognition, building on a BERT pre trained model. All reviewers agree that the contribution has limited novelty. Motivation leading to the chosen architecture is also missing. In addition, the writing of the paper should be improved. 
The authors propose a method to guarantee the stability of a learnt continuous controller by optimizing the objective through a Lyapunov critic. The method is demonstrated on low dimensional continuous control problems such as cart pole.   The reviewers were mixed in their opinion of the paper, especially after the authors  rebuttal. The concerns center around some of the authors  claims regarding theoretical results, in particular that stability guarantees can be asserted for a model free controller. This claim seems to be incorrect especially on novel data where stability cannot be guaranteed, thus indicating that  robust controller  might be a better description. There are also concerns about the novelty and the contributions of the paper. Overall, the method is promising but the claims need to be carefully written. The recommendation is to reject the paper at this time.
After the author response and paper revision, the reviewers all came to appreciate this paper and unanimously recommended it be accepted.  The paper makes a nice contribution to generative modelling of object oriented representations with large numbers of objects.  The authors adequately addressed the main reviewer concerns with their detailed rebuttal and revision.
This paper proposes blockwise masked attention mechanisms to sparsify Transformer architectures, the main motivation being reducing the memory usage with long sequence inputs. The resulting model is called BlockBERT. The paper falls in a trend of recent papers compressing/sparsifying/distilling Transformer architectures, a very relevant area of research given the daunting resources needed to train these models.  While the proposed contribution is very simple and interesting, it also looks a rather small increment over prior work, namely Sparse Transformer and Adaptive Span Transformer, among others. Experiments are rather limited and the memory/time reduction is not overwhelming (18.7 36.1% less memory, 12.0 25.1% less time), while final accuracy is sometimes sacrificed by a few points. No comparison to other adaptively sparse attention transformer architectures (Correia et al. EMNLP 19 or Sukhbaatar et al. ACL 19) which should as well provide memory reductions due to the sparsity of the gradients, which require less activations to be cached. I suggest addressing this concerns in an eventual resubmission of the paper.
This paper analyzes the weights associated with filters in CNNs and finds that they encode positional information (i.e. near the edges of the image).  A detailed discussion and analysis is performed, which shows where this positional information comes from.    The reviewers were happy with your paper and found it to be quite interesting.  The reviewers felt your paper addressed an important (and surprising!) issue not previously recognized in CNNs.
This paper studies the problem of embedding graphs into continuous spaces.  The authors focus on determining the correct dimension and curvature to minimize distortion or a threshold loss of the embedding. The authors  consider a variety of existing notions of curvature for graphs, introduce a notion of global curvature for the entire graph, and how to efficiently compute it.  Reviewers were positive about the problem under study, but agreed that the current manuscript somewhat lacks a clear contribution. They also pointed out that the goal of using a global notion of curvature should be better motivated. For these reasons, the AC recommends rejection at this time. 
Inspired by WaveGAN, this paper proposes a PUGAN to synthesizes high quality audio in a raw waveform. The paper is well motivated. But all the reviewers find that the paper is lack of clarity and details, and there are some problems in the experiments.
The authors provide an empirical and theoretical exploration of Nesterov momentum, particularly in the over parametrized settings. Nesterov momentum has attracted great interest at various times in deep learning, but its properties and practical utility are not well understood. This paper makes an important step towards shedding some light on this approach for training models with a large number of parameters. 
This paper is a clear reject. The paper is very poorly written and contains zero citations. Also, the reviewers have a hard time understanding what the paper is about.
This paper proposes modifying the training loss for neural net based PDE solvers, by adding an L_infty (max) term to the standard L_2 loss.  The motivation for this loss is sensible in that it matches the definition of a strong solution, but this is only a heuristic motivation, and is missing a theoretical analysis.  This paper s lack of novelty and polish, as well as the lack of clarity in the implementation details, makes this a narrow reject.
This paper presents neural architecture search for semantic segmentation, with search space that integrates multi resolution branches. The method also uses a regularization to overcome the issue of learned networks collapsing to low latency but poor accuracy models. Another interesting contribution is a collaborative search procedure to simultaneously search for student and teacher networks in a single run. All reviewers agree that the proposed method is well motivated and shows promising empirical results. Author response satisfactorily addressed most of the points raised by the reviewers. I recommend acceptance. 
This paper investigates the role of locality (ability to encode only information specific to locations of interest) and compositionality (ability to be expressed as a combination of simpler parts) in Zero Shot Learning (ZSL). Main contributions of the paper are (i) compared to previous ZSL frameworks, the proposed approach is that the model is not allowed to be pretrained on another dataset (ii) a thorough evaluation of existing methods.  Following discussions, weaknesses are (i) the proposed method (CMDIM) isn t sufficiently different or interesting compared to existing methods (ii) the paper does not do an in depth discussion of locality and compositionality. The empirical evaluation being extensive, the accept decision is chosen. 
Double coúnterfactual regret minimization is an extension of neural counterfactual regret minimization that uses separate policy and regret networks (reminiscent of similar extensions of the basic RL formula in reinforcement learning). Several new algorithmic modifications are added to improve the performance.   The reviewers agree that this paper is novel, sound, and interesting. One of the reviewers had a set of questions that the authors responded to, seemingly satisfactorily. Given that this seems to be a high quality paper with no obvious issues, it should be accepted.
Most reviewers seems in favour of accepting this paper, with the borderline rejection being satisfied with acceptance if the authors take special heed of their comments to improve the clarity of the paper when preparing the final version. From examination of the reviews, the paper achieves enough to warrant publication. Accept.
In this work, the authors address a multi task learning setting and propose to enhance the estimation of task dependency with an attention mechanism capturing sample dependant measure of task relatedness. All reviewers and AC agree that the current manuscript lacks clarity and convincing empirical evaluations that clearly show the benefits of the proposed approach w.r.t. state of the art methods. Specifically, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) the empirical evaluations need to be significantly strengthened to show the benefits of the proposed methods over SOTA   see R2’s request to empirically compare with the related recent work [Taskonomy, 2018] and R4’s request to compare with the work [End to end multi task learning with attention, 2018]. R4 also suggested to include an ablation study to assess the benefits of the attention mechanism. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that the proposed attention mechanism plays an important role in the performance of the proposed method.  (2) All reviewers see an issue with the presentation clarity of the conceptual and technical contributions    see R4’s and R2’s detailed comments and questions regarding technical contributions; see R3’s and R4’s comments that the distinction between the general task dependency and the data driven dependency is either not significant or is not clearly articulated; finding better examples to illustrate the difference (instead of reiterating the current ones) would strengthen the clarity and conceptual contributions.   A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs more clarifications, empirical studies and polish to achieve the desired goal.  
Straight Through is a popular, yet not theoretically well understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function.   The paper in its current form is not good enough for publication, and the reviewers believe that the paper contains significant mistakes when deriving the estimator. Furthermore, the Fourier analysis seems unnecessary. 
The authors present an approach for learning graph embeddings by first fusing the graph to generate a new graph with encodes structural information as well as node attribution information. They then iteratively merge nodes based spectral similarities to  obtain coarser graphs. They then use existing methods to learn embeddings from this coarse graph and progressively refine the embeddings to finer graphs. They demonstrate the performance of their method on standard graph datasets.   This paper has received positive reviews from all reviewers. The authors did a good job of addressing the reviewers  concerns and managed to convince the reviewers about their contributions. I request the authors to take the reviewers suggestions into consideration while preparing the final draft of the paper and recommend that the paper be accepted.
The authors propose a novel metric to detect distributional discrepancy for text generation models and argue that these can be used to explain the failure of GANs for language generation tasks. The reviewers found significant deficiencies with the paper, including:  1) Numerous grammatical errors and typos, that make it difficult to read the paper.  2) Mischarcterization of prior work on neural language models, and failure to compare with standard distributional discrepancy measures studied in prior work (KL, total variation, Wasserstein etc.). Further, the necessity of the complicated procedure derived by the authors is not well justified.  3) Failure to run experiments on standard banchmarks for image generation (which are much better studied applications of GANs) and confirm the superiority of the proposed metrics relative to standard baselines.   The reviewers were agreed on the rejection decision and the authors did not participate in the rebuttal phase.  I therefore recommend rejection.
The paper makes a solid contribution to understanding the convergence properties of policy gradient methods with over parameterized neural network function approximators.  This work is concurrent with and not subsumed by other strong work by Agarwal et al. on the same topic.  There is sufficient novelty in this contribution to merit acceptance.  The authors should nevertheless clarify the relationship between their work and the related work noted by AnonReviewer2, in addition to addressing the other comments of the reviewers.
This paper proposes applying potential flow generators in conjunction with L2 optimal transport regularity to favor solutions that "move" input points as little as possible to output points drawn from the target distribution.  The resulting pipeline can be effective in dealing with, among other things, image to image translation tasks with unpaired data.  Overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms (e.g., GANs, etc.).  After the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance.  With these borderline/mixed scores, this paper was discussed at the meta review level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue.  As one important lingering issue, R1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space.  The rebuttal concedes this point, but suggests that the method still seems to work.  But as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue.  However, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology.  Additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision.  For example, the image to image translation experiments with CelebA were based on a linear (PCA) embedding and feedforward networks.  It would have been nice to have seen a more sophisticated setup for this purpose (as discussed in Section 5), especially for a non theoretical paper with an ostensibly practically relevant algorithmic proposal.  And consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes.
A somewhat new approach to growing sparse networks.  Experimental validation is good, focussing on ImageNet and CIFAR 10, plus experiments on language modelling.  Though efficient in computation and storage size, the approach does not have a theoretical foundation.  That does not agree with the intended scope of ICLR.  I strongly suggest the authors submit elsewhere.
The paper proposes a generative model that jointly trains an implicit generative model and an explicit energy based model using Stein s method. There are concerns about technical correctness of the proofs and the authors are advised to look carefully into the points raised by the reviewers. 
The paper proposes a method for lossy image compression. Based on the encoder decoder framework, it replaces the discrete codes by continuous ones, so that the learning can be performed in an end to end way. The idea is interesting, but the motivation is based on a quantization "problem" that the authors show no evidence the competing method is actually suffering from. It is thus unclear how much does quantization in existing methods impact performance, and how much will fixing this benefit the overall system. Also, the authors may add some discussions on whether the proposed sampling of z_{c^\star} is indeed also a form of quantization.  Experimental results are not convincing. The proposed method is only compared with one method. While it works only slightly worse at low bit rate region, the gap becomes larger in higher bit rate regions. Another major concern is that the encoding time is significantly longer. Ablation study is also needed. Finally, the writing can be improved.
This provides a new method, called DPAutoGAN, for the problem of differentially private synthetic generation. The method uses private auto encoder to reduce the dimension of the data, and apply private GAN on the latent space. The reviewers think that there is not sufficient justification for why this is a good approach for synthetic generation. They also think that the presentation is not ready for publication.
This paper conducted a number of empirical studies to find whether units in object classification CNN can be used as object detectors. The claimed conclusion is that there are no units that are sufficient powerful to be considered as object detectors. Three reviewers have split reviews. While reviewer #1 is positive about this work, the review is quite brief. In contrast, Reviewer #2 and #3 both rate weak reject, with similar major concerns. That is, the conclusion seems non conclusive and not surprising as well. What would be the contribution of this type of conclusion to the ICLR community? In particular, Reviewer #2 provided detailed and well elaborated comments. The authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. The ACs concur the major concerns and agree that the paper can not be accepted at its current state.
The paper proposed a refined AIRL method to deal with the reward ambiguity problem in image captioning, wherein the main idea is to refine the loss function in word level instead in sentence level, and introduce a conditional term in the loss function to mitigate mode collapse problem.  The results show the proposed method improves the performance and achieves state of the art performance.  However there are concerns from the reviewers that the motivation of the work was not well explained and some inprecise parts exist in the paper.  The concept of "reward ambiguity problem" is not properly addressed according the opinion of reviewer2.  I would like to see these concerns be well addressed before the paper can be accepted.  
This paper proposes a method to learn sentence representations that incorporates linguistic knowledge in the form of dependency trees using contrastive learning. Experiments on SentEval and probing tasks show that the proposed method underperform baseline methods.  All reviewers agree that the results are not strong enough to support the claim of the paper and have some concerns about the scalability of the implementation. They also agree that the writing of the paper can be improved (details included in their reviews below).   The authors acknowledged these concerns and mentioned that they will use them to improve the paper for future work, so I recommend rejecting this paper for ICLR.
This paper aims to estimate the parameters of a projectile physical equation from a small number of trajectory observations in two computer games. The authors demonstrate that their method works, and that the learnt model generalises from one game to another. However, the reviewers had concerns about the simplicity of the tasks, the longer term value of the proposed method to the research community, and the writing of the paper. During the discussion period, the authors were able to address some of these questions, however many other points were left unanswered, and the authors did not modify the paper to reflect the reviewers’ feedback. Hence, in the current state this paper appears more suitable for a workshop rather than a conference, and I recommend rejection.
This paper introduces a realism metric for generated covariates and then leverage this metric to produce a novel method of interpolating between two real covariates. The reviewers found the method novel and were satisfied with the response form the authors to their concerns. However, Reviewer 4 did have reservations about the response to his/her points 3 and 4. Moreover, in the discussion period it was decided that while the method was well justified by intuition and theory, the empirical evaluation—which is the what matters at the end of the day—was unconvincing.  
The paper focuses on multi agent reinforcement learning applications in network systems control settings. A key consideration is the spatial layout of such systems, and the authors propose a problem formulation designed to leverage structural assumptions (e.g., locality). The authors derive a novel approach / communication protocol for these settings, and demonstrate strong performance and novel insights in realistic applications. Reviewers particularly commended the realistic applications explored here. Clarifying questions about the setting, experiments, and results were addressed in the rebuttal, and the resulting paper is judged to provide valuable novel insights.
This article sets out to study the advantages of depth and overparametrization in neural networks from the perspective of function space, with results on univariate shallow fully connected ReLU networks and some experiments on deep networks.  The article presents results on the concentration /dispersion of the slope / break point distribution of the functions represented by shallow univariate ReLU networks for parameters from various distributions. The reviewers found that the article contains interesting analysis, but that the presentation could be improved. The revision clarified some aspects and included some experiments illustrating breakpoint distributions in relation to the curvature of some target functions. However, the reviewers did not find this convincing enough, pointing out that the analysis focuses on a very restrictive setting and that that presentation of the article still could be improved. The discussion of implicit regularisation in section 2.4 seems promising, but it would benefit from a clearer motivation, background, and discussion. 
This paper has a mixture of weak reviews, the majority of which lean towards reject. All reviews mention a lack of novelty, and 2 of 3 a lack of support in experiments. While the authors argue, perhaps legitimately, for the novelty of the paper with respect to current literature, this is not convincing in the exposition. I recommend that the authors improve the justification for the novelty of their methodology, and strengthen the experiments to convince reviewers. As it stands, this paper is not quite ready for publication.
This paper proposes a model architecture and training procedure for multiple nested label sets of varying granularities and shows improvements in efficiency over simple baselines in the number of fine grained training labels needed to reach a given level of performance.  Reviewers did not raise any serious concerns about the method that was presented, but they were also not convinced that it represented a sufficiently novel or impactful contribution to an open problem. Without any reviewer advocating for the paper, even after discussion, I have no choice but to recommend rejection.  I m open to the possibility that there is substantial technical value here, but I think this work would be well served by more extensive comparisons and a potentially revamped motivation to try to make the case for it that value more directly.
The authors propose a novel approach for measuring gradient staleness and use this measure to penalize stale gradients in an asynchronous stochastic gradient set up. Following previous work, they provide a convergence proof for their approach. Most importantly, they provide extensive evaluations comparing against previous approaches and show impressive gains over previous work.  After the author response, the primary concerns from reviewers is regarding the gap between the proposed method and single worker SGD/synchronous SGD. I feel that the authors have made compelling arguments that ASGD is an important optimization paradigm to consider, so their improvements in narrowing the gap are of interest to the community. There were some concerns about the novelty of the theory, and my impression is that theorem is straightforward to prove based on assumptions and previous work, however, I view the main contribution of the paper as empirical.  This paper is borderline, but I think the impressive empirical results over existing work on ASGD is a worthwhile contribution and others will find it interesting, so I am recommending acceptance.
The paper proposed a waveform to waveform music source separation system. Experimental justification shows the proposed model achieved the best SDR among all the existing waveform to waveform models, and obtained similar performance to spectrogram based ones. The paper is clearly written and the experimental evaluation and ablation study are thorough. But the main concern is the limited novelty, it is an improvement over the existing Wave U Net, it added some changes to the existing model architecture for better modeling the waveform data and compared masking vs. synthesis for music source separation.  
The paper proposes to use the representation learned via CPC to do reward shaping via clustering the embedding and providing a reward based on the distance from the goal.  The reviewers point out some conceptual issues with the paper, the key one being that the method is contingent on a random policy being able to reach the goal, which is not true for difficult environments that the paper claims to be motivated by. One reviewer noted limited experiment runs and lack of comparisons with other reward shaping methods.  I recommend rejection, but hope the authors find the feedback helpful and submit a future version elsewhere.
The paper proposes a method for learning multi image matching using graph neural networks. The model is learned by making use of cycle consistency constraints and geometric consistency, and it achieves a performance that is comparable to the state of the art. While the reviewers view the proposed method interesting in general, they raised issues regarding the evaluation, which is limited in terms of both the chosen datasets and prior methods. After rounds of discussion, the reviewers reached a consensus that the submission is not mature enough to be accepted for this venue at this time. Therefore, I recommend rejecting this submission.
The paper proposed the use of a lossy transform coding approach to to reduce the memory bandwidth brought by the storage of intermediate activations. It has shown the proposed method can bring good memory usage while maintaining the the accuracy. The main concern on this paper is the limited novelty. The lossy transform coding is borrowed from other domains and only the use of it on CNN intermediate activation is new, which seems insufficient. 
This paper demonstrates a framework for optimizing designs in auction/contest problems. The approach relies on considering a multi agent learning process and then simulating it.   To a large degree there is agreement among reviewers that this approach is sensible and sound, however lacks substantial novelty. The authors provided a rebuttal which clarified the aspects that they consider novel, however the reviewers remained mostly unconvinced. Furthermore, it would help if the improvement over past approaches is demonstrated in a more convincing way, for example with increased scope experiments that also involve richer analysis. 
This paper proposes to explore nonnormal matrix initialization in RNNs.  Two reviewers recommended acceptance and one recommended rejection.  The reviewers recommending acceptance highlighted the utility of the approach, its potential to inspire future work, and the clarity and quality of writing and accompanying experiments.  One reviewer recommending weak acceptance expressed appreciation of the quality of the rebuttal and that their concerns were largely addressed.  The reviewer recommending rejection was primarily concerned with the novelty of the method.  Their review suggested the inclusion of an additional citation, which was included in a revised version for the rebuttal but not with a direct comparison of results.  On the balance, the paper has a relatively high degree of support from the reviewers, and presents an interesting and potentially useful initialization in a clear and well motivated way.
This paper introduces a new variant of autoencoders with an topological loss term.  The reviewers appreciated part of the paper and it is borderline. However, there are enough reservations to argue for it will be better for the paper to updated and submitted to next conference.  Rejection is recommended.  
This paper proposes meta learning auxiliary rewards as specified by a DSL. The approach was considered innovative and the results interesting by all reviewers. The paper is clearly of an acceptable standard, with the main concerns raised by reviewers having been addressed (admittedly at the 11th hour) by the authors during the discussion period. Accept.
This submission proposes a new paradigm for modelling temporal point processes by using deep learning to learn to mix log normal distributions in order to directly model the conditional distribution of event time intervals themselves.  Strengths of the paper:  Introduces a new modelling paradigm that can lead to further research in this direction, for an important problem.  Extensive experimentation validates the approach quantitatively.  Easy to read.  Weaknesses:  Several reviewers wanted more details on how the mixing parameter K was tuned. This was adequately addressed during the discussion period.  The reviewer consensus was to accept this submission. 
The submission proposes an approach to accelerate network training by modifying the precision of individual weights, allowing a substantial speed up without a decrease in model accuracy. The magnitude of the activations determines whether it will be computed at a high or low bitwidth.  The reviewers agreed that the paper should be published given the strong results, though there were some salient concerns which the authors should address in their final revision, such as how the method could be implemented on GPU and what savings could be achieved.  Recommendation is to accept.
This paper proposes a general framework for constructing Trojan/Backdoor attacks on deep neural networks. The authors argue that the proposed method can support dynamic and out of scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. This paper has been very carefully discussed. While the idea is interesting and could be of interest to the broader community, all reviewers agree that it lacks of experimental comparison with existing methods for backdoor attacks on benchmark problems. The paper needs to be significantly revised before publication. I encourage the authors to improve this paper and resubmit to future conference.
The paper proposes learning a latent embedding for image manipulation for PixelCNN by using Fisher scores projected to a low dimensional space. The reviewers have several concerns about this paper: * Novelty * Random projection doesn’t learn useful representation * Weak evaluations Since two expert reviewers are negative about this paper, I cannot recommend acceptance at this stage. 
Thanks for the detailed replies to the reviewers. Their score was slightly improved, this paper is still below the bar given high competition of ICLR2020. For this reason, we decided not to accept this paper.
This paper provides a new algorithm for learning fair representation for two different fairness criteria accuracy parity and equalized odds. The reviewers agree that the paper provides novel techniques, although the experiments may appear to be a bit weak. Overall, this paper gives new contributions to the fair representation learning literature.  The authors should consider citing and discussing the relationship with the following work: A Reductions Approach to Fair Classification., ICML 2018
This paper presents a new mechanism to train spiking neural networks that is more suitable for neuromorphic chips. While the text is well written and the experiments provide an interesting analysis, the relevance of the proposed neuron models to the ICLR/ML community seems small at this point. My recommendation is that this paper should be submitted to a more specialised conference/workshop dedicated to hardware methods.
The paper presents an architecture for conditional video generation tasks with temporal self supervision and temporal adversarial learning. The proposed architecture is reasonable but looks somewhat complicated. In terms of technical novelty, the so called "ping pong" loss looks interesting and novel, but other parts are more or less some combinations of existing techniques. Experimental results show promise of the proposed method against selected baselines for video super resolution (VSR) and unpaired video to video translation tasks (UVT). In terms of weakness, (1) the technical novelty is not very high; (2) the final loss is a combination of many losses with many hyperparameters; (3) experimentally the proposed method is not compared against recent SOTA methods on VSR and UVT.   The proposed method should be compared against more recent SOTA baselines for VSR tasks (see examples of references below):  EDVR: Video Restoration with Enhanced Deformable Convolutional Networks https://arxiv.org/abs/1905.02716  Progressive Fusion Video Super Resolution Network via Exploiting Non Local Spatio Temporal Correlations ICCV 2019  Recurrent Back Projection Network for Video Super Resolution CVPR 2019  The same comment would apply for baselines for UVT tasks:  Mocycle GAN: Unpaired Video to Video Translation https://arxiv.org/abs/1908.09514  Preserving Semantic and Temporal Consistency for Unpaired Video to Video Translation https://arxiv.org/abs/1908.07683  Particularly for UVT, the evaluated dataset seems limited in terms of scope as well (i.e., evaluations on more popular benchmarks, such as Viper would be needed for further validation). Overall, given that the contribution of this work is an empirical performance with a rather complex architecture/loss, more comprehensive empirical evaluations on SOTA baselines are warranted. 
The reviewers agree that this is an interesting paper but it required major modifications. After rebuttal, thee paper is much improved but unfortunately not above the bar yet. We encourage the authors to iterate on this work again.
Despite the new ideas in this paper, reviewers feel that it needs to be revised for clarification, and that experimental results are not convincing.  I have down weighted the criticisms of Reviewer 2 because I agree with the authors  rebuttal.  However, there is still not enough support among the remaining reviews to justify acceptance. 
This work introduces Moving Average Batch Normalization (MABN) method to address performance issues of batch normalization in small batch cases. The method is theoretically analyzed and empirically verified on ImageNet and COCO. Some issues were raised by the reviewers, such as restrictive nature of some of the assumptions in the analysis as well as performance degradation due lack of centralizing feature maps. Nevertheless, all the reviewers found the contributions of this paper interesting and important, and they all recommended accept. 
Main description:  paper focuses on training neural networks using 8 bit floating point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption.  Discussions reviewer 3:  gives a very short review and is not knowledagble in this area (rating is weak accept) reviewer 4: well written and convincing paper, some minor technical flaws (not very knowledgable) reviewer 1: interesting paper but argues not very practical (not very knowledgable) reviewer 2: this is the most thorough and knowledable review, and here the authors like the scope of the paper and its interest to ICLR.  Recommendation: going mainly by reviewer 2, i vote to accept this as a poster
The paper studies the role of depth on incremental learning, defined as a favorable learning regime in which one searches through the hypothesis space in increasing order of complexity. Specifically, it establishes a dynamical depth separation result, whereby shallow models require exponetially smaller initializations than deep ones in order to operate in the incremental learning regime.   Despite some concerns shared amongst reviewers about the significance of these results to explain realistic deep models (that exhibit nonlinear behavior as well as interactions between neurons) and some remarks about the precision of some claims, the overall consensus   also shared by the AC   is that this paper puts forward an interesting phenomenon that will likely spark future research in this important direction. The AC thus recommends acceptance. 
The authors propose a novel method to estimate the Lipschitz constant of a neural network, and use this estimate to derive architectures that will have improved adversarial robustness. While the paper contains interesting ideas, the reviewers felt it was not ready for publication due to the following factors:  1) The novelty and significance of the bound derived by the authors is unclear. In particular, the bound used is coarse and likely to be loose, and hence is not likely to be useful in general.  2) The bound on adversarial risk seems of limited significance, since in practice, this can be estimated accurately based on the adversarial risk measured on the test set.  3) The paper is poorly organized with several typos and is hard to read in its present form.  The reviewers were in consensus and the authors did not respond during the rebuttal phase.  Therefore, I recommend rejection. However, all the reviewers found interesting ideas in the paper. Hence, I encourage the authors to consider the reviewers  feedback and submit a revised version to a future venue.
The authors introduce a method to automatically generate a learning curriculum (of goals) in a sparse reward RL setting, examining several criteria for goal setting to induce a useful curriculum.  The reviewers agreed that this was an exciting research direction but also had concerns about baseline comparisons, clarity of some technical points, hyperparameter tuning (and the effect on the strength of empirical results), and computational tractability.  After discussion, the reviewers felt most of these points were sufficiently addressed.  Thus, I recommend acceptance at this time.
SAT is NP complete (Karp, 1972) due its intractable exhaustive search. As such, heuristics are commonly used to reduce the search space. While usually these heuristics rely on some in domain expert knowledge, the authors propose a generic method that uses RL to learn a branching heuristic. The policy is parametrized by a GNN, and at each step selects a variable to expand and the process repeats until either a satisfying assignment has been found or the problem has been proved unsatisfiable. The main result of this is that the proposed heuristic results in fewer steps than VSIDS,  a commonly used heuristic.   All reviewers agreed that this is an interesting and well presented submission. However, both R1 and R2 (rightly according to my judgment) point that at the moment the paper seems to be conducting an evaluation that is not entirely fair. Specifically, VSIDS has been implemented within a framework optimized for running time rather than number of iterations, whereas the proposed heuristic is doing the opposite. Moreover, the proposed heuristic is not stressed test against larger datasets. So, the authors take a heuristic/framework that has been optimized to operate specifically well on large datasets (where running time is what ultimately makes the difference) scale it down to a smaller dataset and evaluate it on a metric that the proposed algorithm is optimized for. At the same time, they do not consider evaluation in larger datasets and defer all concerns about scalability to the one of industrial use vs answering ML questions related to whether or not it is possible to  “stretch existing RL techniques to learn a branching heuristic”. This is a valid point and not all techniques need to be super scalable from iteration day 0, but this being ML, we need to make sure that our evaluation criteria are fair and that we are comparing apples to apples in testing hypotheses. As such, I do not feel comfortable suggesting acceptance of this submission, but I do sincerely hope the authors will take the reviewers  feedback and improve the evaluation protocols of their manuscript, resulting in a stronger future submission.
The paper proposes an adversarial inductive transfer learning method that handles distribution changes in both input and output spaces.  While the studied problem is interesting, reviewers have major concerns about the incremental modeling contribution, the lack of comparative study to existing methods and ablation study to disentangling different modules. Overall, the current study is less convincing from either theoretical analysis or experimental results.  Hence I recommend rejection.
This paper provides a new theoretical framework for domain adaptation by exploring the compression and adaptability.  Reviewers and AC generally agree that this paper discusses about an important problem and provides new insight, but it is not a thorough theoretical work. The reviewers identified several key limitations of the theory such as unrealistic condition and approximation. Some important points still require more work to make the framework practical for algorithm design and computation. The presentation could also be improved.  Hence I recommend rejection.
The authors integrate an interpolation based regularization to develop a graph neural network for semi supervised learning. While reviewers enjoyed the paper, and the authors have provided a thoughtful response, there were remaining questions about clarity of presentation and novelty remaining after the rebuttal period. The authors are encouraged to continue with this work, accounting for reviewer comments in future revisions.
The authors identify a limitation of aggregating GNNs, which is that global structure can be mostly lost. They propose a method which combines a graph embedding with the spatial convolution GNN and show that the resulting GNN can better distinguish between similar local structures.   The reviewers were mixed in their scores. The proposed approach is clearly motivated and justified and may be relelvant for some graphnet researchers, but the approach is only applicable in some circumstances   in other cases it may be desirable to ignore global structure. This, plus the high computational complexity of the proposed approach, mean that the significance is weaker. Overall the reviewers felt that the contribution was not significant enough and that the results were not statistically convincing.  Decision is to reject.
The submission is concerned with providing a transport based formulation for generative modeling in order to avoid the standard max/min optimization challenge of GANs. The authors propose representing the divergence with a fluid flow model, the solution of which can be found by discretizing the space, resulting in an alignment of high dimensional point clouds.   The authors disagreed about the novelty and clarity of the work, but they did agree that the empirical and theoretical support was lacking, and that the paper could be substantially improved through better validation and better results   in particular, the approach struggles with MNIST digit generation compared to other methods.  The recommendation is to not accept the submission at this time.
This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out of distribution detection while generating samples with better quality than GAN based approaches. The reviewers are very excited about this work, and the energy based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response.
This paper proposes to quantize the weights of neural networks that can minimize the L_2 loss between the quantized values and the full precision ones. The paper has limited novelty, as many of the solutions presented in the paper have already been discovered in the literature. During the discussion, the reviewers agree that it is an incremental contribution. Parts of the paper can also be clarified, particularly on the optimality of the solution, assumptions used in the approximation, and some of the experimental results. Experimental results can also be made more convincing by adding comparision with the more recent quantization methods.
Main content: BasiGAN, a novel method for  introducing stochasticity in conditional GANs Summary of discussion: reviewer1: interesting work and results on GANs. Reviewer had a question on pre defned basis but i think it was answered by the authors.  reviewer3: interesting and novel work on GANS, wel written paper and improves on SOTA. The main uestion is around bases again like reviewer 1, but it seems the authors have addressed this. reviewer4: Novel interesting work. Main comments are around making Theorem 1 more theoretically correct, which it sounds like the authors addressed. Recommendation: Poster. Well written and novel paper and authors addressed a lot of concerns. 
The paper proposes a doubly robust off policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias. The reviewers unanimously recommend acceptance of this paper.
The paper presents AnoDM (Anomaly detection based on unsupervised Disentangled representation learning and Manifold learning) that combine beta VAE and t SNE for anomaly detection. Experiment results on both image and time series data are shown to demonstrate the effectiveness of the proposed solution.   The paper aims to attack a challenging problem. The proposed solution is reasonable. The authors did a job at addressing some of the concerns raised in the reviews. However, two major concerns remain: (1) the novelty in the proposed model (a combination of two existing models) is not clear; (2) the experiment results are not fully convincing. While theoretical analysis is not a must for all models, it would be useful to conduct thorough experiments to fully understand how the model works, which is missing in the current version.   Given the two reasons above, the paper did not attract enough enthusiasm from the reviewers during the discussion. We hope the reviews can help improve the paper for a better publication in the future.     
After the rebuttal, the reviewers agree that this paper would benefit from further revisions to clarify issues regarding the motivation of the DP based security definition,  any relationship it may have to standard definitions of privacy, and the role of dimensionality in the theoretical guarantees.
The aper introduces simplicial complex networks, a new class of neural networks based on the idea of the subdivision of a simplicial complex. The paper is interesting and brings ideas of algebraic topology to inform the design of new neural network architectures.   Reviewer 1 was positive about the ideas of this paper, but had several concerns about clarity, scalablity and the sense that the paper might still be in an early phase. Reviewer 2 had similar concerns about clarity, comparisons, and usefulness. Although there were no responses form the author, the discussion explored the paper further, but continued to think the idea is still in its early phase.  The paper is not currently ready for acceptance, and we hope the authors will find useful feedback for their ongoing reasearch. 
The paper presents a new architecture that achieves the advantages of both Bi encoder and Cross encoder architectures. The proposed idea is reasonable and well motivated, and the paper is clearly written. The experimental results on retrieval and dialog tasks are strong, achieving high accuracy while the computational efficiency is orders of magnitude smaller than Cross encoder. All reviewers recommend acceptance of the paper and this AC concurs.
This paper introduces a new approach that consists of the invertible autoencoder and a reversible predictive module (RPM) for video future frame prediction.  Reviewers agree that the paper is well written and the contributions are clear. It achieves new state of the art results on a diverse set of video prediction datasets and with techniques that enable more efficient computation and memory footprint. Also, the video representation learned in a self supervised way by the approach can have good generalization ability on downstream tasks such as object detection. The concerns of the paper were relatively minor, and were successfully addressed in the rebuttal.  AC feels that this work makes a solid contribution with well designed model and strong empirical performance, which will attain wide interests in the area of video future frame prediction and self supervised video representation learning.  Hence, I recommend accepting this paper.
This paper presents an efficient RNN architecture that dynamically switches big and little modules during inference. In the experiments, authors demonstrate that the proposed method achieves favorable speed up compared to baselines, and the contribution is orthogonal to weight pruning.  All reviewers agree that the paper is well written and that the proposed method is easy to understand and reasonable. However, its methodological contribution is limited because the core idea is essentially the same as distillation, and dynamically gating the modules is a common technique in general. Moreover, I agree with the reviewers that the method should be compared with more other state of the art methods in this context. Accelerating or compressing DNNs are intensively studied topics and there are many approaches other than weight pruning, as authors also mention in the paper. As the possible contribution of the paper is more on the empirical side, it is necessary to thoroughly compare with other possible approaches to show that the proposed method is really a good solution in practice. For these reasons, I’d like to recommend rejection.  
This paper studies Graph Neural Networks for quantum chemistry by incorporating a number of physics informed innovations into the architecture. In particular, it considers directional edge information while preserving equivariance.  Reviewers were in agreement that this is an excellent paper with strong empirical results, great empirical evaluation and clear exposition. Despite some concerns about the limited novelty in terms of GNN methodology ( for instance, directional message passing has appeared in previous GNN papers, see e.g. https://openreview.net/forum?id H1g0Z3A9Fm , in a different context). Ultimately, the AC believes this is a strong, high quality work that will be of broad interest, and thus recommends acceptance. 
This paper proposes a variant of Hamiltonian Monte Carlo for Bayesian inference in deep learning.  Although the reviewers acknowledge the ambition, scope and novelty of the paper they still have a number of reservations regarding experimental results and claims (regarding need for hyperparameter tuning). The overall score consequently falls below acceptance.  Rejection is recommended. These reservations made by the referees should definitely be addressable before next conference deadline so looking forward to see the paper published asap.
The paper addressed the problem of machine bias when training machine learning models. The authors propose an approach based on representation learning with adversarial training. As opposed to the majority of previous works that trying to create a representation from which it is not possible to predict the sensitive feature (bias), the authors propose to minimize the dependency between the learned features and the sensitive feature with adversarial training. While acknowledging that the proposed model is addressing an important problem and is potentially useful, the reviewers and AC note the following potential weaknesses:   (1) limited technical contribution   the proposed approach is similar to a number of works published in machine learning and computer vision before the submission deadline that were overlooked by the authors. Specifically: i) adversarial training for learning fair representations [Edwards and Storkey, Censoring Representations with an Adversary, ICLR 2016], [Beutel, et al 2017, Data decisions and theoretical implications when adversarially learning fair representations], ii) learning fair representation by minimizing the dependency between the latent representation and the sensitive attributes [The variational fair autoencoder, ICLR 2016 by Louizos et al.; Fairness Constraints: Mechanisms for Fair Classification, by Zafar et al, 2015] or by minimizing the mutual information between feature embedding and bias [Learning Not to Learn: Training Deep Neural Networks with Biased Data, CVPR 2019].  (2) Limited empirical evidence   the baseline methods used in the evaluation are not sufficient to assess the benefits of the proposed approach over the existing SOTA methods mentioned above. In fact, none of the baseline methods used in the evaluation tackle machine bias (via adversarial training or minimizing statistical dependence).  (3) It would be beneficial to also report fairness metrics, e.g. equality of opportunity, statistical parity, to assess the effectiveness of bias removal. R1 has raised some concerns regarding empirical evidence   see the point about mixed results. Also R2 has reported concerns regarding controversial results in experiment 4.2 and suggested ways to justify when and why the results of the CNNs baseline are close to the BR Net. Addressing these concerns would strengthen the contributions of the proposed method.      Among these, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This paper proposes PAC_Bayesian bounds for negative log likelihood loss function. A few reviewers raised concerns around 1) distinguish their contributions better from prior work (eg Alquier). 2) confounders in their experiments. Both reviewers agreed that the paper, as it is written, does not provide sufficient evidence of significance. In addition, experiments shown in the paper varies two things   # parameters (therefore expressiveness and potential generalizability) and depth at each setting. As pointed out, this isn’t right   in order to capture the effect, one has to control for all confounders carefully. Another concerned raised were around Theorem 2   that it contains data distribution on the right hand side, which isn’t all that useful to calculate generalization bounds (we don’t have access to the distribution). We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions.  
The paper extends LISTA by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of LISTA. The authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results. All reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments. All three reviewers recommended accept. 
The paper proposes a method to learn cross lingual representations by aligning monolingual models with the help of a parallel corpus using a three step process: transform, extract, and reorder. Experiments on XNLI show that the proposed method is able to perform zero shot cross lingual transfer, although its overall performance is still below state of the art jointly trained method XLM.  All three reviewers suggested that the proposed method needs to be evaluated more thoroughly (more datasets and languages). R2 and R4 raise some concerns around the complexity of the proposed method (possibly could be simplified further). R3 suggests a more thorough investigation on why the model saturates at 250,000 parallel sentences, among others.  The authors acknowledged reviewers  concerns in their response and will incorporate them in future work.  I recommend rejecting this paper for ICLR.
The authors introducing programming puzzles as a way to help AI systems learn about reasoning. The authors then propose a GAN like generation algorithm to generate diverse and difficult puzzles.  This is a very novel problem and the authors have made an interesting submission. However, at least 2 reviewers have raised severe concerns about the work. In particular, the relation to existing work as pointed by R2 was not very clear. Further, the paper was also lacking a strong empirical evaluation of the proposed ideas.  The authors did agree with most of the comments of the reviewers and made changes wherever possible. However, some changes have been pushed to future work or are not feasible right now.   Based on the above observations, I recommended that the paper cannot be accepted now. The paper has a lot of potential and I would strongly encourage a revised submission addressing the questions/suggestions made by the reviewers.
The submission proposes to train a model to modify objects in an image using language (the modified image is the effect of an action). The model combines CNN, RNN, Relation Nets and GAN and is trained and evaluated on synthetic data, with some examples of results on real images.  The paper received relatively low scores (1 reject and 2 weak rejects).  The authors did not provide any responses to the reviews and did not revise their submission.  Thus there was no reviewer discussion and the scores remained unchanged.  The reviewers all agreed that the submission addressed an interesting task, but there was no special insight in how the components were put together, and the work was limited in the experimental results.  Comparisons against additional baselines (AE, VAE), and ablation studies or examinations of how the components can be varied is needed.  The paper is currently too weak to be accepted at ICLR.  The authors are encouraged to improve their evaluation and resubmit to an appropriate venue.
The authors present an approach to learning from noisy labels. The reviews were mixed and several issues remain unresolved. I do not accept the following as a valid response: "We fully agree that noisily collected labels are common for many problems other than image classification. However, the focus of our paper is image classification, and we thus concentrate on classification problems related to the widely popular CIFAR 10 and ImageNet classification problems." ICLR is a conference on theoretical and applied ML, and the fact that a technique has not been used for image classification before, does not mean you bring something to the table by doing so. The NLP literature is abundant with interesting work on label noise and should obviously be considered related work. That said, there s also missing references directly related to the connection between early stopping/regularization and label bias correction, including:   [0] https://arxiv.org/pdf/1904.11238.pdf [1] https://arxiv.org/pdf/1705.03419.pdf [2] http://proceedings.mlr.press/v80/ma18d/ma18d.pdf  See also this paper submitted to this conference: https://openreview.net/forum?id SJldu6EtDS
Under the optimization formulation of adversarial attack, this paper proposes two methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI FGSM) and Scale Invariant attack Method (SIM). NI FGSM adapts Nesterov accelerated gradient into the iterative attacks to effectively look ahead and avoid the “missing” of the global maximum, and SIM optimizes the adversarial perturbations over the scale copies of the input images so as to avoid “overfitting” on the white box model being attacked and generate more transferable adversarial examples. Empirical results demonstrate the effectiveness of the proposed methods. The ideas are sensible, and the empirical studies were strengthened during rebuttal.
This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied).
This paper proposes augmentation of the state exploration strategy that is interesting and has a potential to lead to improvement. However, the current presentation makes it difficult to properly assess that. In particular, the way the authors convey both the underlying intuition and its implementation is fairly vague and does not build confidence in the grounding of the underlying methodology.
The reviewers all agreed that the proposed modification was minor. I encourage the authors to pursue in this direction, as they mentioned in their rebuttal, before resubmitting to another conference.
The present paper establishes uniform approximation theorems (UATs) for PointNet and DeepSets that do not fix the cardinality of the input set.   Two nonexperts read the paper and came away not understanding what this exercise has taught us and why the weakening of the hypotheses was important. The authors made no attempt to argue these points in their rebuttals and so I went looking at the paper to find the answer in their revisions, but did not find it after scanning through the paper. I think a paper like this needs to explain what is gained and what obstructions earlier approaches met, and why the current techniques side step those. One of the reviewers felt that the fixed cardinality assumption was mild. I m really not sure why the authors didn t attack this idea. Maybe it is mild in some technical sense?  What I read of the paper seemed excellent in term of style and clarity. I think the paper simply needs to make a better case that it is not merely an exercise in topology. I think the result here is publishable on its own grounds, but for the paper to effectively communicate those findings, the authors should have revised it to address these issues. They chose not to and so I recommend ICLR take a pass. Once the reviewers revised the framing and scope/impact, provided it doesn t sound trivial, I think it ll be ready for publication.  
This paper proposes a new graph Hierarchy representation (HAG) which eliminates the redundancy during the aggregation stage and improves computation efficiency. It achieves good speedup and also provide theoretical analysis. There has been several concerns from the reviewers; authors  response addressed them partially. Despite this, due to the large number of strong papers, we cannot accept the paper at this time. We encourage the authors to further improve the work for a future version.     
This paper develops ideas for enabling the data generation with GANs in the presence of structured constraints on the data manifold. This problem is interesting and quite relevant to the ICLR community. The reviewers raised concerns about the similarity to prior work (Xu et al  17), and missing comparisons to previous approaches that study this problem (e.g. Hu et al  18) that make it difficult to judge the significance of the work. Overall, the paper is slightly below the bar for acceptance.
The paper presents an algorithm to compute mixed strategy Nash equilibria for continuous action space games. While the paper has some novelty, reviewers are generally unimpressed with the assumptions made, and the quality of the writing. Reviewers were also not swayed by the responses from the authors. Additionally, it could be argued that the paper is somewhat peripheral to the topic of the conference.¨  On balance, I would recommend reject for now; the paper needs more work.
Thanks to the authors for the revision and discussion. This paper provides a neural architecture search (NAS) method, called Evolutionary Neural hybrid agents (Evo NAS), which combines NN based NAS and Aging EVO. While the authors  response addressed some of the reviewers  comments, during discussion period there is a new concern that the idea proposed here highly overlaps with the method of RENAS, which stands for Reinforced Evolutionary Neural Architecture Search. Reviewers acknowledge that this might discount the novelty of the paper. Overall, there is not sufficient support for acceptance.
The paper proposes an iterative learning method that jointly trains both a model and a scorer network that places a non uniform weights on data points, which estimates the importance of each data point for training.  This leads to significant improvement on several benchmarks.  The reviewers mostly agreed that the approach is novel and that the benchmark results were impressive, especially on Imagenet.  There were both clarity issues about methodology and experiments, as well as concerns about several technical issues.  The reviewers felt that the rebuttal resolved the majority of minor technical issues, but did not sufficiently clarify the more significant methodological concerns. Thus, I recommend rejection at this time.
The reviewers found the aim of the paper interesting (to connect representation quality with adversarial examples). However, the reviewers consistently pointed out writing issues, such as inaccurate or unsubstantiated claims, which are not appropriate for a scientific venue. The reviewers also found the experiments, which are on simple datasets, unconvincing.
This paper proposes an end to end deep reinforcement learning based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines.  The presentation is clear and the results are interesting, but the novelty seems insufficient for ICLR. The proposed model is based on transformer with the following changes: * encoder: position embedding is removed, state embedding is added to the multi head attention layer and feed forward layer of the original transformer encoder; * decoder: three decoders one for the three steps, namely selection, rotation and location. * training: actor critic algorithm
Two reviewers as well as the AC are confused by the paper—perhaps because the readability of it should be improved?  It is clear that the page limitation of conferences are problematic, with 7 pages of appendix (not part of the review) the authors may consider another venue to publish.  In its current form, the usefulness for the ICLR community seems limited.
This paper proposes a method to leverage the Lead (i.e., first sentence of an article) in training a model for abstractive news summarization.   Reviewers  initial recommendations were weak reject to weak accept, pointing out the limitations of the paper including 1) little novelty in modeling, 2) weak evaluation, and 3) lack of deep analysis. After the author rebuttal and revised paper, one of the reviewers increased the score and were leaning toward weak accept.   However, reviewers noted that there was significant overlap with another submission, and we discussed that it would be best to accept one of the two, incorporating the contributions of both papers. Hence, I recommend that this paper not be accepted, and perhaps some of the non overlapping contents of this paper can be included in the other, accepted paper.  Thank you for submitting this paper. I enjoyed reading it.
This paper presents a theoretically motivated method based on homotopy continuation for transfer learning and demonstrates encouraging results on FashionMNIST and CIFAR 10. The authors draw a connection between this approach and the widely used fine tuning heuristic. Reviewers find principled approaches to transfer learning in deep neural networks an important direction, and find the contributions of this paper an encouraging step in that direction. Alongside with the reviewers, I think homotopy continuation is a great numerical tool with a lot of untapped potentials for ML applications, and I am happy to see an instantiation of this approach for transfer learning. Reviewers had some concerns about experimental evaluations (reporting test performance in addition to training), and the writing of the draft. The authors addressed these in the revised version by including test performance in the appendix and rewriting the first parts of the paper. Two out of three reviewers recommend accept. I also find the homotopy analysis interesting and alongside with majority of reviewers, recommend accept. However, please try to iterate at least once more over the writing; simply long sentences and make sure the writing and flow are, for the camera ready version.
This papers addresses the problem of creating sentiment lexicon for a resource limited language (Amharic). This task is time consuming and requires skilled annotators. Hence the authors propose a method for constructing this automatically from News corpora. They start with a seed list of sentiment bearing words and then add new words to this list based on their PPMO scores with existing words.   While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below:  1) Lack of novelty: The work has very few new ideas 2) Lack of comparison with existing work: Several missing citations have been pointed out by the reviewers 3) Weak experiments: The experimental section needs to be strengthened with more comparisons to existing work as well as proving the results for at least one more language.  4) Organisation of the paper: The paper needs to be restructured for better presentation. In particular,  the Results and Discussions section does not really contain any discussions. 5) Grammatical errors: Please proofread the paper thoroughly and fix all grammatical and typo errors.  Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted.  
The paper proposes a contextual reasoning module following the approach proposed by the NIPS 2011 paper for object detection. Although the reviewers find the proposed approach reasonable, the experimental results are weak and noisy. Multiple reviewers believe that the paper will benefit from another review cycle, pointing out that the authors response confirmed that multiple additional (or redoing of) experiments are needed.  
This paper studies the “suspended animation limit” of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion.  I thus recommend reject.
The paper investigates a new approach to classification of irregularly sampled and unaligned multi modal time series via set function mapping. Experiment results on health care datasets are reported to demonstrate the effectiveness of the proposed approach.   The idea of extending set functions to address missing value in time series is interesting and novel. The paper does a good job at motivating the methods and describing the proposed solution. The authors did a good job at addressing the concerns of the reviewers.   During the discussion, some reviewers are still concerned about the empirical results, which do not match well with published results (even though the authors provided an explanation for it). In addition, the proposed method is only tested on the health care datasets, but the improvement is limited. Therefore it would be worthwhile investigating other time series datasets, and most important answering the important question in terms of what datasets/applications the proposed method works well.   The paper is one step away for being a strong publication. We hope the reviews can help improve the paper for a strong publication in the future. 
This was an extremely difficult paper to decide, as it attracted significant commentary (and controversy) that led to non trivial corrections in the results.  One of the main criticisms is that the work is an incremental combination of existing results.  A potentially bigger concern is that of correctness: the main convergence rate was changed from 1/T to 1/sqrt{T} during the rebuttal and revision process.  Such a change is not trivial and essentially proves the initial submission was incorrect.  In general, it is not prudent to accept a hastily revised theory paper without a proper assessment of correctness in its modified form.  Therefore, I think it would be premature to accept this paper without a full review cycle that assessed the revised form.  There also appear to be technical challenges from the discussion that remain unaddressed.  Any resubmission will also have to highlight significance and make a stronger case for the novelty of the results.
The paper presents a method for intrinsically motivated exploration using successor features by interleaving the exploration task with intrinsic rewards and extrinsic task original external rewards. In addition, the paper proposes "successor feature control" (distance between consecutive successor features) as an intrinsic reward. The proposed method is interesting and it can potentially address the limitation of existing exploration methods based on intrinsic motivation. In experimental results, the method is evaluated on navigation tasks using Vizdoom and DeepMind Lab, as well as continuous control tasks of Cartpole in the DeepMind control suite, with promising results.   On the negative side, there are some domain specific properties (e.g., moderate map size with relatively simple structures, different rooms having visually distinct patterns, bottleneck states generally leading to better rewards, etc.) that make the proposed method work well. In addition, off policy learning of the successor features could be a potential technical issue. Finally, the proposed method is not evaluated against stronger baselines on harder exploration tasks (such as Atari Montezuma s revenge, etc.), thus the addition of such results would make the paper more convincing. In the current form, the paper seems to need more work to be acceptable for ICLR.
This paper proposes to train and compose neural networks for the purposes of arithmetic operations. All reviewers agree that the motivation for such a work is unclear, and the general presentation in the paper can be significantly improved. As such, I cannot recommend this paper in its current state for publication. 
This paper tackles an interesting problem: "How should we evaluate models when the test data contains noisy labels?". This is a particularly relevant question in the medical imaging domain where expert annotators often disagree with each other. The paper proposes a new metric "discrepancy ratio" which computes the ratio how often the model disagrees with humans to how often humans disagree with each other. The paper shows that under certain noise models for the human annotations the discrepancy ratio can exactly determine when a model is more accurate than humans, whereas commonly used baselines such as comparing with the majority vote do not have this property. Reviewers were satisfied with the author rebuttal, particularly with the clarification that the goal of the metric is to accurately determine when model performance exceeds that of human annotators, and not to better rank models. The metric should be quite useful, assuming users are cautious of the limitations described by the authors.
This paper provides a series of empirical evaluations on a small neural architecture search space with 64 architectures. The experiments are interesting, but limited in scope and limited to 64 architectures trained on CIFAR 10. It is unclear whether lessons learned on this search space would transfer to large search spaces. One upside is that code is available, making the work reproducible.  All reviewers read the rebuttal and participated in the private discussion of reviewers and AC, but none of them changed their mind. All gave a weak rejection score.  I agree with this assessment and therefore recommend rejection.
The paper presents a quantization method that generates per layer hybrid filter banks consisting of full precision and ternary weight filters for MobileNets. The paper is well written. However, it is incremental. Moreover, empirical results are not convincing enough. Experiments are only performed on ImageNet. Comparison on more datasets and more model architectures should be performed.
This paper proposes to perform sample selection for deep learning   which can be very computationally expensive   using a smaller and simpler proxy network. The paper shows that such proxies are faster to train and do not substantially harm the accuracy of the final network.  The reviewers were all in agreement that the problem is important, and that the paper is comprehensive and well executed. I therefore recommend it should be accepted.
This paper studies the properties of regions where a DNN with piecewise linear activations behaves linearly. They develop a variety of techniques to chracterize properties and show how these properties correlate with various parameters of the network architecture and training method.  The reviewers were in consensus on the quality of the paper: The paper is well written and contains a number of insights that would be of broad interest to the deep learning community.  I therefore recommend acceptance.
The authors introduce a framework for continual learning in neural networks based on sparse Gaussian process methods. The reviewers had a number of questions and concerns, that were adequately addressed during the discussion phase. This is an interesting addition to the continual learning literature. Please be sure to update the paper based on the discussion.
The paper focuses on large scale multi agent reinforcement learning and proposes Learning Structured Communication (LSC) to deal issues of scale and learn sample efficiently. Reviewers are positive about the presented ideas, but note remaining limitations. In particular, the empirical validation does not lead to sufficiently novel insights, and additional analysis is needed to round out the paper.
The paper proposed the use of a shallow layers with large receptive fields for feature extraction to be used in stereo matching tasks. It showed on the KITTI2015 dataset this method leads to large model size reducetion while maintaining a comparable performance.  The main conern on this paper is the lack of technical contributions: * The task of stereo matching is very specialized one, simply presenting the model size reduction and performance is not interesting to general readers. Adding more analysis that help understanding why the proposed method helps in this particular task and for what kind of tasks a shallow feature instead a deeper one is perferred. In that way, the paper would be addressing much wider audiences.  * The discussions on related work is not thorough enough, lacking of analysis of pros and cons between different methods.
This paper proposes performs an empirical study to evaluate CNN based object classifier for the case where the object of interest is very small relative to the size of the image. Two synthetic databases are used to conduct the experiments, through which the authors made a number of observations and conclusions. The reviewers concern that the databases used are too structured or artificial, and one of the two databases is very small as well. On top of that, only one network architecture is used for evaluation. Furthermore, the conclusion from two databases seem inconsistent as well. The authors provided detailed responses to the reviewers  comments but were not able to change the overall rating of the paper. Given these concerns, as well as no methodological contribution, there are general concerns from all reviewers that the contributions of this work is not sufficient for ICLR. The ACs concur the concerns and the paper can not be accepted at its current state.
The authors propose a hardware agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. They then demonstrate that models with fewer parameters achieve far better accuracy after quantization. The main novelty is on the metric ESN. However, ESN is based on ideal hardware, and thus not suitable for existing hardware. Assumptions made in the paper are hard to be proved. Experimental results are not convincing, and related pruning methods are not compared. Finally, the paper is not written clearly, and the structure and some arguments are confusing.
This work presents a routing algorithm for capsule networks, and demonstrates empirical evaluation on CIFAR 10 and CIFAR 100. The results outperform existing capsule networks and are at par with CNNs. Reviewers appreciated the novelty, introducing a new simpler routing mechanism, and achieving good performance on real world datasets. In particular, removing the squash function and experimenting with concurrent routing was highlighted as significant progress. There were some concerns (e.g. claiming novelty for inverted dot product attention) and clarification questions (e.g. same learning rate schedule for all models). The authors provided a response and revised the submission , which addresses most of these concerns. At the end, majority of reviewers recommended accept. Alongside with them, I acknowledge the novelty of using layer norm and parallel execution, and recommend accept. 
This paper theoretically analyzes the use of an oracle to predict various quantities in data stream models.  Building upon Hsu et al., (2019), the overriding goal is to examine the degree to which such an oracle is can provide memory and time improvements across broad streaming regimes.  In doing so, optimal bounds are derived in conjunction with a heavy hitter oracle.  Although the rebuttal and discussion period did not lead to a consensus in the scoring of this paper, two reviewers were highly supportive.  However, the primary criticism from the lone dissenting reviewer was based on the high level presentation and motivation, and in particular, the impression that the paper read more like a STOC theory paper.  In this regard though, my belief is that the authors can easily tailor a revision to increase the accessibility to a wider ICLR audience.
This paper proposes a novel technique for matrix completion, using graphical neighborhood structure to side step the need for any side information.  Post rebuttal, the reviewers converged on a unanimous decision to accept. The authors are encouraged to review to address reviewer comments.
This paper describes a method for generating adversarial examples from images and text such that they maintain the semantics of the input.  The reviewers saw a lot of value in this work, but also some flaws.  The review process seemed to help answer many questions, but a few remain: there are some questions about the strength of the empirical results on text after the author s updates. Wether the adversarial images stay on the manifold is questioned (are blurry or otherwise noisy images "on manifold"?).  One reviewer raises good questions about the soundness of the comparison to the Song paper.  I think this review process has been very productive, and I hope the authors will agree.  I hope this feedback helps them to improve their paper.
This paper proposes an application of capsule networks to code modeling.  I see the potential in this approach, but as the reviewers pointed out, in the current draft there are significant issues with respect to both clarity of motivating the work, and in the empirical results (which start at a much lower baseline than previous work). I am not recommending acceptance at this time, but would encourage the reviewers to clarify the issues raised in the reviews for future submission.
This paper studies Differentiable Neural Architecture Search, focusing on a problem identified with the approximated gradient with respect to architectural parameters, and proposing an improved gradient estimation procedure. The authors claim that this alleviates the tendency of DARTS to collapse on degenerate architectures consisting of e.g. all skip connections, presently dealt with via early stopping.  Reviewers generally liked the theoretical contribution, but found the evidence insufficient to support the claims. Requests for experiments by R1 with matched hyperparameters were granted (and several reviewers felt this strengthened the submission), though relegated to an appendix, but after a lengthy discussion reviewers still felt the evidence was insufficient.  R1 also contended that the authors were overly dogmatic regarding "AutoML"   that the early stopping heuristic was undesirable because of the additional human knowledge involved. I appreciate the sentiment but find this argument unconvincing   while it is true that a great deal of human knowledge is still necessary to make architecture search work, the aim is certainly to develop fool proof automatic methods.   As reviewers were still unsatisfied with the empirical investigation after revisions and found that the weight of the contribution was insufficient for a 10 page paper, I recommend rejection at this time, while encouraging the authors to take seriously the reviewers  requests for a systematic study of the source of the empirical gains in order to strengthen their paper for future submission.
I had a little bit of difficulty with my recommendation here, but in the end I don t feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation.  Standard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating "semantically meaningful" perturbations goes against the standard definition of adversarial attacks. This left me with two questions:  1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this.  2. In what situation would such an attack method would be practically useful?  Even the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here.  While I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!
This paper has been withdrawn by the authors.
This paper proposes 1) using neural guided Monte Carlo Tree Search to search for expressions that match a dataset and 2) Augments the loss to match the asymptotics of the true function when these are given.  The use of MCTS sounds more sensible than standard evolutionary search.  The augmented loss could make sense but seems extremely niche, requiring specific side information about the problem being solved.  Overall, the task is so niche that I don t think it ll be of wide interest.  It s not clear that it s solving a real problem.
The paper investigates out of distribution detection for regression tasks.  The reviewers raised several concerns about novelty of the method relative to existing methods, motivation & theoretical justification and clarity of the presentation  (in particular, the discussion around regression vs classification).   I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
The authors introduce a new associative inference task from cognitive psychology, show shortcomings of current memory augmented architectures, and introduce a new memory architecture that performs better with respect to the task. The reviewers like the motivation and thought the experimental results were strong, although they also initially had several questions and pointed to areas of the paper which lacked clarity. The authors updated the paper in response to the reviewer s questions and increased the clarity of the paper. The reviewers are satisfied and believe the paper should be accepted.
The authors introduce a framework for inverse reinforcement learning tasks whose reward functions are dependent on context variables and provide a solution by formulating it as a convex optimization problem.  Overall, the authors agreed that the method appears to be sound.  However, after discussion there were lingering concerns about (1) in what situations this framework is useful or advantageous, (2) how it compares to existing, modern IRL algorithms that take context into account, and (3) if the theoretical and experimental results were truly useful in evaluating the algorithm.  Given that these issues were not able to be fully resolved, I recommend that this paper be rejected at this time.
The paper proposed a new seq2seq method to implement natural language to formal language translation.  Fixed length Tensor Product Representations are used as the intermediate representation between encoder and decoder.  Experiments are conducted on MathQA and AlgoList datasets and show the effectiveness of the methods.  Intensive discussions happened between the authors and reviewers.  Despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer#3 and reviewer#4 is that there is a gap between the  theory and the implementation in this paper.  The other reviewer (#2) likes the paper but is less confident and tend to agree with the other two reviewers.
This paper aims to analyze CNN representations in terms of how well they measure the perceptual severity of image distortions.  In particularly, (a) sensitivity to changes in visual frequency and (b) orientation selectivity was used. Although the reviewers agree that this paper presents some interesting initial findings with a promising direction, the majority of the reviewers (three out of four) find that the paper is incomplete, raising concerns in terms of experimental settings and results. Multiple reviewers explicitly asked for additional experiments to confirm whether the presented empirical results can be used to improve results of an image generation. Responding to the reviews, the authors added a super resolution experiment in the appendix, which the reviewers believe is the right direction but is still preliminary.  Overall, we believe the paper reports interesting findings but it will require a series of additional work to make it ready for the publication.
This paper aims to estimate the 3D location and orientation of vehicle from a 2D image. Instead of using a CNN based 3D detection pipeline, the authors propose to detect the vehicle’s wheel grounding points and then using the ground plane constraint for the estimation. All three reviewers provided unanimous rating of rejection. Many concerns are raised by the reviewers, including poor generalization to new situations, small improvement over prior work, low presentation quality, the lack of detailed description of the experiments, etc. The authors did not respond to the reviewers’ comments. The AC agrees with the reviewers’ comments, and recommend rejection.
This paper theoretically and empirically studies the inner and outer learning rate of the MAML algorithm and their role in convergence. While the paper presents some interesting ideas and add to our theoretical understanding of meta learning algorithms, the reviewers raised concerns about the relevance of the theory. Further the empirical study is somewhat preliminary and doesn t compare to prior works that also try to stabilize the MAML algorithm, further bringing into question its usefulness. As such, the current form of the paper doesn t meet the bar for ICLR.
This paper studies the effectiveness of self supervised approaches by characterising how much information they can extract from a given dataset of images on a per layer basis. Based on an empirical evaluation of RotNet, BiGAN, and DeepCluster, the authors argue that the early layers of CNNs can be effectively learned from a single image coupled with strong data augmentation. Secondly, the authors also provide some empirical evidence that supervision might still necessary to learn the deeper layers (even in the presence of millions of images for self supervision).  Overall, the reviews agree that the paper is well written and timely given the growing popularity of self supervised methods. Given that most of the issues raised by the reviewers were adequately addressed in the rebuttal, I will recommend acceptance. We ask the authors to include additional experiments requested by the reviewers (they are valuable even if the conclusions are not perfectly aligned with the main message). 
This paper is far more borderline than the review scores indicate. The authors certainly did themselves no favours by posting a response so close to the end of the discussion period, but there was sufficient time to consider the responses after this, and it is somewhat disappointing that the reviewers did not engage.  Reviewer 2 states that their only reason for not recommending acceptance is the lack of experiments on more than one KG. The authors point out they have experiments on more than one KG in the paper. From my reading, this is the case. I will consider R2 in favour of the paper in the absence of a response.  Reviewer 3 gives a fairly clear initial review which states the main reasons they do not recommend acceptance. While not an expert on the topic of GNNs, I have enough of a technical understanding to deem that the detailed response from the authors to each of the points does address these concerns. In the absence of a response from the reviewer, it is difficult to ascertain whether they would agree, but I will lean towards assuming they are satisfied.  Reviewer 1 gives a positive sounding review, with as main criticism "Overall, the work of this paper seems technically sound but I don’t find the contributions particularly surprising or novel. Along with plogicnet, there have been many extensions and applications of Gnns, and I didn’t find that the paper expands this perspective in any surprising way." This statement is simply re asserted after the author response. I find this style of review entirely inappropriate and unfair: it is not a the role of a good scientific publication to "surprise". If it is technically sound, and in an area that the reviewer admits generates interest from reviewers, vague weasel words do not a reason for rejection make.  I recommend acceptance.
This article studies the identifiability of architecture and weights of a ReLU network from the values of the computed functions, and presents an algorithm to do this. This is a very interesting problem with diverse implications. The reviewers raised concerns about the completeness of various parts of the proposed algorithm and the complexity analysis, some of which were addressed in the author s response. Another concern raised was that the experiments were limited to small networks, with a proof of concept on more realistic networks missing. The revision added experiments with MNIST. Other concerns (which in my opinion could be studied separately) include possible limitations of the approach to networks with no shared weights nor pooling. The reviewers agree that the article concerns an interesting topic that has not been studied in much detail yet. Still, the article would benefit from a more transparent presentation of the algorithm and theoretical analysis, as well as more extensive experiments. 
This paper proposes video level 4D CNNs and the corresponding training and inference methods for improved video representation learning. The proposed model achieves state of the art performance on three action recognition tasks.  Reviewers agree that the idea well motivated and interesting, but were initially concerned with positioning with respect to the related work, novelty, and computational tractability. As these issues were mostly resolved during the discussion phase, I will recommend the acceptance of this paper. We ask the authors to address the points raised during the discussion to the manuscript, with a focus on the tradeoff between the improved performance and computational cost.
Two knowledgable reviewers recommend accepting the paper, and the less familiar reviewer is also positive. The final decision is to accept the paper. It s an interesting and timely topic with insightful results.
This paper proposes an expansion based approach for task free continual learning, using a Bayesian nonparametric framework (a Dirichlet process mixture model).  It was well reviewed, with reviewers agreeing that the paper is well written, the experiments are thorough, and the results are impressive. Another positive is that the code has been released, meaning it’s likely to be reproducible.  The main concern shared among reviewers is the limited novelty of the approach, which I also share. Reviewers all mentioned that the approach itself isn’t novel, but they like the contribution of applying it to task free continual learning. This wasn’t mentioned, but I’m concerned about the overlap between this approach and CURL (Rao et al 2019) published in NeurIPS 2019, which also deals with task free continual learning using a generative, nonparametric approach. Could the authors comment on this in their final version?  In sum, it seems that this paper is well done, with reproducible experiments and impressive results, but limited novelty. Given that reviewers are all satisfied with this, I’m willing to recommend acceptance.   
This paper proposes to augment training data for theorem provers by learning a deep neural generator that generates data to train a prover, resulting in an improvement over the Holophrasm baseline prover. The results were restricted to one particular mathematical formalism   MetaMath, a limitation raised one by reviewer.   All reviewers agree that it s an interesting method for addressing an important problem. However there were some concerns about the strength of the experimental results from R4 and R1. R4 in particular wanted to see results on more datasets, an assessment with which I agree. Although the authors argued vigorously against using other datasets, I am not convinced. For instance, they claim that other datasets do not afford the opportunity to generate new theorems, or the human proofs provided cannot be understood by an automatic prover. In their words,   "The idea of theorem generation can be applied to other systems beyond Metamath, but realizing it on another system is highly nontrivial. It can even involve new research challenges. In particular, due to large differences in logic foundations, grammar, inference rules, and benchmarking environments, the generation process, which is a key component of our approach, would be almost completely different for a new system. And the entire pipeline essentially needs to be re designed and re coded from scratch for a new formal system, which can require an unreasonable amount of engineering."   It sounds like they ve essentially tailored their approach for this one dataset, which limits the generality of their approach, a limitation that was not discussed in the paper.   There is also only one baseline considered, which renders their experimental findings rather weak. For these reasons, I think this work is not quite ready for publication at ICLR 2020, although future versions with stronger baselines and experiments could be quite impactful.     
This paper proposes Model Inversion Networks (MINs) to solve model optimization problems high dimensional spaces. The paper received three reviews from experts working in this area. In a short review, R1 recommends Reject based on limited novelty compared to an ICDM 2019 paper. R2 recommends Weak Reject, identifying several strengths of the paper but also a number of concerns including unclear or missing technical explanations and need for some additional experiments (ablation studies). R3 recommends Weak Accept, giving the opinion that the idea the paper proposes is worthy of publication, but also identifying a number of weaknesses including a "rushed" experimental section that is missing details, need for additional quantitative experimental results, and some "ad hoc" parts of the formulation. The authors prepared responses that address many of these concerns, including a convincing argument that there is significant difference and novelty compared to the ICDM 2019. However, even if excluding R1 s review, the reviews of R2 and R3 are borderline; the ACs read the paper and while they feel the work has significant merit, they agree with R2 and R3 that the paper needs additional work and another round of peer review to fully address R2 and R3 s concerns.   
This paper presents Layerdrop, which is a method for structured dropout which allows you to train one model, and then prune to a desired depth at test time. This is a simple method which is exciting because you can get a smaller, more efficient model at test time for free, as it does not need fine tuning. They show strong results on machine translation, language modelling and a couple of other NLP benchmarks. The reviews are consistently positive, with significant author and reviewer discussion. This is clearly an approach which merits attention, and should be included in ICLR.
The paper addresses the task of continual learning in NLP for seq2seq style tasks. The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately, which allows the neural network to leverage compositionality for knowledge transfer and also solves the problem of catastrophic forgetting. The paper has been improved substantially after the reviewers  comments and also obtains good results on benchmark tasks. The only concern is that the evaluation is on artificial datasets. In future, the authors should try to include more evaluation on real datasets (however, this is also limited by availability of such datasets). As of now, I m recommending an Acceptance.
The two most experienced reviewers recommended the paper be rejected.  The submission lacks technical depth, which calls the significance of the contribution into question.  This work would be greatly strengthened by a theoretical justification of the proposed approach.  The reviewers also criticized the quality of the exposition, noting that key parts of the presentation was unclear.  The experimental evaluation was not considered to be sufficiently convincing.  The review comments should be able to help the authors strengthen this work.
The paper presents an approach to forecasting over temporal streams of permutation invariant data such as point clouds. The approach is based on an operator (DConv) that is related to continuous convolution operators such as X Conv and others. The reviews are split. After the authors  responses, concerns remain and two ratings remain "3". The AC agrees with the concerns and recommends against accepting the paper.
This paper proposes to use the GAN (i.e., minimax) framework for adversarial training, where another neural network was introduced to generate the most effective adversarial perturbation by finding the weakness of the classifier. The rebuttal was not fully convincing on why the proposed method should be superior to existing attacks.
This paper proposes a type of adaptive dropout to regularize gradient based meta learning models. The reviewers found the idea interesting and it is supported by improvements on standard benchmarks. The authors addressed several concerns of the reviewers during the rebutal phase. In particular, revisions added results against other regularization mthods. We recommend that further attention is given to ablations, in particular the baseline proposed by Reviewer 1.
The paper is interested in assessing the difficulty of popular few shot classification benchmarks (Omniglot and miniImageNet). A clustering based meta learning method is proposed (called Centroid Network), on which a metric is built (gap between the performance of Prototypical Networks and Centroid Networks). As noted by several reviewers, the proposed metric (critical for the paper) is however not motivated enough, nor convincing enough   after discussion, the logic in the metric reasoning seems to remain flawed. 
The reviewers reached a consensus that the paper was not ready to be accepted in its current form. The main concerns were in regard to clarity, relatively limited novelty, and a relatively unsatisfying experimental evaluation. Although some of the clarity concerns were addressed during the response period, the other issues still remained, and the reviewers generally agreed that the paper should be rejected.
This paper proposes an outlier detection method that maps outliers to low probability regions of the latent space. The novelty is in proposing a weighted reconstruction error penalizing the mapping of outliers into high probability regions. The reviewers find the idea promising. They have also raised several questions. It seems the questions are at least partially addressed in the rebuttal, and as a result one of our expert reviewers (R5) has increased their score from WR to WA. But since we did not have a champion for this paper and its overall score is not high enough, I can only recommend a reject at this stage.
The authors propose a new type of compositional embedding (with two proposed variants) for performing tasks that involve set relationships between examples (say, images) containing sets of classes (say, objects).  The setting is new and the reviewers are mostly in agreement (after discussion and revision) that the approach is interesting and the results encouraging.  There is some concern, however, that the task setup may be too contrived, and that in any real task there could be a more obvious baseline that would do better.  For example, one task setup requires that examples be represented via embeddings, and no reference can be made to the original inputs; this is justified in a setting where space is a constraint, but the combination of this setting with the specific set query tasks considered seems quite rare.  The paper may be an example of a hammer in search of a nail.  The ideas are interesting and the paper is written well, and so the authors can hopefully refine the proposed class of problems toward more practical settings.
The paper is proposed a rejection based on majority reviews.
This paper proposes to combine FMs and GNNs. All reviewers voted reject, as the paper lacks experiments (eg ablation studies) and novelty. Writing can be significant improved   some information is missing. Authors did not respond to reviewers questions and concerns. For this reason, I recommend reject.  
Main content:  Blind review #2 summarizes it well:  Summary: This paper deals with the representation degeneration problem in neural language generation, as some prior works have found that the singular value distribution of the (input output tied) word embedding matrix decays quickly. The authors proposed an approach that directly penalizes deviations of the SV distribution from the two prior distributions, as well as a few other auxiliary losses on the orthogonality of U and V (which are now learnable). The experiments were conducted on small and large scale language modeling datasets as well as the relatively small IWSLT 2014 De En MT dataset.  Pros: + The paper is well written with great clarity. The dimensionality of the involved matrices (and their decompositions) are clearly provided, and the approach is clearly described. The authors also did a great job providing the details of their experimental setup. + The experiments seem to show consistent improvements over the baseline methods (at least the ones listed by the authors) on a relatively extensive set of tasks (e.g., of both small and large scales, of two different NLP tasks). Via WT2 and WT103, the authors also showed that their method worked on both LSTM and Transformers (which it should, as the SVD on word embedding should be independent of the underlying architecture). + I think studying the expressivity of the output embedding matrix layer is a very interesting (and important) topic for NLP. (e.g., While models like BERT are widely used, the actual most frequently re used module of BERT is its pre trained word embeddings.)     Discussion:  The reviewers agree that it is a very well written paper, and this is important as a conference paper to illuminate readers.  The one main objection is that spectrum control regularization was previously proposed and applied to GANs (Jiang et al ICLR 2019). However the authors convincingly point out that the technique is widely used, not only for GANs, and that application to neural language generation has quite different characteristics requiring a different, new approach: "our proposed prior distributions as shown in Figure 2 in our paper are fundamentally different from the singular value distributions learned using their penalty functions (See Figure 1 and Table 7 in Jiang et al.’s paper). Figure 1 in their paper suggests that their penalty function, i.e., D optimal Reg, will encourage all the singular values close to 1, which is well aligned with their motivation for training GAN. However, if we use such penalty function to train neural language models, the learned word representations will lose the power of modeling contextual information, and can result in much worse results than the baseline methods."     Recommendation and justification:  I concur with the majority of reviewers that this paper is a weak accept. Though not revolutionary, it is well written, has usefully broad application, and is supported well empirically.
The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results. As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision.
This paper explores extending k means to allow to clusters with non convex shapes.  This paper introduces a new algorithm, relying on empirical comparisons to illustrate its contribution. The main issue with the paper is that the empirical claims do not support that the new method is indeed better. The paper claims the new method outperforms the competitors in most cases. However, the original submission reported median performance and when the authors provided mean performance and additional baseline methods (at the reviewers  request) there appear to be little evidence to support the claim. In addition there are no measures of significance provided. The authors provided no commentary to help the reviewers understand the new results. There might be some important speed gains at the cost of final performance, but on the evidence provided we are not able to evaluate the cost in final performance.  The text changes size after section 5.3 and is 9% smaller. Watch out for this formatting issue in future submissions  
This paper addresses the problem of few shot classification across multiple domains. The main algorithmic contribution consists of a selection criteria to choose the best source domain embedding for a given task using a multi domain modulator.   All reviewers were in agreement that this paper is not ready for publication. Some key concerns were the lack of scalability (though the authors argue that this may not be a concern as all models are only stored during meta training, still if you want to incorporate many training settings it may become challenging) and low algorithmic novelty. The issue with novelty is that there is inconclusive experimental evidence to justify the selection criteria over simple methods like averaging, especially when considering novel test time domains. The authors argue that since their approach chooses the single best training domain it may not be best suited to generalize to a novel test time domain.   Based on the reviews and discussions the AC does not recommend acceptance. The authors should consider revisions for clarity and to further polish their claims providing any additional experiments to justify where appropriate. 
The authors present an algorithm that utilizes ideas from imitation learning to improve on goal conditioned policy learning methods that rely on RL, such as hindsight experience replay.  Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in way that satisfied the reviewers with respect to their concerns in these areas.  However, after discussion, the reviewers still felt that there were some fundamental issues with the paper, namely that the applicability of this method to more general RL problems (complex reward functions rather than signle state goals, time ) is unclear.  The basic idea seems interesting, but it needs further development, and non trivial modifications, to be broadly applicable as an approach to problems that RL is typically used on.  Thus, I recommend rejection of the paper at this time.
This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods.   The reviewers ultimately decided this paper is well written and has content which is of general interest to the ICLR community.
The reviewers are unanimous in their evaluation of this paper, and I concur.
This paper proposes BOSH attack, a meta algorithm for decision based attack, where a model that can be accessed only via label queries for a given input is attacked by a minimal perturbation to the input that changes the predicted label. BOSH improves over existing local update algorithms by leveraging Bayesian Optimization (BO) and Successive Halving (SH). It has valuable contributions. But various improvements as detailed in the review comments can be made to further strength the manuscript.
This paper studies methods for using weight sparsification to reduce the computational load of network inference.  While there is not absolute consensus on whether this paper should be accepted, one of the main criticisms of this paper is that sparse compute is not always realistic or efficient on a GPU.  While this may be true of the current SOTA in hardware, emerging computing platforms and CPU libraries may handle sparse networks quite well.  For this reason, I am willing to down weight this criticism. Based on the remaining comments, this paper has the merit to be accepted, even if it is a bit forward looking in terms of the hardware platforms it targets.  
This paper studies the problem of federated learning for non i.i.d. data, and looks at the hyperparameter optimization in this setting. As the reviewers have noted, this is a purely empirical paper. There are certain aspects of the experiments that need further discussion, especially the learning rate selection for different architectures. That said, the submission may not be ready for publication at its current stage.
The paper is proposing uncertainty of the NN’s in the training process on analog circuits based chips. As one reviewer emphasized, the paper addresses important and unique research problem to run NN on chips. Unfortunately, a few issues are raised by reviewers including presentation, novelly and experiments. This might be partially be mitigated by 1) writing motivation/intro in most lay person possible way 2) give easy contrast to normal NN (on computers) to emphasize the unique and interesting challenges in this setting. We encourage authors to take a few cycles of edition, and hope this paper to see the light soon. 
This paper studies numerous ways in which the statistics of network weights evolve during network training.  Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process.  Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.
This paper studies the statistics of activation norms and Jacobian norms for randomly initialized ReLU networks in the presence (and absence) of various types of residual connections. Whereas the variance of the gradient norm grows with depth for vanilla networks, it can be depth independent for residual networks when using the proper initialization.  Reviewers were positive about the setup, but also pointed out important shortcomings on the current manuscript, especially related to the lack of significance of the measured gradient norm statistics with regards to generalisation, and with some techinical aspects of the derivations. For these reasons, the AC believes this paper will strongly benefit from an extra iteration. 
The paper proposes a defense for adversarial attacks based on autoencoders that tries to find the closest point to the natural image in the output span of the decoder and "purify" the adversarial example. There were concerns about the work being too incremental over DefenseGAN and about empirical evaluation of the defense. It is crucial to test the defense methods against best available attacks to establish the effectiveness. Authors should also discuss and consider evaluating their method against the attack proposed in https://arxiv.org/pdf/1712.09196.pdf that claims to greatly reduce the defense accuracy of DefenseGAN. 
The authors present a metalearning based approach to learning intrinsic rewards that improve RL performance across distributions of problems.  This is essentially a more computationally efficient approach to approaches suggested by Singh (2009/10).  The reviewers agreed that the core idea was good, if a bit incremental, but were also concerned about the similarity to the Singh et al. work, the simplicity of the toy domains tested, and comparison to relevant methods.  The reviewers felt that the authors addressed their main concerns and significantly improved the paper; however the similarity to Singh et al. remains, and thus the concerns about incrementalism.   Thus, I recommend this paper for rejection at this time.
This paper introduces a model that learns a slot based representation and its transition model to predict the representation changes over time. While all the reviewers agree that this paper is focusing on an important problem, they expressed multiple concerns regarding the novelty of the approach as well as lacking experiments. It certainly is missing multiple important relevant works, thereby overclaiming at a few places. The authors provided a short general response to compare their approach with some of the previous works and conduct stronger experiments for a future submission. We believe this paper is not at the stage to be published at this point.
This paper proposes an equivariant sequence to sequence model for dealing with compositionality of language. They show these models are better at SCAN tasks.  Reviewers expressed two major concerns: 1) Limited clarity of section 4 which makes the paper difficult to understand. 2) Whether this could generalize to more complex types of compositionality.  Authors responded by revising Section 4 and answering the question of generalization. While the reviewers are not 100% satisfied, they agree there is enough novel contribution in this paper.   I thank the authors for submitting and look forward to seeing a clearer revision in the conference.
Main content: Proposes combining flexible activation functions   Discussion: reviewer 1: main issue is unfamiliar with stock dataset, and CIFAR dataset has a bad baseline. reviewer 2: main issue is around baselines and writing.  reviewer 3: main issue is paper does not compare with NAS.  Recommendation: All 3 reviewers vote reject. Paper can be improved with stronger baselines and experiments. I recommend Reject.
This paper presents a method for training sparse neural networks that also provides a speedup during training, in contrast to methods for training sparse networks which train dense networks (at normal speed) and then prune weights.  The method provides modest theoretical speedups during training, never measured in wallclock time.   The authors improved their paper considerably in response to the reviews.  I would be inclined to accept this paper despite not being a big win empirically, however a couple points of sloppiness pointed out (and maintained post rebuttal) by R1 tip the balance to reject, in my opinion.  Specifically:   1) "I do not agree that keeping the learning rate fixed across methods is the right approach."  This seems like a major problem with the experiments to me.  2) "I would request the authors to slightly rewrite certain parts of their paper so as not to imply that momentum decreases the variance of the gradients in general."  I agree.
This papers proposed a solution to the problem of disease density estimation using satellite scene images.  The method combines a classification and regression task.  The reviewers were unanimous in their recommendation that the submission not be accepted to ICLR.  The main concern was a lack of methodological novelty.  The authors responded to reviewer comments, and indicated a list of improvements that still remain to be done indicating that the paper should at least go through another review cycle.
The paper proposes a platform for benchmarking, and in particular hardware agnostic evaluation of machine learning models. This is an important problem as our field strives for more reproducibility.  This was a very confusing paper to discuss and review, since most of the reviewers (and myself) do not know much about the area. Two of the reviewers found the paper contributions sufficient to be (weakly) accepted. The third reviewer had many issues with the work and engaged in a lengthy debate with the authors, but there was strong disagreement regarding their understanding of the scope of the paper as a Tools/Systems submission.  Given the lack of consensus, I must recommend rejection at this time, but highly encourage the authors to take the feedback into account and resubmit to a future venue.
This work presents a learnable activation function based on adaptive piecewise linear (APL) units. Specifically, it extends APL to the symmetric form. The authors argue that S APL activations can lead networks that are more robust to adversarial attacks. They present an empirical evaluation to prove the latter claim. However, the significance of these empirical results were not clear due to non standard threat models used in black box setting and the weak attacks used in open box setting. The authors revised the submission and addressed some of the concerns the reviewers had. This effort was greatly appreciated by the reviewers. However, the issues related to the significance of robustness results remained unclear even after the revision. In particular, as pointed by R4, some of the revisions seem to be incomplete (Table 4). Also, the concern R4 had initially raised about non standard black box attacks was not addressed. Finally, some experimental details are still missing. While the revision indeed a great step, the adversarial experiments more clear and use more standard setup be convincing. 
The reviewers have issues with novelty and quality of exposition. I recommend rejection.
This paper proposes a fractional graph convolutional networks for semi supervised learning, using a classification function repurposed from previous work, as well as parallelization and weighted combinations of pooling function. This leads to good results on several tasks. Reviewers had concerns about the part played by each piece, the lack of comparison to recent related work, and asked for better explanation of the rationale of the method and more experimental details. Authors provided explanations and details, and a more thorough set of comparison to other work, showing better performance in some but not all cases. However, concerns that the proposed innovations are too incremental remain. Therefore, we cannot recommend acceptance.
The paper aims to find locally interpretable models, such that the local models are fit (w.r.t. the ground truth) and faithful (w.r.t. the global underlying black box model). The contribution of the paper is that the local model is trained from a subset of points, selected via an optimized importance weight function. The difference compared to Ren et al. (cited) is that the IW function is non differentiable and optimized using Reinforcement Learning.   A first concern (Rev#1, Rev#2) regards the positioning of the paper w.r.t. RL, as the actual optimization method could be any black box optimization method: one wants to find the IW that maximizes the faithfulness. The rebuttal makes a good job in explaining the impact of using a non differentiable IW function.  A second concern (Rev#2) regards the interpretability of the IW underlying the local interpretable model.   There is no doubt that the paper was considerably improved during the rebuttal period. However, the improvements raise additional questions (e.g. about selecting the IW depending on the distance to the probes). I encourage the authors to continue on this promising line of search. 
A strong paper reporting improved approaches to meta learning.
The paper proposes a method for object detection by predicting category specific object probability and category agnostic bounding box coordinates for each position that s likely to contain an object. The proposed idea is interesting and the experimental results show improvement over RetinaNet and other baselines. However, in terms of weakness, (1) conceptually speaking it s unclear whether the proposed method is a big departure from the existing frameworks; and (2) although the authors are claiming SOTA performance, the proposed method seems to be worse than other existing/recent work. Some example references are listed below (more available here: https://paperswithcode.com/paper/foveabox beyond anchor based object detector).   [1] Scale Aware Trident Networks for Object Detection https://arxiv.org/abs/1901.01892  [2] GCNet: Non local Networks Meet Squeeze Excitation Networks and Beyond https://arxiv.org/abs/1904.11492  [3] CBNet: A Novel Composite Backbone Network Architecture for Object Detection https://arxiv.org/abs/1909.03625  [4] EfficientDet: Scalable and Efficient Object Detection https://arxiv.org/abs/1911.09070  References [3] and [4] are concurrent works so shouldn t be a ground of rejection per se, but the performance gap is quite large. Compared to [1] and [2] which have been on arxiv for a while (+5 months) the performance of the proposed method is still inferior. Despite considering that object detection is a very competitive field, the conceptual/technical novelty and overall practical significance seem limited for ICLR. For a future submission, I would suggest that a revision of this paper being reviewed in a computer vision conference, rather than ML conference. 
The main contribution of this paper is the training of a supervised model jointly with deep CCA for improving the representations learned in a setting where the training data is multi view.  The claimed technical contribution is modifications to deep CCA to enable it to play nicely with the minibatch gradient based training used for the supervised loss.  Pros:  This is an important problem with many applications.  Cons:  The novelty is minimal.  Some previous work has done joint training of supervised models with CCA, and some has addressed training deep CCA in a stochastic setting.  The reviewers (and I) are unconvinced that the differences from previous work are sufficient, and the paper does not carefully compare with the previous work.  The contribution to the tasks may be quite significant, however, so the paper may fit in well in an application oriented conference/journal.
The paper applies the Go Explore algorithm to text based games and shows that it is able to solve text based game with better sample efficiency and generalization than some alternatives.  The Go Explore algorithm is used to extract high reward trajectories that can be used to train a policy using a seq2seq model that maps observations to actions.  Paper received 1 weak accept and 2 weak rejects.  Initially the paper received three weak rejects, with the author response and revision convincing one reviewer to increase their score to a weak accept.  Overall, the authors liked the paper and thought that it was well written with good experiments. However, there is concern that the paper lacks technical novelty and would not be of interest to the broader ICLR community (beyond those that are interested in text based games).  Another concern reviewers expressed was that the proposed method was only compared against baselines with simple exploration strategies and that baselines with more advanced exploration strategies should be included.  The AC agrees with above concerns and encourage the authors to improve their paper based on the reviewer feedback, and to consider resubmitting to a venue that is more focused on text based games (perhaps an NLP conference).
This paper proposes a regularization scheme for reducing meta overfitting. After the rebuttal period, the reviewers all still had concerns about the significance of the paper s contributions and the thoroughness of the empirical study. As such, this paper isn t ready for publication at ICLR. See the reviewer s comments for detailed feedback on how to improve the paper. 
The paper proposes to augment the conditional GAN discriminator with an attention mechanism, with the aim to  help the generator, in the context of image to image translation. The reviewers raise several issues in their reviews. One theoretical concern has to do with how the training of the attention mechanism (which seems to be collaborative) would interact with the minimax, zero sum nature of a GAN objective; another with the discrepancy in how the attention map is used during training and testing. The experimental results were not significant enough, and the reviewers also recommend additional experiment results to clearly demonstrate the benefit of the method. 
Reviewers uniformly suggest acceptance. Please look carefully at reviewer comments and address in the camera ready. Great work!
The paper proposes a Bayesian approach for time series regression when the explanatory time series influences the response time series with a time lag. The time lag is unknown and allowed to be non stationary process. Reviewers have appreciated the significance of the problem and novelty of the proposed method, and also highlighted the importance of the application domain considered by the paper. 
This paper studies the problem of estimating the value function in an RL setting by learning a representation of the value function. While this topic is one of general interest to the ICLR community, the paper would benefit from a more careful revision and reorganization following the suggestions of the reviewers.
This paper proposes constraints to tackle the problems of dead neurons and dead points. The reviewers point out that the experiments are only done on small datasets and it is not clear if the experiments will scale further. I encourage the authors to carry out further experiments and submit to another venue.
This paper extends prototypical networks to few shot 1 way classification. The idea is to introduce a null class to compare against with a null prototype. The reviewers found the idea sound and interesting. However, the response was mixed because the reviewers were not convinced of the significance of the improvements. Furthermore, there were questions raised about the motivation that were not sufficiently addressed in the rebuttal. Batch normalization layers will not necessarily lead to zero mean if the trainable offset is not disabled. The authors did not clarify whether they disable this offset. I encourage the authors to resubmit after addressing the issues raised by the reviewers.
The authors propose to understand spectral bias during training of neural networks from the perspective of the NTK. While reviewers appreciated aspects of the work, the general consensus was that the current version is not ready for publication; some concerns stem from whether the the NTK model and finite neural networks are sufficiently similar that we should be able to gain real practical insights into the behaviour of finite models. This is partly an empirical question, and stronger experiments are required to have a better sense of the answer. Nonetheless, the authors are encouraged to persist with this work, taking into account reviewer comments in future revisions.  
Thanks for your feedback to the reviewers, which helped us a lot to better understand your paper. Through the discussion, the overall evaluation of this paper was significantly improved. However, given the very high competition at ICLR2020, this submission is still below the bar unfortunately. We hope that the discussion with the reviewers will help you improve your paper for potential future publication.
The paper tackles the key question of achieving high prediction performances with few labels. The proposed approach builds upon Contrastive Predictive Coding (van den Oord et al. 2018). The contribution lies in i) refining CPC along several axes including model capacity, directional predictions, patch based augmentation; ii) showing that the refined representation learned by the called CPC.v2 supports an efficient classification in a few label regime, and can be transferred to another dataset; iii) showing that the auxiliary losses involved in the CPC are not necessarily predictive of the eventual performance of the network.  This paper generated a hot discussion. Reviewers were not convinced that the paper contributions are sufficiently innovative to deserve being published at ICLR. Authors argued that novelty does not have to lie in equations, and that the new ideas and evidence presented are worth.   The area chair thinks that the paper raises profound questions (e.g., what auxiliary losses are most conducive to learning a good representation; how to divide the computational efforts among the preliminary phase of representation learning and the later phase of classifier learning), but given the number of options and details involved, these results may support several interpretations besides the authors .   The authors might also want to leave the claim about the generality of the CPC++ principles (e.g., regarding audio) for further work   or to bring additional evidence backing up this claim.   In conclusion, this paper contains brilliant ideas and I hope to see them published with a strengthened analysis of its components. 
This paper introduces a variant of Nesterov momentum which saves computation by only periodically recomputing certain quantities, and which is claimed to be more robust in the stochastic setting. The method seems easy to use, so there s probably no harm in trying it. However, the reviewers and I don t find the benefits persuasive. While there is theoretical analysis, its role is to show that the algorithm maintains the convergence properties while having other benefits. However, the computations saved by amortization seem like a small fraction of the total cost, and I m having trouble seeing how the increased "robustness" is justified. (It s possible I missed something, but clarity of exposition is another area the paper could use some improvement in.) Overall, this submission seems promising, but probably needs to be cleaned up before publication at ICLR. 
This paper extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. All reviewers agree that this approach is interesting (verification and validation) and experiments are solid. One of the reviewers raised concerns are promptly answered by authors, raising the average score to accept.  
The paper proposes a lossless image compression consisting of a hierarchical VAE and using a bits back version of ANS. Compared to previous work, the paper (i) improves the compression rate performance by adapting the discretization of latent space required for the entropy coder ANS (ii) increases compression speed by implementing a vectorized version of ANS (iii) shows that a model trained on a low resolution imagenet 32 dataset can generalize its compression capabilities to higher resolution.  The authors addressed properly reviewers  concerns. Main critics which remain are (i) the method is not practical yet (long compression time) (ii) results are not state of the art   but the contribution is nevertheless solid.
The article studies the set of functions expressed by a  network with bounded parameters in the limit of large width, relating the required norm to the norm of a transform of the target function, and extending previous work that addressed the univariate case. The article contains a number of observations and consequences. The reviewers were quite positive about this article. 
This paper proposes a graph neural network based approach for scaling up imitation learning (e.g., of swarm behaviors). Reviewers noted key limitations in the discussion of related work, size of the proposed contribution in terms of model novelty, and evaluation / comparison to strong baselines. Reviewers appreciated the author replies which resolved some concerns but agree that the paper is overall not ready for publication.
The submission proposes a  co natural  gradient update rule to precondition the optimization trajectory using a Fisher information estimate acquired from previous experience. This results in reduced sensitivity and forgetting when new tasks are learned.   The reviews were mixed on this paper, and unfortunately not all reviewers had enough expertise in the field. After reading the paper carefully, I believe that the paper has significance and relevance to the field of continual learning, however it will benefit from more careful positioning with respect to other work as well as more empirical support. The application to the low data regime is interesting and could be expanded and refined in a future submission.   The recommendation is for rejection.
Authors provide an empirical evaluation of batch size and learning rate selection and its effect on training and generalization performance. As the authors and reviewers note, this is an active area of research with many closely related results to the contributions of this paper already existing in the literature. In light of this work, reviewers felt that this paper did not clearly place itself in the appropriate context to make its contributions clear. Following the rebuttal, reviewers minds remained unchanged. 
Thanks for your detailed responses to the reviewers, which helped us a lot to better understand your paper. However, given that the current manuscript still contains many unclear parts, we decided not to accept the paper. We hope that the reviewers  comments help you improve your paper for potential future submission.
This paper presents a non autoregressive NMT model which predicts the positions of the words to be produced as a latent variable in addition to predicting the words. This is a novel idea in the field of several other papers which are trying to do similar things, and obtains good results on benchmark tasks. The major concerns are systematic comparisons with the FlowSeq paper which seems to have been published before the ICLR submission deadline. The reviewers are still not convinced by the empirical performance comparison as well as speed comparisons. With some more work this could be a good contribution. As of now, I am recommending a Rejection.
This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed. The paper received three reviews by expert working in this area. R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments. R2 recommends Weak Accept and has some specific suggestions and questions. R3 recommends Weak Reject, also citing concerns with experiments and writing. The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating. Given the split decision, the AC also read the paper. While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution. We hope the reviewer comments will hep authors prepare a revision for a future venue.
This paper proposes a method to transfer a pretrained language model in one language (English) to a new language. The method first learns word embeddings for the new language while keeping the the body of the English model fixed, and further refines it in a fine tuning procedure as a bilingual model. Experiments on XNLI and dependency parsing demonstrate the benefit of the proposed approach.  R3 pointed out that the paper is missing an important baseline, which is a bilingual BERT model. The authors acknowledged this in their rebuttal and ran a preliminary experiment to obtain a first set of results. However, since the main claim of the paper depends on this new experiment, which was not finished by the end of the rebuttal period, it is difficult to accept the paper in its current state. In an internal discussion, R1 also agreed that this baseline is critical to support the paper.  As a result, I recommend to reject this paper for ICLR. I encourage the authors to update their paper with the new experiment for submission to future conferences (given consistent results).
The authors address the challenge of sample efficient learning in multi agent systems. They propose a model that distinguishes actions in terms of their semantics, specifically in terms of whether they influence the acting agent and environment or whether they influence other agents. This additional structure is shown to substantially benefit learning speed when composed with a range of state of the art multi agent RL algorithms. During the rebuttal, technical questions were well addressed and the overall quality of the paper improved. The paper provides interesting novel insights on how the proposed structure improves learning.
The paper adapts a previously proposed modular deep network architecture (SHDL) for supervised learning in a continual learning setting.  One problem in this setting is catastrophic forgetting.  The proposed solution replays a small fraction of the data from old tasks to avoid forgetting, on top of a modular architecture that facilitates fast transfer when new tasks are added.  The method is developed for image inputs and evaluated experimentally on CIFAR 100.  The reviews were in agreement that this paper is not ready for publication.  All the reviews had concerns about the lack of explanation of the proposed solution and the experimental methods.  The reviewers were concerned about the choice of metrics not being comparable or justified: Reviewer4 wanted an apples to apples comparison, Reviewer1 suggested the paper follow the evaluation paradigm used in earlier papers, and Reviewer2 described the absence of an explained baseline value.  Two reviewers (Reviewer4 and Reviewer2) described the lack of details on the parameters, architecture, and training regime used for the experiments.  The paper did not not justify which aspects of the modular system contributed to the observed performance (Reviewer4 and Reviewer1).   Several additional concerns were also raised.   The authors did not respond to any of the concerns raised by the reviewers.  
This paper proposes a GA based method for optimizing the loss function a model is trained on to produce better models (in terms of final performance). The general consensus from the reviewers is that the paper, while interesting, dedicates too much of its content to analyzing one such discovered loss (the Baikal loss), and that the experimental setting (MNIST and Cifar10) is too basic to be conclusive. It seems this paper can be so significantly improved with some further and larger scale experiments that it would be wrong to prematurely recommend acceptance. My recommendation is that the authors consider the reviewer feedback, run the suggested further experiments, and are hopefully in the position to submit a significantly stronger version of this paper to a future conference.
The authors received reviews from true experts and these experts felt the paper was not up to the standards of ICLR.   Reviewer 3 and Reviewer 1 disagree as to whether the new notion of generalization error is appropriate. I think both cases can be defended. I think the authors should aim to sharpen their argument in this regard.Several reviewers at one point remark that the results follow from standard techniques: shouldn t this be the case? I believe the actual criticism being made is that the value of these new results do not go above and beyond existing ones. There is also the matter of what value should be attributed to technical developments on their own. On this matter, the reviewers seem to agree that the derivations lean heavily on prior work. 
Three knowledgable reviewers give a positive evaluation of the paper. The decision is to accept.
This paper proposed an improvement on VAE GAN which draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction.  While the idea is interesting, the novelty is not high compared with existing similar works, and the improvement is not significant.
This paper introduces a way to measure dataset similarities. Reviewers all agree that this method is novel and interesting. A few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around GW, are promptly answered by authors, which raised the score to all weak accept.  
In this paper, the authors present adversarial attacks by semantic manipulations, i.e., manipulating specific detectors that result in imperceptible changes in the picture, such as changing texture and color, but without affecting their naturalness. Moreover, these tasks are done on two large scale datasets (ImageNet and MSCOCO) and two visual tasks (classification and captioning). Finally, they also test their adversarial examples against a couple of defense mechanisms and how their transferability. Overall, all reviewers agreed this is an interesting work and well executed, complete with experiments and analyses. I agree with the reviewers in the assessment. I think this is an interesting study that moves us beyond restricted pixel perturbations and overall would be interesting to see what other detectors could be used to generate these type of semantic manipulations. I recommend acceptance of this paper. 
This paper introduces an unsupervised concept learning and explanation algorithm, as well as a concept of "completeness" for evaluating representations in an unsupervised way.  There are several valuable contributions here, and the paper improved substantially after the rebuttal.  It would not be unreasonable to accept this paper.  But after extensive post review discussion, we decided that the completeness idea was the most valuable contribution, but that it was insufficiently investigated.  To quote R3, who I agree with: " I think the paper could be strengthened considerably with a rewrite that focuses first on a shortcoming of existing methods in finding complete solutions. I also think their explanations for why PCA is not complete are somewhat speculative and I expect that studying the completeness of activation spaces in invertible networks would lead to some relevant insights"  
This paper describes how they extend a previous phrase based neural machine translation model to incorporate external dictionaries. The reviewers mention the small scale of the experiments, and the lack of clarity in the writing, and missing discussion on computational complexity. Even though the method seems to have the potential to impact the field, the paper is currently not strong enough for publication. The authors have not engaged in the discussion at all. 
The two main concerns raised by reviewers is that whether the results are significant, and a potential issue in the proof. While the rebuttal clarified some steps in the proof, the main concerns about the significance remain. The authors are encouraged to make this significance more clear.  Note that one reviewer argued theoretical papers are not suitable for ICLR. This is false, as a theoretical understanding of neural networks remains a key research area that is of wide interest to the community. Consequently, this review was not considered in the final evaluation.
All the reviewers pointed out issues with the experiments, which the rebuttal did not address. The paper seems interesting, and the authors are encouraged to improve it.
The paper introduces the concept of an Expert Induced MDP (eMDP) to address imitation learning settings where environment dynamics are part known / part unknown. Based on the formulation a model based imitation learning approach is derived and the authors obtain theoretical guarantees. Empirical validation focuses on comparison to behavior cloning. Reviewers raised concerns about the size of the contribution. For example, it is unclear to what degree the assumptions made here would hold in practical settings.
This paper tackles the problem of safe exploration in RL. The proposed approach uses an imaginative module to construct a connectivity graph between all states using forward predictions. The idea then consists in using this graph to plan a trajectory which avoids states labelled as "unsafe".  Several concerns were raised and the authors did not provide any rebuttal. A major point is that the assumption that the approach has access to what are unsafe states, which is either unreasonable in practice or makes the problem much simpler. Another major point is the uniform data collection about every state action pairs. This can be really unsafe and defeats the purpose of safe exploration following this phase. These questions may be due to a miscomprehension, indicating that the paper should be clarified, as demanded by reviewers. Finally, the experiments would benefit from additional details in order to be correctly understood.  All reviewers agree that this paper should be rejected. Hence, I recommend reject.
The paper proposes a data driven approach to learning atomic resolution energy functions. Experiment results show that the proposed energy function is similar to the state of art method (Rosetta) based on physical principles and engineered features.   The paper addresses an interesting and challenging problem. The results are very promising. It is a good showcase of how ML can be applied to solve an important application problem.   For the final version, we suggest that the authors can tune down some claims in the paper to fairly reflect the contribution of the work. 
This paper proposes to use GMM as the latent prior distribution of GAN. The reviewers unanimously agree that the paper is not well motivated, explanations are lacking and writing needs to be substantially improved. 
The authors extend the framework of randomized smoothing to handle non Gaussian smoothing distribution and use this to show that they can construct smoothed models that perform well against l2 and linf adversarial attacks. They show that the resulting framework can obtain state of the art certified robustness results improving upon prior work.  While the paper contains several interesting ideas, the reviewers were concerned about several technical flaws and omissions from the paper:  1) A theorem on strong duality was incorrect in the initial version of the paper, though this was fixed in the rebuttal. However, the reasoning of the authors on the "fundamental trade off" is specific to the particular framework they consider, and is not really a fundamental trade off.  2) The justification for the new family of distributions constructed by the author is not very clear and the experiments only show marginal improvements over prior work. Thus, the significance of this contribution is not clear.  Some of the issues were clarified during the rebuttal, but the reviewers remained unconvinced about the above points.  Thus, the paper cannot be accepted in its current form. 
While there was some interest in the ideas presented, this paper was on the borderline, and was ultimately not able to be accepted for publication at ICLR.  Reviewers raised concerns as to the novelty, generality, and practicality of the approach, which could have been better demonstrated via experiments.
This paper presents a critical appraisal of variational mutual information estimators, and suggests a slight variance reducing improvement based on clipping density ratio estimates, and prove that this reduces variance (at the cost of bias). They also propose a set of criteria they term "self consistency" for evaluation of MI estimators and, and show convincingly that variational MI estimators fall short with respect to these.  Reviewers were generally positive about the contribution, and were happy with improvements made. While somewhat limited in scope, I believe this is nonetheless a valuable contribution to the conversation surrounding mutual information objectives that have become popular recently. I therefore recommend acceptance.
This paper introduces an FFT based loss function to enforce physical constraints in a CNN based PDE solver.  The proposed idea seems sensible, but the reviewers agreed that not enough attention was paid to baseline alternatives, and that a single example problem was not enough to understand the pros and cons of this method.
The paper presents a very interesting idea for estimating the held out error of deep models as a function of model and data set size. The authors intuit what the shape of the error should be, then they fit the parameters of a function of the desired shape and show that this has predictive power. I find this idea quite refreshing and the paper is well written with good experiments. Please make sure that the final version contains the cross validation results provided during the rebuttal.
The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius.  Strengths:  The majority opinion is that this work is a non trivial extension of prior work to provide radius certification.  The work is more efficient that strong recent baselines and provides better performance.  It successfully achieves this while avoiding adversarial training, which is another novel aspect.  Weaknesses:  There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion.  AC shares the majority opinion and recommends acceptance.
The work presents a novel and effective solution to learning reusable motor skills.  The urgency of this problem and the considerable rebuttal of the authors merits publication of this paper, which is not perfect but needs community attention.
This paper proposes a new policy gradient method based on stochastic mirror descent and variance reduction. Both theoretical analysis and experiments are provided to demonstrate the sample efficiency of the proposed algorithm. The main concerns of this paper include: (1) unclear presentation in both the main results and the proof; and (2) missing baselines (e.g., HAPG) in the experiments. This paper has been carefully discussed but even after author response and reviewer discussion, it does not gather sufficient support.  Note: the authors disclosed their identity by adding the author names in the revision during the author response. After discussion with PC chair, the openreview team helped remove that revision during the reviewer discussion to avoid desk reject.  
This paper proposes to improve VAEs  modeling of out of distribution examples, by pushing the latent representations of negative examples away from the prior.  The general idea seems interesting, at least to some of the reviewers and to me.  However, the paper seems premature, even after revision, as it leaves unclear some of the justification and analysis of the approach, especially in the fully unsupervised case.  I think that with some more work it could be a very compelling contribution to a future conference.
The paper introduces a novel approach to transfer learning in RL based on credit assignment. The reviewers had quite diverse opinions on this paper. The strength of the paper is that it introduces an interesting new direction for transfer learning in RL. However, there are some questions regarding design choices and whether the experiments sufficiently validate the idea (i.e., the sensitivity to hyperparameters is a  question that is not sufficiently addressed). Overall, this research has great potential. However, a more extensive empirical study is necessary before it can be accepted.
The authors consider a parameter server setup where the learner acts a server communicating updated weights to workers and receiving gradient updates from them. A major question then relates in the synchronisation of the gradient updates, for which couple of *fixed* heuristics exists that trade off accuracy of updates (BSP) for speed (ASP) or even combine the two allowing workers to be at most k steps out of sync. Instead, the authors propose to learn a synchronisation policy using RL. The authors perform results on a simulated and real environment. Overall, the RL based method seems to provide some improvement over the fixed protocols, however the margin between the fixed and the RL get smaller in the real clusters. This is actually the main concern raised by the reviewers as well (especially R2)   the paper in its initial submission did not include the real cluster results, rather these were added at the rebuttal. I find this to be an interesting real world application of RL and I think it provides an alternative environment for testing RL algorithms beyond simulated environments.   As such, I’m recommending acceptance. However, I do ask the authors to be upfront with the real cluster results and move them in the main paper. 
The paper proposes a black box algorithm for MRF training, utilizing a novel approach based on variational approximations of both the positive and negative phase terms of the log likelihood gradient (as R2 puts it, "a fairly creative combination of existing approaches").   Several technical and rhetorical points were raised by the reviewers, most of which seem to have been satisfactorily addressed, but all reviewers agreed that this was a good direction. The main weakness of the work is that the empirical work is very small scale, mainly due to the bottleneck imposed by an inner loop optimization of the variational distribution q(v, h). I believe it s important to note that most truly large scale results in the literature revolve around purely feedforward models that don t require expensive to compute approximations; that said, MNIST experiments would have been nice.   Nevertheless, this work seems like a promising step on a difficult problem, and it seems that the ideas herein are worth disseminating, hopefully stimulating future work on rendering this procedure less expensive and more scalable.
The submission presents an approach to visual planning. The work builds on semi parametric topological memory (SPTM) and introduces ideas that facilitate zero shot generalization to new environments. The reviews are split. While the ideas are generally perceived as interesting, there are significant concerns about presentation and experimental evaluation. In particular, the work is evaluated in extremely simple environments and scenarios that do not match the experimental settings of other comparable works in this area. The paper was discussed and all reviewers expressed their views following the authors  responses and revision. In particular, R1 posted a detailed justification of their recommendation to reject the paper. The AC agrees that the paper is not ready for publication in a first tier venue. The AC recommends that the authors seriously consider R1 s recommendations.
This paper proposes a smoothing based certification against various forms of transformations, such as  rotations, translations. The reviewers have concerns on the novelty of the work and several technical issues. The authors have made efforts to address some of issues, but the work may still significantly benefit from a throughout improvement in both presentation and technical contribution.
The paper studies word embeddings using the matrix factorization framework introduced by Levy et al 2015. The authors provide a theoretical explanation for how the hyperparameter alpha controls the distance between words in the embedding and a method to estimate the optimal alpha.  The authors also provide experiments showing the alpha found using their method is close to the alpha that gives the highest performance on the word similarity task on several datasets.   The paper received 2 weak rejects and 1 weak accept.  The reviews were unchanged after the rebuttal, with even the review for weak accept (R2) indicating that they felt the submission to be of low quality.  Initially, reviewers commented that while the work seemed solid and provided insights into the problem of learning word embeddings, the paper needed to improve their positioning with respect to prior work on word embeddings and add missing citations.  In the revision, the authors improved the related work, but removed the conclusion.  The current version of the paper is still low quality and has the following issues 1. The paper exposition still needs improvement and it would benefit from another review pass Following R3 s suggestions, the authors have made various improvements to the paper, including modifying the terminology and contextualizing the work.  However, as R3 suggests, the paper still needs more rewriting to clearly articulate the contribution and how it relates to prior work throughout the paper.  In addition, the conclusion was removed and the paper still needs an editing pass as there are still many language/grammar issues.  Page 5: "inherites"  > "inherits" Page 5: "top knn"  > "top k"  2. More experimental evaluation is needed. For instance, R1 suggested that the authors perform additional experiments on other tasks (e.g. NER, POS Tagging).  The authors indicated that this was not a focus of their work as other works have already looked at the impact of alpha on other task.  While prior works has looked at the correlation of alpha vs performance on the task, they have not looked at whether alpha estimated the method proposed by the author will give good performance on these tasks as well.  Including such analysis will make this a stronger paper.  Overall, there are some promising elements in the paper but the quality of the paper needs to be improved.  The authors are encouraged to improve the paper by adding more experimental evaluation on other tasks, improving the writing, as well as incorporating other reviewer comments and resubmit to an appropriate venue.  
This paper proposes a new normalization scheme that attempts to prevent all units in a ReLU layer from being dead. The experimental results show that this normalization can effectively be used to train deep networks, though not as well as batch normalization. A significant issue is that the paper does not sufficiently establish that their explanation for the success of Farkas layer is valid. For example, do networks usually have layers with only inactive units in practice?
The paper introduces additional layers on top BERT type models for disentangling of semantic and positional information.  The paper demonstrates (small) performance gains in transfer learning compared to pure BERT baseline.  Both reviewers and authors have engaged in a constructive discussion of the merits of the proposed method. Although the reviewers appreciate the ideas and parts of the paper the consensus among the reviewers is that the evaluation of the method is not clearcut enough to warrant publication.  Rejection is therefore recommended. Given the good ideas presented in the paper and the promising results the authors are encouraged to take the feedback into account and submit to the next ML conference.   
This paper introduces a novel architecture and loss for estimating PSD matrices using neural networks.  There is some theoretical justification for the architecture, and a small scale but encouraging experiment.  Overall, I think there is a sensible contribution here, but there are so many architectural and computational choices presented together at once that it s hard to tell what the important parts are.  The main problems with this paper are: 1) The scalability of the approach O(N^3) 2) The derivation of the architecture and gradient computations wasn t clear about what choices were available and why.  Several alternative choices were mentioned but not evaluated.  I think the authors also need to improve their understanding of automatic differentiation.  Backprop through eigendecomposition is already available in most autodiff packages.  It was claimed that a certain kind of matrix derivative provided better generalization, which seems like a strong claim to make in general. 3) The experimental setup seemed contrived, except for the heteroskedastic regression experiments, which lacked competitive baselines.  Why were the GP and MLPs homoskedastic?  As a matter of personal preference, I found that having 4 different "H"s differing only in font and capitalization for the network architecture was hard to keep track of.  I agree that R1 had some unjustified comments and R2 s review was contentless.  I apologize for these inadequate reviews. 
The submission presents an approach that leverages machine learning to optimize the placement and scheduling of computation graphs (such as TensorFlow graphs) by a compiler. The work is interesting and well executed. All reviewers recommend accepting the paper.
This paper presents a very creative threat model for neural networks.  The proposed attack requires systems level intervention by the attacker, which prompts the reviewers to question how realistic the attack is, and whether it is well motivated by the authors.  After conversing with the reviewers on this topic, they have not changed their mind about these issues.  As an AC, I think the threat model is both interesting and potentially realistic in some scenarios, however I agree with the reviewers that the motivation for the threat model could be more powerful.  For example the authors could focus more on realistic types of malicious behaviors that a developer could embed into a neural network.  I also think there s lots of opportunities for a range of applications that exploit the type of "two nets in one" behavior that the authors study.  Despite the interesting ideas in this paper, the post rebuttal scores are not strong enough to accept it.  I encourage the authors to address some of these presentation issues, and resubmit this interesting paper to another venue.
The paper applies tensor analysis techniques to anomaly detection from satellite data. The proposed solution is simple and seems to achieve good results. However, there is limited novelty in methodology and no sufficient experiments have been conducted to explain the performance gain. The paper is not ready for publication in ICLR but could be suitable for an application oriented venue. 
This paper suggests a Bayesian approach to make inference about latent variables for image inference tasks. While the idea in the paper seems elegant and simple, reviewers pointed out a few concerns, including lack of comparisons, missing references, and requested for more extensive validations. While a few comments might have been misunderstandings (eg lack of quantification   seems to be resolved by author’s comments), other comments are not (eg equation (8) needs further justification even if the final results don’t use it). We encourage authors to carefully review comments and edit the manuscript (perhaps some appendix items should be in the main to reduce confusion) for resubmitting to future conferences. 
This paper considers adversarial attacks in deep reinforcement learning, and specifically focuses on the problem of identifying key steps to attack. The paper poses learning these key steps as an RL problem with a cost for the attacker choosing to attack.  The reviewers agreed that this was an interesting problem setup, and the ability to learn these attacks without heuristics is promising. The main concern, which was felt was not adequately addressed in the rebuttals, was that the results need to be more than just competitive with heuristic approaches.  The fact that the attack ratio cannot be reliably changed, even with varying $\lambda$ still presents a major hurdle in the evaluation of the proposed method.  For the aforementioned reasons, I recommend rejecting this paper.
This paper proposed to use Gumbel softmax to optimize the routing matrix in routing network for multitask learning.  All reviewers have a consensus on rejecting this paper.  The paper did not clearly explain how and why this method works, and the experiments are not sufficient.
A nice paper, but quite some unclarities; it s unclear  in particular if the paper improves w.r.t. SOTA.  Esp. scaling is an issue here. Also, the understandability is below par and more work can make this into an acceptable submission.
Quoting a reviewer for a very nice summary:  "In this work, the authors suggest a new point of view on generalization through the lens of the distribution of the per sample gradients. The authors consider the variance and mean of the per sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a nice leading order derivation of the GSNR as a proxy for the measure of the generalization gap in the model."  The majority of the reviewers vote to accept this paper. We can view the 3 as a weak signal as that reviewer stated in his review that he struggled to rate the paper because it contained a lot of math.
This paper addresses the problem of poor generation quality in models for text generation that results from the use of the maximum likelihood (ML) loss, in particular the fact that the ML loss does not differentiate between different "incorrect" generated outputs (ones that do not match the corresponding training sequence).  The authors propose to train text generation models with an additional loss term that measures the distance from the ground truth via a Gaussian distribution based on embeddings of the ground truth tokens.  This is not the first attempt to address drawbacks of ML training for text generation, but it is simple and intuitive, and produces improvements over the state of the art on a range of tasks.  The reviewers are all quite positive, and are in agreement that the author responses and revisions have improved the paper quality and addressed initial concerns.  I think this work will be broadly appreciated by the ICLR audience.  One negative point is that the writing quality still needs improvement.
The authors present a new approach to improve performance for retro synthesis using a seq2seq model, achieving significant improvement over the baseline. There are a number of lingering questions regarding the significance and impact of this work. Hence, my recommendation is to reject. 
This paper studies the question of why a network trained to reproduce a single image often de noises the image early in training. This an interesting question and, post discussion, all three reviewers agree that it will be of general interest to the community and is worth publishing. Therefore I recommend it be accepted. 
The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability. The reviewers were largely unconvinced, though for very different reasons. The idea that "Bayesian ideas" explain what s going on seems obviously wrong to me. The third reviewer seems to think there s a tautology lurking here and that doesn t seem to be true to me either. It is disappointing that the reviewers did not re engage with the authors after the authors produced extensive rebuttals. Unfortunately, this is a widespread pattern this year.   Even though I m inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings. In the next iteration of the paper, I believe it may also be important to relate these ideas to [1]. It would be interesting to compare also on the networks studied in [1], which are more diverse.   [1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611
 This paper focuses on the problem of robustness in the network with random loss of neurons.  However, reviewers had issues with insufficient clarity of the presentation, and lack of discussion about closely related dropout approach.     
This paper proposes to use Graph Convolutional Networks (GCNs) in Bayesian optimization for neural architecture search. While the paper title includes multi objective, this component appears to only be a posthoc evaluation of the Pareto front of networks evaluated using a single objective search   this could be performed for any method that evaluates more than one network. Performance on NAS Bench 101 appears to be very good.   In the private discussion of reviewers and AC, several issues were raised, including whether the approach is compared fairly to LaNAS and whether the GCN will predict well for large search spaces. Also, unfortunately, no code is provided, making it unclear whether the work is reproducible. The reviewers unanimously agreed on a weak rejection score.  I concur with this assessment and therefore recommend rejection. 
Three reviewers have reviewed this submission and scored it as 6/3/3. After rebuttal, the reviewers remained unconvinced. The main criticisms concerns the Jacobian  regularizaton [1] being known which makes the contributions of this submission  look diluted. Additionally, there were concerns over results (degradation) on CIFAR10 and ImageNet and other minor issues. For these reasons, this paper cannot be accepted by ICLR2020.
The submission is proposed a rejection based on majority review.
This paper presents an efficient architecture of Transformer to facilitate implementations on mobile settings. The core idea is to decompose the self attention layers to focus on local and global information separately. In the experiments on machine translation, it is shown to outperform baseline Transformer as well as the Evolved Transformer obtained by a costly architecture search.  While all reviewers admitted the practical impact of the results in terms of engineering, the main concerns in the initial paper were the clarification of the mobile settings and scientific contributions. Through the discussion, reviewers are fairly satisfied with the authors’ response and are now all positive to the acceptance. Although we are still curious how it works on other tasks (as the title says “mobile applications”), I think the paper provides enough insights valuable to the community, so I’d like to recommend acceptance.  
The paper addresses  an important problem of self supervised learning in the context of time series classification. However, all reviewers raised major concerns regarding the novelty of the approach and the quality of empirical evaluation, including insufficient comparison with the state of art and reproducibility issues. The reviewers agree that the paper, in its current state, does not path the ICLR acceptance threshold, and encourage the authors to improve the paper based on the provided suggestions.
The paper empirically studies the behaviour of deep policy gradient algorithms, and reveals several unexpected observations that are not explained by the current theory. All three reviewers are excited about this work and recommend acceptance.
This paper proposes a simple plug and play language model approach to the problem of controlled language generation. The problem is important and timely, and the approach is simple yet effective. Reviewers had some discussions whether  1) there is enough novelty, 2) evaluation task really shows effectiveness, and 3) this paper will inspire future research directions.   After discussions of the above points, reviewers are leaning more positive, and I reflect their positive sentiment by recommending it to be accepted. I look forward to seeing this work presented at ICLR.
All reviewers found the work interesting but worried about the extension to non bilinear games. This is a point the authors should explicitly address in their work before publication.
This paper presents an approach to utilize conventional frequency domain basis such as DWHT and DCT to replace the standard point wise convolution, which can significantly reduce the computational complexity. The paper is generally well written and easy to follow. However, the technical novelty seems limited as it is basically a simple combination of CNNs and traditional filters. Moreover, as reviewers suggested, it is our history and current consensus in the community that learned representations have significantly outperformed traditional pre defined features or filters as the training data expands. I do understand the scientific value of revisiting and challenging that belief as commented by R1, but in order to provoke meaningful discussion, experiments on large scale dataset like ImageNet are definitely necessary. For these reasons, I think the paper is not ready for publication at ICLR and would like to recommend rejection.
This paper offers likely novel schemes for image resizing.  The performance improvement is clear.  Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue.  The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately. 
The paper studies the problem of graph learning with attributes, and propose a 2 D graph convolution that models the node relation graph and the attribute graph jointly. The paper proposes and efficient algorithm and models intra class variation. Empirical performance on 20 NG, L Cora, and Wiki show the promise of the approach.  The authors responded to the reviews by updating the paper, but the reviewers unfortunately did not further engage during the discussion period. Therefore it is unclear whether their concerns have been adequately addressed.  Overall, there have been many strong submissions on graph neural networks at ICLR this year, and this submission as is currently stands does not quite make the threshold of acceptance.
The paper investigates why adversarial training can sometimes degrade model performance on clean input examples.   The reviewers agreed that the paper provides valuable insights into how adversarial training affects the distribution of activations. On the other hand, the reviewers raised concerns about the experimental setup as well as the clarity of the writing and felt that the presentation could be improved.   Overall, I think this paper explores a very interesting direction and such papers are valuable to the community. It s a borderline paper currently but I think it could turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue.    
This work starts with a decomposition of the adversarial risk into two terms: the first is the usual risk, while the second is a stability term, that captures the possible effect of an adversarial perturbation. The insight of this work is that this second term can be dealt with using unlabelled data, which is often in plentiful supply. Unfortunately, the same ideas was developed concurrently and independently by several groups of authors.  The reviewer all agreed that this particular version was not ready for publication. In two cases, the authors compared the work unfavorably with concurrent independent work. I will note that the main bound somewhat ignores the issue of overfitting that the second term deals with via the Rademacher bound. Unless one assumes one has unlimited unlabeled data, could one not get an arbitrarily biased view of robustness from the sample. Seems like a gap to fill.
This paper introduces a new architecture for sparse coding.  The reviewers gave long and constructive feedback that the authors in turn responded at length on. There is consensus among the reviewers that despite contributions this paper in its current form is not ready for acceptance.  Rejection is therefore recommended with encouragement to make updated version for next conference.    
This paper a new method of constructing graph neural networks for the task of reasoning to answer IQ style diagrammatic reasoning, in particular including Raven Progressive Matrices. The model first learns an object representation for parts of the  image and then tries to combine them together to represent relations between different objects of the image. Using this model they achieve SOTA results (ignoring a parallely submitted paper) on the PGM and Raven datasets. The improvement in SOTA is subtantial.  Most of the critique made for the paper is on writing style and presentation. The authors seem to have fixed several of these concerns in the newly uploaded version of the paper. I will further request the authors to revise the paper for readability. However, since the paper presents both an interesting modeling and improved empirical results, I recommend acceptance.
The authors analyze the natural gradient algorithm for training a neural net from a theoretical perspective and prove connections to the K FAC algorithm. The paper is poorly written and contains no experimental evaluation or well established implications wrt practical significance of the results.
This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. The paper is well written, and is overall easy to follow. The proposed algorithm is well motivated, and easy to apply. The method can be expected to perform well empirically, which the experiments verify, and to have potential impact. On the other hand, the novelty is not very high, though this paper uses these existing techniques in a different setting.
This submission proposes an approach to pre train general purpose image and text representations that can be effective on target tasks requiring embeddings for both modes. The authors propose several pre training tasks beyond masked language modelling that are more suitable for the cross modal context being addressed, and also investigate which dataset/pretraining task combinations are effective for given target tasks.  All reviewers agree that the empirical results that were achieved were impressive.  Shared points of concern were:   the novelty of the proposed pre training schemes.   the lack of insight into the results that were obtained.  These concerns were insufficiently addressed after the discussion period, particularly the limited novelty. Given the remaining concerns and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance. 
This paper presents a Markov Random Fields (MRF) for generating adversarial examples in a black box setting, where only it has access to loss function evaluations. The method exploits the structure of input data to model the covariance structure of the gradients. Empirically, the resulting method uses fewer queries than the current state of the art to achieve comparable performance. Overall, the paper has valuable contributions. The main issue is on empirical evaluation, which can be strengthened, e.g., by including results with multi step methods and more thorough analysis of the estimated gradients.
The paper proposes an algorithm for learning a transport cost function that accurately captures how two datasets are related by leveraging side information such as a subset of correctly labeled points. The reviewers believe that this is an interesting and novel idea. There were several questions and comments, which the authors adequately addressed. I recommend that the paper be accepted.
There is insufficient support to recommend accepting this paper.  Generally the reviewers found the technical contribution to be insufficient, and were not sufficiently convinced by the experimental evaluation.  The feedback provided should help the authors improve their paper.
The paper proposes a Bayesian optimization approach to creating adversarial examples. The general idea has been in the air for some years, and over the last year especially there have been a number of approaches using BayesOpt for this purpose. Reviewers raised concerns about differences between this approach and related work, and practical challenges in general for using BayesOpt in this domain (regarding dimensionality, etc.). The authors provided thoughtful responses, although some of these concerns still remain. The authors are encouraged to address all comments carefully in future revisions, which a sufficiently substantial that the paper would benefit from additional review.   
The authors present a method for learning representations of remote sensing images from multiple views. The main ideas is to use the InfoNCE loss to learn from multiple views of the data.   The reviewers had a few concerns about this work which were not adequately addressed by the authors. I have summarised these below and would strongly recommend that the authors address these in subsequent submissions:  1) Experiments on a single dataset and a very specific task: Authors should present a more convincing argument about why the chosen dataset and task are challenging and important to demonstrate the main ideas presented in their work. Further, they should also report results on additional datasets suggested by the reviewers.  2) Comparisons with existing works: The reviewers suggested several existing works for comparison. The authors agreed that these were relevant and important but haven t done this comparison yet. Without such a comparison it is hard to evaluate the main contributions of this work.   Based on the above objections raised by the reviewers, I recommend that the paper should not be accepted.
This paper proposes a spatio temporal embedding loss for video instance segmentation. The proposed model (1) learns a per pixel embedding such that the embeddings of pixels from the same instance are closer than embeddings of pixels from other instances, and (2) learns depth in a self supervised way using a photometric reconstruction loss which operates under the assumption of a moving camera and a static scene. The resulting loss is a weighted sum of these attraction, repulsion, regularisation and geometric view synthesis losses. The reviewers agree that the paper is well written and that the problem is well motivated. In particular, there is consensus that the 3D geometry and 2D instance representation should be considered jointly. However, due to the lack of technical novelty, the complexity of the final model, and the issues with the empirical validation of the proposed approach, we feel that the work is slightly below the acceptance bar.
The authors propose a new method for learning hierarchically disentangled representations. One reviewer is positive, one is between weak accept and borderline and two reviewers recommend rejection, and keep their assessment after rebuttal and a discussion. The main criticism is the lack of disentanglement metrics and comparisons. After reading the paper and the discussion, the AC tends to agree with the negative reviewers. Authors are encouraged to strengthen their work and resubmit to a future venue.
The paper investigates questions around adversarial attacks in a continual learning algorithm, i.e., A GEM. While reviewers agree that this is a novel topic of great importance, the contributions are quite narrow, since only a single model (A GEM) is considered and it is not immediately clear whether this method transfers to other lifelong learning models (or even other models that belong to the same family as A GEM). This is an interesting submission, but at the moment due to its very narrow scope, it seems more appropriate as a workshop submission investigating a very particular question (that of attacking A GEM). As such, I cannot recommend acceptance.
This paper studies the implicit regularization of the gradient descent in homogeneous and shows that when the training loss falls below a threshold, then the smoothed. This study generalizes some of the earlier related works by relying on weaker assumptions. Experiments on MNIST and CIFAR 10 are provided to backup the theoretical findings of the paper.  R2 had some concern about one of the assumptions in this work (A4). While authors admitted that (A4) may not hold for all neural networks and all datasets, they stressed that this assumptions is reasonable when the network is overparameterized and can perfectly fit the training data. Overall, all reviewers are very positive about this submission and find a valuable step toward understanding implicit regularization. 
The paper proposes to improve visual relation prediction by using depth maps.  Since existing RGB images do not contain depth informations, the authors use a monocular depth estimation method to predict depth maps.  The authors show that using depths maps, they are able to improve prediction of relations between ground truth object bounding boxes and labels.    The paper got relatively low scores (with 3 initial weak rejects).  After the revision and suggested improvements, one of the reviewers updated their score so the paper now has 2 weak rejects and 1 weak accept.   The paper had the following weaknesses: 1. The paper has limited technical novelty as it combines off the shelf components.  The components also used different backbones (ResNet at some places, VGGNet at others) that were directly from prior work.  Was there any attempt to have an unified architecture? As the main novelty of the work is not in the model aspect, the paper needs to have stronger experiments and analysis. 2. More analysis on the quality of the depth estimation is needed.  Ideally, the work should provide some insight into whether some of the errors is due to having bad depth estimation?  The depth estimation method used is from 2016, there are newer depth estimation methods now.  Would having better depth estimation give improved results?  Experiments that illustrates that method works well with predicted bounding boxes instead of ground truth bounding boxes will also strengthen the paper.   3. There was the question of whether the related Yang et al. 2018 workshop paper should be included as basis for comparison.  In the AC s opinion, Yang et al. 2018 is not concurrent work and should be treated as prior work.  However, it is not clear whether it is feasible to compare against that work.  The authors should attempt to do so and if infeasible, clearly articulate why that is the case. 4. As pointed out by R3, once there is a depth map available, it is also possible to compare against 3D methods (such as those that operate on point clouds)  Overall the paper had a nice insight by proposing the simple but effective idea of using depth information to help with visual relation prediction.  Still the work is somewhat borderline in quality.  In the AC s opinion, the main contribution and insight of the paper is of limited interest to the ICLR community, and it would be more appreciated in a computer vision conference.  The authors are encouraged to improve the paper with stronger experiments and analysis, incorporate various suggestions from the reviewers, and resubmit to a vision conference. 
This paper investigates theories related to networks sparsification, related to mode connectivity and the so called lottery ticket hypothesis.  The paper is interesting and has merit, but on balance I find the contributions not sufficiently clear to warrant acceptance.  The authors made substantial changes to the paper which are admirable and which bring it to borderline status.  
This paper is concerned with improving data efficiency in multitask reinforcement learning problems. This is achieved by taking a hierarchical approach, and learning commonalities across tasks for reuse. The authors present an off policy actor critic algorithm to learn and reuse these hierarchical policies.  This is an interesting and promising paper, particularly with the ability to work with robots. The reviewers did however note issues with the novelty and making the contributions clear. Additionally, it was felt that the results proved the benefits of hierarchy rather than this approach, and that further comparisons to other approaches are required. As such, this paper is a weak reject at this point.
In this paper, the authors showed that for differentially private convex optimization, the utility guarantee of both DP GD and  DP SGD is determined by the expected curvature rather than the worst case minimum curvature. Based on this motivation, the authors justified the advantage of gradient perturbation over other perturbation methods. This is a borderline paper, and has been discussed after author response. The main concerns of this paper include (1) the authors failed to show any loss function that can satisfy the expected curvature inequality; (2) the contribution of this paper is limited, since all the proofs in the paper are just small tweak of existing proofs; (3) this paper does not really improve any existing gradient perturbation based differentially private methods. Due to the above concerns, I have to recommend reject.
Thanks for your detailed feedback to the reviewers, which helped us a lot to better understand your paper. However, given high competition at ICLR2020, we think the current manuscript is premature and still below the bar to be accepted to ICLR2020. We hope that the reviewers  comments are useful to improve your manuscript for potential future submission.
The paper studies theoretical properties of ridge regression, and in particular how to correct for the bias of the estimator.   The reviewers appreciated the contribution and the fact that you updated the manuscript to make it clearer.  I however advise the authors to think about the best way to maximize impact for the ICLR audience, perhaps by providing relevant examples from the ML literature.
There has been significant discussion in the literature on the effect of the properties of the curvature of minima on generalization in deep learning.  This paper aims to shed some light on that discussion through the lens of theoretical analysis and the use of a Bayesian Jeffrey s prior.  It seems clear that the reviewers appreciated the work and found the analysis insightful.  However, a major issue cited by the reviewers is a lack of compelling empirical evidence that the claims of the paper are true.  The authors run experiments on very small networks and reviewers felt that the results of these experiments were unlikely to extrapolate to large scale modern models and problems.  One reviewer was concerned about the quality of the exposition in terms of the writing and language and care in terminology.  Unfortunately, this paper falls below the bar for acceptance, but it seems likely that stronger empirical results and a careful treatment of the writing would make this a much stronger paper for future submission.
This paper proposes a method for learning sentence embeddings such that entailment and contradiction relationships between sentence pairs can be inferred by a simple parameter free operation on the vectors for the two sentences.  Reviewers found the method and the results interesting, but in private discussion, couldn t reach a consensus on what (if any) substantial valuable contributions the paper had proven. The performance of the method isn t compellingly strong in absolute or relative terms, yielding doubts about the value of the method for entailment applications, and the reviewers didn t see a strong enough motivation for the line of work to justify publishing it as a tentative or exploratory effort at ICLR.
The authors propose an actor critic method for finding Nash equilibrium in linear quadratic mean field games and establish linear convergence under some assumptions. There were some minor concerns about motivation and clarity, especially with regards to the simulator. In an extensive and interactive rebuttal, the authors were able to argue that their results/methods, which appear to be rather specialized to the LQ setting, offer insight/methods beyond the LQ setting.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  Concerns raised included the need for better motivation of the practicality of the approach, versus its computational cost. The need for improved evaluations was also raised.
This paper proposes a new benchmark to evaluate natural language processing models on discourse related tasks based on existing datasets that are not available in other benchmarks (SentEval/GLUE/SuperGLUE). The authors also provide a set of baselines based on BERT, ELMo, and others; and estimates of human performance for some tasks.  I think this has the potential to be a valuable resource to the research community, but I am not sure that it is the best fit for a conference such as ICLR. R3 also raises a valid concern regarding the performance of fine tuned BERT that are comparable to human estimates on half of the tasks (3 out of 5), which slightly weakens the main motivation of having this new benchmark.   My main suggestion to the authors is to have a very solid motivation for the new benchmark, including the reason of inclusion for each of the tasks. I believe that this is important to encourage the community to adopt it. For something like this, it would be nice (although not necessary) to have a clean website for submission as well. I believe that someone who proposes a new benchmark needs to do as best as they can to make it easy for other people to use it.  Due to the above issues and space constraint, I recommend to reject the paper.
The submission presents an approach to uncovering causal relations in an environment via interaction. The topic is interesting and the work is timely. However, the experimental setting is quite simplistic and the approach makes strong assumptions that limit its applicability. The reviewers are split. R2 raised their rating from 3 to 6 following the authors  responses and revision, but R1 maintained their rating of 3 and posted a response that justifies this position. In light of the limitations of the work, the AC recommends against accepting the submission.
This paper proposes a very interesting alternative to feed forward network layers, based on Quaternion methods and Hamilton products, which has the benefit of reducing the number of parameters in the neural network (more than 50% smaller) without sacrificing performance. They conducted extensive experiments on language tasks (NMT and NLI, among others) using transformers and LSTMs.   The paper appears to be clearly presented and have extensive results on a variety of tasks. However all reviewers pointed out that there is a lack of in depth analysis and thus insight into why this approach works, as well as questions on the specific effects of regularization. These concerns were not addressed in the rebuttal period, instead leaving it to future work. My assessment is that, with further analysis, ablation studies, and comparison to alternative methods for reducing model size (quantization, etc), this paper has the potential to be quite impactful, and I look forward to future versions of this work. As it currently stands, however, I don’t believe it’s suitable for publication at ICLR. 
The paper proposes a new imitation learning algorithm that explicitly models the quality of demonstrators.  All reviewers agreed that the problem and the approach were interesting, the paper well written, and the experiments well conducted. However, there was a shared concern about the applicability of the method to more realistic settings, in which the model generating the demonstrations does not fall under the assumptions of the method. The authors did add a real world experiment during the rebuttal, but the reviewers were suspicious of the reported InfoGAIL performance and were not persuaded to change their assessment.  Following this discussion, I recommend rejection at this time, but it seems like a good paper and I encourage the authors to do a more careful validation experiment, and resubmit to a future venue.
The paper proposes a VAE with a mixture of experts decoder for clustering and generation of high dimensional data. Overall, the reviewers found the paper well written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community.  This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so I’m recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.
The reviewers initially gave scores of 1,1,3 citing primarily weak empirical results and a lack of theoretical justification.  The experiments are presented on synthetic examples, which is a great start but the reviewers found that this doesn t give strong enough evidence that the methods developed in the paper would work well in practice.  The authors did not submit an author response to the reviewers and as such the scores did not change during discussion.  This paper would be significantly strengthened with the addition of experiments on actual problems e.g. related to drug discovery which is the motivation in the paper.
The authors present several theorems bounding the generalization error of a class of conv nets (CNNs) with high probability by             O(sqrt(W(beta + log(lambda)) + log(1/delta)]/sqrt(n)),   where W is the number of weights, beta is the distance from initialization in operator norm, lambda is the margin, n is the number of data, and the bound holds with prob. at least 1 delta. (They also present a bound that is tighter when the empirical risk is small.)  The bounds are "size free" in the sense that they do not depend on the size of the *input*, which is assumed to be, say, a d x d image. While there is dependence on the number of parameters, W, there is no implicit dependence on d here.  The paper received the following feedback:  1. Reviewer 3 mostly had clarifying questions, especially with respect to (essentially independent) work by Wei and Ma. Reviewer 3 also pressed the authors to discuss how the bounds compared in absolute terms to the bounds of Bartlett et al. The authors stated that they did not have explicit constants to make such a comparison. Reviewer 3 was satisfied enough to raise their score to a 6.  2. Reviewer 1 admitted they were not experts and raised some issues around novelty/simplicity. I do not think the simplicity of the paper is a drawback. The reviewers unfortunately did not participate in the rebuttal, despite repeated attempts.  3. Reviewer 2 argued for weak reject, despite an interaction with the authors. The reviewer raised the issue of bounds based on control of the Lipschitz constant. The conversation was slightly marred by a typo in the reviewers original comment. I don t believe the authors ultimately responded to the reviewer s point. There was another discussion about simultaneously work and compression based bounds. I would agree with the authors that they need not have cited simultaneous work, especially since the details are quite different. Ultimately, this reviewer still argued for rejection (weakly).  After the rebuttal period ended, the reviewers raised some further concerns with me. I tried to assess these on my own, and ended up with my own questions.  I raise these in no particular order. Each of them may have a simple resolution. In that case, the authors should take them as possible sources of confusion. Addressing them may significantly improve the readability of the paper.  i. Lemma A.3. The order of quantification is poorly expressed and so I was not confident in the statement. In particular, the theorem starts \forall \eta >0 \exists C, .... but then C is REINTRODUCED later, subsequent to existential quantification over M, B, and d and so it seems there is dependence. If there is no dependence, this presentation is sloppy and should be fixed.  ii. Lemma A.4, the same dependence of C on M, B and d holds here and this is quite problematic for the later applications. If this constant is independent of these quantities, then the order of quantifiers has been stated incorrectly. Again, this is sloppy if it is wrong. If it s correct, then we need to know how C grows.  Based on other claims by the authors, it is my understanding that, in both cases, the constant C does not depend on M, B, or d. Regardless, the authors should clarify the dependence. If C does in fact depend on these quantities, and the conclusions change, the paper should be retracted.  iii. Proof of Lemma 2.3. I d remind the reader that the parametrization maps the unit ball to G.   iv. The bound depends on control of operator norms and empirical margins. It is not clear how these interact and whether, for margin parameters necessary to achieve small empirical margin risk, the bounds pick up dependence on other aspects of the learning problem (e.g., depth). I think the only way to assess this would be to investigate these quantities empirically, say, by varying the size and depth of the network on a fixed data set, trained to achieve the same empirical risk (or margin).  I ll add that I was also disappointed that the authors did not attempt to address any of the issues by a revision of the actual paper. In particular, the authors promise several changes that would have been straightforward to make in the two weeks of rebuttal. Instead, the reviewers and myself are left to imagine how things would change. I see at least two promises:  A. To walk back some of the empirical claims about distance from initialization that are based on somewhat flimsy empirical evaluations. I would add to this the need to investigate how the margin and operator norms depend on depth empirically.  B. Attribute Dziugate and Roy for establishing the first bounds in terms of distance from initialization, though their bounds were numerical. I think a mention of simultaneously work would also be generous, even if not strictly necessary. 
The submission presents an approach to single view 3D reconstruction. The approach is quite creative and involves predicting the weights of a network that is then applied to a point set. The presentation is good. The experimental protocol is well informed and the results are convincing. The reviewers  concerns have largely been addressed by the authors  responses and the revision. In particular, R2, who gave a "3", posted "I would now advise to raise my score (3 previously) to a be in line with the 6: Weak Accept given by the other reviewers." This means that all three reviewers recommend accepting the paper. The AC agrees.
The paper proposes a new variational inference based continual learning algorithm with strong performance.  There was some disagreement in the reviews, with perhaps the one shared concern being the complexity of the proposed method. One reviewer brought up other potentially related work, but this was convincingly rebutted by the authors. Finally, one reviewer had an issue with the simplicity with the networks in the experiments, but the authors rightly pointed out that the architectures were simply designed to match those from the baselines.  Continual learning has been an active area for quite some time and convincingly achieving SOTA in a new way is a strong contribution, and will be of interest to the community. Progress in a field is sometimes made by iteratively simplifying an initially complex solution, and this work lays in a brick in that direction. For these reasons, I recommend acceptance.    
Authors propose a novel scheme to perform active learning on image segmentation. This structured task is highly time consuming for humans to perform and challenging to model theoretically as to potentially apply existing active learning methods. Reviewers have remaining concerns over computation and that the empirical evaluation is not overwhelming (e.g., more comparisons). Nevertheless, the paper appears to bring new ideas to the table for this important problem.    
This paper provides theoretical guarantees for adversarial training.  While the reviews raise a variety of criticisms (e.g., the results are under a variety of assumptions), overall the paper constitutes valuable progress on an emerging problem.
The authors proposes a generative model with a hierarchy of latent variables corresponding to a scene, objects, and object parts.   The submission initially received low scores with 2 rejects and 1 weak reject.  After the rebuttal, the paper was revised and improved, with significant portions of the paper completely rewritten (the description of the model was rewritten and a new experiment comparing the proposed model to SPAIR was added).  While the reviewers acknowledged the improvement in the paper and accordingly adjusted their score upward, the paper is still not sufficiently strong enough to be accepted (it currently has 3 weak rejects).   The reviewer expressed the following concerns: 1. The experiments uses only a toy dataset that does not convincingly demonstrate the generalizability of the method to more realistic/varied scenarios. In particular, the reviewers voiced concern that the dataset is tailored to the proposed method  2. Lack of comparisons with baseline methods such as AIR/SPAIR and other work on hierarchical generative models such as SPIRAL. In the revision, the author added an experiment comparing to SPAIR, so this is partially addressed.  As a whole, the paper is still weak in experimental rigor.  The authors argue that as their main contribution is the design and successful learning of a probabilistic scene graph representation, there is no need for ablation studies or to compare against baselines because their method "can bring better compositionality, interpretability, transferability, and generalization".  This argument is unconvincing as in a scientific endeavor, the validity of such claims needs to be shown via empirical comparisons with prior work and ablation studies.  3. Limited novelty The method is a fairly straightforward extension of SPAIR with another hierarchy layer. This would not be a concern if the experimental aspects of the work was stronger.   The AC agrees with the issues pointed by the reviewers.  In addition, the initial presentation of the paper was very poor.  While the paper has been improved, the changes are substantial (with the description of the method and intro almost entirely rewritten). Regardless, despite the improvements in writing, the paper is still not strong enough to be accepted.  I would recommend the authors improve the evaluation and resubmit. 
In this paper the authors highlight the role of time in adversarial training and study various speed distortion trade offs. They introduce an attack called boundary projection BP which relies on utilizing the classification boundary. The reviewers agree that searching on the class boundary manifold, is interesting and promising but raise important concerns about evaluations on state of the art data sets. Some of the reviewers also express concern about the quality of presentation and lack of detail. While the authors have addressed some of these issues in the response, the reviewers continue to have some concerns. Overall I agree with the assessment of the reviewers and do not recommend acceptance at this time.
The author responses and notes to the AC are acknowledged.  A fourth review was requested because this seemed like a tricky paper to review, given both the technical contribution and the application area.  Overall, the reviewers were all in agreement in terms of score that the paper was just below borderline for acceptance.  They found that the methodology seemed sensible and the application potentially impactful.  However, a common thread was that the paper was hard to follow for non experts on MRI and the reviewers weren t entirely convinced by the experiments (asking for additional experiments and comparison to Zhang et al.).  The authors comment on the challenge of implementing Zhang is acknowledged and it s unfortunate that cluster issues prevented additional experimental results.  While ICLR certainly accepts application papers and particularly ones with interesting technical contribution in machine learning, given that the reviewers  struggled to follow the paper through the application specific language it does seem like this isn t the right venue for the paper as written.  Thus the recommendation is to reject.  Perhaps a more application specific venue would be a better fit for this work.  Otherwise, making the paper more accessible to the ML audience and providing experiments to justify the methodology beyond the application would make the paper much stronger.
The paper propose to analyze bitcoin addresses using graph embeddings. The reviewers found that the paper was too incomplete for publication. Important information such as a description of datasets and metrics was omitted.
Reviewers unanimously accepted this paper. 
This paper proposes quantum inspired methods for increasing the parametric efficiency of word embeddings. While a little heavy in terms of quantum jargon, and perhaps a little ignorant of loosely related work in this sub field (e.g. see the work of Coecke and colleagues from 2008 onwards), the majority of reviewers were broadly convinced the work and results were of sufficient merit to be published.
This is an interesting study analyzing learning trajectories and their dependence on hyperparameters, important for better understanding of learning in deep neural networks.  All reviewers agree that the paper has a useful message to the ICLR community, and appreciate changes made by the authors in response to the initial reviews.
The authors propose to extend model based/model free hybrid methods (e.g., MVE, STEVE) to stochastic environments. They use an ensemble of probabilistic models to model the environment and use a lower confidence bound of the estimate to avoid risk. They found that their proposed method yields state of the art performance over previous methods.  The valid concerns by Reviewers 1 & 4 were not addressed by the authors and although the authors responded to Reviewer 3, they did not revise the paper to address their concerns. The ideas and results in this paper are interesting, but without addressing the valid concerns raised by reviewers, I cannot recommend acceptance.
After communicating with each reviewer about the rebuttal, there seems to be a consensus that the paper contains a number of interesting ideas, but the motivation for the paper and the relationship to the literature needs to be expanded.  The reviewers have not changed their scores, and so there is not currently enough support to accept this paper.
This paper extends multi agent imitation learning to extensive form games. There is a long discussion between reviewer #3 and the authors on the difference between Markov Games (MGs) and Extensive Form Games (EFGs). The core of the discussion is on whether methods developed under the MG formalism (where agents take actions simultaneously) naturally can be applied to the EFG problem setting (where agents can take actions asynchronously). Despite the long discussion, the authors and reviewer did not come to an agreement on this point. Given that it is a crucial point for determining the significance of the contribution, my decision is to decline the paper. I suggest that the authors add a detailed discussion on why MG methods cannot be applied to EFGs in the way suggested by reviewer #3 in the next version of this work and then resubmit.
The paper received mixed reviews: WR (R1,R3) and WA (R2). AC has carefully read reviews and rebuttal and examined the paper. Unfortunately, the AC sides with R1 & R3, who are more experienced in this field than R2, and feels that paper does not quite meet the acceptance threshold. The authors should incorporate the comments of the reviewers and resubmit to another venue. 
This submission proposes a statistically consistent saliency estimation method for visual model explainability.  Strengths:  The method is novel, interesting, and passes some recently proposed sanity checks for these methods.  Weaknesses:  The evaluation was flawed in several aspects.  The readability needed improvement.  After the author feedback period remaining issues were:  A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map? How does the performance of LEG change with the resolution (e.g. does it degrade for higher resolution?)? (ii) Figure 6 suggests that SHAP performs best at identifying "pixels that are crucial for the predictions". However, the authors use Figure 7 to argue that LEG is better at identifying salient "pixels that are more likely to be relevant for the prediction". These two observations are contradictory and should be resolved.  The evaluation is still missing some key details for interpreting the results. For example, how representative are the 3 images chosen in Figure 7? Also, in section 5.1 the authors don t describe how many images are included in their sanity check analysis or how those images were chosen.  The new discussion section is not actually a discussion section but a conclusion/summary section.  Because of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods. Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance.  Reviewers largely agreed with the stated shortcomings but were divided on their significance. AC shares the recommendation to reject.
The paper introduces a policy gradient estimator that is based on stochastic recursive gradient estimator. It provides a sample complexity result of O(eps^{ 3/2}) trajectories for estimating the gradient with the accuracy of eps. This paper generated a lot of discussions among reviewers. The discussions were around the novelty of this work in relation to SARAH (Nguyen et al., ICML2017), SPIDER (Fang et al., NeurIPS2018) and the work of Papini et al. (ICML 2018). SARAH/SPIDER are stochastic variance reduced gradient estimators for convex/non convex problems and have been studied in the optimization literature. To bring it to the RL literature, some adjustments are needed, for example the use of importance sampling (IS) estimator. The work of Papini et al. uses IS, but does not use SARAH/SPIDEH, and it does not use step wise IS.  Overall, I believe that even though the key algorithmic components of this work have been around, it is still a valuable contribution to the RL literature. 
Main content: Introduces Progressive Compressed Records (PCR), a new storage format for image datasets for machine learning training. Discussion: reviewer 4: Interesting application of progressive compression to reduce the disk I/O overhead. Main concern is paper could be clearer about setting.  reviewer 5: (not knowledgable about area): well written paper. concern is that related work could be better, including state of the art on the topic. reviewer 2: likes the topic but discusses many areas for improvement (stronger exeriments, better metrics reported, etc.). this is probably the most experienced reviewer marking reject. reviewer 3: paper is well written. Main issue is that exeriments are limited to image classification tasks, and it snot clear how the method works on larger scale.   Recommendation: interesting idea but experiments could be stronger. I lean to Reject.
This paper is consistently supported by all three reviewers and thus an accept is recommended.
The paper predicts properties of quantum states through RNNs.  The idea is nice, but the results are very limited and require more work.  It seems to be more suited for a conference focussing on quantum ML even when the authors have an ML background.  All reviewers agree on a rejection, and their arguments are solid.  The authors offered no rebuttal.
The paper makes broad claims, but the depth of the experiments is very limited to a narrow combination of algorithms.
This paper presents a spatially structured neural memory architecture that supports navigation tasks.  The paper describes a complex neural architecture that integrates visual information, camera parameters, egocentric velocities, and a differentiable 2D map canvas.  This structure is trained end to end with A2C in the VizDoom environment.  The strong inductive priors captured by these geometric transformations is demonstrated to be effective on navigation related tasks in the experiments in this environment.  The reviewers found many strengths and a few weaknesses in this paper.  One strength is that the paper pulls together many related ideas in the mapping literature and combines them in one integrated system.  The reviewers liked the method s ability to leverage semantic reasoning and spatial computation.  They liked the careful updating of the maps and the use of projective geometry.    The reviewers were less convinced of the generality of this method.  The lack of realism in these simulated environments left the reviewers unconvinced that the benefits observed from using projective geometry in this setting will continue to hold in more realistic environments.   The use of fixed geometric transformations with RGBD inputs instead of learned transformations also makes this approach less general than a system that could handle RGB inputs.  Finally, the reviewers noted that the contributions of this paper are not well aligned with the paper s claims.  This paper is not yet ready for publication as the paper s claims and experiments were not sufficiently convincing to the reviewers. 
The authors propose an algorithm for meta rl which reduces the problem to one of model identification. The main idea is to meta train a fast adapting model of the environment and a shared policy, both conditioned on task specific context variables. At meta testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out of distribution tasks than similar methods.  The reviewers agree that the paper has a few significant shortcomings. It s unclear how hyper parameters are selected in the experimental section; the algorithm does not allow for continual adaptation; all policy learning is done through data relabelled by the model.   Overall, the problem the paper addresses is very important, but we do not deem the paper publishable in its current form.
This work shows that there exist neural networks that can be certified by interval bound propagation. It provides interesting and surprising theoretical insights, although analysis requires the networks to be impractically large and hence does not directly yield practical advances. 
This work extends the previously introduced NMN for VQA for handling reasoning over text using symbolic reasoning components that can perform counting, sorting etc and can be compositionally combined. Moreover, to successfully train the model, the authors introduce a simple unsupervised auxiliary loss for training the IE components as well heuristically incorporating inductive biases in the behaviour on couple of components. All reviews agreed that this is a challenging topic and an interesting approach to symbolic reasoning over text. At the same time, reviewers did point that experiments are borderline thin, since the authors start with DROP and drop questions that are not particularly suited for symbolic reasoning, resulting in a substantially smaller dataset. Despite the fact that the experiments could probably be stronger, I’m recommending acceptance cause this topic is very interesting and this is a good paper to raise discussions at ICLR,
The reviewers all appreciated the importance of the topic: understanding the local geometry of loss surfaces of large models is viewed as critical to understand generalization and design better optimization methods.  However, reviewers also pointed out the strength of the assumptions and the limitations of the empirical study. Despite the claim that these assumptions are weaker than those made in prior work, this did not convince the reviewers that the conclusion could be applied to common loss landscapes.  I encourage the authors to address the points made by the reviewers and submit an updated version to a later conference.
This paper addresses the problem of exploration in challenging RL environments using self imitation learning. The idea behind the proposed approach is for the agent to imitate a diverse set of its own past trajectories. To achieve this, the authors introduce a policy conditioned on trajectories. The proposed approach is evaluated on various domains including Atari Montezuma s Revenge and MuJoCo.  Given that the evaluation is purely empirical, the major concern is in the design of experiments. The amount of stochasticity induced by the random initial state alone does not lead to convincing results regarding the performance of the proposed approach compared with baselines (e.g. Go Explore). With such simple stochasticity, it is not clear why one could not use a model to recover from it and then rely on an existing technique like Go Explore. Although this paper tackles an important problem (hard exploration RL tasks), all reviewers agreed that this limitation is crucial and I therefore recommend to reject this paper.
This paper introduces a neural architecture search method that is geared towards yielding good uncertainty estimates for out of distribution (OOD) samples.  The reviewers found that the OOD prediction results are strong, but criticized various points, including the presentation of the OOD results, novelty as a NAS paper, missing citations to some recent papers, and a lack of baselines with simpler ensembles. The authors improved the presentation of their OOD results and provided new experiments, which causes one reviewer to increase his/her score from a weak reject to an accept. The other reviewers appreciated the rebuttal, but preferred not to change their scores from a weak reject and a reject, mostly due to lack of novelty as a NAS paper.  I also read the paper, and my personal opinion is that it would definitely be very novel to have a good neural architecture search for handling uncertainty in deep learning; it is by no means the case that "NAS for X" is not interesting just because there are now a few papers for "NAS for Y". As long as X is relevant (which uncertainty in deep learning definitely is), and NAS finds a new state of the art, I think this is great. For such an "application" paper of the NAS methodology, I do not find it necessary to introduce a novel NAS method, but just applying an existing one would be fine. The problem is more that the paper claims to introduce a new method, but that that method is too similar to existing ones, without a comparison; actually just using an existing NAS method would therefore make the contribution and the emphasis on the application domain clearer.  I have one small question to the authors about a part that I did not understand: to optimize WAIC (Eq 1), why is it not optimal to just set the parameterization \phi such that the variance is minimized, i.e., return a delta distribution p_\phi that always returns the same architecture (one with a strong prediction)? Surely, that s not what the authors want, but wouldn t that minimize WAIC? I hope the authors will clarify this in a future version.  In the private discussion of reviewers and AC, the most positive reviewer emphasized that the OOD results are strong, but admitted that the mixed sentiment is understandable since people who do not follow OOD detection could miss the importance and context of the results, and that the paper could definitely improve its messaging. The other reviewers  scores remained at 1 and 3, but the reviewers indicated that they would be positive about a future version of the paper that fixed the identified issues. My recommendation is to reject the paper and encourage the authors to continue this work and resubmit an improved version to a future venue.
Thanks to the authors for submitting the paper and providing further explanations and experiments. This paper aims to ensure robustness against several perturbation models simultaneously. While the authors  response has addressed several issues raised by the reviewers, the concern on the lack of novelty remains. Overall, there is not enough support among the reviewers for the paper to be accepted.
The paper efficiently computes quantities, such as variance estimates of the gradient or various Hessian approximations, jointly with the gradient, and the paper also provides a software package for this. All reviewers agree that this is a very good paper and should be accepted.
This paper proposes an alternating dialog model based on transformers and GPT 2, that model each conversation side separately and aim to eliminate human supervision. Results on two dialog corpora are either better than or comparable to state of the art. Two of the reviewers raise concerns about the novel contributions of the paper, and did not change their scores after authors  rebuttal. Furthermore, one reviewer raises concerns about the lack of detailed experiments aiming to explain where the improvements come from. Hence,  I suggest rejecting the paper.
Congratulations on getting your paper accepted to ICLR. Please make sure to incorporate the reviewers  suggestions for the final version.
The paper computes an "approximate" generalization bound based on loss curvature. Several expert reviewers found a long list of issues, including missing related work and a sloppy mix of formal statements and heuristics, without proper accounting of what could be gleaned from some many heuristic steps. Ultimately, the paper needs to be rewritten and re reviewed. 
This paper was reviewed by 3 experts, who recommend Weak Reject, Weak Reject, and Reject. The reviewers were overall supportive of the work presented in the paper and felt it would have merit for eventual publication. However, the reviewers identified a number of serious concerns about writing quality, missing technical details, experiments, and missing connections to related work. In light of these reviews, and the fact that the authors have not submitted a response to reviews, we are not able to accept the paper. However given the supportive nature of the reviews, we hope the authors will work to polish the paper and submit to another venue.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  Concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition.
This is a strong submission, and I recommend acceptance. The idea is an elegant one: sparsify a network at initialization using a distribution that achieves approximate orthogonality of the Jacobian for each layer. This is well motivated by dynamical isometry theory, and should imply good performance of the pruned network to the extent that the training dynamics are explainable in terms of a linearization around the initial weights. The paper is very well written, and all design decisions are clearly motivated. The experiments are careful, and cleanly demonstrate the effectiveness of the technique. The one shortcoming is that the experiments don t use state of the art modern architectures, even thought that ought to have been easy to try. The architectures differ in ways that could impact the results, so it s not clear to what extent the same principles describe SOTA neural nets. Still, this is overall a very strong submission, and will be of interest to a lot of researchers at the conference.  
This paper advocates for the application of entanglement entropy from quantum physics to understand and improve the inductive bias of neural network architectures for question answering tasks. All reviewers found the current presentation of the method difficult to understand, and as a result it is difficult to determine what exactly the contribution of this work is. One suggestion for improving the manuscript is to minimize the references to quantum entanglement (where currently is it asserted without justification that entanglement entropy is a relevant concept for modeling question answering tasks). Instead, presenting the method as applications of tensor decompositions for parameterizing neural network architectures would make the work more accessible to a machine learning audience, and help clarify the contribution with respect to related works [1].   1. http://papers.nips.cc/paper/8495 a tensorized transformer for language modeling.pdf
The authors propose a framework for relating adversarial robustness, privacy and utility and show how one can train models to simultaneously attain these properties. The paper also makes interesting connections between the DP literature and the robustness literature thereby porting over composition theorems to this new setting.  The paper makes very interesting contributions, but a few key points require some improvement: 1) The initial version of the paper relied on an approximation of the objective function in order to obtain DP guarantees. While the authors clarified how the approximation impacts model performance in the rebuttal and revision, the reviewers still had concerns about the utility privacy robustness tradeoff achieved by the algorithm.  2) The presentation of the paper seems tailored to audiences familiar with DP and is not easy for a broader audience to follow.  Despite this limitations, the paper does make significant novel contributions on an improtant problem (simultaneously achieveing privacy, robustness and utility) and could be of interest.   Overall, I consider this paper borderline and vote for rejection, but strongly encourage the authors to improve the paper wrt the above concerns and resubmit to a future venue.
All three reviewers agreed that the paper should not be accepted. No rebuttal was offered, thus the paper is rejected.
The paper addresses the setting of learning with rejection while incorporating the ideas from learning with adversarial examples to tackle adversarial attacks. While the reviewers acknowledged the importance to study learning with rejection in this setting, they raised several concerns: (1) lack of technical contribution   see R1’s and R2’s related references, see R3’s suggestion on designing c(x); (2) insufficient empirical evidence   see R3’s comment about the sensitivity experiment on the strength of the attack, see R1’s suggestion to compare with a baseline that learns the rejection function such as SelectiveNet;  (3) clarity of presentation   see R2’s suggestions how to improve clarity. Among these, (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC can confirm that all three reviewers have read the author responses and have revised the final ratings. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
The authors propose a simple modification of local SGD for parallel training, starting with standard SGD and then switching to local SGD. The resulting method provides good results and makes a practical contribution. Please carefully account for reviewer comments in future revisions.
This paper proposes  architectural modifications to transformers, which are promising for sequential tasks requiring memory but can be unstable to optimize, and applies the resulting method to the RL setting, evaluated in the DMLab 30 benchmark.  While I thought the approach was interesting and the results promising, the reviewers unanimously felt that the experimental evaluation could be more thorough, and were concerned with the motivation behind of some of the proposed changes. 
This paper conducts an extensive study of training BERT and shows that its performance can be improved significantly by choosing a better training setup (e.g., hyperparameters, objective functions). I think this paper clearly offers a better understanding of the importance of tuning a language model to get the best performance on downstream tasks. However, most of the findings are obvious (careful tuning helps, more data helps). I think the novelty and technical contributions are rather limited for a conference such as ICLR. These concerns are also shared by all the reviewers. The review scores are borderline, so I recommend to reject the paper.
This paper proposes a graph embedding method for the whole graph under both unsupervised and semi supervised setting. It can extract a fixed length graph level representation with good generalization capability. All reviewers provided unanimous rating of weak accept. The reviewers praise the paper is well written and is value to different fields dealing with graph learning. There are some discussions on the novelty of the approach, which was better clarified after the response from the authors. Overall this paper presents a new effort in the active topic of graph representation learning with potential large impact to multiple fields. Therefore, the ACs recommend it to be an oral paper.
The paper is about learning policies in RL while ensuring safety (avoid constraint violations) during training and testing.   For this meta review, I ignore Reviewer #3 because that review is useless. The discussion between the authors and Reviewer #1 was useful.  Overall, the paper introduces an interesting idea, and the wider context (safe learning) is very relevant. However, I also have some concerns. One of my biggest concerns is that the method proposed here relies heavily on linearizations to deal with nonlinearities. However, the fact that this leads to approximation errors is not being acknowledged much. There are also small things, such as the (average) KL divergence between parameters, which makes no sense to me because the parameters don t have distributions (section 3.1).   In terms of experiments, I appreciate that the authors tested the proposed method on multiple environments. The results, however, show that safety cannot be guaranteed. For example, in Figure 1(c), SDDPG clearly violates the constraints. The figures are also misleading because they show the summary statistics of the trajectories (mean and standard deviation). If we were to look at individual trajectories, we would find trajectories that violate the constraints. This fact is brushed under the carpet in the evaluation, and the paper even claims that "our algorithms quickly stabilize the constraint cost below the threshold". This may be true on average, but not for all trajectories. A more careful analysis and a more honest discussion would have been useful. In the robotics experiment, I would like to understand why we allow for any collisions. Why can t we set $d_0 0$, thereby disallowing for collisions. The threshold in the paper looks pretty arbitrary.  Again, the paper states that  "Figure 4a and Figure 4b show that the Lyapunov based PG algorithms have higher success rates". This is a pretty optimistic interpretation of the figure given the size of the error bars.   There are some points in the conclusion, I also disagree with: 1) "achieve safe learning": Given that some trajectories violate the constraints, "safe" is maybe a bit of an overstatement 2) "better data efficiency": compared to what? 3) "scalable to tackle real world problems": I disagree with this one as well because for all experiments you will need to run an excessive number of trials, which will not be feasible on a real world system (assuming we are talking about robots).  Overall, I think the paper has some potential, but it needs some more careful theoretical analysis (e.g., effect of linearization errors) and some better empirical analysis.   Additionally, given that the paper is at around 9 pages (including the figures in the appendix, which the main paper cites), we are supposed to have higher standards on acceptance than an 8 pages paper.  Therefore, I recommend to reject this paper.
The majority of reviewers suggest rejection, pointing to concerns about design and novelty. Perhaps the most concerning part to me was the consistent lack of expertise in the applied area. This could be random bad luck draw of reviewers, but more likely the paper is not positioned well in the ICLR literature. This means that either it was submitted to the wrong venue, or that the exposition needs to be improved so that the paper is approachable by a larger part of the ICLR community. Since this is not currently true, I suggest that the authors work on a revision.
The paper improves the previous method for detecting out of distribution  (OOD) samples.   Some theoretical analysis/motivation is interesting as pointed out by a reviewer. I think the paper is well written in overall and has some potential.  However, as all reviewers pointed out, I think experimental results are quite below the borderline to be accepted (considering the ICLR audience), i.e., the authors should consider non MNIST like and more realistic datasets. This indicates the limitation on the scalability of the proposed method.   Hence, I recommend rejection.
Maintaining the privacy of membership information contained within the data used to train machine learning models is paramount across many application domains.  Moreover, this risk can be more acute when the model is used to make predictions using out of sample data.  This paper applies a causal learning framework to mitigate this problem, motivated by the fact that causal models can be invariant to the training distribution and therefore potentially more resistant to certain privacy attacks.  Both theoretical and empirical results are provided in support of this application of causal modeling.  Overall, during the rebuttal period there was no strong support for this paper, and one reviewer in particular mentioned lingering unresolved yet non trivial concerns.  For example, to avoid counter examples raised the reviewer, a deterministic labeling function must be introduced, which trivializes the distribution p(Y|X) and leads to a problematic training and testing scenario from a practical standpoint.  Similarly the theoretical treatment involving Markov blankets was deemed confusing and/or misleading even after careful inspection of all author response details.  At the very least, this suggests that another round of review is required to clarify these issues before publication, and hence the decision to reject at this time.
The present paper addresses the problem of imitation learning in multi modal settings, combining vision, language and motion. The proposed approach learns an abstract task representation, and the goal is to use this as a basis for generalization. This paper was subject to considerable discussion, and the authors clarified several issues that reviewers raised during the rebuttal phase. Overall, the empirical study presented in the paper remains limited, for example in terms of ablations (which components of the proposed model have what effect on performance) and placement in the context of prior work. As a result, the depth of insights is not yet sufficient for publication.
This paper presents new non linearity function which specially affects regions of the model which are densely valued. The non linearity is simple: it retains only top k highest units from the input, while truncating the rest to zero. This also makes the models more robust to adversarial defense which depend on the gradients. The non linearity function is shown to have better adversarial robustness on CIFAR 10 and SVHN datasets. The paper also presents theoretical analysis for why the non linearity is a good function.  The authors have already incorporated major suggestions by the reviewers and the paper can make significant impact on the community. Thus, I recommend its acceptance.
The paper presents a technique for learning RL agents to generalize well to unseen environments.  All reviewers and AC think that the paper has some potential but is a bit below the bar to be accepted due to the following facts:  (a) Limited experiments, i.e., consider more appealing baselines/scenarios and provide more experimental details. (b) The proposed method/idea is simple/reasonable, but not super novel, i.e., not enough considering the ICLR high standard (potentially enough for a workshop paper).  Hence, I think this is a borderline paper toward rejection.  
This paper presents a feature normalization method for CNNs by decorrelating channel wise and spatial correlation simultaneously. Overall all reviewers are positive to the acceptance and I support their opinions. The idea and implementation is relatively straightforward but well motivated and reasonable. Experiments are well organized and intensive, providing enough evidence to convince its effectiveness in terms of final accuracy and convergence speed. Also, it’s analogy to biological center surrounded structure is thought provoking. The novelty of the method seems somewhat incremental considering that there already exists a channel wise decorrelation method, but I think the findings of the paper are interesting and valuable enough for ICLR community and would like to recommend acceptance. Minor comments: I recommend authors to mention about zero component analysis (ZCA) normalization, which has been a standard input normalization method for CIFAR datasets. I guess it is quite similar to the proposed method considering 1x1 convolution. Also, comparison with other recent normalization methods (e.g., Group Norm) would be useful.  
This paper investigates the tasks used to pretrain language models. The paper proposes not using a generative tasks ( filling in  masked tokens), but instead a discriminative tasked (recognising corrupted tokens). The authors empirically show that the proposed method leads to improved performance, especially in the "limited compute" regime.   Initially, the reviewers had quite split opinions on the paper, but after the rebuttal and discussion phases all reviewers agreed on an "accept" recommendation. I am happy to agree with this recommendation based on the following observations:   The authors provide strong empirical results including relevant ablations. Reviews initially suggested a limitation to classification tasks and a lack of empirical analysis, but those issues have been addressed in the updated version.    The problem of pre training language model is relevant for the ML and NLP communities, and it should be especially relevant for ICLR. The resulting method significantly outperforms existing methods, especially in the low compute regime.    The idea is quite simple, but at the same time it seems to be a quite novel idea. 
The paper proposes to combine RL and Imitation Learning. It defines a regularized reward function that minimizes the KL distance between the policy and the expert action. The formulation is similar to the KL regularized MDPs, but with the difference that an additional indicator function based on the support of the expert’s distribution is multiplied to the regularized term.  Several issues have been brought up by the reviewers, including: * Comparison with pre deep learning literature on the combination of RL and imitation learning * Similarity to regularized MDP framework * Assumption 1 requiring a stochastic expert policy, contradicting the policy invariance claim * Difficulty of learning the indicator function of the support of the expert’s data distribution  Some of these issues have been addressed, but at the end of the day, one of the expert reviewers was not convinced that the problem of learning an indicator function is going to be easy at all. The reviewer believes that learning such a function requires "learning a harsh approximation of the density of visits of the expert for every state which is a quite hard task, especially in stochastic environments.”   Another issue is related to the policy invariance under the optimal expert policy. In most MDPs, the optimal policy is not stochastic and does not satisfy Assumption 1, so the optimal policy invariance proof seems to contradict Assumption 1.  Overall, it seems that even though this might become a good paper, it requires some improvements. I encourage the authors to address the reviewers’ comments as much as possible.
This paper proposes an alternative loss function, the max mahalanobis center loss, that is claimed to improve adversarial robustness.   In terms of quality, the reviewers commented on the convincing experiments and theoretical results, and were happy to see the sample density analysis.   In terms of clarity, the reviewers commented that the paper is well written.   The problem of adversarial robustness is relevant to the ICLR community, and the proposed approach is a novel and significant contribution in this area.   The authors have also convincingly answered the questions of the authors and even provided new theoretical and experimental results in their final upload. 
This paper leverages the piecewise linearity of predictions in ReLU neural networks to encode and learn piecewise constant predictors akin to oblique decision trees. The reviewers think the paper is interesting, and the idea is clever. The paper can be further improved in experiments. This includes comparison to ensembles of traditional trees or (in some cases) simple ReLU networks. Also  the tradeoffs other than accuracy between the method and baselines are also interesting. 
This paper introduces a new architecture based on intrinsic rewards, to deal with partially observable and sparse reward domains.  The reviewers found the novelty of the work not particularly high, and had concerns about the general utility of the method based on the empirical evidence. This paper has numerous issues and could use significant revision in terms of writing, connections to literature, experiment design, and clarity of results.   Much of the discussion focused on the scaling parameter. From an algorithmic point of view, the scaling parameter is very problematic. It is domain specific and when tuned per domain resulted in very different values. The ablation study showed that only two settings in one domain led to good performance, whereas the other resulted in no learning (for some reason the other two values were not plotted).   There are concerns that the baselines were not completely fair. In many cases different domains were used to compare against RND and ICM, and there appears to be no tuning of these baselines for the new domains this a problem due to the inherent bias in favor one s own method. In the solaris domain which was used in the RND paper, the results don t appear to match the RND paper, and in vizdoom the performance numbers are difficult to compare for ICM because a different metric is used even if you don t like their performance numbers at least report them once so we can be confident the baselines are well calibrated. One reviewer pointed out the meta parameters where different for RND than the published previous, but the paper does not describe what approach was used to tune those parameters and this is not acceptable. We cannot have much confidence that these results are reflective of those methods. Finally, there is no comment on how the performance numbers were computed and no description of how the errorbars where computed or what they represent.   The paper focuses on partially observable domains, the evidence that this method is effective in closer to Markov settings is unclear. The Atari experiments do not yield significant results by large (solairs looks as if there is no learning occurring at all a no comment about it in the text to explain). The paper claims evidence the approach can work well in both cases, but it was not even indicated if frame stacking was used in the Atari experiments. In fact, the result was only alluded too in the conclusion there was no reference in the main text to a specific result in the appendix. Text is very challenging to read. The language is informal and imprecise, and the paper frequently uses terms incorrectly or in different ways through (e.g., the use of the term novelty throughout)  This is clearly an interesting direction. The authors should keep working, but this paper is not ready for publication. I urge the authors to dig deeper in the literature to gain a more nuanced understanding of the topic. Barto et al s excellent paper on the topic is a great place to start: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/ 
This paper introduces a new ECG dataset. While I appreciate the efforts to clarify several points raised by the reviewers, I still believe this contribution to be of limited interest to the broad ICLR community. As such, I suggest this paper to be submitted to a more specialised venue.
This paper presents several models for recognition aware image enhancement. The authors propose to enhance the image quality in the presence of image degradation (e.g., low resolution, noise, compression artifacts) as well as to improve the recognition accuracy in a joint model. While acknowledging that the paper is addressing an interesting direction, the reviewers and AC note the following potential weaknesses: presentation clarity, limited technical contributions, insufficient empirical evidence. AC can confirm all the reviewers have read the rebuttal and have contributed to the discussion. All the reviewers and AC agree that the rebuttal was informative, and the authors have partially addressed some of the concerns (e.g. additional experiments). R2 has raised the score from reject to weak reject. However, at this stage AC suggest the manuscript is below the acceptance bar and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper.
This paper investigates using sound to improve classification, motion prediction, and representation learning all from data generated by a real robot.  All the reviewers were intrigued by the work. The paper provides experiments on real robots (never a small task), and a data set for the community, and a sequence of illustrative experiments. Because the paper combines existing techniques, its main contribution is the empirical demonstrations of the utility of using sound. Overall, it was not quite enough for the reviewers. The main issues were: (1) motion prediction is perhaps expected given the physical setup, (2) lack of comparison with other approaches, (3) lack of diversity in the demonstrations (10 objects, one domain).  The authors added two new experiments with a different setup, further demonstrating their claims. In addition the authors highlighted that the novelty of this task means there are no clear baselines (to which r3 agreed). The new experiments are briefly described in the response (and visuals on a website), but the authors did not update the paper. The new experiments could potentially significantly strength the paper. However, the terse description in the response and the supplied visuals made it difficult for the reviewers to judge their contribution.  Overall, this is certainly a very interesting direction. The results on real world data demonstrate promise, even if they are not the benchmarking style the community is used too.   
It seems to be an interesting contribution to the area. I suggest acceptance.
The submission describes a new two stage training scheme for multi modal image to image translation. The new scheme is compared to a single stage end to end baseline, and the advantage of the new scheme is demonstrated empirically. All three reviewers appreciate the proposed contribution and the quality improvement it brings over the baseline. At the same time, the reviewers see the contribution as incremental and not sufficient for an ICLR paper. The author response and paper adjustment have not changed the opinion of the reviewers, so the overall recommendation is to reject.
The paper studies the impact of rounding errors on deep neural networks. The                                                        authors apply Monte Carlos arithmetics to standard DNN operations.                                                                  Their results indeed show catastrophic cancellation in DNNs and that the resulting loss of                                          significance in the number representation correlates with decrease in validation                                                    performance, indicating that DNN performances are sensitive to rounding errors.                                                                                                                                                                                         Although recognizing that the paper addresses an important problem (quantized /                                                     finite precision neural networks), the reviewers point out the contribution of                                                      the paper is somewhat incremental.                                                                                                  During the rebuttal, the authors made an effort to improve the manuscript based                                                     on reviewer suggestions, however review scores were not increased.                                                                                                                                                                                                      The paper is slightly below acceptance threshold, based on reviews and my own                                                       reading, as the method is mostly restricted to diagnostics and cannot yet be used                                                   to help training low precision neural networks.
This paper presents a upper bound on the curvature of a deep network. After the discussion, the author has addressed some concerns of reviwers, but the results are not very strong, there is some limitation on the applications. There is no strong support for this paper. Due to the high standard of ICLR, the acceptance of the paper need strong results in terms of theory or experiments.
The reviewers were not convinced about the significance of this work. There is no empirical or theoretical result justifying why this method has advantages over the existing methods. The reviewers also raised concerns related to the scalability of the proposal. Since none of the reviewers were enthusiastic about the paper, including the expert ones, I cannot recommend acceptance of this work.
This paper proposes a phrase based attention method to model word n grams (as opposed to single words) as the basic attention units. Multi headed phrasal attentions are designed within the Transformer architecture to perform token to token and token to phrase mappings. Some improvements are shown in English German, English Russian and English French translation tasks on the standard WMT 14 test set, and on the one billion word language modeling benchmark.  While the proposed approach is interesting and takes inspiration in the notion of phrases used in phrase based machine translation, with some positive empirical results, the technical novelty of this paper is rather limited, and the experiments could be more solid. While it is understandable that lack of computational resources made it hard to experiment with larger models (e.g. Transformer big), perhaps it would be interesting to try on language pairs with fewer resources (smaller datasets), where base models are more competitive.
All reviewers unanimously accept the paper.
The paper proposes a method for OOD detection which leverages the uncertainties associated with the features at the intermediate layers (and not just the output layer).  All the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about other relevant baselines, some of the reported empirical results, and clarity of the explanation.  I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
This is an interesting paper that is concerned with single episode transfer to reinforcement learning problems with different dynamics models, assuming they are parameterised by a latent variable. Given some initial training tasks to learn about this parameter, and a new test task, they present an algorithm to probe and estimate the latent variable on the test task, whereafter the inferred latent variable  is used as input to a control policy.  There were several issues raised by the reviewers. Firstly, there were questions with the number of runs and the baseline implementations, which were all addressed in the rebuttals. Then, there were questions around the novelty and the main contribution being wall clock time. These issues were also adequately addressed.  In light of this, I recommend acceptance of this paper.
This paper proposes a method for finding neural architecture which, through the use of selective branching, can avoid processing portions of the network on a per data point basis.   While the reviewers felt that the idea proposed was technically interesting and well presented, they had substantial concerns about the evaluation that persisted post rebuttal, and lead to a consensus rejection recommendation.
One reviewer is positive, while the others recommend rejection. The authors did not submit a rebuttal, thus the reviewers kept their original assessment.
This paper proposes a new method for zero shot policy transfer in RL. The authors propose learning the policy over a disentangled representation that is augmented with attention. Hence, the paper is a simple modification of an existing approach (DARLA). The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited. For this reason I recommend rejection.
This paper proposes a method to allow models to generalize more effectively through the use of latent linear transforms.  Overall, I think this method is interesting, but both R2 and R4 were concerned with the experimental evaluation being too simplistic, and the method not being applicable to areas where a good simulator is not available. This seems like a very valid concern to me, and given the high bar for acceptance to ICLR, I would suggest that the paper is not accepted at this time. I would encourage the authors to continue with follow up experiments that better showcase the generality of the method, and re submit a more polished draft to a conference in the near future.
This paper proposes a new algorithm called Continuous Sparsification (CS) to search for winning tickets (in the context of the Lottery Ticket Hypothesis from Frankle & Carbin (2019)), as an alternative to the Iterative Magnitude Pruning (IMP) algorithm proposed therein. CS continuously removes parameters from a network during training, and learns the sub network s structure with gradient based methods instead of relying on pruning strategies. The papers shows empirically that CS finds lottery tickets that outperforms the ones learned by ITS with up to 5 times faster search, when measured in number of training epochs.  While this paper presents a novel contribution of pruning and of finding winning lottery tickets and is very well written, there are some concerns raised by the reviewers regarding the current evaluation. The paper presents no concrete data on the comparative costs of performing CS and IMP even though the core claim is that CS is more efficient. The paper does not disclose enough detail to compute these costs, and it seems like CS is more expensive than IMP for standard workflows. Moreover, the current presentation of the data through "pareto curves" is misleadingly favorable to CS. The reviewers suggest including more experiments on ImageNet and  a more thorough evaluation as a pruning technique beyond the lottery ticket hypothesis. We recommend the authors to address the detailed reviewers  comments in an eventual ressubmission. 
In this work the authors build on the Dirichlet prior network of Malinin & Gales, replacing the loss function and adding a regularization term which improve training in the setting with a significant number of classes.   Improving uncertainty for deep learning is a challenging but very important problem.  The reviewers of this paper gave two weak rejects (one is of low confidence) and one weak accept.  They found the paper well written, easy to follow and well motivated but somewhat incremental and not entirely empirically justified.  None of the reviewers were willing to strongly champion the paper for acceptance.  Unfortunately as such the paper falls below the bar for acceptance.  It appears that the authors significantly added to the experiments in the discussion phase and hopefully that will make the paper much stronger for a future submission.
This paper proposes a method for improving exploration by implementing intrinsic rewards based on optical flow prediction error. The approach was evaluated on several Atari games, Super Mario, and VizDoom.  There are several strengths to this work, including the fact that it comes with open source code, and several reviewers agree it’s an interesting approach. R1 thought it was well written and quite easy to follow. I also commend the authors for being so responsive with comments and for adding the new experiments that were asked for.  The main issue that reviewers pointed out, and which I am also concerned about, is how these particular games were chosen. R3 points out that these 5 Atari games are not known for being hard exploration games. Authors did conduct further experiments on 6 Atari games suggested by the reviewer, but the results didn’t show significant improvement over baselines.  I appreciate the authors’ argument that every method has “its niche”, but the environments chosen must still be properly motivated. I would have preferred to see results on all Atari games, along with detailed and quantitative analysis into why FICM fails on specific tasks. For instance, they state in the rebuttal that “The selection criteria of our environments is determined by the relevance of motions of the foreground and background components (including the controllable agent and the uncontrollable objects) to the performance (i.e., obtainable scores) of the agent.” But it doesn’t seem like this was assessed in any quantitative way.  Without this understanding, it’d be difficult for an outsider to know which tasks are appropriate to use with this approach. I urge the authors to focus on expanding and quantifying the work they depict in Figure 8, which, although it begins to illuminate why FICM works for some games and not others, is still only a qualitative snapshot of 2 games. I still think this is a very interesting approach and look forward to future versions of this paper.
This paper considers inter domain policy transfer in reinforcement learning. The proposed approach involves adapting existing policies from a source task to a target task by adding a cost related to the difference between the dynamics and trajectory likelihoods of the two tasks.  There are three major problems with this paper as it stands, as pointed out by the reviewers. Firstly, the "KL divergence" is not a real KL divergence and seems to be only empirically motivated. Then, there are issues with the derivative of the policy gradient. Finally, the theory is not well connected to the proposed algorithm. The rebuttals not only failed to convince the reviewer that raised these issues, but another reviewer lowered their score as a result of these raised points.  This is a really interesting idea with compelling experiments, but must be rejected at this point for the aforementioned reasons.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  The most significant concerns raised were about the strength of the experiments, and choice of appropriate baselines.
The paper proposes an alternative to BPTT for training recurrent neural networks based on an explicit state variable, which is trained to improve both the prediction accuracy and the prediction of the next state. One of the benefits of the methods is that it can be used for online training, where BPTT cannot be used in its exact form. Theoretical analysis is developed to show that the algorithm converges to a fixed point. Overall, the reviewers appreciate the clarity of the paper, and find the theory and the experimental evaluation to be reasonably well balanced. After a round of discussion, the authors improved the paper according to the reviews. The final assessments are overall positive, and I’m therefore recommending accepting this paper.
This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image to image translation). The authors analyze the space of exact and approximated solutions under automorphisms.  Reviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image to image translation.  
The paper makes a reasonable contribution to extracting useful features from a pre trained neural network.  The approach is conceptually simple and sufficient evidence is provided of its effectiveness.  In addition to the connection to tangent kernels there also appears to be a relationship to holographic feature representations of deep networks.  The authors did do a reasonable job of providing additional ablation studies, but the paper would be improved if a clearer study were added to investigate applying the technique to different layers.  All of the reviewer comments appear worthwhile, but AnonReviewer2 in particular provides important guidance for improving the paper.
This paper argues that NNs deployed to hardware needs to robust to additive noise and introduces two methods to achieve this.  The reviewers liked aspects of the paper and the paper is borderline. However, all in all sufficient reservations were raised to put the paper below the threshold. The criticism was constructive and can be used in an updated version submitted to next conference.  Rejection is recommended.
The paper received Weak Reject scores from all three reviewers. The AC has read the reviews and lengthy discussions and examined the paper. AC feels that there is a consensus that the paper does not quite meet the acceptance threshold and thus cannot be accepted. Hopefully the authors can use the feedback to improve their paper and resubmit to another venue.
The paper builds a transition based dependency parser for Amharic, first predicting transitions and then dependency labels. The model is poorly motivated, and poorly described. The experiments have serious problems with their train/test splits and lack of baseline. The reviewers all convincingly argue for reject. The authors have not responded. 
This paper proposes a multi frame super resolution method including recursive fusion for co registration and registration loss to solve the problem where the super resolution results and the high resolution labels are not pixel wise aligned. While reviewer #1 is positive about this paper, reviewer #2 and #3 rated weak reject and reject respectively. Both reviewer #2 and #3 have extensive experience in the topic of image super resolution. The major concerns raised by the reviewers include the lack of many references, the comparison of recursive fusion with related work, limited test databases, using a single translational motion for the SR images, and limited novelty on the network modules.  The authors provided detailed response to the concerns, however they did not change the overall rating of the reviewers. While the ACs agree that this work has merits, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.
This paper studies whether adopting strategy adaptation mechanisms helps players improve their performance in zero sum stochastic games (in this case baseball). Moreover they study two questions in particular, a) whether adaptation techniques are helpful when faced with a small number of iterations and 2) what’s the effect of different initial strategies when both teams adopt the same adaptation technique. Reviewers expressed concerns regarding the fact that the author’s adaptation techniques improve upon initial strategies, which seems to indicate that their initial strategies were not Nash (despite the use of CFR). In the lack of theory of why this seems to happen at the current setup (and whether indeed the initial strategies are Nash and why do the improve), stronger empirical evidence from more rigorous experiments seem somewhat necessary for recommending acceptance of this paper.
This paper proposes a method for aligning an input text with the frames in a video that correspond to what the text describes in a weakly supervised way. The main technical contribution of the paper is the use of co attention at different abstraction levels.  Among the four reviewers, one reviewer advocates for the paper while the others find this paper to be a borderline reject paper. Reviewer3 who was initially positive about the paper, during the discussion period, expressed that he/she wants to downgrade his/her rating to weak reject after reading the other reviewers  comments and concerns. The main concern of the reviewers is that the contribution of the paper incremental, particularly since the idea of co attention has been used in many different area in other context. The authors responded to this in the rebuttal that the proposed approach incorporate different components such as Positional Encodings and is different from prior work, and that they experimentally perform superior compared to other co attention usages such as LCGN. Although the AC understands the authors response, the majority of the reviewers are still not fully convinced about the contribution and their opinion stay opposed to the paper.
This paper proposes a method to improve alignments of a multilingual contextual embedding model (e.g., multilingual BERT) using parallel corpora as an anchor. The authors show the benefit of their approach in a zero shot XNLI experiment and present a word retrieval analysis to better understand multilingual BERT.  All reviewers agree that this is an interesting paper with valuable contributions. The authors and reviewers have been engaged in a thorough discussion during the rebuttal period and the revised paper has addressed most of the reviewers concerns.  I think this paper would be a good addition to ICLR so I recommend accepting this paper.
The authors propose a graph inference learning framework to address the issues of sparse labeled data in graphs. The authors use structural information and node attributes to define a structure relation which is then use to infer unknown labels from known labels. The authors demonstrate the effectiveness of their approach on four benchmark datasets.  The approach presented in the paper is sound and the empirical results are convincing. All reviewers have given a positive rating for this paper. Two reviewers had some initial concerns about the paper but after the rebuttal they have acknowledged the answers given by the authors and adjusted their scores. R1 still has concerns about the motivation of the paper and I request the authors to adequately address this in their final version.
The paper proposes a new architecture for unsupervised image2image translation. Following the revision/discussion, all reviewers agree that the proposed ideas are reasonable, well described, convincingly validated, and of clear though limited novelty. Accept.
The paper considers an important problem in medical applications of deep learning, such as variability/stability of  model s predictions in face of various perturbations in the model (e.g., random seed), and evaluates different approaches to capturing model uncertainty. However, it appears to be little innovation in terms of machine learning methodology, so ICLR might not be the best venue for this work, while perhaps other venues focused more on medical applications might be a better fit.   
The reviewers have uniformly had significant reservations for the paper. Given that the authors did not even try to address them, this suggests the paper should be rejected.
The paper proposes a nice and easy way to regularize spectral graph embeddings, and explains the effect through a nice set of experiments. Therefore, I recommend acceptance.
This paper proposes a technique for training embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. The model is trained to play this game from scratch without any prior knowledge of its visual world, and experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation.  While reviewers found the paper explores an interesting direction, concerns were raised that many claims are unjustified. For example, in the discussion phase a reviewer asked how can one infer "hider learns to first turn away from the seeker then run away" from a single transition frequency? Or, the rebuttal mentions "The agent with visibility reward does not get the chance to learn features of self visibility because of the limited speed hence the model received samples with significantly less variation of its self visibility, which makes learning to discriminate self visibility difficult". What is the justification for this? There could be more details in the paper and I d also like to know if these findings were reached purely by looking at the histograms or by combining visual analysis with the histograms.  I suggest authors address these concerns and provide quantitative results for all of the claims in an improved iteration of this paper. 
This paper investigates how the properties of an environment affect the success of reinforcement learning, and in particular finds that random dynamics and non episodic learning makes learning easier, even though these factors make learning more difficult when applied individually. The paper was reviewed by three experts who gave Reject, Weak Reject, and Weak Reject recommendations. The main concerns are about missing connections to related work, overstating some contributions, and experimental details. While the author response addressed many of these issues, reviewers felt another round of peer review is really needed before this paper can be accepted; R2 s post rebuttal comments give some specific, constructive, concrete suggestions for preparing a revision.
This paper introduces a modified GAN architecture that looks a lot like a mixture of experts, to address the problem of learning multiple disconnected manifolds.  They show this method helps on 2D toy experiments, and artificial tasks where different datasets are combined, but not on CIFAR.  They also introduced a new variant of FID that they claim is more sensitive to the improvements made by their model.  R2 didn t seem to think too hard about the paper, and R3 seemed a bit dismissive.  Overall the idea seems sensible but the particulars of this approach aren t all that well motivated in my opinion, especially since the cost of the generator is increased.  Why not just use a mixture of Gaussians in the original untransformed space?  I also found the toy experiments unconvincing, particularly the claim that a standard GAN couldn t learn a mixture of 3 Gaussians.  Learning a mixture of 8 Gaussians was one of the results in the unrolled GAN paper, for instance.  The results on the mixed datasets experiments seem encouraging, but I m afraid that proposing a new GAN architecture in 2019 requires even more baselines than the authors compared against, and the fact that the task was artificially constructed undercuts its importance.
The authors focus on low resource text classifications tasks augmented with "rationales". They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights.  Although the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time. Generally, the reviewers  reservations were in terms of novelty and extent of technical contribution.  Given the large number of submissions this year, I am recommending rejection for this paper. 
The authors present a neural framework for learning SAT solvers that takes the form of probabilistic inference. The whole process consists of propagation, decimation and prediction steps, so unlike other prior work like Neurosat that learns to predict sat/unsat and only through this binary signal,  this work presents a more modular approach, which is learned via energy minimization and it aims at predicting assignments (the assignments are soft which give rise to a differentiable loss). On the other hand, at test time the method returns the first soft assignment whose hard version (obtained by thresholding) satisfies the formula.  Reviewers found this to be an interesting submission, however there were some concerns regarding (among others) comparison to previous work.    Overall, this submission has generated a lot of discussion among the reviewers (also regarding how this model actually operates)  and it is currently borderline without a strong champion. Due to the concerns raised and the limited space in the conference s program, unfortunately I cannot recommend this work for acceptance. 
The authors study dropout for matrix sensing and deep learning, and show that dropout induces a data dependent regularizer in both cases. In both cases, dropout controls quantities that yield generalization bounds.   Reviewers raised several concerns, and several of these were vehemently rebutted. The rhetoric of the back and forth slid into unfortunate territory, in my opinion, and I d prefer not to see this sort of thing happen. On the one hand, I can sympathize with the reviewers trying to argue that (un)related work is not related work. On the other hand, it s best to be generous, or you run into this sort of mess.  In the end, even the expert reviewers were unswayed. I suspect the next version of this paper may land more smoothly.  While many of the technical issues are rebutted, one that caught my attention pertained to the empirical work. Reviewer #4 noticed that the empirical evaluations do not meet the sample complexity requirements for the bounds to be valid (nevermind loose). The response suggests this is simply a fact of making the bounds looser, but I suspect it may also change their form in this regime, potentially erasing the empirical findings. I suggest the authors carefully consider whether all assumptions are met, and relay this more carefully to readers.
Issues raised by the reviewers have been addressed by the authors, and thus I suggest the acceptance of this paper.
The submission proposes a new GAN based method for translating from semantic maps of (synthetic) images/videos (from computer graphics) to photo realistic images/videos with the aid of edge maps. The main innovation is the inclusion of edge maps to the generator, where the edge maps are initially computed using the spatial Laplacian operator, and later output from their DNED network. According to the authors, the edge map allows them to generate images with fine details and to generate output images at higher resolutions.  The authors use their method to generate both single images as well as videos.   The submission received relatively low scores (2 rejects and 1 weak reject).  This was unchanged after the rebuttal (the authors did not submit a revised version of their paper).  The reviewers voiced concerns about the following: 1. Limited novelty All of the reviewers indicated that they felt the novelty of the proposed approach of not high as the work seemed to make only small modifications on prior work.  In the author response, the authors provided some details on where they felt their innovation to be.  The paper can be improved by building on those and having experiments/examples to probe those claims in more detail.   2. Application to other datasets The proposed method is demonstrated only on two datasets of driving scenarios (Cityscapes and Synthia).  It is unclear how the method will generalize to other types of inputs.  Experiments on other datasets will demonstrate whether the proposed approach can work well for other types of images.  3. The overall quality of the writing.   The overall quality of the writing is poor and hard to follow in places. The paper should also include more discussion of domain adaptation in the related work section.  It s possible that with improved writing that situates the work and explains the novel aspects of the work better, that the concern about limited novelty will be partially alleviated.   The paper also needs an editing pass as there are many grammar/spelling/capitalization issues.  Page 2: "We make Three"  > "We make three" Page 3: "as can bee seen in fig 3.2"  > "as can be seen in ..." (it s unclear which figure "fig 3.2" refers to, as figures are labeled Figure 1, Figure 2, etc) Page 4: Equation (3), symbol e is not explained (it is presumably the edge map) Page 7: "bellow"  > "below"  Overall, there are interesting elements in this paper and the reviewers noted that the generated results look good.  However, the paper will need to be improved considerably.   The authors are encouraged to improve their work and submit to an appropriate venue. 
The paper proposed a new metric to define the quality of optimizers as a weighted average of the scores reached after a certain number of hyperparameters have been tested.  While reviewers (and myself) understood the need to better be able to compare optimizers, they failed to be convinced by the proposed solutions. In particular (setting aside several complaints of the reviewers with which I disagree), by defining a very versatile metric, this paper lacks a strong conclusion as the ranking of optimizers would clearly depend on the instantiation of that metric.  Although that is to be expected, by the very behaviour of these optimizers, it makes it unclear what the added value of the metric is. As one reviewer pointed out,  all the points made could have been similarly made with other, more common plots.  Ultimately, it wasn t clear to me what the paper was trying to achieve beyond defining a mathematical formula encompassing all "standard" evaluation metric, which I unfortunately see of limited value.
This paper provides a data driven approach that learns to improve the accuracy of numerical solvers. It solves an important problem and provides some promising direction. However, the presented paper is not novel in terms of ML methodology. The presentation can be significantly improved for ML audience (e.g., it would be preferred to explicitly state the problem setting in the beginning of Section 3).
This is an interesting paper that aims to redefine generalization based on the difference between the training error and the inference error (measured on the empirical sample set), rather than the test error. The authors propose to improve generalization in image classification by augmenting the input with encodings of the image using a source code, and learn this encoding using the compression distance, an approximation of the Kolmogorov complexity. They show that training in this fashion leads to performance that is more robust to corruption and adversarial perturbations that exist in the empirical sample set.   Reviewers agree on the importance of this topic and the novelty of the approach, but there continue to exist sharp disagreement in the ratings. Most have concerns about the formalism and clarity in the presentation. Especially given that the paper is 10 pages, it should be evaluated against a more rigorous standard, which doesn t appear to be met. I encourage the authors to consider a rewrite with a goal towards clarity for a more general ML audience and resubmit for a future conference. 
This paper proposes to learn a visual tracking network for an object detection loss as well as the ordinary tracking objective for enhancing the reliability of the tracking network.  The reviewers were unanimous in their opinion that the paper should not be accepted to ICLR in its current form.  A main concern is that the proposed method shows improvement over a relatively weak base system.  Although the author response proposed to include additional analysis, but the reviewers felt that without the additional analysis already included it was not possible to change the overall review score.
This paper proposes a parametrisation of Euclidean distance matrices amenable to be used within a differentiable generative model. The resulting model is used in a WGAN architecture and demonstrated empirically in the generation of molecular structures.   Reviewers were positive about the motivation from a specific application area (generation of molecular structures). However, they raised some concerns about the actual significance of the approach. The AC shares these concerns; the methodology essentially amounts to constraining the output of a neural network to be symmetric and positive semidefinite, which is in turn equivalent to producing a non negative diagonal matrix (corresponding to the eigenvalues). As a result, the AC recommends rejection, and encourages the authors to include simple baselines in the next iteration. 
This paper tackles the problem of learning off policy in the contextual bandit problem, more specifically when the available data is deficient (in the sense that it does not allow to build reasonable counterfactual estimators). To address this, the authors introduce three strategies: 1) restricting the action space; 2) imputing missing rewards when lacking data; 3) restricting the policy space to policies with "enough" data. All three approaches are analyzed (statistical and computational properties) and evaluated empirically. Restricting the policy space appears to be particularly effective in practice.  Although the problem being solved is very relevant,  it is not clear how this work is positioned with respect to approaches solving similar problems in RL. For example, Batch constrained Q learning ([1]) restricts action space, while Bootstrapping Error Accumulation ([2]) and SPIBB ([3]) restrict the policy class in batch RL. A comparison with these techniques in the contextual bandit settings, in addition to recent state of the art off policy bandit approaches (Liu et al. (2019), Xie et al. (2019)) is lacking. Moreover, given the newly added results (DR method by Tang et al. (2019)), it is not clear how the proposed approach improves over existing techniques. This should be clarified. I therefore recommend to reject this paper.  
This paper proposes a method called iterative proportional clipping (IPC) for generating adversarial audio examples that are imperceptible to humans. The efficiency of the method is demonstrated by generating adversarial examples to attack the Wav2letter+ model. Overall, the reviewers found the work interesting, but somewhat incremental and analysis of the method and generated samples incomplete, and I’m thus recommending rejection.
The authors tackle the questions of automatic metrics for assessing document similarity and propose the use of Transformed based language models as a critic providing scores to samples. As a note, ideas like these have been also adopted in Computer Vision with the use of the Inception score as a proxy the quality of generated images. The authors ask great questions in the paper and they clearly tackle a very important problem, that of automatic measures for assessing text quality. While their first indications are not negative, this paper lacks the rigor and depth of experiments of a conference paper that would convince the research community to abandon BLEU and ROUGE in lieu of some other metric. It s perhaps a good workshop paper or a short paper at a *CL conference. Specifically, we would need more tasks where BLEU/ROUGE is the standard measure and so how the proposed measure correlates better with humans,  so cases where word overlap is in theory a good proxy of similarity assuming reference sentence (e.g., logical entailment is not such a prototypical task). MT is a first step towards that, but summarization is also a necessary I would say. Other questions of interest relate to the type of LM (does it only need to be Roberta?) and the quality of LM (what if i badly tune my LM?)  On a more personal note: We all know that BLEU is not a good metric (especially for document level judgements) and every now and then there have been proposals to replace BLEU that do correlate better (e.g., http://ccc.inaoep.mx/~villasen/bib/Regression%20for%20machine%20translation%20evaluation.pdf) . However, BLEU is still here due to each simplicity. Please keep pushing this research and I’m looking forward to seeing more experimental evidence.
This paper proposes a new black box adversarial attack based on tiling and evolution strategies. While the experimental results look promising, the main concern of the reviewers is the novelty of the proposed algorithm, and many things need to be improved in terms of clarity and experiments. The paper does not gather sufficient support from the reviewers even after author response. I encourage the authors to improve this paper and resubmit to future conference.
This paper presents a case study of training a video classifier and subsequently analyzing the features to reduce reliance on spurious artifacts. The supervised learning task is zebrafish bout classification which is relevant for biological experiments. The paper analyzed the image support for the learned neural net features using a previously developed technique called Deep Taylor Decomposition. This analysis showed that the CNNs when applied to the raw video were relying on artifacts of the data collection process, which spuriously increased classification accuracies by a "clever Hans" mechanism. By identifying and removing these artifacts, a retrained CNN classifier was able to outperform an older SVM classifier. More importantly, the analysis of the network features enabled the researchers to isolate which parts of the zebrafish motion were relevant for the classification.  The reviewers found the paper to be well written and the experiments to be well designed. The reviewers suggested a some changes to the phrasing in the document, which the authors adopted. In response to the reviewers, the authors also clarified their use of ImageNet for pre training and examined alternative approaches for building saliency maps.  This paper should be published as the reviewers found the paper to be a good case study of how model interpretability can be useful in practice. 
This paper presents a mechanism for capsule networks to defend against adversarial examples, and a new attack, the reconstruction attack. The differing success of this attacks on capsnets and convnets is used to argue that capsnets find features that are more similar to what humans use.  Reviewers generally like the paper, but took instance with the strength of the claim (about the usefulness of the examples) and argued that the paper might not be as novel as it claims.  Still, this seems like a valuable contribution that should be published.
There has been a long discussion on the paper, especially between the authors and the 2nd reviewer. While the authors  comments and paper modifications have improved the paper, the overall opinion on this paper is that it is below par in its current form. The main issue is that the significance of the results is insufficiently clear.  While the sender receiver game introduced is interesting, a more thorough investigation would improve the paper a lot (for example, by looking if theoretical statements can be made).
The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance. The paper provides a novel and interesting conceptual contribution and is well written. Experiments could have been more extensive but this is very nice work and deserves publication.
The paper proposes a new way to learn a disentangled representation by embedding the latent representation z into an explicit learnt orthogonal basis M. While the paper proposes an interesting new approach to disentangling, the reviewers agreed that it would benefit from further work in order to be accepted. In particular, after an extensive discussion it was still not clear whether the assumptions of Theorem 1 applied to VAEs, and whether Theorem 1 was necessary at all. In terms of experimental results, the discussions revealed that the method used supervision during training, while the baselines in the paper are all unsupervised. The authors are encouraged to add supervised baselines in the next iteration of the manuscript. For these reasons I recommend rejection.
This paper presents a novel RNN algorithm based on unfolding a reweighted L1 L1 minimization problem. Authors derive the generalization error bound which is tighter than existing methods.  All reviewers appreciate the theoretical contributions of the paper, particularly the derivation of generalization error bounds. However, at a higher level, the overall idea is incremental because RNN by unfolding L1 L1 minimization problem (Le+,2019) and reweighted L1 minimization (Candes+,2008) are both known techniques. The proposed method is essentially a simple combination of them and therefore the result seems somewhat obvious. Also, I agree with reviewers that some experiments are not deep enough to support the theory. For example, for over parameterization (large model parameters) issue, one can compare the models with the same number of parameters and observe how they generalize.  Overall, this is the very borderline paper that provides a good theoretical contribution with limited conceptual novelty and empirical evidences. As a conclusion, I decided to recommend rejection but could be accepted if there is a room. 
This paper proposes a novel approach, Latent Question Reformulation Network (LQR net), a multi hop and parallel attentive network designed for question answering tasks that require multi hop reasoning capabilities. Experimental results on the HotPotQA dataset achieve competitive results and outperform the top system in terms of exact match and F1 scores. However, reviewers note the limited setting of the experiments on the unrealistic, closed domain setting of this dataset and suggested experimenting with other data (such as complex WebQuesitons). Reviewers were also concerned about the scalability of the system due to the significant amount of computations. They also noted several previous studies were not included in the paper. Authors acknowledged and made changes according to these suggestions. They also included experiments only on the open domain subset of the HotPotQA in their rebuttal, unfortunately the results are not as good as before. Hence, I suggest rejecting this paper.
This is an observational work with experiments for comparing iterative pruning methods.  I agree with the main concerns of all reviewers:  (a) Experimental setups are of too small scale or with easy datasets, so hard to believe they would generalize for other settings, e.g., large scale residual networks. This aspect is very important as this is an observational paper. (b) The main take home contribution/message is weak considering the high standard of ICLR.  Hence, I recommend rejection.   I would encourage the authors to consider the above concerns as it could yield a valuable contribution.
This works relates adversarial robustness and Lipschitz constant regularization. After the rebuttal period reviewers still had some concerns. In particular it was felt that Theorem 1 could likely be deduced from known results in optimal transport, and it would be nice to make this connection explicit. There were still concerns about scalability. The authors are encouraged to continue with this work, considering the above points in future revisions. 
The paper presents a novel reinforcement learning based algorithm for contextual sequence generation. Specifically, the paper presents experimental results on the application of the gradient ARSM estimator of Yin et al. (2019) to challenging structured prediction problems (neural program synthesis and image captioning). The method consists in performing correlated Monte Carlo rollouts starting from each token in the generated sequence, and using the multiple rollouts to reduce gradient variance. Numerical experiments are presented with promising performance.   Reviewers were in agreement that this is a non trivial extension of previous work with broad potential application. Some concerns about better framing of contributions were mostly resolved during the author rebuttal phase. Therefore, the AC recommends publication. 
This paper studies the decision boundaries of a certain class of neural networks (piecewise linear, non linear activation functions) using tropical geometry, a subfield of algebraic geometry that leverages piece wise linear structures.  Building on earlier work, such piecewise linear networks are shown to be represented as a tropical rational function. This characterisation is used to explain different phenomena of neural network training, such as the  lottery ticket hypothesis , network pruning, and adversarial attacks.  This paper received mixed reviews, owing to its very specialized area. Whereas R1 championed the submission  for its technical novelty, the other reviewers felt the current exposition is too inaccessible and some application areas are not properly addressed. The AC shares these concerns, recommends rejection and strongly encourages the authors to address the reviewers concerns in the next iteration.  
This paper proposes a novel methodology for applying convolutional networks to spherical data through a graph based discretization.   The reviewers all found the methodology sensible and the experiments convincing.  A common concern of the reviewers was the amount of novelty in the approach, as in it involves the combination of established methods, but ultimately they found that the empirical performance compared to baselines outweighed this.
This paper proposes to use hypernetwork to prevent catastrophic forgetting. Overall, the paper is well written, well motivated, and the idea is novel. Experimentally, the proposed approach achieves SOTA on various (well chosen) standard CL benchmarks (notably P MNIST for CL, Split MNIST) and also does reasonably well on Split CIFAR 10/100 benchmark. The authors are suggested to investigate alternative penalties in the rehearsal objective, and also add comparison with methods like HAT and PackNet.
The paper presents a linear classifier based on a concatenation of two types of features for protein function prediction. The two features are constructed using methods from previous papers, based on peptide sequence and protein protein interactions.   All the reviewers agree that the problem is an important one, but the paper as it is presented does not provide any methodological advance, and weak empirical evidence of better protein function prediction. Therefore the paper would require a major revision before being suitable for ICLR. 
This paper proposes a method to improve the training of sparse network by ensuring the gradient is preserved at initialization. The reviewers found that the approach was well motivated and well explained. The experimental evaluation considers challenging benchmarks such as Imagenet and includes strong baselines. 
The paper proposed a parameterized convolution layer using predefined filterbanks. It has the benefit of less parameters to optimize and better interpretability. The original submission failed to inlcude many related work into the discussion which was addressed during the rebutal.  The main concerns for this paper is the limited novelty and insufficient experimental validation and comprisons:  * There have been existing work using sinc parameterized filters, learnable Gammatones etc, which are very similar to the proposed method. Also in the rebutal, the authors acknowledged that "We did not claim that cosine modulation was the novelty in our paper" and it is "just a way of simplifying implementation and dealing with real values instead of complex ones" and "addressing the question of convergence of parametric filter banks to perceptual scale". * Although the authors addressed the missing related work problem by including them into discussions, the expeirmental sections need more work to include comparisons to those methods and also more validations on difference datasets to address the concern on the generalization of the proposed method. 