This work mainly applies wav2vec 2.0 to multilingual speech recognition and lacks of novelty.  The various pre training and fine tuning mix match are specific to the speech recognition task. As suggested by reviewers, it is recommended to resubmit to a speech conference.  Also the paper lacks comparisons to SOTA on one of the well studied task (i.e. BABEL) in the speech field.  The main factor for the decision is lack of novelty.
This paper proposes to learn symmetries of a physical system jointly with its Hamiltonian from data by learning a canonical transformation that render some of the coordinates constant. The Hamiltonian dynamics and "canonical" transformation are softly enforced via loss terms. A few experiments are performed demonstrating that the idea works and can learn a few approximate invariants, as well as some improvements over baselines agnostic to the symmetries. The idea is interesting, but the experiments are limited in scope. It is not clear how to extend this idea to more complex systems where we do not know the number of conserved quantities in advance. It is also not clear how good are the learned invariants, as the results showing errors in conserved quantities (Fig 3) suggest that it is not very precise beyond a few time steps. 
This work combines normalizing flows with conditional sampling. While there are connections to other works, the paper seems novel and applicable, and has nice experimental results. The authors did a good job clarifying the reviewers questions, and have addressed their major concerns. We appreciate the additional analyses added to the paper.
Summary of reviews and discussions: Reviewers were overwhelmingly negative on this paper due to a variety of factors: unclear writing, heuristic motivation, overpromising in the title while underdelivering on results. Although the authors responded to the reviewers  feedback, and some reviewers increased their score to acknowledge the author s work, they remained unconvinced overall and no reviewer argued strongly for acceptance.
This paper develops an interesting new angle on the behavior of large width neural networks by elucidating the connection between the NNGP and noisy gradient descent and by examining finite width corrections through an Edgeworth expansion. While these contributions are important, the paper would better serve the community if its presentation were significantly improved before publication. The main issue is not one of presentation style   papers with physics style prose are welcomed and appreciated at ICLR   but rather one of presentation substance. In addition to the various specific points raised by the reviewers, I would add that the figures and captions are difficult to interpret, the experiments need a more in depth discussion, and the notations should all be defined at the time of their introduction, among other things. For these reasons, I cannot recommend accepting the paper in its current form, but I hope to see a more polished version of the manuscript at a subsequent conference.
This paper presents new analysis for self supervised learning. All reviewers are positive about some new perspectives of the analysis. However, some serious concerns have been raised about the rigorousness and the presentation clarity. The paper would be significantly improved, if the authors could address the concerns.
This is a nice paper using contrastive learning for code representation. The idea is to generate variations on unlabeled source code (using domain knowledge) by creating equivalent version of code. Improvements over baselines on two multiple tasks are shown. While some of the reviewers liked the (and R4 should have responded), none of the reviewers found the paper exciting enough to strongly recommend its acceptance. 
This paper describes a clever new class of piecewise linear RNNs that contains a long time scale memory subsystem. The reviewers found the paper interesting and valuable, and I agree. The four submitted reviews were unanimous in their vote to accept. The theoretical insights and empirical results are impactful and would be suitable for spotlight presentation.
This paper proposes an approach to learn the causal structure underlying a dataset with acyclicity and other structure constraints, and then used the inferred structure to compute partial causal effects. The authors show that, on simulated data, the proposed method outperforms others in the literature. The manuscript also contains an analysis of real world data that describes the causal effects of the lockdown of cities in the Hubei province (China) to reduce the spread of COVID 19.   Overall, the reviewers think that this is a well structured and written paper. From a novelty viewpoint, the main contribution consists in formalising the causal contribution of mediators, as the method for computing the causal structure is based on a small modification to previous literature.  The main concern raised by the reviewers were on the experimental evaluation. Some of these concerns were addressed by the authors during rebuttal, whilst some on the number of nodes remained. We encourage the authors to consider these concerns in the final version of the manuscript.
This paper has some interesting ideas and is an incremental improvement over previous work. However, it needs further revisions and polishing. The relation to prior work is a bit unclear. Since you mention POMDPs, what would be an equivalent version of your method in POMDPS? Why not compare your algorithm with a state of the art method for small discrete problems? It is also a bit unclear why training a model to predict beliefs would be faster than just calculating them (after all the data must come from somewhere)..
As several reviewers pointed out, the contribution is  too incremental from previous work.
This paper studies the behavior of SGD for linear models fit with the squared Euclidean loss. There are three main results:  The first result (Sec. 4) studies the behavior where instead of regularizing the objective, Gaussian noise is added to the inputs. The main result is a sufficient condition for how the learning rate and noise can jointly change over time in order for SGD on the MSE error with noisy input to asymptotically converge to the same solution as regular gradient descent without noisy input.  The second result (Sec. 5) is slightly more general in that is considers the case where the noise can be an isotropic Gaussian where the variance changes over time. Again, a result is given for how the learning rate interacts with the data in order to asymptotically converge to the unregularized solution. This is first studied in Thm 5.1 then assuming power law decay in the noise in Thm. 5.2. It should be emphasized that though these are asymptotic guarantees, the results give asymptotic *rates* of convergence. In my opinion this is a significant strength of the results that was not emphasized by the reviewers.  The third result (Sec. 6) studies SGD for least squares linear models where the stochasticity is due to data subsampling only. The fraction of subsampled data may change over time.  The primary sentiment from reviewers was that the mathematical complexity of the paper meant that they could not understand it or give a fair review. (More on this below.) For this reason, and because the overall reviews are somewhat borderline, I read the paper in detail. A specific concern raised by two reviewers was that the paper first presents a very general framework but then studies very restricted specific problems. Some reviewers felt that the paper was very well written, while others felt it was poorly written. There are no experiments.  For my part, I mostly concur that the paper is well written (albeit quite technical). However, I agree with the concern from reviewers that the technical results all concern extremely restricted settings, and it s not clear what value the extremely general setup brings. I also find the title of the paper a bit puzzling. For specifics of the results, the practical value of Sections 4 and 5 is unclear. It s well known that adding data noise is exactly equivalent to adding ridge regularization when doing linear regression. But ridge regularized linear regression would be a non stochastic problem. So what is the value of studying the convergence rates in this case? The paper never makes this clear.  I have concerns about the exponential convergence rate in Thm. 6.1. The paper claims that an exponential convergence rate for SGD has been extensively studied. I do not believe this is true. In general SGD does not have an exponential convergence rate. There are modified methods like SAG that achieve this on finite data sets, but that s not what s studied here. The paper cites two papers: The first is Bottou et al. (2018). This is a lengthy review, with no specific reference given. I am familiar with it and also spent time searching but could not find a specific result. Ma et al. (2018) is also cited. This indeed gives an exponential convergence rate but assuming that at the optimum the loss for all datapoints is zero! No such assumption is made in the submitted paper, and the issue is not further discussed. This is cause for grave concern.
Reading the paper and the reviews themselves, I found myself conflicted about this work:    Multiple reviewers commented that this is a rather incremental piece of work, given that it s a rather straightforward combination of existing losses/models.   On the other hand, there is admittedly value in (1) realizing that this combination is meaningful (2) understanding the meaningful ways in which these work or do not work with ablation studies.   I am not quite satisfied that the datasets and experiments in this work represent in any meaningful way real world noise. However, it does appear that the authors ran experiments on common benchmarks using common protocols so there s only so much that they themselves can be blamed for.   Tangentially, I am somewhat surprised about the relatively good ImageNet performance of this method. I suspect the combination of this being done with uniform noise rather than structured noise is helping quite a bit.  All in all, this work is certainly interesting enough, but the results are just not quite compelling enough to pass the bar.
This paper proposed an ensemble of diverse models as a mechanism to protect models from theft.  The idea is quite novel. There are some concerns regarding the robustness of the hashing function (that I share), however not every paper has to be perfect, especially when it introduces a novel setup.   AC
Reviewers have different views on the paper and after going through the reviews, replies and the papers, we believe that there is room for improvement here.   While the part related to  indefinite symmetric kernels, and general similarity functions seems to be well covered, as well as the part on Transformers, the relation with learning in RKBS and Transformer is far from being clear and Reviewer 4 makes a strong point on this. For instance,   * what is the goal of the section 5 and Definition 1 . Indeed it is not clear here if the point of the authors is to learn the kernel parameters in equation 9 or to learn to predict the output of a transformer. If it is the latter, the connection with the first part is unclear.  * In Equation 11, I can understand that x and y are the sequences t and s but what is z_ij and how it is obtained? So again, the learning problem drops in without justification and it is not explained how it can be solved. The theoretical results involving the representer theorem is nice though.  * The experiment does not seem very related to the learning problem in Equation 11 introduced by the authors.it seems to me that they are just trying different kernels on top of the dot product. 
The main contribution of the paper is a novel parametrization of normalizing flows using ideas from optimal transport theory. This new parametrization allows viewing a normalizing flow as the gradient of a convex function, which leads to an efficient method for gradient estimation for likelihood estimation, using only access to convex optimization solvers. The paper is overall well written and provides a clean analysis. Theoretical results from the paper are supported by experiments. The paper was overall viewed favorably by the reviewers. 
This paper attempts to jointly search for the sensor and the neural network architecture. More specifically, the proposed approach jointly optimizes the parameters governing the PhlatCam sensor and the backend CNN model. In terms of the approach, the paper follows a well known DARTS formulation for the differentiable architecture search. A very straightforward solution was proposed for the problem.  Although all the reviewers place that the paper is marginally above the acceptance threshold, none of them strongly support the paper and the reviewers point out that the paper is limited in terms of the setting and data. The problem formulation of the paper itself is interesting, but the AC agrees with the reviewers that the paper is limited and lacks enough technical contributions to warrant the acceptance to ICLR. 
The paper addresses generalization to compositions of rare and unseen sequences. It proposes an unstructured data augmentation, that achieves comparable generalization to structured approaches (e.g. using grammars). The idea is based on recombining prototypes and oversampling  in the tail.   The paper provides a novel approach to an important problem. All four reviewers recommended accept.   
This paper applies multi armed bandits to tuning deep learning code optimization. All reviewers agreed that this is an exploratory paper that opens up a new research area. My main criticism is algorithmic. In particular, the paper applies a 20 year old algorithm to a problem with a small number of arms. It is definitely not as impressive as  https://papers.nips.cc/paper/2018/file/f33ba15effa5c10e873bf3842afb46a6 Paper.pdf  who studied a different (but related) problem. The tuning problem in this paper also seems non stochastic and contextual, while the authors apply a stochastic non contextual bandit algorithm.  I shared these concerns with the reviewers, who insisted that the application is important enough to justify the acceptance of the paper. I respect their opinion and therefore suggest an acceptance. I encourage the authors to take my comments into account when revising the paper.
This submission tackles an important problem and presents interesting ideas. I am confident that the research will lead to good publications. However, in the particular situation here, AnonReviewer2 had serious concerns that are shared by me. The authors made a great effort to clarify the situation, but the current situation still leaves me uncertain about the presentation and correctness of everything. Because some issues were major, it is not easy to re evaluate and take new conclusions in the short time of this process. I hope the authors do not take this too negatively, but given all the comments and discussions, it is best that another round of improvements and reviews be conducted.
Thanks for your submission to ICLR.  When the initial reviews were written, three of the four reviewers were positive about the paper.  Everyone felt it was overall a solid contribution, but there were some concerns about the clarity and presentation, as well as some suggestions for additional experiments.  During the rebuttal/response period, the authors did a very nice job in responding to the concerns of the reviewers.  Ultimately, all of the reviews were in agreement after discussion that the paper is strong and ready for publication.   I also like this paper a lot, and find it to be a nice way to combine LSH with NN training.  I am happy to recommend this paper for publication.
This work presents a practical unsupervised pretraining strategy that does not require layer wise training stages. Clearly this is an area that has lot of potential and the work seems to head in the right direction.  However, despite a very positive review, I share the same concerns raised by the remaining 3 reviewers. Better motivation and clarity is needed and, considering the proposed approach, a much more thorough comparison and analysis of the theoretical advantages and guarantees of such an approach. A main argument is that the method can handle arbitrary networks due to the way it is implemented, it is however not clear how practical that would be and how that would work in presence of non linearities.  Experiments require also additional work to be presented with clear and standard baselines, not by presenting SOTA on arbitrary tasks. This seemed to be a main concern of the reviewers.
The reviewers overall appreciated the efforts of the authors in making NAS more computationally efficient. The paper could greatly benefit from further editing/restructuring with the goal of improving clarity, as it’s currently hard to navigate and understand in places. Future submissions of this work would benefit from more extensive empirical validation that motif networks mimic the original network. The reviewers also agreed that for the method to be appealing/useful, a general way to generate motif networks is needed. Overall, the outcome was that this is a very interesting idea but needs further development along the directions outlined above.
This paper proposes a tuning strategy for Hamiltonian Monte Carlo (HMC). The proposed algorithm optimizes a modified variational objective over the T step distribution of an HMC chain. The proposed scheme is evaluated experimentally.  All of the reviewers agreed that this is an important problem and that the proposed methods is promising. Unfortunately, reviewers had reservations about the empirical evaluation and the theoretical properties of the scheme. Because the evaluation of the scheme is primarily empirical, I cannot recommend acceptance of the paper in its current form.  I agree with the following specific reviewer concerns. The proposed method does not come with any particular guarantees, and particularly no guarantees regarding the effect of dropping the entropy term and using an SKSD training scheme to compensate. While guarantees are not necessary for publication, the paper should make up for this with comprehensive and convincing experiments. I agree with R1 that more careful ablation studies on toy models are needed, if nothing else to reveal the strengths and weaknesses of the proposed approach. I would also recommend a more careful discussion about the computational cost of this method and how it can be fairly compared to baselines. I don t agree that "deliberately wasteful" experiments reveal much, especially if running more realistic experiments reduces the relative impact of the proposed method.
A method is proposed for removing prior knowledge, presented as a distance matrix, from low dimensional embeddings, to focus them on what is new.  The task of visualizing novely in data is interesting and good solutions would potentially be highly useful.  The proposed method essentially substracts a distance matrix from another. While this is sensible, it is not completely clear in what sense this is _the_ right solution for what the embeddings will be used for.  In final discussions among the reviews, the main remaining concerns were considered severe: comparisons to other methods being limited, and possible problems in one of the experiments. 
This paper considers the problem of black box optimization over categorical variables using expensive function evaluations.    Fourier representation is proposed as surrogate model by treating the categorical input as the direct sum of cyclic groups. The parameters are learned using exponentially weighted update algorithm.   To select the inputs for evaluation, simulated annealing and MCTS are employed as search algorithms to optimize the learned surrogate function.    Experiments are performed on two synthetic problems and RNA sequence design problems.  The proposed fourier representation is novel and the results show the promise of this method in terms of computational efficiency over state of the art COMBO method.  There are two unsatisfactory aspects of this paper. 1. In expensive black box optimization problems, number of function evaluations to find better solutions is critical. This paper takes a non Bayesian approach to improve computational efficiency (over prior Bayesian optimization methods), but this same advantage comes at the expense of sample efficiency (number of function evaluations) due to lack of exploration.  2. In fourier representation, mapping categorical values to different group elements may change which basis are used for modelling. From a practitioner s perspective, it is important to verify that the performance is not significantly affected by this choice. This can be verified empirically. Even though one reviewer raised this point, authors  haven t responded though it is an easy experiment to do.  Due to the above shortcomings, the paper is judged to be not ready for publication at the current stage. I strongly encourage to resubmit the paper after addressing the above two concerns.
This paper uses deep kernel learning to develop a compelling framework for hyperparameter optimization in a few shot setting, with empirically strong results. Please carefully account for all reviewer comments in the final version. 
The paper got a quite high disagreement in the scores from the reviewers. R2 voted for rejecting the paper as he did not see the connection of the algorithm to the continuation method and also that the continuation method does not address the distributional shift, which is one of the main problems for offlline RL. Yet, these concerns have been properly answered in the rebuttal of the authors and the distributional shift is also addressed by the continuation method by reducing the error in policy evaluation. Further concerns from the reviewers were raised in terms of related work to a similar algorithm (BRAC), which is also addressed in the revision of the paper.   The reviewers also identified the following strong points of the paper:   The algorithm is a simple and very effective adaptation to SAC    The presented results are exhaustive and convincing   The paper provides strong theoretical results for the presented algorithm   The authors did a very good job with their revision, adding more comparisons and ablation studies.  I agree that this paper very interesting and recommend acceptance.
This paper proposes a unified cross lingual pretraining method that works well for both natural language understanding (NLU)—typically done using encoder only architectures like mBERT and XLM—and conditional natural language generation (NLG) tasks like machine translation—typically done using encoder decoder architectures like mBART.  This paper clearly split reviewers, with 2 quite or very positive on it, and 3 thinking or leaning towards thinking that it didn t have enough novelty to merit publication.  Pro   The model produces good SoTA results   The method is easily replicable   It is good for the community for leading systems on benchmark tasks to have published papers describing how they work.  Con    The work is not groundbreaking in technical novelty   The work has to do a better job of communicating its contributions: It s hard to understand how it differs from other methods  On balance, the overall assessment is that the paper is not yet ready in its current form. The hope is that authors find the reviewer comments useful for preparing a future submission:    The paper **has** to do a better job of communicating its contributions. All that most researchers got from the first version was that there was parameter sharing and that helped. The revised version starts to do a better job of explaining the value of having the IS MLM and CS MLM objectives to doing well on NLU and NLG tasks, but much more is needed, as the discussion here shows. Indeed, even the discussion here is often opaque. In describing the key contribution of the paper, in both the revised paper and discussion, the authors fall back on phrases like "elaborately designed" and "exquisite cooperation of parameter sharing and pre training tasks". **What do "elaborately designed" and "exquisite cooperation" mean?!?** I think you can minimally clearly explain the benefits of having an objective like IS MLM for doing better on NLU tasks than the approach taken in mBART. You could argue for the advantages of MLM vs LM generation, which has been shown in other papers, including the original BERT paper and ELECTRA. Concretely, I wonder if you should reverse the contents of section 2 and start with equation (8) and explain why that is a good objective for your system, and better than ones that have been used previously. This discussion should be at a higher level than the current discussion under (8) which tends to be in the weeds. I haven t worked all the details, but I think you could then describe the objectives of section 2.2 before describing the implementation in section 2.1, and the result might be clearer? It would certainly emphasize the importance of these loss functions.   The initial version didn t have important details like the number of languages covered in the main paper; the current version fixes this to the extent of saying you have 50, but still doesn t give the context of how this compares with XLM R and mBART. And several reviewers had questions about the number of parameters of different models. I think you could fix a lot of these concerns by moving Table 8 to the main paper in a future resubmission. It doesn t take up much space and helps a lot in providing these details and easy to find citations for the models compared in other papers. 
While the authors thought that the paper had some strong experimental comparisons, there were serious concerns with novelty and paper claims. For a stronger ML paper the authors would need to either: (a) design a new training methodology beyond pre training that is better suited for leveraging multiple datasets for Retrosynthesis, (b) design a new model for Retrosynthesis that is better able to leverage mutliple datasets, (c) design new evaluation metrics to describe how well current methods perform in Retrosynthesis and/or metrics that describe how well methods can use data from different sources. That said, if the authors were interested to submit to non ML venues then I agree with R2 that chemistry venues may be better suited to the paper in its current form. 
Knowledgeable R3 found the paper very good (8). He/she found the authors  responses very informative and that edits made the paper much stronger. R2 expressed reservations about rank collapse being the cause of degradation of performance, but also indicated his/her willingness to increase the score if the authors can convincingly respond to his/her concerns. This concerned was shared by other reviewers, and there was an extensive discussion during the discussion period. R3 and R1 found the authors  responses very convincing. Fairly confident R1 found the paper good, appreciated the discussion, and recommends the paper to be accepted. R4 found the paper marginally above the acceptance threshold, however expressing a lower confidence in his/her assessment. In summary, the article contains extensive experiments, theory, and a well motivated idea, elucidating an intriguing phenomenon and useful for designing better bootstrapping based deep RL methods. Although the reviewers expressed some reservations in their initial reviews, there was a lively discussion with quite positive final feedback. Weighing the ratings by confidence and participation in the discussion, I am recommending the paper for acceptance. I would like to encourage the authors to make efforts in making the presentation as clear as possible, having in mind the discussion and comments from the reviewers. 
This paper begins to formalize a connection between value decomposition and difference rewards. Whilst we are in agreement with the authors that papers do not need to make new algorithmic contribution and purely theoretical papers that deepen our understanding of established methods can be significant contributions, all reviewers had doubts on the maturity of the theoretical contribution of this paper.  Given the concerns raised by the authors for the attention of the area chair, I would like to reassure the authors that the majority of reviewers engaged in discussion after the rebuttal but remained unconvinced of the significance of the theoretical results. As these are representative of the potential audience at ICLR, it is clear further improvements to the motivation given in the paper and/or weakening of the assumptions within the theory are needed to engage the interest of the wider machine learning community.  The empirical studies in the paper also seem disconnected from the theoretical contribution and more like a continuation of the paper "Qplex: Duplex dueling multi agent q learning." Given the theoretical connection to difference rewards (e.g. COMA as explicitly noted by the authors in Implication 1) I would expect these methods to be included in the experiments to demonstrate how this theoretical connection affects performance in practical applications.
This paper introduces a conditional discrete VAE for uncertainty estimation on high dimensional data. Reviewers found the paper borderline, and two of the three reviewers stated it doesn t meet the acceptance bar due to lack of clarity in several aspects and limited technical novelty.
The reviewers are split.  Two reviewers consider the technical contribution of the paper to be insufficient, and raise concerns about comparisons with Transformers or using more standard benchmarks for GNN experiments.   The other considers the experiments convincing and the method worth publishing.   My own view is that this work is not ready for inclusion in the conference.  In particular, I think this paper would be much stronger with either:   1: a more practical task to illustrate where this method might be applied in earnest, 2: more analysis and baselines on the synthetic data.  Synthetic data can be enough for a new method if it illuminates the functioning and the benefits and drawbacks.  In this paper, we have synthetic data with little analysis, and imo (concurring with R5) insufficient baselines.  For example, while a vanilla Transformer probably could not do the matrix problems (with the matrices encoded naively), one might expect Transformers with sparse attention to do quite well on e.g. transpose and 90 degree rotation, especially given the training curriculum and proper positional embeddings; a convolutional network seems like a strawman.  I also agree with R5 that standard benchmarks for GNN exist, and these might be appropriate (or at least there should be some discussion of why they are not).  3: some theoretical discussion of what the proposed model can do that other methods fundamentally cannot.  I do think this is interesting work, and encourage the authors to revise and resubmit.
This paper presents a semi supervised model (named CPC VAE) that trains a variational autoencoder (VAE) and a NN classifier simultaneously. The method maximizes an ELBO subject to a task specific prediction constraint and a consistency constraint. The constraints are defined as some expectations of the variational posteriors. Such constraints are known as posterior regularization. Though the consistency constraint seems to be new, the prediction constraint has been well examined under deep generative models (see e.g., max margin deep generative models for (semi )supervised learning, IEEE TPAMI, 2018). The paper needs more a thorough analysis and comparison. 
The paper contributes to the community by introducing an approximation to distribution Q functions, based on the epistemic and aleatoric uncertainty. The reviewers believe the ideas make sense. However the presentation and its experiment results make it hard for them to understand some important details. For example, the reviewers are confused about why the empirical results show the proposed methods are better.     The majority of the reviewers are negative about the paper. After rebuttal, the reviewers are not convinced. Based on this, the meta reviewer recommends rejection. Authors can strengthen paper by improving its presentation and addressing the concerns from the reviewers.   
The submission proposes a novel conditional GAN formulation where continuous scalars (named regression labels) are fed into the GAN as a conditioning variable. Since cGANs with discrete labels are trained to minimize the empirical loss, they fail for continuous conditions, because there might be few or even zero samples for many labels values and also the label cannot be embedded by one hot encoding like discrete labels. As a solution, the authors propose new methods of encoding the label.   The paper received a clear accept, two weak accepts and a weak reject. As agreed by all the reviewers, the paper proposes an interesting framework to eliminate some weaknesses of GANs. The rebuttal adequately addresses the reviewer comments and hence the meta reviewer recommends acceptance. 
The paper investigates several properties of adversarial examples obtained by hard label attacks. There are some interesting findings in this paper, such as the connection between query efficiency and distance to the image manifold. However, all the reviewers think the paper is below the acceptance threshold due to several weaknesses, including insufficient experiments, clarity, and whether the observations made in the paper can benefit query efficiency or quality of hard label attacks. 
The authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets. The results are important in the general context of the "strong" lottery ticket hypothesis, and are of both theoretical and practical interest. Although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel.  Some further concerns of clarity and novelty were addressed by the authors.
The paper shows linear convergence for generalized mirror descent on smooth function under the PL assumption. It extends the result to stochastic generalized mirror descent under an additional assumption on the Jacobian of the mirror map. Reviewers pointed out several technical issues with the submission. While some of the problems have since been resolved in the updated version, the paper still lacks sufficient novelty, and some concerns regarding the correctness/clarity of the claims remain. Unfortunately, I can not recommend acceptance at this time. 
The paper presents a novel regularization scheme for showing provable robustness guarantees for the class of ReLU networks. The reviews of this paper were mixed but leaning more to reject. Even though geometric approaches to adversarial robustness are appealing there are too many issues left open (see below for detailed comments) so that the paper is not above the bar for ICLR.  Detailed comments:    incremental resp. small theoretical contribution: the bounds in Theorem 1 are worse than what has been derived in Croce et al (2019), which is admitted only in the Experiments section but this should directly be discussed after Theorem 1, or are based on other s work Moosavi Dezfooli et al(2019)    the proofs are still not very clear even after the update e.g. it is not clear to me what is meant with the distance to the decision boundary d_D(x) as this quantity cannot be computed (this is the whole point here)    the experimental results are in most of the cases worse than prior work (KW and MMR). The authors argue that their results hold for both threat models but then the l_infty/l_2 robustness of the other models should have been provided to make the point. Moreover, if multiple norm ball threat models is the point, then the authors should compare to the follow up work of Croce et al at ICLR 2020 which explicitly optimizes all l_p balls.    the point with gradient obfuscation of MMR does not make sense to me as the gap between upper and lower bounds on the robust error is quite small for MMR in most cases. Gradient obfuscation would mean that the PGD attack fails and thus the lower bound would have to be close to zero.    Figure 1c) looks like the Figure from the book of Boyd and Vandenberghe   if this is the case then it has to be correctly referenced in the caption 
The submission is acknowledged as having potential value in terms of proposing a new approach for exploration based on ensembles and value functions. However, there are lingering concerns about the discussion of what this paper brings to the table vis a vis prior work, together with a lack of clear demonstration of the explicit gains from the exploration mechanism and more experimental studies. The author(s) would do well to revise as per the feedback given and resubmit a version with a more compelling argument. 
The paper considers whether Neural ODEs have a valid interpretation as an ODE, showing that such an interpretation is not correct unless the discretization is chosen properly.  This is important, given interest in Neural ODEs as models as well as they way they will be used, both for problems involving physical/temporal data as well as more generally.  The paper proposes an algorithm for adapting integration step size during training to partially address this issue, and empirical results are shown.  There was a detailed discussion between reviewers and authors which led to improvements.  The authors should also discuss the relationship of their work with https://arxiv.org/abs/2008.02389, which makes a similar point, in the final version.
This paper tackles a problem of resource allocation using reinforcement learning. An important invariant   permutation invariant   is identified as an important characteristic of this problem. Then it is shown that taking advantage of such an invariant should  dramatically improve the sample efficiency.  On behalf of the reviewers, I would like to thank the authors for addressing many concerns raised in the initial reviews. Unfortunately, a further examination revealed several other potential issues that require further clarification:  1. It seems that real data experiments do not really demonstrate whether the benefits of the approach come from multi task learning or from permutation invariance. It would make sense to run an ablation study. In particular, if the benefit is really coming from multi task learning, then the theory part of the paper becomes less relevant.  2. The metric used for finance application appear to be in adequate. It is typical in finance academic literature to look some form of risk adjusted returns. Is the MTL strategy just taking more risk? How statistically significant are the results?  Given these concerns the paper can not be accepted in its current form but we encourage authors to address these and resubmit. 
This work compares and contrasts the learning rate dynamics of GD and SGD and shows that under practical learning rate settings, SGD is biased to approach the minimum along the direction of steepest descent, leading to better performance. Reviewers agree that the theoretical results are significant. The authors satisfactorily responded to reviewers’ questions and improved the paper’s clarity during the discussion phase.
 This paper studies the effect of label smoothing on knowledge distillation. A previous work on this topic (Muller et al.) has claimed that label smoothing can hurt the performance of the student model in knowledge distillation. The rationale behind this argument is that label smoothing erases information encoded in the labels. This work shows that such claimed effect does not necessarily happen. Specifically, by a comprehensive study on image classification, binary neural networks, and neural machine translation, the authors show that label smoothing can be compatible with knowledge distillation. However, they conclude that label smoothing will lose its effectiveness with long tailed distribution and increased number of classes.  Overall ratings of this paper are all on the positive side, and R2 finding this paper an important step toward understanding the interaction between knowledge distillation and label smoothing. I concur with the reviewers about the importance of this research direction and I think this submission provides a reasonable empirical evidence to change our earlier perspectives. I recommend accept.  While the paper specifically studies the effect of label smoothing on knowledge distillation, I think providing a bigger context and reviewing some of the recent demystifying efforts on understanding knowledge distillation could allow paper to communicate with a broader audience. I hope this can be accommodated in the final version. 
The authors propose a process to leverage the memorization effect of deep learning models to filter out examples at the boundary (hard) that the models are confident on, and argue that identifying those hard confident examples help improve the accuracy when learning under noisy data. The process essentially alternates between confident example selection and classifier updating, where the two parts are expected to help each other to form a positive cycle. Experiments demonstrate superior results over other self purifying approaches.  The reviewers have a very diverse opinion about the paper. On the positive side, everyone agrees that the superior experimental results to be very impressive. The authors have addressed some concerns well, such as the running time. One reviewer pointed out that the current study has not been combined with semi supervised learning yet, but during the discussion, most agreed that it is not a crucial negative point of the current paper. On the actual negative points, there are issues that were not cleared even after the rebuttal, such as whether re initialization in the process helps escape local optimal, and the key difference between the "small loss trick" and "memory momentum trick." While the authors argued the novelty with respect to SELF, more illustrations and experiments are needed to highlight the novelty aspect.  Given the diverse opinions, the AC read the paper in detail, and assessed the reviewer s opinions and the authors rebuttal. Overall a serious concern is the leap of faith that the proposed process is indeed (a) "leveraging the memorization effect" to (b) "extract hard confident examples" to (c) "improve accuracy in noisy learning". For (a), it is mentioned that deep learning models "learns simple patterns on majority of data" first, where the majority in this work is argued is the clean ones. But there is no validation of this claim in the experiments. For instance, there is no figure/discussion that shows how much "clean data" has been correctly captured/memorized by the earlier deep learning models. For (b), the terminology of "hard but confident" is ill defined. If examples being hard means them to be around the boundary, one can argue that they could never be "confident" as one measure of the confidence is the margin. The authors may want to mean "hard but clean", but then more illustrations are needed to analyze whether the extracted examples are really the clean ones, or if there are noisy ones being "confident" from the proposed process as well. For (c), it is then unclear whether the improved performance is caused by noise removal (as the authors hope to argue), or by just zooming in to the boundary (regardless of whether the extracted examples are clean or noisy). The authors are encouraged to not just look at the superior performance on accuracy, but analyze more on what actually happened behind the scenes to understand the proposed process better.  Some more comments from the AC that could help the authors (1) Are the examples kept by the proposed approach similar to the ones kept by SELF? Why or why not? Are there empirical studies on this? (2) For the competitors  approaches, what would their Figure 4 look like? In particular, for approaches that use "small loss trick", what would Figure 4 look like? How would Figure 4 be different for the proposed process (i.e extracting hard "confident" examples) and others (like extracting just hard examples without considering confidence, or just confident examples without considering hardness)? (3) Are there studies that deliberately initialize the process with noisy data, to see how sensitive the process is before the "positive cycle" begins? 
This paper explores the use of a texture based foveation stage in scene categorization.  They show that the foveated system shows presevation of high spatial frequency information relative to other matched transformations.  This paper engendered a lot of discussion and had a wide range of ratings.  An extra review was requested that fell intermediate between the high and low scores.  Generally reviewers agree that the question is interesting but that the paper does not clearly elucidate the logic of the paper.  That is, Reviewer 1, and another reviewer in discussion, had issues with the logic of the paper. I also found the motivation behind the paper, difficult to understand at first.  I had to find the Rosenholtz paper to understand why the authors were considering this particular representation. This should be better explained in the paper   the main points of the paper should be understandable by itself.   There is also concern that the claims are not validated by the presented results.  For example, the authors claim that their foveated network were more robust to occlusion but Reviewer 5 points out that this is likely due to the foveation nets having more unoccluded information.   On the positive side, Reviewer 4 points out that the experiments are extensive and several reviewers commented that they trust that the experiments were done correctly.  Reviewer 2, 4 and 5 all mention that there are too many results reported and recommend paring down to the most important results.  In my view the paper is right at the border of acceptance.  Acceptance/rejection will depend on capacity limits and balancing areas.  I recommend that if accepted,  or resubmitted to another conference, that the results be pared down, and  more space be devoted to explaining the question(s) and why they are  interesting and relevant and why the comparison networks allow the questions  to be answered.    Originality   High Quality   High  Clarity   Could be improved Significance   Could be better articulated and is hard to assess as is. Pros:   interesting idea, many well done experiments Cons:   claims not well validated, clarity could be improved to	emphasize significance Other: paper too dense   should be pared down to improve clarity 
The submission combines meta learning and attention mechanism for generalised zero shot learning. The image guided attention on the semantic space helps to adapt the better class specific semantic information while separate experts operate on the seen and unseen classes. The unseen class expert is trained with the pseudo negative samples with pseudo negative labels. Meta learning based training adapts the model to few shot learning scenario. The submission has received two accept, two weak accept and one weak reject reviews. All reviewers found the methodology interesting but they found it moderately novel. The experimental evaluation has been found strong. The rebuttal addressed all the reviewers  concerns and during the discussion phase all reviewers recommended acceptance. The meta reviewer follows the consensus of all the reviewers and recommends acceptance.
Three reviewers recommend rejecting or weak reject. The studied problem is interesting, but as one reviewer pointed out, it is not that clear how this work changes our theoretical understanding of those methods or what they imply for applications. Overall, I feel this work is on the borderline (probably it deserves higher score than the current score), but probably below the acceptance bar at the current form. 
This paper provides a high level API for working with Neural Tangent Kernels (NTK) and Fisher Information Matrices (FIM). This is an implementation paper, but such concepts are clearly useful in many tasks. However, such methods are available in many in house code (almost every paper on FIM / NTK uses such methods) I would not say it hampers the progress.    Pros:   The proposed methods are already widely used by the communities in the in house codes, but no single library is available             The library implements state of the art approaches  Cons:   This is an implementation library, and no benchmarking is available. More testing is needed to showcase the library.               Matrix by vector products are provided, but typical operations include many more, like Lanczos method for approximating solutions of linear systems and matrix function by vector product, or randomized SVD to compute low rank approximations. I believe that this should be the part of the library in order to make it a serious competitor for existing "local implementations". Also, such kind of methods would be later or sooner part of big libraries such as PyTorch and Tensorflow.   
Reviewers could not reach consensus here and important concerns from one reviewer on empirical results could not be convincingly addressed. The authors have provided a comprehensive response to the reviews, yet failed to convince them. 
This paper studies the important problem of efficiently identifying good hyperparameters for convolutional neural networks. The proposed approach is based on using an SVD of unfolded weight tensors to build a response surface that can be optimized with a dynamic tracking algorithm. Reviewers raised a number of concerns which were not fully addressed in rebuttals and lead me to recommend rejecting this work. In particular: focus on single hyperparameter (learning rate) made it unclear whether the proposed approach could actually be used for other hyperparameters or to jointly optimize combination of hyperparameters, empirical improvements even for learning rate are not strong and baselines are weak, and concern that initial success early in training (5 epochs) may not lead to generalization late in training. Additionally, there were several concerns around the clarity of the presentation, which I also found hard to follow: how is KG related to information theoretic metrics, why is the particular form of averaging across layers reasonable, and how is it related to other generalization / performance metrics? With additional experiments on other hyperparameters (for example L2 regularization), I think the work would be greatly strengthened. 
I thank the authors for their constructive engagement in the review process   this paper clearly benefitted from your prompt attention to initial weaknesses   and I thank the reviewers for their excellent, detailed, careful reviews.  This paper presents methods that allow a multi task learning (MTL) system to performs competitively against single task (ST) fine tuning despite using fewer parameters and less data per task. This is a great goal, which disrupts the currently most pursued approach of ST fine tuning and beats aiming for fully single model like BAM!  Pros  * The paper presents a number of useful methods, some novel like conditional attention mechanisms, for improving MTL. There is a lot in this paper. * Clean, motivated, intuitive model modifications * Comprehensive experiments * Generally well written paper, fairly comprehensive discussion of related work  Cons  The initial paper had a number of weaknesses, the most consistently mentioned being that the results presented tend to overclaim and to confuse (flipping between BERT and RoBERTa, etc.). The authors have done a good job revising the paper to address these concerns but should certainly remember these points in producing the final version.  The paper is a bit of a grab bag of small contributions that together help for MTL, but which isn t as strong a story as one big novel idea carefully presented and evaluated. While this paper has a ton of work behind it, and a lot of good stuff in it, I think this does somewhat weigh against the paper being selected for spotlight presentation.  Requests for the final version:  Further small clean up: e.g., still one "STITLS" to become "STILTS", some cleaning up could be done in the references where "stilts" is lowercase, there isn t capitalization after punctuation in the title of either Clark et al. paper, and the de Marneffe et al. paper, Fisch et al. paper and others lack information on where the work appeared).  Despite the improvements, I think more can and should be done to make the paper clearer and better laid out. Here are a few suggestions:  * More consistency in labeling the contributions might be possible. I m imagining that they might be able to be consistently labeled 1 to 5, rather than being 1 to 5 in subsections of section 2, but 1 to 3 with bulleted sub items in the introduction, and several of them (a) through (c) in Fig 1. * Although the reviewers generally said the paper was clear and well written, I still feel that section 2.1 is harder to get than it should be – and indeed you resorted to pasting PyTorch code in this discussion to help us! While people often say that a figure should be standalone, maybe that doesn t really make sense when it s an inline figure like this, and you d be better off explaining things well once (mainly in the text) rather than badly twice?!? I.e., just make the Fig 2 caption: "Figure 2: Conditional attention". I still think it s harder to get what the equations are doing than it should be, because there is inadequate text explaining the equations and the ideas motivating them. Although you now sort of sneak in to the text that ⨁ means diag, you still don t explicitly say so. I think having even 2 more lines of text at your disposal could really help if well deployed. * As R5 observes, the compactification in pages 7 9 just gets kind of ridiculous. I get that you re trying to deal with page breaks and to fit a lot in etc., but it just makes no sense when the tables embedded into paragraphs on p9 don t even belong with the corresponding paragraph! How might this be fixed? It s difficult, since you do just have a lot of material. The best idea I can come up with would be to shorten the main paper related work section to only 1 paragraph that discusses things at a very high level, and to put your detailed related work (even adding a few more of the things reviewers suggest like MT, etc.) to the Appendix. This might give you another half page in the main paper to make things more sensible.  
The paper proposes a novel way to have weight decay like update rule. Empirically, the authors claim that it improves generalization when applied to momentum based optimizers and optimizers with coordinate wise learning rates.  This paper has been thoroughly discussed, both in public and private mode. The strength of this paper lies in the possible gain in generalization performance due the proposed change. The weaknesses are:   the very confusing and not scientific motivation of the proposed change   the experiments are not fully convincing  More in details, we all found the discussion on "stable" and "unstable" weight decay extremely confusing. The claim of the paper is that "stable" weight decay should be preferred over "unstable" one. However, to validate a scientific claim it is necessary to carry out an empirical or theoretical evaluation. The theoretical one is simply missing: a number of proposition and corollaries are stated with some simple mathematical facts completely disconnected from the optimization or generalization issues. As it is, removing these arguments would actually make the paper better. On the empirical side, there is no experiment that supports the simple claim that "the unstable weight decay problem may undesirably damage performance". Instead, what we see are experiments in which the modified update rule seem to perform better, but they don t actually show that "stability" or "instability" are the specific issues at play here. Indeed, any other explanation is equally valid and the experiments do not support any specific one, but rather they can only support the claim that the proposed algorithm might be better than some other optimization algorithms. The *specific reason* why this is happening is not clear.  Turning to the empirical evaluation, the discussion elicited the fact that, a part for CIFAR10, the experiments are carried out without tuning of the learning rates. Hence, it is difficult for us to even validate the claim of superiority of the method. I don t subscribe to the idea that a deep learning paper requires experiments on ImageNet to be valid. Yet, given that there is no supporting theory in this paper, the empirical evaluation should be solid and thorough.  For the above reasons, the paper cannot be published at ICLR.
There was fairly detailed discussion among three of the four reviewers. The fundamental concern of the reviewers is regarding the contribution of the paper. During the rebuttal, the authors clarified the following:  > while the effects of varying uncertainty / horizon lengths is well understood for Bayes optimal policies, it is not understood for existing meta RL approaches, which is the topic of this paper  That is, the contribution of the paper is to understand the effects of varying uncertainty/horizon lengths for meta RL approaches. However, it is known in prior work that meta RL algorithms such as RL^2 can implement Bayes optimal policies in principle. As a result, it s not clear whether this contribution is significant relative to prior knowledge, and this paper does not seem to bring any new insights.  An alternative framing of the paper would be to consider the question of how meta RL solutions compare to Bayes adaptive optimal policies. While this framing would be interesting and novel, the current version of the paper does not sufficiently answer this question, since the only experiments include RL^2 (and such a study would require experimenting with more sophisticated meta RL algorithms beyond RL^2).  As such, this paper isn t suitable for publication at ICLR in its current form.
All reviewers recommend rejection due to limited novelty and insufficient experimental analysis. The author’s response has addressed several other questions raised by the reviewers, but it was not sufficient to eliminate the main concerns about novelty (as the method is a combination of existing techniques) and missing comparisons to justify the effectiveness of the proposed approach.
This paper gives a way to learn one hidden layer neural networks on when the input comes from Gaussian mixture model. The main algorithm uses [Janzamin et al. 2014] as an initialization and then performs gradient descent. The main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of Gaussian; 2. to give a local convergence result when the samples come from a mixture of Gaussian. The paper claims certain behavior in the input data would make the problem harder and slow down the convergence, although the claim is based on an upperbound and would be stronger if there is some corresponding lowerbound.
There was a consensus among reviewers that this paper should be accepted as the authors addressed reviewers  concerns in the discussion phase. This paper is well written and easy to read. It provides a coherent story and investigation on two important hypotheses: that natural images have a lower intrinsic dimension than the extrinsic dimension (e.g. the number of pixels) and that a lower intrinsic dimension lowers the sample complexity of learning. These results appear to be novel and significant for the ICLR community as it provides justifications for numerous work on understanding and designing convolutional neural networks based on low dimensional assumptions.
The paper argues that GNNs can be understood as a graph signal denoising. While this interpretation is not surprising and not novel, the unified view does seem insightful according to some reviewers. Yet, it is not clear how much insight can be drawn from the presented theory, as no significantly better architecture or experimental results are presented.   Additional criticism was raised wrt unclear relation between GAT and the graph signal denoising, the fact that analysis focused on one layer and does not explain the relations between layers and how nonlinear activation functions affect these theoretical findings, and that the objective of GNN cannot be viewed as a simple combination of graph denoising problems. Several reviewers complained that the paper is hard to follow.   In light of the above, despite the significant efforts of the authors to address these issues in the rebuttal, we believe the paper is below the bar and recommend Rejection. 
Though the method suggested in this paper is interesting, theoretically motivated, and resulted in some practical improvement, the reviewers ultimately had low scores. The reasons for this are: 1) The improvements obtained by this method were rather small, especially on the standard datasets (CIFAR, Imagenet). 2) In the main results presented in the paper, it seems that a proper validation/test split was not done (which seems quite important for demonstrating the validity of this method). In some of the results, presented in supplementary, such a split was done, but this seems to decrease the performance of the method even more. 3) The method requires that features in the last hidden layer approximately span a low dimensional manifold. This seems like a major limitation for the accuracy of this method, which becomes approximate in datasets where the number of datapoints is larger than the size of the last hidden layer (which is the common case).  Therefore, I suggest the authors try to improve all of the above issues and re submit. For example, one simple way to address issue 3 and potentially improve the results (issue 1) is to use the same method on all the features in all the layers, instead of just the last layer. In other words, concatenate all the features and all the layers, and then add a linear layer from this concatenated feature vector directly to the network output, in a direction that is orthogonal to the data.   
All reviewers generally admit that the motivation of realizing search free autoaugment is reasonable and important. However, they also raised many concerns regarding the experimental evaluation to validate the practical effectiveness of the method. In particular, unclear discussion with respect to ablation studies, and the lack of the baselines implemented by the authors were the central issues that obscure the essential effect of the core contribution of the work.  The authors made great efforts to conduct additional experiments and did address some of those issues, however some experiments are not yet ready such as the baseline implementation on ImageNet and testing on large models. After the discussion phase, all reviewers decided to keep their initial scores toward rejection, and the AC agreed with their opinions.   In summary, the paper focuses on an important problem and the proposed method is potentially very useful, but the paper in its current form should be further polished and completed before publication, thus I recommend rejection this time.   
The paper studies an elegant formulation of personalized federated learning, which balances between a global model and locally trained models. It then analyzes algorithm variants inspired by local update SGD in this setting. The problem formulation using the explicit trade off between model differences and global objective was received positively, as mentioned by R1 and R2. After a productive discussion including the authors and reviewers, unfortunately consensus remained that the paper remains below the bar in the current form. The contributions are not presented clearly enough in context, the set of competing algorithms (including e.g. EA SGD, ADMM, SVRG/Scaffold for the heterogeneous setting, and others) needs to be clarified in particular for the modified formulation compared to traditional FL, since objectives are different. Some smaller concerns also remained on the applicability to more general non convex settings in practice. We hope the feedback helps to strengthen the paper for a future occasion.
The paper studies the representation learning problem in the linear bandit setting, where each bandit "task" shares a common low dimensional representation. The paper introduces a novel algorithm, it provides theoretical regret guarantees, and it illustrates the effectiveness of the proposed method in a number of experiments.  There is a general agreement among the reviewers about the relevance of the problem and the contribution of the paper. The authors properly addressed concerns about the novelty (e.g., comparison with linear bandit and low rank structure) and about the underlying assumptions. Although some of them do seem relatively strong (and in some cases stronger than the state of the art in bandit, such as the distribution on the contexts), it is indeed non trivial to understand whether such assumptions can be easily relaxed in the representation learning context.   The novelty of the algorithm is more on the specific problem and set of assumptions, but it mostly relies on known principles (e.g., using method of moment for estimating the underlying representation). In this sense, I see this paper more as a useful addition to the fast growing landscape of representation learning methods in online learning, rather than a breakthrough. Also, the structure of the algorithm seems very "theoretical" in nature, since the explore than commit approach is very rarely a good strategy in practice.   Another issue the authors clarified in their revised submission is the actual improvement obtained in the bounds depending on the parameters T, k, d, N. In this respect, I still would like to encourage the authors to further illustrate the regime where the bound is actually better than for the single task approach. For instance, they could consider N fixed to a convenient value and produce a plot with x axis T and y axis the regret bound and report different curves for varying values of k and d. This would further clarify to the reader when representation learning can *provably* improve over plain single task learning.  Overall, given the general support from the reviewers and the revised version of the paper, I consider this contribution is significant enough to propose acceptance. As mentioned above, I believe it will serve as a reference for developing further the literature in this domain.
Analyzing class wise adversarial vulnerability of models is an interesting direction to pursue. However, the authors should consult the references pointed out in the reviews to put their contributions in the right perspective. Overall, the lines of inquiry explored in this paper are of interest but, as some of the reviewers point out, there are improvement in the methodology that still need to be addressed before this paper is ready for publication. (I very much recommend that the authors do build on this feedback and revise the paper, as it will be a valuable contribution then.) 
The reviewers had a number of concerns which seem to have been addressed by the authors in the discussion phase.  All the reviewers are in favor of accepting the paper. The paper provides an interesting/novel idea for federated learning with heterogenous clients/devices. 
Reviewers appreciated the model and the ideas presented and found them very interesting.  The main reason for rejection is the extent of the empirical work.  Unfortunately, and I think what is a bad sign for the ICLR community, the authors could not do adequate empirical work due to their computational resources.  Not belonging to an organisation with extensive computational resources myself, I am in strong symparthy with the authors, though I do not see any way this can be satisfactorily accounted for in reviewing.  Several reviewers commented on the datasets, the extent of evaluations, and the comparisons made with prior work.  For instance, the small CIFAR10 images are not ideal to demonstrate the technique and comparative results with the other data sets are limited.  The reviewers had a number of concerns on the theoretical work and these were well discussed by the authors.  In summary, this is promising research but needs more empirical work.   
This paper proposes an unsupervised graph learning method [Iterative Graph Self Distillation (IGSD)] by iteratively performing self distillation to contrast graph pairs under different augmented views. This idea is then extended to semi supervised setting where via a supervised contrastive loss and self training. The method is empirically evaluated on some semi supervised graph classification and molecular property prediction tasks, and has achieved promising results.  Reviewers agree that the method is interesting and the paper is well written. The biggest concern from reviewers related to experimental evaluations of the method. The authors responded to this and included additional experiments. Although the reviewers appreciate the provided results and explanations, at the end they were not convinced about the empirical assessments. In particular, R1 s post rebuttal comment indicates concerns about the reported performance of GCKN, which is different from the published one in Table 1 of GCKN paper. I encourage authors to improve on these experimental discrepancies and resubmit. 
The reviews were largely split in the beginning. Although all reviewers find that the idea of diversity and affinity measures for data augmentation is intriguing and potentially useful, they also raised many concerns such as computationally expensive nature of the diversity metric, lack of clear methodology of how to utilize those metrics to design augmentation strategies in practice, and weak organization and presentation of some experiments which are seemingly less related to the main point of the paper. During the discussion phase authors made significant efforts to improve the paper, and some of the concerns are favorably addressed. As a result, two reviewers raised their initial scores, yet we still think the paper is on the borderline.   Overall, this paper presents an interesting and unique idea that potentially stimulate the community, while it also has some key weaknesses and has much room for improvements. Considering both pros and cons, we decided to accept the paper.
As of now, automatic data augmentation methods have mostly been proposed for supervised learning tasks, especially classification. This paper introduces automatic data augmentation to deep (image based) reinforcement learning agents, aiming to make the agents generalize better to new environments. A new algorithm called data regularized actor critic (DrAC) is proposed, with three variants that correspond to different methods for automatically finding a useful augmentation: UCB DrAC, RL2 DrAC, and Meta DrAC. Promising results are reported on OpenAI’s Procgen generalization benchmark which consists of 16 procedurally generated environments (games) with visual observations. Further experiments have been added in the revised version.  **Strengths:**   * This work is among the first attempts that propose an automatic data augmentation scheme for reinforcement learning.   * The paper articulates well the problem of data augmentation for reinforcement learning.   * The experiment results are generally promising.  **Weaknesses:**   * Although the experiment results reported seem promising, there are missing pieces in order to help the readers gain a deeper understanding to justify more thoroughly why the proposed regularization based scheme works.   * Theoretical justification is lacking.  This is a borderline paper. While it presents some interesting ideas supported empirically by experiment results, the paper in its current form is premature for acceptance since a more thorough, scientific treatment is lacking before drawing conclusions. Moreover, considering that there are many competitive submissions to ICLR, I do not recommend that the paper be accepted. Nevertheless, the authors are encouraged to address the concerns raised to fill the gaps when revising their paper for resubmission in the future. 
This paper proposes a new method for label free text style transfer. The method employs the pre trained language model T5 and makes an assumption that two adjacent sentences in a document have the same style. Experimental results show satisfying results compared with supervised methods.  Pros. • The paper is generally clearly written. • The proposed method appears to be new. • Experiments have been conducted.  Cons • The fundamental assumption of the method is not convincing enough. (Issue 1 of R3, Issue 4 of R4, Issue 1 of R2) • The proposed model is also not convincing enough. (Issues 2 and 3 of R3, Issue 3 of R2) • There are problems with the experiments. For example, it would be better to use more datasets in the experiments. (Issue 4 of R3, Issue 2 of R4)  Discussions have been made among the reviewers. The reviewers appreciate the efforts made by the authors in the rebuttal, including the additional experiments. However, they are not fully convinced and still feel that the submission is not strong enough as an ICLR paper.  
This paper proposes a mechanism for fast sampling from the posterior over the weights of the last layer of neural network, by approximating the logits as a Gaussian through equation 8. This is based on earlier work by MacKay, but has some new empirical investigations. This is a very difficult case.  In its favor: * Despite the main ideas coming from older work by MacKay, they are interesting and relevant and worth re surfacing. * The experiments demonstrate some improvements in OOD detection over a diagonal Laplace approximation to the last layer, and is competitive in performance with a KFAC Laplace last layer approximation but much faster at test time. * The authors provided early and thoughtful responses and actively tried to have a discussion with reviewers. It is a pity that the reviewers did not participate in this discussion.   Concerns: * While interesting, it is unclear if the proposed method actually has much practical utility in its current form. The method is presented as a fast approach for uncertainty in Bayesian deep networks. But Eq. 8 requires such significant computations to form that whatever is gained by the fast sampling may not make up for the cost of forming the approximation itself. This computational burden is why the approach is only applied to a last layer. Table 2 and some of the surrounding discussion helps with alleviating these concerns and is most appreciated. But many basic questions persist: (i) do we really need many samples to achieve good performance, especially from a posterior over only a last layer? (we see the KL divergence decreases, but what about performance on interesting problem as a function of sample size?) (ii) in terms of total runtime accuracy would this be competitive with using Bayesian methods over all the parameters, even if these methods are taking fewer samples? It would be easy to try. (iii) Besides OOD detection, how does this approach generally affect accuracy or calibration? (iv) How would this method compare to a basic baseline like retraining the last layer several times and ensembling? (v) could anything be done to significantly accelerate the computations in forming Eq. 8? While not all of the answers to these questions need to be favorable to the Laplace bridge for acceptance, it would certainly improve the paper to at least address most of the questions explicitly. At the end of the paper, an online setting is mentioned, which I think would be amenable to this approach   it could be good to explore this direction. * It is disappointing that the reviewers did not communicate with the authors, despite commendable efforts from the authors. However, the paper continued to lack a clear champion. Given the persisting lukewarm reception of reviewers,  and some of the practical concerns above, it would help to have some "stand out" result, especially since the methodology, while interesting and relevant, is not new. That does raise some expectations for the experiments.  This is not an easy case. The paper has merits. And it s possible some of the concerns could be addressed by simply more clearly rationalizing design decisions (why would we use this approach in its current form over full Bayesian methods, which are now quite fast, with fewer posterior samples?).  At the same time it s clear the paper in its current form is not resonating with reviewers, and there are concerns about the practical applicability and limitations. It s on the borderline. Having some stand out results could really help this paper realize its potential.
 The paper proposes a general framework for learn object centric abstractions represented using PPDDL (a probabilistic planning language).  The work assumes that objects and their attributes / features are identified.  The key contribution of the paper appears to be proposing to group individual objects into object types based on whether objects have the same outcome in planning. Using the learned object types, it would then be possible to transfer learned operators from one task to another.  The framework is demonstrated on block world (blocks are stacked on top of on another) and minecraft.     Review Summary: Initially, the submission received negative to borderline reviews with R4 being the most positive (score 6), and R1, R2, R3 being more negative (scores 4, 3, 5).  After discussion, R4 lowered their score to 4 and indicated that they felt the work was not ready for acceptance at ICLR.  Overall, there was limited discussion by the reviewers.  Reviewers (R2,R4) found the direction of the work promising and interesting.  After the author response, reviewers indicated that the revision and author response clarified some points, but believe that the work is not yet ready for acceptance, as 1) there is a significant amount of hand crafting required and 2) parts of the approach is still not clearly specified.    Clarity: As some reviewers note, the description of the framework is at a very high level, making it difficult to follow with missing details on specific details of how the object types are groups.  The specific aspect of the work that is novel is also not clearly stated, thus making it difficult to judge the originality and significance of the work.  After revision (the authors added brief paraphrase to the introduction to clarify the contribution, and additions to the appendices providing more details on how the difference steps work for the Minecraft scenario as well as failure cases), the manuscript is improved but the overall manuscript is still difficult to follow.  Pros:   Interesting and important problem (combining probabilistic/neural approaches with symbolic approaches) that is timely and deserves attention   The idea of clustering objects based on their effect is interesting (R4).     The framework proposed by the paper is interesting and potentially useful direction and can stimulate followup work   Cons:   The paper is difficult to follow with symbols/terms that are not clearly defined and missing details. (R1) The specific contributions of the work, wrt prior work, is also not clearly stated.   The novelty/contribution of the work on top of existing work (Konidaris et al 2018, Ugur & Piater 2015, etc) is not that clear (R2)   The experimental setup is weak with limited comparisons and no statistical results. Overall, reviewers felt the results are not convincing enough to support claims on transferability and learning efficiency.   Lack of baselines comparisons (R3).  In the rebuttal, the authors argue that there is no appropriate baselines.     The set of steps that is involved is fairly complex (R1), with many important details provided in the appendix   There are concerns about the generalization of the approach as many of the steps are handcrafted (R1, R4).  In the provided scenarios, many of the steps, including the set of provided options, and representation of objects, are manually designed.    Recommendation: The AC agrees with the reviewers that the paper is not ready for acceptance to ICLR.  It is the AC s opinion that the work addressing a very interesting problem and  would potentially be of interest to the community.  However, the exposition of the paper needs to be improved so that 1) the contribution of the work over prior work (Konidaris et al 2018, Ugur & Piater 2015, etc) is clearer 2) the assumptions and details of the proposed method is also clearer and easier to follow.  The authors are encouraged to improve their work and resubmit to an appropriate venue.  
In this paper, the authors proposed a geometric graph generator that applies a WGAN model for efficient geometric interpretation. All the reviewers agree that the idea is interesting and the method has the potentials for graph generation tasks. Unfortunately, the experimental part is unsatisfying, which makes the paper on the borderline. More analytic experiments should be designed to verify the properties of the proposed GG GAN, especially its scalability. Although in the rebuttal phase the authors add a simple example to generate large but simple graphs, we would like to see more experiments and comparisons on more real world large graphs (even if the performance may not be good, the results will be constructive for both readers and authors to understand the work). 
This paper proposes  a new mechanism, called HIRE, to  improve the down stream performance of a pre trained Transformer on NLP tasks. Different from directly using the last layer of transformer, the proposed model allows the system to dynamically decide which intermediate layers to use based on the input through some sort of gating. The model is evaluated on GLUE, a benchmark for natural language understanding.  My major concerns are the following  1.  the gating mechanism on using intermediate sentence representation is not new,  as pointed by some reviewers, although its implementation on transformers is still interesting. 2.  the empirical part is not convincing enough:  a) GLUE data set is relatively simple, the authors should try something more complex, b）the improvement over baseline is rather modest, which could be achieved with simpler modification.  I d suggest to reject this paper.
This is an unusual, but interesting submission. Can we use a simple "quantum computer" (in fact, physical system) to solve classification problems in ML? A single photon passes through the screen. Its state is described by the complex vector. A quantum computer makes a unitary linear transformation on this state in such a way that it maximizes the overlap with a corresponding class. Such a model can be parametrized by conventional means, and trained and later possibly realized by an quantum system  Pros:   1.  The area of QC is very important, and such papers shed a new light on the subject.  2. Inspiration to the ICLR community to work on in this area. 3. Technically correct.   Cons:  1. The accuracies are far from SOTA and use very toy datasets. It is not clear, how to get to the accuracies needed in practice. 2. The actual computational speed of inference is not clear. 3. Discussion of more complicated models and their possibility is necessary. 4. Quite a few misprints are in the text which need to be fixed in the final version.    
Reviewers agree that the paper excels in providing a principle pipeline that combines CNNs and GPs with a Poisson Gamma distribution to provide a generic approach for multiresolution modelling of tumour mutation rates. As a whole such combination of techniques addresses a key challenge in computational biology that also scales to large datasets. 
The paper presents a new idea for detection of model stealing attacks. The new method generates "fingerprint", i.e., adversarial examples that transfer to surrogate models (extracted in model stealing attacks) but not to reference models (i.e., models obtained independently from the same data). If a model owner suspects that some model is stolen, fingerprints can be used for verifications of such claims.   The paper s contribution is novel and significant. It is the first practical tool, to my knowledge, suitable for a reliable characterization of stolen models. The empirical results are quite impressive demonstrating the detection of stolen models with an AUC   1.0. Some presentations issues have been addressed by the authors during the revision.  
Most of the reviewers pointed out a lack of rigor of this submission, unclear contributions, not too convincing claims and empirical gains. I thank the authors for the effort put in revising the paper and responding to the reviewer concerns. However, the reviewers did not deem them convincing enough.
All reviewers expressed interest in this promising approach, but raised questions that were not addressed by the authors during the discussion period. As concerns raised included insufficient repeats of empirical experiments to draw conclusions and the paper appearing to be in an early draft format, we cannot support acceptance for publication at this time. I strongly encourage the authors to act on the feedback given to improve the paper for a future submission.  
The work focuses on a new method for sampling hyper parameter based on an "Population Based Training" schedule that tend to sample more often configurations that gave good performances in the past. The authors have conducted experiments to verify the superior of their method, especially for the effectiveness and generalisability.  Pros:   simple method that can be implemented without much effort,   good empirical performances on Imagenet,   paper well organised and written.  Cons:   lack of explanation about the DensNet121 performance degradation [partially addressed in the rebuttal],   additional simple experiments in Section 4.4 were recommended to evaluate the generality of the method [addressed in Table 5],   empirical validation seems not sufficient [partially addressed in the rebuttal],   similarity with respect to prior art, such as the focal loss [partially addressed in the rebuttal],   clarification of the randomisation strategy in experiments [addressed in the rebuttal].  Despite most of the issues being addressed, the reviewers decided that this paper would benefit more work to be accepted for the conference this year.
The paper proposes a defense against black box adversarial example attacks based on adding small Gaussian noise to the inputs. Its evaluation is carried out empirically using CIFAR 10 and ImageNet datasets.  Despite a somewhat complete experimental evaluation (on two datasets) the lack of theoretical justification strongly affects the significance of the proposed method. It can be clearly seen from the experimental results that the proposed level of noise is a trade off between clean accuracy and attack effectiveness. However, this tradeoff neither implies a substantial degree of security (the attack success rate is roughly halved but this does imply robustness against attacks since the initial success rate is rather high) nor is  the impact to clean accuracy negligible. Furthermore, the robustness of the proposed method against an attack which is aware of such defense (similar to the Kerckhoff s principle in cryptography) is not evaluated. The authors mentioned several directions for addressing this issue in their response but implementation of such improvements is impossible within the level of revisions acceptable in a post review process.  A major revision of the paper taking into account the feedback provided by the current reviews would certainly improve its acceptance chances.  
This work presents a method to combine EBMs and VAEs in two stages. First, the VAE model is learned; second, an EBM based correction term is learned via MLE. The methodology is novel and of interest to the ICLR community.
This paper proposed two variants of the SELU activation function, termed the leaky SELU (lSELU) and scaled SELU (sSELU), respectively, in order to yield a stronger self normalization property. The review process and the discussion find the following issues:    The hyperparameter tuning for the baselines is insufficient for the baselines so that the comparison may be unfair.    The experiment results (Table 2) do not show superiority of the proposed activation functions. In addition, the results appear to be unrelated to each other. (see Reviewer 3 s detailed update)   Reviewer 2 pointed out that the architecture that the authors used was far from the SOTA. I read the authors  response. This paper may benefit from adding some even naive workaround and making fair comparisons under the SOTA architecture.   I do not think (6) is a good way to present this equation. The authors may want to perform change of variable and replace $\sqrt{q}z$ by $z$ in the integral and add this form to the right hand side of (6). In addition, $\epsilon$ appears in Definition 2. However, when the authors mention the self normalization property in Definition 2, they omit $\epsilon$. It might be better to call it $\epsilon$ self normalization property to stress that this definition depends on $\epsilon$.    Other minor issues:   The line right below (15) on page 11, the authors did not need to capitalize "orthogonal".   In eq (16), $W_l^{,T}$, the comma is unnecessary. 
This paper presents an approach, FedMix, for federated learning using mixture of experts (MoE). The basic idea is to learn an ensemble of models and user specific combination weights (mixing proportions).  The reviewers appreciated the MoE formulation for federated learning. However, there were multiple concerns from the reviewers, which include lack of clarity (regarding the variational lower bound that is being used), significant communication cost and privacy concerns (the server can infer the users  label distributions), weak experimental results, and lack of any theoretical support (which isn t that big an issue if the paper were stronger on other aspects). The author feedback was considered and the reviewers engaged in some discussions (also with the authors). In the end, however, the reviewer were still not convinced that the paper is ready to be published in its current state. Based on my own reading of the paper, the reviews, and the author response, I agree with this assessment.  The authors are advised to take into account the feedback from the reviewers and resubmit to another venue.
This paper shows how multiple tasks can be encoded in a single neural network without the need for explicit modular construction for each task. The idea is very interesting and the research work presented is of high quality.   All the reviewers underline their interest in the presented work. However, there is a deviation in the reviewers  score with half voting  for acceptance and the other half for rejection. The main concern of the fellow reviewers with the below acceptance threshold score was the difficulty in grapsing the theory of the research presented due to the lack of important content from the main manuscript due to space limitations. The authors have an extended supplementary material that covers the whole magnitude of their work.   I understand the reviewers  concern on how such a dense presentation does not do justice and harms the presented effort itself. However, given the edits the authors added to address the issue rasied and the interest and potential of this work   acknowledged by all the reviewers and myself I recommend acceptance. This is a work of a quality I would like to keep seeing in ICLR. 
This work provides theoretical analysis for FedAvg, contributing better convergence rates than prior work. Moreover, the paper shows that setting E > 1 can reduce the number of communications.  The contribution is incremental. 
The authors did a nice job of responding to the concerns of reviewers during the discussion phase which increased reviewer scores. Because of this I will vote to accept.   The authors should carefully edit the paper for typos, grammatical errors, and style errors. Some examples:   Abstract: Make this one paragraph without a line break   End of 1st paragraph in Intro: "So there is an urge"  > "So there is an urgent"   Start of 3rd paragraph in Intro: "State of the art cryptographic"  > "The state of the art cryptographic"   Last paragraph of 2.1: "To solve above"  > "To solve the above"   End of 2.3: "Compared to the light weight InstaHide and TextHide, MPC and HE are of advantages in the security guarantees so far."  > "Compared to the light weight methods InstaHide and TextHide, MPC and HE provide much stronger security guarantees."  I also urge the authors to please double check the reviewer comments when preparing a newer version to ensure all concerns are taken into account.
This paper proposed a new method for improving offline RL. AC thinks that the paper has a potential, but all reviewers suggest rejection as the current write up is quite poor. This causes many misunderstandings of reviewers. The authors clarify some misunderstandings/concerns in the discussion phase, but did not update the draft accordingly. Hence, AC cannot suggest acceptance, given the current form.
The paper focuses on anomaly detection in dynamical systems from time series measurement. The originality of the contribution is to detect anomalies not based on the detection of OOD observations but from identified parameters or statistics of the dynamical system. They are using “polynomial neural networks. All the reviewers agree that the paper is not yet mature both in the form and in the technical content. The authors did not provide a rebuttal.
This paper introduces an architecture based on structured causal model for long tailed IE tasks. It incorporates the dependency tree structure of the sentence using a GCN for learning the representations. The key idea is to use counterfactual reasoning to help with the inference in attempt to reduce the impact of spurious relations. There are some concerns about the presentation of this paper. While the high level idea is reasonably clear and well motivated, the paper is quite messy with the notations and technical details.  How to use the causal effect estimation for the final prediction is not explicitly explained except for in Figure 1.  For the experiments on ED and NER, it is unclear if they assume the trigger or span is given. The method seems to need the span information to make the prediction. If span is given, this is a different set up that is much simpler compared to traditional ED and NER where the span or trigger needs to be detected as well. There are also some question regarding the difference between this work and the prior work on using causal reasoning for improving prediction (the TDE work). One difference is the additional term in equation 8(updated version), which appears to be useful empirically, but the motivation is rather hand wavy and needs more clarification. Overall, there are some useful ideas, but the overall novelty does not particularly stands out, and the presentation of the paper made somewhat straight forward ideas more convoluted than necessary.  
This paper is devoted to "dyadic fairness" in representation learning. All the reviewers agreed that the contribution is novel, original and technically sound. However, all the reviewers agreed that the paper should be improved in terms of presentation   for two reviewers, presentation/clarity issues were at the core of their weak rejects. The most positive reviewers highlighted that the problem is still understudied despite the flurry of work on fair machine learning in the last years and therefore the contribution deserves to be accepted. If there is room, this paper can be accepted as a poster.
The paper suggests a simple variant for BERT training that improves classification for smaller training samples.  So it has a very specific applicability unlike other published variants which generally improve a broad range of tasks.  The variant adds a self supervision classification task based on clustering.  Experiments are done but it only shows improvement for small training sizes.  AnonReviewer4 suggested a BOW experiment/baseline which was done by the authors in an updated version.  This confirmed the authors line.  AnonReviewer3 asked for computational details, which were added.  AnonReviewer1 lists a number of limitations which the authors need to address and rephrase the statements in their paper.  So it is publishable work, but somewhat marginal due to its specialised nature and thus rejected.
The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.  
The paper seeks to increase receptive fields of GNNs by aggregating information beyond local neighborhoods with the idea of addressing oversmoothing and/or overfitting issues with message passing algorithms. The proposed method is simple and primarily makes use of node features and local structure similarities. In this sense the approach is related to Pei et al. Several concerns remained as articulated in the reviews, including: oversmoothing is not discussed/analyzed, performance gains are small, more extensive comparisons are needed.  
This paper provides a clear and useful empirical study of how the initialization scale and activations function affects the generalization capability of neural networks. Previous works showing the effect of the initialization scale (Chizat and Bach (2018), Geiger et al. (2019), Woodorth et al. (2020)) had a more limited set of experiments. Moreover, here an extreme case is shown, wherewith sin activation function no generalization is possible at a large init scale (there the kernel regime is useless for generalization since the hidden layer output becomes very sensitive to any small perturbation in the input). Lastly, two alignment measures are suggested, which are correlated with the generalization across several architectures and initialization scales.  All the reviewers argued for acceptance, and one strongly so. I agree that the paper is sufficiently interesting and clear to be accepted. However, despite the high scores, I only recommend a poster and not spotlight/oral: I think the novelty of the empirical study is not groundbreaking, given the experiments in previous works, and the usefulness of the suggested measures are not completely clear without a thorough comparison against previously suggested measures.
This paper proposes a new approximate algorithm for Bayesian logistic regression in the online setting. The primary approximation involved in the algorithm is the use of a diagonal Gaussian approximation. (A probably more minor approximation is approximating the sigmoid with a Gaussian.) The main discussion focused on two issues: Firstly, there was some sentiment that the paper lacked theoretical guarantees. Second, there were concerns about the experimental results. I feel that it is not a serious flaw that the paper lacks a theoretical regret bound. Given the current state of algorithms for this problem practical algorithms remain very much of interest. However, the general sentiment of reviewers was that the experimental results were not as strong (or as numerous) as would be hoped. For an algorithm without a theoretical regret bound, I do agree that stronger empirical evidence would be expected. This was partially addressed in a revision but still I agree with the consensus that more extensive numerical evidence should be expected, and for that reason I am recommending rejection.  Finally, I ll mention some other issues that I view as not counting substantially against the paper. Firstly is the dependence on the prior. Here I am in agreement with the authors that this is an aspect shared by all Bayesian methods. This issue is a (valid) argument about the value of all Bayesian methods, but not one I think we will resolve here. Second, there were suggestions from the reviewers about improvements that could be made to the baseline methods. Here I don t feel that it s fair that we ask the authors to make novel improvements to other algorithms, unless those improvements are very "obvious".
This paper proposes techniques for differentially private training of ResNets inspired by SDEs. The idea has some promise but the paper does not give convincing evidence, either theoretical or empirical that it outperforms existing tehniques. Unfortunately, comparisons with existing techniques are presented in a misleading way that does not clearly provide the privacy parameters.  Another issue with the work is that the authors appear to be unaware of (and hence do not compare with) existing work on privacy of a single prediction (referred to as strategy II in this work).  Approaches for this problem are described in these theoretical works (and several follow ups)  * Dwork, Feldman. Privacy preserving Prediction. COLT 2018 * Bassily,Thakkar,Tahkurta. Model Agnostic Private Learning via Stability. NIPS 2018  Practical results are mentioned in PATE papers of Papernot et al. and more recent work  * van der Maaten,  Hannun. The Trade Offs of Private Prediction https://arxiv.org/abs/2007.05089
The paper combines flow based and energy based models to generate molecular conformations given a molecular graph. For this, a continuous flow model is used to map the graph based molecular representation into a distribution over conformations.  An energy based model (EBM) is used to further help the model capture long range atomic interactions. The proposed method is compared with strong baselines: CVGAE, GraphDG, and RDKit.  The authors addressed most of the reviewers  concerns in the rebuttal.  All the reviewers agree on acceptance.
The paper received low ratings and the reviewers pointed out a number of issues. The authors  short response failed to address these concerns. 
The goal in this submission is to find interpretable samples discriminating two probability distributions. In order to tackle this task the authors propose to use a sliced variant of the Bures distance (where the slicing is implemented via a one rank tensor) and the associated witness function, and illustrate the idea in the discrimination task of fake and real images, and in the detection of covariate shift.   Interpretable discrimination of probability measures with witness functions is a hot topic of machine learning with a large number of applications and available tools (including linear time ones in the sample size, and methods capable of handling independence testing, goodness of fit testing, relative tests among others, beyond the considered two sample setting).   1)The motivation of the paper and the efficiency of the proposed method compared to available baselines are not clear; the relevance of the demos is questionable.  2)Unfortunately, the submission also lacks mathematical contributions: for instance  i)Is the proposed divergence a semi metric or metric, and under what conditions on the domain and the kernel? ii)Does the proposed estimator converge, under what assumptions, and how quickly (rates)?  The contribution represents a potentially interesting idea, but significantly more work is needed before publication.
This paper investigates robustness of the neural networks under bit level network and file corruptions, and proposes corruption agnostic and corruption aware defense approaches. The Bit corruption Augmented Training is introduced, which is about applying the data augmentation at a bit level.  The majority of the reviewers are against the acceptance of the paper. R1 gave the rating of "marginally above acceptance threshold", finding the problem interesting but not the proposed solution. The other reviewers gave rejections and a clear reject rating.  The main concern raised by all of the reviewers is regarding the technical novelty of the proposed approach in the paper. While some reviewers appreciate the importance of the problem and the thoroughness of the experiments more than the others, none of the reviewers find the proposed solution novel and interesting.  The AC agrees with the reviewers that the technical contribution of the paper is not significant despite that it focuses on an interesting problem. We do not recommend this paper to ICLR.
Despite the performance gains of RankingMatch over the benchmarks used in the paper, the reviewers remained concerned about how the paper compares to state of the art in several respects.
This paper presents a GNN architecture for policies that solve multi robot task allocation problems. The proposed architecture extends Koul et al (2019) by adding payload constraints and task deadlines. The paper looks at routing problems of medium to large size, e.g. 20 robots and 200  tasks. The reviewers are happy that most of their concerns were addressed by they are still concerned that the experimental validation is focusing too much on the multi TSP or Vehicle Routing Problems, and request more extensive experimental validation on similar optimization problems as in Nunes et al (2017). I tend to agree. The proposed method has a lot of merit and just needs one more iteration of improvements to incorporate further experiments, before it becomes ready for publication.      
This paper studies various graph measures in depth.  The paper was reviewed by three expert reviewers who complemented the ease of understanding because of clear writing. But they also expressed concerns for limited novelty, theoretical justification, and unrealistic setting. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
In this paper, the authors use a GP classifier to detect if the output of a NN classifier has been decided correctly. The GP takes as input the original input vector x and the output of the NN, i.e. the calibrated posterior probabilities given by the NN. It uses that as an input vector for the GP classifier to decide if the sample was correctly decided. The output of the GP will serve as confidence in the output of the NN. The results are comparable/superior with the state of the art and the authors have repeated the experiments with over 125 different datasets. The reviewers of this paper were all cautiously positive about the paper, but all of them pointed towards the reduced novelty of the paper. Also, none of the reviewers were willing to champion this paper as a must have at ICLR 2021.   For my reading of the paper, I would tend to agree with the reviewers’ comments. Also, I find that using the same NN, rather shallow, with the same configuration for all the datasets seems rather limited. Given that this method is independent of the underlying classifier and that the databases used are low dimensions and a low number of training examples, I would have liked to see what a random forest or a GP can accomplish. Also, I would have used bigger NNs that can be trained to overfit the sigmoid outputs for classification of higher accuracy. I believe that having a diversity of underlying classifiers is more relevant than having 125 datasets. We need to find the best classifier or ensemble and then apply the different mechanisms for estimating if the output is the correct one. Otherwise, the proposed method might only be workable for this specific NN configuration. In the tables, it can be hinted that this might be happening, as about 80% of the cases MCP and RED are indistinguishable in the AUROC values.   Also, for all of these datasets a GP could be used as an underlying classifier, and given the premises of this paper, the authors could check how well calibrate a GP classifier is. Also, there has been considerable work on calibrating NNs when they are trained to overfit. Comparing with those methods should be straightforward, as they provide more information than just a confidence score. This is probably the most influential paper: https://arxiv.org/abs/1706.04599 (1000+ references), but there are some recent papers too.     Finally, if the goal is to use a GP to detect if the classification done by the NNs is accurate, using a GP might be an overkill, as the complexity of the GP, especially for large datasets might end up being larger than the underlying classifier. 
This paper asks a simple question: Do extreme activating synthetic images for a CNN unit help a human observer to predict that unit’s response to natural images, compared with maximally/minimally activating natural images. They authors conducted well designed human studies and found that the synthetic images provide useful information for prediction, but that the benefit is smaller than that provided by simply presenting people with other natural images that maximally or minimally activate a unit.  The paper provides one reasonable metric for evaluating feature visualizations. Feature visualizations are widely used, but there are very few objective metrics for evaluating them. This methodological contribution is the main contribution of this work and could be impactful. Although the conclusion is not very surprising, the paper makes a potentially good contribution to the literature. 
This paper proposes a meta learning approach for inferring the Hamiltonian governing the dynamics of physical systems from observational data, and using it to adapt to new systems from the same class of dynamics quickly. The paper does this by effectively combining the previously published Hamiltonian Neural Networks and MAML/ANIL. The reviewers agree that the paper is well written, and the experiments are comprehensive, however, they also have reservations about the technical novelty of the proposed solution, given that it appears to be combination of pre existing models. Saying this, the authors were able to  address a lot of the reviewers  concerns during the discussion period, hence I recommend this paper for acceptance.
The authors provided a comprehensive rebuttal to the reviewers  feedback that addressed most of the concerns. AnonReviewer3 raised some major concerns that were partially resolved in a revision. The paper has received a split recommendation from the reviewers but within the review and discussion periods, there was no strong support towards accepting the paper. Although the paper has received some positive feedback, some of the reviewers  concerns were not fully addressed. I d recommend the authors to address all the comments and add clarifying notes to the paper to avoid such misunderstandings if they decide to resubmit the paper to another venue.  
This paper presents "stein bridge", a joint training framework that connects an explicit (unnormalized) density estimator and an implicit sample generator via Stein discrepancy. The idea and methodology are valid and of interest. But the raised concerns were not properly addressed. 
The paper focuses on the task of conditional video synthesis starting from a single image. The authors propose an *Action Graph* to model the configuration of objects, their interactions, and actions. They show promising results on two benchmark datasets (one synthetic and another realistic).   Based on the reviewers  comments and the limited discussion that ensued, it seems that some concerns in the paper were addressed by the authors; however, a main concern persists, namely the applicability of this Action Graph representation to more complex realistic videos (*e.g.* for datasets such as Kinetics and AVA). The authors do mention a manner in which an automated extraction of Action Graphs can be done, specifically with off the shelf (spatial or spatiotemporal) detectors for actions, objects, and object object interactions. However, these are complicated tasks in their own right and still open problems in the field. Given that the Action Graph computable from this automated pipeline will undoubtedly contain noise (compounded by the errors of each component of this pipeline), the paper could have made a stronger argument for its contributions in realistic video, if for example an ablation study was done where *noisy* action graphs were used in training. Without more evidence that this representation will be applicable to more realistic scenarios of interest, it is difficult to gauge the impact it will have on the community. Despite its merits and promising initial results, the authors are encouraged to address this persisting concern and the other reviewers  comments to produce a stronger submission in the future. 
The paper proposes a recurrent neural network architecture for abstract rule learning. An LSTM is augmented with a two stream memory structure: one block is populated with visual representations, and the other is populated by hidden state vectors from the RNN controller.  The authors also introduce a set of tasks that require a simple symbolic reasoning on visual inputs and strong extrapolation ability. They show that previous memory augmented neural networks fail on these tasks, whereas their model exhibits excellent generalization with limited training data.  Pro: The work addresses an important and open question in neuroscience and deep learning. The proposed solution is simple and effective. The manuscript is well written. It was also improved in a revised version after the first review round.  Con: The main criticism raised by the reviewers was that the considered tasks may be a too simplified synthetic task.  It would have been good to consider other the more complex tasks involving symbolic reasoning such as CLEVR or bAbI.  While this is a valid criticism, all reviewers agreed that this is an interesting and important work worth publishing. In particular, the considered question is of pivotal importance for the community and the work presents a significant progress.
The paper is proposing a test time adaptation method without modifying the training. The proposed idea is simple and effective, adapting the normalization layers using the entropy of the model predictions as a loss function. The paper presents an extensive empirical study. Paper received unanimously accept scores. It also has potential to be impactful as it is easy to apply without any strong assumption/requirement. A clear accept!
This paper examines under what conditions influence estimation can be applied to deep networks and finds that, among of items, that influence estimates are poorer for deeper architectures, perhaps due to poor inverse Hessian vector approximations for poor for deeper models. The authors provide an extensive experimental evaluation across datasets and architectures, and demonstrates the fragility of influence estimates in a number of conditions. Although the reviewers noted that these issues are now "folk knowledge", there has been less scientific effort in identifying these failures.  Of course, more theoretical understanding would help the community better understand where these fragilities lie, but the experimental evaluation is sufficiently strong to be of broad interest to the community.
The authors present a Bayesian approach for context aggregation in neural processes based models. The article is well written, and provides a nice and comprehensive framework. The reviewers raised some issues regarding the lack of comparisons to proper baselines. The authors provided additional comparisons in the revised version. The comparisons were found satisfactory by some some reviewers, who increased their scores. Based on the revised version, I recommend acceptance.
This paper presents a new dataset for open domain QA where the evidence required for answering a question is gathered from both structured data as well as unstructured data. The authors first show that a standard iterative retriever with a BERT based reader performs poorly on this task. They then propose fused retrieval (grouping relevant tabular and textual elements) followed by a cross block reader which improves performance.    R4 has raised strong objections about the artificiality of the dataset. I agree with that and it is unfortunate that the authors did not adequately address the reviewer s concern but instead digressed a bit. As suggested by R4, the authors should tone down their claims about the nature of the dataset. The authors should also simplify the presentation of the dataset as suggested by R2 and not make it unnecessarily complex for the reader.   However, overall, based on reviewer feedback, the authors have made significant changes to the paper. In particular they have added more baselines, ablation studies and error analysis which makes the paper much more informative.   I am okay with this paper getting accepted with the assumption that the authors will make the changes suggested above.   
The paper proposes to use conditional GANs to generate protein sequence with respect to GO molecular functions. The idea is nice. But the reviewers find that there are many things that are not clear. For example, some sentences, phrase, the model and experiments pointed by the reviewers that should be  rigorous described. The technical contribution is also limited. The author are encouraged to revise the paper according to the comments.
The reviewers are in agreement that this paper could benefit further improvement. There are several areas: novelty of the proposed approach and evaluation on real world datasets (beyond just CLEVR).
The paper studies the problem of being able to control text generated by pre trained language models. The problem is timely and important. The paper   frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm,  Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact. 
The paper suggests that whitening the data harms generalization and optimization performance when learning models of the form h(W x) i.e. those that are based on a linear projection of the inputs (which includes DNNs for instance). The main concern of the reviewers is that their theoretical developments were not that convincing; it seemed more along the lines of providing some specific anecdotes. But more broadly, the caveat is that their development seems very simple: their results in high dimensional settings (where d   dim(x) > number of samples n) is not that relevant since vanilla whitening is anyway fraught in such high dimensions, since the sample covariance matrix is not a good estimator in high dimensions anyway. And when n > d, their result focuses on linear models, where they say that whitening reduces information about the singular vector directions where the input data might mostly lie on. But if the data lies on a lower dimensional linear manifold, then whitening is again fraught: the covariance matrix is singular. The linear manifold assumption also seems very specific given the general title of the paper. Overall, the paper needs to narrow their focus on specific settings where whitening is harmful, but the specific settings above in and of themselves do not necessarily say anything other than to estimate the covariance matrix carefully before doing whitening. 
The paper describes an RL technique to learn how to branch in discrete optimization.  This advances the state of the art in comparison to previous imitation learning techniques.  However, the reviewers and a public reader raised concerns about the validity of the experiments due to several inconsistencies and differences with previous work that might suggest some cherry picking.  This is too bad since the reviewers really liked the work, but it is important to make sure that the experimental evaluation is done fairly.  I read the paper and I share the concerns regarding the experimental methodology.  Hence the experimental evaluation needs to be revised before publication.
The paper augments pre trained language models by introducing “adapter”, where each adapter is another language model pre trained for a specific knowledge source (e.g., Wikidata) and an objective (e.g., relation classification). The representation from each adapter is concatenated to the representation from the generic LM. Specifically, they introduce two adaptors, “factual” (mostly derived from Wikipedia), and “linguistic” (from dependency parser), and the experiment shows modest improvements over various benchmarks.  This is a borderline paper, as both methods and experiments are reasonable yet not very novel or strong. The clarity of the paper can be improved (as pointed by R1 and R4), without any mathematical notations, model details are to be interpolated from figures. The novelty is limited and experimental rigor can be improved (i.e., for many settings, gains are fairly small and no variance reported). 
A simple but sensible idea to improve VAE with good experimental results.
This work develops a novel framework for online continual learning, which they authors name Contextual Transformation Networks (CTN). This framework comprises a base network, which learns to map inputs to a shared feature representation, and a controller that efficiently transforms this shared feature vector to task specific features given a task identifier. Both of these components have access to their own memory. The optimization of the both the controller and base network parameters is framed as a bi level optimization framework.  Pros:   important and challenging problem   strong results  Cons:   Currently the writing creates the impression of limited novelty from a technical perspective. I would encourage the authors to more crisply highlight the technical novelty of their method.  
The major complaint about this paper was the lack of a proper comparison to previous work, both theoretically and empirically. Also, a study of the tradeoff between the accuracy and running time would significantly help this paper. Ultimately these were the main reasons for deciding to not accept the paper. The reviewers did think the algorithm was new and interesting, and so hopefully by addressing the complaints above, a future version of the paper could be more influential.
The reviewers agreed that there were a few issues with the current version of this work, mainly:    Some missing baselines that are mentioned in the paper, but not sufficiently compared to    Problems with the presentation that did not make it easy to understand.    Not an optimal fit with the intended audience of this conference.   
The reviewers appreciate the spatio temporal formulation of amortised iterative inference. However, the paper does not clearly state what is the end goal: if the end goal is video object segmentation, it should compared against other unsupervised object segmentation methods. If the goal is representation learning, it should evaluate the merit of the recovered representations, e.g. by fine tuning them on some downstream task. 
This paper identifies the causal factors behind a major known issue in deep learning for NLP: Fine tuning models on small datasets after self supervised pretraining can be extremely unstable, with models needing dozens of restarts to achieve acceptable performance in some cases. The paper then introduces a simple suggested fix.  Pros:   The motivating problem is important: A large fraction of all computing time used on language understanding tasks involves fine tuning runs under the protocol studied here, and the problem of fine tuning self supervised models should be of broader interest at ICLR.   The proposed fix is simple and well demonstrated. It consists of only an adjustment to the range of values considered in hyperparameter tuning (which is significant, since BERT and related papers *explicitly advise* users to use inappropriate values) and an adjustment to the implementation of the optimizer.  Cons:   The method is demonstrated on a relatively small set of difficult text classification datasets, so the behavior studied here may be different in very different dataset size, task difficulty, or label entropy regimes.  This paper was divisive, so I gave it a fairly close look myself, and I m persuaded by R1 and the other two positive reviewers: This is a classic example of a  strong baselines paper , in that demonstrates that a more careful use of established methods can obviate the need for additional tricks.  R3 raised two major concerns that they presented as potentially fatal, but that I find unpersuasive.   This paper studies stability in model performance, not stability in predictions on individual data points. R3 argues that the latter sense of stability is the more important problem. Stability is an ambiguous term in this context, and both versions of the problem are interesting. However, as the authors pointed out, the definition of stability that is used here is consistent with previous work, and is widely accepted to be a major practical problem in NLP. I don t think this is a weakness of this paper, rather, it s an opportunity for someone else to write another, different paper on a different problem.   R3 claims that the results are described as being more positive than they actually are, and the figure is potentially misleading. Looking at the quantitative results with R3 s points in mind, I still see clear support for both of the paper s main suggestions. R3 opened up some potentially important questions about the handling of outliers in particular, but these questions were raised too late for the authors to be allowed to respond, and I don t see any evidence in the paper that anything improper was done. The marked outliers are clearly much farther from the mean/median in terms of standard deviations than the unmarked outliers. So, I don t see any evidence that these concerns reflect real methodological problems.
This paper presents a hyperparameter optimization (HPO) method in which two search strategies: global and local optimizations, are effectively combined. All reviewers evaluated the proposed method positively. The experimental results clearly show the effectiveness of the proposed method, and it could be an important contribution to the AutoML research community. On the other hand, since there is no theoretical justification for the proposed method, it is not clear why the performance of the proposed method is improved so much. The author s rebuttal has alleviated some of our concerns on this point, but the further theoretical analysis is desirable.
The reviewers mostly agree that this paper presents a new deep reinforcement learning based approach to solving a challenging problem in the communications domain   wireless scheduling. However, the main concern, expressed almost unanimously, is about the novelty of the ideas in the paper beyond the assembly of existing deep RL techniques and the translation of the scheduling problem to the language of MDPs in a careful manner that respects modern communication systems standards such as 5G (e.g., URLLC and eMBB traffic demands). A secondary concern, also expressed during the author rebuttal discussion, is about adequate comparison to competing approaches motivated from the literature in wireless scheduling. In view of these issues, I suggest that the author(s) explore more appropriate avenues to submit this piece of valuable translational work, including venues that address the specific topic of wireless communication where a more comprehensive evaluation and comparison could be possible.   (NOTE: The comments and evaluation above disregard the "enhanced" draft submitted by the author(s) during the rebuttal phase. I was informed that the submission was reverted to the original draft due to space constraints being exceeded in the enhanced version.) 
This paper proposes a new generation technique for multi category marked temporal point processes.  The paper was reviewed by three expert reviewers who expressed concerns for limited novel contributions, theoretical justification, and empirical evidence. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
This paper uses a free energy formulation to develop an approach to learning "jumpy" transition models, which predict surprising future states. This transition model is used in combination with MCTS and applied to a scavenging task in the Animal AI Olympics, outperforming two baselines.  While the reviewers praised the importance of the problem tackled, and the novelty of using a free energy approach, there was a general sense amongst the reviewers that the paper wasn t totally clear (especially for an RL audience). R1 also felt that some of the claims of the paper weren t sufficiently evaluated enough, and several reviewers indicated that they felt the baselines were insufficient (or, at a minimum, not described in enough detail to evaluate whether they were sufficient). Given these points, I feel the paper is not quite ready for publication at ICLR. I encourage the authors to flesh out their analysis a bit more, better describe the baselines (and possibly compare to other existing approaches as mentioned by R4), and overall to frame the paper a bit more for the RL community.  One additional reference the authors may be interested in: Gregor et al (2018). Temporal difference variational auto encoder.
The paper makes an attempt towards byzantine resilient federated learning, in the pressneece of backdoor attacks.   The method presented combines a clustering step with a poison elimination step, and seems to be effective against a range of current attacks.   Both steps are a bit ad hoc in nature, and do not come with provable guarantees.  Moreover, the algorithms presented will have a big negative impact on personalization as several models may be incorrectly discarded during and FL round.  The authors further point in their response that " no existing defense against backdoor attacks preserves the privacy of the clients’ data." This is in fact not true, as the differential privacy defense presented by the "Can you really backdoor FL" paper is in fact fully respective of user privacy.  At the same time, the work on backdoor attacks and defenses is reminiscent of the "cat and mouse" work in adversarial examples: an attack comes out, then a defense claims to protect against it, then an attack that incorporates that defense can be made stronger, and so on. This is similar in the context of backdoor attacks.  In fact, a recent work [1] proposes that detecting backdoors is in the general computationally unlikely, rendering the generality of the proposed algorithm questionable, and also suggest a set of attacks that seem very hard to defend against. (it is fine that the authors do not reference this work as it was published just recently)  As the paper lacks significant algorithmic novelty, solid guarantees, and also is unclear whether it is universally sound, the overall contribution is limited.   [1] Wang et al. Attack of the tails: Yes, you really can backdoor federated learning, neurips 2020 https://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb Paper.pdf 
After discussion with the reviewers, it seems that a. without fine tuning the result is close to being trivial (as noted also by two reviewers) b. with fine tuning results are lower c. The setup of just a linear classification layer is less common (but exists) d. The cases where extraction succeeds the performance is low such that BERT would not even be used.  In response, the authors offer many interesting directions: a. Propose a new hybrid approach that combines learning based and extraction based methods b. Run experiments to try and support the claim that their setup of one linear layer with frozen layers is practical.  These proposed modifications are interesting and show that there is potential in this paper, but it deviates substantially from the original paper and still, caveats remain, so my recommendation is to re submit after further pursuing the new directions proposed in the response.
This paper presents some innovations to transformers allowing some significant reductions in parameter count. While some reviewers were concerned that the proposed innovations seem incremental and may not stand the test of time, all reviewers recommended acceptance after engaging in a rich and interactive author discussion. Given the clear importance of making transformers more efficient I think this paper will be of interest to the community and is worthy of acceptance at ICLR. 
The paper provides an improved analysis of the finite time convergence rate of double Q learning under more reasonable step size rules, comparing to previous work by Xiong et al., 2020.  Understanding the convergence behavior of double Q learning is an obviously interesting theoretical topic and all reviewers appreciate the authors’ improved analysis.    Several reviewers questioned the sample complexity in terms of the dependence on L (thus |S||A|); In the latest revision, the authors claimed they now refined the dependence from O(L^6) to O(L). This major change is yet to be further reviewed since the authors did not leave any clue on why/how such an improvement was attained.  Another outstanding concern relates to the theoretical comparison of the rates between double Q learning and Q learning, which remains clueless. It’s unclear whether the bound in this paper is sharp enough and whether/when double Q learning is provably inferior than Q learning.   Therefore, I am not recommending acceptance at this time, though I encourage the authors to resubmit with a more conclusive theoretical analysis.  
The authors propose a low bit floating point quantization method to reduce energy and time consumption for deep learning training. Dynamic quantization and MLS tensor arithmetic are used to enhance the effectiveness of MLS. The motivation is clear and the efficient training is an important problem to address. However, the effectiveness of proposed method is not well justified and experimental results are less convincing.  In addition, the clarify of paper still needs to be further improved. 
The approach is novel and according to the reviewers  comments addresses a relevant and important problem on EEG data analysis. Differences to related work are discussed. Methods and Experimental results are sound. The authors have provided a comprehensive response to the reviews. 
The initial reviews for this paper were very borderline. The authors provided detailed responses as well as a few additional results and observations. The authors  responses answered the reviewers  questions and addressed their main comments (including in the discussion of related works as well as with more in depth analysis in a new Section 5.1). Unfortunately, the reviewers did not come to a consensus.  Overall, this paper extends some current methodology for emotional classification, is well executed, and provides a reasonably thorough study. The results are somewhat in line with previous results from other fields (and notably NLP), but the authors demonstrate the efficacy of using primary multi task learning for multimodal conversational analysis.   Unfortunately, this paper also has some flaws as highlighted by the initial reviews. As stated above, the authors did provide a strong rebuttal, but given the different comments raised by the reviewers that spanned many aspects of the paper including motivation, possibly limited contribution and novelty, missing related work, somewhat shallow analysis of the results, I find that another full round of reviewing would be useful to assess the paper.  As a result, this remains a very borderline paper, and given the strong competition at this year s conference, I cannot recommend acceptance at this stage.  I suggest that the authors incorporate some of the discussions from this forum (and especially with respect to related work, new findings, and clearly defining the motivation and contribution of this work) into the next version of their paper.
Although all reviewers agree that this is an interesting analysis of sample efficiency in Deep RL over the past few years, there is also a consensus that it is not enough material for an ICLR paper. I also share this sentiment, which motivates the "Reject" decision. This work could have been made stronger by reproducing previous results (even partially) and sharing the corresponding code, so as to provide a fair and controlled comparison of algorithms, and setting the stage for future progress in this area. In its form, it is better suited for a presentation at a workshop than for the main conference.
After reading the reviews and rebuttal and looking over the paper, I feel that the results are indeed strong, and the paper could have an impact in terms of exploiting the relationship among action dimensions. Maybe the only detail that I would add is that going through the example given in Fig 1 completely could be useful as it might not be perfectly obvious how (e.g. considering a simple mixing function like summation) one retrieves the q values for someone not familiar with this particular topic. 
The reviews were a bit mixed, with some concerns on the novelty and experimental evaluation. While the authors  efforts during rebuttable were appreciated, the overall sentiment is that this work, in its current form, cannot be accepted to ICLR yet. Please consider revising your work based on the excellent reviews. Some more comments from the AC s independent assessment:   (a) Further elaboration on the novelty is needed. Currently the main message appears to be that if we combine two existing approaches (AT and EntM or LS) then we get better results. This is perhaps not too surprising and more elaboration on the significance would be appreciated.   (b) More comparisons in the experiments, including the SOTA performances and alternative defenses (some below).   (c) The analysis in Section 6 adds more confusion than clarification. It is clear that EntM and LS would largely decrease M_f, but why would they also decrease the Lipschitz constant even more sharply? If this explanation is useful, why not directly regularize the Lipschitz constant and maximize the margin M_f? There is in fact a large body of work on this, see for example:  1. Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation  2. Parseval Networks: Improving Robustness to Adversarial Examples  3. L2 Nonexpansive Neural Networks  4. and the many references since.  
This paper explores a very challenging problem of biased label selection and its effect on graph neural networks. It highlights that GNNs are indeed vulnerable to this issue, and then proposes a regularizer to reduce the learning of spurious correlations from the node embeddings. All of the reviews agree that the problem is relevant and important, but that there are still some outstanding issues.  It’s unclear the degree to which this problem occurs in the real world. It is also important to establish the effectiveness of the method across a range of datasets. The four datasets presented in the paper (and the rebuttal) are a good start, but the reviewers feel that more is still needed to present a convincing argument.  On the theory side, the reviewers are concerned about the linearity assumptions in the theory, and how this will translate into the more realistic nonlinear setting. Even though the authors state that they do not rely on a causal model, the paper and their responses really do seem to point in that direction. This could simply be a clarity issue, in which case I would encourage the authors to revisit this framing this to avoid confusion.  Overall, the paper is promising, but the reviewers feel that more work is needed to provide a comprehensive and convincing case. 
The authors proposed to train a large network and a small network simultaneously with a new loss function. The parameters are shared between the two networks, and the loss also incorporates the KL divergence between the outputs of the two models. In this way, the authors claim that one can train a small network with similar accuracy to the large one, while using less memory and having faster inference speed.  The reviewers think the papers is at the borderline. It has some interesting results, however, it also has quite a few problems: 1)	The technical novelty of the paper as compared to the teacher student models is not adequate.  2)	There are many missing references and baselines, since model compression has been a long studied problem. 3)	Experiments on more different NN models are preferred in order to verify the generalization ability of the proposed approach 4)	The compatibility with the pretraining framework is not very clear  The authors provided their rebuttals to the review comments. However, according the discussions among the reviewers, their concerns were not fully addressed yet and most of them would like to stand on their original scores. As a result, we do not think the paper should be accepted in its current form. 
This paper proposes an empirical method to automatically schedule the learning rate in stochastic optimization methods for deep learning, based on line search over a locally fitted model. The idea looks interesting and promising, but the reviewers have serious concerns in the lack of principled support and insufficient empirical evidences. Therefore I recommend rejection of the paper and encourage the author(s) to strengthen the idea and contribution with further theoretical and empirical study.
The paper shows convergence results for RMSprop in certain regimes. The reviews are uniformly positive about this paper and I recommend acceptance.
This work proposes to analyse convergence of episodic memory based continual learning methods by looking at this problem through the lense of nonconvex optimisation. Based on the analysis a method is proposed to scale learning rates such that the bounds on the convergence rate are improved.  Pros:   I agree with the reviewers that this is an interesting and novel perspective on continual learning  Cons:   Reviewers point out concerns/issues with the clarity of the manuscript with respect to several parts:    reviewers raise concerns with respect to the significance of the evaluation    reviewers point out that the theoretical analysis itself is somewhat standard and not novel in itself, and 2 reviewers raise concerns with respect to the analysis made  Unfortunately the authors seem to have missed the upload of the revised version. The reviewers have nevertheless considered the rebuttal by the authors and the consensus is that this manuscript is not ready yet in it s current form. 
The paper attempts at generating two types of summaries for scientific papers: summary of contribution and summary of background context. Most reviewers appreciated the motivation and found this research to be quite interesting and useful, however all reviewers had concerns regarding both execution/presentation of the ideas. While authors try to address some of the concerns, there are many clarifying questions and points raised by reviewers. Addressing all these points requires rather a major revision. Therefore the paper is not quite ready yet and would benefit from another iteration.
The main goal of this paper is to develop a new adaptive strategy to remove the need of hyperparameter fine tuning, which hinders the performance of DGS (Zhang et al., 2020) method. This paper applies a line search of the step size parameter of DGS  to reduce tuning.  A heuristic update rule of the smooth parameter in DGS (Zhang et al., 2020)) is also used.   Pros + The topic of learning hyperparameters on the fly is well motivated and an important direction to improve many existing methods. + A wide range of tasks are considered in the experiments are interesting, with a wide range of tasks considered.    Cons   Reviewers have found the contribution to be incremental and marginal. Specifically, the reviewers have expressed concerns on the impact of the work since it verifies improvement upon DGS only. The paper could be stronger by showing evidence of improving other algorithms.     In the work of [Zhang et. al., 2020], there are two parameters that require careful selection   the learning rate and smoothing radius. However, the proposed approach still relies on some hyperparameters. Although the authors claim that the method is not sensitive to these hyperparameters, it could be better justified.    The initial version lacks evaluation of the adaptive mechanism, although the authors added the comparison in the revised version.    The paper could be further improved by comparing against other hyperparameter optimization methods.   We acknowledge the detailed response and the modifications of the manuscript. We believe the paper will make a more profound contribution and impact after addressing some of the major concerns raised by the reviewers.  
This work proposes a shortest path constraint for the reinforcement learning algorithm to improve efficiency in sparse reward scenarios. The experiments are shown in navigation tasks in first person maze and grid world. Reviewers found the idea interesting and the paper well written but none of them championed the paper for clear acceptance. The authors provided a detailed thoughtful rebuttal. All the reviewers acknowledged the rebuttal followed by discussion. After considering rebuttal, review, and discussion, both AC and reviewers feel that experiments don t fully support and justify the algorithm. The main issue is that the results are shown only for the shortest pathfinding problems where the shortest path constraint is shown to be helpful. Hence, it is recommended to run it on diverse scenarios and standard benchmarks like the Atari games suite. Please refer to the reviews for final feedback and suggestions to strengthen the future submission.
This paper proposes a deterministic policy gradient method that does not require to inject noise in the action selection. Although the reviewers acknowledge that this paper has merits (novel and interesting idea, well written, technically sound), they have some doubts about the motivations for the proposed approach and about its empirical performance: a deeper analysis is requested. The paper is borderline and needs to be revised before being ready for publication.
This paper presented an online continual learning method where there may be a shift in data distribution at test time. The paper proposes a Conditional Invariant Experience Replay (CIER) approach to correct the short which matches the distribution of inputs conditioned on the outputs. This is based on an adversarial training scheme.  The reviewers found the problem setting interesting but found the approach to be lacking in novelty and problem formulation somewhat restrictive (e.g.,  requiring domain id during training). The author feedback was taken into account but the reviewers stayed with their original assessment and, even after the rebuttal phase, none of the reviewers is in favor of accepting the paper.  The authors are advised to consider the feedback from the reviewers which will hopefully help to improve the paper for a future submission to another venue.
The paper presents a method for automatically generating levels of varying complexity for training the agent.  The results are well summarized in the paper abstract, "significantly improved sample efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments." The work is clearly presented, and the experiments are thorough.   R1, R2, and R3 voted to accept the paper with 7, 6, and 7 scores. R4 voted to reject the paper with a score of 5. The reviewers mostly agree (except for R4) that significant performance gains have been achieved. R4 is unsatisfied as he/she believes that performance gains are small and exploit the simulator (e.g., using resets).   The paper s main pro is well summarized by R4 s comment, "The method of the paper is simple and can be incorporated into many existing RL algorithms."  The main drawback of the paper is that many curriculum learning techniques have been proposed in the past. E.g., Matiisen et al. (https://arxiv.org/pdf/1707.00183.pdf). In fact the authors discuss this work in the related work section, but dub it multi agent RL work. This is not true. The method of Matiisen et al. is very similar to the proposed approach but uses a different criterion for learning progress. Comparison to this work is warranted, without which the paper should not be accepted. In the post rebuttal discussion, R2 and R3 agree that this comparison is necessary. Therefore, I recommend that this paper be rejected for now and resubmitted to a future venue after incorporating a comparison with Matiisen et al.    
The paper describes very interesting work that advances the state of the art in Zork by going beyond an important state bottleneck.  While there is an important engineering contribution, the reviewers raised important concerns regarding the novelty of the question answering approach to construct knowledge graphs, the clarity of the backtracking heuristic and the extent to which the proposed approach outperforms previous work.  I also read the paper and I agree with the concerns of the reviewers.  In particular, I encourage the authors to provide more details about the backtracking procedure, a formal description of the algorithm and its assumptions to help readers apply the approach in other domains, as well as a formal analysis to better understand when it will or will not pass a bottleneck state.
The paper studies the problem of learning the step size of gradient descent for quadratic loss. Interesting theoretical results are presented, which formally support the empirically observed problems of exploding/vanishing gradients, as well as another result showing that if meta learning is done based on the validation performance, optimal performance can be achieved for a simple linear regression task.  On the negative side, there are several issues which preclude publication of the paper in its current stage:  1. The claims in the text seem to be much stronger than what is actually proved.  2. The contributions are not properly connected to the literature (e.g., the relation to Metz et al. 2013 is not properly discussed). 3. Not mentioned in the reviews, but the paper does not explore the connection to similar results coming from online learning/sequential optimization. Recently there has been a surge of papers analyzing meta learning from an online learning perspective; as an example,  Khodak et al. (2019) presents an adaptive step size tuning with guarantees for a much more general problem setting. It could also be interesting to explore if the exploding gradient problem is also related to issues with mirror descent as described in Section 4.1 of Orabona and Pal (2018). 4. The presentation in the main text does not provide enough insight about the results, as too much material is relegated to the appendix. 5. The presentation is often imprecise; it is somewhat questionable (though it is a matter of taste) if the informal theorems are useful (why call them theorems?), but Corollary 1 is not indicated to be informal, yet it is hard to interpret formally. There are other issues such as the statements of Theorems 5 and 6 where conditional expectations are used without explicitly showing the conditions, high probability bounds are stated although the error probability never appears, etc. 6. It is not clear how meta learning helps in Theorem 6 compared to methods adaptively tuning the step size (as a recent work, see, e.g., Joulani et al. 2020 and the references therein).    M. Khodak, M F. Balcan, A. Talwalkar. Adaptive Gradient Based Meta Learning Methods. NeurIPS 2019. F. Orabona, D. Pal. Scale free online learning. Theoretical Computer Science 716, 50 69, 2018. P. Joulani, A. Raj, A. Gyorgy, C. Szepesvari. A simpler approach to accelerated optimization: iterative averaging meets optimism. ICML 2020.  
I thank the authors and reviewers for their discussions about this paper. The proposed AT GAN is a GAN based method to generate adversarial examples. Similar methods (e.g. Song et al) have been proposed to use GANs to generate adv. examples more efficiently. Authors show their method has some numerical benefits. However, more experiments are needed to further justify it. Also, creating "unrestrictive" adv. examples can cause a risk of generating samples where the true label is flipped. Authors need to clarify it. Given all, I think the paper needs a bit of more work to be accepted. I recommend authors to address the aforementioned concerns in the updated draft.      AC
The paper studies benchmarking of bias mitigation methods. The authors propose a synthetic dataset of images (alike colored MNIST) that enables a controlled setup over different types of correlations between a binary sensitive attribute, dataset features, and a binary outcome label. The authors have evaluated 2K models that are the variants of three recently proposed debiasing methods using fair representation learning across various settings.  While the reviewers acknowledged the importance of benchmarking fair learning methods in a systematic controlled setting, they have raised several concerns:  (1) the proposed benchmark is too abstract/unrealistic (R4, R2, R3); it is not clear whether the findings from this benchmark can be generalized to real world data with real sensitive features, (2) the proposed benchmark is limited to pseudo sensitive attributes (R1) that are binary (R1, R2), (3) the paper lacks in depth analysis on why certain methods work under certain conditions (R3). Among these, (2) did not have a major impact on the decision, but would be helpful to address in a subsequent revision, (3) was partially addressed in the rebuttal. However, (1) makes it very difficult to assess the benefits of the proposed benchmark, and was viewed by AC as a critical issue.  The authors provided a detailed rebuttal addressing multiple of the reviewers’ concerns. AC can confirm that all four reviewers have read the author responses and have contributed to the discussion. A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. See R1 post rebuttal encouragement and suggestions how to strengthen the work. We hope the detailed reviews are useful for improving the paper. 
After reading the meta reviews and the authors comment, the meta reviewer thinks the paper is not ready for publication in a high impact conference like ICLR. The paper is not well positioned with respect to the literature, and the proposed techniques are not well discussed in relation with predominant paradigms like optimism in the face of uncertainty.
This paper introduces a pair of related regularization oriented techniques for fine tuning pretrained transformer models for NLP tasks, and shows that both are more efficient and more effective than prior work in thorough experiments on a wide range of tasks. The techniques are motivated by the idea of  representational collapse , which is defined as drops in the ability of a linear model trained on an input representation to solve tasks _other than_ the one being trained on.  Pros:   The new method is demonstrated to be broadly efficient and effective on a wide range of tasks.  Cons:   It s not clear why  representational collapse  warrants a new term, or whether it s desirable in general.   The motivations for some of the precise technical decisions behind the new methods are unclear.
This paper uses the double oracle method from game theory and applies it to GANs.  This idea is interesting and Double Oracle actually seems like a good fit to train GANs. This could lead to interesting results in the future.  Reviewers disagree on the clarity of the paper, probably because the game theory vocabulary is not something that is common among the papers published at ICLR, so extra care should be taken to explain these notions.  They also point out that the method only applies to some GANs and not all (in particular the loss needs to be zero sum). The experimental section is too weak and the metrics used need to be changed. Results are too far from the state of the art to be convincing. The authors based their experimental setup on DCGAN: this is too old, too many improvements have made since then. Other criticisms of the experimental section include: comparison to newer methods must be made, analysis and discussion of the results must be pushed further.  Generally, the average score of the reviews is too low for acceptance. The reviewers agree that the idea is both interesting and pertinent, but this paper cannot be published as it is now. The theoretical part mostly consists in applying double oracle to GANs, and the experimental section is too weak. At least one of these parts (preferably both) must be strengthened for this paper to be impactful.
This paper is right on the borderline. It questions the utility of episodic training from a novel perspective, driven by a comparison to NCA, with thorough experiments. The hypothesis that more pairwise comparisons per batch/episode benefit learning is also quite interesting, but some reviewers didn’t feel this was convincingly presented.  Prototypical networks are indeed a popular method for FSL, but I do as well think that NCA is more closely related to matching networks, and that it makes more sense for that to be the focus of experimentation. Matching networks involve more direct pairwise comparisons, and so a leave one out baseline with this model would probably be a useful comparison.  While I appreciate the desire to focus on a fundamental aspect of FSL and not chase state of the art, I think that it’s important to show where one should go from here. That is, as the reviewers pointed out there are many mechanisms beyond vanilla PNs that have yielded better results than those presented in this paper. I don’t think matching SOTA is necessary here, but it would be nice to show that the insights here complement other mechanisms in FSL. 
In this paper, the authors claim to propose a distributed large batch adversarial training framework to robustify DNN. Although the authors made efforts to clarify reviewers  concerns,  it is clear that the authors still cannot convince some reviewers in several points after several rounds of discussion between reviewers and authors.  The reviewers were not in consensus on acceptance and some concerns were still not clearly addressed in the rebuttal phase. Hence, I recommend acceptance only if there is a room.  
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper has limited novelty as there are already previous studies on setting floating point configurations. Additionally, the particular hardware setting that the authors provide seems to rely on a fp32 FMA, which defeats the purpose of a low bit floating point(where smaller FMA could have been used).Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
The authors extend prior work showing that networks trained to be certifiably robust using interval bound propagation are universal approximators. They extend prior results applicable to ReLU networks to a much broader class of networks with general activation functions.   The paper makes an interesting contribution to the literature relative to prior literature showing that one need not sacrifice universal approximation guarantees while training networks with IBP to be certifiably robust to l_inf attacks.  Since the paper is primarily theoretical, the main concern raised by the reviewers was around novelty and the theoretical significance of ideas presented relative to prior work. While proof techniques may be novel, the extension of AUA results to alternate activation functions is not surprising and do not substantially contribute to the field s understanding of learning certifiably robust networks particular since most SOTA results for IBP based training have been achieved with ReLU based networks. The authors  rebuttal did not providing convincing arguments for the reviewers to revise their scores. Hence I do not feel that the contributions of the paper justify acceptance at this time.
The reviewers were split (with all scores hovering around borderline) and I found it difficult to reach a conclusion. I like the paper, and agree with the authors that it may offer an interesting "middle ground" between bottom up and top down approaches. On the other hand, I was concerned with some of the execution flaws that were brought up in the reviews, in particular, insufficient comparisons to other embedding methods, lack of results on COCO, and to a significant degree, lack of focus in presentation. I think this could be a much stronger paper, and it will benefit from additional time to line up those missing components. To clarify the concern re: experiments on COCO, since the authors bring up computational constraints: I agree that running these experiments during the rebuttal period is not a reasonable expectation. But the conclusion is that these results should have been in the original submission! Semantic/panoptic segmentation is now a very mature area, and COCO (along with CityScapes) is one of the standard benchmarks. (BTW, "different methods often perform similarly"  on COCO and CityScapes, but not always   partially due to the significant differences in the statistics of the two datasets). I don t think it s reasonable to have a submission in this area which does not include results on it, since it makes it very hard to assess how much empirical progress is being made.
All reviews were negative for this paper, due to various issues. I think the main issue was that the experimental results were too weak to be convincing. For example, the reviewers were not sure if the differences in performance between different activations are significant. The reviewers also required more datasets and more experiments. The authors added std to results, more experiments and argued that the current datasets are sufficient, but the reviewers seemed to remain unconvinced.
The paper proposes a variational inference method for Bayesian neural networks where the approximate posterior models the correlations between the weights at all layers, using the concept of “global” inducing points.   Some concerns raised by the reviewers regarding how global inducing points allow us to capture uncertainty across the compositional structure and clarity of the manuscript have been addressed by the authors. However, the more general issue of interpretability has been left for future work.   One of the main deficiencies of this paper is that it seems to ignore other scalable approaches that also provide more complex posteriors, for example, those based on stochastic gradient Hamiltonian Monte Carlo (see, e.g., https://arxiv.org/abs/1806.05490, and references therein). Overall, although there was support for this paper, it is unclear if approaches such as those presented here are really necessary. A comparison between the two methodologies maybe not only illustrative but required.  
The reviewers and I all agree that this analysis of multi task and transfer learning from the random matrix perspective is novel and theoretically sound. While some reviewers expressed concern about the restriction to Gaussian mixtures, the strength of the explicit results undoubtedly justifies this assumption, and the generalization to concentrated random vectors significantly mitigates any concerns. I recommend acceptance.
The authors study bias amplification [Zhao et al, 2017] and propose an improved metric for measuring it. The authors also discussed normative issues in bias amplification (predicting a sensitive feature), and how to measure amplification when we do not have labels, or where labels correspond to uncertain future events. While the reviewers acknowledged the importance to study bias amplification, normative measures and social context, they raised several important concerns:   (1) limited technical contributions (R3 and R4)   see reviewers’ concerns that the metric is the only technical contribution; one possible suggestion is to propose a mitigation strategy based on the proposed metric similarly to [Zhao et al, 2017];   (2) ‘the usage of error bars because of the Rashomon effect seems incomplete and almost trivial’ (R3, R4), ‘lacks a proper grounding’ (R1)   see two suggestions by R3 how to revise; these suggestions have not been discussed in the rebuttal;   (3) empirical evidence lacks a controlled scenario of tuning the bias source to evaluate consistency (suggested by R3) and is limited in the context of algorithmic fairness benchmarks (R4)   this has been partly addressed in the rebuttal;   (4) normative contributions and broader discussions are oversimplified – see R3’s comments and suggestions on how to better position the paper.  Among these, (3) and (4) did not have a major impact on the decision but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed work and were viewed by AC as critical issues.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs technical strength, more empirical studies and polish to achieve the desired goal. We hope the detailed reviews are useful for revising the paper. 
The paper offers a direction for mult agent RL that builds on results for actor critic methods [Zhang, ICML 2018], extending that work to address deterministic policies.  The authors establish convergence under a number of assumptions.   Both on policy setting and off policy settings are treated.  The reviewers point out several concerns and the consensus seems to be that, while the direction looks promising, the paper deserves further work. 
This paper received 4 reviews with mixed initial ratings: 4,5,6,7. The main concerns of R2 and R4, who gave unfavorable scores, included lack of clarity around design choices and inconclusiveness of some of the experiments. In response to that, the authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews, which seemed to have addressed these concerns. R2 and R4 upgraded their scores and recommended acceptance. As a result, the final recommendation is to accept for presentation at ICLR as a poster.
This paper proposes applying AlphaGo Zero style ideas for solving combinatorial optimization problems over graphs with two changes:  Replacing CNNs with graph neural networks Normalization of rewards  The reviewers raised valid points about this paper. 1. Lack of technical novelty 2. Experimental results are not strong enough to draw meaningful conclusions 3. Since techniques are mostly off the shelf, extracting general knowledge, insights and conclusions from empirical evaluation is important, but unfortunately was missing.  I agree with the review comments. Overall, my assessment is that the paper requires more work  before it is ready for publication.
All reviewers recommend acceptance. The authors have addressed several of the reviewers  concerns in their comments, conducted additional experiments, and updated the manuscript accordingly.  A concern was raised regarding the size of the dataset introduced and used by the authors for this work. However, I agree with the authors that it doesn t necessarily make sense to compare this to datasets designed for training video classification and/or generation models; In the compression setting, the quality of individual data points matters much more than their quantity, as the authors argue.  Reviewer 2 was curious about the potential of a pre trained optical flow module. I believe the authors have convincingly argued that end to end learning is likely to be more effective and practical (and indeed, there is plenty of evidence for this in other ML contexts where training data is not scarce). I agree that a direct comparison in the paper would have been interesting, but this would constitute a significant investment of time and effort on the authors  part (as they also point out, training such a module separately could actually be more difficult), and I think it would be unreasonable to make this a condition for acceptance.
The paper is concerned with improving the scalability of GCNs which is an important problem and relevant to the ICLR community. For this purpose, the authors propose a new distributed training method for GCNs which uses a boundary sampling strategy to reduce the number of boundary nodes. The paper is written well and, overall, good to follow. Reviewers highlight the promising improvements in throughput and memory footprint as well as and the possibility to avoid potential loss of information compared to other methods.  However, currently there exist still concerns around the manuscript. Reviewers raised concerns with regard to the novelty of the method (straightforward combination of existing techniques) as well as the experimental evaluation (overhead of METIS, full batch accuracy, edge boundary vs node boundary, etc.) The revised version addresses some concerns (comparison to DropEdge, p 0, etc.) and clearly improves the paper. However, given the aforementioned issues and the absence of strong support from reviewers, I agree to that the current version would require an additional revision to iron out these points. The presented results are indeed promising and I d encourage the authors to revise and resubmit their work considering the reviewers  feedback.
This paper proposes a deep reinforcement learning algorithm Supe RL that combines model free RL with genetic updates. The idea is to periodically mutate and evaluate the actor and greedily choose the best performing child, and incorporate it in the main actor via Polyak averaging on a target policy network. The algorithm can be in principle combined with any gradient based deep RL method. Supe RL was demonstrated by combining it with Rainbow and PPO and evaluated in navigation tasks as well as standard MuJoCo benchmarks.  Overall, the reviewers found the idea interesting and to have value to the RL community. The reviewers raised some questions regarding technical rigor, evaluations, and the choice of base DL algorithms. As is, I find this a slightly above borderline submission, and thus recommend acceptance. However, I would encourage the authors to test their method also with a state of the art off policy algorithms, such as TD3 or SAC, in continuous domains, to better calibrate its overall performance.
The paper considers the problem of learning a new task with few examples by using related tasks which can exploit shared representations for which more data is available. The paper proves a number of interesting (primarily theoretical) results.
This paper got mixed reviews. One for acceptance, one for reject and two borderline. After the rebuttal, AR2 raises the review to borderline.  AR1 gives the highest recommendation but did not provide detailed supporting evidence. Other reviews provide comment on the strength and also share the concerns. Most of the concerns concentrate on the motivation (whether the proposed method is violating the objective of knowledge distillation) and the brought additional computation overhead. Also the scope of this paper was defined wider than the actual one. The authors only did experiments for BERT but did not consider and compare with existing KD method. Overall, AC read the paper and also has the similar concerns, the novelty is limited. the brought increase in inference time is violating the KD objective and the scope of this paper was not defined clearly. The authors should improve the submission in these aspects. At its current status, AC does not recommend acceptance. 
The paper s stated contributions are:  (1) a new perspective on learning with label noise, which reduces the problem to a similarity learning (Ie, pairwise classification) task  (2) a technique leveraging the above to learn from noisy similarity labels, and a theoretical analysis of the same  (3) empirical demonstration that the proposed technique surpasses baselines on real world benchmarks  Reviewers agreed that (1) is an interesting new perspective that is worthy of study. In the initial set of reviews, there were concerns about (2) and (3); for example, there were questions on whether the theoretical analysis studies the "right" quantity (pointwise vs pairwise loss), and a number of questions on the experimental setup and results (Eg, the computational complexity of the technique). Following a lengthy discussion, the authors clarified some of these points, and updated the paper accordingly.  At the conclusion of the discussion, three reviewers continued to express concerns on the following points:    *Theoretical justification*. Following Theorem 3, the authors assert that their results "theoretically justifies why the proposed method works well". The analysis indeed provides some interesting properties of the reduction, such as the fact that it preserves learnability (Appendix F), and that the "total noise" is reduced (Theorem 2). However, a complete theoretical justification would involve guaranteeing that the quantity of interest (Ie, the clean pointwise classification risk) is guaranteed to be small under the proposed technique. Such a guarantee is lacking.      This is not to suggest that such a guarantee is easy   as the authors note, this might involve a bound that relates pointwise and pairwise classification in multi class settings, and such bounds have only recently been shown for binary problems   or necessary for their method being practical useful (per discussion following Theorem 3). Nonetheless, without such a bound, there are limits to what the current theory justifies about the technique s performance in terms of the final metric of interest.    *Comparison to SOTA*. Reviewers noted that the gains of the proposed technique are often modest, with the exception of CIFAR 100 with high noise. Further, the best performing results are significantly worse than those reported in two recent works, namely, Iterative CV and DivideMix. The authors responded to the former in the discussion, and suggested that they might be able to combine results with the latter. While plausible, given that the latter sees significant gains (Eg, >40% on CIFAR 100), concrete demonstration of this point is advisable: it is not immediately apparent to what extent the gains of the proposed technique seen on "simple" methods (Eg, Forward) would translate more "complex" ones (Eg, DivideMix).     In the response, the authors also mentioned that (at least the initial batch of) the experiments are intended to be a proof of concept. This would be perfectly acceptable for a work with a strong theoretical justification. However, per above, this point is not definitive.    *Creation of Clothing1M*. The authors construct a variant of Clothing1M which merges the classes 3 and 5. Given that prior work compares methods on the original data, and that this potentially reflects noise one may encounter in some settings, it is advisable to at least report results on the original, unmodified version.    *Issues with clarity*. There are some grammatical issues (Eg, "is exact the"), typos (Eg, "over 3 trails"), notational inconsistencies (Eg, use of C for # of classes in Sec 2, but then c in Sec 3.1), and imprecision in explanation (Eg, Sec 3.2 could be clearer what precise relationships are used from [Hsu et al. 2019]).     These are minor but ought to be fixed with a careful proof read.  Cumulatively, these points suggest that the work would be served by further revision and review. The authors are encouraged to incorporate the reviewers  detailed comments.
During discussion, the reviewers acknowledge improvement of the revised version of the paper through author rebuttal and agree with that the paper is overall well written.  However, the novelty of the paper is not strong, and reviewers share the concern that distinction between self supervised learning and deep clustering is not convincing. Also, the concerns (2) raised by the Reviewer #2 is important, which is about the effectiveness of the proposed two step procedure: first apply t SNE to transform into two dimensions, followed by running K means, while the answer is not well  justified. In my opinion, parameter sensitivity should be studied more carefully. The authors mention that $k$ and $\kappa$ have little effect on clustering performance, while this could imply that the associated terms do not have impact on the proposed method. This point should be clarified further.  Overall, the paper is still not ready for publication, I will therefore reject the paper.
The consensus view was that the reviewers were not convinced that the analysis done in the paper was sufficient motivated.
This paper received 2 borderline accepts, 1 accept, and 1 reject.  In general, there is broad agreement that this is solid experimental work and that the differences found between recognition and viewpoint estimation were interesting.  The main issue brought up by the more negative reviewer is that some of the experiments are subject to interpretation (specifically the extrapolation problem could become more of an interpolation problem as the number of training examples increases). This issue is acknowledged by the other reviewers who nonetheless see some value in the paper being published and potentially paving the way for additional studies. My suggestion for the authors is to prominently discuss this issue in a revision of this paper. Unfortunately, because of space constraints,  I have to recommend this paper be rejected.
The paper introduces an approach to counterfactual fairness based on data pre processing, and compare it to other two counterfactual fairness approaches on the Adult and COMPAS datasets.  The reviewers are in agreement that, in its current state, the paper should not be accepted for publication at the venue. Their main concerns are around the metric used to measure fairness, and these were not resolved during the discussion. The reviewers would have also appreciated more experiments on real world datasets to get a more comprehensive comparison of the methods. Finally,  discussion and comparison with other methods to achieve counterfactual fairness from the literature were limited. 
The average review rating is 5.5 which means it’s somewhat borderline. One of the reviewers planned to increase the score but apparently didn’t do so formally. A subset of the main pros and cons the reviewers pointed out are:   Pros:  “Some empirical support is provided for the theory.” “ It is particularly interesting that the authors show that the second order effect of the SGD noise in the Hessian induces a power law distribution over the iterates.”  Cons:  “The escaping efficiency of the power law dynamic is only analyzed in low dimension case. ...” The author responded that Theorem 7 proves the multi dimensional case. But the AC noted that it’s very likely that escaping time is exponential in dimension (because kappa needs to be larger than d as the author noted and the det() might also be exponential in d. The author did say in the revision that the dimension should be considered as the effective dimension of the hessian, but the AC couldn’t find a formal argument about it.) “The assumptions made are somewhat strong and may not hold in some cases...”  The reviewers also had a few clarity questions which the author addressed in revisions with re organized writing. The AC weighed the pros and cons and found that the unclarity and potential exponential escaping time in the multi dimensional case outweigh the pros.     
Originality: The paper can be developed into a very nice contribution, if the value of the newly introduced optimization variance is evaluated more thoroughly (e.g., through simple theory, or through more rigorous experiments).  Main pros:   One of the early works studying epoch wise double descent   Optimization variance is an interesting concept and might very well be useful for finding good early stopping points.  Main cons:   Findings about epoch wise double descent remain inconclusive   optimization variance is not sufficiently evaluated to judge its usefulness: theoretical justification for why is this an important quantity and when does it arise naturally is something missing at this point.  Overall: there was a consensus that the paper focuses and provides an interesting story, with new ideas; however, paper s conclusions are not strongly supported by experiments; more experiments are needed to make arguments conclusive. 
Three out of four reviewers are positive about the paper after the author response and during the discussion.  Strengths include * The proposed method for parameter reduction in transformers allows end 2 end learning cross modal representations especially on long videos, which has not been possible before * Good performance on audio and video understanding * Extensive set of ablations  Concerns include a somewhat incremental nature of the paper and the still large computational resources to run the experiments. I think, both, the ideas and results are interesting to the community and recommend accept.
This paper introduces All Alive Pruning, an approach which checks for and removes the connections to and from units with zero gradient ("dead" units). The method is shown to improve performance of IMP at extreme (>128x) compression ratios on MNIST, CIFAR 10, and Tiny ImageNet. All reviewers felt that the problem the authors study   how to identify and remove dead units   is an interesting one.   However, there were concerns about the practical utility of the method, given that AAP only improves performance for extreme compression ratios in which performance is already substantially degraded relative to unpruned models. I share these concerns, which mute the practical impact of this work. There were also concerns about a lack of proper baseline comparisons to more simple approaches to removing dead units. As mentioned by R1, given that the problem the study is an interesting one, the paper could make up for the lack of practical utility by providing detailed analyses of the settings in which dead units emerge, differences among pruning approaches, etc., but analyses provided here are limited.   I would encourage the authors to explore these areas in a future revision of the paper, but recommend that the paper be rejected in its current form. 
This work is well written and easy to follow and proposes a novel framework to utilize unlabeled output data. The authors have also given a detailed proof that the denoiser reduces the required complexity of the predictor. However, ultimately the experimental results are somewhat weak and leave doubts as to how effective the approach is. More convincing experimental results such as significant improvements on a well understood task and acknowledging that the approach is mostly useful when combined with pre training and back translation would improve the work.  Pros   Well written.   Technically novel approach to the problem of utilizing unlabeled output data.   Interesting proof on the reduced complexity requirement for the predictor.  Cons:   Experimental results are not convincing. Showing significant improvements on a well understood task would be more convincing.   The approach is only really useful when combined with pre training or back translation.
The paper proposes a method for using multiple word embeddings in structured prediction tasks. The reviewers shared the concerns that the method seems rather specific to this use case and the empirical improvements do not justify the complexity of the approach. They also questioned the definition of the method as "architecture search" vs a particular ensembling method. Finally, I think the authors should provide more discussion of why using all the embeddings (in the sense of bias variance tradeoffs).  
This paper introduces a method for hierarchical classification with deep networks. The idea is interesting, and as far as I know novel: namely, the authors add a regularizer to the last layer in order to enforce a hierarchical structure onto the classifiers. The idea of placing spheres (with a fixed radius) around each classifier and forcing the child classifiers to lie on these spheres is quite clever.  The reviewers have pointed out some concerns with this paper. Some had to do with terminology (which the authors should fix but which is no big deal), but the main weakness are the experimental results and the ablation study. The reviewers were not convinced that the optimization in the Euclidean space wouldn t be sufficient. A more thorough ablation study could help here.   This is the kind of paper that I really want to see published eventually, but right now isn t quite ready yet. If you make one more iteration (in particular adding a stronger ablation study) it should be a strong submission to the next conference. Good luck!
Overall the reviewers had various positive things to say about the paper, including that it was well written and easy to understand, topical, that the method was sensible, novel and interesting and that the computational efficiency (i.e. real time) was appealing.  However, all the reviewers thought it wasn t quite ready for acceptance, mainly citing concerns with the empirical evaluation.  It seems they had trouble interpreting the empirical results and placing the work with respect to other relevant methods.    It seems in the author response, the authors did much to add to the experiments, but ultimately the reviewers were not comfortable with acceptance.  Taking the reviewers  feedback into account and adding the desired empirical evaluation would make this a much stronger submission to a subsequent conference.
This paper discusses a method to update/optimise invertible matrices via low rank updates. The key property of the proposed method is that it keeps track of the matrix inverse and its determinant through the optimisation (with updates that are much cheaper to compute than a direct inversion/determinant computation).  While the method of performing low rank updates for invertible matrices itself has already been extensively studied in the literature as pointed out by reviews, this work focuses (after extensive revision) on the properties of this update method.  Since the updates may leave the manifold of invertible matrices, a numerical stabilisation step was introduce whereby updates that produce ill conditioned matrices are rejected during optimisation.  Rank one updates allow for fast update of matrix inverse and determinants. So this is particularly interesting when applied to normalising flows, as it allows for cheaper computation of the log det Jacobian terms.    The novelty of this approach is rather limited (as also pointed out by R2). The experiments and, in particular, the application to normalising flows are interesting, well executed. It is not clear if there are advantages of the method in other domains where log det Jacobians are not necessary relative to existing literature. 
This paper proposed a novel Adversarial Deep Metric Learning approaches. The reviews pointed out the paper proposes an interesting idea and it is among the rare works that address directly robust metric learning which an important topic for efficient metric learning.  Some concerns were raised about the analysis and the lack of comparisons notably with other types of adversarial attacks.  The authors provide a rebuttal where they addressed some concerns raised by reviewers with some precisions on the work, its positioning with respect to other related papers and additional comparisons notably with other types of attacks.  A minor remark: there is a typo in Eq(13), where the $z$ in the loss function is actually not defined and should be included in the max function. That being said, the contribution is still limited in considering only the infinite norm, analysis and comparisons to prior work remain weak. The paper does not meet the requirements for acceptance to ICLR in its current form. I have then to propose rejection. 
The paper proposes a method for solving challenging sparse reward problems by performing task reduction followed by self imitation learning from solution trajectories to the reduced tasks.  The core innovation seems to me to be the uses of the reduction search, which is essentially a form of recursive subgoal selection, but where the subgoals are sure to be achievable as assessed by leveraging the learned value function.  This idea seems rather general, though its use is strongly facilitated in this paper by definition of the space (i.e. that object target is the space, is pre specified; there is only one, rather limited result on a pixel based task).   Note: Another submission to this conference also explores a quite similar idea to the task reduction proposed in this paper   see "Divide and Conquer Monte Carlo Tree Search".  The breaking down of the problem into sub problems using the value function is similar, but the details of how the papers proceed from there is quite distinct.  This is a difficult meta review decision due to the fairly mixed reviews, coupled with limited engagement in the discussion phase.  Two reviewers felt the paper was solid and could be accepted (R1 and R2 with scores 7 and 6 respectively).  R3 gave a borderline review that leaned towards reject (score 5).  R3 replied to the initial author response, which provided helpful feedback to the authors. Ultimately, in my assessment, the authors did a fairly thorough job of addressing some of the points raised by R3, including by adding an additional comparison even where they didn t agree with the reviewer. R4 assigned the paper the lowest score of 3.  The authors provided a lengthy reply to this review asserting that the review may have reflected misunderstanding of paper details, but the reviewer did not respond to the authors.      Two core issues raised about this paper relate to the definition of the space for subgoals and the limited difficulty of the tasks. However, this method does not claim to be entirely ignorant of the task space so I don t see the fact that they do include some domain knowledge in designing the goal space to be totally undermining of the method.  They focus on the complementary issue of how to break down difficult problems into sub problems.  While it would be considerably more impressive if the goal space were learned, I think this harder version of the problem remains a fundamental and deep problem within AI, so it seems to me too much to ask of the present paper (especially given that it was not the stated focus of the paper).  And while the tasks explored in the paper are a little contrived (some repetitive motifs and designed with a relatively small search space over subtasks), these problems do have some complex structure.  Compared to many works in this field, I applaud the authors for engaging with problems with both long horizon task structure as well as complex high DoF continuous control component.  While I agree with some of the concerns raised, my overall assessment is that I find the contributions sufficiently innovative and substantial to justify acceptance.  The authors proposed a specific innovation and evaluated that innovation.  Insofar as their innovation is somewhat general, I don t think this paper can be the last word on how well it compares with the diverse approaches it could be set against.  And while the experiments are not definitive, I do think they do constitute a fairly ambitious initial validation of the core idea.   
All three referees have provided detailed comments, both before and after the author response period. While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses). The authors are encouraged to further improve their paper for a future submission.
This paper proposes an approach to training language instruction following agents that aims to improve their compositional generalization., by means of an entropy regularization method to reduce redundant dependency on input.  All four expert reviewers agreed that the paper is not ready for publication in its current form. Of biggest concern is the fact that the reviewers could not interpret the exposition of the method, so were unable to be sure exactly how the method worked. This can be addressed in a future submission by clearer presentation.   Another concern was that the authors only consider a single benchmark, and fail to situate the work relative to other grounded language learning tasks and datasets. Thus, reviewers were concerned about the generality of the method, and suspected it may be too specific to the gSCAN setup.   That said, the reviewers were all impressed by the strong results on the gSCAN benchmark. It strikes me that there is some interesting insight here that can be derived from this impressive performance, that may also be applicable to other grounded language learning settings. However, to make the paper acceptable for publication the authors must do a much better job of communicating how their method works, what that specific insight is and how it is relevant beyond the gSCAN dataset (ideally via direct experimentation in other settings).
This paper presents a new approach to grounding language based RL tasks via an intermediate semantic representation, in an architecture called language goal behavior (LGB).  The architecture permits learning a mapping from internal goals to behavior  (GB) separately from learning a mapping from language to internal goals (LG), and prior to flexibly combining all three (LGB).  The architecture is studied in a specific implementation called DECSTR.  The architecture has multiple desired attributes including support for intrinsic motivation, decoupling skill acquisition from language grounding, and strategy switching.  The experiments demonstrate the utility of different components in the architecture with a variety of ablation results.  The reviews initially found the paper to be poorly organized with required content described only in the appendix (R1, R2, R4), with unclear main contributions (R1, R2, R4), and with results restricted to demonstrations (R3).  Despite these reservations, the reviewers found the content to be potentially relevant though narrow in scope.  The authors substantially revised the paper. They improved its organization, clarified contributions, separated the architecture from the specific examples, and improved the experimental baselines.  After reading the revised paper, the reviewers agreed that the paper s organization and insights were improved, making the new paper s contribution and insight clear.  The experimental baselines were also improved, providing more support for the potential utility of the proposed method.  Three reviewers indicate to accept this paper for its contribution of a novel approach to grounding language and behavior with an intermediate semantic representation. No substantial concerns were raised on the content of the revised paper. The paper is therefore accepted.
This paper proposed an augmentation construction to mitigate the double descent. For any pairs of data points, the constructed input is simply concatenation of two inputs and the constructed label is the average of their corresponding labels. The authors further empirically show that this would mitigate double descent.  Reviewers unanimously like the main idea of the paper but they have other major concerns about this work. The main concern is that we already know double descent is not a practical issue since it can be mitigated by early stopping or proper regularization (Nakkiran 20 ). Therefore, the main benefit from this paper could come from a better understanding of double descent using the observations from this construction. However, the paper does not provide us with insightful theoretical or empirical findings beyond the main observation. There are a couple of other concerns as well about discussions around #samples and the fact that the proposed construction is not i.i.d. and also lack of proper discussion about the relationship between the proposed construction and regularization techniques.  Unfortunately, authors did not responded to reviewers concerns. Nonetheless, I encourage authors to read reviewers  specific feedbacks, incorporate them and resubmit their work.  Given the above concerns, I recommend rejecting the paper.
The authors present a model based method for cooperative multi agent reinforcement learning and propose to use communication of future predictions (as given by a learned world model) as a way to overcome partial observability.  Overall, all reviewers found this work to be of great interest and the combination of planning + communication novel. However, all reviewers pointed that the claims that the papers makes are not fully supported by the experimental framing of the paper pointing to several shortcomings around experimental design in general and better control of appropriate baselines. The authors have since clarified several aspects in their paper and also included a new RL environment.   However, as the paper still stands does not fully provide convincing evidence of their proposal, which is however very intriguing. I would like to echo though reviewers  suggestions that the authors work a bit more on the experimental design and I really hope this work will appear at a later venue.
The paper presents a method for future trajectory generation. The main contribution is in proposing a technique for data augmentation in the latent space which encourages prediction of trajectories that are both plausible, but also different from the training set. The results clearly show superior performance on standard benchmarks. The evaluation is thorough and ablations show that the proposed innovation matters.   R2, R3, R4 recommend that the paper be accepted with scores 6, 8, and 6 respectively. R1 recommends the paper be rejected with a score of 5. The main concern of reviewers are:   R1: " In summary, the paper suffers from lack of a clear justification of the proposed contributions, unfair evaluations, and questionable significance of the results." The authors addressed this concern in their rebuttal.    R2: "Some other points remain still open such as the limited focus on Trajectron in evaluations." Since trajectron is a recent SOTA, I think this is not a big concern. Authors compare against other baseline methods too.   R4: Comparison to Mercat, Jean, et al., ICRA 2020 is missing. The authors mention that their code is unavailable and therefore cannot compare.    R4: "underlying reasons for the success of different components (classification of latent intent and hallucinative latent intent) are hard to explain". I agree with this and this is also my major concern which I detail below.   The paper proposes to find diverse trajectories by generating two latent vectors: z, z . The first h time steps are generated by latent vector z and the remainder using z . The generated trajectory is evaluated by a discriminator that ensures plausibility. The latent vectors are chosen to be discrete and a classifier is trained to recognize z from ground truth trajectories. To encourage diverse trajectories, authors use a loss that encourages mis classification of the latent variable inferred from the generated trajectory. Since the generated trajectory cannot be classified well, it is assumed to be different from the training set.   This formulation is rather adhoc. If the trajectory is indeed different from the training distribution, then it will also fool the discriminator. If it doesnot, then it s not very different. The mis classification, is akin to encouraging high entropy in the z space inferred from predicted trajectories. With this view, it is possible that there is no need to generate two latent vectors z, z , but simply generate one and use the entropy penalty. I would love to see this experiment and see the authors demystify their method. It would also lead to significant changes in writing. Even now, writing needs improvement. Due to the proposed method being a adhoc trick, that is not well justified, I would normally not recommend acceptance. However, the empirical results are strong, tilting the recommendation to acceptance.
While the paper s topic is on a topic of interest and presents an evaluation on three synthetic datasets,  PartNet Chairs, Shop VRB Simple, CLEVR dataset, several concerns and weaknesses remain after the author response.  Main Concern and Weaknesses: * The main improvement comes from the additional supervision provided by language, which provides a strong supervision signal as the language is scripted and the parser has nearly "perfect accuracy (>99.9%) on test questions/captions", as the authors state. * Limited contribution: combination of MONet/Slot Attention with NS CL;  * Experiments limited to synthetic images with no background (relatively easy to segment) and synthetic (templated) language (easy to parse). This is especially concerning when the task is segmentation and the supervision comes from templated language, making it a strong supervision signal. * The positive impact of the objectness score on performance was not sufficiently demonstrated * Additionally, in the final discussion phase, reviewers were concerned that the with limited visual reasoning training on a subset of 25% of the data, reduces the performance [I note that I did not take this as the decisive reason for rejection as the authors did not have a chance to respond to this concern but the authors should discuss this in any future version of the paper]  The paper initially received borderline and reject scores and the authors took significant effort to address several of the comments of reviewers. While the paper was improved several of the main concerns remained and reviewers recommended reject after reading the author response and each others comments.  I agree to the concerns and recommend reject.
The reviewers all agree that the problems studied in this paper are interesting, and the solutions provided are reasonable.  However qualitative and quantitative comparisons to state of the art methods are missing, and the sensing model assumed by the paper needs to be more well motivated. 
The reviewers are in consensus that this paper is not ready for publication: cited concerns include simple (interesting) ideas but need to be carefully analyzed empirically, contextualized (other similar studies exist), identifying convincing empirical evidences,. etc.   The AC recommends Reject.
The authors proposed to pre process the original input features into a low dimensional term and its corresponding residual term via SVD. The paper empirically demonstrated the neural networks trained on such factorized exhibit faster convergence in training. Several issues of clarity were addressed during the rebuttal period by the authors.   However, the reviewers still felt that there were some remaining fundamental issues with the paper,   1)  The motivation is not echoed in the experiments, namely most of the experiments on CIFAR and CatDog dataset using a low dimensional factorization of d 1 which is trivial and often part of the whitening preprocessing.   2) The proposed factorization via SVD will be difficult to scale up to high dimensional features, large training sets and higher d >> 1.    3) The empirical experiments show a marginal improvement in the training speed, especially in the image recognition tasks, yet there seems an early plateau in test performance when compared to the baselines.  4) The theoretical analysis in Section 2 studied linear models. Yet, the rest of the paper focuses on non linear neural networks. It is difficult to see the connection between the analysis and the rest of the paper.   Thus, I recommend rejection of the paper at this time as the current version of the paper needs further development, and non trivial modifications, to be broadly applicable.
The authors propose a new dataset to evaluate the robustness of image classifiers. The dataset consists of data from three sources: a crowdsourced dataset collected by the authors called ImageNet Renditions, images from Google street view, and data sampled from DeepFashion2. This new dataset allows the authors to test robustness to different renditions of an object (e.g. artistic depictions of an object category) and robustness to changes in geography and camera type. In addition, they propose a new augmentation strategy called DeepAugment which consists of an encoder/decoder style network that transforms the appearance of the input image by simply applying different random perturbations of the weights of the augment network. Robustness results are presented on the previously described datasets where the proposed augmentation strategy in combination with an existing approach (AugMix) performs best in some cases. However, the results are not convincing and AugMix often outperforms the new method.    In general, the authors did a good job addressing many of the comments (e.g. they provided more detail about how ImageNet R was collected), but there were still several lingering concerns. R4 was the most positive about the paper, but unfortunately was one of the least vocal during the discussion. R1 was concerned that the paper did not do a great job of defining what was meant by robustness. This AC doesn t agree fully with their concerns, but does agree that more care could have been taken to position the paper better in light of the existing datasets that are already available (see R1’s comments). As the reviewers and authors note, collecting new datasets is a lot of work so care should be taken to ensure that this is not duplicate effort. The authors addressed these concerns in their response to some extent, but more discussion is needed in the paper.   There was a lot of discussion between the authors and reviewers about this paper. The new dataset has a lot of merit, but there is some concern that the paper does not do a great job of clearly presenting its findings and conclusions. In addition, the proposed augmentation technique is slightly underwhelming performance wise and not very clearly described in the main paper. R2 sums up the opinion of this AC: “I think this work is interesting and is in principle asking the right questions. However, the analysis and conclusions currently do not provide robust and generalizable insights that advance the field.” There is clearly a lot of promise here, and the current recommendation is a weak reject. The authors are strongly encouraged to take the detailed feedback they have received on board and to revise the paper to further improve it for a future submission.  
The AC, the reviewers, and the authors had many discussions about the results in the paper during the discussion period. Below is a brief summary.   1. The paper shows that with $O(N^{⅔})$ parameters, a feedforward neural network can memorize $N$ inputs with arbitrary labels if the inputs satisfy some mild assumptions.   2. AC brought up in the discussion phase two central questions (one of which has been raised by R3 as well)   a. The results rely on using the infinite precisions of real values in the neural networks. The results wouldn’t hold if the precision of the neural nets is finite. The subtlety about the infinite precision was not prominently discussed in the paper.   b. It’s unclear to the AC what’s the practical implication of the results to generalization or optimization. In particular, it’s unclear to the AC what a finite sample memorization result within infinite precisions would entail. The AC thinks there is a fundamentally big difference between expressivity and finite sample expressivity. Expressivity is a very important topic to study, whereas the motivation for studying finite sample expressivity with infinite precision is unclear. (This is raised by R3 in the reviews as well.)  3. R1 supports the paper with the following main points (The AC rephrased these with some approximations, and might misinterpret to a certain degree.)  a. The paper’s result is surprising and mathematically non trivial.  b. Memorization is an important question to study. Many prior works study it, e.g., for showing tight VC dimension bound. It can be considered as an established setting.   c. Relying on the infinite dimension is not uncommon in ML theory.   4. R2 does not object R1’s point 3a, but seems to have a reservation to strongly recognize the technical significance of the results because it seems potentially likely to obtain the results by combining existing methods. Both R2 and the AC had some (partial) arguments to obtain the results of the paper with non standard architecture or non standard activations (which doesn’t subsume the paper’s results because the paper uses standard activations and feedforward net). This does make the AC unwilling to strongly recognize the technical significance of the result, but the AC doesn’t think the results are trivial either. In any case, this issue is not among the main concerns of the AC.   5. Regarding 3b, the AC thinks that unlike the prior work, the memorization results in this paper do not have an implication to the VC dimension (and in return, the dependency on $N$ is better), and this makes the significance and impact of the result in this paper somewhat unclear.  6. In summary, because the paper’s average score is somewhat borderline and because the AC has the concern 2a and 2b and was not quite convinced by the R1’s points or the authors’ responses, the AC is recommending rejection for the paper. The AC personally thinks the paper’s result has a strong potential and with additional clarification for the subtlety in 2a and additional results on the connections to generalization or optimization, the paper can be a strong one for future ML venues.  
While the updated version of this manuscript did motivate one reviewer to give the paper a marginal accept rating, all other reviewers really felt that the paper could use more work along the lines of their suggestions. The aggregate view of the reviewers is just not positive enough at this time to warrant an accept recommendation by the AC at this time. The work does seem to have promise and the authors are encouraged to continue to improve the paper for another round of peer review elsewhere.
This paper gives a method of performing experimental design (one round active learning) in overparameterized regression. Although the comparison with the coreset method baseline is a nice addition, the reviewers still have concerns in the following aspects:   The novelty compared to the classical v optimality design is limited   The hyperparameter t is hard to choose in practice. This is important because each different hyperparameter setting would induce a different set of examples for label queries.    Computational complexity   It is unclear exactly how the proposed method (or other experimental design method) can mitigate double descent   We encourage the authors to take these into account in the revision.  
This paper proposes a new differentiable physics benchmark for soft body manipulation. The proposed benchmark is based on the  DiffTaichi system. Several existing reinforcement learning algorithms are evaluated on this benchmark. The paper identify a set of key challenges that are posed by this specific benchmark to RL algorithms. Short horizon tasks are shown to be feasible by optimizing the physics parameters via gradient descent. The reviewers agree that this paper is very well written, the  problem tackled in it is quite interesting and challenging, and the use of differentiable physics in RL for manipulating soft objects quite intriguing.
The paper analyzes neuron activations for neural networks trained via RL to perform reaching with planar robot arms. This analysis includes an evaluation of the correlation between neurons of different models trained to control arms with different degrees of freedom. In performing these evaluations, the paper proposes a heuristic pruning algorithm that reduces the size of the network and increases information density. Correlation is assessed based on a projection of the source network on the target network.  The paper is well written and considers a challenging problem of interest to the community. The proposed pruning strategy as a means of maximizing information content is reasonable and seems to perform well. However, the significance of the contributions is limited by the experimental evaluation. The experiments consider a large number of models, however the scope of problems on which the method is evaluated is narrow, making it difficult to draw conclusions about the merits and significance of the work. The authors are encouraged to extend the analysis to a more diverse set of problems.
This work studies statistics of ensemble models that capture the prediction diversity between ensemble members.  The goal of the work is to identify or construct a metric which is predictive of the holdout accuracy achieved by the ensemble prediction.  Pros: * Studies empirically how measures of ensemble diversity relate to ensemble prediction accuracy. * Proposes improvements to diversity metrics that correlate better with accuracy.  Cons: * Unclear/confusing presentation. * Limited empirical validation that relies mostly on CIFAR 10 results to justify claims * Some claims made (trend between ensemble diversity and accuracy, Q diversity capturing not capturing negative correlations) are not substantiated.   All reviewers recommend this paper to be rejected and the authors did not reply to any reviews.
Post rebuttal, the reviewers all recommend acceptance.
Overall, this paper has been on the very borderline. All reviewers agree that the motivation and the idea of the paper are reasonable (although somewhat incremental) and make an interesting extension of mixup type data augmentation. However, one expert reviewer raised some concerns which are unfortunately not fully resolved despite the intensive interaction.   The first one is the issue of diversity claimed in the paper. The authors explanation is mostly qualitative, and I, as an AC, also felt a logical jumping here, although I do understand that the proposed method somehow results in better generalization ability w.r.t. the number of generated samples as shown in Fig.3 in the appendix. This point is important because if the better generalization is coming from the label modification rather than the diversity in image space, then the novelty of the paper is limited. Another concern is that there are some inconsistencies in the scores of previous methods implemented by the authors and in the original papers. We understand that the exact score is not always easy to reproduce, but at least it is desirable to follow the original setting (such as the number of epochs) for each method as far as possible, because the accuracy should be the most important criterion as the goal of data augmentation is better generalization. Then, authors may separately discuss computational efficiency.   Based on the discussion with reviewers as above, I conclude that the paper should be further polished and completed before publication. Thus I recommend rejection for this time.
The reviewers unanimously agree that the paper is timely, well motivated and correct, with potential to significantly impact digital contact tracing. 
The paper proposes a layer wise magnitude based tuning method through the adoption of LAMP score, motivated by minimizing the model output distortion. The new importance score differs from vanilla magnitude based score in that it incorporate more layer wise information. Extensive experiments are conducted on image and language models to show the improved accuracy upon prior arts under same model compression ratio. Ablation study is also provided to further explain the intuition and comparison of LAMP with other pruning methods.   Though the experiments are extensive, some reviewers raised questions that only image datasets are tested. In the rebuttal, the authors addressed more on Appendix D which provides non image results, and also modified the abstract to highlight the efficacy on image data. In all, given the extensive empirical evaluation on various datasets and model architectures, the improvement of LAMB over prior methods seems convincing. Nevertheless, we urge the authors to include more experimental results, for example ResNet 18 on ImageNet as promised to Reviewer 1, to make the results more solid. It is also suggested to include and discuss some relevant papers mentioned by the reviewers in the final version.  
The new non linearity proposed in this paper present interesting observations and improvements on image and text datasets. However, reviewers point out that there should’ve been more comparisons to other efficient transformers and on more datasets. The speed improvements are also not clear. I’d encourage the authors to revise and submit in the future.
The paper provides a neural generalization of decision trees with the idea of maintaining interpretability. The approach falls a bit short on theoretical grounds. For example, the main theorem portraying interpretability isn t properly defined and some definitions appear implicitly in the proof. The view of decision trees as a sequence of soft decisions appears to need to model how the full probability distribution over the nodes propagates at each depth. A much stronger case for interpretability (rather than assuming that each T_i is interpretable) should be made if this is kept as one of the main arguments for the architecture. Interpretability of decision trees does not directly carry over to these models.   
This paper proposes a model based RL algorithm which, instead of simply fitting a parameterized transition model and uses rollout for planning, learns latent landmarks via distance based clustering and conducts planning on the learned graph. Although some of these ideas themselves have appeared in literatures, the overall approach is very nice, novel and sophisticated. The experimental results appear strong and interesting. Most reviewers feel positive about the contributions of the paper, but there remain concerns that need to be addressed.  The proposed approach is highly nontrivial, and more ablation, generalization and environments need to be studied to fully justify what s going on. The authors agree to expand the paper and add the needed results, which would require substantial work thus reviewers recommend that the paper be submitted again to a future conference and receive another round of review. Showing the generalization is nontrivial, and it would be make the paper stronger if the authors put more thoughts into this issue, although it is not a must.  Minor: Another technical comment is that the approach seems heavily rely the choice of embedding distance. Learning the best embedding with meaningful embedding distance has been considered in other scenarios, see eg https://arxiv.org/abs/1906.00302. It would be interesting to try out and compare difference choices of the embedding distance.  
While the general idea of the paper is certainly interesting and highly relevant, there is consensus that the paper cannot be published in the current form. There were serious concerns about   the correctness and generality of the method   clarity of presentation   experimental evaluation  The authors graciously accepted the feedback, we wish them all the best in thoroughly revising and resubmitting the paper.
The paper received 5 reviews, one of which had positive feedback. Although there are merits associated with the paper, several concerns raised in the reviews and the discussion period that prevents the paper to be accepted. It appears that experiments on noisy graphs are not properly done and competitive baselines are not used for validations. The quality of the learned graph structure is not adequately analyzed. and the experimental setup was not clearly explained. All these indicate that there is a need for a major revision before the paper can be considered for acceptance.
The paper describes a new data augmentation approach for image based RL.  The approach is both simple and effective.  It improves significantly the performance of several algorithms across a number of tasks.  The reviewers were unanimous about the benefits of the proposed technique.  This represents an important advance for RL.
This paper presents an inference softmax cross entropy (I SCE) loss, a modification to the widely adopted "Softmax Cross Entropy" (SCE) loss, to achieve better robustness against adversarial attacks. The original submission had critical issues on motivation, theoretical analysis and experiments. Although the authors provided a revised version, it needs another round of thorough examination before publishing. 
The presented idea is aligned with past work using multiple experts or multiple sources for transfer. However, it is positioned uniquely and cleverly in that the approach is developed with scalability in mind. Within this setting, the paper is convincing. Although the approach does not come with strong backing theory, it is intuitive and seems to work well. During the discussions phase, the authors have clarified some questions that made the paper convincing, even if it is a relatively heuristic approach. The results are strong if one is concerned with both quantitative performance and efficiency, a combination of objectives very often encountered in practice. Overall, it is expected that this idea can stimulate further research along those lines, especially since this paper is very nice and easy to read.
This paper initially received mixed ratings but after the rebuttal, all reviewers recommended acceptance. Reviewers appreciate the novel technical ideas and extensive experimental results. 
After engaging in some good interactive discussions all but one reviewer settled on a rating of marginal accept. The most negative reviewer didn t really provide a clear enough explanation of what was lacking in the work. The other reviewers felt that the observed gains for this multi task learning framework were clear enough that the work is worthy of some attention by the community. The AC recommends acceptance, but one may consider this recommendation as a just past the line for acceptance recommendation.
The major concerns about this paper are that (1) There are too many hyper parameters, such as those needed for ADMM. I d point out that there are adaptive variants of ADMM and heuristics methods for choosing optimization hyper parameters, although it would be nice if the authors addressed these issues in the paper.  (2) Some reviewers are concerned that, compared to other related attacks, it’s unclear why flipping fewer bits is an important objective   an attacker might only care about poisoning performance and clean data performance.  The authors respond that flipping fewer bits makes the attack more effective when bits are manipulated by a physical method such as manipulating memory.  Despite these criticisms, reviewers agree that the paper is a well thought out approach that improves the state of the art by some metrics. 
The paper proposes an interesting architecture that dues Graph Neural Networks (GNN) and Gradient Boosting Decision Tree. This new architecture works on graphs with heterogeneous tabular features and BGNNs work well on graphs where the nodes contain heterogeneous tabular data and is optimized end to end and seems to obtain great SOTA results. End to end learning is done by iteratively adding trees that fit the GNN gradient updates, allowing the GNN to backpropagate into the GBDT. All reviewers agreed that the idea is interesting, the paper is well written, and the results found in the paper are impressive. In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score. Therefore, I recommend acceptance.
This paper presents a graph neural network based approach to forecasting multiple time series. It incorporates structure learning similar in some ways to NRI (Kipf et al.) and a recurrent graph convolution forecaster given the inferred graph based on DCRNN (Li et al.). The paper shows consistently improved performance over several different kinds of baselines (classical, deep learning, and graph based deep learning) on three datasets, two of which are public and one proprietary.  Reviewer 1 thought the paper was “well presented and easy to understand”. I agree. The reviewer liked the simplicity of the approach but wondered whether the empirical improvement was sufficient. The reviewer asked about applicability outside the time series domain and the authors provided a satisfying response.  Reviewer 3 thought that simultaneously learning graph structure and forecasting was an “understudied topic”. The reviewer pointed out several strengths including: the end to end nature of the approach, the reduction in training cost due to the direct parameterization of the adjacency matrix structure, the optimal structural regularization scheme, and the extensive experimentation. Like R1, they thought the paper was well written. They suggested several points of improvement, which were mainly seeking clarification. They made a good point regarding the PMU dataset in that only one month was considered, which was not enough to capture long term seasonalities. This seems like a limitation to me. The authors responded to each of the points in turn. Regarding the point about the PMU dataset, the authors stated that the data was extremely noisy, so they settled on a month that was comparatively clean.  The review from R4 was not as positive as the other reviews. R4 proposed that the present work may be a “simplification of NRI” rather than being novel/different because NRI was a windowed based approach, while GTS was based on the entire time series. The reviewer also pointed out what appears to be a highly relevant paper (Wu et al.); though this appeared in KDD 2020 and in my opinion it’s understandable if the authors missed it. Finally, R4 raised several issues with the empirical evaluation. They make a very good point that the analysis on regularization used the kNN graph where a “ground truth” graph was not available. Why not evaluate the regularizer on the other datasets where ground truth is available? The authors responded with an updated paper addressing this point. The authors responded to other points of criticism and a fairly extensive debate ensued. The key points of the debate were: A difference in opinion between the significance in departure between the structure learning mechanism in this work (GTS) and NRI (Kipf et al.) Overlap between a recently proposed paper (Wu et al., KDD 2020). I am fairly sympathetic with the authors here. That work is fairly late breaking and they do point out a key difference: "the advantage of our method over MTGNN is clear: we can get structures we desire, not restricted to a degree k graph. This advantage is an important contribution, because MTGNN hard wires the graph parameterization and enforces that all nodes have the same number of (out) neighbors. What if one desires instead a graph that approximately obeys spatial proximity, like that in the case of METR LA? Our method yields such a graph." Some minor concerns with Fig. 2 and the structural prior/regularization analysis  I have read the paper, and while R4’s concerns are legitimate, I think this paper is clearly over the bar. I support this paper’s acceptance and ask the authors to take the reviews into consideration when revising their paper. As R4 suggests, they could add a controlled set of experiments on a perhaps synthetic dataset to show that the proposed GTS is better than NRI. If the scalability of NRI is a concern, then this can be highlighted to the same extent as LDS.
All reviewers appreciate the framework described in the paper and say it is a "useful tool", a "flexible, efficient, extensible, and secure visual analysis framework"  and "in full fledged form may help the productivity while building visual analysis applications."  However, the reviewers also point to significant shortcomings in terms of fit to ICLR, e.g. "looks more like a technical report than a research paper", "key contributions of the VideoFlow should be only counted as engineering efforts rather than any novelty in the scientific or research perspective", or "major concern is that if it is appropriate for ICLR to publish this tutorial which may be regarded as an endorsement to this software".  The authors did not reply to the reviewers comments.  Overall the paper does not seem to contain sufficient scientific contributions for being accepted.
The paper presents an interesting model to sample from the Gibbs distribution using diffusion based method. The theory is interesting and it is related well to the current research in the field. All reviewers agree that this is a noteworthy contribution to ICLR. 
This work proposes a novel network structure, spatial dependency networks that is introduced as an alternative to convolutional neural networks. This new architecture is used successfully to get state of the art performance for a number of common image generation benchmarks when compared with non autoregressive approach (even much larger CNNs). There is a lot of useful feedback in the reviews themselves: a thing to consider in the final version is the fact that the authors had motivated SDNs as drop in replacements for CNNs, but do experiments mostly in VAE like settings. This is a point that was raised by multiple reviewers and is clearly something that should be dealt with as explicitly as possible.  While there are legitimate reasons to be wary of the increased computation time, I tend to side with the authors that baselines that are being compared with SDNs are likely to have more optimized primitives. From the inference numbers presented in the rebuttal, it doesn t appear like the speed issues are insurmountable.   Given the high quality of writing, the excellent performance on image density modeling, the various ablations and understanding of the disentangling effects, I think this is an interesting piece of work that the field would benefit from.
This paper proposes a novel unsupervised task of colour conversion. In this respect, the task becomes more like a regression problem   rather than autoencoding the decoder needs to reconstruct the pixels in a different color system.  While the idea is potentially interesting, there are fundamental problems with the paper:  * The motivations of the paper are obscure, (understanding colour representation in complex visual systems? Learning better representations? Disentangling color related information from the rest) * No analysis is provided to highlight what the novel objective is achieving  The answers of the authors to AnonReviewer1 are not very convincing. As AnonReviewer1 has pointed out, the mapping between the color spaces is typically a simple invertible map so any conclusion that the authors arrive about ‘substantial impact’ could be simply the artefact of the particular architecture choice. The other claim, that ‘the proposed framework is able to encompass additional constraints relevant in understanding why the considered representations could have emerged in the brain’ quite far fetched and speculative at best.  The authors have a point in their reply ii) to AnonReviewer1 but if the claim is about the particular color coding schemata, it would be natural to include simple experiments where some arbitrary 3x3 invertible mapping (e.g. rgb in spherical or cylindrical coordinates) next to other color schemata to make a stronger point.  In point iii) the authors refer to tasks without being very explicit about what the tasks are. Colorization is a known proxy pretext task for learning representations when the downstream classification task is not known a priori. The paper would have been much more easy to motivate if the authors could demonstrate the merit of the proposed objective using a more extensive and careful representation learning evaluation methodology.  In light of the above points, I feel that the paper needs further iterations to be presented at ICLR. 
Overall the review is borderline: R2 and R4 are slightly positive and R3 is slightly negative. All the reviewers like the novel shading consistency loss proposed in the paper and, improved DIP that produces consistent image decomposition inferences, and good experimental results. However, reviewers also shared concerns about speed and the thoroughness of the evaluation, and human tolerance of shading inaccuracy. These points were addressed in details in the rebuttal, and reviewers didn’t change their initial scores.  The AC is concerned about the cut and paste neural rendering results. Because there are no cast shadows, the rendering doesn’t look realistic under the lighting conditions in the new image. It’s unclear that the proposed method would lead to a promising direction of copying and pasting contents into images for photorealistic editing. Consequentially, the paper is not ready for publication at its current form. 
This paper proposes a method called Federated Bias variance attacks (FedBVA) to generate adversarial examples for federated learning, which can be used to make the model more robust to adversarial attacks.  All the reviewers found the problem and the approach very interesting. Their concerns include the following main points (please see the reviews for more details): * The decomposition of the bias and variance can be made more rigorous * The necessity for a shared dataset of adversarial examples makes the application a little limited * Need fairer experimental baselines * Vanilla federated learning is not guaranteed to preserve privacy   the authors should edit this claim in the motivation * Need compatibility with secure aggregation approaches where the central server cannot access local updates  The authors did do a great job of responding to the reviewers  comments. Given the interest in the problem and the novelty of the idea, I think an improved version of the paper would be quite well received.
The reviewers overall liked the paper and the mathematical contribution seems substantial and elegant. The two dominant concerns were whether this is really applicable to GANS, and whether the increment from symmetric to normal matrices (e.g., real to complex eigenvalues, still unitary eigenvectors) was significant enough. Our consensus is that this result is a step toward analyzing practical GANS, and (based on the authors  response) that the extension to complex eigenvalues was substantial enough. Hence I m happy to recommend the paper.
This paper proposes a hybrid algorithm that combines RL and population based search. The work is interesting and well written. But, the contribution of the work is very limited, in comparison with the state of the art. 
This paper proposes a GNN architecture for multi relational data to better address long range dependencies in graphs. The proposed GR GAT model is a variant of graph attention networks (GAT) with, among other modifications, vector based edge type embeddings and GRU type updates. Results are presented on AIFB, AM, and on synthetic benchmarks.  The reviewers agreed that this is an interesting contribution and that the results on the chosen synthetic benchmarks are insightful, but that experimental evaluation on real data and overall motivation of the architecture is lacking. In the rebuttal period, the authors have improved the writing and strengthened the motivation of the paper. However, given the limited amount of time, the authors were not able to sufficiently address the lack of experimental validation on real data (beyond AIFB & AM). I am inclined to agree with the reviewers that this paper needs significantly more work on the experimental evaluation, the overall presentation needs to be refined and it needs to more carefully analyse the effect of each individual architectural modification to meet the bar for acceptance. 
Reviewers could not reach consensus here and legitimate concerns are raised on novelty and on empirical results, although this can be attributed to the important computation times required to run experiments on 3D MRI volumes. The authors have provided a comprehensive response to the reviews, the general feedback is that the work has merit but it fails to convince on its real contribution to the state of the art. At this stage, I fear this work cannot be recommended for acceptance.
This paper proposes a method for automatically discovering graph algorithms using GNNs. In general, the reviewers find the paper well written, and the problem and the approach interesting.  However, there is a concern on the practical usefulness of proposed method as shown in the following comments:  “My main concerns are on Q2, i.e., the practical usefulness of the algorithm”[R1]; “It sounds like the proposed model is hard to generalize to different datasets” [R3]; “The proposed explainer does not generate practically useful outputs for discovering new algorithms”[R4]. 
This paper introduces a new dataset for evaluating disentanglement and its impact on out of distribution generalization based on the trifinger robotics platform. Using this dataset, the authors rigorously investigate the performance of beta VAEs in this setting under a number of conditions, finding that weak supervision is necessary to induce disentangled representations, and that, perhaps surprisingly, disentanglement does not help for sim2real settings despite the similarity between the simulator and the real data. Reviewers were divided on the work, but had a number of concerns related to the claims of novel architecture, comparisons to baselines, and issues with the clarity of the paper, some of which were addressed in the authors  response. I agree with some of these concerns, particularly with respect to the claims of novel architectures since the modifications could simply be viewed as tweaking hyperparameters and are not rigorously compared to baselines. However, I think the novelty of the dataset and the rigorous evaluation of OOD generalization settings is likely to be valuable enough to the community to merit acceptance. I d encourage the authors, however, to tone down some of the claims regarding the architecture (or provide sufficient baseline comparisons), and instead focus on the dataset and the OOD results. I recommend acceptance. 
This paper discusses the conditional independence test using GAN.   In the same way as GCIT (Bellot & van der Schaar, 2019), they realize sampling under the null hypothesis by generating sample from P(X|Z) approximately with GAN.  They propose to use a test statistic defined by the maximum of generalized covariance measures (GCM) over random neural networks.  They theoretically discuss the advantage of GCM and show the asymptotic results of the proposed test statistic, which demonstrates improved justification over GCIT.  Experimental results show favorable performances over existing conditional independence tests.   The proposed method gives an advance in the methodology of conditional independence tests for continuous domain, which is an important but difficult problem because of the difficulty of obtaining the null distribution.  In the line of Bellot & van der Shcaar (2019), they solve it using the strong conditional sampling ability of GAN, which is an important research area. The theoretical analysis and experimental results are also making good contributions.    However, there are some weakness in the proposed method and comparison with existing methods. First, as R4 points out, there are many hyperparameters in the proposed method, and their choice is not easy.  While the authors addressed some aspects of this issue in their rebuttal and revision, it is still unclear how to justify the choice of B, the functions h_j, and the neural networks of GAN, which should potentially have significant influence on the test performance.  Second, the comparison with Bellot and van der Schaar (2019) is not very clear.  In the paper, the GCIT has been used with the distance correlation, which is known to be an instance of HSIC (MMD) with a specific choice of positive definite kernel (Sejdinovic et al 2013).  The HSIC can be formulated as the maximum of generalized covariance measures over the unit ball of the RKHS.  Thus, the difference of GCIT with distance correlation and the proposed methods are essentially the difference of the function classes for the maximum.  On the other hand, the experimental results show significant difference in the test performance.  I think more elaborate and careful comparison is needed for these two methods.   Overall, the paper is a good contribution on the topic.  However, the evaluation of the reviewers is not high enough to justify the acceptance in the high competition of ICLR.  I encourage the authors complete their work by reflecting reviewers’ comments and submit this work to another conference or journal.   Reference: Sejdinovic, D., Sriperumbudur, B., Gretton, A., & Fukumizu, K. (2013). Equivalence of distance based and RKHS based statistics in hypothesis testing. Annals of Statistics, 41(5), 2263–2291. 
This paper introduces a technique called TOMA to learn abstract graph representations of MDPs. Such an approach is said to be more efficient both in terms of memory and computation. Despite this being an interesting research topic, the reviewers unanimously recommend rejecting the paper. They all agree that the writing needs to be improved for clarity, that some of the algorithmic choices seem arbitrary, and that there are relevant baselines missing. Moreover, the authors didn’t submit a response to most reviewers. 
The paper received two borderline accept recommendations and one accept recommendation from three reviewers with low confidence and a reject recommendation from an expert reviewer.   Although all reviewers found that the paper addresses an important and challenging problem of semantically constraining adversarial attacks as opposed to constraining them artificially by an artificial norm ball. However, during the discussion phase it has been pointed out that there were some important weaknesses indicating that the paper may need one more evaluation round.  The meta reviewer recommends rejection based on the following observations.   In terms of evaluation, while it is understandable the authors were unable to compare to Gowal et al. due to the lack of publicly available implementation, showing Song et al. s adversarials hurt performance and and are farther than the image manifold has been found puzzling, as this was done by Song et al. only to keep human prediction the same while changing model prediction. Furthermore, the paper did not contain a user study similar to Song et al. for a fair comparison Finally, the discussion revealed that the comparison to "norm bounded adversarial inputs" may not have clarified whether this experiment faithfully demonstrates an advantage for the contribution as the norm could be contained to a point where accuracy is not reduced, and the discussion on the certified defense being "broken" was inconclusive.
A good paper with significant contribution on XAI and the on  vs off  data manifold explainability. Reviewers have appreciated authors’ feedback and update of the paper (R1, R2, R4). I would like to personally thank the authors for a smooth, extensive and focused interaction w/ updates. 
With the advent of non recurrent sequence processing models, it has become costumary to augment input tokens with positional embeddings providing implicit positional information. Despite their potentially crucial role in modern architectures, such positional embeddings are rarely addressed in analytical studies. The current paper provides a systematic investigation of positional embeddings, characterized in terms of properties such as monotonicity, translation invariance, and symmetry. These properties are studies for different positional embeddings using language models fine tuned on two separated benchmarks, with an emphasis on visual analysis.  The authors provided an impressive rebuttal, adding many of the experiments required by the reviewers. The latter are still somewhat split about the paper. I lean towards the positive side. I find that some of the criticism, while valid, is not really granting a rejection, especially after the authors  clarifications. In particular, one reviewer assumed that the authors claim that symmetry should be a property of an ideal positional embedding, whereas the authors are rather studying whether it is an important property of them, in light of the previous literature. Some claims about the results being "interesting" or "surprising" enough might depend on what the reader is looking for in the paper. I think that many readers in the "black box NLP" community will find the methods and analyses presented in this paper interesting and useful. 
The submission considers a new attack model for adversarial perturbation in a framework where the attacker has neither access to the trained model nor the data used for training the model. The submission suggests a"domain adaptation inspired attack": learn a different model on a similar domain and generate the adversarial perturbations using that model. The authors then also develop a defense for this type of attack and provide some empirical evaluations of the resulting losses on a few NLP benchmark datasets.  The paper refers to the literature on domain adaptation theory to motivate their suggested defense, but this analysis remains on an intuitive (rather than a formally rigorous) level. Furthermore, the empirical evaluation does not compare to a variety of attacks and the defense is only evaluated with respected to the self suggested attack. This is a very minimal bar for a defense to meet.  The reviewers have criticized the submission for the rather minimal extend of empirical evaluation. Given that the submission also doesn t provide a sound theoretical analysis for the  proposed attack and defense, I agree with the reviewers that the submission does not provide sufficient novel insight for publication at ICLR.   In contrast to some of the reviewers, I do find it legitimate (and maybe recommendable even) to focus on one chosen application area such as NLP. I don t see a requirement to also present experiments on image data or re inforcement learning applciations. However, I would recommend that the authors highlight more explicitly what general lessons a reader would learn from their study. This could be done through a more extensive and systematic set of experiments or a through analysis in a well defined theoretical framework.
The paper presents a sound and efficient (but not complete) algorithm for verifying that a piecewise linear neural network is constant in an Lp ball around a given point. This is a significant contribution towards practical protection from adversarial attacks with theoretical guarantees. The proposed algorithm is shown to be sound (that is, when it returns a result, that result is guaranteed to be correct) and efficient (it is easily parallelizable and can scale to large networks), but is not complete (there exist cases where the algorithm will return "I don t know"). The experiments show good results in practice. The reviewers are positive about the paper, and most initial concerns have been addressed in the rebuttal, with the paper improving as a result. Overall, this is an important contribution worth communicating to the ICLR community, so I m happy to recommend acceptance.
The paper s initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline.   The paper addresses a relevant and challenging problem in the RL domain. However, in my opinion, from the reviewers  and authors  remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy. Primary among these is the quantum of novelty   as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised. Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper   even a two time step trajectory with normal rewards per state transition can exhibit a mixture of Gaussians type reward distribution for the second state, breaking the assumption. As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective. There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results. The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (e.g., Hotelling vs. Mean vs. UDT etc.), which does not indicate an advantage of the proposed method.   I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work.  
This paper considers the analysis of momentum for nonconvex problems. While this is a worthy direction, as some reviewers pointed out, the examples considered are rather specialized and one can argue they are mostly "convex like". Therefore it is not clear that these results are generalizable and whether the analysis offers insights about how momentum helps (including faster escape of saddle in nonconvex regime, and faster convergence in the convex regime). In the current form, these results have limited scopes. 
This work proposes a model based optimization using an approximated normalized maximum likelihood (NML). It is an interesting idea and has the advantage of scaling to large datasets. The reviewers are generally positive and are satisfied with authors  response.  
this paper adds onto the line of research in investigating the mechanism by which a recurrent network solves a supervised sequence classification problem, following the recent studies such as Maheswaranathan et al., 2019 and Maheswaranathan & Sussillo (2020). in doing so, this paper hypothesizes and confirms that the internal hidden state of a recurrent net, be it GRU or LSTM, evolves over a planar (approximate) attractor as it reads the input, amounting to integrating the evidence as it processes the input sequence, and demonstrates the existences of these attractors and integration dynamics on three types of problems (classification, ordered classification and multi label classification.)   there were some potentially misleading or confusing statements throughout the manuscript in the initial version, which were pointed out by the reviewers. the authors however did a commendable job of addressing these concerns by the reviewers to the point that most of them have revised their scores up.   based on the reviewers  assessments, authors  response and their exchange, i strongly believe this work will enrich our understanding of recurrent nets further.
The paper is interested in the Lipschitz constant estimation of deep equilibrium models. The estimation of this constant provides us the ability to certify classification decisions and understand robustness as well as has important bearings on the generalization ability of a neural network.  Overall a solid theoretical contribution with rigorous theory in a well written paper.  
This paper proposes a promising solution to a very interesting and challenging problem, and the authors have improved the paper during the rebuttal by adding an important missing baseline. However, all reviewers still agree that the paper currently lacks sufficient analysis that would be required to understand properly the implications of past history on the regret. More specifically, the fact that assumption A2 does not apply to the given problem raises questions that should be addressed before publication. Theoretical analysis was provided for previous similar work (e.g. NeuralUCB). Providing this for the proposed method would significantly improve the impact of this work.
This paper presents a novel theoretical analysis for unsupervised domain adaptation based on f divergences. The reviews unanimously pointed out the interest and the quality of the theoretical part. However, some limitations in the experiments, presentation and the significance of the result have been raised. The authors provided a rebuttal that addresses some concerns. However, the reviewers agree that the experimental part still requires some extension to fully support the claim of the paper, as well as some writing improvement. The paper was evaluated to be not ready for ICLR, thus I recommend rejection. 
In this paper, the authors proposed a new variant of the Wasserstein autoencoder (WAE), which matches the joint distribution of data and the latent codes induced by the encoder and the joint distribution induced by the decoder in the framework of optimal transport. Because of matching the distributions that are not considered by existing autoencoders like WAEs or VAEs, I agree with the authors that the proposed method is novel to some degrees.   However, the experimental part does not support the superiority of the method well. For example, some reviewers (including me) think the results of the baselines shown in Figures 8 10 are underestimated. According to my personal experience, the WAE should perform much better on CelebA than that shown in Figure 10. The experiments in Figures 11 17 provide more reasonable results, but the advantage of the proposed method is not convincing.  Here is my suggestion: 1) Because the proposed method can achieve flexible prior, besides randomly generating data, the authors can consider adding some experiments on conditional generation, i.e., generating data from a single modality of the learned prior. I believe the proposed method will be more convincing if it can show some advantages in the conditional generation task. 2) The runtime comparison for the method and the baselines in the training phase should be discussed.  3) The short name "SWAE" is in conflict with an earlier work "Sliced Wasserstein Autoencoder", which is also called "SWAE".
This paper proposed to theoretically explain why a pre trained embedding network with self supervised training (SSL) can provide representation for downstream few shot learning (FSL) tasks. The review process finds that the paper may over claim the results and that the results seem unsatisfactory. Both Reviewer 4 and Reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper. The paper needs a substantial revision to improve clarity and accessibility. As pointed out by Nikunj Saunshi’s public comment, this paper may benefit from discussing the differences from the previous works, including [1].    [1] Arora et al., A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019
Reviewers generally agree that the main result of the paper, which generalizes the classical Wigner Eckart Theorem and provides a  basis for the space of G steerable kernels for any compact group G, is a significant result. There are also several concerns that need to be addressed. R4 notes that the use of the Dirac delta function (e.g. Theorem C.7) is informal and mathematically imprecise and needs to be fixed. R1 notes that it would be helpful to at least describe how this general formulation can be applied in machine learning.  Presentation and accessibility: the current version of the paper will be accessible to only  a small part of the machine learning audience, i.e. those already with advanced knowledge in mathematics and/or theoretical physics, in particular in representation theory. If the authors aim to make it more accessible, the writing would need to be substantially improved.
The submission introduces a theoretically justified solution to  client drift  in federated learning.  Generally, the reviewers agreed that this could be a strong paper, and several of their minor concerns have been addressed in the rebuttal. Unfortunately, a major issue raised during the discussion was the correctness of Lemma 7. Despite the extra clarifications provided by the authors, Reviewer2 still believes it is incorrect and R4 also sided with them.  As theoretical analysis is the major contribution of this work, we have to reject the submission. I would strongly encourage the authors to fix the issue (or clarify the proof), and resubmit it to one of the upcoming top ML conferences. 
This paper presents a approach to the distributed kernel k means problem using a combination of random features to efficiently approximate the kernel matrix, a distributed stochastic proximal gradient algorithm which calls a distributed lanczos algorithm as a primitive to find a low rank approximation to the kernel matrix, and additional compression to reduce the cost of the communications.   The algorithm is a novel combination of prior ideas, and empirically works well. However, the claimed theoretical convergence rate is not convincing: e.g., the convergence rate depends on the Frobenius norm of the error in approximating the kernel with random feature maps, which is O(n^2) for a problem with n data samples. This implies that O(n^2) iterations must be used in the algorithm, which is already slower than a naive approach to kernel k means.  This paper takes a promising approach to the problem, but as the potential contribution lies in combining prior ideas in order to obtain a provably guaranteed approximate solution to the distributed kernel k means problem, and the proposed algorithm was not shown to satisfy this promise, the recommendation is to reject.
This work proposes an algorithm for generating training data to train automatic theorem proving models. In particular, it allows users to pose specific generalization challenges to theorem proving models and evaluate their performance. In doing so, it provides a degree of control over the task space that is greater than when working with  organic  corpora of real theorems and proofs.   The authors demonstrated the utility of their generated data by training well known models such as transformers and GNNs, and were able to derive insights such as the value of MCTS style planning for finding proofs in particular settings.   After the rebuttal period, all authors agreed that the work was well executed and that the algorithm creates datasets that will be of value to the (learning based) theorem proving community. As such, they all recommended acceptance to a greater or lesser degree. I am convinced by their arguments, because I think there is real value in using controlled synthetic data alongside real data when making scientific progress on hard problems like theorem proving. I am particularly convinced by the observation that the data generated by this method has already led to improved performance on a real corpus of proofs, as the authors state in their rebuttal. If they have not done so already, I encourage the authors to report this fact in the camera ready version of their paper citing the relevant work. 
The paper investigates an active learning strategy for speeding up the convergence for SSL deep learning algorithms. When the SSL objective could learn a good approximation of the optimal model, the proposed method efficiently converges to the result with a few queries. The main idea is that when the eigenvalues of the NTK are large, the convergence rate is faster. The proposed algorithm maximizes the smallest eigenvalue of the NTK. An empirical investigation is also reported. The reviewers appreciated the general idea, but questioned about the actual execution of this paper in terms of both experimental comparison and (lack of) supportive theoretical results. I would like to encourage the authors to consider improving their paper along one of these two lines.  Unfortunately, as it currently stands, this paper is not ready for publication.
This is a clear accept. Solid and timely work extending normalizing flows to implicitly defined mappings. Convincing presentation. Supported by all four reviewers. Best paper in my batch. Has the potential to spark further developments in the field. I recommend to feature this paper as a spotlight.
This paper presents a novel method for general purpose supervised domain transfer that trains both generator and discriminator to compete in a minimax game in order to reconstruct data. This setup is meant to address a common issue in conditional GAN setups: they often ignore conditioning information. Results are positive and span two very different tasks: image to image translation and silent video to speech reconstruction. Overall reviewers were quite positive about this paper: they found the method to be novel and well motivated, and after rebuttal, found experimental results to be sufficiently convincing. Several concerns were brought up: (a) lack of emphasis that the approach is in fact supervised, (b) need for comparisons with stronger or task specific baselines, (c) lack of description of experimental details for reproducibility, and (d) lack of discussion of ethical implications. All of these concerns were satisfactorily addressed by authors in rebuttal and reviewers unanimously vote for acceptance. I agree, and recommend this paper be accepted. 
The reviewers unanimously raised concerns over the clarity and technical correctness of the theory and the Imagenet experiments during the first round. The authors submitted a highly revised version during the rebuttal which allayed concerns for multiple reviewers, however all the reviewers raised the concern that the paper has gone through a very significant change, almost becoming "a new paper" in the rebuttal phase and should go through another cycle of resubmission and review to correctly judge the contributions and claims.  The reviewers were aligned in their judgement that the original manuscript with minor changes was not ready for publication. The authors are strongly encouraged to polish the version submitted during rebuttal and resubmit.  
This paper presents an approach for conformal prediction where, in its standard paradigm, a set of prediction candidates is identified as opposed to a single one.  The authors advance the CP framework by presenting a rigorous methods that allows for a smaller set of admissable predictions with a covergae quarantee. Their further contribution is a methodolgy based on cascading that filters out non promising candidates.  After the discussion period, _all_ the reviewers are in favour of accepting the manuscript with the average being marginally above acceptance.  My recommendation is therefore to accept the paper.   Strong points: The advance of a smaller set of admissible predictions in the CP framewrok is quite useful especially in scenaria where the set can grow (expensively) large.  Thorough experimental analysis with good presentation of performance gain and usefulness in real world data.   Weak points: Lack of novelty in the techniques make the work a weaker candidate compared to the rest of submissions. 
The authors were responsive to the comments of the reviewers, both in the rebuttal and in the revision to the manuscript.  However, the reviewers were still concerned about the lack of clarity of the manuscript, the motivation for the design decisions, and errors, present also in the rebuttal revisions. 
Two referees indicate reject, one supports (weak) accept. My impression is that major points of criticism raised by the reviewers   mostly about limited novelty and somewhat inconclusive experimental results   could not be addressed in a clearly convincing way during the rebuttal phase. I will therefore recommend rejection. 
This paper presents a new graph neural network (GNN) architecture with attention and with applications to Boolean satisfiability.  The reviewers expressed concerns over various aspects of the paper such as a need for better ablations and an analysis of the difficulty level of the SAT problems used in evaluation.  No rebuttal was provided.
 The paper presents a side channel attack in a scenario where the attacker is able to place a induction sensor near the power cable of the victim s GPU. The authors train a neural network to analyse the magnetic flux measured by the sensor to recover the structure (layer type and layer parameters) of the target neural network. The authors also show that for a wide range of target network structure, by training a network with the inferred structure, they produce adversarial examples as effective as a white box attack.  The points raised by the reviewers were the following: 1) the result that this type of side channel attack works is interesting, 2) the practicality of the attack is unclear because the attacker needs hardware access to the victim s GPU, 3) the ML contribution is not really clear and a venue on cyber security might be more appropriate.   Side channel attacks on deep neural networks can be of relevance to ICLR (as pointed to by the authors by the ICLR papers/submissions on system side channel attacks). Nonetheless, I tend to agree with R1 and R2 that the ML contribution is limited (either in terms of application of ML or methodology), and the concerns of practicality of the approach make me lean towards rejection.
I thank the authors and reviewers for the discussions. Reviewers agreed the work is interesting but there are some aspects of the paper that need improvements. In particular, authors need to better address concerns raised by R5. Given all, I think the work still needs a bit more work before being accepted.  
This work extends previous work on unsupervised learning of goal conditioned policies: an abstract skill policy, which drives exploration of the state space, is used to propose goals as well as derive rewards for a goal conditioned policy.   Reviewers agreed the approach was novel and interesting. All reviewers raised significant concerns about clarity and/or lack of details, as well as a lack of comparison to DIAYN/DISCERN, though these points were adequately addressed in revisions. One remaining issue raised by two reviewers are that the content related to the information bottleneck/disentangled representation learning seems out of place and ill justified. Detailed discussion of this aspect of the work has been relegated to the appendix.  This is an important problem and a growing area of study, and while the submission has potential, improvements needed are not minor, and given the short process, we can only accept papers as is, rather than expecting certain changes. We urge the authors to further improve the focus of the work and perhaps plan to investigate the role and importance of disentanglement with IB in this setting in follow up work wherein they have the space to properly do justice to the topic in its own right.
The paper focuses on the task of learning efficient representation models for video classification. To avoid the excessive computational cost of performing 3D convolutions on video, the authors propose to break the channel dimension of video representations into sub dimensions that are treated separately. This cuts down on computation and improves classification performance over many methods in the literature. Extensive experiments were run on well known benchmarks to justify the claims of the model. Such backbone architectures can be very useful in the realm of video understanding. The authors should be commended for the amount of work they did in the rebuttal period to address the comments and inquiries brought up by the reviewers. Extra experiments were done and more in depth analysis was made possible. 
The reviewers positively valued the proposed idea of performing permutation selection in permutation decoding via combining node embedding and self attention, which seems to be of high originality. I found that this paper is mostly clearly written, except Section 3.2 as AnonReviewer5 commented. The main concern among the reviewers is regarding applicability of the proposal beyond the BCH codes.   Pros:   The proposal of utilizing self attention for permutation selection in permutation decoding is novel and interesting.   Computational complexity in the decoding phase is only slightly increased compared with random permutation selection, and is far smaller than performing decoding for all permutations. The GPS classifier can be parallelizable to further reduce latency.   The proposal should be applicable beyond the BCH codes to those with decoding based on the Tanner graph, including polar codes.  Cons:   Only the BCH codes were considered, whereas in the authors responses they will add a short analysis on polar codes.   It seems that systematic enumeration of the PG is required, which would limit applicability of the proposal.   There is a room for improvement in presentation:   In Section 3.2, the description of "positional encodings" was unclear to me, in that the ordering of the codeword entries is arbitrary, unlike typical sequence transduction problems to which attention mechanism is being applied.   Dependence of the input vector sequences of the attention head on the permutation $\pi$ is not clearly explained.   Performance of the proposed method might depends on choices of the parity check matrix, which is however not discussed in this paper at all.  Based on the above concerns, the paper is not yet ready for publication in its current form.  Minor points: In page 3, line 14, "that" should be deleted. In references list, "hdpc" should be in capital. "reed muller" should be "Reed Muller".
As one of the reviewers concisely summarized: This paper investigates maximum entropy (MaxEnt) inference and compares it to a Bayesian estimator and regularized maximum likelihood for finite models.   Two reviewers specifically question whether they have learned anything new after reading. This combined with various other drawbacks described during the review phase led to strong agreement among the reviewers about a variety of deficiencies in this paper. One reviewer initially gave a relatively high score but has since revised his/her opinion in light of the other reviews and discussion. I find that the significance of this work is not high enough to warrant acceptance at this time, but the authors would do well to incorporate the reviewers suggestions to improve the paper. 
Most reviewers believe that the paper is not ready for publication. Among their concerns are:   whether the new experiment with 10 runs are conducted correctly,   the significance of the theoretical part,   correctness of Lemma 2,   generalization claims may not follow from the theoretical results,   comparison with Zhang et al. (2020).  Given these and the lack of support from reviewers, unfortunately I cannot recommend acceptance of this paper at this stage. I encourage the authors to improve their paper according to these concerns.  I copy paste some of the comments that came after Nov. 24th. The authors might want to use them to improve their work.   First, given the assumptions, the theorems/lemmas are not sufficient to be a solid contribution. I think what important is, can the Alg 1 lead to the optimal policy? More specifically, I do not doubt lemma 2; I am concerned about if updating the representations in an online manner (it should be highly nonstationary) can result in the optimal policy. Some two time scale analysis may address this question. As an additional note, since f can be a many to one mapping, policy pi_i is a multimodal distribution. I am unsure if the authors consider this during implementation.  Second, without the two time scale analysis, I would give weak acceptance if the authors can persuade that the superior performances are indeed due to the proposed jointly embedding learning method. That s why I ask for a baseline that directly considers environment model learning as an auxiliary task.  In the abstract, the authors state that "In this work, we propose a new approach for jointly learning embeddings for states and actions ...," in fact, this is not new. Almost any deep RL algorithm can be thought of as the process of learning state and/or action representations. In the response, the authors say, "We do not believe ..., as ... are embedded into the same space." How to do embedding is more like an implementation issue; one can encode them into different spaces and learn them by learning an environment model. It is nothing fancy/novel.  I consider this paper s main novelty to learn the optimal policy in the embedded state and action spaces, and the embeddings are learned by environment model learning. Thus, the authors need to have strong evidence to persuade people: 1) using an environment model to learn the embeddings is really useful; 2) a separate process of learning the policy in the embedded spaces is essential. Such evidence is necessary to make this paper a solid contribution.  Learning an environment model has been used as an auxiliary task in deep RL. Using such a baseline is to validate that it is necessary to learn the policy in the embedded space separately. The authors should also actively design other baselines to substantiate their claims.
The paper presents a theoretical analysis of the expressive power of equivariant models for point clouds with respect to translations, rotations and permutations. The authors provide sufficient conditions for universality, and prove that recently introduced architectures, e.g. Tensor Field Networks(TFN), do fulfil this property.   The submission received positive reviews ; after rebuttal, all reviewers recommend acceptance and highlight the valuable paper modifications made by the authors to clarify the intuitions behind the proofs.   The AC also considers that this paper is a solid contribution for ICLR, which will draw interest for both theoreticians and practitioners in the community. \ Therefore, the AC recommends acceptance. 
This paper considers a new and practical setting of meta learning for out of domain task adaptation where a pretrained model exists but the original meta training data is not available. The authors incorporate several ideas including deep ensembles, adversarial training and uncertainty based step sizes, and achieve competitive performance under this particular setting.  The combination of various methods appears complicated, but the authors provide detailed ablation study to show the effectiveness of each component empirically. During rebuttal and discussion, they addressed many of the concerns from the reviewers. As pointed out by a reviewer, their proposed method would have a value in the domain adaptation area beyond meta learning.  The remaining concern is on the somewhat ad hoc combination of multiple methods and lack of a clear single solution for addressing the OOD few shot learning problem. Nonetheless, the proposed methods show a convincing empirical improvement on the vanilla MAML baseline in the experiments.
The major criticism of this paper after the initial reviews was a lack of experimental results on deeper and more modern architectures that include skip connections.  The authors added results to the paper to address these issues.
This paper investigates knowledge distillation in the context of non autoregressive machine translations. All reviewers are supportive of acceptance, especially after the thoughtful author responses. A well motivated and simple to implement approach that is giving good empirical results.
This paper proposes a method for predicting higher order structure in time varying graphs. The paper was reviewed by three expert reviewers, and while they expressed appreciation for the sensible solution, they have remaining concerns about the novel contributions and comparisons (analytical and empirical) with previous approaches. Also, the paper would be clearer if examples are used to illustrate the important points of the paper. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
This paper proposes a model parallelism scheme (CMP) for training differentiable NAS with large supernets, which performs the forward and backward passes for multiple tasks at the same time, to increase hardware utilization. Moreover, since CMP consumes large GPU memory due to having multiple computational graphs in memory at the same time, the authors further propose binary neural architecture search (NASB), which binarizes the parameters and gradients to reduce memory footprints and computations. The experimental validation shows that the proposed parallelization technique is more efficient than prior parallelization techniques and can significantly reduce the search cost of differentiable NAS methods. Specifically, the proposed NASB CMP yields architectures with competitive performance on CIFAR 10, at lower search cost compared with baselines that have memory reduction mechanisms such as PC DARTS.  This paper received split reviews, with the majority of the reviewers leaning toward rejection (three 5’s) and one leaning positive (6). The reviewers in general agreed that the problem of achieving resource and time efficiency for NAS is important, and that the proposed idea of consecutive model parallelism is novel and may have some practical impact. The reviewers also found the paper to be mostly clear and well written.  However, reviewers had a common concern that the experimental validation is weak, since 1) the improvement in search cost seems less meaningful with small workloads such as CIFAR 10, 2) some important baselines are missing, and 3) unclear contribution of CMP and NASB due to a missing ablation study. During the interactive discussion period, the authors provided results on additional baselines (NAO and AlphaX), and intermediate results of ImageNet experiments which shows that the proposed method is faster than the baseline. Yet, the reviewers kept their original ratings even after the internal discussion, as they found the incomplete experiments and missing ablation study unsatisfactory.  In summary, this is a well written paper proposing a novel idea to tackle the resource and time efficiency of differentiable NAS, which is a practically important problem. Yet, the experimental validation is too weak to validate the effectiveness and practicality of the proposed method, and thus it seems like a preliminary work not yet ready for publication. However, the work is well motivated and promising, and addressing the reviewers’ common concerns on missing large scale experiments and ablation study will make the paper stronger and significantly increase its chance of getting accepted in the next submission. 
The paper provides a study of the impact of preconditioning/second order methods on generalization by giving a precise analysis in tractable regression settings. It illustrates conditions under which preconditioning might be useful for better generalization.  The readability issues raised by the reviewers have been taken into account, as well as some missing references, except  Wu, D. and Xu, J. "On the Optimal Weighted Regularization in Overparameterized Linear Regression" NeurIPS 2020, raised by reviewer (though it is a really recent reference). Overall the contributions are significant enough to accept the paper for publication.
The paper proposes new techniques for improving the generalization ability of deep learning models for Knowledge Tracing (KT). Instead of designing more sophisticated models, the paper investigates simple data augmentation techniques that can be applied to train existing models. In particular, three different augmentation strategies are proposed based on replacement, insertion, and deletion in the training data. These strategies are then applied with appropriate regularization loss ensuring consistency and monotonicity in the training process. Extensive experiments are performed using three popular neural models for KT and four publicly available datasets. Overall, the paper studies an interesting problem in an important application domain of online education. The results are promising and open up several exciting follow up research directions to explore more complex data augmentation techniques for KT.  I want to thank the authors for actively engaging with the reviewers during the discussion phase and sharing their concerns about the quality of the reviews.  The reviewers generally appreciated the paper s ideas; however, there was quite a bit of spread in the reviewers  assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers  feedback to better position the work w.r.t. the existing literature on data augmentation and state of the art results, better motivate the data augmentation strategies in the context of educational applications possibly through additional data analysis, and add more ablation studies w.r.t. the hyperparameters associated with data augmentation. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing future revisions of the paper. 
The paper analyzes connections between algorithmic fairness and domain generalization literatures. The reviewers found the paper interesting but they also raised some important concerns about it.  The applicability of the method presented in the paper is not clear nor well discussed in the paper.  The papers and the revised version do not not cite important related work.  The mathematical exposition in the paper is a bit hard to read. Even after revision, the reviewers find part of the paper(Appendix F) very hard to read.  Overall, the paper in the current version is below the high acceptance bar of ICLR.
This paper presents a general self supervised time series representation learning framework. The organization is good, and the architecture is well motivated. However, the paper has limited novelty, and is a straightforward application of ideas in self supervised learning literature.  Experimental results are not entirely convincing. The used dataset is small scale that makes the task simple. A more thorough comparison with recent related work is needed. The presentation is also sometimes hard to follow.
The paper studies nonconvex strongly concave min max optimization using  proximal gradient descent ascent (GDA), assuming Kurdyka Łojasiewicz (KŁ) condition holds. The main contribution is a novel Lyapunov function, which leads to a clean analysis. The main downsides of the paper as discussed by the reviewers are the lack of experiments and somewhat stringent assumptions needed in the analysis. Nevertheless, the paper was overall viewed favorably by the reviewers, who considered it a worthwhile contribution to the area min max optimization.  
The paper considers an interesting application of Bayesian neural nets to the geophysics domain;  however, the paper does not make a novel contribution from the machine learning perspective, and the improvements on top of the previously proposed approach by Ahamed & Daub (2019) seem to be quite modest. Overall, the paper does not seem to be ready for publication at ICLR.  
I thank the authors and reviewers for the lively discussions. Although reviewers mentioned the work has potentials to improve adversarial robustness, they agreed that the current draft needs a bit more work specially to strengthen its experimental results and comparisons with related works.   
This paper provides approximation results for functions that can be represented by hybrid quantum classical circuits. It is felt that venues such as QIP would be a more suitable venue, and perhaps some experiments/simulations could be added.
This paper was pretty borderline, but ultimately I am recommending rejection, for the following reason:  The two most negative reviewers (in terms of original score) were concerned about both the quality of the evaluation and whether the evaluation metrics actually meant what the paper claimed they meant.  The authors did make a good faith effort to update the paper to respond to some of the concerns about quality,  but R4 (who has read the rebuttal) is still not convinced that the results of the evaluation are meaningful, and I think I agree with their concern (and I don t see any attempt to address that concern in the rebuttal?).  I view (maybe naively) the goal of this review process as being mostly a correctness check,  and I don t think this paper has passed the correctness check to my satisfaction or to the satisfaction of the majority of reviewers.   However! This is a fixable issue. The paper is definitely cool and interesting, and I would urge the authors to think harder about what sort of evaluation makes sense here and resubmit to the next suitable machine learning conference.
Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning  The topic is maximally timely and important: Understanding human decision making behaviour based on observational data. Any tangible steps towards this challenging goal are bound to be significant, and those this paper makes.  A Bayesian policy learning method is introduced for this task, and validated on both simulated data and user exeperiments in a real decision making task. The novel contribution is on learning interpretable decision dynamics  The paper is written clearly enough..  The updated paper clarified most major concerns the reviewers had. In particular, they added a user study.  The biggest remaining weaknesses are that    relationship to the AMM model did not become completely clear yet    the real user study has been carried out with only a small set of users. But a large cohort study would be too much work to ask for a paper which has also a strong methodological contribution. 
This work develops a weight quantization method for deep neural networks that is suitable for a type of analog hardware system known as crossbar enabled analog computing in memory (CACIM).  The goal of this work is to train models on GPUs in such a way that they retain their predictive accuracy during inference when deployed on the analogue hardware system.  Pros: * Good adaptation of quantization methods to the CACIM system * Simple method * Validation of the proposed method on multiple datasets and models  Cons: * Lack of novelty: the proposed method is a simple combination of two popular methods, LLoyd s quantization and noise aware training  All reviewers appreciate the simplicity of the method and the good fit to the hardware.  The authors responded to all reviews and two reviewers acknowledged the authors  response.  The authors acknowledge some reviewer observations (motivation of quantization as reducing analogue noise, lack of experiments on the actual CACIM system), and the authors added an experimental evaluation on the actual physical CACIM system showing that their method performs well.  Overall the work is well executed and the proposed method is a good fit to the CACIM system.  However, the proposed quantization method is a straightforward adaptation of popular quantization methods.
The reviews are a bit mixed, so the AC independently examined the submission as well. While the authors  response helped clarifying some issues, the draft would still need some major revision, especially the motivation and the experiment part. Here are some concrete suggestions:  (a) The problem definition in Eq (1) is already problematic, as the authors term the least squares reconstruction as an "inversion." When exact reconstruction is not possible, one needs to question the choice of the 2 norm here: why 2 norm and what happens if we change the norm? How does this choice of norm enter the proof and the theorem statements? The uniqueness, as pointed out by one of the reviewers, requires further elaborations: It is not clear that for applications like image denoising or impainting it is necessary to have a unique latent vector, if all one cares about is good reconstruction.  (b) The authors need to explain the motivation of the "inversion" problem better, in the context of generative models. In generative models, G pushes a distribution p on R^{n_0} to q on R^n. If both p and q have nontrivial support and n_0 < n, the map G can be highly irregular and the reconstruction problem can be very ill conditioned. Putting assumptions on G (such as incoherence) might heavily constrain what kind of distributions q we can learn. This trade off was hardly discussed. In fact, if we take n_0   n, then there exist normalizing flows that can learn any distribution q and that can be trivially inverted. If one is interested in inversion, why not use a universal normalizing flow?   (c) The authors mentioned a few possible applications (image denoising, compressed sensing, image inpainting) of the inversion problem, but none of them (to my best knowledge) relies on modelling the underlying distribution. There are also classic algorithms for each of these applications. It would be much more convincing if the authors could explain why a deep generative model is advantageous for these applications and compare the proposed algorithms on standard benchmarks of these applications. The AC agrees with the reviewers that the current experiments are a bit toy ish (which is not wrong by itself but does require a bit more elaboration when the motivation is in question).  (d) The significance of Theorem 1 requires further elaboration. How does one verify its conditions? How often does a trained network satisfy these conditions? How do these conditions restrict the expressiveness of the underlying model? Some of these questions were asked in the reviews but regrettably the authors largely dismissed them. Results like Theorem 2 also require further clarification: what does random weight mean for a generative model? The authors seemed to only care about inversion while completely ignored the expressiveness of a generative model. As a trivial example, one could take a linear network, with regularization we can always invert (in the sense of the authors  definition) the signal. Why is this any different from (if not better than) the authors  results?
The paper proposes a hybrid VAE normalizing flow for extracting local and global representations of images.  While the reviewers found the model itself to be "conceptually simple" and "straightforward", all were convinced by the empirical evaluation that, indeed, interesting representation learning is going on, resulting in a unanimous vote to accept.
New generative model to come up with data that is needed when doing contrastive learning. Like the fact that multiple modalities were considered and evaluated. The Viewmaker methods appears to do well on CIFAR 19 and outperforms baselines on speed and wearable domains. The reviewers praise the method for being simple, well described and well motivated. The main drawbacks stem from the fact that viewmaker cannot make certain types of image specific augmentations (crop & rescale, as an example), but it s fair that the authors argue that their method is more domain agnostic; and one can indeed add more domain specific stuff if needed.  All in all, this seems like a solid paper with an easy to implement idea that is quite general and that has been shown to work in a variety of settings. It definitely belongs at ICLR.
The approach proposed here have raised major concerns from multiple reviewers especially concerning the novelty and the experimental validation procedure.
The paper addresses the important problem of classification with unbalanced semantic classes. The key idea is a two stage process that first learns a representation under various distributions (experts), then assign a cascade of experts to hard samples, whose predictions are combined. The approach can be added on top of various backbone networks. Experiments are systematic and extensive, showing improvement on three standard benchmarks for this task.  All four reviewers recommended accept.  The paper extends a recent research direction of learning "balanced" representations, followed by distribution aware experts. This general approach could have wider impact on architectures designed for out of distribution and low shot learning.  The authors should update the final paper based on their answers and on reviewer feedback. We also encourage authors to make their code available. 
The reviewers raised a number of concerns which are addressed by the authors. The paper provides an interesting/novel perspective for federated learning (as a posterior inference problem rather than an optimization problem) which can potentially allow for faster and more accurate solutions. 
This paper presents a new benchmark for evaluating continual learning(CL) algorithms on transferability and scalability. It also introduces a data driven prior to reduce the architecture searching space. Experiments show the new benchmark helps to analyze the properties of CL algorithms and the proposed algorithm performs better than baselines.  The reviewers raised concerns about evaluation metric, weak baselines, and limited experimental cases for evaluating transferability. The authors added more experiments with stronger baselines and revised the paper based on the reviewers  suggestions. However, the authors also admit that how to evaluate transferability is still an open question.   Despite the concerns, the reviewers generally agreed that the paper is well written, and the new benchmark is an important contribution for evaluating continual learning algorithms on transferability and scalability.  Hence it makes a worthwhile contribution to ICLR and I m recommending acceptance of the paper.
The paper presents an algorithm for real time auto ML based on zero shot learning, which matches an ML pipeline to a dataset via the meta features of the pipeline and the dataset.  It aims to address an important problem, and the idea of the proposed solution seems interesting. However, there are several issues with the current draft:   (1) A central question is how to justify the complexity of the proposed solution, given that simple alternatives, such as Random Forest, can achieve similar or better results than the proposed method.   (2) The proposed solution lacks in novelty contribution of methodology and theoretical justification   (3) Various technical details are missing in the current draft.  The authors  feedback did not fully address the issues above. We hope that reviews can help the authors improve the draft for a strong publication in the future. 
The paper introduces a variant to the option critic framework that encourages options to display a certain level of "diversity" and this is induced by introducing a mutual information objective between the options and their transitions. The authors conjecture that such criterion makes options more suitable for exploration.  Overall, reviewers agree that the idea behind the proposed method and the general approach is sound and interesting. Nonetheless, there is general consensus that the current submission suffers from a number of shortcomings that make it unsuitable for acceptance.  Following the detailed comments provided by the reviewers, I strongly encourage the authors to focus on the following dimensions to improve the paper: 1  The current experiments indeed provide a first illustration of how the proposed algorithm works, but they need significant improvement in variety and scope: As pointed out by the reviewers, the current experiments do not cover single reward challenging exploration benchmarks (such as Montezuma). I agree with the authors that the inductive bias implemented in their algorithm is designed with diversity of goals in mind, but if the main point is to improve exploration, it is natural to expect results in that respect. Alternatively, the authors should state more explicitly the type of problems their method is intended to solve from the very beginning of the paper and design experiments accordingly. 2  The initial mutual information objective is simplified across multiple steps and it is unclear how much the approximations impact the original "semantic" of the objective. 3  A more thorough comparison with mutual information based methods such as DIAYN or VIC is needed. Also, I wonder what is the connection with more goal based exploration approaches such as GoExplore or SkewFit.
I agree with the reviewers, and I find the careful analysis of CL approaches relying on regularization for RNN useful and insightful. I do feel that a lot of the interesting content is still in the appendix (from a quick skim and looking at the plots in the appendix) but I think something like this can potentially be unavoidable.   I do like the separation between sequence length and memory requirements. I think making observations about different types of recurrent architectures is hard, but I think the paper does a good job to raise some interesting questions.   A note that I would make (that I haven t seen raised through a quick look in the paper) is that is not clear how the Fisher Information Matrix should be computed in case of a recurrent model (which is a problem in general). E.g. a typical thing is to compute it as for a feed forward model (using the gradients coming from BPTT) which is feasible computationally, but actually that is problematic as you first sum gradients before taking their outer product rather than summing the outer products corresponding of the different terms in the gradient. I m wondering if that plays a role here as well.  Overall I think the paper does careful analysis and ablation studies and raises some interesting observation of how one should approach CL algorithms for RNN models.  
This paper gives a new PAC Bayesian generalization error bound for graph neural networks (GCN and MPGNN). The bound improves the previously known Rademacher complexity based bound given by Garg et al. (2020). In particular, its dependency on the maximum node degree and the maximum hidden dimension is improved.  This paper gives an interesting improvement on the generalization analysis of GNNs. The writing is clear, where its connection to existing work and its technical contribution are well discussed.  The biggest concern is its technical novelty. Indeed, the proof follows the out line of Neyshabur et al. (2017). Given that the technical novelty would be a bit limited, however, the analysis should properly deal with the complicated structure specific to GNNs which makes the analysis more difficult than usual CNN/MLP and requires subtle and careful manipulations.  In addition to that, the improvement of the generalization bound is valuable for the literature (while the improvement seems a bit minor for graphs with small maximum degree).   For these reasons, I recommend acceptance for this paper.
The paper introduces a model agnostic heuristic for batch active learning.  There was an agreement among the reviewers that it s a good approach to try and report about, but the paper was ultimately rejected after calibration.  There were two concerns raised in the reviews, and the authors are encouraged to address them in a revision:  1) Several reviewers commented on issues with readability, affecting the paper s reproducibility (see reviews for details).  The reviewers would have also liked to see more evidence of empirical robustness to various choices made.  2) For the paper to be compelling, it should either compare with gradient based approaches like BADGE (Ash et al. 2019) or include experiments with a representation where BADGE can t be applied (to support the model agnostic distinction the authors are making).  The core motivation is the same, with both approaches trying to explicitly incorporate predictive uncertainty and sample diversity, and it would be interesting to see a comparison.
The paper proposes a DP method for generative modelling based on optimal transport. The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either. So, even though this is a valid approach, correctness is not sufficient for acceptance at ICLR.  
The paper is about an approach that combines successor representation with marginalized importance sampling. Although the reviewers acknowledge that the paper has some merits (interesting idea, good discussion, extensive experimental analysis) and the authors  responses have solved most of the reviewers  issues, the paper is borderline and the reviewers did not reach a consensus about its acceptance. In particular, the reviewers feel that the contributions of this paper are not significant enough. I encourage the authors to modify their paper by taking into consideration the suggestions provided by the reviewers and try to submit it to one of the forthcoming machine learning conferences.
The authors propose a new model to learn voice style transfer using an encoder decoder framework with the aim of disentangling content and style representations.  The strengths of the paper are: + the method is well motivated with sound theoretical justification + the authors improve up on the prior work by augmenting the loss with an information theoretic term + empirical evaluations demonstrate performance improvements in speaker verification and speech similarity tasks + demonstrate improvements in the challenging zero shot task  Several reviewers requested improvements in readability + “ideally the central intuitions and actual specific bottom line criteria used would be much clearer.” + more clarify on empirical details including challenges that needed to be addressed 
The paper proposes an approach that generates pseudo labels along with confidence to help semi supervised learning. Then, selected pseudo labels are used to update the model. Moreover, the authors include a variation of mixup for data augmentation to train a more calibrated model. Experimental results justify the validity of the proposed approach.  Several reviewers believe that the paper is somewhat well written. The main concern is on the novelty of the work. In particular, many works have discussed selected treatment of unlabeled data, data augmentation for semi supervised learning, and label confidence estimation. Those works deserve more discussions/comparisons. The paper can also be improved with deeper experimental studies that better justify the main assumption and merits of the proposed approach.  
This paper analyzes dropout and shows it selectively regularizes against learning higher order interactions. The paper received mixed reviews, with two in favor of rejection and one in favor of acceptance. Specifically, while all reviewers find the intuitions and ideas in the paper adequate/plausible, two reviewers didn t find sufficient evident that supports the conclusions. The reviewers provided very detail feedback, which the authors responded to, but it is apparent that some of the analysis needs to be reviewed again before the paper can be published.
This paper defines a truly unsupervised image translation scenario. Namely, there are no parallel images or domain labels. To achieve robust performance in this scenario, the authors use 1) clustering and 2) generator discriminator structure to map images from different domains and generate images for target domains.    In all, all the reviewers agree that this definition of unsupervised image translation is interesting. However, there are also several concerns for the real world practical application and empirical results.  Unlike unsupervised text translation whose target language is known, the truly unsupervised image translation is difficult to make sense without identifying what is the target domain. This limits the contribution of this paper to some specific tasks instead of more general tasks. For the empirical results, the selection of data and the hyperparameter K do not convince the reviewers.  
The reviewers agree that the proposed method for reducing overconfidence in ReLU networks is novel and interesting. However, the presentation of the theoretical results is too informal and imprecise to warrant acceptance without a strong accompanying experimental section, which is unfortunately lacking. I therefore cannot recommend acceptance of the paper in its current form.
This paper proposes a selection mechanism to choose between a certified model with low clean accuracy and a naturally trained model with high accuracy, to improve the standard clean accuracy for certifiably robust models.  At a high level, the idea behind this combined system is that when the certified model cannot certify, one should avoid using it for classification, but rather should use a naturally trained model.  A state of the art naturally trained networks is used as the "core network", and a small certification network with high certifiable robustness is used as the "certification network". The major contribution is a selection network that adaptively chooses between these two networks.   Pro + The idea of using two networks adaptively is novel. The proposed selection mechanism has been shown to be able to combine the merits of both networks to obtain better natural accuracy with good certified robustness.    Con   The experiment section still has room for improvement. Specifically, the presentation of the results were not convincingly conveying the tradeoff between the clean accuracy and the certified accuracy.  After the rebuttal, the authors made some improvements that addressed many of the concerns about the clarity and reproducibility issues. However, reviewers suggest further polishing the experiment section.   Overall, I think the novelty of the paper combined with the promising results achieved outweigh the presentation issues. I would recommend accepting this paper. 
Reviewers agreed on the value of theoretical contribution, especially the surprising conclusion that the weight tied and untied RNTK are identical. The empirical results were updated in response to reviewer s suggestion. I believe this would be of interest to ICLR audience.
The paper proposes a new spatial temporal point cloud convolution. However, many reviewers suggest the paper can be improved with better baselines and motivations.
The paper discusses an extension of BERT for learning user representations based on activity patterns in a self supervised setting. All reviewers have concerns about the validity of the claims and the significance of the experimental results. Overall, I agree with the reviewers that the paper needs more work to be published at ICLR. I recommend rejection.
This paper combines considers the task of finding a minimal set of inputs that explain predictions of trained neural models. The authors propose a method that they refer to as "scaling symbolic methods using gradients" (SMUG). This method use integrated gradients methods to score first layer neurons on the degree to which they influence the prediction and then produces and solve an SMT problem (restricted to first layer activations) that finds the minimal mask that changes these influential neurons.   Reviewers had somewhat mixed perspectives on this submission. All reviewers were broadly in agreement that the paper is clearly written and presents an interesting combination of symbolic (i.e. SMT based) and gradient based methods for model explanation. R2 questions the need for sparsity (and therefore the SMT component) in model explanations, and R3 similarly notes that SMUG does not necessarily rely on SMT at all. That said, no reviewers raise major concerns with the quality of exposition, experimental evaluation, or the level of technical contributions in this work. The metareviewer is inclined to say that this work is above the bar for acceptance, and represents a reasonable approach to integrating SMT based and gradient based methods for model explanation.
The reviewers pointed out that the claims made in this submission have already appeared (in even stronger forms) before, to which the authors seem to agree. Therefore, this submission is not ready for publication in its current form. 
This paper presents a useful contribution to the growing literature on uncertainty estimation with deep learning. The review process has significantly helped with strengthening this paper, specifically with the concerns about novelty and sufficient comparisons to existing work. I hope you will continue to improve this work for submission to a future venue.
This paper is concerned with the ongoing research program of mapping the approximation power of different GNN architectures. It provides significant advances in the study of equivariant GNNs and nice extensions in the invariant case by closing existing gaps between distinct GNN families.  All reviewers agreed that this is a strong submission with substantial new theoretical results. The AC recommends a strong acceptance. 
In federated learning, distributed and resource limited client nodes cooperatively train a model without sharing their local data. The results thus far on analyzing the  convergence of federated learning are restricted to “unbiased” client participation, where the probability of a client c being selected is proportional to c’s data size. This work presents the first convergence analysis of federated learning for biased client selection, and quantifies the impact of selection skew on time to convergence. Specifically, biasing toward clients with higher local loss is shown to be beneficial, and a protocol is developed based on this, to trade between convergence time and solution bias.  The paper is in general well written, and develops a natural idea.   The strong convexity assumption is a concern: how much can it be weakened? The authors are also asked to run experiments systematically on (much) larger datasets. The test accuracy and possible overfitting concerns also need to be addressed in more depth. The authors are also encouraged to see how much Assumption 3.4 uniformly bounded stochastic gradients can be dispensed with.  
This paper aims at improving the adoption of Bayesian NNs by providing a practical and user friendly variational inference method. The main ideas consist of two parts: 1. Warm start the variational inference from a pre trained deterministic NN. It takes advantage of existing deep learning library features for easy implementation including weight decay, batch matrix multiplication, etc. 2. Calibrating uncertainty estimation for out of domain detection using adversarial examples.  Pros: 1. A practical way of implementing DNN variational inference with reduced variance, without sacrificing classification accuracy of the pretrained NN model. 2. Significantly better OOD detection accuracy compared to other BNN approaches without taking OOD into account explicitly.  Cons: 1. During discussion, it becomes clear that most of the techniques have been proposed similarly in the literature. Krishnan, 2020 applied BNN starting from MAP of NN, Flipout (Wen et. al., 2018) applies instance wise sampling, Hendrycks et. al., 2018 and Hafner et. al., 2018 improves detection accuracy by training on OOD examples. The novelty of the proposed method is therefore limited. 2. There s not much benefit on the classification performance compared to the initial MAP and is inferior to MCMC based SOTA BNNs. One of the reviewers considers the SGLD type approach may be more appealing to ML practitioners with the overhead of VI in training additional variance parameters. 3. The authors argue MCMC based BNN methods cannot achieve good performance without temperature scaling. But the main performance improvement of the paper is in the OOD detection with uncertainty regularization that modifies the posterior as well. The method of training with OOD samples is orthogonal to applying Bayesian inference to NNs, and the detection performance is limited to the distribution close to examples during training.  This paper falls on the borderline for acceptance. With the goal of improving adoption of BNN in practice, it is not convincing yet making mean field VI easier to implement could realize it without achieving competitive performance. 
The paper presents a PAC Bayesian approach for meta learning that utilizes information of the task distribution in the prior. The presented localized approach allows the authors to derive an algorithm directly from the bound   this is a worthwhile contribution. Nevertheless there are several concerns that were raised by the reviewers and in its current form the work is not ready to appear in ICLR.    
The paper proposes a new teacher student framework where the teacher network guides the student network in learning useful information from trajectories of a dynamical system. The proposed framework is inspired by the Knowledge Distillation method. The teacher learns what information should be used from the trajectories and distills this information for the student in the form of target activations. In a nutshell, the framework allows the student to interpolate between model based and model free approaches in an automated fashion. Experimental evaluation on both the hand crafted and simulated tasks demonstrate the effectiveness of the proposed framework. The reviewers had borderline scores in their initial reviews and raised several questions for the authors. The reviewers appreciated the rebuttal, which helped in answering their key questions   I want to thank the authors for engaging with the reviewers during the discussion phase. The reviewers have an overall positive assessment of the paper, and believe that the proposed teacher student framework is novel and potentially useful for many real world problems. The reviewers have provided detailed feedback in their reviews, and I would like to strongly encourage the authors to incorporate this feedback when preparing the final version of the paper.
Four knowledgeable referees reviewed this paper; one reviewer (weakly) supports accept and other three indicate reject. Even with the rebuttal, all reviewers (including positive reviewer) have concerns on unconvincing experimental results (due to missing baselines for instance). I basically agree on negative reviews that this submission fails to have enough quality considering the high standard of ICLR.
The concept of increasing entropy in novel states to promote exploration appears to be quite interesting. I do appreciate this idea, and I would encourage the authors to study it further. I think the reviewers also agree with this. Unfortunately, the paper as written has a number of issues: (1) the theoretical motivation for this is quite weak   it intuitively makes sense, but for a published paper, we need more than intuition; (2) the empirical results are not especially compelling, it seems like the authors have to kind of thread the needle in arguing that they are concerned specifically with dense reward exploration   a less chartable view is that the method just doesn t work all that well compared to other exploration settings in problems that present a major exploration challenge. The reviewers generally found the evidence in favor of the method to be a bit questionable. Therefore, in the balance, while the paper presents what I think is a really nice idea, it still needs work to justify both theoretically and empirically. I would encourage the authors to flesh out this work more   I think with a bit more work, it could be a really nice paper, but for now it s probably not quite ready.  A few more comments (which did not influence the decision, but I recommend addressing in the future):    Presenting exploration and efficiency results in a table, like Table 1, is not good. It hides actual patterns in performance, especially if you are talking about exploration. To the authors  credit, it appears that this trend was started by prior work (e.g., Kimin Lee et al.), but it was bad scholarship then, and it s bad scholarship now   it s quite easy to pick a checkpoint where a given method looks better than all the other methods (which might be why some prior work opted for this format), but it s misleading to the reader and should not be done.    I don t agree with Reviewer 2 s comments about random seeds. Certainly it s better to have more random seeds than less, but this doesn t appear out of line with the standard in the field. That said, the results in Figure 4 do look quite close, so trying an actual statistical significance test might be a good idea (again, this didn t influence my decision   the standard in RL holds that 4 6 seeds is plenty, and we have to review work by the current standard in the field).
This submission aims to improve adversarial training by making it involve also layer wise (instead of only input wise) perturbations. This is an interesting idea and it is accompanied by an interesting ODE based perspective on the resulting dynamics. However, as the comments and reviews detail, the current manuscript misses the discussion of very relevant previous work, does not specify important details of the approach (e.g., how to bound the extent of the perturbations used), and relies on weak primitives (FGSM vs PGD).   The consensus is that this would be an interesting and valuable contribution but only after addressing the above shortcomings. 
Reviewers were concerned about the technical novelty because the two stage sampling strategy is similar to BBN and the decoupling of features and classifiers. Rebuttal addressed some concerns about the experiments, but Reviewers  major concerns remained. 
The paper received mixed reviews that overall lean negative.   The main concern shared by reviewers is the novelty of the findings. Although the paper presents a systematic study that certainly has value, reviewers do not find sufficient insights from the analysis. The ACs agree with the reviewers that the paper is below the bar for acceptance. 
The paper extends results from the recent work of Steinke and Zakynthinou (SZ) for the test loss of randomized learning algorithms. They provide bounds in the single draw as well as PAC Bayes setting. The main result is about fast rates the proof of which follows with minor modifications from the corresponding result in SZ. It is unclear to me the contribution over existing work is sufficient to merit acceptance.
The paper considers new notions of adversarial accuracy and risk which are called "genuine" with an aim to fix issues with the existing definitions in the literature. A number of issues in the paper, including lack of motivation and intuition, and poor formalism were identified by the reviewers. The paper also fails to cite some of the previous literature that has identified similar issues. The authors have only responded to some of the questions raised by the reviewers. 
The manuscript presents a deep network approach for heteroscedastic regression problem. It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss.  Three reviewers agreed that the manuscript is not ready for publication. The major issue is the lack of novelty. Heteroscedastic regression is a classic problem in statistics. And reweighting using the inverse variance is a textbook method.   R2 and R4 confirmed that they have read author response. The rebuttals are useful to clarify some points, especially related to experimental settings and results. However, they are not convinced by the authors  argument on novelty and whether the assumption is realistic.  
The paper introduces a new variant (SREDA Boost) of a variance reduced method SEDRA for nonconvex strongly concave min max optimization. Given that SEDRA is already optimal in the worst case, the proposed modification is intended to improve practical performance of the method, by relaxing conditions needed at initialization and allowing larger step sizes. While the reviewers appreciated the main ideas of the paper, they shared concerns about the significance of the paper s technical contributions, which were ultimately not addressed by the authors in the rebuttal phase. 
 The paper proposes ALFWorld, which combined TextWorld and ALFRED to create aligned scenarios (one that is text only, and the other in an embodied visual simulator) so that high level policies in language can be learned in a simpler world, and then transferred to the embodied one (using the proposed BUTLER architecture).  The proposed BUTLER model consists of three components: 1) a perceptual module (converts environment observation to specification of objects and relations using text), 2) goal planning module for generating textual specification of subgoals (from observed environment state) and 3) controller module which takes outputs from 1) and 2) and generates a sequence of actions.  Experiments show that using the textual specification, it is possible to models pretrained in the text world can generalize better to embodied settings.  Review Summary: The submission received slightly divergent reviews with R2, R3 recommending acceptance (score 7) and R1 recommending reject (score 4).  All reviewers recognized the novelty of the work, and the potential for follow up work based on the submission.  After considering the author response and discussion between reviewers, both R2 and R3 agreed that there are indeed flaws with the work as pointed out by R1 (R3 lowered their rating to 6).  Despite the concerns, both R2,R3 remained on the positive side.  Pros:    The work and proposed framework can stimulate further research on transferring policies from simple text environments to more realistic visual environment. (R2)   The decomposition of high level goals into low level actions sequences is a good direction for future research (R3)   Good set of experiments and comparisons (R3)   the paper is clearly written and easy to understand (R2)  Cons:   The main claim of the work (high level policies learned in a text based environment can be transferred to a physically simulated environment) is not properly substantiated by the experimental results. (R1)   The proposed method is a complex system and simpler baselines should be considered (R2)   Some assumptions are made in ALFWorld need to be hand designed and may miss important aspects of perception (R3)    More experiments and ablations are needed to properly evaluate the framework  Despite the issues pointed out by R1, the AC believe that the work can inspire future work in this area, and thus recommend acceptance.  The paper is also well written and easy to understand. 
This paper follows the observations of Renda et al. (2020) that the learning rate in the fine tuning or retraining phase of neural network pruning is an under considered component of the pruning process. Renda et al. (2020) argue for a technique that uses the learning rate schedule of the original training regime for fine tuning. However, their work does not offer a hypothesis or an explanation for why this works.  This work instead offers more insight into why reusing the original learning rate is productive. Specifically, it shows that using high learning rates is the key component. To demonstrate this, the paper includes a study of using the original step wise learning from the original training regimen, except accelerated for a given number of fine tuning epochs. The paper also demonstrates that Cyclic Learning Rate Restarting (CLR) also provides an effective, if not better, learning rate schedule for fine tuning.  As noted by the reviewers, the core observations and contributions of this work are modest, but are still a valuable addition to the literature in the pruning community.    Having said that, there are some confounding issues with CLR. Specifically, that CLR itself may simply be a more effective learning rate schedule for training neural networks, independent of the particular application to fine tuning (Reviewer 1). The revision includes an additional appendix that dispels some of this concern. However, indeed, the CLR does improve the base network performance for some configurations.  Broadly, the value proposition here is a thorough demonstration of learning rate schedules for fine tuning with an overall take that comparisons between techniques need be more sensitive to this choice as previous work perhaps has not thoroughly considered alternative learning rates.  
The paper proposes to speed up self supervised learning for semi supervised learning by combining self supervised pretraining and supervised fine tuning into a single objective. The proposed supervised loss builds on Neighbourhood Components Analysis and soft nearest neighbor losses. Most reviewers are concerned about the novelty of the approach and the significance of empirical results. I agree with both concerns. I appreciate the comparison between $\log \sum \exp$ and $\sum \log \exp$, but it seems a simpler cross entropy loss also achieves a similar goal (potentially somewhat slower). I believe adding more experiments comparing different supervised loss functions across different architectures can help improve the paper. 
This paper studies the effect of using unlabelled out of distribution (OOD) data in the training procedure to improve robust (and standard) accuracies. The main algorithmic contribution is a data augmentation based robust training algorithm to train a loss which is carefully designed to benefit from the additional OOD data. What s also interesting is that the OOD data is fed with random labels to the training procedure. As demonstrated in the theoretical results, this way of feeding OOD data helps to remove the dependency to non robust features and hence improves robustness.    As pointed out by all the reviewers (which I agree with), the idea of using unlabelled OOD data at training is novel/interesting, and the paper also shows how this can be done algorithmically. The numerical results also confirm the effectiveness of the proposed methods. 
Three of the reviewers are significantly concerned about this submission, while R1 is positive. Meanwhile, R1 is also concerned about some details in the paper, including space and time complexity etc. The authors provided detailed feedback to these comments, but R1 does not provide support to this work during discussions. Thus a reject is recommended.
The paper provides a simple prediction procedure to defend against (rectangular) patch attacks, and also a method to obtain some random estimates of the certified robustness of the method. The simplicity of the method is certainly appreciated. On the other hand, there are a number of issues preventing the acceptance of this paper. The main problem is that the paper deals with a randomized predictor, yet the certification guarantee developed for deterministic predictors is applied. This leads to several problems, starting from the target being undefined to unfair comparisons. While the authors made an attempt to address this in the rebuttal, more work is needed to properly settle this issue.
This paper introduces a consistency loss for instance discrimination by adding a term to maximize the squared dot product between two views of the same image. The impact of the proposed approach is evaluated on a variety of settings with mixed improvements. While reviewers generally found the proposed method to be interesting, there were concerns regarding the novelty of the approach, the size of the performance improvement, and the choice to focus on instance discrimination vs. more recent approaches based on contrastive instance discrimination.   While I do not share the reviewer concerns regarding novelty, I am sympathetic to the concerns regarding the size of the improvement and the focus on instance discrimination. As such, I recommend that the paper be rejected in its current form. I would encourage the authors to apply their analysis and method to more recent contrastive instance discrimination approaches such as SimCLR and SWaV as well as non explicitly contrastive, but high performing methods like BYOL. I would also encourage the authors to provide quantitative empirical analyses demonstrating the impact of the consistency term on large models rather than just toy models to demonstrate the impact of the consistency term in representational space. 
The paper introduces a GNN approach to solve the problem of source detection in an epidemics. While the paper contains some interesting new ideas, the reviewers raised some important concerns about the paper and so the paper should not be accepted in the current form. In particular,    the paper does not motivate the ML approach to the problem   the experiments are limited for an empirical paper   the method used in the paper is not very novel   the proofs presented in the paper are not formal enough 
This paper improves on previous work (adv BNN) with hierarchical variational inference. It observes that mean field VI training for BNNs often result in close to deterministic approximate posterior distributions for weights, which effectively makes the BNN closer to deterministic neural network, thereby loosing the robustness advantage of stochastic neural networks. To address this, a hierarchical prior is proposed on the weights, which, together with the corresponding approximate posterior design, aims at preventing the collapse of the variances of the weights towards zero. This improved version of adv BNN is shown to be reasonably better than the original adv BNN and their deterministic counter part against the PGD and EOT attacks on a various of benchmark dataset in the adversarial robustness literature.  Reviewers initially had questions about whether the comparison is fair to the original adv BNN since the reported results were very different. This issue has been addressed by the authors during the author feedback period, after that reviewers agreed that the proposed approach is a good extension of adv BNN towards making it more robust. They also agree that the analysis of the original adv BNN in terms of posterior variance collapse is interesting and potentially useful, although they also pointed out the link of increased variance (with the proposed method) and better uncertainty estimation is unclear.  In revision, I would encourage the authors to clear up the confusions of the reviewers by clearly stating the comparison setting with the original adv BNN, and better clarify the methodology.
The authors extends the transformer to multivariate time series. The proposed extension is simple, and lacks novelty. Some design decisions of the proposed method should be better justified. Similar works that also use the transformer for timeseries are not compared.  Experimental results are not convincing. The settings are unclear, and the selection of datasets needs more justifications. Some important experiments are missing.  Finally, writing can also be improved.
This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5). The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough. Some of these issues are addressed in the rebuttal, though. Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper.
This paper is a variant of the large growing class of Neural ODEs, and adds dependency on a time delay to the baseline, which allows to model a larger class of physical systems, in particular adding the possibility of crossing paths in phase space.  After initial evaluation, the paper was on the fence, with 2 reviewers providing favorable reviews, and 2 reviewers recommending rejection. A particular important issue raised was positioning with respect to prior art, [Dupont 2019], with some substantial overlap between the papers; requests of theoretical discussions of the class of studied systems and its properties.  Most of these remarks have been addressed by the authors, in particular positioning and experimental comparisons.  The AC judged that the paper had been sufficiently improved and recommends acceptance.
All reviewers recommend acceptance. Some concerns were raised about the precision of theorem 2 (now renamed to proposition 1), as well as the analysis of hyperparameter choices and quantitative evaluation, which I believe the authors have adequately addressed. Based on a suggestion of reviewer 1, experiments with flow based models were also added, which demonstrates that the method is not strictly tied to autoregressive models. Personally, I was also curious about the connection between noise injection and quantisation, which the authors responded to by adding a paragraph discussing this connection in the manuscript.  I would recommend that the authors also add the kernel inception distance (KID) results reported in the comments to the manuscript.  This work stands out to me in that it combines a relatively simple, easy to understand idea with nice results, which is a trait of many impactful papers. I will therefore join the reviewers in recommending acceptance.
The paper presents an KL divergence minimisation approach to the action–perception loop, and thus presents a unifying view on concepts such as Empowerment, entropy based RL, optimal control, etc.  The paper does two things here: it serves as a survey paper, but on top of that puts these in a unifying theory.  While the direct merit of that may not be obvious, it does serve as a good basis to combine the fields more formally.  Unfortunately, the paper suffers from the length restrictions.  With more than half of the paper in the appendix, it should be published at a journal or directly at arXiv.   Not having a page limit would improve the readability much.  ICLR may not be the best venue for review papers.
This paper proposes a model for learning disentangled representations by assuming the slowness prior over transitions between two frames. The model is well justified theoretically, and evaluated extensively experimentally. The results are good, and all reviewers agree that this paper is among the top papers they have reviewed. For this reason, I am pleased to recommend this paper for an Oral.
This paper presents a new NAS benchmarks for hardware aware NAS. For each of the architectures in the search space of NAS Bench 201, it measures hardware performance (energy cost and latency) for six different hardware devices. This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these hardware aware measurements on as many as six (very different) devices.   The code has been made available to the reviewers during the author response window and has been checked by the reviewers in the meantime. All reviewers appreciated the paper and gave (clear) acceptance scores.   Before this work, it was very hard for the average NAS researcher to assess their method properly in a hardware aware setting, and I expect this work to change this, and to open up the very important field of hardware aware NAS to many more researchers. For this reason I recommend to accept this paper as a spotlight.  
This is a borderline paper. Initially, it received  weaker reviews (than the current ones) and after rebuttal, one reviewer slightly increased the review rating.  Among 4 reviews, there is one clearly positive one.  Among negative reviews, there are repeated concerns about evaluation. While recognizing the difficulty of finding (large scale) datasets for sequential  regression tasks,  reviewers suggest running the methods on benchmark datasets. One reviewer also points out the need to compare to efficient ensemble methods for uncertainty quantification.  The AC also read  the manuscript. The AC appreciates the strong performance of the proposed method. However, given the limited number of datasets this method is evaluated, and the lacking of analysis of understanding why the method is successful or could fail, the AC recommends Reject.  The author(s) could improve the manuscript by following the suggestion of the reviewers, as well as a more detailed analysis of how robust the proposed method is: there are many parameters tuned in the empirical results and it is not clear why sensitive those parameters are. 
This paper proposes an approach for coordinating teams with dynamic composition consisting of an attention mechanism, regularization and communication. The clarity of the paper is currently low seemingly due to the conflated message of the multiple parts of the framework. Improvements to the text via the suggested edits of all reviewers should be a relatively quick fix, but the clearer placement of this piece within the wider literature may require additional experiments to compare against so would be a larger change.   The reviewers did continue to discuss the paper after the end of the open discussion period with the authors and appreciated the additional experiments performed. In the absence of supporting theory, empirical results in a second domain significantly improve the evidence that the method may be more generally applicable. However, the new experiments raised new questions (included in the reviewers later replies) indicating more experiments in the second domain are needed which would require further peer review.  I hope the authors will take the constructive feedback provided here as intended; to improve the paper, submit the work again at a later stage when the second experimental domain is sufficiently explored to support the proposed framework and in doing so maximise its potential for impact. 
This paper explores a network that has a parvo (fine, detailed, slow) and magno (low res, quick) stream.  The ideas are interesting and the results intriguing, and one reviewer is in favor of acceptance. Several reviewers criticized the clarity of the paper. and the lack of details for, explanations of, and critical evaluation of, the design decisions.  For example, how do the results depend on certain design decisions?  I think that with a bit more work, this paper has potential to be a very impactful paper.  I would encourage the authors to follow the  detailed suggestions and resubmit the work to a high impact conference or  journal.  
This paper studies test time adaptation in the context of adversarial robustness. The key idea is to use a maximin framework, which illustrates non trivial robustness (under transfer attack) using domain adversarial neural network (DANN) to Linf norm and unseen adversarial attacks. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.  However, there exists some limitations, for example, 1) The adaptive attack results are concerning. Comparing Table 1 and Table 3, with the adaptive attack (J FPAM), the accuracy in the homogeneous setting is below that of adversarial training (Adv S) in Table 1, which somehow echoes my concern on not testing the transductive setting using strong attacks. It seems that adversarially trained models can better defend the adaptive attacks. 2) The paper says "This threat model enables us to study whether a large test set can benefit a defender for adversarial robustness", yet there is no any experiments in the main paper that correspond to this discussion. The appendix seems lacking this discussion either. 3) Due to the page limit, a lot of details have been moved into the appendix, making the paper difficult to read.  In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.
The submission proposes to leverage a commonsense knowledge graph and an attention GNN based model to aggregate the node features on the graph for the problem of zero shot learning. It received three reviews two of them recommending rejection, and another review was initially borderline however they moved to acceptance after the rebuttal. The meta reviewer finds that the paper is not yet ready for publication and recommends rejection based on the following observations.  Although the model is interesting, as agreed by the reviewers the initial version of the paper fell short on convincingly evaluating the method, e.g. generalized zero shot learning (GZSL) setting as pointed out by R5, ImageNet and small scale dataset results as pointed out by R1. Similarly, the main paper (without the annexes) has been found to fall short on providing enough details of the model as pointed out by R1.   The authors ran additional experiments during the rebuttal phase which showed some promise, however one more review round may be necessary to carefully validate these results. As R1 pointed out, the paper only reports results on two small datasets i.e., AWA2 and aPY, which contain classes similar to ImageNet. It would be interesting to observe the behaviour of the model on more challenging scenarios on other publicly available benchmark datasets of fine grained nature whose distribution are far from ImageNet. This would indicate the generalisation ability of the model.  Furthermore, as pointed out by R1, moving the details on the implementation and the architecture details from appendix and from python scripts to the main paper may be beneficial. However, this would end up significantly extending the paper. Hence, one more review round may be necessary for this paper.
Does not seem to be a complete submission (only one page), all reviewers agree on rejecting.
The authors argue that uniform priors for the high level latent representations improve transferability, which is beneficial in a number of tasks involving transference. The approach is evaluated on deep metric learning, zero shot domain adaptation and few shot meta learning.  Pro:   A simple yet effective method   Signifiant gains in experimental study  Cons:   Close variants of this approach were proposed in previous works, and so the novelty of the current work is limited.   There is no accompanying analysis which may shed new light on the advantages of the approach.
The paper studies the benefit of having multiple servers (with partial coverage) in increase the training speed and latency in Federated Learning.  Of course optimization/learning in the multi server setting comes with a number of challenges which the authors seek to address via novel algorithmic procedures (e.g. FedMes).  I believe the paper is suggesting an important, and potentially impactful, methodology to improve the training speedup/latency of FL. I also acknowledge the additional experiments provided by the reviewers which were quite helpful in addressing some of the concerns. However, as the paper mainly relies on experimental studies to evaluate the performance of the proposed methods, the reviewers (and myself) believe that the paper needs some more investigation in which (i) some of the assumptions (e.g. faster communication between the servers) are either removed or validated; and (ii) more complicated  topologies are considered. 
The paper presents a new algorithm, BOIL, on the importance of representation change vs reuse in MAML. All reviewers found the paper insightful, with some proposing a few changes to make the paper even stronger. Like them, I recommend accepting the paper.
This paper propose to learn the embedding of audio segment in the framework of stochastic neighbor embedding (SNE), where the embeddings of the same word shall be close to each other. The method was initially demonstrated for name recognition. The use of SNE for acoustic embedding is novel and this is recognized by all reviewers. There has been quite some discussions between the authors and reviewers/AC, and the papers has got improved since. To summarize: 1. The discussion of the properties of SNE (the reduction from SNE to weighted least squares terms) was not accurate and confused multiple reviewers. The authors have made some clarifications and added citations. As the author claim this to be a contribution, I feel this part of the main text can be further strengthened and made more accurate. 2. For the experiment in main paper, the comparison between proposed method and prior work was not fair since the proposed method use outputs from an ASR system to obtain phone posteriors. The authors then added more results for the word discrimination task in the appendix. But reviewers are still concerned that the authors are not comparing with the strongest variant of He et al, 2017, and that comparisons are shown for embeddings with low dimensions. The reviewers believe this set of experiments shall be more illuminating, and be moved to the main text. 3. The biggest concern at the intuition level is whether it is the best choice to make the affinity binary, which does not take into account the more fine grained similarity between different words. Quoting the comment from Reviewer 6:  "The argument of trying to be as simple as possible is reasonable, but we would have liked to see it motivated by some experiments. Something along the lines of a method which introduced rudimentary edit distance based affinities and then presented their version with hard affinities, and then show some results comparing them. These edit distance based versions could be as simple or complicated as they felt necessary, but it would be nice for some comparison to be made in order to then dismiss them."  Overall, we think the paper is borderline in the current stage, and the paper can be further improved if the above concerns are properly addressed. 
This paper proposes a new clustering method that takes into account side information.  The paper was reviewed by four expert reviewers who expressed concerns for novelty, empirical and theoretical depth, and unclear parts of the paper. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
The paper contributes to the literature of deep kernel learning by proposing a deep probabilistic model based on inverse Wishart distributions. This is an interesting addition to the literature, and the authors have provided some experimental evidence of the superiority of their method compared to state of the art on deep GPs and NNGPs. The major concerns with the paper, though, are related to the clarity of writing and its readability. Additionally, the experimental comparison is thin and lacks exploration for alternative configurations for DGPs and NNGPs. 
This paper proposes a simple but effective method to obtain ensembles of classifiers (almost) for free.  Essentially you train one network on multiple inputs to predict multiple outputs. The authors show that this leads to surprisingly diverse networks   without a significant increase in parameters   which can be used for ensembling during test time.  Because of its simplicity, I can imagine that this approach could become a standard trick in the "deep learning tool chest".    AC
This paper is rejected.  The authors contributions are: * Propose PFPN as an expressive action policy for continuous action spaces. * Introduce a reparameterization trick for training PFPN with off policy methods. * Experiments claiming PFPN outperforms unimodal Gaussian policies and a uniform discretization scheme and that it is more sample efficient and stable across different training trials.  I and the reviewers appreciate the additions by the authors. The GMM baseline is an important addition addressing concerns from several reviewers. However, I agree with R2 s comment that "most interesting contribution of the paper is the resampling scheme. However, there is minimal evaluation of the benefit of this scheme [...] However, the added experiments with random sampling are somewhat worrying the performance improvement of the proposed re sampling scheme is quiet minor over random resampling. In the future, the authors may want to investigate the random resampling for the systems in figure 14." Without resampling, the proposed method is a location/scale state independent GMM policy. It is interesting that this outperforms the fully state dependent GMM, and the authors could investigate that further. To justify the additional complexity of the resampling step, the authors need to perform further investigation and move that to the main text.  In addition, the evaluated environments omit standard OpenAI gym environments (which the authors do have access to as evidenced by their experiments w/ DDPG on them in the Appendix). This makes evaluating baseline method performance challenging. Furthermore, the authors cite Tang & Agrawal (2018) which introduces a normalizing flow policy that outperforms GMM. It would be natural to compare to that baseline. Finally, Figurnov et al. (2018) among others shows how reparameterization gradients can be computed through GMMs. The authors should explain why this is not applicable.
This article proposes a weakly supervised few shot learning method for medical imaging segmentation. While initially, the article presented several problems indicated by the reviewers, e.g., the explanation of the novelty and contributions, the explanation of the method, and the experimental evaluation, the authors made a great effort addressing most of the reviewers  comments and uploaded an updated version of the article. However, still, the evaluation part of the article is a bit weak. But the article contains interesting contributions. Accordingly, I recommend accepting the paper at ICLR2021.
This paper introduces a method to increase diversity/individuality of agents in a MARL setup, based on intrinsic rewards coming from a classifier over behaviours.  Reviewers tend to agree that this is an important/interesting problem, which is related to exploration, a central problem in reinforcement learning. Several reviewers point out that the paper is well written. I appreciate that the authors have been responsive to reviews and have answered and/or addressed several points of concern of the reviewers. The proposed method performs well on the experiments carried out.  Reviews still point out several things that could be improved. The experiments mostly report reward curved, and only few results are actually clearly pointing out the individuality between agents. The fact that this method outperforms the baselines is good, but does not prove individuality and may simply be due to the authors spending more time on the tasks, or other undiscovered phenomenon. A reviewer is concerned that this extra reward could encourage trivial behaviours, and it seems clear that it will if the relative weight of the intrinsic reward is too high. This should be discussed more. Finally, a reviewer points out that classifier based intrinsic reward for diversity already exists in published works and that this paper is incremental work.  The average score for this paper is very close to the acceptance threshold, but based on the reviews I recommend to reject this paper for ICLR 2021. I am confident that when the authors address further the reviewers concerns and improve the experimental results, this paper will be published in a future venue.
All reviewers appreciate the good quality of this submission with a good idea and solid execution (as said by R3). The paper is clearly written and the addition during the discussion have greatly improved it as acknowledged by all reviewers.  However, a major weakness of the submission still needs to be addressed before a publication at ICLR.   As said in the paper, the task of question generation is a task whose main impact is to improve downstream tasks, and primarily QA. The evaluations follow that and extra experiments (e.g. BioASQ) and discussion wrt state of the art (e.g. Alberti et al.) reinforce them. Yet, as pointed out by R1 & R4, the effect on downstream QA performance is only shown for weaker models than the current state of the art (e.g. T5, BART). Since the rebuttal period was not long enough to run these experiments, it is impossible to assess how the proposed approach compare to them with the current draft. Adding the experiments on T5 (Small) is a step in the right direction but it is not enough for that. Without those experiments, one can not conclude that this pretraining strategy would also help over the strongest existing pretrained model.  The authors should run those experiments to make the arguments presented in the submission much stronger.          
In this paper, the authors proposed a solution to the problem of multi source domain adaptation. All the reviewers have two concerns: 1) the technical contribution/novelty is limited, and 2) the experimental results are not convincing. Therefore, this paper does not meet the standard of being published in ICLR. The authors are encouraged to improve this work by addressing the issues raised by the reviewers. 
All reviewers are for accepting the paper: in particular, R1 and R3 found the rebuttal sufficiently convincing to increase their scores from their initial assessment leaning towards rejection.   Strengths: + Clarity  + Simplicity of the proposed approach  + Convincing experiments outperforming reasonable baselines across all problem instances  Weaknesses: + Scale (as noted by R2 and R3) to larger problem sizes, beyond the setting of less than a dozen.  I agree with some hesitation that the paper is narrow in scope (both in interest from the community and scale and ultimately whether it would interest the overall quantum computing audience). However, I think the paper makes significant advances toward the area of adiabatic quantum computation.
There was a fair amount of discussion about the paper.  Several reviewers felt that the paper would have been stronger if it tried to do less but better.  The reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements.  Doing this requires a better understanding of the algorithm s behavior and a valid ablation study, a new concern raised during the discussion with the authors.   The reviewers felt that the proposed approach is potentially interesting and would like to see this paper done well.
Non autoregressive modelling for text to speech (TTS) is an important and challenging problem. This paper proposes a deep VAE approach and show promising results. Both the reviewers and the authors have engaged in a constructive discussion on the merits and claims of the paper. This paper will not be the final VAE contribution to TTS but represents a significant enough contribution to the field to warrant publication. It is highly recommended that the authors take into account the reviewers  comments.
This paper is concerned with sequence segmentation. The authors introduce a framework which they call  lexical unit analysis    a neural network is used to score spans and then dynamic programming is used to find the best scoring overall segmentation. The authors present extensive experiments on various Chinese NLP tasks, obtaining better results than the systems they compare to.  Reviewers raised concerns, including about novelty. In my view, beyond beating the state of the baselines on the chosen tasks, it is hard to extract an actionable insight or novel conceptual understanding. Therefore, the paper is not recommended for acceptance in its current form.
The paper proposes an interesting take on Graph Matching by posing the problem as learning the Topology through Graph Convolutional Networks.  There is consensus that the methods proposed are new but the impact is not clear. One major point against the paper seems to be that Code is yet to be released.
This paper proposes an lightweight method for cross domain few shot learning, using a meta learning approach to predict batch normalization statistics. After the extensive paper revisions and discussion, the reviewers all agreed that this paper is above the bar for acceptance, assuming that the authors will include results for both the standard and expanded target set size in the final version of the paper. The authors are strongly encouraged to include these results in the camera ready version of the paper.
This submission provides a new bound and derived method for unsupervised domain adaptation, based on adversarial training.  The method is then extensively evaluated empirically.  Pro:   the proposed method seems empirically successful  Con:   I agree with one of the reviewers that the presented theoretical justification is not convincing. There should be assumptions on how f_S and and f_T relate in order to drive meaningful guarantees in the proposed framework. The presented results here seem to be a case of a method that was motivated by some theoretical consideration, and worked well after some heuristic approximations were made. It would be nice to see a cleaner analysis of the conditions under which the method will be successful.
The paper studies optimization landscapes arising the fitting of sparse linear networks to data. It argues that for scalar outputs, every local minimum is global, while for d >  3 dimensional outputs, there can be spurious local minimizers. The paper also argues that similar results hold for deep networks. Counterexamples on the existence of non global local minimizers are constructed analytically and corroborated by probing the optimization landscape experimentally.   Pros and cons:  [+] Network sparsification is an important practical problem, and there are relatively few theoretical guidelines on when and how sparsification can be achieved. In particular, results that help to explain why trained networks are often sparsifiable and/or provide theoretical guarantees for sparsification algorithms would be significant.   [ ] Reviewers raised concerns about the significance of the paper’s results. In particular, they found it difficult to connect the paper’s analysis of landscapes of linear networks to the question of when and why practically occurring networks can be pruned. They also had difficulty isolating new ideas in the mathematical analysis beyond previous works on the landscape of linear networks. Finally, several reviewers expressed concerns about the extent to which the paper’s observations on linear networks generalize to nonlinear networks.  [ ] Reviewers raised concerns about the clarity of the paper. The mathematical exposition is unclear in places: conditions are not clearly stated, terminology is occasionally vague. Moreover the paper’s handling of optimality conditions for constrained optimization problems is unclear (e.g., the proof of Theorem 6 uses the unconstrained optimality condition $\ell^T X   0$ in the inductive step, even though the inductive hypothesis pertains to a constrained problem).   [ ] The technical results make assumptions that are occasionally quite strong. For example, Theorem 7 requires orthogonality of the data matrices, when restricted to indices where the weights are nonzero. As reviewers note, this assumption seems highly restrictive.    Overall, the paper addresses a topic that is important to the ICLR community: developing theoretical analyses of network sparsification. However, the significance of its results is not clear, and the technical exposition would need significant improvement to meet the bar for publication. 
This paper presents an intriguing empirical phenomenon in deep learning. They train a variety of architectures for different tasks using different datasets and study the relationship between the learned representations. In particular they collect the representations into a large matrix and take the top left singular vector and measure the cosine of the angle. They show that it is much smaller than one might expect, about 10 degrees or so, and has an approximate monotonicity property as the network is being trained although it does not seem to converge to zero. Moreover this measure also correlates with performance.   The reviewers had divided opinions on this paper. On the one hand, the range of experiments is impressive and truly demonstrates that this is a pervasive phenomenon. On the other hand, it is not so clear what it means. In particular, suppose we have a collection of graphs which have close to the same degree distributions. If we take the top left singular vectors of all the adjacency matrices, they would also have low angles between them. While this is a very different setting and there is no analogy between the experiments in this paper and this toy model, it does raise philosophical questions about whether the phenomenon is meaningful or is a byproduct of something else about the data. This may be a challenging question to answer, but one reviewer brought up a natural next step: One could measure the principal angle between the subspace of the top k left singular vectors across experiments for larger values of k. The authors do bring up the point that the spectrum decays very quickly, so it could be that beyond a certain point the singular vectors behave somewhat randomly. 
The authors propose a methodoloy for dynamic feature selection. They use differentiable gates with  an RNN architecture to select different subsets of features at each time point thus resulting in dynamic selection.  The reviewers agree that the idea is interesting and the method could be useful and I share their opinion.  The majority vote is towards rejection. The overarching mwssage of the reviews is that the manuscript raises confusion in a number of points. I see this work as one with good potential for impact but its current presentation is confusing. The vivid discussion that it raised is also an indication of it. The authors have done a good job replying to the concerns and the questions raised. However, the reviewers were still unsatisfied with the authors response to their concerns.  I recommend rejection at this time, while encouraging the authors to take seriously the reviewers  requests for a clearer presentation of their approach s contribution in order to strengthen their paper for future submission.  
The reviewers still have several concerns about the paper after the author feedback stage: the novelty of the paper is not sufficient; the experimental results are not very encouraging. We encourage the authors fixing these issues in the next revision.
This paper studies a problem setup of parameter efficient transfer learning for large scale deep models. The approach consists of learning a diff vector with a sparsity constraint and then pruning the vector using magnitude pruning. A group penalty is also introduced to enhance structured sparsity. The main motivation is that for each new task, we only need to add a few parameters based on a pre trained model without fine tuning it.  The proposed approach possesses technical soundness and shows empirical efficacy for the studied problem setup. During the rebuttal and discussion phases, two of the reviewers raised two major concerns based on which they strongly disagreed with acceptance:   The problem setup is not elaborated sufficiently and falls short of plausibility. An approach targeting at efficiency should either improve inference speed or reduce storage cost. Unfortunately, neither advantage has been well approached.   The technical novelty is somewhat incremental, given the rich previous work on residual adapter, network re parameterization, and network compression (pruning, sparsity etc.).  AC read the paper and agreed that, while the paper has some merit such as a better model for the particular problem setup, the reviewers  concerns are reasonable and need to be addressed in a more convincing way. For example, try to study a practical application in which the proposed approach is essential and useful for efficiency enhancement.
 This paper presents "Automunge" a python library for pre processing tabular data.  The authors develop a useful library that can be used by practicioners for data engineering in NNs applications.  The reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the  performance of the models that is applied on. A common concern was the lack of performance plots compared to other alternatives.  In the response the authors have done a rather thorough job of addressing the reviewers comments and adding material in the supplementary. However, given the current presentation, the manuscript needs a considerable amount of  rewriting to incorporate the suggested changes into the main paper. As it is, I don t think ICLR is the right venue for the manuscript.  It might reach its audience better in venues like SysMl or PyCon also suggested by a reviewer.  
The addresses open set recognition, namely, detecting anomalous samples that belong to classes not observed during training.  It has been shown that existing methods fail on open world images. The current paper shows empirically that performance can be greatly improved if based on low dimensional features.   Reviewers had grave concerns about the novelty of the approach and the logic behind the workflow. They found merit in the paper but chose to retain their scores after reviewing the rebuttal. As a future recommendation, it would be useful to provide more evidence about what component of the method or workflow are novel and what makes them work well. 
This paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models. The authors introduce a method they call Fair Robust Learning using Lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes.    In discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication. The main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general. Please see reviews and public discussion for further details.
The reviewers and AC liked the basic idea of how this paper improves on ALISTA, and the initial scores were high. Because the contributions rely quite a lot on empirical demonstrations, the reviewers asked for more experiments, changes to experiments, and timing results. The revision and rebuttal addressed most of these requests.  The multipath channel estimation problem was interesting though outside the scope of the AC and reviewer s expertise, so it is hard to evaluate how helpful the method is in that particular setting.
This paper studies the reasons for failure of trained neural network models on out of distribution tasks. While the reviewers liked the theoretical aspects of the paper, one important concern is about the applicability of these insights to real datasets. The authors added an appendix to the paper showing results on a real dataset that mitigates this concern to an extent. Further, there are interesting insights in the paper to merit acceptance.
This paper presents a secure aggregation method to ensure byzantine robustness. The reviewers thought that the idea was interesting, but had the following concerns. * Relaxing the assumptions used in the theoretical analysis as much as possible * Run more extensive experiments I encourage the authors to their feedback into account when preparing the revised draft.   
This paper presents a new method for training GAN by adding a precondition Layer. All reviewers are positive about the empirical results. However, some concerns were raised about the justification: (1) Only linear networks are considered, which is a bit impractical; (2) Existing work has shown the importance of controlling the whole spectrum instead of the condition number. There should be some connection missing between the proposed result and existing results; (3) The computational cost is a bit high. The paper would be much stronger if these concerns could be addressed.
This work proposes a novel metric for measuring calibration error in classification models.  Pros: * Novel calibration metric addressing limitations of previously used metrics such as ECE  Cons: * Limited experimental validation on CIFAR 10/CIFAR 100 only * Unclear impact beyond proposing a new calibration metric * Unclear value of using the proposed UCE metric for regularization and OOD detection  All reviewers appreciate the aim of the work to produce a calibration metric that addresses shortcomings of commonly used existing metrics such as expected calibration error (ECE), which is known to be sensitive to discretization choices.  However, all reviewers remain in doubt whether the proposed metric (uncertainty calibration error, UCE) is truly a better metric of calibration than previous proposals.  This doubt comes from two sources: 1. limited experiments that do not convincingly show the usefulness of UCE; and 2. interpretability of UCE not being as intuitive to the reviewers.  The experiments also use UCE as regularizer but the benefit of doing so over simple entropy regularization is not clear.  Overall the work is well motivated and written and the proposed UCE measure is interesting.  However, the reviewers remain unconvinced of the claimed benefits and the potential impact for measuring or improving calibration.
There is growing evidence that optimized deep networks (typically dense in the sense of nonzero parameters) often contain sparse sub networks that can be trained from scratch to achieve similar performance as the full network. Such “skeletonization” is of obvious importance, given the rate at which deep networks in practice are increasing in size. However, many approaches to find such optimal sub networks train the full model (and hence implicitly, the intermediate sub networks as well), which is not a scalable path.   Some recent works show that skeletonization at initialization may provide all the efficiency benefits of sparsity, while minimally impacting accuracy. This work first notes that accuracy in such approaches degrades significantly beyond a certain level of sparsity (around 95%). One of the ideas of this work is to resurrect parameters that were pruned away earlier in this work’s iterative skeletonization, via an approach called foresight connection sensitivity (FORCE) where the “trainability”  of the pruned network is also taken into consideration. An additional idea is “Iterative SNIP”, building on the SNIP approach of Lee et al. (2019). The empirical improvements, and the observations showing the limitations of SNIP and GRASP in the regime of high sparsity, are useful.   The evaluation and overall contribution were generally appreciated by the reviewers.  
This meta review is written after considering the reviews, the authors’ responses, the discussion, and the paper itself.  The paper proposes a system for learning disentangled object centric 3D based representations of scenes and shows that the proposed model works well on several tasks, including few shot classification and VQA.   The reviewers point out that the direction is important (R1, R3), the model is sensible (R2), and the reported results are good (R1, R4); on the downside, they note that the system is complicated (R1), the considered datasets are relatively simplistic (R1, R3, R4), some ablations are missing (R2, R3), and comparisons with baselines are not necessarily convincing (R2, R3). The authors did a good job of addressing the concerns in the rebuttal, by reporting additional ablation results, baselines, and experiments on the realistic Replica dataset.  All in all, I recommend acceptance. The direction of the work is important and complex, the experimental evaluation presented in the paper is extensive, and the results are good relative to relevant baselines. On the downside, the proposed system and the paper are somewhat complicated and overwhelming, which may limit the benefit for the readers. I hope the authors will take this into account in the future. 
The paper considers a new linear algebraic problem motivated by applications such as metagenomics which requires the algorithm to partition the coordinates of a long noisy vector according to a few known subspaces. A number of theoretical questions were asked (e.g., identifiability;  efficient algorithms and their error bounds; etc).    The reviewers generally liked the paper for what it does. Specific suggestions were raised by the reviewers, including how the paper went into length about the motivating applications but did not end up evaluating the proposed algorithms on any motivating applications; and that the main theoretical results were not technically challenging / nor surprising (although the authors provided a fair justification in their rebuttal).  The AC finds the paper an outlier in terms of the topics among papers typically received by ICLR, but liked the paper precisely because it is different.  The authors are encouraged to discuss the connections of the specific problem to the context of representation learning and machine learning in general.  Overall, I believe the paper is a solid borderline accept.    
This paper introduces a framework for automatic differentiation with weighted finite state transducers (WFSTs), which would allow user specified graphs in structured output prediction tasks and easy plug and play of graphs through the composition operation (demonstrated with variants of CTC). The authors demonstrated their framework on the OCR and ASR domains, which are important application scenarios. All reviewers agree the work is useful and can potentially be significant. However, the reviewers think the paper needs more discussions of similar/parallel work and the key differences from them, and clear description of the novelty in terms of either machine learning insights or algorithmic implementations.  We understand that this may be an implementation heavy work, but the level of details provided in the current version does not convince the reviewers that the proposed approach is already efficient and can scale up. This could be shown by fair comparison with existing approaches (e.g., hard coded error back propagation implementation with a fixed graph) in runtime and accuracy. 
Like the reviewers, I find this paper extremely borderline. On the one hand, it is clearly written, about a topic I find fascinating, and generally well motivated if not shockingly novel (i.e. removing some of the simplifying assumptions from Zhong et al. 2020, e.g. requiring grounding to be learned, use of real language rather than synthetically generated). On the other hand, I agree with the leitmotiv present amongst the reviews that the problem at the centre of the experimental setting is very, very simple (3 objects, 3 descriptions). I am mindful of the fact that access to computational resources is unevenly distributed, and am not expecting a paper like this to immediately scale their experiments to highly complex settings with photorealism, etc, but I can t help but feel that a more challenging task, with a deeper analysis of the problems presented by both grounding and the use of non synthetic language, would both have been highly desirable to make this paper uncontroversially worth accepting.  As a result, the decision is to not accept the paper in its present form. Work on this topic should definitely be presented at ICLR, but it s a shame this paper did not make a stronger case for itself.
The authors propose an MPC based approach for learning to control systems with continuous state and actions   the dynamics, control policy and a Lyapunov function are parameterized as neural networks and the authors claim to derive stability certificates based on the Lyapunov function.  The reviewers raised several serious technical issues with the paper as well as the lack of clarity in the presentation of the main technique in the initial version of the paper. While the clarity concerns were partially addressed during the rebuttal, the technical concerns (in particular those raised by reviewer 1) remain unaddressed   the stability certificate derived is questionable due to the fact that sampling based approaches to certifying that a function is a valid Lyapunov function are insufficient to derive any stability guarantee. Further, the experimental results are only demonstrated on relatively simple dynamical systems. Hence I recommend rejection.   However, all reviewers agree that the ideas presented in the paper are potentially interesting   I would suggest that the authors consider revising the paper to address the feedback on technical issues and submit to a future venue.  
This paper presents two self supervised learning objectives that can be used as intermediate pre training tasks to refine the T5 sequence to sequence model between pre training and task fine tuning. It shows that, at small to moderate model sizes, adding this step significantly improves performance on commonsense oriented target tasks.  Pros:   This appears to be a fairly straightforward improvement in self supervised learning in NLP, with fairly extensive experiments.  Cons:    This model isn t trained at the same extremely large scales (10B+ words) as state of the art models, and it performs significantly below the state of the art. It s not clear that the released model represents a useful model for any application as is, and while it s likely, it s not proven that the ideas in this paper would still be useful at larger scales.   Given that, it seems like the most likely audience for this work is other developers of pretrained models in NLP, which makes the fit to a general ML conference less clear.   The framing around  concepts  and, more importantly, the model name  concept aware LM , gives the unwarranted impression that the new model handles  concepts  in a way that T5 doesn t. It is not reasonable to use the word  concept  to refer to specific parts of speech in your title (even if you later explain that), and whether your model handles concepts in a categorically different way from T5 would take a substantial analysis to show, which doesn t seem to be present. I don t think this paper is up to ICLR s standards with the current name, and urge the authors to change it. 
The paper considers adaptive stochastic optimization methods and shows that they can be re interpreted as first order trust region methods with an ellipsoidal trust region, they consider a related second order method, and they show convergence properties and empirical results.  The results are of interest, but the significance of some of the results is not clear.  Part of this has to do with substance, and part of this has to do with presentation that can be improved.  Empirical results are weak, including appropriate baselines and details of the empirical results.
This paper proposes a model for learning using ensemble clustering. The reviewers found the general idea promising. However, while promising, all reviewers noted that in its curent form the paper is not fit for publication. The reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work. Because of all these reasons, this paper does not meet the bar of acceptance. I recommend the authors take into account the feedback provided in the reviews and discussion and resubmit to another venue.
The authors appreciated this submission because (a) the aspect of explainability is novel, (b) its strong performance, (c) the clarity of the paper. I urge the authors to double check all of the reviewer comments to make sure they are all addressed in the updated version. I vote to accept.
Three of the four reviewers recommend rejection; one additional reviewer considers the paper to be marginally above threshold for acceptance but is very uncertain and this is taken into account.  The AC is in consensus with the first three reviewers that this paper is not ready yet for publication.    There is concern from the reviewers that ICLR is not the right venue for this submission.  The author response in "Submission Update" does not clarify this concern.  Training a neural network to solve the problem does not automatically mean that ICLR or other ML conferences are necessarily the right venue.  Regardless, due to the many other raised concerns e.g. limited experimental results and comparisons as well as clarity,  the AC recommends rejection for this paper and resubmission at a more appropriate venue.  
The consensus recommendation is that the paper is not ready for publication at this time.
The paper studies the problem of satisfying group based fairness constraints in the situation where some demographics are not available in the training dataset. The paper proposes to disentangle the predictions from the demographic groups using adversarial distribution matching on a "perfect batch" generated by a clustered context set.  Pros:   The problem of satisfying statistical notions of fairness under "invisible demographics" is a new and well motivated problem.   Creative use of recent works such as DeepSets and GANs applied to the fairness problem.   Cons:   Makes a strong assumption that the clustering of the context set will result in a partitioning that has information about the demographics. This requires at the very least a well behaved embedding of the data w.r.t. the demographic groups, and a well tuned clustering algorithm (where optimal tuning is difficult in practice on unsupervised problems)   but at any rate, as presented, the requirements for a "perfect batch" is neither clear nor formalized.   Lack of theoretical guarantees.   Various concerns in the experimental results (i.e. proposed method does not clearly outperform other baselines, high variance in experimental results, and other clarifications).  Overall, the reviewers agreed the studied problem is new, interesting and relevant to algorithmic fairness; however, there were numerous concerns (see above) which were key reasons for rejection. 
This paper is rejected.  All of the reviewers found the empirical results strong. However, R3 and R4 pointed out concerns with the positioning of the work relative to prior work and that their approach is conceptually similar to previous work. The authors have tried to address these concerns in their rebuttal. Both reviewers appreciate the changes, but still have remaining concerns that I agree with. Based on these concerns, it is unclear if the strong empirical results are mostly due to using the NVAE architecture, rather than a methodological improvement over previous methods. The authors should work on positioning their paper in the context of prior work and the comparisons requested by R3 and R4 for a resubmission. 
The authors propose adversarial training using contrastive divergence based on ideas from Hybrid Monte Carlo methods. On the positive side the shown experimental results are promising both in terms of robustness and efficiency. On the negative side the paper seems to be written in a hurry. At several places terms are not defined, not explained or important details (e.g. parameters of the attack algorithms) are missing.   Thus the paper is not ready for publication yet and below the bar for ICLR but I encourage the authors to submit a significantly revised version to another conference.  Details:   in (7) N(x) is nowhere explained or defined, here also several threat models are introduced but later on only l_infty seems to be used e.g in Algorithm 1 (should be clarified)   as noted in the reviews the definition in (13) is based on quantities nowhere introduced   the potential U is only defined in the Algorithm (but then the arguments do not match with the RHS)   the kinetic energy in the algorithm is different from what has been used before   the parameters for the attacks used are not reported   why is AutoAttack not used for all datasets? They report 28% robust accuracy for the model trained by the Madry group  (https://github.com/MadryLab/robustness) whereas in the present paper it is 35%.   the scales of the plots should be chosen such that the curves can be distinguished
Since the authors have decided to withdraw this submission, it has been rejected from the conference.
 This paper proposes an efficient attention mechanism linear in time and space using random features. The approach has some similarities with the simultaneous ICLR 2021 submission "Rethinking Attention with Performers", with a key difference of a gating mechanism present in this work, motivated by recency bias. This paper is a valuable contributions to the efficient attention research topic. The reviewers appreciate the experiments and the in depth analysis. I recommend acceptance.  A noteworthy concern brought up in the discussion period has to do with whether the attention mechanism dominates the feed forward computations in the neural network, and how much this is architecture specific. The authors provide TPU timings, but I encourage the authors to add a discussion and timings of relative performance of feed forward vs. attention layers that covers GPU and CPU optimizers as well.
Most of the reviewers had concerns on the model being considered and there are additional concerns in the paper s discussion on the experimental results. 
Summary of discussion: Three reviewers rated the paper Good (7) while Reviewer2 disagreed. R2 s criticism was focussed on how this work is placed within existing/related literature, and no technical problem was identified. The authors have addressed some of R2 s comments/concerns, R2 has not participated in the discussion.  Novelty and contributions: Overall the reviews seem consistent with an incremental paper which is technically valid, improves the state of the art on a reasonably difficult task. However, it does not appear from the reviews that the paper substantially advances our understanding of machine learning more broadly beyond this specific application.  Experiments: There is some disagreement among reviewers on the adequacy of the experiments, with at least two reviewers calling for experiments involving  natural photos . I believe the author s responses adequately address these concerns: they pointed out that the key selling point of their paper is the ability to model structured noise which is less relevant in natural photos.  On the balance of things, I think this paper should be accepted, but I wouldn t argue if it did not make the cut due to its narrow scope. For this reason, I recommended poster presentation.
The paper presents a method for meta learning the loss function. The analysis mainly concerns the recently proposed TaylorGLO method on the (slightly less recent) Baikal loss. There was no consensus on this paper, but no reviewer was willing to fight for acceptance either. I found the paper not self contained, with important non standard elements undefined, starting with the Baikal loss, notations that are not defined in the main text, and a nomenclature that is also unusual with important terms such as "attractor" or "invariant" used in meanings that are non standard in optimization or machine learning.  Regarding content, most of the analyses refer to properties of the Baikal loss (not presented in the main text) that are deemed to be positive, without any theoretical support (Theorems 1 and 2). The inability to overfit is here posed as an obvious quality of a training loss. Then, a way to prevent the failure of the meta training algorithm is presented in Theorem 3. Finally, an experiment is provided, showing that the proposed meta training algorithm performs better than "vanilla" training with respect to adversarial attacks with FGSM. There is no comparison with other defense mechanisms and no analysis explains the results. Overall, although some interesting aspects may be developedin this paper, they are currently not well served by writing or the experimental evidences, so I recommend rejection. 
The paper seeks to point out the difficulty of size generalization in GNNs for node prediction and analyze why this happens. The analysis is anchored on the construction of so called d patterns. The main argument is presented in Corollary 3.4 which shows that discrepancy in d patters between small and large graphs introduces the possibility of finding a GNN that fits the d patterns well in a small graph while yielding poor answers on a large graph (assuming the task is solvable by a constant depth GNN). While good to show, the existence of bad parameters does not necessarily suggest that this setting is likely to arise after training. As pointed out by AnonReviewer1, size generalization is also not a new issue but has already been introduced / highlighted in previous publications (Barrett et al 2018, Joshi et al 2020, Dai et al 2017). The authors do provide an algorithm and empirical arguments to mitigate the effect of d pattern discrepancy. A revised version of the manuscript can hopefully make a stronger case for d patterns, constant depth computation, and the relevance of these for size generalization. 
The paper proposes a new pre trained language model for information extraction on documents. It consists of a new pre training strategy with area masking and a new graph based decoder to capture the relationships between text blocks. Experimental results show better performances of the proposed approach.  Pros • The paper is generally clearly written. • Experimental results show better performances on the benchmark datasets.  Cons • Novelty of the work might not be enough. For example, the graph based decoder is not new. The masking technique is also a natural extension of that in BERT. • Significance of the work might not be enough. For example, the improvement from the area masking is not so significant. • There are additional experiments which can be added, as pointed out by Reviewer 3. • Presentation can be further improved. Some of the issues indicated by the reviewers have been addressed in the rebuttal. We appreciate the authors’ efforts.  During the rebuttal, the authors have addressed the clarity issues pointed out by the reviewers. However, the main issues in novelty and significance still exist. The reviewers think that the quality of the work is still not high enough as an ICLR paper.  
The focus of the submission is blind source separation (BSS). The authors propose a log linear model based formulation to tackle the task and to relax assumption/restrictions (linear mixing, non convex objective, ...) present in previous techniques. They use the maximum likelihood approach [Eq. (3)] with natural gradient descent for optimization, and illustrate the efficiency of the approach in two toy examples (separation of mixed images and that of sin/sign/sawtooth signals).  BSS is an important task in machine learning with various applications. As assessed by the reviewers, however, the submission is in a quite preliminary stage: (i) Section 1 is rather long, still it lacks providing relevant context to the work. (ii) The introduction of the main ideas/motivation, the assumptions imposed, and the explanation of the notations are missing. (iii) The usefulness of the proposed approach is questionable; the demos focus on artificial toy examples. More work and significant revision are needed before publication.
This paper proposes a simple extension to BERT like pre training for source code models, which allows incorporation of data flow information. This is a new way of incorporating code structural information into models, and it appears practical and effective. Reviewers are all in favor of accepting the paper.
While the reviewers appreciated the aim of the work, they found the technical contribution to be too incremental to be of sufficient interest and the exploration of the problem and its significance to be incomplete in the paper s current state.
This paper presents an efficient secure aggregation algorithm in federated learning scenarios, which employs sparse random secure sharing clients. Four experienced reviewers left valuable comments on this paper, and three of them are unfortunately negative to this work (4, 4, 3) while one reviewer is slightly on the positive side.   The reviewers are generally positive about the main idea and the direction for this work, but they are not convinced of its mathematical soundness and practical benefits; the theoretical analysis and mathematical proof has been conducted only for simplified models while their practical advantage is not clear enough. Also, even the most positive reviewer (R3) is concerned about the novelty of the proposed approach. Although the concerns raised in the original reviews have been partially clarified during the discussion phase, there still remain several critical limitations, which makes this paper require (probably) multiple rounds of revision before publication and this AC has a reservation for accepting this paper.
The pursued here goal to explore what a broader and more nuanced notion of "imperceptible" perturbation is quite intriguing and could be a basis of really impactful investigations. However, as pointed out in the reviews and comments, the current treatment of this topic suffers from significant presentation and framing shortcomings.   In particular, this work would benefit from stating clearly what is the main topic of study (and what is not), as current framing tends to confuse the readers. It would be also useful to discuss (and acknowledge) that having the split into background and foreground being driven by a (most likely non robust) model makes the resulting notion of perturbation rather tricky to consistently analyze/certify. Also, the analysis of the properties of the model, although done fairly competently, is missing some important aspects.  Still, once these points are properly addressed, this would constitute a valuable contribution. 
This paper proposes a technique of communicating predicted local states between agents in multi agent reinforcement learning to deal with the delay in communication.  While the paper addresses an important practical problem, the reviewers have concerns about the insufficiency of novelty and experimental validation.
This application paper applies hyperbolic convolutions in VAE learning to perform unsupervised 3D segmentation.  Addition of these components enables performance improvements in the unsupervised segmentation task. Overall, the paper is borderline and the reviewers mention the limited novelty of the approach, which largely uses components that have been developed before. Even though the paper presents an application of these methods to a relevant and significantly more challenging task than prior work, I recommend rejection from ICLR due to concerns about novelty.
The paper considers the question of identifying whether a model is bad from an OOD perspective or certifying that it is good. The reviews agree that there are interesting ideas in the paper, however, lack of sufficient experiments and presentation issues were pointed out which make the paper not ready for acceptance at this stage.
The scores here are bimodal.  The low scoring reviewers have problems with the evaluation, and I agree it could be improved.  The high scoring reviewers seem to mostly agree with those complaints, but think that the paper is interesting enough  to be accepted anyway.  One of the low scoring reviewers has some complaints about novelty that I don t find super convincing. The other low scoring reviewer has suggested that they d be OK with a decision of Accept.   Part of me thinks that I should reject this paper with a message of "come back later with the experiments improved", and that that would be the best thing for the field, because the paper can already be publicized on arXiv anyway.  But the other part of me thinks: what if they do that and get unlucky with a bad batch of reviews the next time (the current reviewers were great and had a really thorough discussion)? With some amount of trepidation, I m recommending accept, but *please* reward my faith in you (the authors) and make an effort to fix the things reviewers complained about before the camera ready.
 A "quantum deformed" generalization of a probabilistic binary neural network is introduced, which can be either run on a quantum computer or simulated with a classical computer. Reviewers agreed that the paper is well written, introduces some new ideas merging quantum computing with a variational Bayesian framework, and the reported numbers on MNIST and Fashion MNIST outperform prior QNN approachers. However, reviewers questioned how useful the proposed ideas are, noting that: the reported gains could be attributed to increased parameterization (this was not carefully ablated with baseline approaches). Additionally, while the quantum supremacy experiments seem technically correct, there was no clear motivation for empirically demonstrating quantum supremacy when no theoretical guarantees are provided. Taken together, there was no clear path to practical improvements of real systems from the proposed ideas.
The paper proposes an auto encoder framework IMA, a scalable model that learns the importance of modalities along with robust multimodal representations through a novel cross covariance based loss function, in an unsupervised manner. They have compared their approach to SOTA methods via multiple experiments and shown how IMA gives better performance.  The authors have addressed some of the reviewers  feedback. However, as pointed out by the reviewers, the experimental section needs better analysis of results and comparison to other methods, and the modeling section needs to be better explained and motivated. The authors have made changes in their revision, however the ICLR review process does not allow for checking the camera ready. Since we cannot accept the paper in its current form (or with small variations) and there have been many competitive submissions, we would encourage the authors to make their revisions and resubmit to other venues.
The authors introduce an approach for designing pseudo labels in semi supervised segmentation. The approach combines the idea a refining pseudo labels with self attention grad CAM (SGC) and a calibrated prediction fusion, and consistency training by enforcing pseudo labels to be robust to strongly augmented data.   The reviewers overall like idea and point out the good level of performance obtained by the method in the challenging semi supervised context. However, they also point out the limited novelty of the approach, and the need for a better positioning with respect to related works. After rebuttal, reviewers were satisfied with authors  answers and paper modifications, and all recommend acceptance. \ The AC considers that the submission is a nice combination of existing techniques and likes the simplicity of the one stage approach, which reaches good performances. Therefore, the AC recommends acceptance.
Three of the reviewers are very positive about this work, and R3 is slightly concerned about the datasets, writing, and notations etc. The authors responded to these concerns in detail and have agreed to take care of these comments. Thus an accept is recommended based on the understanding that the authors will fulfil their commitments.
This paper proposes a hierarchical flow based generative model to learn disentangled features at different levels of abstractions.  The key technical contribution is a combination of renormalization group and flow based models. The reviewers do find the idea interesting. However, the merit of the work with respect to StyleGAN and StyleFlow has not been well established. AR3 made the following comment:  “Specially, compared with the style based generator[1,2], …, I don’t find superiorities of the proposed method.” The authors responded to the comment briefly (but not convincingly) in their rebuttal. There is no mention of it in the revised paper. A proper account of the issue would require major revision to the paper.
This paper tackles an interesting problem (that of federated continual deep learning) and proposes an effective approach for it with good results.  This is a good contribution. However, there are presentation issues in several aspects of the paper that require improvement before publication. The authors  claims of the novelty of the federated continual learning problem is overstated (there was an AAMAS 18 paper they cited which IS applicable to the federated setting, although their method is certainly a substantial improvement, more flexible, and supports deep models), and there are aspects of the analysis and experiments that could be improved (the authors are somewhat dismissive of one reviewer s concerns about the insensitivity to the regularization parameters. While I agree with the authors that this aspect of the review is focusing on this one detail in lieu of the bigger picture, the author s insensitivity study in Figure 6 and subsequent analysis are lacking and could use improvement).  The authors  revisions did help clarify/address a number of issues raised in the initial reviews,  but it was felt that more improvement was needed. In particular, there are still mistakes with characterizing this work with respect to existing work, especially in overstating the novelty of the federated continual learning problem.  I do believe this paper is the first to call it "federated continual learning" (a name I like), but the paper should be less dismissive of existing works on "multi agent lifelong learning" that could also apply to this setting, albeit with shallow models.  Consequently, while this reduces the novelty of the federated continual learning problem, the authors still have a substantive contribution   just one that needs improvement in presentation before publication.
This work was deemed interesting by the reviewers, but they highlighted the following weaknesses in this version of the paper:    Lack of comparison to other methods.    Lack of novelty compared to previous work.    Fundamental problem with training only on one dataset (MNIST), issue with possible overfitting.
Paper addresses the problem of sim2real (training with synthetic data and then applying the  learned model on real data) in the context of scene graph generation.  The paper was reviewed by four expert reviewers  who identified the following pros and cons of  the method.   > Pros:   Paper addresses relevant and important problem [R1, R2, R4]   Paper containing compelling results with respect to a number of baselines [R2, R4]  > Cons:   Lack of clear motivation, focus, and explanation of novelty [R4]   Missing details, which makes paper hard to follow [R3, R4]   Lack of explanations for baselines [R4]   Lack of focused analysis of specific contributions [R4]   Lack of discussion on the limitations of the approach [R1, R2]   Presented evidence is largely on toy data or very domain specific [R2]  A number of the shortcoming were addressed by the authors during the rebuttal through revisions.  However, opinion of reviewers on the paper remained split, with paper receiving  following scores:    5: Marginally below acceptance threshold   6: Marginally above acceptance threshold   7: Good paper, accept   5: Marginally below acceptance threshold  Overall, all reviewers and AC agree that the paper addresses an important and interesting problem. At the same time  AC agrees with R2 and others that point out that there are significant limitation in terms of applicability of the approach in more complex scenarios, where readily available simulator may not exist. On balance, and considering the large number of high quality submissions to ICLR this year, the paper was deemed marginally below the acceptance threshold.  
All reviews are somewhat below the acceptance threshold. The main concerns are in terms of lack of novelty, and that some of the paper s main claims are unsupported. Many of the criticisms are quite focused on specific details, but these seem significant enough to have been deal breakers for this submission.
This paper presents a very interesting investigation. While deep neural networks are typically best in non private settings, the authors show that linear models with handcrafted features (ScatterNets) perform better in certain settings of the privacy parameter. The reviewers all found this to be important and insightful, with a thorough investigation, and I tend to agree, recommending acceptance.
This paper addresses an interesting problem and all reviewers agree.  Most reviewers found the paper difficult to understand and it was hard to see the novel contributions.    The paper will need a significant revision before publication.
Description: The paper presents a patch based 3D representation of man made shapes that can be computed with deep learning and used directly in existing CAD applications. This representation is based off a deformable parametric template with Coons patches. Results in sketch based modelling tasks shows comparable results with STOA  Strengths:   The patch based representation provide several advantages: compact, sparse, interpretable, consistent and easily editable.   Can infer the right template, and thus does not require manually created templates  Weaknesses   Limited evaluation restrained to mostly sketch based modelling, and missing evaluation against a few STOA methods  The paper has introduced a very impactful new representation for 3D shapes and has strong technical novelty. I recommend, as reviewers have suggested, more in depth quantitative evaluation (against other work and ablation studies)
The reviewers found found the paper well motivated and well written, they found both the theoretical contributions limited in novelty and the experiments too rudimentary to be insightful.
This is a nice paper on generating adversarial programs. The approach is to carefully use program obfuscators. After discussion and improvements, reviewers were generally satisfied with the approach and evaluation. The problem domain was also found to be of interest.
This paper received 4 reviews with mixed initial ratings: 6,7,7,5. The main concerns of R1, who gave an unfavorable score, included lack of clarity (the manuscript is hard to follow) and limited empirical evaluation (the method is tested on a single synthetic dataset, CLEVRER). The latter point is echoed in other reviews as well. In response to that, the authors submitted a new revision and provided detailed responses to each of the reviews separately, which seemed to have addressed these concerns. R1 upgraded the rating and recommended acceptance. As a result, the final recommendation is to accept this submission for presentation at ICLR as a poster.
This work raised quite a few questions, and left the reviewers somewhat divided. The authors have done their best to answer these questions, conducting additional experiments where needed.  The close relation of this work to Mask Predict (Ghazvininejad et al. 2019) was noted by several reviewers. Although the current version of the manuscript addresses this, the introduction still frames Mask Predict as an iterative model, and does not explicitly make the connection between GLAT and single iteration Mask Predict. My impression is that this understates the relationship between these models somewhat.  Taking single iteration Mask Predict as a baseline, the proposed extension is fairly simple, and seemingly effective, which is a potentially impactful combination. However, the manuscript is still held back by presentation issues (including but not limited to spelling and choice of words), and I concur with Reviewer 2 that the connection with curriculum learning should be elucidated not just in words, but with supporting experimental analysis.  Regarding training cost, given that training for GLAT seems to be more costly for the same number of training iterations, a comparison where the total compute budget is held constant could be interesting   though I appreciate that this is not a key point of the paper, as the authors point out (whereas inference cost is).  I believe the changes made by the authors in response to the reviewers  comments are substantial enough that they merit a further review cycle, and may still fall short of the reviewers  expectations in some aspects. Therefore, I will not recommend acceptance, though I want to add that this was a tough call to make. I would also like to encourage the authors to resubmit their updated manuscript.
The paper considers using local spectral graph clustering methods such at the PPR Nibble method for graph neural networks.  These local spectral methods are widely used in social networks, and understanding neural networks from them is interesting.  In many ways, the results are interesting and novel, and they deserve to be more widely known, but there are several directions to make the work more useful to the community.   These are outlined in the reviewer comments, which the authors answered partially but not completely satisfactorily.  Much of this has to do with explaining how/where these (these very fundamental and ubiquitous) methods are useful in a particular application (GNNs here, and node embeddings below).  An example of a paper that successfully did this is "LASAGNE: Locality And Structure Aware Graph Node Embedding, E. Faerman, et al.  Proc. 2018 Conference on Web Intelligence."  (That is mentioned not since it is directly relevant to this paper, but since it provides an example of how to present the use of a method such as PPR Nibble for the community.
This paper proposes a learning based approach for solving combinatorial optimization problems such as routine using continuous optimizers. The key idea is to learn a continuous latent space via conditional VAE to represent solutions and perform search in this latent space for new problems at the test time. The approach is novel and experiments showed good results including ablation analysis.   Reviewer comments are adequately addressed during the response phase and I find the changes satisfactory. Overall, this is a good paper and I recommend accepting it.  One last comment: It would be a great addition if the paper could add discussion about the applicability of this approach to arbitrary combinatorial optimization problems and what design choices are critical to come up with an effective instantiation.
This paper applies an existing tool (copulas) to MARL to represent dependencies between variables.  The reviewers appreciate the use of the copulas for this problem. The experimental section shows promising results on several problems. I appreciate that the authors have answered and addressed many points of concerns of the reviewers. The paper is well written  The reviewers seem to see this paper as a first step only, showing promising results but of moderate significance in itself. In particular, reviewer 3 would like to see more justifications for the use of the copulas, and a more experimental settings would make a stronger paper.  
The paper studies Distantly Supervised Relation Extraction(DSRE) in distributed settings. Though DSRE has been studied in Centralized setting it has not been studied in distributed platform. This  paper leverages the federated learning setup for this problem and proposes to use Lazy MIL for this purpose.  The paper  identifies the main challenge as label noise but does not attempt to characterise the severity of the problem vis a vis the centralised setup.  Though intuitive but a formal approach would have helped in understanding the importance of the derived results better.  
The paper tackles the Q value overestimation problem by proposing a regularization technique to maximize diversity in representation space, preventing ensemble "collapse", in order to improve the efficacy of techniques such as Maxmin and Ensemble Q learning. Reviewers praised the originality of the method and the interesting connections drawn to economic theory, and seemed to agree that the method is somewhat effective. On the other hand, R3 pointed out that hyperparameters are tuned per domain, the number of domains considered is small (echoed by R4), and criticized the paper for failing to truly validate its central hypothesis experimentally (echoed by R1).  R4 raised the issue of unfair comparisons in between ensemble and non ensemble methods, while R1 raised a multitude of criticisms ranging from ahistorical attributions to confusing figures, which I will not exhaustively repeat here. Based on the reviews, and the fact that the majority of reviewers  concerns remain entirely unaddressed (the authors only responded to R2), this manuscript is not a candidate for acceptance at this time. 
This paper received mixed reviews.  One reviewer is positive, while the remaining three reviewers are either negative or feel that the paper is below the threshold for acceptance.  The ideas presented in the paper are interesting and novel   this was acknowledged by three of the reviewers, even those who did not recommend acceptance.  The AC also recognizes the technical novelty presented.  However, as all the reviewers pointed out to varying degrees, the experimentation is problematic and the AC in agreement with this.  In particular, the heavy focus on improvement on top of SLIC makes it the applicability of the proposed approach highly limited and also not so convincing.  Recommendation for the paper is to reject and resubmit with improved experimentation.
All Reviewers and myself agree that the paper presents several major issues that require important rethinking of the research done, as well as a full rewriting of the manuscript. Hence, my recommendation is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Code is available and clarifies parts of the approach.   Use of publicly available data sets.   Samples are provided.   Interesting problem.  Cons:   There are several problems with language and writing. Sometimes there is also incorrect terminology.   There are several problems with the description of the approach, which makes it opaque and hard to understand.   Proposed model not addressing basic limitations in the existing literature.   Insufficient evaluation.   No clear indication of successful results.   Potential lack of broad impact/interest to the ICLR community.   Unclear contribution.   Unconvincing samples.
The paper analyzes the gradient flow dynamics of deep equilibrium models with linear activations and establishes linear convergence for quadratic loss and logistic loss; several exciting results and connections, solid contribution, accept! 
This paper focuses on adversarial robustness with unlabeled data. The philosophy behind sounds quite interesting to me, namely, utilizing unlabeled data to enforce labeling consistency while reducing adversarial transferability among the networks via diversity  regularizers. This philosophy leads to a novel algorithm design I have never seen, i.e., ARMOURED, an adversarially robust training method based on semi supervised learning.  The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all  comments in the final version. 
In this paper, the authors proposed a large margin based domain adaptation method for cross domain sentiment analysis.  The idea of developing a large margin based method for domain adaptation is not new. Though the proposed method contains some new ideas,  the difference between the proposed method and the existing large margin based methods needs to be discussed and studied empirically.  In addition, the experimental results are not convincing: some related baselines are missing and experiments need to be conducted on more datasets.   Though the authors did provide long responses to each reviewer, after a lot of discussions, the reviewers still find that their concerns are not well addressed.   Therefore, this paper is not ready to be published in ICLR based on its current shape. 
This paper proposes a novel problem of polymer retrosynthesis, and a method to solve it. The authors formally define the polymer retrosynthesis optimization problem as a constrained problem to identify the monomers and the unit polymer, with the recursive and stability constraints. Further, since the main challenge with polymer retrosynthesis is the extremely scarce training data, the authors propose a domain adaptation technique that can utilize a single step retrosynthesis model trained on a large amount of data. The authors also use Retro* [Chen et al. 20] for synthesizability check of the monomers. The proposed method, PolyRetro, is validated against few naive baselines for top k recovery performance, and is shown to outperform them.   All reviewers found the problem of polymer retrosynthesis tackled to be important as well as novel, and the paper to be very well written. However, all reviewers had a common concern on the limited technical novelty and meaningless baselines that makes it difficult to evaluate the significance of the results. Some of the reviewers were also concerned with the insignificant performance gain with the proposed domain adaptation technique (PolyRetro vs. PolyRetro USPTO in Figure 4), and its limited applicability to a condensation polymerization. The authors provided new results with more baselines, which fine tune the single step retrosynthesis model (MLP, seq to seq) trained on USPTO.  The below is the summary of pros and cons:  **Pros**   The tackled polymer retrosynthesis problem is novel and practically important.    The proposed problem formulation and constraints are interesting and make sense.   The paper is well written and easy to follow even for non domain experts.  **Cons**   The proposed solution with recursive and stability constraints is rather straightforward, as well as the use of Retro* for screening out the monomers.    The domain adaptation technique, which is advertised as an important contribution to combat extreme data scarcity, is both straightforward and yields small performance gain.   The baselines in the original version of the paper are simply meaningless strawmans, and the new baseline (seq2seq retro) in Section D of the Appendix seems quite strong, making it difficult to validate the effectiveness of the proposed method.  The paper received split reviews, with three leaning toward acceptance and one leaning toward rejection. After the interactive discussion period with the authors, the reviewers had an in depth discussion, where all reviewers agreed that the technical novelty or contribution to the general machine learning field, or general applicability to polymer synthesis is limited. The reviewers did not reach a consensus, which makes the paper a borderline case, and after the discussion with the program chairs, we decided to reject the paper due to the unresolved concerns.     I believe that the proposed problem specific solution is adequate, although it has little technical novelties, since this is an application paper. However what is more problematic is the inconclusive experimental validation results due to lack of meaningful baselines. I suggest the authors to compare against seq2seq retro + Retro* in order to properly validate the effectiveness of the proposed method. Also, results in Figure 7, or the polymerization examples in Section A of the appendix should be incorporated into the main paper. I also suggest that the authors drop domain adaptation from the title since it constitutes a small part of the method and thus is misleading. 
This paper introduces a method for approximating real time recurrent learning (RTRL) in a more computationally efficient manner. Using a sparse approximation of the Jacobian, the authors show how they can reduce the computational costs of RTRL applied to sparse recurrent networks in a manner that introduces some bias, but which manages to preserve good performance on a variety of tasks.  The reviewers all agreed that the paper was interesting, and all four reviewers provided very thorough reviews with constructive criticisms. The authors made a very strong effort to attend to all of the reviewers  comments, and as a result, some scores were adjusted upward. By the end, all reviewers had provided scores above the acceptance threshold.  In the AC s opinion, this paper is of real interest to the community and may help to develop new approaches to training RNNs at large scale. As such, the AC believes that it should be accepted and considered for a spotlight.
Summary: This paper proposes an interesting idea where additional auxiliary tasks allow an agent to more quickly learn in a sparse reward task.  Comments:  * The authors may want to look at "Parallel Multi Environment Shaping Algorithm for Complex Multi step Task" https://www.sciencedirect.com/science/article/abs/pii/S092523122030655X as it has a somewhat related idea and is also in RTS games. * It wasn t clear to me how the auxiliary tasks were generated/selected, or how the algo would work if a poor auxiliary task was used. * I wasn t sure why SAC X wasn t empirically compared to in a domain where it and this method could both apply.  Discussion: The reviewers agreed that this paper could be significantly improved in multiple dimensions.  Recommendation: I recommend we reject this paper. However, I encourage the authors to work to improve it as I d really like to understand where and why this method is successful   I would eventually like to incorporate it into my own work.
The paper analyzes the behavior of SGD using diffusion theory. They focus on the problem of escaping from a minimum (Kramers escape problem) and derive the escape time of continuous time SGD and Langevin dynamics. The analysis is done under various assumptions which although might not always hold in practice do not seem completely unreasonable and have been used in prior work. Overall, this is a valuable contribution which is connected to some active research questions regarding the flatness of minima found by SGD (with potential connections to generalization). I would advise the authors to improve the quality of the writing and address other problems raised by the reviewers. I think this would help the paper maximize its impact.
The main merit of the paper is to try to address some important issues about GNN, e.g. expressivity power and data augmentation, from a novel perspective and using well grounded mathematical tools. Unfortunately, however, this novel perspective is also introducing some confusion about its meaning in the context of graphs. In fact, it is not clear how, in the general case, direction as introduced in the paper makes sense, especially when considering the data augmentation approach. Moreover, although well grounded mathematical tools are used, proofs of theorems, as well as justification of related corollaries, are not sufficiently clear to guarantee their correctness.  In summary, a potentially interesting contribution that needs more work to better clarify motivations, grounding to common graph concepts, better presentation of the theoretical results.
This paper proposes a new framework for improving supervised learning via invariant mechanisms. The reviewers agree that overall, this paper is well written and contributes to a growing body of work on invariant prediction and causality in supervised learning. At the same time, there are some concerns regarding novelty and significance in light of previous work, as well as the overall organization of the paper, which could be improved to highlight the main contributions more clearly. Ultimately, this was a borderline decision, but it is clear that the paper needs a major revision before acceptance. Although the authors have already incorporated some of the minor comments which is appreciated, the authors are urged to consider the major comments (e.g. see R2 s comments regarding presentation) when revising the paper.
This paper proposes a method for out of distribution modeling and evaluation in  the human motion prediction task. Paper was reviewed by four expert reviewers who identified the following pros and cons.  > Pros:   New benchmark for testing out of distribution performance [R1]   Compelling performance with respect to the baselines [R1,R4]   Paper is well written and easy to  follow  [R2]   Generative model in the context  of  out of distribution modeling of human motion is novel [R1,R2,R4]  > Cons:   Lack of support for interpretability claim  [R1]   Validity and usefulness of the metric [R1]   Lack of "effectiveness" of the proposed approach [R2,R4]   Technical contributions are not significant [R3,R4]   Experimental validation lacks comparisons to other state of the art in motion prediction  methods [R3]    Lack of evaluation on additional datasets and for the main task [R4]  Authors tried to address the comments in the rebuttal, but  largely unconvincingly to the  reviewers.  On balance, reviewers felt that negatives outweighed the positives and unanimously suggest rejection. AC concurs and sees no reason to overturn this consensus.  
The authors introduce new evaluation criteria and methods for identifying salient features: rather than earlier approaches which attempt to  remove  or marginalize out features in various ways, here they consider robustness analysis with small adversarial perturbations in an Lp ball.  For text classification, a user study is included which is appreciated.  In discussion, the authors addressed many points and all reviewers converged to recommend acceptance.  A couple of points could be discussed further if space permits: the impact of type of perturbation employed; and the connection between optimizing for adversarial robustness and optimizing for insertion/deletion criteria.
This paper uses an autoencoder with neural style transfer to generate images from previously seen classes to avoid catastrophic forgetting in continual learning.  While reviewers had some concerns about the paper (experiments on high resolution images, comparison with FearNet), authors have addressed all the concerns. R1 s concern about the motivation for generation instead of replaying actual images is not necessary since this is not the first work to use generative replay.
The paper introduces a framework for learning dynamical system models from observations consisting of discrete spatio temporal series. It is composed of two components trained sequentially.  A first one learns embedding from observations using a seq2seq approach, where the embeddings are constrained to follow linear dynamics. This is inspired by approximation schemes for Koopman operators. These embeddings are then used as the spatio temporal series representions and are fed to a transformer trained as an autoregressive predictor. Experiments are performed on different problems using data generated from PDEs through numerical schemes. Comparisons are performed with different ML baselines.  The paper is well written with experiments on problems with different complexities. The original contributions of the paper are 1) the combination of pretrained embeddings with a transformer auto regressor, 2) a seq2seq architecture for learning time series representations constrained by linear dynamics.  On the cons side, the paper original contribution and significance are over claimed. Closely related ideas for learning approximate Koopman operators and observables have already been developed and used in similar contexts. Besides there is no discussion here on the properties or physical interpretability (which is often an argument for Koopman) of the learned representations. Then the baselines are mainly composed of simple regressors (LSTM, conv LSTM, etc.) and this is not a surprise that they cannot learn dynamics over long term horizons. There is no comparison with dynamical models incorporating numerical integration schemes that could model the temporal dynamics of the system. There is now a large literature on this topic exploiting discrete (ResNet like) or continuous formulations (as started with Neural ODE).
The reviewers are unanimous that the submission does not clear the bar for ICLR.
This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets   something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments,  comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re submission.
There is a general consensus on the fact that the paper is not yet ready for publication. I encourage the authors to carefully address the detailed concerns raised by the reviewers, which include  among other: i) the incompleteness of the literature overview, which should include the references provided by the reviewers, ii) poor (or bias towards the proposed approach) experimental evaluation, and iii)  a vague treatment of key terms in the interpretability literature  like feasibility (e.g., to make sure that the counterfactual lie in high data density regions). 
The premise of the work is simple enough: investigate if networks that are trained with an adversarial objective end up being more suitable for transfer learning tasks, especially in the context of limited labeled data for the new domain. The work uncovers the fact that shape biased representations are learned this way and this helps for the tasks they considered.  There was rather robust back and forth between the authors and the reviewers. The consensus is that this work has merit, has good quality experiments and investigates something with high potential impact (given the importance of transfer learning in general). I hope that most of the back and forth findings are incorporated in the final version of this work (especially the discussion and comparison with Shafahi et al., as well as all the nuances of the shape bias).
This submission considers the problem of learning disentangled representations from data in which there are correlations between underlying factors of variation (FoVs). Much of the work on learning disentangled representations has considered simulated datasets in which the FoVs are conditionally independent. The authors perform an extensive experimental evaluation (4000+ trained models). The main findings of this evaluation are that:    Existing methods fail to disentangle when ground truth FoVs are correlated, in the sense that learned factors will reflect the correlations in the training data   Metrics for disentanglement do not necessarily reveal correlations in underlying factors   Semi supervision and weak supervision can be used to induce learned factors that align with true FoVs  Reviewers expressed diverging opinions on this paper:    R2 is a favor of acceptance, but does note that the paper is somewhat difficult to follow, owing to the fact that it presents a large number of results and does not quite arrive at a streamlined narrative.    R4 was initially critical and posted detailed comments relating to framing, interpretation of existing work, and the conclusions that can be drawn from the presented experiments. The authors were able to address a number of these concerns in a detailed discussion with the reviewer.     R3 is critical of the experimental setup, which considers linear correlations between two underlying factors, and feels the semi supervised and weakly supervised experiments have limited novelty. This reviewer did not respond to author feedback.   The metareviewer has carefully read the reviews, author feedback, and subsequent discussion. Owing to the fact that reviews are diverging the meteareviewer also read the paper.   As R4 notes, the results in this paper are in some sense unsurprising – we would expect correlations between underlying factors to lead to correlated learned factors. In fact one could even argue that dimensions in the latent space should reflect correlations in the training data. That said, the metareviewer feels that a paper need not present results that are surprising, as long as the experimental evaluation is rigorous and there are no major problems with framing and exposition.   In this context, the metareviewer would like to express their appreciation to R4 for taking the time to follow up in detail with the authors, and for checking their revisions. The metareviewer feels that the fact that these revisions have a fairly large edit distance should not in itself not impede acceptance, as long as reviewers are agree that the edits improve the paper.   The metareviewer is not entirely convinced by the criticisms presented by R3. The reviewer is of course correct that real world datasets will not just have linear correlations between two factors. That said, an experimental evaluation of how correlations affect the degree of disentanglement has to start somewhere, and even these comparatively simple experiments represent a substantial effort on the part of the authors.   Having read the submission, the metareviewer agrees with R2s assessment that the exposition is difficult to follow, even for readers who know the relevant literature. As the reviewer notes, the overall narrative could be clearer. Extracting a clear narrative is challenging when there are many experiments to report, but it is nonetheless something that the authors should spend additional time and thought on. Another factor that hurts clarity is that experiments are described in long paragraphs that often would benefit from an equation or two, for example to describe the substitution function in Section 4.1, or to more precisely describe the form of weak supervision that the authors employ in Section 4.2.   As a final note, the metareviewer would suggest more explicit discussion on what authors think a VAE *should* do when factors are correlated. Arguably learning factors that reflect correlations is the "correct" when the training data exhibits such correlations. Currently the authors do not provide much of an arguments for *why* they think a VAE should learn a representation that ignores these correlations. A possible argument is that train time correlations might not be representative of test time correlations. Here, testing to what extent a learned representation generalizes to test time data with a shift in correlation would also strengthen results.  On balance, the metareviewer s assessment is that this paper falls narrowly below the threshold for acceptance. While the experimental evaluation represents a substantial effort that in itself is above the bar, there are problems with narrative and the clarity of  writing that rise above the level of minor revisions that could be addressed by camera ready without additional review. Based on this, the metareviewer will recommend rejection. With a little bit more work on writing and exposition, this will make a great paper at a future conference. 
This paper presents an interesting method dubbed quotient manifold modeling to handle the "multi manifold" structure of natural data and generalize to new manifolds that arise from novel discrete combinations. While some of the methods and ideas were appreciated by reviewers, there were a number of experimental and clarity concerns. The authors s did not submit a rebuttal, and the many unaddressed concerns (especially around experimental baselines) lead me to recommend rejecting this work.
The paper present a new learning based approach` to solve the Maximum Common Subgraph problem. All the reviewers find the idea of using GCN and RL to guide the branch and bound interesting although, even after reading the rebuttal, there are some important concerns about the paper.  The main issue raised by many reviewers are on scalability of the methods and motivation of the problem. It would be nice to add a scalability experiments on large networks(>1M nodes) to show that the method could potentially scale. In fact, the original motivation based on drug discovery, chemoinformatics etc. application is a bit weak because in those area domain specific heuristic should work better.  Overall, the paper is interesting but it does not meet the high publication bar of ICLR.
The submitted paper is well written and easy to follow and also the idea of using VAEs for making inferences about the opponents on which a policy can be conditioned on is sensible. Also the reported performance in comparison to two baselines is good (although I have concerns about the selection of the baselines—see below). Acceptance of the paper was suggested by 3 of the reviewers and rejection by one of the reviewers. While I don’t share all concerns of the negative reviewer, I also suggest to reject the paper.  My suggestion to reject the paper is mainly based on seeing concerns of the positive reviewers more critical as these reviewers themselves and some concerns I have on my own. In particular, I don’t think that all MARL approaches can simply be discarded for comparison—no matter whether the opponents are learning or not. Regarding the evaluation, I think that an environment with real opponents must be considered and that robustness is a key property that should be studied (otherwise an approach with a fixed set of best response policies and inference about the opponent might perform as well). In that regard I also find the selection of baselines insufficient—the minimum I would expect is to consider a NOM baseline using an RNN (which as far as I can tell is not the case) such that it could make inferences about the opponent.  I want to acknowledge that the paper improved quite a lot during the rebuttal period in which the authors extended their discussion of related work on opponent modeling.  In summary, the paper could be improved substantially by an extended empirically study (more environments + baselines + "mismatch" settings). If the currently observed performance gains also hold in these settings, this can become a good paper but in its current form I think the paper is not demonstrating that the proposed approach performs favorably over natural baselines and works well against real opponents. 
This work proposes a fully explored masking strategy, segmenting the input text, which maximizes the Hamming distance between any two sampled masks on a fixed text sequence. The hope is to reduce the large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM lead to undesirably large gradient variance, which typically hurts training efficiency with stochastic gradient optimization algorithms.  Pro    A clear and interesting, novel leading theoretical idea. The paper has one good theme that it pursues.   A mostly well written paper   Contains good theoretical discussion   Experiments support the idea  Con    The experiments could be better, especially they don t actually measure a reduction in gradient variance only accuracy   The proofs are at best loose   Alternative methods of reducing variance like using large mini batches are not considered   The results might go away with use of stronger (larger) contextual LMs   There isn t good comparison to other methods of masking like span masking and salient term masking   Some of the things included seem quite haphazard (the ablations don t seem to the point, it s not really clear what this has to do with continual learning)  Overall, this paper feels to be in a premature state. The idea is interesting, but the idea and the paper needs to be developed more, with stronger results. I think it doesn t deserve to be accepted at this time. 
Well written paper that proposes a flow based model for categorical data, and applies it to graph generation with good results. Extending flow models to handle types of data that are not continuous is a useful contribution, and graph generation is an important application. Overall, the reviewers were positive about the paper, and only few negative points were raised, so I m happy to recommend acceptance.
This paper present novel formulations to address the problem of unbalanced Gromov. The Conic formulation is very interesting but stays theoretical until optimization algorithms are available. The Unbalanced Gromov is a nice extension of Gromov and comes with relatively efficient solvers. Some very limited numerical experiment show the proposed UGW used between 2D distributions (two moons) and graphs.  The paper had some mixed reviews with reviewers acknowledging the novelty of the approach (albeit an extension similar to unbalanced OT) and of the theoretical results. The detailed a very well written response to the reviewers comment has been appreciated. But all reviewers also noted a lack of numerical experiments outside of the very simple illustrations in the paper. This paper is a very nice contribution to the theory of optimal transport but fails at illustrating its relevance to the ML community.  Despite acknowledging the theoretical contributions of the paper, the  AC recommends a reject but strongly encourages the authors to complete the experimental section with some ML applications or at least proof of concepts (graph classification, domain adaptation, ...).  
This paper proposes a software package to ease and provide a standard way for Hessian related computation, both for loss analysis and second order optimization. I think all reviewers agree with the usefulness of this work but differ in their assessment whether this work is ready for publication. Given the emphasize on providing a software package I share the view that careful testing and support of usability is important. While there is quite some spread in the scores I think in this case the average score is an appropriate way to compensate for subjective differences between reviewers. I think it is justified to encourage the authors to invest a bit more work to turn this into a fully convincing contribution.  
This paper proposes to solve the fuel optimization problem in hybrid electric vehicles using reinforcement learning. The work is interesting, but the reviewers consider it lacks novelty and it has different concerns on the assumptions of the modeling. The paper is quite difficult to follow. 
This paper is a computational linguistic study of the semantics that can be inferred form text corpora given parsers (which are trained on human data) are used to infer the verbs and their objects in text. The reviewers agreed that the work was well executed, and that the experiments comparing the resulting representations to human data were solid. The method employed has little or no technical novelty (in my opinion, not necessarily a flaw), and it s not clear what tasks (beyond capturing human data) representations could be applied to (again, not a problem if the goal is to develop theories of cognition).   The first draft of the work missed important connections to the computational linguistics literature, where learning about  affordances for verbs  (referred to as  selectional preferences ) has long been an important goal. The authors did a good job of setting out these connections in the revised manuscript, which the reviewers appreciated.   The work is well executed, and should be commended for relating ideas from different sub fields in its motivation and framing. But my sincere view is that it does not meet the same standards of machine learning or technical novelty met by other papers at this conference. It is unclear to me what the framing in terms of  affordance  adds to a large body of literature studying the semantics of word embeddings, given various syntactically and semantically informed innovations.  It feels to me like this work would have been an important contribution to the literature in 2013, but given the current state of the art in representation learning from text and jointly learning from text and other modalities, I would like to have seen some attempt to incorporate these techniques and bridge the gap between the notion of affordance in text/verbs (selectional preference) and Gibson s notion of object affordance (what you can do physically with an object) in experiments and modelling, not just in the discussion. Such a programme of research could yield fascinating insights into the nature of grounding, and the continuum from the concrete, which can be perceived and directly experienced, to the abstract, which must be learned from text. I encourage the authors to continue in this direction. An alternative is to consider submitting the current manuscript to venue where the primary focus is cognitive modelling, and accounting for human, behavioural data, and where there is less emphasis on the development of novel methods or models.  For these reasons, and considering the technical scope of related papers in the programme, I cannot fairly recommend acceptance in this case. 
Although the reviewers acknowledge some contributions of the paper, it has some limitations on both theoretical results and numerical experiments. It is still unclear about the effectiveness of the proposed method. The authors should consider the following issues for the future submission:   1) The justification of $\tau$ is not clear in federated learning with respect to communication efficiency (please see Reviewer 1’s comments).  2) The bounded stochastic gradient assumption is not applicable in the strongly convex case. This issue has been discussed clearly in [Nguyen et. al, “SGD and Hogwild! Convergence Without the Bounded Gradients Assumption”, ICML 2018]. Therefore, the constant G in Section 3.2. would damage the complexity bound since it could be arbitrarily large.   3) Although the goal is to illustrate the benefits of the proposed quantization approach, the numerical experiments and the theoretical contributions are limited. The theoretical results are incremental from the existing optimization theory (both strongly convex and non convex). Moreover, network architecture and data sets are not enough to justify the efficiency of the method.  
The work focuses on detecting whether a certain data sample was used to train a deep network based conditional image synthesis model. The key idea is not to rely on just reconstruction error but normalizing it via a proposed difficulty score. The reviewers found the problem statement important and the paper easy to follow. However, a common concern in the discussion was the approach is specific to image to image translation problems. Many other minor questions were answered by the authors in the rebuttal. Upon discussion post rebuttal, the reviewers decided to maintain their score. AC and reviewers believe that the paper will benefit from better analysis and description of difficulty score and its correlation with reconstruction error. It would be ideal to see how these ideas are useful for a broader problem than image translation. Please refer to the reviews for final feedback and suggestions to strengthen the submission.
This paper shows how "road rules" (e.g., implicit designation of fast lanes on a highway) naturally emerge in a multi agent MDP. The paper shows that interesting traffic rules do emerge, and it presents a detailed analysis of the factors that lead to this emergence. The paper is complemented by documented source code, with the aim to encourage the community to further work on the topic.  The reviewers agreed that this is original work, and appreciated its simplicity. Two concerns that were recurrently voiced were that 1) there is no algorithmic innovation and 2) there is no comparison to baseline models, or more generally a better placement in the context of existing literature.  The authors provided a detailed and, to my eyes, convincing response. With respect to the two concerns above, I would go as far as saying that 1) (no algorithmic innovation) is a feature, not a bug. The paper is interesting exactly because it studies emergent phenomena after framing multi agent driving as a standard RL problem. Concerning 2) (lack of baselines), it seems to me somewhat besides the point: The paper is not claiming state of the art on some benchmark for a new algorithm, but studying how certain implicit rules emerge in a given setup. In this sense, as the authors point out, rather than looking at alternative baselines, it is informative to look at which aspects of the setup contribute to rule emergence, which is what the paper does.  Although I realize that in proposing this I am going beyond the reviewers  ratings, I found this to be an original and exciting paper, that I would strongly like to see accepted at the conference. 
In this paper, the authors study how to incorporate experimental data with interventions into existing pipelines for DAG learning. Mixing observational and experimental data is a well studied problem, and it is well known how to incorporate interventions into e.g. the likelihood function, along with theoretical guarantees and identifiability. Ultimately there was a general consensus amongst the reviewers that without additional theoretical results to advance the state of the art, the contribution of this work is limited.
This work describes a system for collaborative learning in which several agents holding data want to improve their models by asking other agents to label their points. The system preserves confidentiality of queries using MPC and also throws in differentially private aggregation of labels (taken from the PATE framework). It provides expriments showing computational feasibility of the system. The techniques use active learning to improve the models.  Overall the ingredients are fairly standard but are put together in a new (to the best of my , admittedly limited, knowledge of this area). This seems like a solid attempt to explore approaches for learning in a federated setting with strong limitations on data sharing.
This paper improves the dynamic convolution operation by replacing the dynamic attention over channel groups with channel fusion in a low dimensional space. It includes extensive experiments with reasonable baselines. Dynamic convolutions are a fruitful method for making convnets more efficient, and this paper further improves their efficiency and efficacy with a novel technique. Reviewers all agreed that the paper was clearly written (though some parts were improved after rebuttal).
CausalWorld is a benchmark for robotic manipulation to address transfer and structural learning. The benchmark includes (i) a variety of tasks (picking, pushing, tower, etc) relating to manipulating blocks, (ii) configurable properties for environments (properties of blocks, gravity, etc), (iii) customizable learning settings involving intervention actors, which can change the environment to induce a curriculum.  The reviewers found the paper compelling and with many strengths, including ‘interesting and important ideas’ (R4), ‘simple API with a standardized interface’ for ‘procedural generation of goals’ (R5), ‘strongly motivated and tackles a real and practical problem’ (R3), and ‘benchmark with many good properties’ (R2). By and large, the reviewers agree that the paper presents an important benchmark satisfying several desiderata, which I certainly agree with.   On the other hand, most of the reviewers (3 out of 4) also raised serious concerns, more prominently, about the experimental results and the causal inference component. For instance, R5 commented that “all the SOTA algorithms fail,” and it is hard to quantify how agents would perform well in different tasks. R3 pointed out the lack of “qualitative results exploring the relationship between the identified and proposed causal variables,” emphasizing that ‘the benchmark is well motivated, but not backed up with strong experimental results.‘’ R2 identified the lack of clear causal component in the paper while the paper mentions “opportunity to investigate causality” and “underlying structural causal model (SCM).” All in all, these are valid concerns.  The authors  rebuttal was quite detailed, and appreciated, but left some important questions unanswered.  The first and critical issue is about the causal nature of the simulator. The simulator s name is "causalworld" and its stated goal is to provide "a benchmark for causal structure and transfer learning in a robotic manipulation environment."  Also, the first bullet in the list of contributions is: "We propose CausalWorld, a new benchmark comprising a parametrized family of robotic manipulation environments for advancing out of distribution generalization and causal structure learning in RL." After reading the paper, I was quite surprised to realize there is no *single* example of a causal model, in any shape or form (e.g., SCM, DAG, Physics) or a structural learning benchmark. In other words, there is a serious, somewhat nontrivial gap between the claimed contributions and what was realized in the paper. One way to address this issue would be to make the causality more explicit in the paper, for example, by sharing the underlying structural causal model, how variables form causal relationships, what causal structures are being learned, and how these learned structures compare with the ground truth. I think these would be reasonable expectations of a simulator that aims to disentangle the causal aspect of the learning process.   The second issue is about the experimental results in terms of generalizability. The authors emphasized on different occasions that "The primary goal of this work is to provide the tools to build and evaluate generalizable agents in a more systematic fashion, rather than building generalizable agents for the tasks specified," or "the experiments is to showcase the flexibility regarding curricula and performance evaluation schemes offered with CausalWorld, rather than solving new tasks or proposing new algorithms." These responses are somewhat not satisfactory given that the goal of the paper is providing tools to build generalizable agents, while the authors seem to suggest they are not committed to actually building such agents. Specifically, the experiments did not demonstrate the simulator as a benchmark but only showcased its flexibility (i.e., offering a large number of degrees of freedom). One suggestion would be to evaluate how algorithms (agents) with varying degrees of "generalizability" power perform across tasks with various difficulty levels. As it currently stands, the tasks are too easy or too hard for the standard, uncategorized algorithms, which makes it difficult to learn any lessons from running something in the simulator.   Lastly, I should mention that the work has a great potential to introduce causal concepts and causal reasoning to robotics, there is a natural and compelling educational component here. Still, the complete absence of *any* discussion of causality and the current literature results hurt this connection and the realization of this noble goal. I believe that after reading the paper, the regular causal inference researcher will not be able to understand what assumptions and types of challenges are entailed by this paper and robotics research. On the other hand, the robotics researcher will not be able to understand what a causal model is and the tools currently available in causal reasoning that may be able to help solve the practical challenges of robotics. In other words, this is a huge missed opportunity since there is a complementary nature of what the paper is trying to do in robotics and the results available in causal inference. I believe readers expect and would benefit from having this connection clearly articulated and realized in a more explicit fashion.  If the issues listed above are addressed, I believe the paper can be a game changer in understanding and investigating robotics & causality.  Given the aforementioned potential and reasons, I recommend the paper s acceptance *under the assumption that* the authors will take the constructive feedback provided in this meta review into account and revise the manuscript accordingly. 
This paper clearly has great ideas and reviewers appreciated that. However, the lack of experiments that can be validated by the community (only 1 experiment on the proprietary dataset) is an issue. We don t know if the reported accuracy is a respectable one (in the public domain).   Having a proprietary dataset is a plus, but no public benchmark raises concerns about reproducibility.  We recommend the authors to add some tasks and benchmarks for the community to check for themselves that the numbers reported are non trivial.  
This paper proposed a  new type of models that are invariant to entities by exploring the symbolic property of entities. This problem is important in language modeling since it gives intrinsically more proper representation of sentences, which can better generalize to new entities.  However I still suggest to reject this paper for the following reasons  1. The description of model is not clear enough which can certainly use a serious round of revision. 2. The experiments on bAbi is not convincing enough since it is an overly simple and toyish data set with many ways to hack 3. Similar entity invariant idea has been explored long time ago by  (https://arxiv.org/pdf/1508.05508.pdf) which attempted to represent entities as “variables”  
The reviewers have mixed views about this paper.   However, it seems to me that the paper is missing some important related work on near optimal exploration and it is only picking a couple of superficially similar approaches to look at. In particular, the standard benchmarks of Rmax, UCRL and Posterior Sampling do are not mentioned. I encourage the authors to look at such methods closely. They perform exploration byplanning over a sample or set of possible MDPs.   I also want to raise another issue mentioned by the reviewers. The paper focuses extensively on neural networks, however a count based metric is inherently for the tabular case. Why would a neural network be appropriate in such a setting? (The authors use a hash table because they are using a large discrete space. However, does it makes sense to essentially uniformly randomly cluster states together? Could there be another, better method? How about continuous spaces?)  The algorithm idea is interesting, and the core is given already in (1) as:   give reward in newly visited states . However, the algorithm as described is incomplete.  It is OK as a high level description, but normally we d require sufficient detail to reimplement the method from scratch. You should for example specify how this intrinsic reward value is going to be used. Most of the reviewers, including me, could not understand how a student/teacher network would be  combined with (1) to produce the intended exploration. Please try to explain in as much detail as possible your algorithm in order for the reviewers to be able to make an informed decision. 
All reviewers noted the significance of the problem tackled by this paper and felt that it is going in the right direction. However, they also all noted that the paper was not finalized and polished well enough to be granted publication: details missing, typos, clarifications needed. The reviewers acknowledged the large amount of work that went into improving the paper during the discussion period. R1 even increased their score to reflect that.  Still the paper still needs some work to be accepted at ICLR. In particular, we encourage the authors to improve on 2 axes. 1. Clarifying motivations and contribution: it is still unclear if the main point of the paper is to propose new methods around FTM & constrained updates, etc. or around proposing a new benchmark for catastrophic forgetting, lifelong learning. 2. Reorganizing experimental section: the experimental section should be organized to support #1. Reviewers made a lot of suggestions, like moving Table 4 from the appendix, that should be further refined  We hope that this will allow to increase the clarity and impact of this research work.
This paper proposes an algorithm for offline RL, that consists in solving a finite MDP derived from a fixed batch of transitions. The initial reviews were overall positive, and the concerns raised at this stage were nicely addressed by the rebuttal and the revision from the authors. The final discussion led to the consensus that this paper should be accepted at ICLR.
In this paper, the authors propose a method to find disentangling embeddings of the structure and the attribute of the graph. Overall, this is an interesting paper and the paper is well written and easy to follow, and the paper has some merits. However, the reviewers were still not convinced by the response, and the paper is still below the acceptance threshold.  I encourage authors to revise the paper based on the reviewer s comments and resubmit it to a future venue.   
This paper studies RL with low switching cost under the deep RL setting. It provides new heuristics for doing so. The reviewers are worrying about whether the problem is important in practice, whether the policies obtained can be used in practice, and the theories might not be strong enough. The paper can be strengthened if better theory and more experiments are provided.  
The paper discusses the dynamics of training neural nets and how they are related to features that are robust and predictive (following Ilyas et al). The reviewers had many comments regarding the presentation of the claim and the validity of the empirical results, as well as their unclear practical implications. The authors have improved the writing somewhat but reviewers still thought the manuscript should be substantially improved so that the claims are clearer and empirical validation is more convincing.  The authors are also encouraged to discuss their results in the context of results on  inductive bias of deep learning (e.g., results on NTK, rick regimes, margin maximization etc).    
Very good paper: it proposes a novel parameterization of orthogonal convolutions that uses the Cayley transform in the Fourier domain. The paper discusses several aspects of the proposed parameterization, including limitations and computational considerations, and showcases it in the important application of adversarial robustness, achieving good results. The reviews are all very positive, so I m happy to recommend acceptance.  Also, a big shout out to the reviewers and to the authors for being *outstanding* during the discussion period. The reviewers engaged with the paper to a great depth, and the authors improved the paper considerably as a response. Well done to all of you.
This paper proposes a new kind of CNN that convolves on deformable regions and cooperates with the Poisson equation to determine the deformable regions. Experiments on texture segmentation look promising.  Pros: 1. The paper is well written and easy to follow. 2. The idea is interesting and the reviewers liked it.  3. The experiments on texture segmentation are promising.  Cons: 1. Actually, convolution on non rectangular region is not new, in contrast to the authors  claim and reviewers  belief, although the authors may argue that the mechanisms of determining the region for convolution are different and the CNNs are used for different tasks. See, e.g.,   Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei: Deformable Convolutional Networks. ICCV 2017: 764 773  and other papers by the same first author. So the AC would discount the novelty of the paper.  2. As most of the reviewers commended, the experiments on texture segmentation were insufficient. Although extra experiments were added (thank the authors  effort on doing this), the reviewers actually still deemed that they were not convincing enough, e.g., should compare with more state of the art methods.  Reviewers #2&#4 both confirmed this issue in confidential comments.  Although Reviewer #1 increased his/her score, the final average score is still below the threshold. So the AC decided to reject the paper.
The paper received mixed reviews. Reviewers were concerned about the clarity of the presentation, including both the analyses of gradients and the approach. Some reviewers also suggested improvements to the experiments. The two reviewers who gave a rating of 6 liked the gradient perspective to the long tailed recognition, but their concerns seemed to overshadow their excitement. AC suggested authors improving the paper, especially its clarity and empirical part, following Reviewers  comments and submitting the paper elsewhere. 
This paper analyzes deep networks optimized using non convex noisy gradient descent. The main result shows that in a teacher student setting, the excess risk converges in a fast rate and is stronger than any linear estimators (which include kernel methods). The paper also gives a convergence rate result that depends on some spectral gaps (which can be very small) but not on dimension. Overall the paper is interesting. It should probably emphasize that the dependency on spectral gaps (and the fact that they could be exponentially small) on the convergence as the current abstract suggests efficient convergence.
# Paper Summary  This paper proposes "variance based sample weighting" (VBSW). The key observation is that, in areas where the labeling function is changing rapidly, more samples may be required to achieve a good fit. In Section 3, they justify this intuition by showing that areas in which the label gradient is higher disproportionately impact the generalization performance, and go on to propose upweighting examples proportionately to the local label variance.  This label variance is approximated, for each training example, by finding its k nearest neighbors (either in the feature space, or a latent space they use the latter in their experiments), and calculating the sample variance of the labels. There s a bit of a leap here: the authors started advocating for drawing more samples from rapidly changing regions, but ended up upweighting the existing samples. These are not the same thing, but this issue is not discussed.  The experiments are well thought out and comprehensive (although Reviewer 4 complained about a lack of nontrivial baselines, and I agree). The first and fourth set of experiments are particularly impressive. The first uses toy datasets to enable easy visualization, while the fourth (added during the response based on a comment of Reviewer 1) explores how robust VBSW is to hyperparameter choices and label noise. The second and third sets of experiments are on "realistic" problems, and while the gains are arguably marginal, VBSW does show consistently positive results.  # Pros  1. The reviewers agreed that the fundamental idea was intuitive and well explained 1. The algorithm is general, and easy to implement 1. Reviewer 1 asked how robust VBSW was to hyperparameter choices and label noise, and the authors added a new section that I think does an excellent job alleviating such concerns 1. Experiments are comprehensive, including both toy datasets on which the behavior of the algorithm can be easily visualized, and more realistic experiments on MNIST, CIFAR 10,  RTE, STS B AND MRPC. They show consistently positive results (although not especially large ones)  # Cons  1. The reviewers had mixed opinions on the writing quality, although they generally agreed that it was well organized. There are a large number of typos, grammatical errors, and awkward phrases 1. Reviewer 4 noted that Section 3 assumes that there is no label noise, which seems unrealistic 1. Reviewers 2 and 4 complained about a lack of nontrivial baselines in the experiments (reviewer 4 called this "inexcusable"). In response, the authors added a new experiment comparing with active bias, on which the results were arguably positive, but not convincingly so. Regardless, this is only one experiment there should be baselines for all experiments (except perhaps for the ones on UCI datasets), ideally multiple baselines (would a curriculum learning approach be appropriate on any of these tasks?)  # Conclusion  Three of the four reviewers recommended acceptance, but reviewer 4 gave a very negative score (3: clear rejection). Most of this reviewer s criticisms were fairly minor, with the two major ones being (i) that Section 3 unrealistically assumes that there is no label noise, and (ii) that the experiments have no nontrivial baselines. This first criticism is, I think, not a *huge* deal: Section 3 is only intended to provide intuition. The authors attempted to address the second criticism by adding one experiment comparing to "active bias", but I think that this is insufficient. In addition, while the paper is well structured, I agree with the reviewers who complained about the paper s lack of polish.  All reviewers praised the fundamental idea, and said that the authors gave good intuition for their approach. The experimental results are also fairly comprehensive (aside from the lack of nontrivial baselines), and show positive results. The new section on robustness (in response to Reviewer 1) is a great addition that, I think, fills in most of the remaining "gaps" in this work. The major outstanding issues, in my opinion, are the writing quality, and the lack of nontrivial experimental baselines. These are very fixable, but I think that they re too significant for the paper to be accepted. Overall, I think that this is a borderline paper, but it s on the rejection side of the boundary.
This paper proposes an efficient approach for computing equivariant spherical CNNs, significantly reducing the memory and computation costs. Experiments validate the effectiveness of the proposed approach.  Pros: 1. Speeding up equivariant spherical CNNs is a valuable topic in deep learning.  2. The proposed approach is effective, in all parameter size, memory footprint and computation time. 3. The theory underpinning the speedup method is sound.  Cons: 1. The readability should be improved. Two of the reviewers complained that the paper is hard to read and only Reviewer #2 reflected that it is "easy" to read (but only under the condition that the readers are familiar with the relevant mathematics), and this situation is improved after rebuttal. Nonetheless, this should be further done. 2. The experiments are a bit limited. This may partially be due to limited benchmark datasets for spherical data, but for the existing datasets used for comparison, Esteves et al. (2020) is not compared on all of them. Esteves et al. (2020) is only reported on spherical MNIST, which has very close performance to the proposed one. This worries the AC, who is eager to see whether on QM7 and SHREC’17 the results would be similar.   After rebuttal, three of the reviewers raised their scores. So the AC recommended acceptance.
This paper tackles the important problem of endowing deep RL agents with added interpretability. Action values are decomposed as the combination of GVFs learned on externally specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities. Reviewers praised the approach, as well as the level of detail for reproducibility purposes. R3 had concerns about the generality of the method but follow up experiments have allayed these concerns. Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance.
The paper proposes an adjustment to the ECE metric to make it less biased in the small sample case by including the assumption that the confidence output by a classifier is monotonic with the true correctness probability.  The main idea is to successively make finer bins until a non monotonicity is observed.  The paper is interesting, but the magnitude of the contribution would be just enogh for a short paper if such a track existing in ICLR.    Reviewers have raised concerns about the discrepancy between their revised ECE formula and the Algorithm accompanying it, although that has been fixed through the author feedback phase.  Another concern is that for a paper whose core technical contribution is a revised metric for measuring calibration, a more thorogh empirical study over larger datasets is required.
This paper investigates an improvement to the direct feedback alignment (DFA) algorithm where the "backward weights" are learned instead of being fixed random matrices. The proposed approach essentially applies the technique of DFA to Kolen Pollack learning. While reviewers found the paper reasonably clear and thought the experiments were acceptable, there were significant concerns about the novelty of the approach and the fact that the proposed approach was a straightforward combination of existing ideas. Further, the paper could have done a better job situating (and applying) the proposed method to DFA variants that have been proposed since the original DFA paper came out.
This paper proposes methods to estimate how informative a single training data is wrt the weights and output of the neural network. All reviewers think this is an interesting problem and the proposed method is easy to implement. On the other hand, the reviewers also raise a few questions:  1.	There is a large body of work analyzing the informativeness of a feature wrt the model. The authors should compare their work to the feature importance analysis. 2.	The derived informativeness of a data depends not only on the network architecture, but also depends on the training algorithm, such as initialization and number of epochs. This makes the notion of data informativeness less general. 3.	The writing should be substantially improved. 
This paper proposes an interesting new direction for low cost NAS. However, the paper is not quite ready for acceptance in its current form. The main area of improvement is around the generalizability of the score presented, both empirically and (ideally) theoretically. The two main directions of generalizability that would be worth investigating are 1) different image datasets (see comments around Imagenet 16) 2) different/larger search spaces. Even simple search spaces consisting of a few architectural modification starting from standard architectures (e.g. resnets) would go a long way in convincing the community that the proposed method generalizes past NasBench.
This paper is overall clearly written, and the proposed approach of performing clustering on the space of persistence diagrams can be a significant contribution.  However, during the discussion, the reviewers share the concern about insufficient empirical evaluation. In particular, datasets are limited and (Lacombe et al. 2018) is not included as a comparison partner, although the authors had a chance to include it in the author response phase. Since this point is crucial, I will reject the paper.  Addressing these points will largely improve the paper, and also reviewers put a great effort to give detailed reviews for the paper. So I hope the authors take the reviews into consideration for further revision of the paper.
The authors propose to use counterfactual (a.k.a as contrastive, as they do not account for causal mechanism) explanations  to explain the errors of an already trained predictive model with images as input data. To this end the authors rely on the manipulation of the latent space of  a VAE with disentangled representations. In general the idea is simple (and based on approaches from prior works) and extends work on counterfactual explanations which have been broadly studied in other domains like decision making where the input data have often semantic meaning (in contrast with the pixel of an image).  While the technical contribution is quite limited, I believe that the general approach of the paper is interesting.   However, even after the rebuttal and reading the updated version, it is still unclear what exactly means key concepts for the paper like trivial and actionable, and more importantly how to use the proposed approach in practice, beyond checking/correcting for potential gender bias in  the data (given that you have access to gender information). In particular, it is not clear how you measure "non triviality" for the predictive task when you do not have additional knowledge (like the gender) or when the disentangled latent representation do not correspond to semantical features (which as far as I understand they do not need to).  Similarly, while actionability and diversity have been broadly discussed in the decision making domain, it is again not clear what an actionable feature means here to me. Is it just that you can perturb the latent space?   Furthermore, I believe it is worth exploring the connection to approaches for adversarial examples. As it has already discussed in the literature, in terms of formulation,  counterfactual explanations resemble the problem of adversarial examples, but it seems substantially different semantically. At times when reading the paper, it feels that it is indeed more related to adversarial examples than to counterfactual explanations, as the explainability part seems quite superficial. Thus, I would encourage the authors to better position their paper.   In summary, I believe that the paper requires further work before being ready for publication. In particular, the paper would significantly from: i) a better positioning of paper with respect to the literature; ii) formally introduce  key concepts like actionability, diversity and triviality, explaining what they mean in this context, and how to measure them; and more importantly, iii)  explaining how the proposed approach (which by the way involves training a generative model)  can be used in general to  understand  a model. On a final note, I believe that the paper would benefit from from bringing back to the main body of the paper the experiments that were moved to the appendix during the rebuttal.  
The paper was evaluated by 4 knowledgeable reviewers and got mixed scores. While most reviewers appreciated the new intuitive approach to meta RL. there were severe concerns about algorithmic choices and the evaluations that led to a poor score from some reviewers. These concerns are summarized below:   The motivation of experience relabeling for out of distribution samples is not clear (R2)   It is unclear why experience relabeling does not work for in distribution samples (R2, R4)   The reported performance is not a fair comparison as it is typically not known when a task is in distribution or out distribution, so we would either have to take always experience relabeling or never (or learn when do use which algorithm)   The paper falls short in terms of evaluations (R3, R4), in particular it remains unclear to me if MIER can, under realistic circumstances. It is suggested to use more established benchmarks such as Meta World to evaluate the performance of MIER.   For the given reasons, I recommend that the authors do these corrections and  go through another round of reviews at another conference. 
Three reviewers have reviewed this paper and they maintain their findings after the rebuttal. The reviewers are mainly concerned about the novelty (several highly related papers exist) and well as the technical contribution (more theoretical developments are needed). Therefore, this paper in its current form cannot be accepted.
Following a strong consensus across the reviewers, the paper is recommended for rejection. They have all acknowledged some weaknesses of the paper, for instance  * Inadequate reference to prior work * Unsatisfactory level of polishing * Too limited evaluation, with more comparisons to baselines required * The proposed approach ("Dijkstra algorithm") is not enough justified and motivated  * Clarity (missing definitions of key components).  This list, together with the detailed comments of the reviewers, highlight opportunities to improve the manuscript for a future resubmission. 
The focus of the paper is stochastic backpropagation for both continuous and discrete random variables. By using standard results from Fourier analysis the authors rewrite the corresponding gradients in an infinite weighted sum form ((3) and (9)), extending the results of (Rezende et al. 2014) and (Fellows. et al., 2018). The efficiency of the approach is illustrated in 2 toy examples.  As summarized by the reviewers, the problem tackled is interesting. However, they also pointed out that the novelty of the approach is quite limited and its practical usefulness is not clear (it should by demonstrated against state of the art baselines, on realistic benchmarks).
This paper first makes the observation that incidental supervisory data can be used to define a new prior from which to calculate a PAC Bayes generalization guarantee.  This observation can be applied to any setting where there is unsupervised or semi supervised pre training followed by fine tuning on labeled data.  The PAC Bayes bound is valid when applied to the fine tuning.  For example, one could use an L2 bound (derived from PAC Bayes) on the difference between the fine tuned parameters and pre trained parameters.  But the paper proposes evaluating the value of pre training before looking at any labeled data. Let $\pi_0$ be the prior before unsupervised or semi supervised training and let $\tilde{\pi}$ be the prior after pre training.  The paper proposes using the entropy ratio $H(\pi_0)/H(\tilde{\pi})$ as a measure of the value of the pre training.  As the reviewers note, this is not really related to PAC Bayes bounds.  Furthermore, it is clearly possible that the pre training greatly focuses the prior but in a way that is detrimental to learning the task at hand.  I have to side with the reviewers that feel that this is below threshold.
This paper proposes a model for predicting edits to trees given an edit specification that comes either from the ground truth before after state (“gold” setting, like reconstruction error of auto encoder) or from the before after state of an analogous edit. The problem setting follows mostly from Yin et al (2019).   There are several shortcomings of this paper:  1. The technical novelty of the model is somewhat limited, as it’s an assembly of components that have been used in related work. Authors insist in the discussion on the novelty of the tree edit encoder (Sec 3.2), but I think this is overstated. The related tree edit models (e.g., Tarlow et al (2019)) perform a very similar encoding *in the decoder* when training with teacher forcing. While it’s true that decoders are typically thought of as monolithic entities that generate a sequence of edits from a state, inside the teacher forced training, the models are computing a representation of a prefix of ground truth edits, which are then repeatedly used to predict next edits. AFAIU, the proposal is basically to use this hidden representation as the edit encoder.   2. The claim that the approach is more language agnostic than Dinella et al (2020) also seems shaky, as the authors admit in their response that language specific grammars need to be handled specially. E.g., I expect that the authors of Dinella et al would find it easier to extend their existing code to use a new language than to adapt this approach.  3. The submission relies too heavily on the “gold” setting (where the target output is fed as an input), and I’m skeptical of their characterization of Yin et al’s intentions when the authors say in comments, “Because of this, models that are able to reproduce the desired output effectively have a demonstrably better inductive bias that allows them to do so efficiently. This was the original motivation expressed by Yin et al. (2019).”  I don’t see this stated in the Yin et al paper. I see Yin et al. characterizing this setting as an upper bound and saying “better performance with the gold standard edit does not necessarily imply better (more generalizable) edit representation.” (Yin et al., 2019). It’s worrying that the proposed model only seems to do better in this setting, which would be very easy to game if one were aiming to directly optimize for it.   Having said this, (1) is not a standard way to think about encoding edits, (2) is debatable, and we can hope that future work does not treat improvements in the “gold” setting as a valid research goal. Further, there is another contribution around imitation learning that the reviewers appreciate. In total, reviewers did an excellent job and generally believe the paper should be accepted. I won’t go against that recommendation.
This paper extends past work on kNN augmentation for language modeling to the task of machine translation: a classic parametric NMT model is augmented with kNN retrieval from an external datastore. Decoder internal token level representations are used to index and retrieve relevant contexts (source + target prefix) that weigh in during the final probability calculation for the next target word. Results are extremely positive across a range of MT setups including both in domain evaluation and domain transfer. Reviews are thorough, but quite divergent. There is general agreement that the proposed approach is reasonable, well motivated, and clearly described   and further, that experimental results are both solid and relatively extensive. However, the strongest criticism concerns the paper s relationship with past work.  In terms of ML novelty, everyone agrees (including the paper itself) that the proposed methodology is a relatively simple extension of past work on non conditional language modeling. However, two of the four reviewers strongly feel that, in light of the potentially prohibitive decoding costs, the positive experimental results are not sufficient to make this paper relevant to an ICLR audience given the lack of ML novelty. In contrast, another reviewer strongly takes an opposite stand point:  rather, that the results will be extremely impactful to the MT subcommunity at ICLR since they are unexpected (i.e. that a non parametric model might compete with highly tuned NMT systems) and very positive across a range of domains and settings (i.e. in domain, out of domain, multilingual)   further, that the approach has substantial novelty in the context of MT where parametric models are the norm and that it might inspire substantial future work  (e.g. on efficient decoding techniques and further non parametric techniques) given that it so drastically breaks the current MT mold. The final reviewer shares the concern of the former two about novelty, but is swayed by the experimental results and potential uses for the model (given kNN augmentation is possible without further training) and therefore votes for a marginal accept. After thorough, well reasoned, and well intentioned discussion between all four reviewers, the reviews land just barely in favor of acceptance, but with substantial divide. After considering the paper, reviews, rebuttal, and discussion I am swayed by the argument that (a) these experimental results are largely unexpected, (b) they are both extremely positive and offer a new trade off between test and train compute in MT, and (c) that the paper may therefore inspire substantial discussion and follow up work in the community. Thus I lean in favor of acceptance overall.
The paper is motivated by the observed similarity between learned filters at the low layers of a convolutional neural network and oriented Gabor filters. It proposes to replace the lower layers with dual tree wavelet packet transforms, which yield fixed oriented frequency selective features. Instead of learning filters from scratch, it proposes to learn only a scalar importance for each of these features, reducing the number of learned parameters. Experiments with the AlexNet architecture on ImageNet indicate that this modification does not reduce performance, but does significantly reduce the number of parameters. The paper argues that this modification also improves the interpretability (and in the case of complex dual tree wavelets, potentially the invariance properties) of the low level features.   Pros and cons:  [+/ ] As the paper clearly argues, replacing learned filters with wavelet packet transforms improves the interpretability of the low layers of a convolutional network. While other works have pursued similar ideas, limiting the conceptual novelty, at a technical level this is the first work to use the dual tree complex wavelet transform for this purpose. The DT CWT may have mathematical advantages. The paper and rebuttal argue that it it is conceptually cleaner (“sparser”, since the transform is generated by a single filter) although there may not be a greater reduction in the number of trainable parameters.  [+] The additional per channel weights are redundant in terms of the representation capacity of the network, but may effectively introduce sparse regularization (see work on the “Hadamard parameterization” in implicit sparse regularization), allowing the network to select relevant wavelet features.  [+] The paper is well organized and cleanly written. The authors revision has done a good job of addressing all clarity concerns of reviewers.   [ ] Several reviewers raised concerns about the limited scope of the experiments: the paper only replaces a single layer of a particular architecture (AlexNet) and evaluates on one particular dataset (ImageNet).   [ ] The main proposed benefit of this modification is in the interpretability of the network and its potential amenability to mathematical analysis. This claim would be stronger if the paper either 1. showed the benefit by exhibiting some rudimentary mathematical theory for this network or 2. used this idea to demonstrate networks that are significantly more interpretable, say by replacing all learned convolutional layers with DT CWPT.   The paper’s reviews were split. All reviewers appreciate the paper’s clean exposition of a reasonable idea, and note the novelty of using dual tree wavelets in this context. However, reviewers express concerns about the paper’s significance: it could do more to show how replacing the lowest layer with DT CWPT yields new insights, and do more to demonstrate (both rhetorically and experimentally) the generality of its ideas. Based on the bulk of the reviews, as written the paper falls slightly below the threshold for acceptance. 
This paper studies the problem of uncertainty estimation under distribution shift. The proposed approach (PAD) addresses this under estimation issue, by augmenting the training data with inputs that the network has unjustified low uncertainty estimates, and asking the model to correct this under estimation at those augmented datapoints. Results show promising improvement over a set of common benchmark tasks in uncertainty estimation, with comparisons to a number of existing approaches.  All the reviewer agreed that the experiments are well conducted and the empirical results are very promising. However, they also had a shared concern on the justification of the approach. Reviewers are less willing to accept a paper merely for commending its empirical performance.  I share the above concern as the reviewers, and I personally found the presentation of the approach a bit rush and disconnected from the motivation. For example, the current presentation feels like the method is motivated by BNNs but it is not clear to me how the proposed objective connects to the motivation. Also no derivation of the objective is included in either main text or appendix.   In revision, I would suggest a focus on improving the clarity and theoretical justification of the proposed objective function.
The paper is acknowledged by all the reviewers as making a novel contribution   the proposal to reweight state action pairs depending on the variation in their Q value estimates during learning. However, despite its extensive reporting of numerical experiments, its arguments in favor of the proposed approach are found to be wanting on both empirical and theoretical fronts. Reviewer 3 points out (correctly, in my opinion) that 5 (or even 10 in the updated version) independent trials are not sufficient to establish the validity of the approach up to statistical significance, and that even a well reasoned heuristic explanation of why reweighting is expected to work in terms of reducing Q value estimation error is missing. I agree with this point, which also struck me when reading the submission myself, that at the very least, the submission ought to contain a basic (and not necessarily rigorous) argument as to why the variance reduction ostensibly achieved due to reweighting should lead the estimation algorithm to the right Q function in a general function approximation setting. For instance, even in the simplest multi armed bandit setting, it is of interest to ask why this procedure should perform consistently without introducing unwanted bias in an unforeseen sense, and a clear explanation offered for this would be interesting. Another important concern that most reviewers are left with is about the lack of sufficient insight into the action of the UCB mechanism against the backdrop of the reweighting procedure (reviewers 1, 2, 3). I hope that the author(s) assimilate the feedback to strengthen the paper s main pitch further and make a strong case in the near future. 
The paper proposes a distributional perspective on the value function and uses it to modify PPO for both discrete and continuous control reinforcement learning tasks. The referees had noticed a number of wrong/misleading statements in the initial version of the submission, and the AC had also pointed out several problematic statements in a revised version. While the authors had acknowledged these mistakes and made appropriate corrections, there are several places that still need clear improvement before the paper is ready for publication. The paper seems to introduce a novel actor critic algorithm. However, the correctness of its key step, the  SR($\lambda)$ algorithm, has not been rigorously justified. For example, it is unclear how the geometric random variables would arise in that algorithm. For experiments, the AC seconds the comments provided by Reviewer 2 during the discussion: "The empirical comparisons are overall still lacking: for the smaller scale experiments, whilst the authors have been actively engaged in improving these comparisons during the rebuttal, at present, they are still in need of updating to make a fair comparison, for example in terms of the number of parameters included. The authors have acknowledged this, although the rebuttal period ran out before they were able to post new plots. The large scale empirical results are still lacking reasonable baselines against existing distributional RL agents."  
The work proposed a new approach to encode time series that are irregularly sampled and multivariate using time attention module and an encoder decoder framework based on VAE. All the reviewers find the approach novel and the experiments extensive with encouraging results. Please continue to improve the presentation of the paper. I would  suggest to move the diagram showing the overall architecture to the main text to assist the explanation. Reviewers also would like to see more explanation on the experimental results and some ablation studies to show the importance of each component of the proposed architecture. 
This paper gives a new theoretical tool to connect the gap between the spectral perspective and spatial perspective of graph neural networks. The frame work is considerably broad and can deal with several existing methods. From this view point, the connection between the spatial and spectral perspectives are made explicit while they are noticed in an informal way by existing researches. The frequency response of several methods are analyzed through theories with support by some numerical experiments.  The idea of connecting spatial and spectral perspective would not be entirely new, but the main novelty of this paper is to make it explicit and analyzed the frequency response of well known methods concretely. This is informative to the literature and extends some known results to more general settings. The numerical experiments well justify the plausibility of the theory. For reasons mentioned above, I think this paper is worth publishing in ICLR2021.
The paper proposes to use reconstruction error of autoencoder as the energy function and normalize the resulting density for detecting anomalous/OOD examples. Reviewers have raised several concerns with the paper, including, lack of insights into why the AE energy is better for OOD detection than other energy function parameterizations, and incremental nature of the proposed method. Authors have not responded to these concerns. The paper is not suitable for publication in its current form. 
This paper proposed a method to train quantized supernets which can be directly deployed without retraining. A main concern is that there is limited novelty. The proposed method looks like a combination of well known techniques. Experimental results are promising. However, it is not clear if the comparisons are fair and if all the methods are using the same setup. It is desirable to have additional analysis and ablation studies. The writing can also be improved.
The paper proposes a modification to the DeepMind Control Suite to measure generalization with respect to visual variation. The authors run baseline experiments against their new benchmark and discover, unsurprisingly, that RL agents learning from visual observations overfit to spurious details of the observations.  Reviewers generally found the work to be clearly written, and the experimental analysis to be thorough and well done, though concerns about the rather simple nature of the visual augmentations persisted even after updates and author rebuttals. There were also concerns that by focusing only on Soft Actor Critic in the experiments.  3 of 4 reviewers felt the work met the acceptance bar, albeit only marginally. The dissenting reviewer s concerns centered on clarity (many specific issues appear to have been remedied), the relatively limited nature of the augmentations, and the fact that reviewers were not given access to the code.   While the submission has potential, improvements needed are not minor, and given the short process, we can only accept papers as is, rather than expecting certain changes. Please take the reviewers  comments into consideration as you revise and resubmit to a future venue.
Four reviewers have reviewed this paper. After rebuttal, the reviewers  recommendations were borderline. Rev. 4 remains concerned about relation of second order approaches in CV and second order filters. Indeed, there exists a connection although it is perhaps subtle in its nature and equally concerning is the connection with general Polynomial filters in many GCN papers. As other reviewers point out, MixHop and Jumping Knowledge also allow multi hop designs. More importantly, APPNP and SGC networks allow multiple hops. From that point of view, the proposed approach is rather a recap of existing observations and contributions. Finally, even Rev. 2 has indicated that the paper is perhaps  average  after checking with comments of other reviewers. Therefore, at this point, the paper is slightly below the acceptance threshold. 
Reviewers raised several concerns about the paper guided by unfounded heuristics as well as the artificiality of the tasks involved.  Rebuttal only answered a few of them and did not convince the reviewers which has been clearly stated in the response. We hope that the authors will improve the paper for future submission based on the reviews. 
The paper presents a new algorithm for byzantine resilient nonconvex distributed optimization. The presentation is clear, the motivation is solid, and the problem setting is interesting. The novelty of the present work is sufficient for publicaiton. The new scheme comes with some provable guarantees, improving the prior state of the art. Some of these guarantees are arguably not corresponding to strong operational robustness guarantees, however they compare well with convergence proofs of the related literature. Some concerns were raised with regards to comparison with some prior work, but the authors addressed it in the rebuttal.
The paper swaps characteristics of an object in one image onto those of another object in another image for example, adding fur to a car.  The authors give some examples where the task could be useful.  Further, they successfully argue  that this task is an illustration that the disentanglement task has been done well.  Two reviewers argued for acceptance, two for just below the bar rejection.  The 2nd of those in favor of rejection engaged thoughtfully with the authors and raised the score by 1 after that engagement.  We have decided to accept the submission as a poster.
The paper develops an approach to training generative models with binary weights. The reviewers are split. Two reviewers regard binary networks and the general theme of reducing the computational cost of training as important, and the presented work as a solid contribution. Two reviewers raise concerns about the motivation and the quality of the results. The authors  responses somewhat alleviated the quality concerns of R3, but not the concerns of R4. Overall, the reviewers lean on the positive side. There is disagreement on the importance of the problem, but there is clearly a non trivial subset of the community that welcomes research in this direction. The AC supports acceptance.
The authors consider local  why  or  abductive  explanations for a model and a given class, which identify a minimal subset of features such that they re sufficient to imply that the model predicts the class; and  why not  or  contrastive  explanations, which identify a minimal subset s.t. they re sufficient to imply that the model predicts a different class. The two types of explanation are related using earlier work on minimal hitting sets going back to Reiter (1987).   Reviewers were divided in their opinions. R4 was very positive but with little detail and only medium confidence, then did not participate in discussion. R2 was the only reviewer with high confidence, leaning against acceptance. The paper relies on FOL which was hard for reviewers to grapple with, and may make it challenging for readers. The presentation could be improved by clearly linking to existing work and demonstrating why the new approach is important.
This paper analyses the interaction between data augmentation strategies  and model ensembles with regards to calibration performance. The authors note how strategies such as mixup and label smoothing, which reduce a single model s over confidence, lead to degradation in calibration performance when such models are combined as an ensemble. They propose a simple solution. The paper merits publication.
This paper has received three positive reviews. In general, the reviewers have commented on the importance of the question related to how much selectivity is needed from units of a neural network for good classification   from both the neuroscience and ML perspectives. The reviewers also commented on the thoroughness of the experiments and the general readability of the paper. This paper should be accepted if possible.
The reviewers were in agreement that the paper is below the bar for acceptance, and the authors did not provide a response to reviewer concerns.
This work performs a frequency domain analysis on gradient based adversarial perturbations. The authors argue that the perturbation deltas are largely concentrated in the high frequency domain and suggest a low pass filtering technique to improve the robustness of image classifiers. Reviewers raised several concerns as to how broadly applicable the given claims will hold, noting that these findings will be largely dependent on how the model was trained, what data the model is trained on, and how the adversarial perturbation will be constructed. Additionally, the proposal to apply a low pass filter to improve robustness is not new it has been attempted in several works in the past and current consensus is this only provides improved robustness to high frequency perturbations/corruptions. As a recent example, Yin et. al. studied the robustness properties of models trained with a low pass filter and found that while robustness to high frequency noise and perturbations is improved, this procedure also degrades robustness to low frequency corruptions and perturbations. By focusing the analysis quite narrowly to restricted l2 perturbations the work is missing critical nuances that are well known in the literature studying distribution shift. This AC recommends the authors connect their analysis to the broader issue of distribution shift, for example can the theory provide understanding into how to improve robustness to both high and low frequency corruptions?
The paper shows that standard transformers can be trained to generate satisfying traces for Linear Temporal Logic (LTL) formulas. To establish this, the authors train a transformer on a set of formulas, each paired with a single satisfying trace generated using a classical automata theoretic solver. It is shown that the resulting model can generate satisfying traces on held out formulas and, in some cases, scale to formulas on which the classical solver fails.  The reviewers generally liked the paper. While the transformer model is standard, the use of deep learning to solve LTL satisfiability is novel. Given the centrality of LTL in Formal Methods, the paper is likely to inspire many follow up efforts. There were a few concerns about the evaluation; however, I believe that the authors  comments address the most important of them. Given this, I am recommending acceptance. Please add the new experimental results (about out of distribution generalization) to the final version of the paper. 
This paper performs visual odometry using variational information bottleneck. It assumes video and pose observations and aims to find a latent state that is maximally predictive of the pose observations, while minimizing the mutual information between the image observations and the latent state. It approximates this cost using variational inference. The paper also makes use of deterministic+stochastic latent transition models, as in Hafner et al 2019, 2020. The paper contributes generalization bounds that are based on recent work by Xu and Raginsky 2017 and Zhang et al 2018. This is useful to include, but this contribution is not the main focus of the work in my opinion, and it is not clear how tight the bounds are, especially considering that the original cost function is approximated.    Pros:  The idea of using the deterministic+stochastic transition models of Hafner et al and related works for visual odometry is very interesting and promising avenue for research.   The fact that the experiments are done on some of the major camera and IMU datasets is great.    Needs fixing:  Major: The paper mentions "Extensive ablation studies were conducted to examine the effects of (1) the deterministic component, (2) sample size and (3) extra sensors.". These are great, but I would also have expected to see a range of variations in terms of the weight gamma, including a value of zero. The appendix is vague in this regard and says " and perform a non intensive and small range grid searching." The utility of the information bottleneck idea depends heavily on the weight gamma, and I would have liked more results confirming that the method does well under a range of choices for gamma.  Major: The rotation results produced by this method on EuroC should be improved to be more competitive with okvis. As it stands, it is unclear why the method does not perform as well and the explanations offered in the paper are speculative.     Medium: The paper also mentions in the appendix: "Though 3D von Mises Fisher distribution and 4D Bingham distribution can be arguably more appropriate to model Euler angles and quaternions respectively, it is non trivial to evaluate and use them for training in practice." So, the paper represents rotations using Euler angles. The authors are encouraged to look at https://www.gilitschenski.org/igor/publication/202004 iclr deep_orientation_uncertainty_learning/  Minor: MSCKF was originally coined by Mourikis and Roumeliotis https://ieeexplore.ieee.org/document/4209642 and even though the term is used by follow up works,  it is worth adding the reference.  Finally, I would disagree with one of the reviewers that this paper needs to compare with ORBSLAM2. I think comparing with OKVIS and an MSCKF variant is sufficient for "classic" SLAM and odometry methods.    I think the paper needs one more iteration to fix these issues, even though it is very promising work.      
Dear Authors,  Thank you very much for your detailed feedback to the reviewers in the rebuttal phase. This certainly clarified some of the concerns raised by the reviewers and contributed highly to deepen their understanding of your work.  We positively evaluated the novelty and the superior empirical performance of the proposed method. However, we still have concern about the justification since the proposed model is so complex that it is not clear what was the key for the good performance.   For this reason, I suggest rejection of this submission, in comparison with many other strong submissions. I hope that the reviewers  feedback is useful for improving your work for future publication.  Best, AC
This submission proposes a variant of population based training (PBT) for hyperparameter selection/evolution, aimed at addressing drawbacks of existing variants (e.g. the coupling of the choice of checkpoint with the choice of hyperparameters). Reviewers generally agreed that the paper is interesting and covers an important topic, and the evaluation does show improvements over existing PBT variants. On the other hand they also raised a few important issues:  1. The `hoptim` library is claimed as a primary contribution of the work, but it is not clear from the manuscript what benefits this library offers over existing software. When claiming a library as a main contribution, it is helpful to provide a more thorough description of the software and its benefits, and/or ideally a link (anonymized for review) to the software. The authors did respond by providing a brief description of the benefits of the library, mitigating this issue somewhat. However it s still difficult to discern how/whether to weigh the open source library as a main contribution of the paper.  2. The evaluation is not very convincing: the differences are small and error margins are not provided for the neural network based experiments, meaning that any differences could be due to noise. The authors fairly point out that it is difficult to perform multiple runs of these experiments as the resource requirements are large, and they have done 20 runs of the Rosenbrock experiment with smaller compute requirements. But the reviewers were not convinced that the Rosenbrock experiment reflects the method s application to neural network hyperparameter selection; the problems are too different. The submission would be significantly stronger if it included results over multiple runs of an "intermediate" sized experiment on a problem involving a neural network demonstrating that ROMUL outperforms competing approaches by a statistically significant margin.  3. The proposed approach is ultimately heuristic. This is not necessarily a problem if there are strong empirical results demonstrating the efficacy of the proposed heuristic, but in this case the empirical results didn t convince (see point 2).  Given these concerns raised by reviewers, the submission is not quite ready for ICLR. I hope the authors will consider resubmitting the paper after improving it based on the reviewers  feedback.