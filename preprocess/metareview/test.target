The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.  In practice, the synthetic experiments seem to show a greater improvement than appears in real data.  There are concerns about the clarity, lack of theoretical proofs, and at times overstated claims that do not have sufficient support.  The ratings before the rebuttal and discussion were 7 4 6.  After, R1 adjusted their score from 6 to 4.  R2 initially gave a 7 but later said "I think the authors missed an opportunity here. I rated it as an accept, because I saw what it could have been after a good revision. The core idea is good, but fully agree with R1 and R3 that the paper needs work (which the authors were not willing to do). I checked the latest revision (as of Monday morning). None of R3 s writing/claims issues are fixed, neither were my additional experimental requests, not even R1 s typos." There is therefore a consensus among reviewers for reject. 
This paper studies non smooth and non convex optimization and provides a global analysis for orthogonal dictionary learning. The referees indicate that the analysis is highly nontrivial compared with existing work.   The experiments fall a bit short and the relation to the loss landscape of neural networks could be described more clearly.   The reviewers pointed out that the experiments section was too short. The revision included a few more experiments. The paper has a theoretical focus, and scores high ratings there.   The confidence levels of the reviewers is relatively moderate, with only one confident reviewer. However, all five reviewers regard this paper positively, in particular the confident reviewer. 
All reviewers wrote strong and long reviews with good feedback but do not believe the work is currently ready for publication. I encourage the authors to update and resubmit. 
This paper focuses on  communication efficient Federated Learning (FL) and proposes an approach for  training  large models on heterogeneous edge devices.   The paper is well written and the approach is promising, but all reviewers pointed out that both novelty of the approach and empirical evaluation, including comparison with state of art, are somewhat limited. We hope that suggestions provided by the reviewers will be helpful for extending and improving this work.
This paper develops a stagewise optimization framework for solving non smooth and non convex problems. The idea is to use  standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates   which is standard in many proximal methods. The paper combines this with the analysis for non smooth functions giving a more general convergence results. Reviewers agree on the usefulness and novelty of the contribution. Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue.  The main weakness is that the results only holds for \mu weekly convex functions and the algorithm depends on the knowledge of \mu. Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication. I suggest authors to address these issues in the final version. 
Following the unanimous vote of the submitted reviews, this paper is not ready for publication at ICLR. Among other concerns raised, the experiments need significant work, and the exposition needs clarification.
This paper shows that combining GAN and VAE for video prediction allows to trade off diversity and realism. The paper is well written and the experimentation is careful, as noted by reviewers. However, reviewers agree that this combination is of limited novelty (having been used for images before). Reviewers also note that the empirical performance is not very much stronger than baselines. Overall, the novelty is too slight and the empirical results are not strong enough compared to baselines to justify acceptance based solely on empirical results.
This paper proposes a neural network based method for computing committor functions, which are used to understand transitions between stable states in complex systems. The authors improve over the techniques of Khoo et al. with a method to approximately satisfy boundary conditions and an importance sampling method to deal with rare events. This is a good application paper, introducing a new application to the ML audience, but the technical novelty is a bit limited. The reviewers see value in the paper, however scaling w.r.t. dimensionality appears to be an issue with this approach.
The paper presents a promising approach for continual learning with no access to data from the previous tasks. For learning the current task, the authors propose to find an optimal structure of the neural network model first (select either to reuse, adapt previously learned layers or to train new layers) and then to learn its parameters.   While acknowledging the originality of the method and the importance of the problem that it tries to address, all reviewers and AC agreed that they would like to see more intensive empirical evaluations and comparisons to state of the art models for continual learning using more datasets and in depth analysis of the results – see details comments of all reviewers before and after rebuttal.  The authors have tried to address some of these concerns during rebuttal, but an in depth analysis of the results (evaluation in terms on accuracy, efficiency, memory demand) using different datasets still remains a critical issue.  Two other requests to further strengthen the manuscript: 1) an ablation study on the three choices for structural learning (R3), and especially the importance of ‘adaptation’ (R3 and R1) The authors have tried to address this verbally in their responses but a proper ablation study would be desirable to strengthen the evaluation. 2) Readability and proofreading of the manuscript is still unsatisfying after revision. 
+ the ideas presented in the paper are quite intriguing and draw on a variety of different connections   the presentation has a lot of room for improvement. In particular, the statement of Theorem 1, in its current form, requires rephrasing and making it more rigorous.   Still, the general consensus is that, once these presentation shortcomings are address, this will be an interesting paper. 
The paper proposes and evaluates an asynchronous hyperparameter optimization algorithm.  Strengths:  The experiments are certainly thorough, and I appreciated the discussion of the algorithm and side experiments demonstrating its operation in different settings.  Overall the paper is pretty clear.  It s a good thing when a proposed method is a simple variant of an existing method.  Weaknesses:  The first page could have been half the length, and it s not clear why we should care about the stated goal of this work.  Isn t the real goal just to get good test performance in a small amount of time?  The title is also a bit obnoxious and land grabby   it could have been used for almost any of the comparison methods.  The proposed method is a minor change to SHA.  The proposed change is kind of obvious, and the resulting method does have a number of hyper hyperparameters.  Consensus:  Ultimately I agree with the reviewers that is just below the bar of acceptance.  This does seem like a valid contribution to the hyperparameter tuning literature, but more of an engineering contribution than a research contribution.  It s also getting a little bit away from the subject of machine learning, and might be more appropriate for say, SysML.
The authors propose a method for low resource domain adaptation where the number of examples available in the target domain are limited. The proposed method modifies the basic approach in a CycleGAN by augmenting it with a “content” (task specific) loss, instead of the standard reconstruction error. The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target → source and source  > target). Experiments are conducted on both supervised as well as unsupervised settings. The main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low resource conditions where it is feasible to label a few new instances. Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version. 
AR1 is concerned about the novelty and what are exact novel elements of the proposed approach. AR2 is worried about the novelty (combination of existing blocks) and lack of insights. AR3 is also concerned about the novelty, complexity and poor  evaluations/lack of thorough comparisons with other baselines. After rebuttal, the reviewers remained unconvinced e.g. AR3 still would like to see why the proposed method would be any better than GAN based approaches.  With regret, at this point, the AC cannot accept this paper but AC encourages the authors to take all reviews into consideration and improve their manuscript accordingly. Matters such as complexity (perhaps scattering networks aren t the most friendly here), clear insights and strong comparisons to generative approaches are needed.
The paper presents a method for coarse and fine inference for question answering.  It originally measured performance only on WikiHop and then later added experiments on TriviaQA.  The results are good.  One of the concerns regarding the paper was the novelty of the work, and lack of enough experiments.  However, the addition of TriviaQA results allays some of that concern.  I d suggest citing the paper by Swayamdipta et al from last year that attempted coarse to fine inference for TriviaQA:  Multi Mention Learning for Reading Comprehension with Neural Cascades.  Swabha Swayamdipta, Ankur P. Parikh and Tom Kwiatkowski.  Proceedings of ICLR 2018.  Overall, there is relative consensus that the paper is good with a new method and some strong results.
The paper proposes an interesting idea for efficient exploration of on policy learning in sparse reward RL problems.  The empirical results are promising, which is the main strength of the paper.  On the other hand, reviewers generally feel that the proposed algorithm is rather ad hoc, sometimes with not so transparent algorithmic choices.  As a result, it is really unclear whether the idea works only on the test problems, or applies to a broader set of problems.  The author responses and new results are helpful and appreciated by all reviewers, but do not change the reviewers  concerns.
Strengths: The method extends [21], which proposes an unordered set prediction model for multi class classification. The submission proposes a formulation to learn the distribution over unobservable permutation variables based on deep networks and uses a MAP  estimator for inference. While the failure of NMS to detect overlapping objects is expected, the experiments showing that perm set prediction handles them well is interesting and promising.   Weaknesses:  Reviewer 1: "I find the paper still too scattered, trying to solve diverse problems with a hammer without properly motivating / analyzing key details of this hammer. So I keep my rating." Reviewer 2: "I m glad that the authors are seeing good performance and seem to have an effective method for matching outputs to fixed predictions, however the quality of the paper is too poor for publication."  Points of contention:   Although there was one reviewer who gave a high rating, they were not responsive in the rebuttal phase.  The other two reviewers took into account the author responses, and a contributed comment by an unaffiliated reviewer, and both concluded that the paper still had serious issues.  The main issues were: lack of clear methodology and poor clarity (AnonReviewer2), and poor organization and lack of motivation for modeling choices (AnonReviewer1).
The paper introduces a setting called high fidelity imitation where the goal one shot generalization to new trajectories in a given environment. The authors contrast this with more standard one shot imitation approaches where one shot generalization is to a task rather than a precise trajectory. The authors propose a technique that works off of only state information, which is coupled with an RL algorithm that learns from a replay buffer that is populated by the imitator. The authors emphasize that their approach can leverage very large deep learning models, and demonstrate strong empirical performance in a (simulated) robotics setting.   A key weakness of the paper is its clarity. All reviewers were unclear about the precise setting as well as relation to prior work in one shot imitation learning. As a result, there were substantial challenges in assessing the technical contribution of the paper. There were many requests for clarification, including for the motivation, difference between the present setting and those addressed in previous work, algorithmic details, and experiment details.  I believe that a further concern was the lack of a wide range of baselines. The authors construct several baselines that are relevant in the given setting, but did not consider "naive baseline" approaches proposed by the reviewers. For example, behavior cloning is mentioned as a potential baseline several times. The authors argue that this is not applicable as it would require expert actions. Instead of considering it a baseline, BC could be used as an "oracle"   performance that could be achieved if demonstration actions were known. As long as the access to additional information is clearly marked, such a comparison with a privileged oracle can be properly placed by the reader. Without including such commonly known reference approaches, it is very challenging to assess the proposed method s performance in the context of the difficulty of the task. Generally, whenever a paper introduces both a new task and a new approach, a lot of care needs to be taken to build up insights into whether the task appropriately reflects the domain / challenge the paper claims to address, how challenging the task is in comparison to those addressed in prior work, and to place the performance of the novel proposed method in the context of prior work. In the present paper, on top of the task and approach being novel, the pure RL baseline D4PG is not yet widely known in the community and it s performance relative to common approaches is not well understood. Including commonly known RL approaches would help put all these results in context.  The authors took great care to respond to the reviewer comments, providing thorough discussion of related work and clarifications of the task and approach, and these were very helpful to the AC to understand the paper. The AC believes that the paper has excellent potential. At the same time, a much more thorough empirical evaluation is needed to demonstrate the value of the proposed approach in this novel setting, as well as to provide additional conceptual insights into why and in what kinds of settings the algorithm performance well, or where its limitations are.  
The paper was found to be well written and conveys interesting idea. However the AC notices a large body of clarifications that were provided to the reviewers (regarding the theory, experiments, and setting in general) that need to be well addressed in the paper. 
This paper provides further insight into using RL for active learning, particularly by formulating AL as an MDP and then using RL methods for that MDP. Though the paper has a few insights, it does not sufficiently place itself amongst the many other similar strategies using an MDP formulation. I recommend better highlighting what is novel in this work (e.g., more focus on the reward function, if that is key). Additionally, avoid general statements like “To this end, we formalize the annotation process as a Markov decision process”, which suggests that this is part of the contribution, but as highlighted by reviewers, has been a standard approach. 
Significant spread of scores across the reviewers and unfortunately not much discussion despite prompts from the area chair and the authors. The most positive reviewer is the least confident one. Very close to the decision boundary but after careful consideration by the senior PCs just below the acceptance threshold. There is significant literature already on this topic. The "thought delta" created by this paper and the empirical results are also not sufficient for acceptance.
This paper proposes a new type of activations function based on q calculus. The reviewers found that the papers is significantly lacking in its presentation, in clarity, and in its experimental evaluation. The motivation of the method raises several significant questions to the reviewers, and the proposed method is not sufficiently compared to existing approaches for (noisy) activation functions. After reviews, the authors have failed to present any updates to their paper.
Dear authors,  The reviewers all appreciated your goal of improving dimensionality reduction techniques. This is a field which does not enjoy the popularity it once did but remains nonetheless important.  They also appreciated the novel loss and the use of triplets.to get the global structure.  However, the paper lacks some guidance. In particular, it oscillates between showing qualitative results (robustness to outliers, "nice" visualizations) and quantitative ones (running time, classification performance). I agree with the reviewers that the quantitative ones should have used the same preprocessing for t SNE and TriMap (either PCA or no PCA), regardless of the current implementation in software tools.  Given that the quantitative results are not that impressive, may I suggest focusing on the qualitative ones for a resubmission? The robustness of the emeddings to the addition or removal of a few points is definitely interesting and worth further investigation, optionally with a corresponding metric.
Word vectors are well studied but this paper adds yet another interesting dimension to the field.
pros:   the paper is well written and precise   the proposed method is novel   valuable for real world problems  cons:   Reviewer 2 expresses some concern about the organization of the paper and over generality in the exposition   There could be more discussion of scalability
  + sufficiently strong results  + a fast / parallelizable model     Novelty with respect to previous work is not as great (see AnonReviewer1 and AnonReviewer2 s comments)    The same reviewers raised concerns about the discussion of related work (e.g., positioning with respect to work on knowledge distillation). I agree that the very related work of Roy et al should be mentioned, even though it has not been published it has been on arxiv since May.    Ablation studies are only on smaller IWSLT datasets, confirming that the hints from an auto regressive model are beneficial (whereas the main results are on WMT)     I agree with R1 that the important modeling details (e.g., describing how the latent structure is generated) should not be described only in the appendix, esp given non standard modeling choices.  R1 is concerned that a model which does not have any autoregressive components (i.e. not even for the latent state) may have trouble representing multiple modes.  I do find it surprising that the model with non autoregressive latent state works well however I do not find this a sufficient ground for rejection on its own. However, emphasizing this point and discussing the implication in the paper makes a lot of sense, and should have been done.  As of now, it is downplayed. R1 is concerned that such model may be gaming BLEU: as BLEU is less sensitive to long distance dependencies, they may get damaged for the model which does not have any autoregressive components.  Again, given the standards in the field, I do not think it is fair to require human evaluation, but I agree that including it would strengthen the paper and the arguments.   Overall, I do believe that the paper is sufficiently interesting and should get published but I also believe that it needs further revisions / further experiments.   
This paper proposes an image to image translation technique which decomposes into style and content transfer using a semantic consistency loss to encourage corresponding semantics (using feature masks) before and after translation. Performance is evaluated on a set of MNIST variants as well as from simulated to real world driving imagery.   All reviewers found this paper well written with clear contribution compared to related work by focusing on the problem when one to one mappings are not available across two domains which also have multimodal content or sub style.   The main weakness as discussed by the reviewers relates to the experiments and whether or not the set provided does effectively validate the proposed approach. The authors argue their use of MNIST as a toy problem but with full control to clearly validate their approach. Their semantic segmentation experiment shows modest performance improvement. Based on the experiments as is and the relative novelty of the proposed approach, the AC recommends poster and encourages the authors to extend their analysis of the current results in a final version.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    strong qualitative and quantitative results   a good ablative analysis of the proposed method.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    clarity could be improved (and was much improved in the revision).   somewhat limited novelty.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
 Interesting idea, reviewers were positive and indicated presentation should be improved. 
This paper presents a method whereby a model learns to describe 3D shapes as programs which generate said shapes. Beyond introducing some new techniques in neural program synthesis through the use of loops, this method also produces disentangled representations of the shapes by deconstructing them into the program that produced them, thereby introducing an interesting and useful level of abstraction that could be exploited by models, agents, and other learning algorithms.  Despite some slightly aggressive anonymous comments by a third party, the reviewers agree that this paper is solid and publishable, and I have no qualms in recommending it from inclusion in the proceedings.
This paper addresses an important problem, quantizing deep neural network models to reduce the cost of implementing them on hardware such as FPGAs without severely affecting task performance. The approach explored in the paper combines three ideas: (1) injecting noise into the network to simulate the effects of quantization noise, (2) a smart initialization of the parameter and activation clamping along with learning of the activation clamping using the straight through estimator, and (3) a gradual approach to quantization. While the reviewers agreed that the problem is important, they raised concerns about the novelty of the proposed approach and the quality of the experiments. The authors did not respond to the reviewers in the discussion period, and did not revise their submission.
The authors propose a generative model based on variational autoencoders that provides means to manipulate the high level attributes of a given input. The attributes can be either pre defined ground truth attributes or unknown attributes automatically discovered from the data.  While the reviewers acknowledged the potential usefulness of the proposed approach, they raised important concerns that were viewed by AC as a critical issue: (1) very limited experimental evaluation (e.g. no baseline or ablation results, no quantitative results); comparisons on other more complex datasets and more in depth analysis would substantially strengthen the evaluation and would allow to assess the scope of the contribution of this work  – see, for example, R3’s suggestion to use other dataset like dSprites or CelebA, where the ground truth attributes are known; (2) lack of presentation clarity – see R2’s latest comment how to improve.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarification, more empirical studies and polish to achieve the desired goal. 
although i (ac) believe the contribution is fairly limited (e.g., (1) only looking at the word embedding which goes through many nonlinear layers, in which case it s not even clear whether how word vectors are distributed matters much, (2) only considering the case of tied embeddings, which is not necessarily the most common setting, ...), all the reviewers found the execution of the submission (motivation, analysis and experimentation) to be done well, and i ll go with the reviewers  opinion.
This paper presents an extensive empirical study to sentence level pre training. The paper compares pre trained language models to other potential alternative pre training options, and concludes that while pre trained language models are generally stronger than other alternatives, the robustness and generality of the currently available method is less than ideal, at least with respect to ELMO based pretraining.   Pros: The paper presents an extensive empirical study that offers new insights on pre trained language models with respect to a variety of sentence level tasks.     Cons: The primarily contributions of this paper is empirical and technical novelty is relatively weak. Also, the insights are based just on ELMO, which may have a relatively weak empirical impact. The reviews were generally positive but marginally positive, which reflect that insights are interesting but not overwhelmingly interesting. None of these is a deal breaker per say, but the paper does not provide sufficiently strong novelty, whether based on insights or otherwise, relative to other papers being considered for acceptance.  Verdict: Leaning toward reject due to relatively weak novelty and empirical impact.  Additional note on the final decision:  The insights provided by the paper are valuable, thus the paper was originally recommended for an accept. However, during the calibration process across all areas, it became evident that we cannot accept all valuable papers, each presenting different types of hard work and novel contributions. Consequently, some papers with mostly positive (but marginally positive) reviews could not be included in the final cut, despite their unique values, hard work, and novel contributions. 
`This paper tackles the problem of learning with one hidden layer non overlapping conv net for XOR detection problem. For this problem the paper shows that over parametrized models perform better, giving insights into why larger neural networks generalize better   an interesting question to study. However reviews opined that the setting considered in this paper is too specific to this XOR problem and the simplified network architecture,  and the techniques are not generalizable to other models. Generalizing these results to more complex architectures or other learning problems will make the paper more interesting. 
There s precious little work asking existential questions about adversarial examples, and so this work is most welcome. The work connects with deep results in probability to make simple and transparent claims about the inevitability of adversarial examples under some assumptions. The authors have addressed the key criticisms of the authors around clarity.
This paper applied probability matching to A* sampling in order to provide an approximate variant without a bound function. It is a novel idea and a good contribution to the A* sampling family. The authors also provided regret analysis for the adoption of PM.  However, as pointed out by R1 and R3, the authors failed to clarify the approximation introduced by the PM and its implication in the output samples. The empirical comparison should also take into account this difference. Further analysis of the bias in the sample distribution would also help clarify the pros and cons of the proposed method.  R3 also raised the concern that the description of the preliminary section and the main contribution in section 4 was not clear.
This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter. Reviewers agree that this is interesting and also very useful for the community. However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper. Post rebuttal, one of the reviewer increased their score, but the other has reduced the score. Overall, the reviewers are in agreement that more work is required before this work can be accepted.  Some of existing work on variational inference has not been included which, I agree, is problematic. Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear. The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise. Such insights are currently missing in the paper.  Reviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work. In its current form, the paper is not ready to be accepted and I recommend rejection. I encourage the authors to resubmit this work. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The paper    tackles an interesting problem   makes a concerted effort to provide qualititative results that give insight into the models behaviour.   sufficiently cites related work.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The model architecture lacks novelty.   There was also agreement that the contributions   (i) minor modifications of existing sequential attention based models, and (ii) application to the RL domain   are minor.   A lot of space in the paper (section 4.2) is devoted to exploring the use of this model for image classification and video action recognition. However the proposed model performed poorly compared to SOTA methods for this task and no motivation was given for why the proposed model would be useful for such tasks.  All three points impacted the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There was high agreement between the reviewers on the main drawbacks of the paper, before and after the rebuttal. The AC considered the rebuttals by the authors (in which they argued that there was sufficient contribution) but, in the end, agreed with the reviewers  assessments.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
The reviewers all agree that the work is interesting, but none have stood out and championed the paper as exceptional. The reviewers note that the paper is well written, contributes a methodological innovation, and provides compelling experiments. However, given the reviewers  positive but unenthusiastic scores, and after discussion with PCs, this paper does not meet the bar for acceptance into ICLR.
This paper explores the use of multi step latent variable models of the dynamics in imitation learning, planning, and finding sub goals. The reviewers found the approach to be interesting. The initial experiments were a main weakpoint in the initial submission. However, the authors updated the experimental results to address these concerns to a significant degree. The reviewers all agree that the paper is above the bar for acceptance. I recommend accept.
All three reviewers expressed concerns about the writing of the paper. The AC thus recommends "revise and resubmit".
The paper investigates a detailed analysis of reduced precision training for a feedforward network, that accounts for both the forward and backward passes in detail.  It is shown that precision can be greatly reduced throughout the network computations while largely preserving training quality.  The analysis is thorough and carefully executed.  The technical presentation, including the motivation for some of the specific choices should be made clearer.  Also, the requirement that the network first be trained to convergence at full 32 bit precision is a significant limitation of the proposed approach (a weakness that is shared with other work in this area).  It would be highly desirable to find ways to bypass or at least mitigate this requirement, which would provide a real breakthrough rather than merely a solid improvement over competing work.  The reviewer disagreement revolves primarily around the clarity of the main technical exposition: there appears to be consensus that the paper is sound and provides a serious contribution to this area.  Although the persistent reviewer disagreement left this paper rated at the borderline, I am recommending acceptance, with the understanding that the authors will not disregard the dissenting review and strive to further improve the clarity of the presentation.
The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results. All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors. I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version.
This is a proposed method that studies learning of disentangled representations in a relatively specific setting, defined as follows: given two datasets, one unlabeled and another that has a particular factor of variation fixed, the method will disentangle the factor of variation from the others. The reviewers found the method promising, with interesting results (qual & quant).  The weaknesses of the method as discussed in the reviews and after:    the quantitative results with weak supervision are not a big improvement over beta vae like methods or mathieu et al.   a red flag of sorts to me is that it is not very clear where the gains are coming from: the authors claim to have done a fair comparison with the various baselines, but they introduce an entirely new encoder/decoder architecture that was likely (involuntarily, but still) tuned more to their method than others.   the setup as presented is somewhat artificial and less general than it could be (however, this was not a major factor in my decision). It is easy to get confused by the kind of disentagled representations that this work is aiming to get.  I think this has the potential to be a solid paper, but at this stage it s missing a number of ablation studies to truly understand what sets it apart from the previous work. At the very least, there is a number of architectural and training choices in Appendix D   like the 0.25 dropout   that require more explanation / empirical understanding and how they generalize to other datasets.  Given all of this, at this point it is hard for me to recommend acceptance of this work. I encourage the authors to take all this feedback into account, extend their work to more domains (the artistic style disentangling that they mention seems like a good idea) and provide more empirical evidence about their architectural choices and their effect on the results.
The paper proposes Variational Beta Bernoulli Dropout,, a Bayesian method for sparsifying neural networks. The method adopts a spike and slab pior over parameter of the network. The paper proposes Beta hyperpriors over the network, motivated by the Indian Buffet Process, and propose a method for input conditional priors.  The paper is well written and the material is communicated clearly. The topic is also of interest to the community and might have important implications down the road.  The authors, however, failed to convince the reviewers that the paper is ready for publication at ICLR. The proposed method is very similar to earlier work. The reviewers think that the paper is not ready for publication.
The authors propose a method for distilling a student network from a teacher network and while additionally constraining the intermediate representations from the student to match those of the teacher, where the student has the same width, but less depth than the teacher. The main novelty of the work is to use the intermediate representation from the teacher as an input to the student network, and the experimental comparison of the approach against previous work.    The reviewers noted that the method is simple to implement, and the paper is clearly written and easy to follow. The reviewers raised some concerns, most notably that the authors were using validation accuracy to measure performance, and were thus potentially overfitting to the test data, and regarding the novelty of the work. Some of the criticisms were subsequently amended in the revised version where results were reported on a test set (the conclusions are as before).  Overall, the scores for this paper were close to the threshold for acceptance, and while it was a tough decision, the AC ultimately felt that the overall novelty of the work was slightly below the acceptance bar.
This paper proposes permutation invariant loss functions which depend on the distance of sets. This has an interesting interpretation as the roots of a polynomial, and potentially leads to a more efficient method.  It is not clear, however, whether the method works well in practice for multiple reasons: (i) the experiments are performed in a limited setting, and the rebuttal specifically declined to consider more realistic datasets, (ii) there is an open question about the stability of the resulting gradients, which has been pointed out both in the paper and the reviews.   There was initially a majority vote for rejection. After author response, the only reviewer recommending acceptance wrote "As the other reviews (and my original review) say, the experimental results are not totally convincing. So I would not champion the paper in its present form."
The paper proposes an original and interesting alternative to GANs for optimizing a (proxy to) Jensen Shannon divergence for discrete sequence data. Experimental results seem promising. Official reviewers were largely positive based on originality and results. However, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a "low variance, bias free algorithm" to optimize JS.  Given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue (review reposted on the current OpenReview thread on Nov. 6), the AC feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. The current version of the paper does neither. The algorithm is not unbiased for at least two reasons pointed out in discussions: a) in practice a parameterized mediator will be unable to match the true P+G, at best yielding a useful biased estimate (not unlike how GAN s parameterized discriminator induces bias). b) One would need to use REINFORCE (or similar) to get an unbiased estimate of the gradient in Eq. 13, a key detail omitted from the paper. From the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with Eq. 13 (they commented "most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this."). But rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the REINFORCE term thus effectively trading variance for bias.  On December 4th authors posted a justification confirming heuristically dropping the REINFORCE terms when taking the gradient of Eq. 13, and said they could attach detailed analysis and experiment results in the camera ready version.  However if one of the "most important idea" of the paper is how to avoid REINFORCE (as still implied and highlighted in the abstract), the AC finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by (unreportedly) dropping the term.   The approach remains original, interesting, and potentially promising, but as it currently stands, AC and SAC agreed that inexact theoretical over claiming and insufficient justification and in depth analysis of key heuristic shortcuts/tradeoffs (however useful) are too important for their fixing to be entrusted to a final camera ready revision step. A major revision that clearly adresses these issues in depth (both in how the approach is presented and in supporting experiments) will constitute a much more convincing, sound, and impactful research contribution.  
The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm.  Yet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation.  While R1 R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments.  The AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to "revise and resubmit" the paper.
This paper proposes to learn continuous of k mer embeddings for RNA seq analysis. Major concerns of the paper include: 1. novelty seems limited; 2. questions about the scalability of the approach; 3. evaluation experiments were not suitable for supporting the aim. Overall, this paper cannot be accepted yet. 
This paper proposes a method for improving robustness to black box adversarial attacks by replacing the cross entropy layer with an output vector encoding scheme. The paper is well written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns.
This paper proposes a method for learning neural RFs with the inclusive divergence minimization problem.  Reviewers generally agree that the experiments are sufficient and convincing, and that the method is evaluated well. Results are comparable with SOTA methods for image generation. The paper is reasonably well written.  The paper is also somewhat lacking in background; most people at ICLR will not be very familiar with this learning problem. More information on the inclusive divergence minimization problem would be helpful. A major concern of reviewers is whether novelty of the method is sufficient for publication. 
This paper analysis the convergence properties of a family of  Adam Type  optimization algorithms, such as Adam, Amsgrad and AdaGrad, in the non convex setting. The paper provides of the first comprehensive analyses of such algorithms in the non convex setting. In addition, the results can help practitioners with monitoring convergence in experiments. Since Adam is a widely used method, the results have a potentially large impact.  The reviewers agree that the paper is well written, provides interesting new insights, and that is results are of sufficient interest to the ICLR community to be worthy of publication.
I would like to commend the authors on their work engaging with the reviewers and for working to improve training time. However, there is not enough support among the reviewers to accept this submission. The reviewers raised several important points about the paper, but I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:  1. [premises] It has not been adequately established that "large batch training often times leads to degradation in accuracy" inherently which is an important premise of this work. Reports from the literature can largely be explained by other things in the experimental protocol. Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information. Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.  2. [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.  3. [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.  Additionally there are a variety of framing of issues around hyperparameter tuning, but, because they are easier to fix, they are not as salient for the decision.  
This paper proposes to combine RL and imitation learning, and the proposed approach seems convincing.    As is typical in RL work, the evaluation of the method is not strong enough to convince the reviewers.  Increasing community criticism on RL methods not scaling must be taken seriously here, despite the authors  disagreement. 
Strong paper in an interesting new direction. More work should be done in this area.
This paper considers the problem of learning symbolic representations from raw data. The reviewers are split on the importance of the paper. The main argument in favor of acceptance is that bridges neural and symbolic approaches in the reinforcement learning problem domain, whereas most previous work that have attempted to bridge this gap have been in inverse graphics or physical dynamics settings. Hence, it makes for a contribution that is relevant to the ICLR community. The main downside is that the paper does not provide particularly surprising insights, and could become much stronger with more complex experimental domains. It seems like the benefits slightly outweigh the weaknesses. Hence, I recommend accept.
Dear authors,  The reviewers all appreciated the question you are asking and the study of the impact of each layer is definitely an interesting one.  They were however uncertain about the actual metrics you used to emphasize your points. Further, as you noted, there were quite a few presentation issues that led to skepticism of the reviewers, despite them spending quite a bit of time reading the paper and engaging in discussion.  Hence, I regret to inform you that your work is not yet ready for publication. A more focused analysis would be a great addition to the questions you raise.
The paper receives a unanimous accept over reviewers, though some concerns on novelty exist. So it is suggested to be a probable accept. 
Dear authors,  There was some disagreement among reviewers on the significance of your results, in particular because of the limited experimental section.  Despite this issues, which is not minor, your work adds yet another piece of the generalization puzzle. However, I would encourage the authors to make sure they do not oversell their results, either in the title or in their text, for the final version.
This paper presents a mean field analysis of the effect of batch norm on optimization. Assuming the weights and biases are independent Gaussians (an assumption that s led to other interesting analysis), they propagate various statistics through the network, which lets them derive the maximum eigenvalue of the Fisher information matrix. This determines the maximum learning rate at which learning is stable. The finding is that batch norm allows larger learning rates.  In terms of novelty, the paper builds on the analysis of Karakida et al. (2018). The derivations are mostly mechanical, though there s probably still sufficient novelty.  Unfortunately, it s not clear what we learn at the end of the day. The maximum learning rate isn t very meaningful to analyze, since the learning rate is only meaningful relative to the scale of the weights and gradients, and the distance that needs to be moved to reach the optimum. The authors claim that a "higher learning rate leads to faster convergence", but this seems false, and at the very least would need more justification. It s well known that batch norm rescales the norm of the gradients inversely to the norm of the weights; hence, if the weight norm is larger than 1, BN will reduce the gradient norm and hence increase the maximum learning rate. But this isn t a very interesting effect from an optimization perspective. I can t tell from the analysis whether there s a more meaningful sense in which BN speeds up convergence. The condition number might be more relevant from a convergence perspective.  Overall, this paper is a promising start, but needs more work before it s ready for publication at ICLR.  
 pros:   The paper is well written and includes a lot of interesting connections to cog sci (though see specific clarity concerns)   The tasks considered (visual and symbolic) provide a nice opportunity to study analogy making in different settings.  cons:   There was some concerns about baselines and novelty that I think the authors have largely addressed in revision  This is an intriguing paper and an exciting direction and I think it merits acceptance.
AR1 is concerned about the overlap of this paper with Gama et al., 2018 as well as lack of theoretical analysis and poor results on REDDIT 5k and REDDIT 5B datasets. AR2 reflects the same concerns (lack of clear cut novelty over Zou & Lerman, 2018, Game, 2018. AR3 also points the same issue re. lack of theoretical results. The austhors admit that Zou and Lerman, 2018, and Gama, 2018, focus on stability results while this submission offers empirical evaluations.  Unfortunately, reviewers did not find these arguments convincing. Thus, at this point, the paper cannot be accepted for publication in ICLR. AC strongly encourages authors to develop their theoretical  edge  over this crowded market of GCNs and scattering approaches.
Both authors and reviewers agree that the ideas in the paper were not presented clearly enough.
The paper addresses sample efficient robust policy search borrowing ideas from active learning. The reviews raised important concerns regarding (1) the complexity of the proposed technique, which combines many separate pieces and (2) the significance of experimental results. The empirical setup adopted is not standard in RL, and a clear comparison against EPOpt is lacking. I appreciate the changes made to address the comment, and I encourage the authors to continue improving the paper by simplifying the model and including a few baseline comparisons in the experiments.
The paper proposes a novel approach to interfacing robots with humans, or rather vv: by mapping instructions to goals, and goals to robot actions.   A possibly nice idea, and possibly good for more efficient learning.  But the technical realisation is less strong than the initial idea.  The original idea merits a good evaluation, and the authors are strongly encouraged to follow up on this idea and realise it, towards a stronger publication.  It be noted that the authors refrained from using the rebuttal phase.
This paper attempts at modeling text matching and also generating rationales.  The motivation of the paper is good.  However there is some shortcomings of the paper, e.g. there is very little comparison with prior work, no human evaluation at scale and also it seems that several prior models that use attention mechanism would generate similar rationales.  No characterization of the last aspect has been made here.  Hence, addressing these issues could make the paper better for future venues.  There is relative consensus between the reviewers that the paper could improve if the reviewers  concerns are addressed when it is submitted to future venues.
this is an interesting approach to use reinforcement learning to replace CRF for sequence tagging, which would potentially be beneficial when the tag set is gigantic. unfortunately the conducted experiments do not really show this, which makes it difficult to see whether the proposed approach is indeed a viable alternative to CRF for sequence tagging with a large tag set. this sentiment was shared by all the reviewers, and R1 especially pointed out major and minor issues with the submission and was not convinced by the authors  response.
Paper develops a dataset and model for learning to refer to 3D objects. Reviewers raised concerns about lack of novelty. Fundamentally, it seems unclear what (if any) the take away for an ML audience would be after reading this paper. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future (perhaps a more applied) venue. 
This paper proposes a new image to image translation technique, presenting a theoretical extension of Wasserstein GANs to the bidirectional mapping case.   Although the work presents promise, the extent of miscommunication and errors of the original presentation was too great to confidently conclude about the contribution of this work.   The authors have already included extensive edits and comments in response to the reviews to improve the clarity of method, experiments and statement of contribution. We encourage the authors to further incorporate the suggestions and seek to clarify points of confusion from other reviewers and submit a revised version to a future conference.
This work proposes a simple but useful way to train RNN with binary / ternary weights for improving memory and power efficiency. The paper presented a sequence of experiments on various benchmarks and demonstrated significant improvement on memory size  with only minor decrease of accuracy. Authors  rebuttal addressed the reviewers  concern nicely.   
Connecting different fields and bringing new insights to machine learning are always appreciated. But since it is challenging to do it needs to be done well. This paper falls short here.  
Adversarial training has quickly become important for training robust neural networks.  However this training generally results in poor generalization behavior. This paper proposes using margin loss with adversarial training for better generalization. The paper provides generalization bounds for this adversarial training setup motivating the use of spectral regularization. The experimental results using the spectral regularization with adversarial training are very promising and all the reviewers agree that they show non trivial improvement. Even though the spectral regularization techniques have been tried in different settings, hence of limited novelty, the experimental results in the paper are encouraging and I believe will motivate further study on this topic. Reviewers also opined that the writing in the paper is currently not that great with limited explanation of the theoretical results. More discussions interpreting the theoretical results and their significance can help the readers appreciate the paper better.
This paper studies the role of pooling in the success underpinning CNNs. Through several experiments, the authors conclude that pooling is neither necessary nor sufficient to achieve deformation stability, and that its inductive bias can be mostly recovered after training.   All reviewers agreed that this is a paper asking an important question, and that it is well written and reproducible. On the other hand, they also agreed that, in its current form, this paper lacks a  punchline  that can drive further research. In words of R6, "the paper does not discuss the links between pooling and aliasing", or in words of R4, "it seems to very readily jump to unwarranted conclusions". In summary, the AC recommends rejection at this time, and encourages the authors to pursue the line of attack by exploring the suggestions of the reviewers and resubmit.  
This work proposes a class of neural networks that can jointly perform classification and regression in the output space. The authors explore the concept of polar prototypes which are points on the hypersphere in the output space. For classification, each class is described by a single polar prototype and training is equivalent to minimizing angular distances between examples and their class prototypes. For regression, training can be performed as a polar interpolation between two prototypes. As rightly acknowledged by R3, “it is nice to see an alternative to the dominant cross entropy loss and l2 loss for deep classification and regression respectively, also the ability to tackle both” at the same time.  However, all reviewers and AC agreed that the current manuscript lacks convincing empirical evaluations that clearly show the benefits of the proposed approach. To strengthen the evaluation, (1) see R1’s concern regarding the state of the art performance on CIFAR 10;  (2) see R3’s suggestion to use more challenging datasets (e.g. ImageNet), stronger backbone networks (e.g. densenet), and also other applications (e.g. object recognition and pose estimation; face recognition and age estimation as classification and regression problems); (3) see R2’s suggestions for more baselines to be compared to. Two other requests to further strengthen the manuscript are: (1) finding alternative ways to MC or evolutionary algorithms (R2); (2) exploring class correlation in the prototype space (R2).   In the response, the authors acknowledged that their initial results were not aimed for state of the art comparison, but to show that the proposed objective is comparable to minimizing softmax cross entropy loss. The authors provide additional experiments using DenseNet as the base network and the results are still slightly inferior to state of the art performance.  The experiments using ImageNet dataset have been promised by the authors (in response to R3), but are not included in the current revision.   AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.  
This paper presents empirical evaluation and comparison of different generative models (such as GANs and VAE) in the continual learning setting.  To avoid catastrophic forgetting, the following strategies are considered: rehearsal, regularization, generative replay and fine tuning. The empirical evaluations are carried out using three datasets (MNIST, Fashion MNIST and CIFAR).   While all reviewers and AC acknowledge the importance and potential usefulness of studying and comparing different generative models in continual learning, they raised several important concerns that place this paper bellow the acceptance bar: (1) in an empirical study paper, an in depth analysis and more insightful evaluations are required to better understand the benefits and shortcomings of the available models (R1 and R2), e.g. analyzing why generative replay fails to improve VAE, why is rehearsal better for likelihood models, and in general why certain combinations are more effective than others – see more suggestions in R1’s and R2’s comments. The authors discussed in their response to the reviews some of these questions, but a more detailed analysis is required to fully understand the benefits of this empirical study. (2) The evaluation is geared towards quality metrics for the generative models and lacks evaluation for catastrophic forgetting in continual learning (hence it favours GANs models)   See R3’s suggestion how to improve.   To conclude, the reviewers and AC suggest that in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This was a difficult decision to converge to. R2 strongly champions this work, R1 is strongly critical, and R3 did not participate in the discussions (or take a stand). On the one hand, the AC can sympathize with R1 s concerns   insights developed on synthetic datasets may fail to generalize and fundamentally, the burden is not on a reviewer to be able to provide to authors a realistic dataset for the paper to experiment on. Having said that, a carefully constructed synthetic dataset is often *exactly* what the community needs as the first step to studying a difficult problem. Moreover, it is better for a proceeding to include works that generate vigorous discussions than the routine bland incremental works that typically dominate. Welcome to ICLR19. 
I have to agree with the reviewers here and unfortunately recommend a rejection.  The methodology and task are not clear. Authors have reformulated QA in SQUAD as as ranking and never compared the results of the proposed model with other QA systems. If authors want to solve a pure ranking problem why they do not compare their methods with other ranking methods/datasets.
+ a simple method + producing diverse translation is an important problem     technical contribution is limited / work is incremental   R1 finds writing not precise and claims not supported,  also discussion of related work is considered weak by R3   claims of modeling uncertainty are not well supported   There is no consensus among reviewers.  R4 provides detailed arguments why (at the very least) certain aspects of presentations are misleading (e.g., claiming that a uniform prior promotes diversity). R1 is also negative, his main concerns are limited contribution and he also questions the task (from their perspective  producing diverse translation is not a valid task; I would disagree with this).  R2 likes the paper and believes it is interesting, simple to use and the paper should be accepted. R3 is more lukewarm.   
The authors take two algorithmic components that were proposed in the context of discrete action RL   priority replay and parameter noise   and evaluate them with DDPG on continuous control tasks. The different approaches are nicely summarized by the authors, however the contribution of the paper is extremely limited. There is no novelty in the proposed approaches, the empirical evaluation is inconclusive and limited, and there is no analysis or additional insights or results. The AC and the reviewers agree that this paper is not strong enough for ICLR.
This work builds on MAML by (1) switching from a single underlying set of parameters to a distribution in a latent lower dimensional space, and (2) conditioning the initial parameter of each subproblem on the input data. All reviewers agree that the solid experimental results are impressive, with careful ablation studies to show how conditional parameter generation and optimization in the lower dimensional space both contribute to the performance. While there were some initial concerns on clarity and experimental details, we feel the revised version has addressed those in a satisfying way.
This paper attempts to address a problem they dub "inverse" covariate shift where an improperly trained output layer can hamper learning. The idea is to use a form of curriculum learning. The reviewers found that the notion of inverse covariate shift was not formally or empirically well defined. Furthermore the baselines used were too weak: the authors should consider comparing against state of the art curriculum learning methods.
This work introduces a reward shaping scheme for multi agent settings based on the TD error of other agents.   Overall, reviewers were positive about the direction and the presentation but had a variety of concerns and questions and felt more experiments were necessary to validate the claims of flexibility and scalability, with results more comparable to the scale of the contemporary multi agent literature. One note in particular: a feed forward Q network is used in a partially observable environment, which the authors seemed to dismiss in their rebuttal. I agree with the reviewer that this is an important consideration when comparing to baselines which were developed with recurrent networks in mind.  A revised manuscript addressed concerns with the presentation but did not introduce new results or plots, and reviewers were not convinced to alter their evaluation. There is agreement that this is an interesting paper, so I recommend that the authors conduct a more thorough empirical evaluation and submit to another venue.
 + An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low dimensional space + As the space is low dimensional (2D), it can be directly visualized.  + I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t SNE plots of high dimensional embeddings + Though not the first method to embed words as densities but seemingly the first one which shows that multi modality  / multiple senses are captured (except for models which capture discrete senses) + The paper is very well written     The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset)    The approach is not very scalable (hence evaluation on 17M corpus)    The method cannot be used to deal with data sparsity, though (very) interesting for visualization    This is mostly an empirical paper (i.e. an interesting application of an existing method)  The reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting.      
The reviewers and authors had a productive conversation, leading to an improvement in the paper quality. The strengths of the paper highlighted by reviewers are a novel learning set up and new loss functions that seem to help in the task of protein contact prediction and protein structural similarity prediction. The reviewers characterize the work as constituting an advance in an exciting application space, as well as containing a new configuration of methods to address the problem.  Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse. The resulting model is similar to R2P2, i.e. it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance. Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation. 
The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights. They show that various Bayesian deep learning algorithms tend to converge to layers of this variety. This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods. 
This paper proposes an amortized proximal optimization method to adapt optimization hyperparameters. Empirical results on many problems are performed.   Reviewers overall find the ideas interesting, however there still are some questions whether strong baselines are used in the experimental comparisons. The reviewers also point that the theoretical results are not useful ones since the assumptions are not satisfied in practice. One of the reviewer increased their score, but the other has maintained that the paper requires more work.  The presentation of the result is also a bit problematic; the font sizes in the figure are too small to read.  The paper contains interesting ideas, but it does not make the bar for acceptance in ICLR. Therefore I recommend a reject. I encourage the authors to resubmit this work after improving the presentation and experiments. 
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
Two out of three reviews for this paper were provided in detail, but all three reviewers agreed unanimously that this paper is below the acceptance bar for ICLR. The reviewers admired the clarity of writing, and appreciated the importance of the application, but none recommended the paper for acceptance due largely to concerns on the experimental setup.
This manuscript proposes a generative model for images, then proposes a training procedure for fitting a convolutional neural network based on this model. One novelty if this result is that the generative procedure seems to be more complex than generative assumptions required for previous work. It is clear that the problem addressed   training methods that may improve on SGD, with convergence guarantees   is of significant interest to the community.  The reviewers and AC note several issue (i) the initial version of the manuscript includes several assumptions that are not clearly stated. This seems to have been fixed in the updated manuscript (ii) reviewers suspect that the accumulation of stated assumptions may result in an easily separable generative model   limiting the generality of the results (iii) experiemental results are underwhelming, and only comparable to much older published results.
The authors propose an approach for visual navigation that leverages a semantic knowledge graph to ground and inform the policy of an RL agent. The agent uses a graphnet to learn relationships and support the navigation. The empirical protocol is sound and uses best practices, and the authors have added additional experiments during the revision period, in response to the reviewers  requests. However, there were some significant problems with the submission   there were no comparisons to other semantic navigation methods, the approach is somewhat convoluted and will not survive the test of time, and the authors did not conclusively show the value of their approach. The reviewers uniformly support the publication of this paper, but with a low confidence. 
The paper received mixed and divergent reviews. As a paper of unusual topic in ICLR, the presentation of this work would need improvement. For example, it is difficult to understand what s the overall objective function, why a specific design choice was made, etc. It s nice to see that the authors somehow did quite a bit of engineering to make their model work for classification and drawing tasks, but (as an ML person) it’s difficult to get a clear rationale on why the method works other than that it’s biologically motivated. In addition, the proposed model (at a functional level) looks quite similar to Mnih et al. s "Recurrent Models of Visual Attention" work (for classification) and Gregor et al s DRAW model (for generation) in that all these models use sequential/recurrent attention/glimpse mechanisms, but no direct comparisons are made. For classification, the method achieves strong performance on MNIST but this may be due to a better architecture choice compared to Mnih s model but not due to the difference of the memory mechanism. For image generation/reconstruction, the proposed method seems to achieve quite good results but they are not as good as those from DRAW method. Overall, the paper is on the borderline, and while this work has some merits and might be of interest to some subset of audience in ICLR, there are many issues to be addressed in terms of presentation and comparisons. Please see reviews for other detailed comments.
This work presents extensions of dialogue systems to simultaneously capture speakers  "personas" (in the framing of Li et al s work) and adapt to them. While the ideas are interesting, reviewers note that the incremental contribution compared to previous work is a bit too limited for ICLR s expectation, without being offset by strongly convincing experimental results. Authors are encouraged to incorporated their ideas into future submissions after having combined them with other insights to provide a stronger overall contribution.
Pros:   novel method for continual learning   clear, well written   good results   no need for identified tasks   detailed rebuttal, new results in revision  Cons:   experiments could be on more realistic/challenging domains  The reviewers agree that the paper should be accepted.
This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto encoder framework. Significant gains are found through semi supervised learning.  The largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over stating the perceived utility.  Overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.
The paper examined the folk knowledge that there are highly selective units in popular CNN architectures, and performed a detailed analysis of recent measures of unit selectivity, as well as introducing a novel one. The finding that units are not extremely selective in CNNs was intriguing to some (not all) reviewers. Further, they show recent measures of selectivity dramatically over estimate selectivity.  There was not tight agreement amongst the reviewers on the paper s rating, but it trended towards rejection. Weaknesses highlighted by reviewers include lack of visual clarity in their demonstrations, the use of a several generations old CNN architecture, as well as a lack of enthusiasm for the findings.
In this work, a central idea introduced by CycleGAN is extended from 2D convolutions to 3D convolutions to ensure better consistency of style transfer across time. Authors demonstrate improvements on a variety of datasets in comparison to frame by frame style transfer.   Reviewer Pros: + Seems to be effective at enforcing improved consistency over time + Proposed medical dataset may be good contribution to community.  + Good quality evaluation  Reviewer Cons:   All reviewers felt the technical novelty was low.   Some questions arose around quantitative results, left unanswered by authors.   Experiments missing some baseline approaches   Architecture limited to fixed length video segments  Reviewer consensus is to reject. Authors are encouraged to continue their work and take into account suggestions made by reviewers, including adding additional comparison baselines 
The paper presents a novel with compelling experiments. Good paper, accept.  
Reviewers had several concerns about the paper, primary among them being limited novelty of the approach. The reviewers have offered suggestions for improving the work which we encourage the authors to read and consider.
There is no author response for this paper. The paper formulates a definition of easy and hard examples for training a neural network (NN) in terms of their frequency of being classified correctly over several repeats. One repeat corresponds to training the NN from scratch. Top 10% and bottom 10%  of the samples with the highest and the lowest frequency define easy and hard instances for training. The authors also compare easy and hard examples across different architectures of NNs. On the positive side, all the reviewers acknowledge the potential usefulness of quantifying easy and hard examples in training NNs, and R1 was ready to improve his/her initial rating if the authors revisited the paper.   On the other hand, all the reviewers and AC agreed that the paper requires (1) major improvement in presentation clarity   see detailed comments of R1 on how to improve as well as comments/questions from R3 and R2; try to avoid  confusing terminology such as ‘contradicted patterns’. R1 raised important concerns that the proposed notion of easiness is drawn from the experiment in Fig. 1 of Arpit et al (2017) which is not properly attributed. R3 and R2 agreed that in its current state the experimental results are not conclusive and often non informative. To strengthen the paper the reviewers suggested to include more experiments in terms of different datasets, to propose a better metric for defining easy and hard samples (see R3’s suggestions). We hope the reviews are useful for improving the paper. 
The paper proposes a new method to approximate the nonlinear value function by estimating it as a sum of linear and nonlinear terms. The nonlinear term is updated much slower than the linear term, and the paper proposes to use a  fast least square algorithm to update the linear term. Convergence results are also discussed and empirical evidence is provided.   As reviewers have pointed out, the novelty of the paper is limited, but the ideas are interesting and could be useful for the community. I strongly recommend taking reviewers comments into account for the camera ready and also add a discussion on the relationship with the existing work.   Overall, I think this paper is interesting and I recommend acceptance. 
Pros:   good results on Montezuma  Cons:   moderate novelty   questionable generalization   lack of ablations and analysis   lack of stronger baselines   no rebuttal   The reviewers agree that the paper should be rejected in its current form, and the authors have not bothered revising it to take into account the detailed reviews.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.     The method and justification are clear   The quantitative results are promising.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The contribution is minor   Analysis of the properties of the method is lacking. The first point was the major factor in the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Reviewer opinion was quite divergent but both AR1 and AR2 had concerns about the 2 weaknesses mentioned in the previous section (which remained after the author rebuttal).   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  No consensus was reached. The source of disagreement was on how to weigh the pros vs the cons. The final decision was aligned with the lower ratings. The AC agrees that the contribution is minor. 
Strengths:      Solid experiments    The paper is well written  Weaknesses:    The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already  suggested that LM objectives are preferable and also using LM objective for pretraining is already the  standard practice (see details in R1 and R3).   There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.  
The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods. The authors provide theoretical guarantees and a broad set of experiments.   In the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches. The authors mention that the lines are too thick to reveal the difference. It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods. AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side.   The comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion. Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence.   The referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold.    Although the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time. 
This paper presents an heuristic method to detect periodicity in a time series such that it can handle noise and multiple periods.   All reviewers agreed that this paper falls off the scope of ICLR since it does not discuss any learning related question. Moreover, the authors did not provide any response nor updated manuscript addressing the reviewers remarks. The AC thus recommends rejection. 
The paper presents a new approach for domain generalization whereby the original supervised model is trained with an explicit objective to ignore so called superficial statistics present in the training set but which may not be present in future test sets. The paper proposes using a differentiable variant of gray level co occurrence matrix to capture the textural information and then experiments with two techniques for learning feature invariance. All reviewers agree the approach is novel, unique, and potentially high impact to the community.   The main issues center around reproducibility as well as the intended scope of problems this approach addresses. The authors have offered to include further discussions in the final version to address these points. Doing so will strengthen the paper and aid the community in building upon this work. 
The paper proposes a simple method for improving the sample efficiency of GAIL, essentially a way of turning inverse reinforcement learning into classification. As reviewers noted, the method is based on a simple idea with potentially broad applicability.  Concerns were raised about the multiple components of the system and what each contributed, and missing pointers to the literature. A baseline wherein GAIL is initialized with behaviour cloning, although only suggested but not tried in previous works. The authors did, however, attempt this setting and found it to hurt, not help, performance. I find this surprising and would urge the authors to validate that this isn t merely an uninteresting artifact of the setup, however I commend the authors for trying it and don t believe that a surprising result in this regard is a barrier to publication.  As several reviewers did not provide feedback on revisions addressing their concerns, this Area Chair was left to determine to a large degree whether or not reviewer concerns were in fact addressed.  I thank AnonReviewer4 for revisiting their review towards the end of the period, and concur with them that many of the concerns raised by reviewers have indeed been adequately dealt with.  
This paper proposes a new scoring function for link prediction model that is based on a generative model for the knowledge graph, based on a random walk model previously used for word embeddings. The new scoring function, as it is accompanied by the generative model, provides interesting theoretical results that the reviewers also appreciate. Finally, the results are quite strong, as they obtain state of art on the primary benchmarks for the task.  Based on the submitted version, the reviewers and AC note the following potential weaknesses: (1) the reviewers felt that the proposed work is a direct application of the random walk model from Arora et al. and thus limited in novelty, (2) given the generative model, the reviewers felt that the paper would benefit from an analysis of the learned embeddings, and their difference from ones from existing approaches, (3) The reviewers noted that the authors were using an incorrect version of FB15k and WN18, (4) the authors were not providing results for all the metrics, (5) the coverage of related work is quite limited.  The authors addressed many of the concerns raised by the reviewers in their comments and revision, in particular, they obtained state of art results for the corrected versions of the benchmarks. Further, they clarified the assumptions made in their modeling and revised the related work to include the papers that the reviewers mentioned. However, the concerns regarding the lack of novelty of the proposed approach, w.r.t Arora et al 2016 and the need for further analysis of the learned embeddings, still remain.  This paper comes really close to getting accepted, but ultimately the reviewers agree that the remaining concerns need to be addressed.
This is a very clearly written, well composed paper that does a good job of placing the proposed contribution in the scope of hyperparameter optimization techniques.  This paper certainly appears to have been improved over the version submitted to the previous ICLR.  In particular, the writing is much clearer and easy to follow and the methodology and experiments have been improved.  The ideas are well motivated and it s exciting to see that sampling from a k DPP can give better low discrepancy sequences than e.g. Sobol.  However,  the reviewers still seem to have two major concerns, namely novelty of the approach (DPPs have been used for Bayesian optimization before) and the empirical evaluation.    Empirical evaluation:  As Reviewer1 notes, there are much more recent approaches for Bayesian optimization that have improved significantly over the TPE method, also for conditional parameters.  There are also more recent approaches proposing variants of random search such as hyperband.    Novelty:  There is some work on using determinantal point processes for Bayesian optimization and related work in optimal experimental design.  Optimal design has a significant amount of literature dedicated to designing a set of experiments according to the determinant of their covariance matrix   i.e. D Optimal Design.  This work may add some interesting contributions to that literature, including fast sampling from k DPPs, etc.  It would be useful, however, to add some discussion of that literature in the paper.  Jegelka and Sra s tutorial at NeurIPS on negative dependence had a nice overview of some of this literature.  Unfortunately, two of the three reviewers thought the paper was just below the borderline and none of the reviewers were willing to champion it.  There are very promising and interesting ideas in the paper, however, that have a lot of potential.  In the opinion of the AC, one of the most powerful aspects of DPPs over e.g. low discrepancy sequences, random search, etc.  is the ability to learn a distance over a space under which samples will be diverse.  This can make a search *much* more efficient since (as the authors note when discussing random search vs. grid search) the DPP can sample more densely in areas and dimensions that have higher sensitivity.  It would be exciting to learn kernels specifically for hyperparameter optimization problems (e.g. a kernel specifically for learning rates that can capture e.g. logarithmic scaling).  Taking the objective into account through the quality score, as proposed for future work, also seems very sensible and could significantly improve results as well.  
Strengths:  The proposed method is relatively principled.  The paper also demonstrates a new ability: training VAEs with autoregressive decoders that have meaningful latents.  The paper is clear and easy to read.  Weaknesses:  I wasn t entirely convinced by the causal/anticausal formulation, and it s a bit unfortunate that the decoder couldn t have been copied without modification from another paper.  Points of contention: It s not clear how general the proposed approach is, or how important the causal/anti causal idea was, although the authors added an ablation study to check this last question.  Consensus:  All reviewers rated the paper above the bar, and the objections of the two 6 s seem to have been satisfactorily addressed by the rebuttal and paper update.
The reviewers all appreciate the idea, and the competitive performance, however the consensus is that this is a simple extension of the work of Han et al. and therefore the current submission contains little novelty. There are also numerous issues regarding clarity that the reviewers have pointed out. It is unfortunate that the authors have not engaged in discussion with the reviewers to resolve these, however they are encouraged to consider the reviewer feedback in order to improve the paper.
The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow.  The work is well written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables. The reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger scale real world data and model would provide a more complete and convincing evidence.  The paper should be made stronger on at least one (and ideally both) of these accounts.  
Reviewers agree the paper should be accepted. See reviews below.
The paper received unanimous accept over reviewers (7,7,6), hence proposed as definite accept. 
Multiple reviewers had concerns about the clarity of the presentation and the significance of the results. 
This paper presents an empirical study of the applicability of genetic algorithms to deep RL problems. Major concerns of the paper include: 1. paper organization, especially the presentation of the results, is hard to follow; 2. the results are not strong enough to support that claims made in this paper, as GAs are currently not strong enough when compared to the SOTA RL algorithms; 3. Not quite clear why or when GAs are better than RL or ES; Lack of insights. Overall, this paper cannot be accepted yet.  
This paper studies the notion of certified cost sensitive robustness against adversarial examples, by building from the recent [Wong & Koller 18]. Its main contribution is to adapt the robust classification objective to a  cost sensitive  objective, that weights labelling errors according to their potential damage.  This paper received mixed reviews, with a clear champion and two skeptical reviewers. On the one hand, they all highlighted the clarity of the presentation and the relevance of the topic as strengths; on the other hand, they noted the relatively little novelty of the paper relative [W & K 18]. Reviewers also acknowledged the diligence of authors during the response phase. The AC mostly agrees with these assessments, and taking them all into consideration, he/she concludes that the potential practical benefits of cost sensitive certified robustness outweight the limited scientific novelty. Therefore, he recommends acceptance as a poster. 
The paper proposes an approach to learn policies that can effectively transfer to new environments. The perspective on this problem from the perspective of streaming submodular optimization is nice; the paper introduces new ideas that are likely of interest to the ICLR community. Unfortunately, there are significant concerns about how convincing the results are. Multiple reviewers were concerned about there only being two experiments, and the lack of comparison to ep opt on the half cheetah experiment. Without a more solid empirical validation of the ideas, the paper does not meet the bar for publication at ICLR.
The paper describes a constrained optimization strategy for optimizing on an intersection of two manifolds.  Unfortunately, the paper suffers from generally weak presentation quality, with the technical exposition seriously criticized by two out of the three reviewers.  (The single positive review is too short and devoid of content to be taken seriously.  Even there, concerns are expressed.) This paper requires substantial improvement before it could be considered for publication.
The paper proposes to use Anderson Mixing to accelerate value iteration and DQN.  The idea is interesting, with some theoretical and empirical support.  However, reviewers feel that the contribution is somewhat limited, and certain parts (e.g., the DP view) can be further developed to strengthen the technical contribution.  Furthermore, one reviewer points out that the empirical results are not very strong, where the improvements on 3 Atari games are not very substantial.  Overall, while the paper is interesting and does have the potential, it seems too preliminary to be published in its current form.  Minor comments: 1. The paper is partially motivated by the claim given at the beginning of section 3: "Based on the observation that full policy evaluation accelerates convergence, ..."  Can a reference be given?  2. Another way to look at Anderson Mixing is the standard linear value function approximation framework, where the previous K value functions serve as basis functions.  See Mahadevan & Maggioni (JMLR 07), Parr et al. (ICML 08) and Konidaris et al. (AAAI 11) for a few examples of constructing basis functions; the approach here seems to provide another way to automatically construct basic functions.  A discussion would be helpful.
The paper received borderline ratings due to concerns regarding novelty and experimental results/settings (e.g. zero shot learning). On my side, I believe that the proposed method would need more evaluations on other benchmarks (e.g., SUN, AWA1 and AWA2) for both ZSL and GZSL settings to make the results more convincing. Overall, none of the reviewers championed this paper and I would recommend weak rejection.
This paper presents an extension of an existing topic model, DocNADE. Compared to DocNADE and other existing bag of word topic models, the primary contribution of this work is to integrate neural language models into the topic model in order to address two limitations of the bag of word topic models: expressiveness and interpretability. In addtion, the paper presents an approach to integrate external knowledge into the neural topic models to address the empirical challenges of the application scenarios where there might be only a small training corpus or limited context available.   Pros:  The paper presents strong and extensive empirical results. The authors went above and beyond to strengthen their paper during the rebuttal and address all the reviewers  questions and suggestions (e.g., the submitted version had 7 baselines, and the revised version has 6 additional baselines per reviewers  requests).  Cons: The paper builds on an earlier paper that introduced the DocNADE model. Thus, the modeling contribution is relatively marginal. On the other hand, the extended model, albeit based on a relatively simple idea, is still new and demonstrates strong empirical results.  Verdict: Probably accept. While not groundbreaking, the proposed model is new and the empirical results are strong. 
Pros:   new multi objective approach to IRL   new algorithm   strong results   real world dataset  Cons:   straightforward theoretical extensions   unclear motivation   inappropriate empirical assessment metrics   weak rebuttal  All the reviewers feel that the paper needs further improvements, and while the authors comment on some of these concerns, their rebuttal and revised paper does not address them sufficiently. So at this stage it is a (borderline) reject.
The paper presents a simple and effective convolution kernel for CNNs on spherical data (convolution by a linear combination of differential operators). The proposed method is efficient in the number of parameters and achieves strong classification and segmentation performance in several benchmarks. The paper is generally well written but the authors should clarify the details and address reviewer comments (for example, clarity/notations of equations) in the revision.  
This paper proposes an effective method to train neural networks with quantized reduced precision. It s fairly straight forward idea and achieved good results and solid empirical work. reviewers have a consensus on acceptance. 
This paper analyses the dynamics of RNNs, cq GRU and LSTM.    The paper is mostly experimental w.r.t. the difficulty of training RNNs; this is also caused by the fact that the theoretical foundations of the paper seem not to be solid enough.  Experimentation with CIFAR10 is not completely stable.  The review results make the paper balance at the middle.  The merit of the paper for the greater community is doubted, in its current form.
The authors conduct experiments to study orientation selectivity in neural networks.   The reviewers generally agreed that the paper was clearly written and easy to follow. Further, the experimental analysis demonstrates that contrary to what was claimed in some previous work, the learned orientation selectivity can be useful for generalization.   However, the reviewers also raised a number of concerns: 1) that the conclusions are drawn on the basis of a couple of neural network architectures; the authors attempted to add results using a Resnet50 model, but this analysis was ultimately removed when the authors discovered a bug; 2) in the context of the contributions in neuroscience it was not clear that the limited results on the two artificial networks are sufficient to help draw such conclusions, and that 3) since the network is trained to recognize objects, it would seem natural that the model would learn neurons that are sensitive to orientation and that it is not clear how the author’s observations might lead to better trained models.  While the reviewers were not completely unanimous in their scores, the AC agrees with a majority of the reviewers that the work while interesting could be strengthened by additional experiments on other architectures.  
The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. However, because the ICLR call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well written and executed, I would recommend it for acceptance.
The paper addresses questions on the relationship between model free and model based reinforcement learning, in particular focusing on planning using learned generative models. The proposed approach, GATS, uses learned generative models for rollouts in MCTS, and provide theoretical insights that show a favorable bias variance tradeoff. Despite this theoretical advantage, and high quality models, the proposed approach fails to perform well empirically. This surprising negative results motivates the paper and providing insights on it is the main contribution.  Based on the initial submitted version, the reviewers positively emphasized the need to understand and publish important negative results. All reviewers and the AC appreciate the import role that such a contribution can bring to the research community. Reviewers also note the careful discussion of modeling choices for the generative models.   The reviewers also noted several potential weaknesses. Central were the need to better motivate and investigate the hypothesis proposed to explain the negative results. Several avenues towards a better understanding were proposed, and many of these were picked up by the authors in the revision and rebuttal. A novel toy domain "goldfish and gold bucket" was introduced for empirical analysis, and experiments there show that GATS can outperform DQN when a longer planning horizon is used.   The introduced toy domain provides additional insights into the relationship between planning horizon and GATS / MCTS performance. However, it does not address key questions around why the negative result is maintained. The authors hypothesize that the Q value is less accurate in the GATS setting   this is something that can be empirically evaluated, but specific evidence for this hypothesis is not clearly shown. Other forms of analysis that could shed further light on why the specific negative result occurs could be to inspect model errors. For example, if generated frames are sorted by the magnitude of prediction errors   what are the largest mistakes? Could these cause learning performance to deteriorate?  The reviewers also raised several issues around the theoretical analysis, clarity (especially of captions) and structure   these were largely addressed by the revision. The concern that most strongly affected the final evaluation is the limited insight (and evidence) of the factors that influence performance of the proposed approach. Due to this, the consensus is to not accept the paper for publication at ICLR at this stage.
This paper presents a novel idea of transferring gradients between tasks to improve multi task learning in neural network models. The write up includes experiments with multi task experiments with text classification and sequence labeling, as well as multi domain experiments. After the reviews, there are still some open questions in the reviewer comments, hence the reviewer decisions were not updated. For example, the impact of sequential update in pairwise task communication on performance can be analyzed. Two reviewers question task relatedness and the impact of how and when it is computed could be good to include in the work. Baselines could be improved to reflect reviewer suggestions.
The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming "inner ensembles".  Reviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed). One reviewer was unconvinced without direct comparison to full ensembles. Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013). Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method.  The AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited.  The AC also concurs that a full ensemble baseline would strengthen the paper s claims. In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time.
The paper introduces a modification of batch normalization technique. In contrast to the original batch normalization that normalizes minibatch examples using their mean and standard deviation, this modification uses weighted average  of mean and standard deviation from the current and all previous minibatches. The authors then provide some theoretical justification for the superiority of their variant of BatchNorm.  Unfortunately, the empirical demonstration of the improved performance seems not sufficient and thus fairly unconvincing. 
The reviewers all argued for acceptance citing the novelty and potential of the work as strengths.  They all found the experiments a little underwhelming and asked for more exciting empirical evaluation.  The authors have addressed this somewhat by including multi modal experiments in the discussion period.  The paper would be more impactful if the authors could demonstrate significant improvements on really challenging problems where MCMC is currently prohibitively expensive, such as improving over HMC for highly parameterized deep neural networks.  Overall, however, this is a very nice paper and warrants acceptance to the conference.
This paper proposes a new measure to quantify the contribution of an individual neuron within a deep neural network. Interpretability and better understanding of the inner workings of neural networks are important questions, and all reviewers agree that this work is contributing an interesting approach and results.
There is a clear reviewer consensus to reject this paper so I am also recommending rejecting it. The paper is about an interesting and underused technique. However, ultimately the issue here is that the paper does not do a good enough job of explaining the contribution. I hope the reviews have given the authors some ideas on how to frame and sell this work better in the future.  For instance, from my own reading of the abstract, I do not understand what this paper is trying to do and why it is valuable. Phrases such as "we exploit the sparsity" do not tell me why the paper is important to read or what it accomplishes, only how it accomplishes the seemingly elided contribution. I am forced to make assumptions that might not be correct about the goals and motivation. It is certainly true that the implicit one hot representation of words most common in neural language models is not the only possibility and that random sparse vectors for words will also work reasonably well. I have even tried techniques like this myself, personally, in language modeling experiments and I believe others have as well, although I do not have a nice reference close to hand (some of the various Mikolov models use random hashing of n grams and I believe related ideas are common in the maxent LM literature and elsewhere). So when the abstract says things like "We show that guaranteeing approximately equidistant vector representations for unique discrete inputs is enough to provide the neural network model with enough information to learn" my immediate reaction is to ask why this would be surprising or why it would matter. Based on the reviews, I believe these sorts of issues affect other parts of the manuscript as well. There needs to be a sharper argument that either presents a problem and its solution or presents a scientific question and its answer. In the first case, the problem should be well motivated and in the second case the question should not yet have been adequately answered by previous work and should be non obvious. I should not have to read beyond the abstract to understand the accomplishments of this work.  Moving to the conclusion and future work section, I can see the appeal of the future work in the second paragraph, but this work has not been done. The first paragraph is about how it is possible to use random projections to represent words, which is not something I think most researchers would question. Missing is a clear demonstration of the potential advantages of doing so. 
The paper according to Reviewers needs more work for publication and significantly more clarifications. The Reviewers are not convinced on publishing even after intensive discussion that the AC read in full. The AC recommends further improvements on the paper to address better Reviewer s concerns.
The submission evaluates maximum mean discrepancy estimators for post selection inference. It combines two contributions: (i) it proposes an incomplete u statistic estimator for MMD, (ii) it evaluates this and existing estimators in a post selection inference setting.  The method extends the post selection inference approach of (Lee et al. 2016) to the current u statistic approach for MMD.  The top k selection problem is phrased as a linear constraint reducing it to the problem of Lee et al.  The approach is illustrated on toy examples and a GAN application.  The main criticism of the problem is the novelty of the paper.  R1 feels that it is largely just the combination of two known approaches (although it appears that the incomplete estimator is key), while R3 was significantly more impressed.  Both are senior experts in the topic.  On the balance, the reviewers were more positive than negative.  R2 felt that the authors comments helped to address their concerns, while R3 gave detailed arguments in favor of the submission and championed the paper.  The paper provides an additional interesting framework for evaluation of estimators, and considers their application in a broader context of post selection inference.
A deep neural network pipeline for multiview stereo is presented. After rebuttal and discussion, all reviewers learn toward accepting the paper. Reviewer3 points to good results, but is concerned that the technical aspects are somewhat straightforward, and thus the contribution in this area is limited. The AC concurs with the reviewers.
The reviewers that provided extensive and technically well justified reviews agreed that the paper is of high quality. The authors are encouraged to make sure all concerns of these reviewers are properly addressed in the paper.
The presented method uses mode connectivity to help illustrate the surfaces of parameter space between various selections of models (either through changes of parameters, learning methods, or epochs), and canonical correlation analysis (CCA) to visualize the similarity of model layers across two different selected models.  These analyses are then used to study 3 forms of learning heuristics: stochastic gradient descent with restart (SGDR), warmup, and distillation.   Reviews tend to be leaning toward acceptance.   Pros: + R1: Well written + R1: Papers that analyze learning strategies are generally informative to the larger community. These experiments haven t been previously performed. + R1: Thorough experiments + R3: Results brought into context of prior hypotheses  Cons:   R3: Batch normalization not studied, but authors have added experiments in response.   R3 & R2: Practical implications not clear, but authors have added a discussion.  
This paper studies the roots of the existence of adversarial perspective from a new perspective. This perspective is quite interesting and thought provoking. However, some of the contributions rely on fairly restrictive assumptions and/or are not properly evaluated.   Still, overall, this paper should be a valuable addition to the program. 
This paper propose a novel CNN architecture for learning multi scale feature representations with good tradeoffs between speed and accuracy. reviewers generally arrived at a consensus on accept.
The paper studies the convergence of a primal dual algorithm on a special min max problem in WGAN where the maximization is with respect to linear variables (linear discriminator) and minimization is over non convex generators. Experiments with both simulated and real world data are conducted to show that the algorithm works for WGANs and multi task learning.  The major concern of reviewers lies in that the linear discriminator assumption in WGAN is too restrictive to general non convex mini max saddle point problem in GANs. Linear discriminator implies that the maximization part in min max problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a non convex optimization instance and proves its first order convergence with descent lemma. This technique however can’t be applied to general non convex saddle point problem in GANs. Also the experimental studies are also not strong enough. Therefore, current version of the paper is proposed as borderline lean reject.  
The paper presents the combination of a model based (probabilistic program representing the physics) and model free (CNN trained with DQN) to play Flappy Bird.  The approach is interesting, but the paper is hard to follow at times, and the solution seems too specific to the Flappy Bird game. This feels more like a tech report on what was done to get this score on Flappy Bird, than a scientific paper with good comparisons on this environment (in terms of models, algorithms, approaches), and/or other environments to evaluate the method. We encourage the authors to do this additional work.
Pros:   a method that obtains convergence results using a using time dependent (not fixed or state dependent) softmax temperature.  Cons:   theoretical contribution is not very novel   some theoretical results are dubious   mismatch of Boltzmann updates and epsilon greedy exploration   the authors seem to have intended to upload a revised version of the paper, but unfortunately, they changed only title and abstract, not the pdf   and consequently the reviewers did not change their scores.  The reviewers agree that the paper should be rejected in the submitted form.
Dear authors,  All reviewers agreed that, while the problem considered was of interest, the theoretical result presented in this work was of too limited scope to be of interest for the ICLR audience.  Based on their comments, you might want to consider a more theoretically oriented venue for such a submission.
Authors present a technique to learn embeddings over physiological signals independently using univariate LSTMs tasked to predict future values. Supervised methods are them employed over these embeddings. Univariate approach is taken to improve transferability across institutions, and Shapley values are used to provide interpretable insight. The work is interesting, and authors have made a good attempt at answering reviewers  concerns, but more work remains to be done.  Pros:   R1 & R3: Well written.   R3: Transferrable embeddings are useful in this domain, and not often researched.  Cons:    R3: Method builds embeddings that assume that future task will be relevant to drops in signals. Authors confirm.   R3: Performance improvement is marginal versus baselines. Authors essentially confirm that the small improvement is the accurate number.   R2 & R3: Interpretability evaluation is not sufficient. Medical expert should rate interpretability of results. Authors did not include or revise according to suggestion.  
This paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive soft max inference scales linearly with the vocabulary size. The proposed approach is based on a clustering step combined with per cluster, smaller soft maxes. It retains differentiability with the Gumbel softmax trick. The experimental results are impressive. There are some minor flaws, however there s consensus among the reviewers the paper should be published. 
The authors present a learnt scheduling mechanism for managing communications in bandwidth constrained, contentious multi agent RL domains. This is well positioned in the rapidly advancing field of MARL and the contribution of the paper is both novel, interesting, and effective. The agents learn how to schedule themselves, how to encode messages, and how to select actions. The approach is evaluated against several other methods and achieves a good performance increase. The reviewers had concerns regarding the difficulty of evaluating the overall performance and also about how it would fare in more real world scenarios, but all agree that this paper should be accepted.
This paper shows how to implement a low rank version of the Adagrad preconditioner in a GPU friendly manner. A theoretical analysis of a "hard window" version of the proposed algorithm demonstrates that it is not worse than SGD at finding a first order stationary point in the nonconvex setting. Experiments on CIFAR 10 classification using a ConvNet and Penn Treebank character level language modeling using an LSTM show that the proposed algorithm improves training loss faster than SGD, Adagrad, and Adam (measuring time in epochs) and has better generalization performance on the language modeling task. However, if wall clock time is used to measure time, there is no speedup for the ConvNet model, but there is for the recurrent model. The reviewers liked the simplicity of the approach and greatly appreciated the elegant visualization of the eigenspectrum in Figure 4. But, even after discussion, critical concerns remained about the need for more focus on the practical tradeoffs between per iteration improvement and per second improvement in the loss and the need for a more careful analysis of the relationship of this method to stochastic L BFGS. A more minor concern is that the term "full matrix regularization" seems somewhat deceptive when the actual regularization is low rank. The AC also suggests that, if the authors plan to revise this paper and submit it to another venue, they consider the relationship between GGT and the various stochastic natural gradient optimization algorithms in the literature that differ from GGT primarily in the exponent on the Gram matrix.
This paper considers an important problem of aligning two knowledge graphs (the entities and relations therein). The reviewers found the use of adversarial training quite novel and appropriate for the the task, especially as it works in the unsupervised setting as well. The reviewers were also impressed that the proposed work outperforms existing approaches in terms of the accuracy of the alignment.  The following potential weaknesses were raised by the reviewers and the AC: (1) Reviewer 3 brings up the fact that the hyperaparameters were set different from the original publications of the baselines, and thus are not convinced of the soundness of the results, (2) Reviewer 2 notes that the evaluation is limited, and more variations should be considered, such as varying the overlap, taking larger subsets of knowledge graphs, and going beyond TranE as the choice for embedding, and (3) Reviewer 3 notes that a simpler baseline based on alignment discrepancy should be  considered, which would alleviate the need for RL based training.  Although the reviewers raised very different concerns with the paper, none of them were addressed in a response or revision, and thus they agree that the paper should be rejected.
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
Reviewers are in full agreement for rejection.
The proposed method proposes a new architecture that uses mixture of experts to determine what to share between multiple languages for transfer learning. The results are quite good.  There is still a bit of a lack of framing compared to the large amount of previous work in the field, even after initial revisions to cover reviewer comments. I think that probably this requires a significant rewrite of the intro and maybe even title of the paper to scope the contributions, and also make sure in the empirical analysis that the novel contributions are evaluated independently on their own (within the same experimental setting and hyperparameters).  As such, and given the high quality bar of ICLR, I can t recommend this paper be accepted at this time, but I encourage the authors to revise this explanation and re submit a new version elsewhere.
Pros:   an explicitly multi objective approach to neural architecture search   multiple datasets   ablation experiments  Cons:   lack of baselines like hyperparameter search   ill justified increase in capacity after search   ineffective use of the multiple objectives in assessment   not clearly beating random search baseline  The reviewers adjusted their scores upward after the rebuttal, but serious concerns remain, and the consensus is still to (borderline) reject the paper.
This paper suggests augmenting adversarial training with a Lipschitz regularization of the loss, and suggests that this improves the adversarial robustness of deep neural networks. The idea of using such regularization seems novel. However, several reviewers were seriously concerned with the quality of the writing. In particular, the paper contains claims that not only are not needed but also are incorrect. Also, the Reviewer 2 in particular was also concerned with the presentation of prior work on Lipschitz regularization.   Such poor quality of the presentation makes it impossible to properly evaluate the actual paper contribution. 
Important problem (modular & interpretable approaches for VQA and visual reasoning); well written manuscript, sensible approach. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance. 
This paper proposes a framework for generating auxiliary tasks as a means to regularize learning. The idea is interesting, and the method is simple. Two of the three reviewers found the paper to be well written. The experiment include a promising result on the CIFAR dataset. The reviewer s brought up several concerns regarding the description of the method, the generality of the method (e.g. the requirement for class hierarchy), the validity and description of the comparisons, and the lack of experiments on domains with much more complex hierarchies. None of these concerns were not addressed in revisions to the paper. Hence, the paper in it s current state does not meet the bar for publication.
This paper proposes to unroll power iterations within a Slow Feature Analysis learning objective in order to obtain a fully differentiable slow feature learning system. Experiments on several datasets are reported.   This is a borderline submissions, with reviewers torn between acceptance and rejection. They were generally positive about the clarity and simplicity of the presentation, whereas they raised concerns about the relative lack of novelty (especially related to the recent SpIN model), as well as the current limitations of the approach on large scale problems. Reviewers also found authors to be responsive and diligent during the rebuttal phase. The AC agrees with this assessment, and therefore recommends rejection at this time, encouraging the authors to resubmit to the next conference cycle after addressing the above points. 
This paper proposes an input dependent baseline function to reduce variance in policy gradient estimation without adding bias. The approach is novel and theoretically validated, and the experimental results are convincing. The authors addressed nearly all of the reviewer s concerns. I recommend acceptance.
Strong points:    Interesting, fairly systematic and novel analyses of recurrent NMT models, revealing individual neurons responsible for specific type of information (e.g., verb tense or gender)    Interesting experiments showing how these neurons can be used to manipulate translations in specific ways (e.g., specifying the gender for a pronoun when the source sentence does not reveal it)    The paper is well written  Weak points    Nothing serious (e.g., maybe interesting to test across multiple runs how stable these findings are).  There is a consensus among the reviewers that this is a strong paper and should be accepted.  
In this paper, neural networks are taken a step further by increasing their biological likeliness.  In particular, a model of the membranes of biological cells are used computationally to train a neural network.  The results are validated on MNIST.  The paper argumentation is not easy to follow, and all reviewers agree that the text needs to be improved.  ˜The neuroscience sources that the models are based on are possibly outdated.  Finally, the results are too meagre and, in the end, not well compared with competing approaches.  All in all, the merit of this approach is not fully demonstrated, and further work seems to be needed to clarify this.
This paper builds on the recent DCFNet (Decomposed Convolutional Filters) architecture to incorporate rotation equivariance while preserving stability. The core idea is to decompose the trainable filters into a steerable representation and learn over a subset of the coefficients of that representation.  Reviewers all agreed that this is a solid contribution that advances research into group equivariant CNNs, bringing efficiency gains and stability guarantees, albeit these appear to be incremental with respect to the techniques developed in the DCFNet work. In summary, the AC believes this to be a valuable contribution and therefore recommends acceptance. 
This paper proposes factorized prior distributions for CNN weights by using explicit and implicit parameterization for the prior. The paper suggest a few tractable methods to learn the prior and the model jointly. The paper, overall, is interesting.  The reviewers have had some disagreement regarding the effectiveness of the method. The factorized prior may not be the most informative prior and using extra machinery to estimate it might deteriorates the performance. On the other hand, estimating a more informative prior might be difficult. It is extremely important to discuss this trade off in the paper. I strongly recommend for the authors to discuss the pros and cons of using priors that are weakly informative vs strongly informative.  The idea of using a hierarchical model has been around, e.g., see the paper on "Hierarchical variational models" and more recently "semi implicit Variational Inference". Please include a related work on such existing work. Please discuss why your proposed method is better than these existing methods.  Conditioned on the two discussions added to the paper, we can accept it. 
While the idea of revisiting regression via classification is interesting, the reviewers all agree that the paper lacks a proper motivating story for why this perspective is important. Furthermore, the baselines are weak, and there is additional relevant work that should be considered and discussed.
 pros:   good, clear writing   interesting analysis   very important research area   nice results on multi task omniglot  cons:   somewhat limited experimental evaluation  The reviewers I think all agree that the work is interesting and the paper well written. I think there is still a need for more thorough experiments (which it sounds like the authors are undertaking).  I recommend acceptance.  
This paper presents quasi hyperbolic momentum, a generalization of Nesterov Accelerated Gradient. The method can be seen as adding an additional hyperparameter to NAG corresponding to the weighting of the direct gradient term in the update. The contribution is pretty simple, but the paper has good discussion of the relationships with other momentum methods, careful theoretical analysis, and fairly strong experimental results. All the reviewers believe this is a strong paper and should be accepted, and I concur. 
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The paper introduces a method for using information directed sampling, by taking advantage of recent advances in computing parametric uncertainty and variance estimates for returns. These estimates are used to estimate the information gain, based on a formula from (Kirschner & Krause, 2018) for the bandit setting. This paper takes these ideas and puts them together in a reasonably easy to use and understandable way for the reinforcement learning setting, which is both nontrivial and useful. The work then demonstrates some successes in Atari. Though it is of course laudable that the paper runs on 57 Atari games, it would make the paper even stronger if a simpler setting (some toy domain) was investigated to more systematically understand this approach and some choices in the approach.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The paper tackles an interesting and relevant problem for ICLR: guided image modification of images (in this case of facial attributes).   The proposed method is in general well explained (although some details are lacking)   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The training set of faces and associated attributes were annotated using a pre trained model which introduced a bias into the annotations used for training the method.   The experimental results weren t convincing. The qualitative results showed no clear advantage of the proposed method and the quantitative comparison to StarGAN only considered two attribute manipulations and only found a statistically significant different in performance for one of those. The second weakness was the key determining factor in the AC s final recommendation.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There were no major points of contention and no author feedback.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
This paper presents a meta learning approach which relies on a learned prior over neural networks for different tasks.   The reviewers found this work to be well motivated and timely. While there are some concerns regarding experiments, the results in the miniImageNet one seem to have impressed some reviewers.   However, all reviewers found the presentation to be inaccurate in more than one points. R1 points out to "issues with presentation" for the hierarchical Bayes motivation, R2 mentions that the motivation and derivation in Section 2 is "misleading" and R3 talks about "short presentation shortcomings".  R3 also raises important concerns about correctness of the derivation. The authors have replied to the correctness critique by explaining that the paper has been proofread by strong mathematicians, however they do not specifically rebut R3 s points. The authors requested R3 to more specifically point to the location of the error, however it seems that R3 had already explained in a very detailed manner the source of the concern, including detailed equations.   There have been other raised issues, such as concerns about experimental evaluation. However, the reviewers  almost complete agreement in the presentation issue is a clear signal that this paper needs to be substantially re worked. 
The manuscript presents a promising new algorithm for learning geometrically inspired embeddings for learning hierarchies, partial orders, and lattice structures. The manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via Gaussian convolutions. This is shown to be particularly effective for non overlapping boxes, where the previous method fail.  The primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the non domain expert. This can be improved by including an additional general introduction. Otherwise, the manuscript was well written.  Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. In our opinion, this paper is a clear accept.
The submission proposes a setting of two agents, one of them probing the other (the latter being the "demonstrator"). The probing is done in a way that learns to imitate the expert s behavior, with some curiosity driven reward that maximizes the chance that the probing agents has the expert do trajectories that the probing agent hasn t seen before.  All the reviewers found the idea and experiments interesting. The major concern is whether the setup and the environments are too contrived. At least 2 reviewers commented on the fact that the environments/dataset seemed engineered for success of the given method, which is a concern about how this method would generalize to something other than the proposed setup.  I also share the concern with R3 regarding the practicality of the proposed method: it is not obvious to me what problems this would actually be *useful* for, given that the method requires online interaction with an expert agent in order to succeed. The space of such scenarios where we can continuously probe an expert agent many many times for free/cheap is very small and frankly I m not entirely sure why you would need to do imitation learning in that case at all (if the method was shown to work using only a state, rather than requiring a state/action pair from the expert, then maybe it d be more useful).  It s a tough call, but despite the nice results and interesting ideas, I think the method lacks generality and practical utility/significance and thus at this point I cannot recommend acceptance in its current form.
The authors propose a hierarchical attention layer which combines intermediate layers of multi level attention. While this is a simple idea, and the authors show some improvements over the baselines, the authors raised a number of concerns about the validity of the chosen baselines, and the lack of more detailed evaluations on additional tasks and analysis of the results. Given the incremental nature of the work, and the significant concerns raised by the reviewers, the AC is recommending that this paper be rejected.
The paper presents a unified system for perception and control that is trained in a step wise fashion, with visual decoders to inspect scene parsing and understanding. Results demonstrate improved performance under certain conditions. But reviewers raise several concerns that must be addressed before the work is accepted.  Reviewer Pros: + simple elegant design, easy to understand + provides some insight behind system function during failure conditions (error in perception vs control) + improves performance under a subset of tested conditions   Reviewer Cons:   Concern about lack of novelty   Evaluation is limited in scope   References incomplete   Missing implementation details, hard to reproduce   Paper still contains many writing errors
 Pros:  *  High quality evaluation across different benchmarks, plus human eval  *  The paper is well written (though one could quibble about the motivation for the method, see Cons)  Cons:  *  The approach is incremental, the main contribution is replacing marginalization or RL with G S. G S has already been studied in the context of VAEs with categorical latent variables, i.e. very similar models.  *  The main technical novelty is varying amount of added noise (i.e. downscaling Gumbel noise). In principle, the Gumbel relaxation is not needed here as exact marginalization can be done (as) effectively. Unlike the standard strategy used to make discrete r.v. tractable in complex models, samples from G S are not used in this work to weight input to the  decoder  (thus avoiding expensive marginalization) but to weight terms corresponding to reconstruction from individual latent states (in constract, e.g., to SkimRNN of Seo et al (ICLR 2018)). Presumably adding noise to softmax helps to force sharpness on the posteriors (~ argmax in previous work) and stochasticity may also help exploration.    (Given the above, "to preserve differentiability and circumvent the difficulties in training with reinforcement learning, we apply the reparameterization trick with Gumbel softmax" seems slightly misleading)   *  With contextualized embeddings, which are sense disambiguated given the context, learning discrete senses (which are anyway only coarse approximations of reality) is less practically important  Two reviewers are somewhat lukewarm (weak accept) about the paper (limited novelty), whereas one reviewer is considerably more positive. I do not believe that the reviews diverge in any factual information though.    
This paper and revisions have some interesting insights into using ER for catastrophic forgetting, and comparisons to other methods for reducing catastrophic forgetting. However, the paper is currently pitched as the first to notice that ER can be used for this purpose, whereas it was well explored in the cited paper "Selective Experience Replay for Lifelong Learning", 2018. For example, the abstract says "While various methods to counteract catastrophic forgetting have recently been proposed, we explore a straightforward, general, and seemingly overlooked solution – that of using experience replay buffers for all past events". It seems unnecessary to claim this as a main contribution in this work. Rather, the main contributions seem to be to include behavioural cloning, and do provide further empirical evidence that selective ER can be effective for catastrophic forgetting.   Further, to make the paper even stronger, it would be interesting to better understand even smaller replay buffers. A buffer size of 5 million is still quite large. What is a realistic size for continual learning? Hypothesizing how ER can be part of a real continual learning solution, which will likely have more than 3 tasks, is important to understand how to properly restrict the buffer size.  Finally, it is recommended to reconsider the strong stance on catastrophic interference and forgetting. Catastrophic interference has been considered for incremental training, where recent updates can interfere with estimates for older (or other values). This definition does not precisely match the provided definition in the paper. Further, it is true that forgetting has often been used explicitly for multiple tasks, trained in sequence; however, the issues are similar (new learning overriding older learning). These two definitions need not be so separate, and further it is not clear that the provided definitions are congruent with older literature on interference.    Overall, there is most definitely useful ideas and experiments in this paper, but it is as yet a bit preliminary. Improvements on placement, motivation and experimental choices would make this work much stronger, and provide needed clarity on the use of ER for forgetting.
The submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations.  Although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference.  On the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks.  The experiments on MNIST and ImageNet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack.  The paper is also praised for its clarity, and cleaner formulation compared to Xiao et al. (see below).  Additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information e.g. about the effect of the smoothness of the deformation.  There were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis (R1).  R2 would have appreciated more analysis on how to defend against the attack.  A controversial point is the relation /  novelty with respect to Xiao et al., ICLR 2018.  As e.g. pointed out by R1: "The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.])."  On the balance, all three reviewers recommended acceptance of the paper.  Regarding novelty over Xiao et al., even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks.
This paper proposes to regularize neural network in function space rather than in parameter space, a proposal which makes sense and is also different than the natural gradient approach.  After discussion and considering the rebuttal, all reviewers argue for acceptance. The AC does agree that this direction of research is an important one for deep learning, and while the paper could benefit from revision and tightening the story (and stronger experiments); these do not preclude publishing in its current state.  Side comment: the visualization of neural networks in function space was done profusely when the effect of unsupervised pre training on neural networks was investigated (among others). See e.g. Figure 7 in Erhan et al. AISTATS 2009 "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre Training". This literature should be cited (and it seems that tSNE might be a more appropriate visualization techniques for non linear functions than MDS).
This paper presents two extensions of Relation Networks (RNs) to represent a sentence as a set of relations between words: (1) dependency based constraints to control the influence of different relations within a sentence and (2) recurrent extension of RNs to propagate information through the tree structure of relations.  Pros: The notion of relation networks for sentence representation is potentially interesting.  Cons: The significance of the proposed methods compared to existing variants of TreeRNNs is not clear (R1). R1 requested empirical comparisons against TreeRNNs (since the proposed methods are also of tree shape), but the authors argued back that such experiments are necessary beyond BiLSTM baselines.  Verdict: Reject. The proposed methods build on relatively incremental ideas and the empirical results are rather inconclusive.
The paper proposes a discriminator dependent rejection sampling scheme for improving the quality of samples from a trained GAN. The paper is clearly written, presents an interesting idea and the authors extended and improved the experimental analyses as suggested by the reviewers.
While there has been lots of previous work on training dictionaries for sparse coding, this work tackles the problem of doing son in a purely local way. While previous work suggests that the exact computation of gradient addressed in the paper is not necessarily critical, as noted by reviewers, all reviewers agree that the work still makes important contributions through both its theoretical analyses and presented experiments. Authors are encouraged to work on improving clarity further and delineating their contribution more precisely with respect to previous results.
 Pros:   Great work on getting rid of the need for QP and the corresponding proof of the update rule   Mostly clear writing   Good experimental results on relevant datasets   Introduction of a more reasonable evaluation methodology for continual learning  Cons:   The model is arguably a little incremental over GEM.  In the end I think all the reviewers agree though that the practical value of a considerably more efficient and easy to implement approach largely outweighs this concern.  I think this is a good contribution in this area and I recommend acceptance.
The paper presents a method for unsupervised/semi supervised clustering, combining adversarial learning and the Mixture of Gaussians model. The authors follow the methodology of ALI, extending the Q and P models with discrete variables, in such a way that the latent space in the P model comprises a mixture of Gaussians model.   The problem of generative modeling and semi supervised learning are interesting topics for the ICLR community.  The reviewers think that the novelty of the method is unclear. The technique appears to be a mix of various pre existing techniques, combined with a novel choice of model. The experimental results are somewhat promising, and it is encouraging to see that good generative model results are consistent with improved semi supervised classification results. The paper seems to rely heavily on empirical results, but they are difficult to verify without published source code. The datasets chosen for experimental validation are also quite limited, making it it difficult to assess the strengths of the proposed method.
The paper presents a GAN based generative model, where the generator consists of the base generator followed by several editors, each trained separately with its own discriminator. The reviewers found the idea interesting, but the evaluation insufficient. No rebuttal was provided.
This paper proposes a faster approximation to batch norm, which avoids summing over the entire batch by subsampling either random examples or random image locations. It analyzes some of the tradeoffs of computation time vs. statistical efficiency of gradient estimation, and proposes schemes for decorrelating the samples to make good use of smaller numbers of samples.  The proposal is a reasonable one, and seems to give a noticeable improvement in efficiency. However, it s not clear there is a substantial enough contribution for an ICLR paper. The idea of subsampling is fairly obvious, and various other methods have already been proposed which decouple the computation of BN statistics from the training batch. From a practical standpoint, it s not clear that the observed benefit is large enough to justify the considerable complexity of an efficient implementation.  
This paper formulates a method for training deep networks to produce high resolution semantic segmentation output using only low resolution ground truth labels. Reviewers agree that this is a useful contribution, but with the limitation that joint distribution between low  and high resolution labels must be known. Experimental results are convincing. The technique introduced by the paper could be applicable to many semantic segmentation problems and is likely to be of general interest. 
The first reviewer summarizes the contribution well: This paper combines [a CNN that computes both a multi scale feature pyramid and a depth prediction, which is expressed as a linear combination of "depth bases"]. This is used to [define a dense re projection error over the images, akin to that of dense or semi dense methods]. [Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg Marquardt (LM). By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back propagation and thus learning of the networks  parameters.]   Strengths: While combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. The authors accounted for reviewer feedback during the review cycle and improved the manuscript leading to an increased rating.   Weaknesses: Weaknesses were addressed during the rebuttal including better evaluation of their predicted lambda and comparison with CodeSLAM.  Contention: This paper was not particularly contentious, there was a score upgrade due to the efforts of the authors during the rebuttal period.  Consensus: This paper addresses an interesting  area of research at the intersection of geometric computer vision and deep learning and should be of considerable interest to many within the ICLR community. The discussion of the paper highlighted some important nuances of terminology regarding the characterization of different methods. This paper was also rated the highest in my batch. As such, I recommend this paper for an oral presentation. 
I would like to highlight to the PCs that reviewers highlighted clear evidence of plagiarism from prior work, which I was able to easily verify (a full paragraph of text was copied, word for word, from a paper describing one of the baselines the current work compares against). Further, all reviewers unanimously agreed that the paper was poorly written, and contains no useful advances for the ICLR audience. I recommend a rejection, and further, examination by the PCs of the conduct of the authors.
The paper describes a method to improve generalization by mixing examples in the hidden space. Experiments on CIFAR 10 and CIFAR 100 showed that the proposed method improves the generalization of the networks. The reviewers found these results promising, but argue that the experimental section was too weak in its current form   notable lacking experiments on larger scale datasets such as Imagenet. Notably the paper should compare more with the relevant baselines to better understand its significance.
Dear authors,  All reviewers pointed out the fact that your result is about the expressivity of the big network rather than its accuracy, a result which is already known for the literature.  I encourage you to carefully read all reviews should you wish to resubmit this work to a future conference.
The paper presents quite a simple idea to transfer a policy between domains by conditioning the orginal learned policy on the physical parameter used in dynamics randomization.  CMA ES then finds the best parameters in the target domain. Importantly, it is shown to work well,  for examples where the dynamics randomization parameters do not span the parameters that are actually changed, i.e., as is likely common in reality gap problems.  A weakness is the size of the contribution beyond UPOSI (Yu et al. 2017), the closest work. The authors now explicitly benchmark against this, with (generally) positive results. AC: It would be ideal to see that the method does truly help span the reality gap, by seeing working sim2real transfer.  Overall, the reviewers and AC are in agreement that this is a good idea that is likely to have impact. Its fundamental simplicity means that it can also readily be used as a benchmark in future sim2real work. The AC recommend it be considered for oral presentation based on its simplicity, the importance of the sim2real problem, and particularly if it can be demonstrated to work well on actual sim2real transfer tasks (not yet shown in the current results). 
This paper proposes using conditional VAEs for multi domain transfer and presents results on CelebA and SCUT. As mentioned by reviewers, the presentation and clarity of the work could be improved. It is quite difficult to determine the new/proposed aspects of the work from a first read through. Though we recognize and appreciate that the authors updated their manuscript to improve its clarity, another edit pass with particular focus on clarifying prior work on conditional VAEs and their proposed new application to domain transfer would be beneficial.   In addition, as DIS is the main metric for comparison to prior work and for evaluation of the final approach, the conclusions about the effectiveness of this method would be easier to see if a more detailed description of the metric and analysis of the results were provided.   Given the limited technical novelty and discussion amongst reviewers of the desire for more experimental evidence, this work is not quite ready for publication.
The reviewers have reached a consensus that this paper is very interesting and add insights into interpolation in autoencoders.
The paper contains useful information and shows relative improvements compared to mixup. However, some of the main claims are not substantiated enough to be fully convincing. For example, the claims that manifold mixup can prevent can manifold collision issue where the interpolation between two samples collides with a sample from other class is incorrect. The authors are encouraged to incorporate remarks of the reviewers.
This paper points out methods to obtain sparse convolutional operators. The reviewers have a consensus on rejection due to clarity and lack of support to the claims.
The authors derive and experiment with quaternion based recurrent neural networks, and demonstrate their effectiveness on speech recognition tasks (TIMIT and WSJ), where the authors demonstrate that the proposed models can achieve the same accuracy with fewer parameters than conventional models. The reviewers were unanimous in recommending that the paper be accepted.
The paper summarizes existing work on binary neural network optimization and performs an empirical study across a few datasets and neural network architectures. I agree with the reviewers that this is a valuable study and it can establish a benchmark to help practitioners develop better binary neural network optimization techniques.  PS: How about "An empirical study of binary neural network optimization" as the title? 
I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.  A paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:  1. Show that the errors found can be used to meaningfully improve the models.   This requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).  2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non obvious to a researcher in the field.  This is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non obvious and it seems to work fine, making the Gumbel method unnecessary.  3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.  I do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).  4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models  Given that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326 However, I believe the paper would need to be rethought and rewritten to make this sort of contribution.   Ultimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea. 
The paper proposes a data augmentation technique to ensemble classifiers. Reviewers pointed to a few concerns, including a lack of novelty, a lack of proper comparison with state of the art models or other data augmentation approaches. Overall, all reviewers recommended to reject the paper, and I concur with them.
All reviewers agreed that this paper addresses an important question in deep learning (why doesn t SVRG help for deep learning)? But the paper still has some issues that need to be addressed before publication, thus the AC recommends "revise and resubmit".
The authors propose implementing intrinsic motivation as a differentiable supervised loss coming from the error of a forward model, rather than the black box style of curiosity reward. The motivation is that this approach will lead to more sample efficient exploration for real robots. The use of a differentiable loss for policy optimization is interesting and has some novelty. However, the reviewers were unanimous in their criticism of the paper for poor baselines, unclear experiments and results, and unsupported claims. Even after substantial revisions to the paper, the AC and reviewers were unconvinced of the basic claims of the paper.
Strengths:    well written    strong results for non autoregressive NMT   a novel soft EM version of VQ VAE  Weaknesses:     as pointed out by reviewers, the improvements are mostly not due to the VQ VAE modification rather due to orthogonal (and not interesting) changes e.g., knowledge distillation. If there is a genuine contribution of VQ VAE, it is small and required extensive parameter selection     the explanations provided in the paper do not match the empirical results  Two reviewers criticize the experiments / experimental section: rigour / their discussion.  Overall, there is nothing wrong with the method but the experiments are not showing that the modification is particularly beneficial.  Given these results and also given that the method is not particularly novel (switching from EM to Soft EM in VQ VAE), it is hard for me to argue for accepting the paper.
The paper gives an extension of scattering transform to non Euclidean domains by introducing scattering transforms on graphs using diffusion wavelet representations, and presents a stability analysis of such a representation under deformation of the underlying graph metric defined in terms of graph diffusion.   Concerns of the reviewers are primarily with what type of graphs is the primary consideration (small world social networks or point cloud submanifold samples) and experimental studies. Technical development like deformation in the proposed graph metric is motivated by sub manifold scenarios in computer vision, and whether the development is well suitable to social networks in experiments still needs further investigations.   The authors make satisfied answers to the reviewers’ questions. The reviewers unanimously accept the paper for ICLR publication.
This paper presents an approach that combines rule lists with prototype based neural models to learn accurate models that are also interpretable (both due to rules and the prototypes). This combination is quite novel, the reviewers and the AC are unaware of prior work that has combined them, and find it potentially impactful. The experiments on the healthcare application were appreciated, and it is clear that the proposed approach produces accurate models, with much fewer rules than existing rule learning approaches.  The reviewers and AC note the following potential weaknesses: (1) there are substantial presentation issues, including the details of the approach, (2) unclear what the differences are from existing approaches, in particular, the benefits, and (3) The evaluation lacked in several important aspects, including user study on interpretability, and choice of benchmarks.  The authors provided a revision to their paper that addresses some of the presentation issues in notation, and incorporates some of the evaluation considerations as appendices into the paper. However, the reviewer scores are unchanged since most of the presentation and evaluation concerns remain, requiring significant modifications to be addressed.
This paper provides a technique to learn multi class classifiers without multi class labels, by modeling the multi class labels as hidden variables and optimizing the likelihood of the input variables and the binary similarity labels.   The majority of reviewers voted to accept.
The paper proposes the use of reinforcement learning to learn heuristics in backtracking search algorithm for quantified boolean formulas, using a neural network to learn a suitable representation of literals and clauses to predict actions. The writing and the description of the method and results are generally clear. The main novelty lies in finding a good architecture/representation of the input, and demonstrating the use of RL in a new domain. While there is no theoretical justification for why this heuristic should work better than existing ones, the experimental results look convincing, although they are somewhat limited and the improvements are dataset dependent. In practice, the overhead of the proposed method could be an issue. There was some disagreement among the reviewers as to whether the improvements and the results are significant enough for publication.
The paper proposes a simple approach for computing a sentence embedding as a weighted combination of pre trained word embeddings, which obtains nice results on a number of tasks.  The approach is described as training free but does require computing principal components of word embedding subspaces on the test set (similarly to some earlier work).  The reviewers are generally in agreement that the approach is interesting, and the results are encouraging.  However, there is some concern about the clarity of the paper and in particular the placement of the work in relation to other methods.  There is also a bit of concern about whether there is sufficient novelty compared to Arora et al. 2017, which also compose sentence embeddings as weighted combinations of word embeddings, and also use a principal subspace of embeddings in the test set.  This AC feels that the method here is sufficiently different from Arora et al., but agrees with the reviewers that the paper clarity needs to be improved, so that the community can appreciate what is gained from the new aspects of the approach and what conclusions should be drawn from each experimental comparison.
The proposed method suggests a way to do robust conditional image generation with GANs. The premise is to make the image to image translation model resilient to noise by leveraging structure in the output space, with an unsupervised "pathway".  In general, the qualitative results seem reasonable on a a number of datasets, including those suggested by reviewers. The method appears simple, novel and easy to try.  The main concerns seem to be that the idea is maybe too simple, but I m not particularly bothered by that. The authors showed it working well on a variety of tasks (synthetic and natural), provide SSIM numbers that look compelling (despite SSIM s short comings) and otherwise give compelling arguments for the technical soundness of the approach.  Thus, I recommend acceptance.
This paper studies the problem of training binary neural networks using quantum amplitude amplification method. Reviewers agree that the problem considered is novel and interesting. However the consensus is that there are only few experiments in the current paper and the paper needs more experiments on different datasets with comparisons to proper baselines. Reviewers opined that the paper was not so easy to follow initially,  though later revisions may have somewhat alleviated this problem.
The paper presents an interesting theoretical analysis by deriving polynomial sample complexity bounds for the training of GANs that depend on the approximator properties of the discriminator. Even if it is not clear if the theory will help to pick suitable discriminators in practice, it provides new and interesting theoretical insights on the properties of GAN training. 
A hierarchical method is presented for developing humanoid motion control, using low level control fragments, egocentric visual input, recurrent high level control. It is likely the first demonstration of 3D humanoids learning to do memory enabled tasks using only proprioceptive and head based ego centric vision. The use of control fragments as opposed to mocapclip based skills allows for finer grained repurposing of pieces of motion, while still allowing for mocap based learning  Weaknesses: It is largely a mashup up of previously known results (R2).  Caveat: this can be said for all research at some sufficient level of abstraction. The motions are jerky when transitions happen between control fragments (R2,R3). There are some concerns as to whether the method compares against other methods; the authors note that they are either not directly comparable, i.e., solving a different problem, or are implicitly contained in some of the comparisons that are performed in the paper.  Overall, the reviewers and AC are in broad agreement regarding the strengths and weaknesses of the paper.  The AC believes that the work will be of broad interest. Demonstrating memory enabled, vision driven, mocap imitating skills is a broad step forward. The paper also provides a further datapoint as  to which combinations of method work well, and some of the specific features required to make them work.  The paper could acknowledge motion quality artifacts, as noted by the reviewers and  in the online discussion.  Suggest to include  [Peng et al 2017] as some of the most relevant related HRL humanoid control work, as per the reviews & discussion.  
Pros:   novel idea of intra life curiosity that encourages diverse behavior within each episode rather than across episodes.  Cons:   privileged/ad hoc information (RAM state, distinguishing rooms)   lack of sufficient ablations/analysis   insufficient revision/rebuttal  The reviewers reached consensus that the paper should be rejected in its current form.
This  paper is on graph based semi supervised learning where the goal is to develop an approach to jointly the node labeling function together with the edge weights. A natural way to formulate this problem as a bi level optimization problem. However, the authors claim that this approach introduces two main difficulties: (a)  the "upper" objective function is itself the solution to the "lower" optimization problem (Eq. (2)), and (b) optimization is challenging (Eq. (3)). The AC disagrees. Firstly, there is a close connection between the constrained version and the regression version of the problem (e.g., Belkin, Matveeva and Niyogi)   the former is infact a special case of the latter for a certain choice of regularization parameter. The latter reduces to an linear system. The outer problem can be optimized using standard gradient descent using the implicit function theorem trick common in bilevel optimization. Reviewers have also raised concerns about clarity, and experimental support in this paper and comparisons with related work.  
The paper tried to introduce a new interpretation of dropout and come with improved algorithms. However, the reviewers were not convinced that the presented arguments were correct/novel, and they found the paper difficult to follow. The authors are encouraged to carefully revise their paper to address these concerns. 
Pros:   compelling idea to use VAEs to reduce the dimensionality of the space in which to run evolution   non trivial benchmark results   clearly written, solid background  Cons:   moderate novelty (as compared to [1])   performance results are sup par   no rebuttal, despite constructive and detailed review comments (and an explicit willingness to raise scores by multiple points!)  The reviewers agree that the paper should be rejected in its current form, but would plausibly have been willing to reassess their scores for a major revision   which did not materialize.
The paper investigates a novel initialisation method to improve Equilibrium Propagation.  In particular, the results are convincing, but the reviewers remain with small issues here and there.  An issue with the paper is the biological plausibility of the approach.  Nonetheless publication is recommended.  
The paper presents an architecture search method which jointly optimises the architecture and its weights. As noted by reviewers, the method is very close to Shirakawa et al., with the main innovation being the use of categorical distributions to model the architecture. This is a minor innovation, and while the results are promising, they are not strong enough to justify acceptance based on the results alone.
This paper present a framework for creating meaning preserving adversarial examples. It then proposes two attacks within this framework: one based on k NN in the word embedding space, and another one based on character swapping.   Overall, the goal of constructing such meaning preserving attacks is very interesting. However, it is unclear how successful the proposed approach really is in the context of this goal.   Additionally, it is not clear how much novelty there is compared to already existing methods that have a very similar aim.
This paper gives explicit hyperparameter gradients for several models with convex losses.  The idea is well motivated and clearly presented, but because it s relatively incremental, it needs a more systematic experimental section, or at least a stronger characterization of its scope and limitations.  I would also recommend an investigation of more expressive hyperparameterizations (like in Maclaurin et al 2015) and/or an investigation of overfitting on the validation set.
This paper conducts experiments evaluating several different metrics for evaluating GAN based language generation models. This is a worthy pursuit, and some of the evaluation is interesting.  However as noted by Reviewer 2, there are a number of concerns with the execution of the paper: evaluation of metrics with respect to human judgement is insufficient, the diversity of the text samples is not evaluated, and there are clarity issues.  I feel that with a major re write and tighter experiments this paper could potentially become something nice, but in its current form it seems below the ICLR quality threshold. 
The reviewers viewed the work favorably, with only one reviewer providing a score slightly below acceptance. The authors thoroughly addressed the reviewer s original concerns, and they adjusted their score upwards afterwards. The low rating reviewer remains skeptical of the significance of the work, but the other two reviewers make firm cases for the appeal of the work to the ICLR audience. In follow up discussion after the author s responses were submitted and discussed, the low rating reviewer did not make a clear case for rejecting the paper, and further, the higher rating reviewers  arguments for the impact of the paper were convincing. Therefore, I recommend accepting this paper.
Pros:   The paper is well written and clear and presented with helpful illustrations and videos.   The  training methodology seems sound (multiple random seeds etc.)   The results are encouraging.  Cons:   There was some concern generally about how this work is positioned relative to related work and the completeness of the related work.  However, the authors have made this clearer in their rebuttal.  There was a considerable amount of discussion between the authors and all reviewers to pin down some unclear aspects of the paper. I believe in the end there was good convergence and I thank both the authors and reviewers for their persistence and dilligence in working through this.  The final paper is much better I think and I recommend acceptance.
The paper received mixed reviews. It proposes a variant of Siamese network objective function, which is interesting. However, it’s unclear if the performance of the unguided method is much better than other baselines (e.g., InfoGAN). The guided version of the method seems to require much domain specific knowledge and design of the feature function, which makes the paper difficult to apply to broader cases.  
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
This paper provides an approach to jointly localize and repair VarMisuse bugs, where a wrong variable from the context has been used. The proposed work provides an end to end training pipeline for jointly localizing and repairing, as opposed to independent predictions in existing work. The reviewers felt that the manuscript was very well written and clear, with fairly strong results on a number of datasets.  The reviewers and AC note the following potential weaknesses: (1) reviewer 4 brings up related approaches from automated program repair (APR), that are much more general than the VarMisuse bugs, and the paper lacks citation and comparison to them, (2) the baselines that were compared against are fairly weak, and some recent approaches like DeepBugs and Sk_p are ignored, (3) the approach is trained and evaluated only on synthetic bugs, which look very different from the realistic ones, and (4) the contributions were found to be restricted in novelty, just uses a pointer based LSTM for locating and fixing bugs.   The authors provided detailed comments and a revision to address and clarify these concerns. They added an evaluation on realistic bugs, along with differences from DeepBugs and Sk_p, and differences between neural and automated program repair. They also added more detail comparisons, including separating the localization vs repair aspects by comparing against enumeration. During the discussion, the reviewers disagree on the "weakness" of the baseline, as reviewers 1 and 4 feel it is a reasonable baseline as it builds upon the Allamanis paper. They found, to different degrees, that the results on realistic bugs are much more convincing than the synthetic bug evaluation. Finally, all reviewers agree that the novelty of this work is limited.  Although the reviewers disagree on the strength of the baselines (a recent paper) and the evaluation benchmarks, they agreed that the results are quite strong. The paper, however, addressed many of the concerns in the response/revision, and thus, the reviewers agree that it meets the bar for acceptance.
The paper can also improved thorough a more thorough evaluation. 
This paper proposes a framework for using invertible neural networks to study inverse problems, e.g., recover hidden states or parameters of a system from measurements. This is an important and well motivated topic, and the solution proposed is novel although somewhat incremental. The paper is generally well written. Some theoretical analysis is provided, giving conditions under which the proposed approach recovers the true posterior. Empirically, the approach is tested on synthetic data and real world problems from medicine and astronomy, where it is shown to compared favorably to ABC and conditional VAEs. Adding additional baselines (Bayesian MCMC and Stein methods) would be good. There are some potential issues regarding MMD scalability to high dimensional spaces, but overall the paper makes a solid contribution and all the reviewers agree it should be accepted for publication.
The paper develops an original extension/generalization of standard batchnorm (and group norm) by employing a mixture of experts to separate incoming data into several modes and separately normalizing each mode. The paper is well written and technically correct, and the method yields consistent accuracy improvements over basic batchnorm on standard image classification tasks and models. Reviewers and AC noted the following potential weaknesses: a) while large on artificially mixed data, improvements are relatively small on single standard datasets (<1% on CIFAR10 and CIFAR100)  b) the paper could better motivate why multi modality is important e.g. by showing histograms of node activations c) the important interplay between number of modes and batch size should be more thoroughly discussed d) the closely related approach of Kalayeh & Shah 2018 should be presented and contrasted with in more details in the paper. Also comparing to it in experiments would enrich the work. 
The paper proposes a neural network architecture for video compression. The reviewers point out lack of novelty with respect to recent neural compression works on static images, which the present paper extends by adding a temporal consistency loss. More importantly, reviewers point our severe problems with the metrics used to measure compression quality, which the authors promise to take into account in a future manuscript. 
This paper proposes an algorithm for training sequence to sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state of the art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision.
Overall this paper presents a few improvements over the code2vec model of Alon et al., applying it to seq2seq tasks. The empirical results are very good, and there is fairly extensive experimentation.  This is a relatively crowded space, so there are a few natural baselines that were not compared to, but I don t think that comparison to every single baseline is warranted or necessary, and the authors have done an admirable job. One thing that still is quite puzzling is the strength of the "AST nodes only baseline", which the authors have given a few explanations for (using nodes helps focus on variables, and also there is an effect of combining together things that are close together in the AST tree). Still, this result doesn t seem to mesh with the overall story of the paper all that well, and again opens up some obvious questions such as whether a Transformer model trained on only AST nodes would have done similarly, and if not why not.  This paper is very much on the borderline, so if there is space in the conference I think it would be a reasonable addition, but there could also be an argument made that the paper would be stronger in a re submission where the above questions are answered.
Very solid work, recognized by all reviewers as worthy of acceptance. Additional readers also commented and there is interest in the open source implementation that the authors promise to provide.
The reviewers unanimously agreed that the paper was a significant advance in the field of machine learning on graph structured inputs. They commented particularly on the quality of the research idea, and its depth of development. The results shared by the researchers are compelling, and they also report optimal hyperparameters, a welcome practice when describing experiments and results.  A small drawback the reviewers highlighted is the breadth of the content in the paper, which gave the impression of a slight lack of focus. Overall, the paper is a clear advance, and I recommend it for acceptance. 
This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition. The authors also introduce a separate contribution of self adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task.  The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper.  The reviewers appreciated the author s comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self adversarial sampling, and comparisons to TorusE, (2) improved presentation.  With the revision, the reviewers agreed that this is a worthy paper to include in the conference. 
This paper addresses a promising and challenging idea in Bayesian deep learning, namely thinking about distributions over functions rather than distributions over parameters.  This is formulated by doing MCMC in a functional space rather than directly in the parameter space.  The reviewers were unfortunately not convinced by the approach citing a variety of technical flaws, a lack of clarity of exposition and critical experiments.  In general, it seems that the motivation of the paper is compelling and the idea promising, but perhaps the paper was hastily written before the ideas were fully developed and comprehensive experiments could be run.  Hopefully the reviewer feedback will be helpful to further develop the work and lead to a future submission.  Note: Unfortunately one review was too short to be informative.  However, fortunately the other two reviews were sufficiently thorough to provide enough signal.  
This paper proposes verification algorithms for a class of convex relaxable specifications to evaluate the robustness of neural networks under adversarial examples.  The reviewers were unanimous in their vote to accept the paper. Note: the remaining score of 5 belongs to a reviewer who agreed to acceptance in the discussion.
The reviewers agree that the paper needs significantly more work to improve presentation and is not fully empirically and conceptually convincing.
This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. TopicGAN operates in two steps: it first generates latent topics and produces bag of words corresponding to those latent topics. In the second step, the model generates text conditioning on those topic words.  Pros:  It combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation.  Cons:  There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty. Of these, the first two were the main concerns. In particular, R1 and R2 raised concerns about insufficient component wise evaluation (e.g., text classification from topic models) and insufficient GAN based baselines. Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3). The technical novelty is not extremely strong in that the proposed model combines existing components together. But this alone would have not been a deal breaker if the empirical results were rigorous and strong.  Verdict: Reject. Many technical details require clarification and experiments lack sufficient comparisons against prior art.
The authors have described a navigation method that uses co grounding between language and vision as well as an explicit self assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers  conclusion is that the paper should be accepted. 
Strengths:  This paper proposed to use graph based deep learning methods to apply deep learning techniques to images coming from omnidirectional cameras.  Weaknesses:  The projected MNIST dataset looks very localized on the sphere and therefore does not seem to leverage that much of the global connectivity of the graph All reviewers pointed out limitations in the experimental results. There were significant concerns about the relation of the model to the existing literature.  It was pointed out that both the comparison to other methodology, and empirical comparisons were lacking.   The paper received three reject recommendations.  There was some discussion with reviewers, which emphasized open issues in the comparison to and references to existing literature as highlighted by contributed comment from Michael Bronstein.  Work is clearly not mature enough at this point for ICLR, insufficient comparisons / illustrations
 * Strengths  The paper addresses a timely topic, and reviewers generally agreed that the approach is reasonable and the experiments are convincing. Reviewers raised a number of specific concerns (which could be addressed in a revised version or future work), described below.  * Weaknesses  Some reviewers were concerned the baselines are weak. Several reviewers were concerned that relying on failures observed during training could create issues by narrowing the proposal distribution (Reviewer 3 characterizes this in a particularly precise manner). In addition, there was a general feeling that more steps are needed before the method can be used in practice (but this could be said of most research).  * Recommendation  All reviewers agreed that the paper should be accepted, although there was also consensus that the paper would benefit from stronger baselines and more close attention to issues that could be caused by an overly narrow proposal distribution. The authors should consider addressing or commenting on these issues in the final version.
This paper addresses an importnant and more realistic setting of multi task RL where the reward function changes; the approach is elegant, and empirical results are convincing. The paper presents an importnant contribution to the challenging multi task RL problem.
The paper s strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The theory is novel (but it seems to relate closely to the work https://arxiv.org/abs/1711.02771.) The main drawback the reviewer raised includes a) it s not clear how tight the lower bound is; b) the theory only applies to a particular subcase of GANs   it seems that the only reasonable instance that allows efficient generator is the case where Y   G(x)+\xi where \xi is Gaussian noise. The authors addressed the issue a) with some new experiments with linear generators and quadratic loss, but it lacks experiments with deep models which seems to be necessary since this is a critical issue. Based on this, the AC decided to recommend reject and would encourage the authors to add more experiments on the tightness of the lower bound with bigger models and submit to other top venues.  
This paper makes a substantial contribution to the understanding of the approximation ability of deep networks in comparison to classical approximation classes, such as polynomials.  Strong results are given that show fundamental advantages for neural network function approximators in the presence of a natural form of latent structure.  The analysis techniques required to achieve these results are novel and worth reporting to the community.  The reviewers are uniformly supportive.
The paper addresses the complexity issue of Determinantal Point Processes via generative deep models.  The reviewers and AC note the critical limitation of applicability of this paper to variable ground set sizes, whether authors  rebuttal is not convincing enough.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The work presents a new way to generate images from sounds. The reviewers found the problem ill defined, the method not well motivated and the results not compelling. There are a number of missing references and things to compare to, which the authors should change in a follow up.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
This heavily disputed paper discusses a biologically motivated alternative to back propagation learning.   In particular, methods focussing on sign symmetry rather than weight symmetry are investigated and, importantly, scaled to large problems.  The paper demonstrates the viability of the approach.  If nothing else, it instigates a wonderful platform for debate.  The results are convincing and the paper is well presented.  But the biological plausibility of the methods needed for these algorithms can be disputed.  In my opinion, these are best tackled in a poster session, following the good practice at neuroscience meetings.  On an aside note, the use of the approach to ResNet should be questioned.  The skip connections in ResNet may be all but biologically relevant.
The paper proposes improvements on the area of neural network inference with homomorphically encrypted data. Existing applications typically have high computational cost, and this paper provides some solutions to these problems. Some of the improvements are due to better "engineering" (the use of the faster SAEL 3.2.1 over CryptoNet). The idea of using pre trained AlexNet features is new, but pretty standard practice. The presentation has been greatly improved in the updated version, however the paper could benefit from additional discussions and experiments. For example, when a practitioner wants to solve a new problem with some design need (e.g. accuracy, latency vs. bandwidth trade off), what network modules should be used and how should they be represented? To summarize, the problem considered is important, however, as pointed out by the reviewers, both the empirical and the theoretical results appear to be incremental with respect to the existing literature. 
The paper gives an bilevel optimization view for several standard RL algorithms, and proves their asymptotic convergence with function approximation under some assumptions.  The analysis is a two time scale one, and some empirical study is included.  It s a difficult decision to make for this paper.  It clearly has a few things to be liked: (1) the bilevel view seems new in the RL literature (although the view has been implicitly used throughout the literature); (2) the paper is solid and gives rigorous, nontrivial analyses.  On the other hand, reviewers are not convinced it s ready for publication in its current stage: (1) Technical novelty, in the context of published works: extra challenges needed on top of Borkar; similarity to and differences from Dai et al.; ... (2) The practical significance is somewhat limited.  Does the analysis provide additional insight into how to improve existing approaches?  How restricted are the assumptions?  Are the online vs batch distinction from Dai et al. really important in practice? (3) What does the paper want to show in the experiments, since no new algorithms are developed?  Some claims are made based on very limited empirical evidence.  It d be much better to run algorithms on more controlled situations to show, say, the significance of two timescale updates.  Also, as those algorithms are classic Q learning and actor critic (quote the authors in responses), how well do the algorithms solve the well known divergent examples when function approximation is used? (4) Presentation needs to be improved.  Reviewers pointed out some over claims and imprecise statements.  While the author responses were helpful in clarifying some of the questions, reviewers felt that the remaining questions needed to be addressed and the changes would be large enough that another full review cycle is needed.
This paper proposes a principled solution to the problem of joint source channel coding. The reviewers find the perspectives put forward in the paper refreshing and that the paper is well written. The background and motivation is explained really well.  However, reviewers found the paper limited in terms of modeling choices and evaluation methodology. One major flaw is that the experiments are limited to unrealistic datasets, and does not evaluate the method on a realistic benchmarks. It is also questioned whether the error correcting aspect is practically relevant.    
AR2 is concerned about the marginal novelty, weak experiments and very high complexity of the algorithm. AR3 is concerned about lack of theoretical analysis and parameter setting. AR4 is concerned that the proposed method is useful in very restricted settings and the paper is incremental.  Unfortunately, with strong critique from reviewers regarding the novelty, complexity, poor presentation and restricted setting, this draft cannot be accepted by ICLR.
This work proposes and interesting approach to learn approximate set membership. While the proposed architecture is rather closely related to existing work, it is still interesting, as recognized by reviewers. Authors s substantial rewrites has also helped make the paper clearer. However, the empirical merits of the approach are still a bit limited; when combined with the narrow novelty compared to existing work, this makes the overall contribution a bit too thin for ICLR. Authors are encouraged to strengthen their work by showing more convincing practical benefit of their approach.
The reviewers of this paper agreed that it has done a stellar job of presenting a novel and principled approach to attention as a latent variable, providing a new and sound set of inference techniques to this end. This builds on top of a discussion of the limitations of existing deterministic approaches to attention, and frames the contribution well in relation to other recurrent and stochastic approaches to attention. While there are a few issues with clarity surrounding some aspects of the proposed method, which the authors are encouraged to fine tune in their final version, paying careful attention to the review comments, this paper is more or less ready for publication with a few tweaks. It makes a clear, significant, and well evaluate contribution to the field of attention models in sequence to sequence architectures, and will be of great interest to many attendees at ICLR.
This paper provides an alternative way to enable differentiable optimization to the neural architecture search problem.  Different from DARTS, SNAS reformulates the problem and employs Gumbel random variables to directly optimize the NAS objective. In addition, the resource constrained regularization is interesting. The major cons of the paper is that the empirical results are not quite impressive, especially when compared to DARTS, in terms of both accuracy and convergence. I think this is a borderline paper but maybe good enough for acceptance.  
All reviewers gave a 5 rating. The author rebuttal was not able to alter the consensus view of reviewers. See below for details.
As the reviewers pointed out, the strength of the paper mostly comes from the analysis of the non linear quantization which depends on the double log of the Lipschitz constants and other parameters. The AC and reviewers agree with the dimension independent nature of the bounds, but also note that dimension independent gound may not necessarily be significantly stronger than the dimension dependent bounds as the metric of measuring the difficulty of the problem also matters. Although the paper does seem to lack result that shows the empirical benefit of the non linear quantization. In considering the author response and reviewer comments, the AC decided that this comparison was indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without such a comparison. 
This paper proposes a training algorithm for ConvNet architectures in which the final few layers are fully connected.  The main idea is to use direct feedback alignment with carefully chosen binarized (±1) weights to train the fully connected layers and backpropagation to train the convolutional layers. The binarization reduces the memory footprint and computational cost of direct feedback alignment, while the careful selection of feedback weights improves convergence. Experiments on CIFAR 10, CIFAR 100, and an object tracking task are provided to show that the proposed algorithm outperforms backpropagation, especially when the amount of training data is small. The reviewers felt that the paper does a terrific job of introducing the various training algorithms   backpropagation, feedback alignment, and direct feedback alignment   and that the paper clearly explained what the novel contributions were. However, the reviewers felt the paper had limited novelty because it combines ideas that were already known, that it has limited applicability because it will not work with fully convolutional architectures, that the baselines in the experiments were somewhat weak, and that the paper provided no insights on why the proposed algorithm might be better than backpropagation in some cases. Regrettably, only one reviewer (R2) participated in the discussion, though this was the reviewer who provided the most constructive review. The AC read the revised paper, and agrees with R2 s concerns about the limited applicability of the proposed algorithm and lack of insight or analysis explaining why the proposed training algorithm would improve over backpropagation.
There was major disagreement between reviewers on this paper. Two reviewers recommend acceptance, and one firm rejection. The initial version of the manuscript was of poor quality in terms of exposition, as noted by all reviewers. However, the authors responded carefully and thoroughly to reviewer comments, and major clarity and technical issues were resolved by all authors.   I ask PCs to note that the paper, as originally submitted, was not fit for acceptance, and reviewers noted major changes during the review process. I do believe this behavior should be discouraged, since it effectively requires reviewers to examine the paper twice. Regardless, the final overall score of the paper does not meet the bar for acceptance into ICLR.
The paper formulates the problem of unsupervised one to many image translation and addresses the problem by minimizing  the mutual information.  The reviewers and AC note the critical limitation of novelty and comparison of this paper to meet the high standard of ICLR.   AC decided that the authors need more works to publish.
The paper is well written and develops a novel and original architecture and technique for RNNs to learn attractors for their hidden states (based on an auxiliary denoising training of an attractor network). All reviewers and AC found the idea very interesting and a promising direction of research for RNNs. However all also agreed that the experimental validation was currently too limited, in type and size of task and data, as in scope. Reviewers demand experimental comparisons with other (simpler) denoising / regularization techniques; more in depth experimental validation and analysis of the state denoising behaviour; as well as experiments on larger datasets and more ambitious tasks. 
In general the reviewers found the work to be interesting and the results to be promising.  However, all the reviewers shared significant concerns about the clarity of the paper and the correctness of technical claims made.  This paper would significantly benefit from rewriting and restructuring the paper to improve clarity, better motivate the approach and provide more careful exposition of related work and technical claims.
The paper studies the credit assignment problem in meta RL, proposes a new algorithm that computes the right gradient, and demonstrates its superior empirical performance over others.  The paper is well written, and all reviewers agree the work is a solid contribution to an important problem.
The authors supplied an updated paper resolving the most important reviewer concerns after the deadline for revisions. In part, this was due to reviewers requesting new experiments that take substantial time to complete.  After discussion with the reviewers, I believe that if the revised manuscript had arrived earlier, then it should be accepted. Without the new results I would recommend rejecting since I believe the original submission lacked important experiments to justify the approach (inductive setting experiments are very useful).  The community has an interest in uniform application of the rules surrounding the revision process. It is not fair to other authors to consider revisions past the deadline and we do not want to encourage late revisions. Better to submit a finished piece of work initially and not assume it will be possible to use up a lot of reviewer time and fix during the review process.  We also don t want to encourage shoddy, rushed experimental work. However, the way we typically handle requests from reviewers that require a lot of work to complete is by rejecting papers and encouraging them to be resubmitted sometime in the future, typically to another similar conference.  Thus I am recommending rejecting this paper on policy grounds, not on the merits of the latest draft. I believe that we should base the decision on the state of the paper at the same deadline that applies to all other authors.  However, I am asking the program chairs to review this case since ultimately they will be the final arbiters of policy questions like this.
Strengths: Interesting work on using latent variables for generating long text sequences. The paper shows convincing results on perplexity, N gram based and human qualitative evaluation.  Weaknesses: More extensive comparisons with hierarchical VAEs and the approach in Serban et. al in terms of language generation quality and perplexity would have been helpful. Another point of reference for which additional comparisons were desired was: "A Hierarchical Latent Structure for Variational Conversation Modeling" by Park et al. Some additional substantive experiments were added during the discussion period.  Contention: Authors differentiated their work from Park et al. and the reviewer bringing this work up ended up upgrading their score to a 7. The other reviewers kept their scores at 5.  Consensus: The positive reviewer raised their score to a 7 through the author rebuttal and discussion period.  One negative reviewer was not responsive, but the other reviewer giving a 5 asserts that they maintain their position. The AC recommends rejection. Situating this work with respect to other prior work and properly comparing with it seems to be the contentious issue. Authors are encouraged to revise and re submit elsewhere.
This paper was reviewed by three experts. After the author response, R2 and R3 recommend rejecting this paper citing concerns of novelty and experimental evaluation. R1 assigns it a score of "6" but in comments agrees that the manuscript is not ready for ICLR. The AC finds no basis for accepting this paper in this state.  
The authors propose to define  Expressiveness  in deep RL by the rank of a matrix comprising a number of feature vectors from propagating observations through the learnt representation, and show a correlation between higher rank and higher performance. They try 3 regularizers to increase rank and show that they improve the final score on Atari games compared to A3C or DQN. The AC and reviewers agree that the paper is interesting and novel and could have general significance for the RL field. Also, the authors were very responsive to the reviewers and added more details, plus several experiments and analyses to support their claims. However, the reviewers were concerned about a number of aspects and have recommended that the authors clean up their presentation and analysis a bit more. In particular, the fact that the regularization coefficient is tuned for each Atari game makes it very hard to compare to DQN/A3C which are very careful to keep the same hyperparameters across every game.
This paper proposes to a simple method for  tuning parameters of HMC by maximizing the log density under the final sample of the MCMC, and apply it for training VAE. The reviews and discussion raises some critical concerns and questions, which unfortunately, which unfortunately, is not adequately addressed. 
The reviewers are polarized on this paper and the overall feeling is that it is not quite ready for publication. There is also an interesting interpretability aspect that, while given as a motivation for the approach, is never really explored beyond showing some figures of alignments. One of the main concerns of the method’s effectiveness in practice is the computational cost. There is also concern from one of the reviewers that the formulation could result in creating sparse matching maps where only a few pixels get matched. The authors provide some justification for why this wouldn’t happen, and this should be put in a future draft. Even better would be to show statistics to demonstrate empirically that this doesn’t happen.  There were a number of clarifications that were brought up during the discussion, and the authors should go over this carefully and update the draft to resolve these issues. There is also a typo in the title that should be fixed. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The proposed method performed well on 3 visual content transfer problems.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The paper is hard to follow at times   The problem being addressed is technically interesting but not well motivated. That is, the question "why is this of interest to the ICLR community" was not well answered.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There were no major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
This paper is on active deep learning in the setting where a label hierarchy is available for multiclass classification problems: a fairly natural and pervasive setting. The extension where the learner can ask for example labels as well as a series of questions to adequately descend the label hierarchy is  an interesting twist on active learning.  The paper is well written and develops several natural formulations which are then benchmarked on CIFAR10, CIFAR100, and Tiny ImageNet using a ResNet 18 architecture.  The empirical results are carefully analyzed and appear to set interesting new baselines for active learning. 
The main criticisms were around novelty: that the analysis is rather standard. Given that all the reviewers agreed the paper is well written, I m inclined to think the paper will be a useful contribution to the literature. The authors also highlight the analysis of the discretization, which seems to be missed by the most critical reviewer. I would suggest to the reviewers that they use the criticisms to rework the paper s introduction, to better explain which parts of the work are novel and which parts are standard. I would also suggest that standard background be moved to the appendix so that it is there for the nonexpert, while making the body of the work more focused on the novel aspects.
the authors propose to incorporate an additional layer between the consecutive steps in LSTM by introducing a radial basis function layer (with dot product kernel and softmax) followed by a linear layer to make LSTM similar to or better at (by being more explicit) capturing DFA like transition. the motivation is relatively straightforward, but it does not really resolve the issue of whether existing formulations of RNN s cannot capture such transition. since this was not shown theoretically nor intuitively, it is important for empirical evaluations to be thorough and clearly show that the proposed approach does indeed outperform the vanilla LSTM (with peepholes) when the capacity (e.g., the number of parameters) matches. unfortunately it has been the consensus among the reviewers that more thorough comparison on more conventional benchmarks are needed to convince them of the merit of the proposed approach.
The authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. The experiments demonstrate that this leads to better performance compared to training with small batches. However, as noted by Reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. Considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. The rebuttal didn’t address the reviewers  concerns, and they argue for rejection.
This paper presents a reinforcement learning approach for online cost aware feature acquisition. The utility of each feature is measured in terms of expected variations of the model uncertainty (using MC dropout sampling as an estimate of certainty) which is subsequently used as a reward function in the reinforcement learning formulation. The empirical evaluations show improvements over prior approaches in terms of accuracy cost trade off on three datasets. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.   Initially, R1 and R2 raised important concerns regarding low technical novelty. R1 requested an ablation study to understand which of the following components gives the most improvement: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that MC dropout certainty plays a crucial rule in the performance of the proposed method. R1 subsequently increased the assigned score to 6. R2 raised concerns about related prior work Contardo et al 2016, which similarly evaluates the most informative features given budget constraints with a recurrent neural network approach. After a long discussion and a detailed rebuttal, R2 upgraded the rating from below the threshold to 7, albeit acknowledging an incremental technical contribution. R3 raised important concerns regarding presentation clarity that were subsequently addressed by the authors. In conclusion, all three reviewers were convinced by the authors rebuttal and have upgraded their initial rating, and AC recommends acceptance of this paper – congratulations to the authors! 
The paper suggests using an ensemble of Q functions for Q learning. This idea is related to bootstrapped DQN and more recent work on distributional RL and quantile regression in RL. Given the similarity, a comparison against these approaches (or a subset of those) is necessary. The experiments are limited to very simple environment (e.g. swing up and cart pole). The paper in its current form does not pass the bar for acceptance at ICLR.
This paper proposes reducing so called "negative transfer" through adversarial feature learning. The application of DANN for this task is new. However, the problem setting and particular assumptions are not sufficiently justified. As commented by the reviewers and acknowledged by the authors there is miscommunication about the basic premise of negative transfer and the main assumptions about the target distribution and it s label distribution need further justification. The authors are advised to restructure their manuscript so as to clarify the main contribution, assumptions, and motivation for their problem statement.  In addition, the paper in it s current form is lacking sufficient experimental evidence to conclude that the proposed approach is preferable compared to prior work (such as Li 2018 and Zhang 2018) and lacks the proper ablation to conclude that the elimination of negative transfer is the main source of improvements.   We encourage the authors to improve these aspects of the work and resubmit to a future venue. 
The proposed method is an extension of Kim & Bengio (2016) s energy based GAN. The novel contributions are to approximate the entropy regularizer using a mutual information estimator, and to try to clean up the model samples using some Langevin steps. Experiments include mode dropping experiments on toy data, samples from the model on CelebA, and measures of inception score and FID.  The paper is well written, and the proposal seems sensible. But as various reviewers point out, the work is a fairly incremental extension of Kim and Bengio (2016). Most of the new elements, such as Langevin sampling and the gradient penalty, have also been well explored in the deep generative modeling literature. It s not clear there is a particular contribution here that really stands out.  The experimental evidence for improvement is also fairly limited. Generated samples, inception scores, and FID are pretty weak measures for generative models, though I m willing to go with them since they seem to be standard in the field. But even by these measures, there doesn t seem to be much improvement. I wouldn t expect SOTA results because of computational limitations, but the generated samples and quantitative evaluations seem worse than the WGAN GP, even though the proposed method includes the gradient penalty and hence should be able to at least match WGAN GP. The MCMC sampling doesn t appear to have helped, as far as I can tell.  Overall, the proposal seems promising, but I don t think this paper is ready for publication at ICLR. 
While the authors made a strong rebuttal, none of the reviewers were particularly enthusiastic about the contributions of this paper and we unfortunately have to reject borderline papers. Concerns were expressed about the presentation, as well as the scalability of the approach. The AC encourages the authors to "revise and resubmit".
This submission proposes an interesting new approach on how to evaluate what features are the most useful during training. The paper is interesting and the proposed approach has the potential to be deployed in many applications, however the work as currently presented is demonstrated in a very narrow domain (stability prediction), as noted by all reviewers. Authors are encouraged to provide stronger experimental validation over more domains to show that their approach can truly improve over existing multitask frameworks.
Strengths: This paper develops a method for learning the structure of discrete latent variables in a VAE.  The overall approach is well explained and reasonable.  Weaknesses: Ultimately, this is done using the usual style of discrete relaxations, which come with tradeoffs and inconsistencies.  Consensus: The reviewers all agreed that the paper is above the bar.