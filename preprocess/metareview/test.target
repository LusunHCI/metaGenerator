This paper proposes a pre training technique for semantic parsing with an emphasis on semantic parsing and the technical details required to actually make it work in practice. Overall, all reviewers agree that the results are very good, and you see nice improvement across multiple text to SQL datasets. Some reservations have been raised on (a) the difficulty of creating the SCFG for generating the synthetic data, but this seems to have been properly addressed by the authors and requires a reasonable amount of effort. and (b) how tailored the pre training task is to a particular task (text to SQL) and dataset (Spider). Overall, I tend to agree that the fact that one sees improvement on Spider is slightly less compelling as the grammar is derived from it, but the authors rightfully claim that consistent improvements are also evident in other datasets, even if the gains are somewhat smaller. One can hope the idea can also be generalized to other setups where synthetic data can be generated and the details of how to combine synthetic data with real data should be useful. 
Verifying robustness of neural networks is an important application in machine learning. The submission takes on this challenge via the interval bound propagation (IBP) framework and provides a theoretical analysis on the training procedure. They establish, in the large network with case, that the certification via IBP reflects the robustness of the neural network. Despite the tensions between the changing architecture and the required accuracy, the results are insightful. The AC recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.
The paper proposes a top down approach to train deep neural networks   freezing top layers after supervised pre training, then re initializing and retraining the bottom layers. As mentioned by all the reviewers, the novelty is on the low side. The paper is purely experimental (no theory), and the experimental section is currently too weak. In particular:   Experiments on different domains should be performed.   Different models should be evaluated.   Ablation experiments should be performed to understand better under which conditions the proposed approach works.   For speech recognition, WER should be reported   even if it is without a LM   such that one can compare with existing work. 
The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs). Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection. Reviewer #1 likes the general idea of the work, and consider the contribution to be sound. However, he concerns the reproducibility of the work due to the niche database from e commerce applications. Reviewer #2 concerns the poor presentation, especially section 3. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
The paper focuses on the task of conditional video synthesis starting from a single image. The authors propose an *Action Graph* to model the configuration of objects, their interactions, and actions. They show promising results on two benchmark datasets (one synthetic and another realistic).   Based on the reviewers  comments and the limited discussion that ensued, it seems that some concerns in the paper were addressed by the authors; however, a main concern persists, namely the applicability of this Action Graph representation to more complex realistic videos (*e.g.* for datasets such as Kinetics and AVA). The authors do mention a manner in which an automated extraction of Action Graphs can be done, specifically with off the shelf (spatial or spatiotemporal) detectors for actions, objects, and object object interactions. However, these are complicated tasks in their own right and still open problems in the field. Given that the Action Graph computable from this automated pipeline will undoubtedly contain noise (compounded by the errors of each component of this pipeline), the paper could have made a stronger argument for its contributions in realistic video, if for example an ablation study was done where *noisy* action graphs were used in training. Without more evidence that this representation will be applicable to more realistic scenarios of interest, it is difficult to gauge the impact it will have on the community. Despite its merits and promising initial results, the authors are encouraged to address this persisting concern and the other reviewers  comments to produce a stronger submission in the future. 
The work introduces a method that uses the Feature Statistics Alignment paradigm to improve sequence generation with GANs. The contribution is interesting and novel (although marginally), clarity is also good. However the reviewers raised several concerns calling for more comprehensive and thorough evaluation. Experiments show an improvement comparing to selected baselines and the revised paper addressed, at least partially, a serious evaluation concern of one reviewer. Although the excellent revision work some important open questions still seem to remain, in particular the choose of alignment metrics and a thorough evaluation. 
Thank you for submitting you paper to ICLR. ICLR. The consensus from the reviewers is that this is not quite ready for publication. In particular, the experimental results are promising, but further work is required to fully demonstrate the efficacy of the approach.
This paper presents an attempt to infer controllable aspects of the environment dynamics by imposing an architecture on a pixel prediction model, such that the prediction is the sum of an action aware and action agnostic prediction, trained on a fixed dataset gathered under a uniform random policy, in such a way as to incentivize the action aware predictor to model the residual of what the action agnostic predictor is capable of predicting on its own. The utility of the method is demonstrated qualitatively and quantitatively by comparing both the effect identification performance and performance when used in the context of intrinsically motivated RL.  Reviewers found the work to be well motivated, highly relevant, clearly described and insightful. Several reviewers pointed to the relatively limited empirical evaluation, appropriateness of baselines, and questions as to how the method would apply to richer observation spaces. Reviewer hgSF, whose score improved by 2 points following discussion, in particular questioned the motivation for the baseline method (ADM) and asked for a hyperparameter ablation study which the authors provided. Reviewer Y2tc, whose score improved by 3 points, found that several experiments poorly supported the paper s claims and was concerned about the uniform policy requirement for training; after discussion, clarifications and a softening of some claims, the reviewer was satisfied. Reviewer fGPn raised concerns around metrics and questioned applicability to first person observations, and while the former were addressed to the reviewer s satisfaction, the latter were not, and the reviewer maintained their marginal accept leaning appraisal. MeF5 s biggest concern revolves around a conceptual issue around the expectation in equation 5 which judging by the discussion was clarified but not fully resolved.  The paper has improved significantly since submission through the diligent engagement of the reviewers. The method proposed is clever and effective in the domains considered but as fGPn points out is probably limited in applicability to observation spaces wherein non controllable effects remain stationary, ruling out 3D locomotion or camera control for example. It is unclear whether this limitation is addressable, and the current version of the manuscript does not discuss this a perspective a great deal (in fact, direct mention of this  limitation is absent from the "limitations" section).  After discussion among the senior program committee, it was decided that the manuscript, while much improved by subsequent revisions, fell short of the acceptance bar this year, particularly in terms of rigorous evaluation of the method. We are therefore unable to recommend acceptance at this time.
All reviewers recommend acceptance. Some concerns were raised about the precision of theorem 2 (now renamed to proposition 1), as well as the analysis of hyperparameter choices and quantitative evaluation, which I believe the authors have adequately addressed. Based on a suggestion of reviewer 1, experiments with flow based models were also added, which demonstrates that the method is not strictly tied to autoregressive models. Personally, I was also curious about the connection between noise injection and quantisation, which the authors responded to by adding a paragraph discussing this connection in the manuscript.  I would recommend that the authors also add the kernel inception distance (KID) results reported in the comments to the manuscript.  This work stands out to me in that it combines a relatively simple, easy to understand idea with nice results, which is a trait of many impactful papers. I will therefore join the reviewers in recommending acceptance.
This is an interesting paper that is concerned with single episode transfer to reinforcement learning problems with different dynamics models, assuming they are parameterised by a latent variable. Given some initial training tasks to learn about this parameter, and a new test task, they present an algorithm to probe and estimate the latent variable on the test task, whereafter the inferred latent variable  is used as input to a control policy.  There were several issues raised by the reviewers. Firstly, there were questions with the number of runs and the baseline implementations, which were all addressed in the rebuttals. Then, there were questions around the novelty and the main contribution being wall clock time. These issues were also adequately addressed.  In light of this, I recommend acceptance of this paper.
This paper targets improving the computation efficiency of super resolution task. Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance. 
This paper proposes a new self supervised pre trained speech model that improves speech recognition performance.   The idea combines an earlier pre training approach (wav2vec) with discretization followed by BERT style masked reconstruction.  The result is a fairly complex approach, with not too much novelty but with a good amount of engineering and analysis, and ultimately very good performance.  The reviewers agree that the work deserves publication at ICLR, and the authors have addressed some of the reviewer concerns in their revision.  The complexity of the approach may mean that it is not immediately widely adopted by others, but it is a good proof of concept and may well inspire other related work.  I believe the ICLR community will find this work interesting.
The paper proposes a text normalisation model for Amharic text. The model uses word classification, followed by a character based GRU attentive encoder decoder model. The paper is very short and does not present reproducible experiments. It also does not conform to the style guidelines of the conference. There has been no discussion of this paper beyond the initial reviews, all of which reject it with a score of 1. It is not ready to publish and the authors should consider a more NLP focussed venue for future research of this kind.  
Three reviewers have reviewed this manuscript, and they had severe reservations regarding the presentation quality and the lack of sufficient theoretical support behind empirical observations. Even after rebuttal, the reviewers maintained that the above issues are not fully resolved. Unfortunately, this paper cannot be accepted in its current form.
A focused contribution that is clearly presented. That being said, the task of low resource named entity recognition is fairly narrow and it is hard to tell how significant the empirical results are. The paper could be much stronger if it evaluated on a second task (and third task). Right now it is unclear whether the technique would generalize to other tasks.
This paper is a study of neural network scaling, with models containing hundred of billions of parameters. To that end, the paper introduce a new module called GShard, consisting of annotations APIs on how to split computations across accelerators, which is integrated in the XLA compiler. This enables the training of models with hundreds billions of parameters. To scale efficiently to very large models, the paper proposes to use transformer networks, where every other feed forward sub layer is replaced by a sparse mixture of experts (similar to Shazeer et al. 2017). This model is then evaluated on a multilingual machine translation task, from 100 languages to English.  On the one hand, I believe that the contributions of the paper are significant: scaling to 600B parameters, and showing that this leads to better translation quality are important achievement. The analysis of transformer networks scaling could also have an important impact. Finally I think that GShard and its integration in XLA could be very valuable. On the other hand, I agree with some of the concerns raised by the reviewers, regarding the writing of the paper and the reproducibility. I found the paper not well written, and hard to identify the differences with previous work. As GShard is one of the main contribution, I would expect a better description of it in the main text (compared to the MoE which seems more incremental). Regarding reproducibility, I do not think that the authors provided a good reason not to evaluate on standard benchmarks: the test sets could be excluded from the train set through various deduplication heuristics.   To conclude, I am leaning toward accepting the paper, but believe it is borderline. The reason is that the contributions are significant, and worth publishing. But I would not oppose a rejection based on the reproducibility and writing issues.
The paper presents an interesting extension of the SkipThought idea by modeling sentence embeddings using several document structure related information.  Out of the various kinds of evaluations presented, the coreference results are interesting   but, they fall short by a bit (as noted by Reviewer 2) because they don t compare with recent work by Kenton Lee et al.  In summary, the idea provides an interesting bit on building sentence embeddings, but the experimental results could have been stronger.
The paper proposes an approach to remedying mode collapse problem in GANs. This approach relies on using multiple discriminators and assigning a different portion of each minibatch to each discriminator.   + preventing mode collapse in GAN training is an important problem    the exact motivation for the proposed techniques is not fully fleshed out    the evaluation and baselines used are lacking
The idea of the paper   imposing a GAN type loss on the latent interpolations of an autoencoder   is interesting. However there are strong concerns from R2 and R3 about limited experimental evaluation of the proposed method which falls short of demonstrating its advantages over latent spaces learned by existing GANs. Another point of concern was the use of only one real dataset (CelebA). Authors made substantial revisions to the paper in addressing many of the reviewers  points but these core concerns still persist with the current draft and it s not ready for publication at ICLR. Authors are encouraged to address these concerns and resubmit to another venue. 
This paper proposes Model Inversion Networks (MINs) to solve model optimization problems high dimensional spaces. The paper received three reviews from experts working in this area. In a short review, R1 recommends Reject based on limited novelty compared to an ICDM 2019 paper. R2 recommends Weak Reject, identifying several strengths of the paper but also a number of concerns including unclear or missing technical explanations and need for some additional experiments (ablation studies). R3 recommends Weak Accept, giving the opinion that the idea the paper proposes is worthy of publication, but also identifying a number of weaknesses including a "rushed" experimental section that is missing details, need for additional quantitative experimental results, and some "ad hoc" parts of the formulation. The authors prepared responses that address many of these concerns, including a convincing argument that there is significant difference and novelty compared to the ICDM 2019. However, even if excluding R1 s review, the reviews of R2 and R3 are borderline; the ACs read the paper and while they feel the work has significant merit, they agree with R2 and R3 that the paper needs additional work and another round of peer review to fully address R2 and R3 s concerns.   
There was a fair amount of discussion about the paper.  Several reviewers felt that the paper would have been stronger if it tried to do less but better.  The reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements.  Doing this requires a better understanding of the algorithm s behavior and a valid ablation study, a new concern raised during the discussion with the authors.   The reviewers felt that the proposed approach is potentially interesting and would like to see this paper done well.
 This paper considers the problem of learning a k dimensional latent simplices given perturbations of data points in the simplex: this problem is of wide relevance in machine learning as it encompasses many latent variable models including Latent Dirichlet Allocation and Stochastic Block Models. It presents a modification of the recent algorithm of Bhattacharya & Kannan (SODA, 2020) which takes time O(k * nnz(A)), where A is the matrix of perturbed data points. The modified algorithm works with a low dimensional sketch of the matrix A instead of A, and thereby avoids the dependence on k in the running time of the original algorithm, which used k passes over the data set. The main result of the paper is thus that the latent simplex problem can be solved in O(nnz(A)) time for instances of the problem that satisfy the "Spectrally Bounded Perturbation" property introduced by the authors.  The main questions of the reviewers concerned the question of whether the assumptions needed for the analysis of the novel algorithm to apply hold on real data sets. The authors point out that the assumptions may be stronger than are needed in practice, and suggest that the assumptions could be weakened to assuming that a spectrally accurate sketch could be used  this would increase the run time dependence from just nnz(A), but weakens the assumptions needed. It was also observed that, in addition to a faster runtime, the method outperforms the benchmark method.  This paper should be accepted, due to its theoretical and practical contributions to the problem of latent simplex recovery: it presents an algorithm that provably runs in true input sparsity time given an amenable instance, and practically this algorithms performs well relative to the baseline, verifying the theoretical claims.
The authors present the task lift the flap where an agent (artificial or human) is presented with a blurred image and a hidden item. The agent can de blur the parts of the image by clicking on it. The authors introduce a model for this task (ClickNet) and they compare this against others. As reviewers point, this paper presents an interesting set of experiments and analyses. Overall, this type of work can be quite influential as it gives an alternative way to improve our models by unveiling human strategies and using those as inductive biases for our models. That being said, I find the conclusions of this paper quite narrow for the general audience of ICLR (as R2 and R3 also point), as authors look into an artificial task and show ClickNet performs well. But what have we learned beyond that? How do we use these results to improve either our models or our understanding of these models? I believe these are the type of questions that  are missing from the current version of the paper and that if answered would greatly increase its impact and relevance to the ICLR community. At the moment though, I cannot recommend this paper for acceptance. 
This paper proposes a new method for multi label classification, which leverages the advantage of the emery based model. However, one reviewer and the area chair have two serious concerns on the experiments: (1) The proposed method is only evaluated on low dimensional datasets; (2) Some important baselines methods are missing, which makes the comparison not convincing. I suggest the authors to evaluate their methods on more datasets, and add the results from well known multi label classification method for comparison.
This paper conducts a theoretical and empirical analysis of the Generative Adversarial Training method (GAT). Although many comments have been addressed in the rebuttal, the reviewers still have few (but important) concerns, including the memorization effects and the lack of comparisons. 
The paper introduces a generative approach to reconstruct 3D images for cryo electron microscopy (cryo EM).  All reviewers really liked the paper, appreciate the challenging problem tackled and the proposed solution.  Acceptance is therefore recommended. 
This is a nice paper which shows that KL regularized natural policy gradient (assuming exact access to the MDP, meaning no noise in the reward and Q function estimates), which achieves linear convergence, can use ideas from quasi newton methods and recover their quadratic convergence.  Given the excitement surrounding policy gradient methods and their convergence rates, this is a valuable direction and family of ideas.  Unfortunately, the reviewers had many concerns about presentation, and also of the exact meaning and relationship of the results to prior work; I ll add to this and note that one issue with quasi newton methods is that it is unclear how long the "burn in" phase is, meaning the phase before their quadratic convergence kicks in, and this is still an issue in the present work s theory; another issue, as raised by reviewers, is the difference between the regularized and unregularized optimal policies.  As such, it makes sense for this paper to receive more time and polish.
This paper explores a foundational problem in AI around learning abstractions that allow for easier planning.  The work proposes a specific procedure for learning temporally abstract, discrete representations in which it becomes tractable to perform graph based search.  Evaluation is performed on two 2D tasks where a goal is specified visually and the system must produce the actions to achieve this target state/observation.   The reviewers were in a uncommonly tight consensus as to their evaluation of the paper (all 4).  All reviewers essentially expressed that the motivation of the paper was solid but that the domains considered were too simplistic for validation of this class of approach, especially in light of substantial previous work in the area that was not adequately captured in the baseline comparisons.  The authors did not respond to the reviewers.  My decision is to reject the paper.
The authors present a study where they investigate whether meta learning techniques leverage the underlying task distribution. To do so, the authors come up with two conditions, in the first they generate tasks using a grammar and in the second condition, which is the null condition essentially, the tasks have the same statistical properties as the compositional task but they are not derived from a simple grammar. The authors find that while humans are better in the compositional condition, models are better in the null condition.  All reviewers have been positive with this work, but some concerns were raised regarding clarity around the use of some terms, such as compositionality. The rebuttal period has been very productive and the reviewers have acknowledged the improvements on the manuscript.   All in all, I think this is a good study to appear on ICLR and I believe researchers would benefit from the design of the study that will perhaps open new opportunities around careful evaluation of meta learning agents.
The reviewers are unanimous in accepting the paper.  They generally view it as introducing an original approach to online RL using bandit style selection from a fixed portfolio of off policy algorithms.  Furthermore, rigorous theoretical analysis shows that the algorithm achieves near optimal performance.  The only real knock on the paper is that they use a weak notion of regret i.e. short sighted pseudo regret.  This is considered inevitable, given the setting.
The paper presents a modification of the convolution layer, where the convolution weights are generated by another convolution operation. While this is an interesting idea, all reviewers felt that the evaluation and results are not particularly convincing, and the paper is not ready for acceptance.
The paper proposes an unsupervised domain adaptation solution applied for semantic segmentation from simulated to real world driving scenes. The main contribution consists of introducing an auxiliary loss based on depth information from the simulator. All reviewers agree that the solution offers a new idea and contribution to the adaptation literature. The ablations provided effectively address the concern that the privileged information does in fact aid in transfer. The additional ablation on the perceptual loss done during rebuttal is also valuable and should be included in the final version.   The work would benefit from application of the method across other sim2real dataset tasks so as to be compared to the recent approaches mentioned by the reviewers, but the current evaluation is sufficient to demonstrate the effectiveness of the approach over baseline solutions. 
Main content:  Physical driven architecture of DeepSFM to infer the structures from motion Discussion: reviewer 1: well motivated model with good solid experimental results. not clear about the LM optimization in BA Net is memory inefficient  reviewer 2: main issue is the experiments could be improved. reviewer 3: well written but again experimental section is lacking Recommendation: Good paper and results, but all 3 reviewers agree experiments could be improved. Rejection is recommended.
The paper proposes to train a rejection sampler in the latent space of a GAN to learn disconnected data manifolds. Reviewers raised concerns about some theoretical aspects of the method as well as about the lack of larger scale datasets (ImageNet) in the experiments. Authors responded to these concerns but some of them still remain (including $\hat{\gamma}(z)$ not guaranteed to be a probability distribution and lack of more convincing experiments). I still think the work is promising, and encourage the authors to revise and resubmit the paper addressing these points highlighted by the reviewers. 
The paper proposes a new unsupervised learning scheme via utilizing local maxima as an indicator function.  The reviewers and AC note the novelty of this paper and good empirical justifications. Hence, AC decided to recommend acceptance.  However, AC thinks the readability of the paper can be improved.
This paper provides a novel path auxiliary algorithm for more efficiently exploring discrete state spaces within a Metropolis Hastings sampler for energy based models. In particular, it essentially replaces the "single site update" by instead proposing an entire path using local information, thus enabling the chain to take larger steps, which can improve acceptance/mixing significantly as they demonstrate. The work is a timely contribution that improves upon exciting recent work. After much discussion among several knowledgeable reviewers and clarifications regarding some details of the main theorem from the authors, there is consensus that the contributions are correct, novel, and likely of impact to the machine learning community. Since the revision period, the empirical evaluations have also been improved and the contributions have methodological novelty as well as promising practical performance.
The work proposes a method for smoothing a non differentiable machine learning pipeline (such as the Faster RCNN detector) using policy gradient. Unfortunately, the reviewers identified a number of critical issues, including no significant improvement beyond existing works. The authors did not provide a rebuttal for these critical issues. 
The authors explore the hypothesis of whether grounded representations can be leaved from text only. They show that a language model trained with relatively little data can make conceptual domains such as color to a grounded world representation such as RGB coordinates. The paper was positively received by the reviewers, specifically after a fruitful discussion to further clarify the points that the authors were making and their conclusions. The authors have already edited some parts of the paper, I ask them to go back and include other points that the reviewers have made. I recommend this paper for acceptance, it will generate good discussion and ideas at ICLR.
This paper introduces Signatory, a library for computing functionality related to the signature and logsignature transforms. Although a large body of the initial literature on the signature in ML focuses on using it as a feature extractor, more recent works have incorporated within modern deep learning architectures and therefore, the importance of having GPU capable libraries (with automatic differentiation) that implement these transforms. Several algorithmic improvements are incorporated into the library. Some of the computational benefits of this library wrt to previous ones are demonstrated empirically.   There were some concerns from the reviewers about accepting library papers at ICLR. Library papers clearly fall into the ICLR CFP and, therefore, library, frameworks and platform papers that can be relevant and impactful are welcome contributions to the community. Additionally, more signature related papers are appearing are mainstream ML venues, hence, despite the poor scalability wrt input dimensions, this paper is definitely relevant.   Perhaps one the drawbacks of this paper is the lack of a more rigorous empirical evaluation. The authors have added a deep learning benchmark, which is welcome but only on a toy dataset. There are still some concerns about the wide applicability of the signature (and its relatives) given its exponential scaling. That’s why applications on more realistic problems will be welcome. At the very least, It will be good if the authors incorporate a separate section discussing the limitations of the signature transform (and the library), especially in terms of computations and scalability.  
This paper proposes a new clustering method that takes into account side information.  The paper was reviewed by four expert reviewers who expressed concerns for novelty, empirical and theoretical depth, and unclear parts of the paper. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
This work presented a broad set of interesting applications of model information toward understanding task difficulty, domain similarity, and more. However, reviewers were concerned around the validity and rigor of the conclusions. Going into more depth in a subset of the areas presented would strengthen the paper, as would further discussions and experiments around the limitations of model information with regards to specific models and dataset sizes (as you have begun to discuss in Section 8). Additionally, reviewers found the updated paper with connections to Kolmogorov complexity interesting, but reviewers wanted a more formal treatment and analysis of the relationship. 
The paper proposes using GANs for disentangling style information from speech content, and thereby improve style transfer in TTS. The review and responses for this paper have been especially thorough! The authors significantly improved the paper during the review process, as pointed out by the reviewers. Inclusion of additional baselines, evaluations and ablation analysis helped improve the overall quality of the paper and helped alleviate concerns raised by the reviewers. Therefore, it is recommended that the paper be accepted for publication.
This paper was reviewed by four experts in the field and received mixed scores (1 borderline accept, 3 borderline reject). The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper. AC feels that this work has great potential, but needs more work to better clarify the contribution and include additional ablated study. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper studies a dropout variant, called fraternal dropout. The paper is somewhat incremental in that the proposed approach is closely related to expectation linear dropout. Having said that, fraternal dropout does improve a state of the art language model on PTB and WikiText2 by ~0.5 1.7 perplexity points. The paper is well written and appears technically sound.  Some reviewers complain that the authors could have performed a more careful hyperparameter search on the fraternal dropout model. The authors appear to have partly addressed those concerns, which frankly, I don t really agree with either. By doing only a limited hyperparameter optimization, the authors are putting their "own" method at a disadvantage. If anything, the fact that their method gets strong performance despite this disadvantage (compared to very strong baseline models) is an argument in favor of fraternal dropout.
The paper looks at subspace recovery in the presence of outliers, of which there have been many formulations. They study a recent formulation, DPCP, but relax the requirement that the dimension of the subspace is known   obviously very important in practice. The approach is quite clever: they exploit the fact that for this non convex problem, starting a simple algorithm at a randomly chosen starting point will converge to a local minimizer, and they can run an ensemble of these algorithms (each with different starting points) and be guaranteed the solutions will span an appropriate subspace. This idea alone is a nice contribution. The paper has theory and experiments.  Most reviewers were positive about the paper. The most critical review, by 1qf1, still acknowledged that this paper has a lot of potential, but in their opinion the paper was not in a state ready for acceptance, especially regarding the formulation of the main result, Theorem 7.  The other reviewers were OK with the state of the paper, and the authors made changes in the rebuttal. Hence, while acknowledging the paper could possibly still be improved (what paper couldn t be!), I think the paper is in a good enough state to accept it for ICLR. I don t think there would be enough benefit to the community (authors, readers and reviewers) to ask for this to go through one more round of submission/revision.
The paper posits that VAEs, if made sufficiently deep, are able to implement autoregressive models, and could possibly outperform them. Experimentally, the authors attempt make VAEs sufficiently deep so that they are able to outperform autoregressive models on image generation. The authors use a variety of tricks to scale the depth of the model to up to 78 stochastic layers, and achieve SOTA, or near SOTA NLLs on a number of datasets. Furthermore, in comparison to other models (in particular the recently proposed Nouveau VAE), the models achieve these scores using far fewer parameters.  Although the tricks are a bit ad hoc and the novelty is a bit weak, the experimental results are quite strong and would be of interest to anyone working on VAE research. Moreover, one of the weakness of the paper, a lack of ablations, was addressed during the rebuttal. All reviewers believed that the paper should be accepted, and I see nothing in the paper or the reviews to suggest otherwise.
This paper presents a batch active learning approach (where in each active learning round, instead of a single input, we wish to select several inputs to be labeled). The paper attempts to solve this problem by posing it as a sparse approximation problem and shows that their approach performs favorably as compared to some of the existing methods such as BALD and Bayesian Coresets for batch active learning.  While the reviewers appreciated the basic idea and the general framework, there were several concerns from the reviewers (as well as myself upon reading the manuscript). Firstly, the idea of batch active learning as a sparse subset selection problem is not new (Pinsler et al, 2019). While previous methods such as (Pinsler et al, 2019) have used ideas such as Coresets, this paper uses sparse optimization techniques such as Greedy and IHT. Moreover, there were concerns about experimental settings relying on various heuristics, and lack of a more extensive and thorough comparison with important baselines, such as BatchBALD and others, which the authors acknowledged.  The reviewers have read the authors  response and engaged in discussion but their assessment remained unchanged. Based on their assessment and my own reading of the manuscript, the paper does not seem to be ready for publication. The authors are advised to consider the points raised in the reviews which I hope will help strengthen the paper for a future submission.
This paper proposed a new semi supervised object detection approach using Unbiased Teacher to jointly address the pseudo labeling bias and overfitting issues. Significant improvements over SOTA were reported on COCO and VOC. Reviewers agree that the proposed method is simple and effective, and the experimental results are solid and convincing.  While the novelty of technical contributions for individual components may not be very significant, the idea is simple and well executed with strong results and good presentation. Overall, the paper is recommended for acceptance (poster). 
The authors provide an interesting method to infuse hierarchical information into existing word vectors. This could help with a variety of tasks that require both knowledge base information and textual co occurrence counts. Despite some of the shortcomings that the reviewers point out, I believe this could be one missing puzzle piece of connecting symbolic information/sets/logic/KBs with neural nets and hence I recommend acceptance of this paper.
All but one of the reviewers recommended rejecting this submission. The reviewer recommending acceptance (PBhC) was not confident in their assessment and was unwilling to champion the paper during the discussion phase, making it very difficult for me to unilaterally overrule the de facto reviewer consensus and recommend accepting the submission. Although some of the reviewers recommending rejecting the submission made relatively weak arguments, others raised more compelling points in favor of rejecting the paper. The discussion and reviews convinced me that the preponderance of the evidence indicated that I should recommend rejecting on the merits of the case anyway. Ultimately, I am recommending rejecting this submission, primarily because I do not believe the empirical contributions are strong enough, nor are they polished enough. Holistically, it is hard to see what impact this work can have without improved empirical evidence, given how little guidance the theoretical results give to practitioners. That said, I hope the authors iterate some more on the experiments and refocus the narrative a bit in that direction.  The paper exhibits a problem where gradient descent with momentum provably generalizes better than gradient descent without momentum. Given that momentum does not universally improve the out of sample error of neural networks trained with gradient descent, we should strongly suspect that there also exist problems where adding momentum to gradient descent degrades out of sample performance. Therefore, what actionable insights do we have? The paper suggests that perhaps the details of the problem (constructed in the submission) where momentum helps gives us an ability to predict when momentum will be helpful in practice, but we would need to see several more successful predictions of this form on typical datasets from the literature or other real (non synthetic) datasets. Furthermore, has the literature and this submission even demonstrated convincingly enough that momentum improving out of sample error for the same training loss is a common occurrence? And has this submission even made a convincing empirical case on CIFAR10, let alone a larger selection of problems? The latter question would be sufficient to reject the submission, but resolving it favorably would not, in my view, be sufficient to accept the submission without also more evidence for the prevalence of this momentum generalization phenomenon or without demonstrating successful predictions about relative generalization performance on more problems.  Has the literature established that gradient descent or minibatch stochastic gradient descent often generalizes better when using momentum? The paper says "While these works shed light on how momentum acts on neural network training, they fail to capture the generalization improvement induced by momentum (Sutskever et al., 2013)." but Sutskever et al. to my recollection only measures training set loss and never properly considers questions of generalization. Certainly, in many places in the literature we see momentum get better validation error, but rarely do we get information on whether it does so for the same training loss and a priori we should suspect optimization speed is the primary effect at play. The paper also claims "Although it is well accepted that Momentum improve generalization in deep learning...", but the submission does not provide enough evidence that this is well accepted. The results of Leclerc & Madry (2020) are equivocal and may well be confounded by batch norm, but would need to be investigated further. So no, at least with the citations in this submission, it is far from well established that momentum often improves generalization performance, i.e. that momentum results in better validation loss for the same training loss. Of course it won t always do this, but we should observe it regularly in the wild (the more dramatically the better) for this to be interesting.  Ok, but what about the experiments on CIFAR10? These experiments are hard to interpret because they seem to compare misclassification error (zero one loss) with the actual optimization objective of cross entropy error. These issues may be resolvable, but in their current form leave open too many loose ends. Just because two training runs both get zero classification errors on the training set does not mean that they do not differ in the log loss and even a small difference in log loss might explain a large difference in out of sample classification error. Although we often use these quantities as proxies for each other, that isn t quite safe and a better way to conduct this measurement would be to select an iterate of GD without momentum that has an almost identical (but slightly better) training cross entropy loss than a specific iterate of GD with momentum and then compare the cross entropy loss on the validation set, repeating for many different runs and iterates.  In the final analysis, stochastic gradient descent without momentum rarely gets used in practice and full gradient descent even more rarely, so this submission needs to do a better job of making a case for the impact it will have on researchers in this field. Perhaps a stronger case can be made, but I do not quite find the current version sufficiently compelling.
This paper proposes a new and unusual way of training hard attention mechanisms in vision models. Instead of training with reinforcement learning (as is typical), the authors develop a procedure for generating "glimpse sequences" that can be effectively used as supervision. Models trained in this way produce qualitatively "better" glimpse sequences, higher accuracy, and converge in fewer steps. There was some disagreement and discussion about the merits of the paper. Overall, there were some major concerns:   The process for obtaining glimpse sequences is very computationally expensive. The authors argue that this cost can be amortized because the same glimpse sequences can be computed once for a given dataset and reused. However, there was limited real world motivation for this setup, apart from mentioning neural architecture search (a niche method that is not widely used, and probably has never been used to develop hard attention models).   The method relies on a "convincing" generative model for a given dataset. This limits experiments to simple datasets with unrealistically constrained visual structure. The authors point out that as generative models get better, their method could be applied to more realistic datasets, but as it stands the experimental validation is correspondingly weak.   The improvement in performance is not huge   the "WSRAM" baseline outperforms the use of "near optimal" glimpse sequences. While it is certainly true that the proposed method converges faster, the fact that the proposed method requires such an expensive preprocessing step downgrades this benefit significantly   While the authors provided insightful distillation based baselines in the rebuttal, it remains to be seen whether simple distillation from stronger RAM models (e.g. WSRAM which outperforms the proposed method) could be made to work better/more efficiently. These factors lead to a reject decision overall.
Multiple reviewers point out the interesting improvement to mix attention maps at different layers via convolution based prediction modules. This module is sufficient to show improvements only on encoder side while comparing to concurrent work Synthesizer. However, the novelty of the work is limited as compared to other papers and the results though improved did not convince the reviewers fully to gain a strong accept. 
The authors propose a technique for pruning networks by using second order information through the Hessian. The Hessian is approximated using the Fisher Information Matrix, which is itself approximated using KFAC. The paper is clearly written and easy to follow, and is evaluated on a number of systems where the authors find that the proposed method achieves good compression ratios without requiring extensive hyperparameter tuning.   The reviewers raised concerns about 1) the novelty of the work (which builds on the KFAC work of Martens and Grosse), 2) whether zeroing out individual connections as opposed to neurons will have practical runtime benefits, 3) the lack of comparisons against baselines on overall training time/complexity, 4) comparisons to work which directly prune as part of training (instead of the train prune finetune scheme adopted by the authors). In the view of the AC,  4) would be an interesting comparison but was not critical to the decision. Ultimately, the decision came down to the concern of lack of novelty and whether the proposed techniques would have an impact on runtime in practice.   
Existing implementation of information bottleneck need access to privileged information which goes against the idea of compression. The authors propose variational bandwidth bottleneck which estimates the value of the privileged information and then stochastically decided whether to access this information or not. They provide a suitable approximation and show that their  method improves generalisation in RL while reducing access to expensive information.  These paper received only two reviews. However, both the reviews were favourable. During discussions with the AC the reviewers acknowledged that most of their concerns were addressed. R2 is still concerned that VBB does not result in improvement in terms of sample efficiency. I request the authors to adequately address this in the final version. Having said that, the paper does make other interesting contributions, hence I recommend that this paper should be accepted.
+ the ideas presented in the paper are quite intriguing and draw on a variety of different connections   the presentation has a lot of room for improvement. In particular, the statement of Theorem 1, in its current form, requires rephrasing and making it more rigorous.   Still, the general consensus is that, once these presentation shortcomings are address, this will be an interesting paper. 
The paper focuses on the task of learning efficient representation models for video classification. To avoid the excessive computational cost of performing 3D convolutions on video, the authors propose to break the channel dimension of video representations into sub dimensions that are treated separately. This cuts down on computation and improves classification performance over many methods in the literature. Extensive experiments were run on well known benchmarks to justify the claims of the model. Such backbone architectures can be very useful in the realm of video understanding. The authors should be commended for the amount of work they did in the rebuttal period to address the comments and inquiries brought up by the reviewers. Extra experiments were done and more in depth analysis was made possible. 
Reviewers are in a consensus and recommended to reject. However, the reviewers did not engage at all with the authors, and did not acknowledge whether their concerns have been answered. I therefore lean to reject, and would recommend the authors to resubmit. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit.  
The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The method are simple and effective. The paper is clear and easy to follow. However, the real speedup on CPU/GPU is not demonstrated beyond the theoretical FLOPs reduction. Reviewers are also concerned that the idea of dynamic channel pruning is not novel. The evaluation is on fairly old networks.
Meta Review of Learning from One and Only One Shot  The motivation of this work is to address the problem of learning from very few samples, which is of high relevance for many machine learning problems. The paper proposes an (interpretable) approach for one or few shot learning, which tries to simulate the human recognition ability for “distort” objects. To achieve few shot learning, they first model the topological distance with training data points while minimizing the distortions, to find neighbors that are conceptually similar to the input image. Their experimental results show that this simple method can achieve good performance when only very few samples are available and no pre training is allowed.  All reviewers, including myself, agree that this paper is well motivated, nicely written, and appreciated how they connected ideas from neuro psychology to their ML model, and the novelty is recognized. But there are issues raised by reviewers with the paper that prevent it from meeting the bar for me to recommend it for acceptance at ICLR 2022.  The main issues raised by all reviewers is that the proposed method is only experimentally verified on simple datasets such as MNIST, EMNIST and Omniglot (and to some extent, Quick, Draw!). The authors (to their credit) in the rebuttal noted that the narrative of the paper is to focus on abstract images, and the purpose is more from a scientific investigation perspective (rather than proposing an algorithm that is immediately useful for ML practitioners), and this is a fair point. However, I do believe the issue here is beyond the simple criticism of "it works on MNIST, how about ImageNet?" as I think some reviewers genuinely think there are fundamental aspects of the approach that might prevent it from scaling (as planned in future work, even by the authors in the last section). For instance, as gUCx noted:  1) The proposed approach is based on topological similarity. It seems that it only is suitable for images with simple topological structure, such as the character images. Maybe it is hardly used to classifier complex nature images since we need more information for natural image classification, not only use topological structure.  2) The authors did not provide the experimental comparison with enough training data, such as the whole training set in MNIST. The reviewer wonders about the upper performance of this approach with enough data.  I tend to agree with these points. Non topological similarity can be displayed in abstract images / datasets. Even in "abstract" images, the paper should describe limitations of the approach, and whether it breaks down (like in the "abstract" Quick, Draw! dataset, there are different types of distinct "yoga" poses in the yoga class. And likewise, in the cat or pig class, there are animals with only the heads, and animals with the head and the full body). Conveniently, Quick, Draw! had not been used in any of their classification experiments [1], and only for a simple clustering example.  And for the other points, reporting the terminal MNIST performance will be useful, even if it doesn t look good, so the readers have an idea of the limitations of the approach, where it is good, where it is not, and what needs to be improved. I would love to see improvements (either in the writing or in the experiments) in future work where the paper can effectively convince the readers that the direction has the promise of being able to scale to "real" or "complex" images. (Perhaps even performing the approach on the output of a pre trained self supervised autoencoder on ImageNet, as a method to get "abstract" versions of real photos, like a parallel of the giraffe experiment, though this may distract from the narrative of no pre training). All in all, I don t want to discourage the authors as we are all excited about the direction of this work. I hope to see an updated version of this work published in a future venue, good luck!  [1] https://www.kaggle.com/c/quickdraw doodle recognition
This paper proposes a new algorithm that combines imitation learning and reinforcement learning, based on an extension of the free energy principal. The expert s demonstrations are encoded as a policy prior, and a posterior policy is inferred by maximizing expected rewards. While at a high level this is a promising direction, all the reviewers found the paper difficult to follow and verify its claims. This mostly due to a use of unusual and non conistent notations. The authors are advised to take into account the issues about clarity that the reviewers raised and improve the readability of their paper accordingly.
This paper introduces a methodology for jointly optimizing neural network architecture, quantization policy, and hardware architecture. There are two key ideas:   Heterogeneous sampling strategy to tackle the dilemma between exploding memory cost and biased search.    Integrates a differentiable hardware search engine to support co search in a differentiable manner.  The paper tackles an important research problem and experimental results are good.  There are two related issues with this paper: 1. Comparison to one shot NAS: one shot NAS methods only need to train the super net once and then can be applied to multiple use cases, while the proposed methodology needs to be executed for each use case. 2. It is not clear whether differentiable search is needed for this joint optimization problem modulo existing tools.  Overall, my assessment is that the paper is somewhat borderline and with some more work will be ready for publication. 
Most of the reviewers had serious problems with clarity to start out.  The authors have addressed some, but not all of these problems.   More importantly, there were issues of significance and experimental evaluation. I concur with r4 on the experimental evaluation.  I think if you re going to explicitly specialize toward disentangling affine transform parameters,  that s fine, but then you re in application paper land, and I think there needs to be more of an attempt to show that it will work "in the wild".  For this reason, and for the general reason that reviewers unanimously voted to reject, I am recommending rejection.
This paper proposes an interpretable machine learning method, ProtoAttend, that bases decisions on few relevant "prototypes." The proposed method uses an attention mechanism (possibly sparse, via sparsemax) that relates the encoded representations to samples in order to determine prototypes. The resulting model enables similarity based interpretability, confidence estimation by quantifying the mismatch across prototype labels, and can be used for distribution mismatch detection.   While the proposed model is interesting, the reviewers raised several concerns regarding the choice of prototypes and the evaluation of human interoperation. The paper would benefit from more experiments besides the provided user studies to check if the provided prototypes can help human users correctly guess the model prediction. I encourage the authors to address these suggestions in a future resubmission.
Pros:    The paper is well written  Cons:    Not very novel    Evaluation only on sentiment classification, whereas approaches applicable in broader context exists     There are question re baselines (R3)  Neither reviewer was particularly enthusiastic about the paper, I believe, mostly because of the limited score and novelty.   
This paper is consistently supported by all three reviewers during initial review and discussions. Thus an accept is recommended.
This paper uses the double oracle method from game theory and applies it to GANs.  This idea is interesting and Double Oracle actually seems like a good fit to train GANs. This could lead to interesting results in the future.  Reviewers disagree on the clarity of the paper, probably because the game theory vocabulary is not something that is common among the papers published at ICLR, so extra care should be taken to explain these notions.  They also point out that the method only applies to some GANs and not all (in particular the loss needs to be zero sum). The experimental section is too weak and the metrics used need to be changed. Results are too far from the state of the art to be convincing. The authors based their experimental setup on DCGAN: this is too old, too many improvements have made since then. Other criticisms of the experimental section include: comparison to newer methods must be made, analysis and discussion of the results must be pushed further.  Generally, the average score of the reviews is too low for acceptance. The reviewers agree that the idea is both interesting and pertinent, but this paper cannot be published as it is now. The theoretical part mostly consists in applying double oracle to GANs, and the experimental section is too weak. At least one of these parts (preferably both) must be strengthened for this paper to be impactful.
This paper proposes guiding principles with which to design objective functions for proposal distributions for MCMC. They design one such objective based on GSM (Titsias and Dellaportas, 2019). The two concerns raised by reviewers that resonated the most with me were:    it was not clear that the actual proposed objective was the best way to implement these guiding principles   a weak empirical evaluation that did not consider online tuning and high dim, highly non Gaussian targets.   After rebuttal, revision, and discussion, reviewers felt that the authors did a reasonable job of addressing the issue of online tuning, but very highly non Gaussian targets were not addressed. There was still a sense that the ultimate instantiation of the design principles was a somewhat adhoc loss. Ultimately, I think that this work is just below the bar for acceptance and it can be improved by clarifying the choices made in implementing the objective and some more ambitious experiments.
The submission proposes a model to generate images where one can control the fine grained locations of objects. This is achieved by adding an "object pathway" to the GAN architecture. Experiments on a number of baselines are performed, including a number of reviewer suggested metrics that were added post rebuttal.  The method needs bounding boxes of the objects to be placed (and labels). The proposed method is simple and likely novel and I like the evaluating done with Yolov3 to get a sense of the object detection performance on the generated images. I find the results (qual & quant) and write up compelling and I think that the method will be of practical relevance, especially in creative applications.  Because of this, I recommend acceptance.
Reviewers were concerned about the technical novelty because the two stage sampling strategy is similar to BBN and the decoupling of features and classifiers. Rebuttal addressed some concerns about the experiments, but Reviewers  major concerns remained. 
All authors agree that the relativistic discriminator is an interesting idea, and a useful proposal to improve the stability and sample quality of GANs. In earlier drafts there were some clarity issues and missing details, but those have been fixed to the satisfaction of the reviewers. Both R1 and R3 expressed a desire for a more theoretical justification of why the relativistic discriminator should work better, but the empirical results are strong enough that this can be left for future work.
This paper discusses the (lack of) correlation between the image semantics and the likelihood assigned by flow based models, and implications for out of distribution (OOD) detection.  The reviewers raised several important questions: 1) precise definition of OOD: definition of semantics vs typicality (cf. definition in Nalisnick et al. 2019 pointed by R1) There was a nice discussion between authors and the reviewers. At a high level, there was some agreement in the end, but lack of precise definition may cause confusion. I think adding a precise definition will add more clarity and improve the paper.  2) novelty: similar observations have been made in earlier papers cf. Nalisnick et al. 2018. R3 also pointed a recent paper by Ren et al. 2019 which showed that likelihood can be dominated by background pixels. Older work has shown that the likelihood and sample quality are not necessarily correlated. The reviewers appreciate that this paper provides additional evidence, but weren t convinced that the new observations in this paper qualified for a full paper.  3) experiments on more datasets  Overall, while this paper explores an interesting direction, it s not ready for publication as is. I encourage the authors to revise the paper based on the feedback and submit to a different venue.
The three reviewers seem to reach a consensus that the assumptions made in the paper are too strong and hard to interpret. In particular, R1&R2 made the comments that the generative model for the data by itself uses attention, which seems to be make the comparison unfair. The authors seem to argue that the attention model is still expressive enough, which in the AC s opinion could be true but does not justify the use of the generative model when comparing the sample complexity of the two methods. The reviewers also pointed out a few other limitations of the paper. The AC mostly agrees with the reviewers  points (though perhaps with one or two exceptions.) In summary, I think the assumption of using an attention model itself seems to be a big enough issue that makes the paper not read for publication at ICLR. 
This paper proposes a way to lean context dependent policies from demonstrations, where the context represents behavior labels obtained by annotating demonstrations with differences in behavior across dimensions and the reduced in 2 dimensions. Results are conducted in the domain of StarCraft. The main concerns from the reviewers related to the paper’s novelty (as pointed by R2) and experiments (particularly the lack of comparison with other methods and the evaluation of only 4 out of the 62 behaviour clusters, as pointed by R3). As such, I cannot recommend acceptance, as current results do not provide strong empirical evidence about the superiority of the method against other alternatives.
This paper is about unsupervised learning for ASR, by matching the acoustic distribution, learned unsupervisedly, with a prior phone lm distribution. Overall, the results look good on TIMIT. Reviewers agree that this is a well written paper and that it has interesting results.  Strengths   Novel formulation for unsupervised ASR, and a non trivial extension to previously proposed unsupervised classification to segmental level.   Well written, with strong results. Improved results and analysis based on review feedback.  Weaknesses   Results are on TIMIT   a small phone recognition task.   Unclear how it extends to large vocabulary ASR tasks, and tasks that have large scale training data, and RNNs that may learn implicit LMs. The authors propose to deal with this in future work.  Overall, the reviewers agree that this is an excellent contribution with strong results. Therefore, it is recommended that the paper be accepted.
Reviewers viewed the proposed approach to image retrieval as both simple and effective, the manuscript as well written and well motivated, and the presented experiments as relatively compressive (spanning three datasets). There was some discussion about novelty   all reviewers viewed the approach as simple, but one had concerns about the approach s novelty relative to past work. This concern, as well as some concerns about experimental evaluation, were adequately addressed in author response.
One might assume that the k means problem has already been beaten to death, but this paper shows there are still remaining questions. And rather interesting ones at that, with a novel angle of having additional help from a prediction algorithm of cluster memberships. This connects to learning augmented algorithms research.  The reviewers agreed that the problem is interesting and gives a novel angle, and the interestingness stems from novelty, and the ability to "escape" from NP hardness.  The reviewers and authors had nice discussions about details and conclusions, on how limiting is it that the authors focus on reasonably accurate predictors, for instance, and where could the predictors come from. This is a good paper, and hopefully the discussion helped make it even better.
This paper introduces a new method for fine tuning large language models, which is lightweight since it only adds a small amount of parameters, while keeping the original parameters frozen. The main idea is to add a low rank matrix which is learned during fine tuning to the original weight matrices of the model, which are frozen. The reviewers agreed that the method is simple, original and well motivated. Moreover, it compares well compared to other fine tuning baselines, such as adaptors or full fine tuning. For these reasons, I recommend to accept this work to the ICLR conference.
The paper studies population based training for MARL with co play, in MuJoCo (continuous control) soccer. It shows that (long term) cooperative behaviors can emerge from simple rewards, shaped but not towards cooperation.  The paper is overall well written and includes a thorough study/ablation. The weaknesses are the lack of strong comparisons (or at least easy to grasp baselines) on a new task, and the lack of some of the experimental details (about reward shaping, about hyperparameters).  The reviewers reached an agreement. This paper is welcomed to be published at ICLR.
The paper provides a thorough study of the evolution of Hessian depending on a wide variety of aspects such as initialization, architectural choices, and common training heuristics. The paper makes a number of interesting observations. Some of them are not really new but overall, the experimental evaluation of the paper makes it a valuable resource for the community.  The reviewers are overall quite positive. One reviewer notes that more investigation of the behavior of batch normalization is required. I encourage the author to address this concern in the final manuscript. There is a lot of recent work on batch normalization that might be worth discussing, e.g.: Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs Jonathan Frankle, David J. Schwab, Ari S. Morcos  Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, Aurelien Lucchi  A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent Yongqiang Cai, Qianxiao Li, Zuowei Shen
This paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pre trained on some dataset and subsequently fine tuned on the target dataset. The authors theoretically analyse two layer fully connected networks and provide an extensive empirical evaluation arguing that the loss landscape of  appropriately pre trained networks is easier to optimise (improved Lipschitzness).  Understanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. However, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. As a result, I will recommend rejecting this paper in the current form.  Nevertheless, as the problem is extremely important I encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.
The paper benchmarks three strategies to adapt an existing TTS system (based on WaveNet) to new speakers.  The paper is clearly written. The models and adaptation strategies are not very novel, but still a scientific contribution. Overall, the experimental results are detailed and convincing. The rebuttals addressed some of the concerns.  This is a welcomed contribution to ICLR 2019.
This paper proposes a novel neural voice camouflage method that learns predictive attacks without any constraints about input and output. It is general, robust, and real time that could be used in a real world scenario. The experiments are solid, the in depth analyses are convincing.
The paper is an interesting take on representation learning, using (prior) tasks to determine which information is important. The problem setting is somewhat difficult to pin down, so that that finding the correct comparisons is not obvious and opinions differ on many details of the setup. However, this is not a fault of the paper; it is a general problem the further one moves away from clean settings like classical supervised learning.  There was a lengthy and detailed back and forth between the authors and reviewers, where the authors clarified most of the points raised, extended their results, resulting in one reviewer switching from reject to accept.
The paper proposes to minimize the loss while regularizing its sharpness: so that the minimum will lie in a region with uniformly low loss. The reviewers uniformly appreciated the paper. They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version.  
The authors combine TAGI with Q learning to create an approximate Bayesian Q learning algorithm. They evaluate their approach and show it has comparable performance to DQN.  All of the reviewers were positive about the potential of the paper. Unfortunately, the paper suffers from lack of clarity, of motivation, and comparison to relevant approaches. All of the reviewers brought up nearly the same concerns and I agree with these concerns. The authors have not address them and the reviewers do not think this paper is ready for publication at this time. I agree and recommend rejection.  Evaluating TAGI DQN is a valuable contribution, but alone it is not sufficient. The reviewers have made many suggestions on how to improve the paper, and I hope the authors follow up on these suggestions.
I thank the authors for their submission and participation in the author response period. The updated experiments are appreciated. However, after discussion all reviewers unanimously agree that the paper is not ready for publication and encourage resubmission to another venue. In particular, R2 and R3 have raised concerns regarding additional widely available baselines that need to be evaluated against and that the rebuttal has not addressed. I agree with this assessment, and thus recommend rejection.
The paper advocates neuroscience based V1 models to adapt CNNs.  The results of the simulations are convincing from a neuroscience perspective.  The reviewers equivocally recommend publication.
In this paper they adapt unsupervised contrastive learning to the problem of representation learning for proteins from 3D structure, using sub structure sampling for the data transformation. The reviewers have concerns that the application tasks used for evaluation are not particularly impactful tasks, and that additionally, they are likely to not require protein representations that require more nuanced information. There are also concerns about the clarity of the manuscript, and novelty of the technical approach.
### Description  The paper enhances flow based generative models by putting them into a coarse to fine multi resolution framework. The key technical challenge as I understand is designing up scaling conditional flow modules. Since the operation needs to be invertible, the paper carefully designs what degrees of freedom need to be injected in addition to the low resolution image to compose a higher resolution one.  ### Decision The paper received 5 expert and rather detailed reviews. I have read and understood the paper and all reviews. Reviewers remark that the paper is well written, addresses a challenging problem. However reviews were in a consensus on that the contribution of the paper is marginal. The average score was 4.4. The authors did not respond to reviewers  and did not update the paper. There was no post rebuttal discussion and or additional feedback from reviewers. Therefore, must reject.  ### Comments I have only minor comments on the writing and organization of the paper. There are many self repetitions in the text, restating what was already said above in same or very similar sentences. Some questions studied in appendices are not presented in the main papar.
There are numerous known methods for memory reduction used in CNNs. This paper takes two such quantization (Q) and random projection (RP) and applies them to GNNs.  This is a novelty, but I agree with the reviewers: on its own this novelty would not be "surprising" enough to report at ICLR.    The paper further goes to show empirically that these methods, when applied to a reasonable set of datasets, do indeed produce their predicted memory reductions (unsurprising) with a small ($\approx 0.5\%$) drop in accuracy (surprising, in the sense of not being something one could predict without doing the experiment).  All of the above is in one sense "just engineering", with only a small inventive step.  Any real world deployment of GNNs would, if an army of engineers were available, naturally implement quantization and RP in order to see what kind of improvements they might make.  This would be just two more hyperparameters (R,B) to add to the sweep, and the deployment would vary them until the required accuracy was achieved in the minimum time (OOM is a red herring   one would vary batch size, other compression, or ship values to CPU in order to make progress).  However, "simply adding two more hyperparameters" is a significant increase in the deployment burden, which is where the paper s third contribution comes in: the theoretical analysis of the effects of the two processes, with straightforward but nonobvious calculations of the effect on gradient variance of the two processes, and, usefully, their interaction.  The value of this theory is twofold: first, it gives us new tools to analyse such processes; and second, it allows us to be much more judicious in the selection of these hyperparameters.  In all, the reviewers  objection of no great novelty in porting ideas from CNN  to GNN is sustained; but the authors  claim as to the value of the theory is sustained, and no reviewer provides prior art to dispute the novelty of the theory calculations.  The revised paper has already expanded the key sections in Appendix E, and added welcome experiments which strengthen the paper.  I would encourage a final copy (and certainly the poster presentation) to emphasize some of the insights over the raw experimental numbers.  As the authors hint, those numbers are subject to vagaries of what PyTorch happens to implement, while the underlying analyses are a little longer lasting.  Some other comments:   A lot of discussion time was spent on the question of whether 0.5% is negligible.  This is entirely application dependent, and is part of the hyperparameter/architecture tuning process.      the extra time overhead of swapping "can go up to 80%, which is not feasible in practice".   Not so: if choosing between OOM or 1.8x slowdown, I will of course choose the latter.     "for a fair comparison, we do not change any hyperparameters"   Again, not relevant: in a real application (which is where this paper contributes), we of course change the learning rate when batch size changes.     "the accuracy drop of sampling may be greater than EXACT"   Again, whether that drop is too much depends entirely on the actual application.    And please do take a look at typos/grammar/English etc.
This paper addresses an important problem, quantizing deep neural network models to reduce the cost of implementing them on hardware such as FPGAs without severely affecting task performance. The approach explored in the paper combines three ideas: (1) injecting noise into the network to simulate the effects of quantization noise, (2) a smart initialization of the parameter and activation clamping along with learning of the activation clamping using the straight through estimator, and (3) a gradual approach to quantization. While the reviewers agreed that the problem is important, they raised concerns about the novelty of the proposed approach and the quality of the experiments. The authors did not respond to the reviewers in the discussion period, and did not revise their submission.
The reviewers in general agree that the proposed complex valued DP method is interesting and novel. However, there are two key concerns due to which the paper might not be ready for publication at ICLR: a. the key technical contribution of the work is not clear, as the methods seem relatively straightforward extension of real valued DP methods to complex valued domains.  b. More importantly, the experimental results (and hence the motivating applications) are not convincing and do not strongly support the claims of i) complex data provides more flexibility and hence provide better model, ii) proposed method is accurate.  For example, the accuracy numbers for SpeechCommands even without DP seem quite low. For example, standard methods like matchboxnet for keyword detection have accuracy numbers in the range of 97%. While the work considers a subset of keywords, but it would be important to show how the standard methods work on this dataset. If the gap is this large, then the case for using complex valued datasets itself is weak.  Similarly, on CIFAR10 it seems like that the considered architecture is quite poor as the accuracy is just ~80% while most standard architectures get >93% on the dataset. So the experiment claims of the paper might not hold for practically relevant architectures.
Pros   A novel way to incorporate LM into an end to end model, with good adaptation results.  Cons   Lacks results on public corpora or results are not close to SOTA (e.g., for Librispeech).  Given the reviews, it is clear that the experimental evaluations can be improved. But the presented approach is novel and interesting. Therefore I am recommending the paper to the workshop track.
This paper is proposed to deeply investigate the hot refresh model upgrades of image retrieval systems. The hot refresh model is very useful since the model can be quickly updated after the gallery is backfilled. To address the model regression with negative flips, this paper introduces a Regression Alleviating Compatible Training (RACT) method by reducing negative flips. The proposed method has been verified on the large scale image retrieval benchmark such as Google Landmark. The key contribution of this paper is the new setting targeting an important application in real world image retrieval systems. However, some of the technical details are not fully explained. Despite these minor concerns, the AC will rate this paper as a poster acceptance based on the overall contributions.
The paper proposed weighted MOCU, a novel objective oriented data acquisition criterion for active learning. The propositions are well motivated, and all reviewers find the analysis of the drawbacks of several popular myopic strategies (e.g. ELR tends to stuck in local optima; BALD tends to be overly explorative)) interesting and insightful. Reviews also appreciate the novelty of the proposed weighted strategy for addressing the convergence issue of MOCU based approaches. Overall I share the same opinions and believe the paper offers useful insights for the active learning community.  In the meantime, there were shared concerns among several reviewers in the readability (structure and intuition), lack of empirical results on more realistic active learning tasks, and limited discussion on the modeling assumptions. Although the rebuttal revision does improve upon many of these points, the authors are strongly encouraged to take into account the reviews, in particular, to further strengthen the empirical analysis and discussions, when preparing a revision. 
Two reviewers are negative on this paper while the other reviewer is slightly positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.
The paper proposes a multi agent RL framework that make decisions in a more human like manner by incorporating rational inattention. The approach is evaluated on two game theoretic problems. The reviewers agree that the topic of the paper is interesting. However, there are concerns about the significance of the proposed approach. As the method incorporates human inspired limitations, it s aim is not to outperform SOTA RL methods on regularly considered domains; at the same time, as the approach is only evaluated on two simulation based tasks, it is unclear how it would perform in more realistic scenarios that may benefit from human like decision making. For these reasons, I recommend rejection.
There is a broad consensus that this paper explores an interesting and novel problem space. Nonetheless, in their initial assessment, the reviewers pointed to a few limitations of the paper including lack of strong baselines, lack of an ablation study, and weaker results according to the HIT@10 metric.   The authors provided an improved version of their paper as a response. The new paper added two baselines, is better written, and justifies some of the HIT@10 results (basically, the metric is biased for this task).  After discussion, the reviewers find that the contribution of the current manuscript falls short of the acceptance threshold.  In particular, the reviewers find that: 1) this contribution is for a specific domain of recommender systems, an area of interest, but perhaps only relevant to a subset of the ICLR community; 2) while more recent baselines helped, there has been lots of more recent work on collaborative filtering models for recommender systems over the last few years (the Wide&Deep baseline is from 2016); 3) since some of the usual recommender systems  metric does not seem appropriate here, why not suggest new ones (or propose a slightly different evaluation protocol); 4) the proposed model is useful, but somewhat incremental given prior work. All in all, while any of these limitations on their own might not have been sufficient to warrant rejection, I find that their combination does.   Given the interest in this new task, I do strongly encourage the authors to pursue their work. I also find that the qualitative study propsoed by Reviewer 4 could add another interesting angle to this paper (I also imagine that it might not be that easy to carry out). 
This paper proposed a training free method to detect noisy labels based a given pretrained representation. The author suggested two methods based on either voting or ranking to filter noisy instances with corrupted labels without training. Reviewers generally agree that the technical novelty and contributions are only limited or marginally significant. Also experiments are not very convincing as there lacks of extensive comparisons with many existing methods for either noisy label removal or learning with noisy labels and the datasets are somewhat simple and not complex enough.
This paper studies the problem of training binary neural networks using quantum amplitude amplification method. Reviewers agree that the problem considered is novel and interesting. However the consensus is that there are only few experiments in the current paper and the paper needs more experiments on different datasets with comparisons to proper baselines. Reviewers opined that the paper was not so easy to follow initially,  though later revisions may have somewhat alleviated this problem.
There is insufficient support to recommend accepting this paper.  The reviewers unanimously criticize the quality of the exposition, noting that many key elements in the main development and experimental set up are not clear.  The significance of the contribution could be made stronger with some form of theoretical analysis.  The current paper lacks depth and insufficient justification for the proposed approach.  The submitted comments should be able to help the authors improve the paper.
The reviewers and AC appreciate the improvements made to the paper and thank the authors for engaging with the reviewer questions. There are now quite a few neuro symbolic approaches, and they are all rather similar. This places a larger burden on the authors to have a thorough and systematic experimental comparison and related work discussion. Reviewers also believe the clarity of the paper should still be improved. The revised paper already made good progress in addressing these concerns, yet the reviewers still believe the paper would strongly benefit from another round of revisions.
This paper proposes to learn clinical prototypes via supervised contrastive learning to facilitate the reliable retrieval of clinical information and clustering in large datasets. The presentation of the paper could be substantially improved – e.g., the overview and motivation of the paper, the definition of clinical prototypes, selection of certain evaluation criteria, clarification of terminology in equations, the description of the motivations and settings of the experiments, etc.  In addition to the need to substantial improvement in clarity, major concerns include lack of comparison with more supervised approaches and discussion of relevant literatures raised by reviewers.  
The contributions of this paper are twofold: 1) datasets of tasks are provided, and 2) based on the datasets and hyperparameter lists on the datasets, a transfer learning approach for hyperparameter optimization (HPO) is proposed. Many reviewers positively evaluated the idea and approach discussed in this paper. However, the common concern of multiple reviewers and area chair is that it is not clear whether the provided datasets and their hyperparameter lists are generally applicable to other practical problems. Since there is no discussion on how the datasets are constructed, it is not clear whether they have generally or not. In addition, the comparison with existing HPO approaches is not sufficiently made, and it is not clear whether the performance of the proposed method is advantageous over existing methods. Overall, although the idea is very interesting and potentially useful, I cannot recommend the acceptance in its current form due to the lack of evidence on its generality. 
 The paper presents a side channel attack in a scenario where the attacker is able to place a induction sensor near the power cable of the victim s GPU. The authors train a neural network to analyse the magnetic flux measured by the sensor to recover the structure (layer type and layer parameters) of the target neural network. The authors also show that for a wide range of target network structure, by training a network with the inferred structure, they produce adversarial examples as effective as a white box attack.  The points raised by the reviewers were the following: 1) the result that this type of side channel attack works is interesting, 2) the practicality of the attack is unclear because the attacker needs hardware access to the victim s GPU, 3) the ML contribution is not really clear and a venue on cyber security might be more appropriate.   Side channel attacks on deep neural networks can be of relevance to ICLR (as pointed to by the authors by the ICLR papers/submissions on system side channel attacks). Nonetheless, I tend to agree with R1 and R2 that the ML contribution is limited (either in terms of application of ML or methodology), and the concerns of practicality of the approach make me lean towards rejection.
The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, i.e. by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. All reviewers and AC agree that this work is clearly of interest to ICLR, however the reviewers have noted the following potential weaknesses: (1) presentation clarity   see R3’s detailed suggestions e.g. comparison to Dwork et al, see R2’s comments on how to improve, (2) empirical evaluations   see R1’s question about using more complex models, see R3’s question on the usefulness of the word embeddings.  Pleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept. All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted   congratulations to the authors! The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision.
The authors propose a framework which combines pretext tasks and data augmentation schemes with the goal of improving robustness of image representations. The authors show that this approach empirically can lead to increased accuracy on both corrupted and uncorrupted data simultaneously. Furthermore, the authors propose a regularization procedure which can be used to maintain a robust representation during transfer to arbitrary downstream tasks.   The studied problem is significant and highly relevant to the ICLR community. The reviewers agreed that the work is timely, and appreciated the clarity of exposition. At the same time, the reviewers remained in disagreement in terms of novelty and significance of the results   the proposed method is seen as a clear incremental application of existing techniques in the self supervised setting. The authors argue that it was not clear that such augmentations would improve the robustness as well as accuracy, but these methods were developed and optimised to improve robustness. In fact, in [1] the authors conclude that “...today s supervised and self supervised training objectives end up being surprisingly similar” as well as point out that SimCLR is more robust than competing self supervised methods. Hence, establishing that there is indeed some empirical benefit is a step in the right direction, but not sufficient to meet the bar of acceptance. Furthermore, given the recent trend of scaling up existing approaches, in particular in terms of the neural architectures and the batch sizes, the computational costs of the proposed regulariser in Eq (4) coupled with the additional hyperparameter to optimize make the approach less practical and general than claimed. In addition, the reviewers pointed out the lack of comparison to a proper baseline, as well as the issue with hyperparameter selection for the baseline after the author response. Finally, given access to additional data augmentations and one more hyperparameter to tune the results should substantially outperform the baselines.  For the reasons outlined above and the incremental nature of the work, I will recommend rejection. That being said, this was a borderline case, and I urge the authors to carefully revise the manuscript with the received feedback.  [1] https://arxiv.org/abs/2010.08377 
This work proposes a new adaptive method for solving certain min max problems.  The reviewers all appreciated the work and most of their concerns were addressed in the rebuttal. Given the current interest in both adaptive methods and min max problems, this work is suited for publication at ICLR.
Four reviewers have reviewed this paper and after rebuttal, they were overall positive about the proposed idea. We congratulate authors on the paper.
This paper presents two novel approaches to provide explanations for the similarity between two samples based on 1) the importance measure of individual features and 2) some of the other pairs of examples used as analogies.  The proposed approach to explain similarity prediction is a relatively less explored area, which makes the problem addressed and the proposed method unique. However, reviewers expressed concerns about evaluation methods and there were some concerns about the design choices that were not well motivated. The major issue is, as pointed out by the majority of the reviewers, the evaluation methods. Given the paper, reviews, and responses of the authors and the reviewers, it appears that there is certainly room for improvement for more convincing evaluation methodologies to convince a cross section of machine learning researchers that the proposed approach advances the field. Overall, this is a good paper, but appears to be borderline to marginally below the threshold for the acceptance.
The paper proposed Channel Equilibrium (CE) to overcome the over sparsity problem in CNNs using  BN+ReLU . Experiments on ImageNet and COCO show its effectiveness by introducing little computational complexity. However the reviewers pointed a number of problems in the writing and the clarity of the paper. Although the authors addressed all the se concerns in details and agreed to make revisions in the paper, it s better for the authors to submit the revised version to another opportunity.
This paper compresses neural networks via so called Sparse Binary Neural Network designs. All reviewers agree that the paper has limited novelty. Experiments are only performed on small datasets with simple neural networks. However, even with toy experiments, results are very weak. There is no comparison with the SOTA. Numerous related works are missed by the authors. Besides, the paper is poorly written, and there are misleading notations.
The authors analyze the natural gradient algorithm for training a neural net from a theoretical perspective and prove connections to the K FAC algorithm. The paper is poorly written and contains no experimental evaluation or well established implications wrt practical significance of the results.
The paper investigates how to distill an ensemble effectively (using a prior network) in order to reap the benefits of uncertainty estimation provided by ensembling (in addition to the accuracy gains provided by ensembling).   Overall, the paper is nicely written, and makes a valuable contribution. The authors also addressed most of the initial concerns raised by the reviewers. I recommend the paper for acceptance, and encourage the authors to take into account the reviewer feedback when preparing the final version.
AR1 is concerned about the poor organisation of this paper.  AR2 is concerned about the similarity between TRL and TR. The authors show some empirical results to support their intuition, however, no theoretical guarantees are provided regarding TRL superiority.  Moreover,  experiments for the Taskonomy dataset as well as on RNN have not been demonstrated, thus AR2 did not increase his/her score.  AR3 is the most critical and finds the clarity and explanations not ready for publication.  AC agrees with the reviewers in that the proposed idea has some merits, e.g. the reduction in the number of parameters seem a good point of this idea. However, reviewer urges the authors to seek non trivial theoretical analysis for this method. Otherwise, it indeed is just an intelligent application paper and, as such, it cannot be accepted to ICLR.
The authors extends the transformer to multivariate time series. The proposed extension is simple, and lacks novelty. Some design decisions of the proposed method should be better justified. Similar works that also use the transformer for timeseries are not compared.  Experimental results are not convincing. The settings are unclear, and the selection of datasets needs more justifications. Some important experiments are missing.  Finally, writing can also be improved.
This paper proposes a measure of task complexity based on a decision DAG like "encoder" where we iteratively branch on some test on the input and the selection of future tests depends on the answer to previous tests until we reach a terminal node in the DAG.  We require that if $x$ and $x $ reach the same terminal node then $P(y|x)   P(y|x )$.  The complexity of the task (the complexity of the distribution $p(x,y)$) is the minimum over all such DAGs of the expected depth of the terminal node for $x$ when drawing $x$ from the marginal $p(x)$.  The reviewers are not enthusiastic and I agree.
The paper is extremely well written with a clear motivation (Section 1). The approach is novel. But I think the paper s biggest strength is in its very thorough experimental investigation. Their approach is compared to other very recent speech discretization methods on the same data using the same (ABX) evaluation metric. But the work goes further in that it systematically attempts to actually understand what types of structures are captured in the intermediate discrete layers, and it is able to answer this question convincingly. Finally, very good results on standard benchmarks are achieved.  To authors: Please do include the additional discussions and results in the final paper. 
The reviewers recommended rejection. There was no reply from the authors. The main weaknesses are:   No experiment on real life dataset (only simulated)   Unsubstantiated claims about the literature    No discussion on the time complexity   Incremental contribution
The paper proposed the use of dynamic convolutional kernels as a way to reduce inference computation cost, which is a linear combination of static kernels and fused after training for inference to reduce computation cost. The authors evaluated the proposed methods on a variety models and shown good FLOPS reduction while maintaining accuracy.   The main concern for this paper is the limited novelty. There have been many works use dynamic convolutions as pointed out by all the reviewers. The most similar ones are SENet and soft conditional computation. Although the authors claim that soft conditional computation "focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations", the methods are pretty the same and moreover in the abstract of soft conditional computation they have "CondConv improves the performance and inference cost trade off".
The main consensus among the reviewers was that although the approach is interesting, this submission suffers from two main weaknesses:    The methodology is not very novel, and the proposed parts of the method not well justified (in particular regarding the interplay of a differentiable sorting approach and of the random choice of k)    The results, compared to a standard cross entropy loss are not very convincing: there does not seem to be a statistically significant advantage.
This paper focuses on answering complex logical queries over an incomplete KG and use neural networks to do so flexibly handling multiple operations from FOL. Overall reviews agree that empirical performance is impressive. One reviewer gave a strong accept, one leaning to accept and two leaning to reject. Overall, the reviewers who are leaning to reject had mostly clarity issues which seem to have been addressed by the authors (without response from reviewers).  Given this I recomment acceptance.
A nice paper, but quite some unclarities; it s unclear  in particular if the paper improves w.r.t. SOTA.  Esp. scaling is an issue here. Also, the understandability is below par and more work can make this into an acceptable submission.
The paper proposes a deep learning framework to solve large scale spectral decomposition.  The reviewers and AC note that the paper is quite weak from presentation. However, technically, the proposed ideas make sense, as Reviewer 1 and Reviewer 2 mentioned. In particular, as Reviewer 1 pointed out, the paper has high practical value as it aims for solving the problem at a scale larger than any existing method. Reviewer 3 pointed out no comparison with existing algorithms, but this is understandable due to the new goal.  In overall, AC thinks this is quite a boarderline paper. But, AC tends to suggest acceptance since the paper can be interested for a broad range of readers if presentation is improved.
The key motivation for the work is producing both an efficient (parallelizable / fast) and accurate reading comprehension model. At least two reviewers are not convinced that this goal is really achieved (e.g., no comparison to hierarchical modeling, performance is not as strong).   I also share concerns of R1 that, without proper ablation search and more careful architecture choice, the modeling decisions seem somewhat arbitrary.  + the goal (of achieving effective reading comprehesion models) is important   alternative parallelization techniques (e.g., hierarchical modeling) are not considered   ablation studies / more systematic architecture search are missing   it is not clear that the drop in accuracy can be justified by the potential efficiency gains (also see details in R3  > no author response to them) 
The reviewers agree this paper is not good enough for ICLR.
The paper presents an approach to select visual tokens in images and reorganize them for the object classification, within Transformers. All four reviewers find the paper interesting and novel, and they are also very positive about the experimental results. The authors also addressed minor concerns of the reviewers successfully through the discussion phase, clarifying details and adding experiments.  We recommend accepting the paper.
This manuscript proposes and analyzes a federated learning procedure with more uniform performance across devices, motivated as resulting in a fairer performance distribution. The resulting algorithm is tunable in terms of the fairness performance tradeoff and is evaluated on a variety of datasets.  The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on fairness in federated learning. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. In reviews and discussion, the reviewers noted insufficient justification of the approach and results, particularly in terms of broad empirical evaluation, and sensitivity of the results to misestimation of various constants. In the opinion of the AC, while the paper can be much improved, it seems to be technically correct, and the results are of sufficiently broad interest to consider publication.
The reviewers agree that the paper is below threshold for acceptance in the main track (one with very low confidence), but they favor submitting the paper to the workshop track.  The paper considers policy gradient methods for two player zero sum Alternating Markov games.  They propose adversarial policy gradient (fairly obviously), wherein the critic estimates min rather than mean reward.   They also report promising empirical results in the game of Hex, with varying board sizes.  I found the paper to be well written and easy to read, possibly due to revisions in the rebuttal discussions.  The reviewers consider the contribution to be small, mainly due to the fact that the key algorithmic insights were already published decades ago.  Reintroducing them is a service to the community, but its novelty is limited.  Other critiques mentioned that results in Hex only provide limited understanding of the algorithm s behavior in general Alternating Markov games.  The lack of comparison with modern methods like AlphaGo Zero was also mentioned as a limitation.  Bottom line: The paper provides a small but useful contribution to the community, as described above, and the committee recommends it for workshop. 
This paper studies the transferability of adversarial attacks in deep neural networks. In particular, it proposes the reverse adversarial perturbation (RAP) method to boost attack transferability by flattening the landscape of the loss function.  The reviewers acknowledge the strengths of the paper, which include effectiveness of the simple RAP method proposed and the extensive experimentation presented.  However, a number of outstanding concerns still remain. Some of them include the technical novelty of the paper, insufficient theoretical justification of the proposed method, lack of grounded justification between flatness of the loss landscape and model generalization under the specific context of attack transferability, similarity of the optimization problem with some existing work, potential difficulties of the min max attack generation problem, among others.  As it stands, this is a borderline paper that is reasonably good, but not great. Addressing the outstanding concerns will make the paper more ready for publication in ICLR.
The paper proposes a graph aligning approach generating rich and detailed labels given normal labels. Authors cast the problem in a domain adaptation setting, considering a source domain where "expensive" labels are available, and a target domain where only normal labels are available. The application scenario is the prediction of chemical compound graphs from 2D images, where a fully mediating layer is introduced to represent using a planar embedding of the chemical graph structure to be predicted.   The paper received ratings all below threshold. The main issue transversal to all reviewers relate to clarity of the presentation. Clear motivations for some of the adopted choices of the method and of the experimental procedure were also missing. In particular, missed to provide the clear usefulness of the main paper s contribution, i.e., to neatly show the importance of the mediating layer (ref. R4, R2).  The lack of important details in the method description and experimental results were also deemed a major shortcoming: cost of the optimization, model generalization not discussed, contradictory results on the different datasets considered, comparative analysis, partial ablation, are among the main quoted remarks.   Authors  rebuttal is carefully provided in general, but several issues are still remaining.  Hence, overall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021. 
The authors propose a new method of securely evaluating neural networks.   The reviewers were unanimous in their vote to accept. The paper is very well written, the idea is relatively simple, and so it is likely that this would make a nice presentation.
Two reviewers are borderline and one recommends rejection. The main criticism is the simplicity of language, scalability to a more complex problem, and questions about experiments. Due to the lack of stronger support, the paper cannot be accepted at this point. The authors are encouraged to address the reviewer s comments and resubmit to a future conference.
All reviewers agreed that analysis of PPO is interesting.  During the discussion, however, there was an agreement that the current work is too thin in novelty and contribution: it provides only convergence analysis under very strong assumptions, and heavily builds on techniques from prior works. Meanwhile, for conventional policy gradient, recent works provided convergence rates. As one reviewer pointed out   this work does not further our theoretical understanding on why PPO is better than vanilla policy gradient, as all the established results hold for policy gradient, even with less assumptions. I encourage the authors to strengthen their paper by relaxing Assumption 4 (perhaps based on the robust classification idea raised in the discussion), and by further providing rate results.
The main detractor of this paper feels that the paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017).  However, several other reviewers, who had more engagement in the discussion, were strong supporters. Having looked at the paper myself I thought the selection of experimental problems undermined the results.  Experiments are most compelling when many unaffiliated groups compete on the same benchmarks.  But the basic idea of integrating HER with AlphaZero, and a reasonable attempt at this, seems to be interesting enough to warrant a poster.
This work proposes a branch and bound framework for adversarial attacks, based on the MIP formulation of attacking against ReLU networks. It adopts several heuristic tricks to accelerate the attack efficiency, and shows better attack performance on hard examples, compared to off the shelf MIP solvers.   The main concerns in the first round reviews include:  1. The limited novelty, since the problem and formulation is not novel, and the proposed method is the combination of several heuristic tricks, without theoretical analysis.  2. The experiments are insufficient, and only MIP solvers are compared, while other attack methods and the ablation study of different tricks are not presented.  3. The efficiency is higher than off the shelf MIP solvers, but significantly lower than other adversarial attack methods.  The authors made great efforts to respond these concerns, such as adding some baseline attack methods and ablation studies. Most reviewers appreciated the authors  efforts and raised their initial score. However, after reading the revised manuscript, reviews and responses, I think many serious issues still exist.  1. Since the proposed method is only applicable to the MIP formulation of attacking ReLU networks, it could not become a new baseline to attack mainstream deep neural networks, which significantly restrict its practical contribution to the community. And, it didn t provide novel theoretical tools or better theoretical results of analyzing the robustness of ReLU networks. It just reduce the gap to the verified robustness of existing verification methods. Thus, I cannot find significant contributions to the community of adversarial examples, from both practical and theoretical perspectives.  2. The experiments are still insufficient. There have been massive advanced white box and black box attack methods. The added FAB, square (black box), DIFGSM, MIFGSM (white box), are popular methods, but not SOTA methods. Besides, only small images of MNIST and CIFAR 10 are tested, while large scale images like ImageNet are not tested. It has been observed in many works, and according to my own experience, the attack performance on between small images (MNIST and CIFAR 10) and large images (ImageNet) is not always consistent. And, the low efficiency weakness (compared to other attack methods) of the proposed method may be further highlighted.   It is not easy to decide to reject a submission with average score 7. The authors s efforts during their submission and the responses are greatly appreciated. However, I would like to accept works that can bring in real contributions to the research community. Hope the reviews and meta review could be helpful to further improve this work.
The paper presents an extensive empirical evaluation of several loss functions and regularization techniques used in deep networks. The authors conclude that the classical softmax is significantly outperformed by the other approaches, but there is no clear winner among them. Moreover, the authors have noticed two interesting facts, (1) the choice of loss function affects only upper layers of neural networks with the lower layers being very similar to each other, (2) losses that result in greater class separation also result in higher accuracy, but their features are less transferable to other tasks.  I agree with the authors that the comments of Reviewer 2 are shallow and not informative. Therefore, they were not taken into account in making the final decision and, as AC, I read the paper very carefully. Regarding Reviewer 4, however, I found his comments to be valid. There is a message that the authors want to communicate, and the reader that needs to decode this message using a noisy channel. Therefore, I encourage the authors to accordingly revise the paper to make the message much clearer.   The experimental papers that compare a wide spectrum of methods are always hard to judge and this judgement is often very subjective. There are several seminal papers of this type, but not so many for several reasons. I agree with the authors that such studies are very valuable and give evaluation being not biased by authors of a given method. They are also very time  and resource consuming. But there should be a general consensus how such an experiment should be performed. The authors of the particular methods should also be able to give right feedback to make their methods to be run appropriately. Therefore, there exist several websites and initiatives that try to fulfill these requirements. As said above, any paper of this type will be judged very subjectively.    The discoveries made by authors should also be presented in a different way. One should start with a hypothesis that, for example, the lower layers are not affected by the loss function and then perform appropriate theoretical and empirical studies to verify the hypothesis. The same applies to the other discovery. In that way the message of the paper would be much clearer. I suppose that analysis of each of the discoveries deserves its own paper.   
In this paper, the authors proposed a solution to the problem of multi source domain adaptation. All the reviewers have two concerns: 1) the technical contribution/novelty is limited, and 2) the experimental results are not convincing. Therefore, this paper does not meet the standard of being published in ICLR. The authors are encouraged to improve this work by addressing the issues raised by the reviewers. 
This paper presents a method named LowKey, which is designed to protect user privacy. This is done by taking advantage of adversarial attacks to pre process facial images against the black box facial recognition system in social media, yet the processed facial images remain visually acceptable. The paper experimentally illustrates that it is effective against two existing commercial facial recognition APIs.   The reviewers unanimously agree that this is an interesting and important problem, and recommend the paper for acceptance. The ACs agree.
This paper proposes a Bayesian non parametric method for task incremental continual learning. It is more general than previous work in that it considers the network structure as a random variable and works for both supervised and unsupervised settings. Experimental results show that the proposed method outperforms prior work in the proposed tasks.  Pros:   It is well motivated.   It s theoretically sound.   It can do task inference.   It outperforms other methods in the proposed tasks.  Cons:   The experimental setup was not very challenging, because the dataset(MNIST) was simple and the network was shallow.   There was no ablation study to analyze the contributions of the algorithm to the performance.   There is not enough experiments to support the advantage of task inference.   The paper did not compare with the SOTA task incremental learning algorithms HAT and DEN.  The main concerns of reviewers are on the experimental section as listed in cons and the difference from previous work. The authors explained that their method has the advantages over previous work that it can do task inference. R3 agreed with the advantage and suggested more experiments in this direction should be performed. The authors conducted additional experiments suggested by the reviewers including comparison with HAT. They also uploaded a revised version to incorporate the comments from the reviewers.  I think the paper is well motivated and the idea of applying Bayesian non parametric for continuous learning is interesting. It could potentially motivate interesting future work on CL. However, the main advantages/contributions are not well presented and supported by the experiments. So at present time I believe there is much room for the authors to improve their paper before publication. I hope that the authors will be able to address the feedback they received to make this submission get where it should be.
This paper introduces a self training strategy for semi supervised learning for few shot sequence learning.   It builds on ideas from an existing work on robust deep learning that adaptively reweights examples for learning to reduce impact of noisy examples, here the noisy examples are introduced to the student network training by the teacher network.  Two main novel points, one is on  selectively constructing the validation set used for adaptive reweighting.  Another idea is to move from the sentence level reweighting to token level reweighting.   The paper shows strong results suggesting the proposed method can effectively learn under few shot learning.  A primary concern from the reviewers is that the paper has limited novelty given that it primarily applies existing ideas to a slightly different problem.  Another concern is that the system consists of many components, each of the choices could have other viable options.  The ablation studies indicate these components are useful compared to when removed, but fail to explore possible alternative choices. One of the questions is whether token level reweighting is necessary. It would have been nice to see an ablation study comparing against a baseline using sentence level reweighting.   
The paper concerns learned Q functions in continuous action spaces wherein the action value function is assumed to be quadratic in the action variable, and thus can be maximized in closed form. The paper identifies a class of optimal control problems for which the approximation is reasonable and produces a proof to this effect.  Reviewers found the manuscript clear (6AfP). Reviewer J1Yy notes that the main result of the paper is useful and good to have, as "problems with non linear dynamics but costs quadratic in the control effort are very common in practice and of high practical significance". On the negative side, reviewer J1Yy considered the contribution beyond the central proof rather light and the empirical study inconclusive and questioned the appropriateness of the venue; a comparison to DDPG was added and while a convincing argument was made as to ICLR s appropraiteness, J1Yy was not willing to move their score beyond a 4 (it does not seem the upward adjustment was actually made). 6AfP noted concerns with the presentation and number of seeds, though their concerns seem to have been addressed in an update. 8xxB was the paper s most ardent critic, who found fault with the presentation (starting with the title). The core of 8xxB s criticism seems to be that by narrowing the scope of problems considered, we are left with a problem domain that is already well solved by classical control, as well as contention about the use of "continuous". The authors offered a thorough rebuttal but the authors and 8xxB were unfortunately unable to see eye to eye on these issues. 8xxB requested more explanation, though a request by the authors to specify the precise scope of what further was required went unanswered.  The AC s own reading of the paper matches J1Yy s assessment most closely. There is a contribution here, in the form of connecting previously proposed RL algorithms for continuous action spaces and discretized time to the literature on optimal control, and exploring cases that match NAF s inductive assumptions, but agree that the contribution is of a rather limited nature. I also believe that the paper has improved through the feedback of reviewers J1Yy and 6AfP. I hesitate to recommend acceptance given a universally negative appraisal and in particular the fact that 8xxB was not satisfied in the end. I hope the authors will continue to improve the manuscript with a more thorough empirical interrogation and adjustments in presentation in light of 8xxB and 6AfP s feedback.
The authors propose an intriguing alternative to IFT or unrolled GD as a method for optimizing through arg min layers in a neural net, by using a differentiable sampling based optimization approach. I found the general idea in the paper to be intriguing and thought provoking. The reviewers generally seem to have also appreciated the method, and many of the reviewers  concerns were addressed by the authors during the rebuttal. Although the paper does have a number of flaws   in particular, the evaluation is a bit hard to appreciate, since improvement over prior work is either unclear, or no meaningful comparison is offered,   I think in this case the benefits outweigh the downsides. The work is far from perfect, but the ideas that are presented are interested and valuable to the community, and I think that ICLR attendees will appreciate learning about this work. I would encourage the authors however to improve the paper, and especially the empirical evaluation, as much as possible for the camera ready, and to take reviewer comments into account insofar as feasible. I m also not sure how much I buy the "overfitting to hyperparameters" argument for unrolled GD, and a less charitable interpretation is that the authors present this issue largely to make up for the comparative lack of other benefits. That s not necessarily a bad thing, but I think making such a big deal of it is a bit strange. It s probably fair to say at this stage that the actual benefits of this approach are a bit modest (though improvements in runtime are a good thing...), but the idea is interesting, and may spur future research.
This work defines the new problem of lifelong few shot language learning where the goal is to continually learn new few shot tasks and use those to benefit future tasks while not forgetting previous tasks. With larger models, this is an important goal due to the cost of updating and retraining these models. The work also shows superiority to existing approaches like EWC and MAS. After the author s rebuttal, the experimental section is also thorough with evaluation on a good range of tasks and approaches such as adapters showing good results. While this setting appears simpler than the full lifelong learning setting and the approach combines existing ideas, this work s contribution to the definition and thinking about this problem is valuable. However, the authors should more clearly state the advantages of their approach vs standard prompt tuning (with an emphasis of benefiting future tasks) since two reviewers seem caught up on this point. The other two reviewers comments were addressed by the rebuttal as they stated in their comments.
The paper provides additional empirical evidence that self supervised learning methods can help disentangling factors of variation in a dataset. That said, the paper can benefit from better framing and perhaps comparison with existing work (e.g., https://arxiv.org/abs/2102.08850 and https://arxiv.org/abs/2007.00810). Furthermore, the authors acknowledge that there was a bug in their code, which I believe should at least lead to softening the claims about group disentanglement. Accordingly, please consider revising the paper and re submitting to other venues.
This paper presents an approach to learn graph grammars for molecule generation in a very data efficient way.  The approach combines bottom up grammar construction by contracting graph substructures and evaluation driven learning of the parameters in grammar construction in order to optimize metrics of interest.  This paper is well written and the graph grammar learning approach is novel and can potentially have impact beyond just generating molecules.  All reviewers unanimously recommended acceptance of this paper.  A few things emerged in the reviews and discussions with authors, regarding in particular the computational cost and scalability of the approach and the actual molecule sampling process after learning.  I hope the authors can clarify these in the paper, and as the authors said in the discussion that exploring a more expressive model that uses learned probabilities on the production rules can make the model more powerful, which is a promising direction for the future.
The authors present a method called "AdaRL" that learns a structured latent representation that characterizes relationships between different variables in an RL system. The method is evaluated on modified Pong and Cart Pole domains and it is shown to outperform other transfer learning baselines. The reviewers agree that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments. The reviewers also point out that the evaluated domains are rather simple and the paper would benefit from evaluations in a more complex environment as well as better writing. Please focus on improving these aspects in the final version of the paper.
The paper makes broad claims, but the depth of the experiments is very limited to a narrow combination of algorithms.
The paper proposes a strategy for incrementally pruning deep learning models based on activation values. The approach can satisfy different kinds of requirements, trading off between accuracy and sparsity.  The approach seems promising and seems to have competitive performance. However, the method is described by reviewers as a combination of ideas that have been proposed in the literature, and the experimental evaluation relies too much on a dataset considered too small to be reliable in such experiments   CIFAR10. We do not expect substantial experiments within the rebuttal period: such comparisons with relevant SOTA methods should have been present in the submission. Moreover, the strategy proposed for selecting a threshold seems to rely on some doubtful assumptions, and there are no benchmarks on actual runtime.  The writing has improved based on reviewer input, and the reviewers are satisfied with this aspect. I would still add that I would prefer some clarity in the method presentation: is there a quantity being optimized? is there a value we can monitor to ensure our reimplementation is correct? etc. In addition I would like to ask authors in the next revision to be mindful to the difference between `\citet` and `\citep` in author year citations   see e.g. the first two ones in 3.1.
This paper introduces a novel method for learning distributional robust machine learning models when only partial group labels are available to improve performance of learning algorithms on minority groups.   Pros: The paper is well motivated and written.  The ideas are interesting.  Most work on distributional robust optimization (DRO) are in unsupervised settings where group information is not available.  They provide an approach for the semi supervised setting through a constraint set.  Cons:  The empirical results do not show better performance over unsupervised baselines as pointed out by reviewers.    The authors claim one of the benefits of their proposed approach is a one stage approach, in contrast to competing models that require a two stage approach; hence, allowing their approach to reduce compute time.  It’ll be helpful to strengthen this point by showing time comparisons.  Missing labels in this case due to participants withholding sensitive information is not an MCAR case, but the proposed work makes an MCAR assumption.  It’ll help to add a discussion and point out such limitations of the approach.  Summary:  This paper has novel and interesting ideas, but still has several issues as pointed out by the reviewers before it is ready for publication.
The pros and cons of the paper can be summarized as follows:  Pros: * The method of combining together multiple information sources is effective * Experimental evaluation is thorough  Cons: * The method is a relatively minor contribution, combining together multiple existing methods to improve word embeddings. This also necessitates the model being at least as complicated as all the constituent models, which might be a barrier to practical applicability  As an auxiliary comment, the title and emphasis on computing embeddings "on the fly" is a bit puzzling. This is certainly not the first paper that is able to calculate word embeddings for unknown words (e.g. all the cited work on character based or dictionary based methods can do so as well). If the emphasis is calculating word embeddings just in time instead of ahead of time, then I would also expect an evaluation of the speed or memory requirements benefits of doing so. Perhaps a better title for the paper would be "integrating multiple information sources in training of word embeddings", or perhaps a more sexy paraphrase of the same.  Overall, the method seems to be solid, but the paper was pushed out by other submissions. 
This paper presents a semi supervised model (named CPC VAE) that trains a variational autoencoder (VAE) and a NN classifier simultaneously. The method maximizes an ELBO subject to a task specific prediction constraint and a consistency constraint. The constraints are defined as some expectations of the variational posteriors. Such constraints are known as posterior regularization. Though the consistency constraint seems to be new, the prediction constraint has been well examined under deep generative models (see e.g., max margin deep generative models for (semi )supervised learning, IEEE TPAMI, 2018). The paper needs more a thorough analysis and comparison. 
The paper introduces LEAD, a decentralized optimizer with communication compression that can achieve linear convergence rate in the strongly convex setting. In terms of novelty, the authors should still add a discussion of `Magnusson et al., 2019, On Maintaining Linear Convergence of Distributed Learning and Optimization under Limited Communication`,  which is a related linear convergence result in the deterministic (full gradient) case, and relates to the analysis here which is stochastic but also exploits the deterministic case. Nevertheless, reviewers reached consensus *with communication compression in the given time* that the paper in its current form is well written and the results are presented clearly in both experiments and theory (which builds up on the earlier NIDS algorithm). The presentation of the algorithm can be slightly improved. We hope the authors will incorporate the remaining smaller open points such as mentioned by R1, such as making the constants in the convergence bounds explicit when comparing with other methods.
This paper studies the patch based convolutional kernels for image classification, and finds that making the kernel dependent on data is necessary for designing competitive kernels for image classification. The proposed simple method shows comparable results to those end to end deeper architectures on CIFAR 10 and ImageNet datasets.   All reviewers feel that the paper is interesting, important, and the performance is impressive. During the rebuttal, the authors have addressed most of the questions and concerns raised by the reviewers. In particular, authors have clarified the motivation, discussed the model size of the proposed method (requested by R1), added precise details about the spectrum definition and intrinsic dimension (requested by R4), and taken the suggestions from all reviewers to improve their paper.   After rebuttal, all reviewers agree on accepting the paper. After checking the discussions between the authors and reviewers, I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted. 
A new setting for lifelong learning is analyzed and a new method, AOP, is introduced, which combines a model free with a model based  approach to deal with this setting.  While the idea is interesting, the main claims are insufficiently demonstrated. A theoretical justification is missing, and the experiments alone are not rigorous enough to draw strong conclusions. The three environments are rather simplistic and there are concerns about the statistical significance, for at least some of the experiments.
The authors propose adversarial training using contrastive divergence based on ideas from Hybrid Monte Carlo methods. On the positive side the shown experimental results are promising both in terms of robustness and efficiency. On the negative side the paper seems to be written in a hurry. At several places terms are not defined, not explained or important details (e.g. parameters of the attack algorithms) are missing.   Thus the paper is not ready for publication yet and below the bar for ICLR but I encourage the authors to submit a significantly revised version to another conference.  Details:   in (7) N(x) is nowhere explained or defined, here also several threat models are introduced but later on only l_infty seems to be used e.g in Algorithm 1 (should be clarified)   as noted in the reviews the definition in (13) is based on quantities nowhere introduced   the potential U is only defined in the Algorithm (but then the arguments do not match with the RHS)   the kinetic energy in the algorithm is different from what has been used before   the parameters for the attacks used are not reported   why is AutoAttack not used for all datasets? They report 28% robust accuracy for the model trained by the Madry group  (https://github.com/MadryLab/robustness) whereas in the present paper it is 35%.   the scales of the plots should be chosen such that the curves can be distinguished
Reviewers appreciate the numerical results presented in this paper. However, the paper needs a more rigorous theoretical investigation of the empirical phenomenon, or a more comprehensive empirical exploration to pinpoint the key factors. I recommend the authors to incorporate the suggestions from the reviewers and submit the paper to the next top conference.
Thank you very much for your feedback to the reviewers, which helped us a lot to better understand your paper. However, the paper is still premature to be accepted to ICLR2020. We hope that the detailed reviewers  comments help you improve your paper for potential future submission. 
This article is concerned with sensitivity to adversarial perturbations. It studies the computation of the distance to the decision boundary from a given sample in order to obtain robustness certificates, and presents an iterative procedure to this end. This is a very relevant line of investigation. The reviewers found that the approach is different from previous ones (even if related quadratic constraints had been formulated in previous works). However, they expressed concerns with the presentation, missing details or intuition for the upper bounds, and the small size of the networks that are tested. The reviewers also mentioned that the paper could be clearer about the strengths and weaknesses of the proposed algorithm. The responses clarified a number of points from the initial reviews. However, some reviewers found that important aspects were still not addressed satisfactorily, specifically in relation to the justification of the approach to obtain upper bounds (although they acknowledge that the strategy seems at least empirically validated), and reiterated concerns about the scalability of the approach. Overall, this article ranks good, but not good enough.  
This paper introduces an algorithm for optimization of discrete hyperparameters based on compressed sensing, and compares against standard gradient free optimization approaches.  As the reviewers point out, the provable guarantees (as is usually the case) don t quite make it to the main results section, but are still refreshing to see in hyperparameter optimization.  The method itself is relatively simple compared to full featured Bayesopt (spearmint), although not as widely applicable. 
The reviewers have unanimously expressed strong concerns about the technical correctness of the theoretical results in the paper. The paper should be carefully revised and checked for technical errors. In its current form, the paper is not suitable for acceptance at ICLR 2018. 
This paper focuses on generalization bounds for exponential family Langevin dynamics, which extends related recent work for stochastic iterative algorithms such as SGLD in several ways. They derive expected stability bounds for a more general class of noisy stochastic iterative algorithms, leading to an exponential family variation of Langevin dynamics and a noisy version of the sign SGD algorithm. The contributions are technical and quite positively received by one reviewer, while the others were not convinced to change their opinions during the author response as there were concerns on the limitation of the theoretical contributions and the extent to which these contributions have implications on achieving state of the art performance. While I find it valid that the scope of the paper focuses on generalization bounds and provide improvements over the existing literature, rather than on empirical benchmarks or on optimization related aspects, the overall borderline impression of the reviewers on the whole suggests that a refined version of the paper that further clarifies the contributions and makes clear its impacts as well as limitations may make for a stronger and more impactful paper.
This manuscript proposes and analyses a bucketing method for Byzantine robustness in non iid federated learning. The manuscript shows how existing Byzantine robust methods suffer vulnerabilities when the devices are non iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information theoretic lower bound for certain settings.  During the review, the main concerns are related to the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and have satisfied the reviewers. After discussion, reviewers are generally strongly positive about the strength of the manuscript contributions. The authors are reminded to make the final changes agreed in the public discussion e.g., discussion of the reduction to SGD when  $\delta 0$
Dear Authors,  Thank you very much for your detailed feedback to the reviewers in the rebuttal phase. This certainly clarified some of the concerns raised by the reviewers and contributed highly to deepen their understanding of your work.  We positively evaluated the novelty and the superior empirical performance of the proposed method. However, we still have concern about the justification since the proposed model is so complex that it is not clear what was the key for the good performance.   For this reason, I suggest rejection of this submission, in comparison with many other strong submissions. I hope that the reviewers  feedback is useful for improving your work for future publication.  Best, AC
Two out of three reviews for this paper were provided in detail, but all three reviewers agreed unanimously that this paper is below the acceptance bar for ICLR. The reviewers admired the clarity of writing, and appreciated the importance of the application, but none recommended the paper for acceptance due largely to concerns on the experimental setup.
The paper proposes a gradient rescaling method to make deep neural network training more robust to label noise. The intuition of focusing more on easier examples is not particularly new, but empirical results are promising. On the weak side, no theoretical justification is provided, and the method introduces extra hyperparameters that need to be tuned. Finally, more discussions on recent SOTA methods (e.g., Lee et al. 2019) as well as further comprehensive evaluations on various cases, such as asymmetric label noise, semantic label noise, and open set label noise, would be needed to justify and demonstrate the effectiveness of the proposed method. 
This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity.   These concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers  concerns and providing proper comparison to related works will go a long way in that direction.
The paper proposes a method for compressing unconditional generative models by leveraging a knowledge distillation framework. Two reviewers consider the paper slightly above the acceptance threshold for the interesting topic studied in the paper and the simplicity of the method. However, the other three reviewers consider the paper below the acceptance threshold with two reviewers rating the paper slighting below the acceptance threshold and one reviewer rating the paper as not good enough. Several issues were raised, including that the paper only contains results from one unconditional model (StyleGAN2) and that the presented results are not convincing enough. Consolidating the reviews and the rebuttal, the meta reviewer found the concern raised by the reviewers justified. It would be more ideal if the paper can present results on different unconditional models and more datasets. The authors are encouraged to incorporate the reviewers  feedback to make the paper stronger for a future venue.
The authors propose Group Connected Multilayer Perceptron Networks which allow expressive feature combinations to learn meaningful deep representations. They experiment with different datasets and show that the proposed method gives improved performance.   The authors have done a commendable job of replying to the queries of the reviewers and addresses many of their concerns. However, the main concern still remains: The improvements are not very significant on most datasets except the MNIST dataset. I understand the author s argument that other papers have also reported small improvements on these datasets and hence it is ok to report small improvements. However, the reviewers and the AC did not find this argument very convincing. Given that this is not a theoretical paper and that the novelty is not very high (as pointed out by R1) strong empirical results are accepted.  Hence, at this point, I recommend that the paper cannot be accepted.  
The reviewers are concerned that the evaluation quality is not sufficient to convince readers that the proposed embedding method is indeed superior to alternatives. Though the authors attempted to address these comments in a subsequent revision but still, e.g., the evaluation is only intrinsic or on contrived problems. Given the limited novelty of the approach (it is a fairly straightforward generalization of Levy and Goldberg s factorization of PPMI matrix; the factorization is not new per se as well), the quality of experiments and analysis should be improved.  + the paper is well written   novelty is moderate   better evaluation and analysis are necessary 
The paper shows that show that methods for probabilstic lifted inference can also be used to "compress symmetries" in convolutional models over structured data. The resulting structured convolutional models are then shown to yield speed ups for learning graph neural networks, too. This is highly interesting since the existing literature rather considered how to make use of weisfeiler lehman for classification of graphs, both in neural and a kernel way. This paper, however, shows how to compress the computations. And it paves the way to connect equivariant learning lifted inference by exploiting the connection between lifted probabilistic inference / weisfeiler lehman and their algebraic formulations. 
The authors theoretically analyze the approximation of Korobov functions by neural networks, obtaining upper and nearly matching lower bounds, for shallow and deep networks, with different activation functions.  The bounds are stronger than what can be proved for the more commonly studied Sobolev functions.  But Korobov functions are a natural and fairly wide class of functions.  This work makes a substantial step forward in clarifying what kind of functions are especially amenable to representation by neural networks.  The reviewers also appreciated the clarity of the writing.
Despite the performance gains of RankingMatch over the benchmarks used in the paper, the reviewers remained concerned about how the paper compares to state of the art in several respects.
 This paper proposes an adversarial learning framework for dialogue generation. The generator is based on previously proposed hierarchical recurrent encoder decoder network (HRED) by Serban et al., and the discriminator is a bidirectional RNN. Noise is introduced in generator for response generation. The approach is evaluated on two commonly used corpora, movie data and ubuntu corpus.  In the original version of the paper, human evaluation was missing, an issue raised by all reviewers, however, this has been added in the revisions. These supplement the previous automated measures in demonstrating the benefits and significant gains from the proposed approach.  All reviewers raise the issue of the work being incremental and not novel enough given the previous work in HRED/VHRED and use of hierarchical approaches to model dialogue context. Furthermore, noise generation seems new, but is not well motivated, justified and analyzed.  
The paper proposes to use the mirror descent algorithm for the binary network. It is easy to read. However, novelty over ProxQuant is somehow limited. The theoretical analysis is weak, in that there is no analysis on the convergence and neither how to choose the projection for mirror mapping construction. Experimental results can also be made more convincing, by adding comparisons with bigger datasets, STOA networks, and ablation study to demonstrate why mirror descent is better than proximal gradient descent in this application.
The authors made no response to reviewers. Based on current reviews, the paper is suggested a rejection as majority.
The submission proposes a method to make a pre existing model equivariant to desired symmetries: frame averaging. The strategy relies on a significant reduction of the number of symmetries to average over (with respect to the Reynolds operator) and uniform subsampling. The paper also demonstrates the usefulness of this method theoretically (universal approximation result) and practically (competitive performance). The contributions are clear and the core idea is simple. I recommend this paper for acceptance with spotlight.
First off, this was a difficult paper to decide on. There was some vigorous discussion on the paper centering around the choices that were available to the conv nets.  The author s strongly emphasized the improvements on the PTB task.  For my part, I think the method is very compelling   sharing weights for all the models we are optimizing on seems like a great idea   and that we can make it work is even more interesting. So from this point of view, I think its a novel contribution and worth accepting.  On the other hand, I m likely to agree with some of the motivations behind the questions raised by R3. Are all the choices really necessary ? perhaps the gains came from just a couple of things like number of skip connections and channels, etc. That exploration is useful. On the flip side, I think it may be an irrelevant question   the model is able to make the correct decisions from a big set.  The authors emphasize the language modelling part, but for me, this was actually less compelling. The authors use some of the tricks from Merity in their model training (perplexity 52.8), and as a result are already using some techniques that produces better results. Further, PTB is a regularization game   and that s not really the claim of this paper. Although, one could argue that weight sharing between different models can produce an ensembling / regularization effect and those gains may show up on PTB. A much more compelling story would have been to show that this method works on a large dataset where the impact of the architecture cannot be conflated with controlling overfitting better.  As a result, this puts the paper on the fence for me; even though I very much like the idea. Polishing the paper and making a more convincing case for both the CNNs and RNNs will make this paper a solid contribution in the future.
The paper investigates calibration for regression problems. The paper identifies a shortcoming of previous work by Kuleshov et al. 2018 and proposes an alternative.   All the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about motivation, clarity of the presentation and lack of in depth empirical evaluation.  I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
This paper proposes to repeatedly apply the classifier two sample tests (proposed by Kim, Ramdas, Singh, Wasserman, in 2016, and developed further by Lopez Paz, Oquab, in 2017) for the purpose of detecting covariate shift. The authors propose methods to extend the aforementioned tests to a sequential setting. Overall, the reviewers do not lean towards acceptance, and neither do I. Several constructive suggestions are provided by reviewers, some are summarized below.  The authors claim that sequential tests are not desirable in such a setting, and thus choose to pay a multiple testing price by repeatedly applying a batch test. However, sequential tests are in fact applicable (they will control type 1 error) but may have a worse power if the alternative is not true at the very start   but these were entirely dropped from the simulations; in fact, comparing the increased type 1 error of the authors  approach to the increased type 2 error of sequential approaches may be worth clarifying.   Perhaps the "right" solution that the authors are looking for could be gotten by converting a sequential test into a sequential changepoint detection algorithm (via repeated application of a sequential test, each started at a new time). Also see "Conformal test martingales for change point detection" and "Inductive Conformal Martingales for Change Point Detection" by Vovk et al., which are currently not cited.
The paper proposes an autoencoder framework for learning joint distributions over observations and latent states. The reviewers expressed concerns regarding the motivation for this work, the presentation with respect to prior work, and unconvincing experiments. In its current form the paper is not ready for acceptance to ICLR 2020.
The main issue with the work in its current form is a lack of motivation and some clarity issues. The paper presents some interesting ideas, and will be much stronger when it incorporates a more clear discussion on motivation, both for the problem setting and the proposed solutions. The writing itself could also be significantly improved. 
This paper presents a conditional CNF based on the InfoGAN structure to improve ODE solvers. Reviewers appreciate that the approach shows improved performances over the baseline models.   Reviewers all note, however, that this paper is weak in clearly defining the problem and explaining the approach and the results. While the authors have addressed some of the reviewers concerns through their rebuttal, reviewers still remain concerned about the clarity of the paper.  I thank the authors for submitting to ICLR and hope to see a revised paper at a future venue.
This paper extends the recent theoretical understanding on geometric properties for word embeddings to relations and entities of knowledge graph. It categorizes relations into different types and derive requirements for their representations. Empirically they experiment several graph embedding approaches and show that when the loss function is aligned with the requirement of the relation type, we can achieve better performance.  The reviewers generally find the paper to be solid, well executed and provides useful insights. The authors are encouraged to strengthen the discussion of the motivation of this work, and improve the presentation based on reviewers  comments. 
The paper gives an extension of scattering transform to non Euclidean domains by introducing scattering transforms on graphs using diffusion wavelet representations, and presents a stability analysis of such a representation under deformation of the underlying graph metric defined in terms of graph diffusion.   Concerns of the reviewers are primarily with what type of graphs is the primary consideration (small world social networks or point cloud submanifold samples) and experimental studies. Technical development like deformation in the proposed graph metric is motivated by sub manifold scenarios in computer vision, and whether the development is well suitable to social networks in experiments still needs further investigations.   The authors make satisfied answers to the reviewers’ questions. The reviewers unanimously accept the paper for ICLR publication.
This paper proposes the use of recently propose neural ODEs in a flow based generative model.   As the paper shows, a big advantage of a neural ODE in a generative flow is that an unbiased estimator of the log determinant of the mapping is straightforward to construct. Another advantage, compared to earlier published flows, is that all variables can be updated in parallel, as the method does not require "chopping up" the variables into blocks.  The paper shows significant improvements on several benchmarks, and seems to be a promising venue for further research.  A disadvantage of the method is that the authors were unable to show that the method could produce results that were similar (of better than) the SOTA on the more challenging benchmark of CIFAR 10. Another downside is its computational cost. Since neural ODEs are relatively new, however, these problems might resolved with further refinements to the method. 
The reviewers raise a number of concerns including no methodological novelty, limited experimental evaluation, and relatively uninteresting application with very limited real world application. This set of facts has been assessed differently by the three reviewers, and the scores range from probable rejection to probable acceptance. I believe that the work as is would not result in a wide interest by the ICLR attendees, mainly because of no methodological novelty and relatively simplistic application. The authors’ rebuttal failed to address these issues and I cannot recommend this work for presentation at ICLR.
This paper received 4 reviews with mixed initial ratings: 4, 8, 5, 7. The main concerns of R1 and R2, who gave unfavorable scores, included limited methodological novelty beyond the data generation and insufficient empirical evaluation of state of the art methods on the proposed dataset. The authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews separately: it addressed some of the concerns, but did not change the overall position of the reviewers. AC agrees with R3 and R4 that the proposed dataset and the environment may have certain practical impact and enable new research in learning CAD reconstruction. However, the contributions are indeed specific to a narrow CAD community, and R1 felt that the paper needs another round of peer reviews before acceptance, as a significant number of new results have been added during the discussion stage. After discussion with PCs, the final recommendation is to reject.
The paper investigates how to improve the performance of dropout and proposes an omnibus dropout strategy to reduce the correlation between the individual models.  All the reviewers felt that the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about novelty of the method relative to existing methods, significance of performance improvements and clarity of the presentation.   I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
The paper studies how adaptive methods help train GANs to achieve better FID scores. It empirically shows that the adaptive magnitude in ADAM is the reason for ADAM s wide adoption for GAN training. The paper receives three reviews: one ranked the paper "accept, good paper" and two ranked the paper "marginally below the acceptance threshold". The supportive reviewer likes the findings in the paper interesting but does not provide enough explanation on the significance of the findings. On the other hand, the negative reviewers raise several concerns, including the GAN architectures used in the paper are outdated and the achieved performance gain is not major. As the paper focuses on performance instead of convergence, the meta reviewer feels it would be better to include results on SOTA GAN architectures. The provided rebuttal does not lead to any review score change. Consolidating the review and rebuttal, the meta reviewer feels the paper needed to be improved to meet the bar and would not recommend its acceptance.
This paper introduces a method to increase diversity/individuality of agents in a MARL setup, based on intrinsic rewards coming from a classifier over behaviours.  Reviewers tend to agree that this is an important/interesting problem, which is related to exploration, a central problem in reinforcement learning. Several reviewers point out that the paper is well written. I appreciate that the authors have been responsive to reviews and have answered and/or addressed several points of concern of the reviewers. The proposed method performs well on the experiments carried out.  Reviews still point out several things that could be improved. The experiments mostly report reward curved, and only few results are actually clearly pointing out the individuality between agents. The fact that this method outperforms the baselines is good, but does not prove individuality and may simply be due to the authors spending more time on the tasks, or other undiscovered phenomenon. A reviewer is concerned that this extra reward could encourage trivial behaviours, and it seems clear that it will if the relative weight of the intrinsic reward is too high. This should be discussed more. Finally, a reviewer points out that classifier based intrinsic reward for diversity already exists in published works and that this paper is incremental work.  The average score for this paper is very close to the acceptance threshold, but based on the reviews I recommend to reject this paper for ICLR 2021. I am confident that when the authors address further the reviewers concerns and improve the experimental results, this paper will be published in a future venue.
This paper presents a model based RL approach to Atari games based on video prediction. The architecture performs remarkably well with a limited amount of interactions.  This is a very significant result on a question that engages many in the research community.  Reviewers all agree that the paper is good and should be published. There is some disagreement about the novelty of it. However, as one reviewer states, the significance of the results is more important than the novelty. Many conference attendees would like to hear about it.  Based on this, I think the paper can be accepted for oral presentation.
The paper presents a novel framework from transfer learning over GNNs. Experiments ought to better substantiate how structural differences/similarities are measured, as well as relying on prior art to measure transferability success. A plan for incorporating (structural) features would also strengthen the present work.
This paper provide an extensive set of benchmarks for Deep Model based RL algorithms.  This paper contains a large number of algorithms, environments, and empirical results. The reviewers all recognized the need for such a study to provide some clarity, insights, and common standards. The reviewers we concerned about several aspects of the implementation of the effort. (1) All the performance is based on 4 runs, smoothed curves, and default errors (often extensively overlapping). The paper cites Henderson et al, and yet does not follow the advice laid out therein. (2) The results were fairly inconclusive perhaps to be expected we didn t learn much (more on this below). (3) The paper has communication issues.  The overall approach taken was a bit perplexing. Some algorithms we given access to the dynamics. The reward functions were converted to diff. forms, and early stopping in a domain specific way was employed. This all seems like simplifying the problem in different ways so that some methods can be competitive, but it is not at all clear why. If we take the typical full rl problem and limit domain knowledge, many of these approaches cannot be applied and others will fail. Those are the results we want. One could actually view these choices are unfair to more general algorithms algorithms that need diff rewards pay no price for this assumption. This also leads to funny things, for example, like using position as the reward in mountain car (totally non standard, and invalid without discounting). The paper claims a method can solve MC, but that is unclear from the graph. The paper motivates the entire enterprise based on the claimed lack of standardization in the literature, but then proceeds to redefine classic control tasks with little discussion or explanation.   The paper has communication issues. For example, all the domains are use continuous actions (and the others in the response highlight that is their main focus), but this is never stated in the paper. The paper refers to and varies "environment length", but this was not defined in the paper and has no obvious meaning. The tasks are presumably discounted but the the value of gamma is not specified anywhere in the paper (could be there, but I searched for a while). Pages of parameter settings in the appendix with many not discussed or their ranges justified.  This paper is ambitious, but I urge the authors to perhaps limit the scope and do less, and consider a slightly broader audience in both the writing and experiment design.    
The paper examines the approach of modeling aleatoric uncertainty by fitting a neural network, that estimates mean and variances of a heteorscedasitic Gaussian distribution, based on log likelihood maximization. The authors identify the problem that gradient based training on the netgative log likelihood (NLL) may result in suboptimal solutions where a high predicted variance compensates for the predicted mean being far from the true mean. To solve this problem, the authors suggest to adjust the log likelihood objective by weighting the log likelihood of each single data point by the corresponding beta exponentiated variance estimate. This adjusted objective is referred to as beta NLL.  All reviewers agreed that the identified problem and the proposed solution are interesting, that the paper is well written and organized, and that the contributions are significant and somewhat new. The main criticism was on the side of the empirical evaluation. It was criticized that the empirical analysis did not compare the proposed method to other approaches to solving the same problem, that the identified problem and the proposed method should be also analyzed on high dimensional data, that the results on the synthetic experiments could be improved by investigating more than a single run and by incorporating the the MSE in corresponding Figure 1,  and that standard errors were not reported.   Based on the reviews the authors added several new experiments and investigations in the revised version of their manuscript to improve their empirical analysis: 1) new experiments on high dimensional data sets were conducted applying variational autoencoders on MNIST and Fashion MNIST and performing Depth map prediction from images on the NYUv2 dataset. 2) For comparison several baseline methods were added to the experiments on the UCI and the dynamics datasets. 3) Three more UCI datasets (carbon, superconductivity, wine white) were included in the empirical analysis. 4) An evaluation of calibration of predictive variance for the heteroscedastic sine dataset was added. 5) Several more repetitions of the experiment represented in Figure 1 were conducted. (6) An analysis of undersampling behavior on FetchPickAndPlace was added. Moreover, the authors reported two errors in their previous experiments that they discovered and corrected.   All reviewers were satisfied with the changes in the revised version and the answers to their specific questions and increased their scores accordingly, now commonly voting for acceptance. The paper should therefore be accepted.
Thanks for your detailed feedback to the reviewers, which helped us a lot to better understand your paper. However, given high competition at ICLR2020, we think the current manuscript is premature and still below the bar to be accepted to ICLR2020. We hope that the reviewers  comments are useful to improve your manuscript for potential future submission.
The paper reports unusally rapid convergence of the ResNet 56 model on CIFAR 10 when a single cycle of a cyclic learning rate schedule is used.  The effect is analyzed from several different perspectives. However, the reviewers were not convinced because the effect is only observed for one task, so they question the significance of the result. There was significant discussion of the paper by the reviewers and area chair before this decision was reached.  Pros: + Paper illustrates a "super convergence" phenomenon in which training of a ResNet 56 reaches an accuracy of 92.4% on CIFAR 10 in 10,000 iterations using a single cycle of a cyclic learning rate schedule, while a more standard piecewise constant schedule reaches 91.2% accuracy in 80,000 iterations. + There was partial, independent replication of the results on other tasks reported on OpenReview.  Cons:   In the paper, the effect is shown for only one architecture and one task.   In the paper, the effect is shown for only a single run.   There are no error bars to indicate which differences are significant. 
This manuscript tackles an interesting and significant line of research of long term prediction and out of distribution generalization in time series models. I strongly believe this problem is an important one to solve. However, in its current form, its novelty is marginal, and the experiments fail to decisively show advantages. It also lacks of systematic improvements and error analysis. Further work could make it ready for publication at a next conference.
The submission proposes methodology for quantizing neural networks.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR.  Concerns included novelty over previous works, comparatively weak baseline comparisons, and overly restrictive assumptions.
PAPER: This paper proposes a method to learn joint representations from potentially missing data when (1) cross generation may be difficult, and/or (2) with large number of modalities. This is achieved by minimizing the divergence between a surrogate joint posterior and inferences from arbitrary subsets. DISCUSSION: The reviews and discussion brought many relevant issues and concerns. The authors submitted a revised version that improved the clarity of the paper and added an important experiment with PolyMNIST. In their responses, authors also addressed some misunderstanding about JMVAE KL. The comparison with a relatively similar work, from Sutter et al., 2020, was only mentioned in the related work, with no direct comparisons. Also, the authors did not directly address the issue of studying tradeoffs between quality of generated samples and their coherences. It should also be noted that the advantage of the proposed SMVAE is marginal when the number of modalities increases, for the latent representation experiments on PolyMNIST. SUMMARY: Enthusiasm for this paper was not unanimous. The reviewers brought some concerns about its differentiation with priori work, such as Sutton et al., 2020, and about a more detailed analysis of the tradeoffs. While the clarity of the paper improved during the revision, a good number of issues remained. I am leaning towards rejection.
This paper proposes to analyze the space of known sentence to vector functions by comparing the ways in which they induce nearest neighbor lists in a text corpus.  The primary results of the study are somewhat unclear, and the reviewers do not find the method to be novel enough—or sufficiently well motivated a priori—to warrant publication in spite of these results.
The reviewers have agreed this work is not ready for publication at ICLR.
This paper was reviewed by 3 experts, who recommend Weak Reject, Weak Reject, and Reject. The reviewers were overall supportive of the work presented in the paper and felt it would have merit for eventual publication. However, the reviewers identified a number of serious concerns about writing quality, missing technical details, experiments, and missing connections to related work. In light of these reviews, and the fact that the authors have not submitted a response to reviews, we are not able to accept the paper. However given the supportive nature of the reviews, we hope the authors will work to polish the paper and submit to another venue.
This work analyses the use of parameter averaging in GANs. It can mainly be seen as an empirical study (while also a convergence analysis of EMA for a concrete example provides some minor theoretical result) but experimental results are very convincing and could promote using parameter averaging in the GAN community. Therefore, even if the technical novelty is limited, the insights brought by the paper are intesting. 
The authors present an algorithm that utilizes ideas from imitation learning to improve on goal conditioned policy learning methods that rely on RL, such as hindsight experience replay.  Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in way that satisfied the reviewers with respect to their concerns in these areas.  However, after discussion, the reviewers still felt that there were some fundamental issues with the paper, namely that the applicability of this method to more general RL problems (complex reward functions rather than signle state goals, time ) is unclear.  The basic idea seems interesting, but it needs further development, and non trivial modifications, to be broadly applicable as an approach to problems that RL is typically used on.  Thus, I recommend rejection of the paper at this time.
We also had some discussions about the paper that are not visible to the authors. To summarize: the reviewers appreciated the efforts the authors put into the replies and updates. While those clarifies quite a few points, the paper unfortunately is still not publishable in its current form at ICLR.  Overall the paper tackles a very relevant and important question, proposing a tool that could be extremely useful for research on RL. On the downside the paper is mainly descriptive, outlining WHAT the tool can do. Multiple reviewers pointed out that deeper, new insights are missing, e.g., WHY certain features were included and whether the tool actually is helpful for practitioners. A user study has been commenced, which is an excellent step in this direction.
This paper presents an approach for using prior knowledge to constrain transitions for consecutive time steps and aims to replace conditional random fields for sequence tagging tasks in sequence labeling. However, the paper seems incomplete with no experimental results and analysis to validate the proposed ideas.
This work studies the approximation and estimation errors of using neural networks (NNs) to fit functions on infinite dimensional inputs that admit smoothness constraints. By considering a certain notion of anisotropic smoothness, the authors show that convolutional neural networks avoid the curse of dimensionality.    Reviewers all agreed that this is a strong submission, tackling a core question in the mathematics of DL, namely developing functional spaces that are compatible with efficient learning in high dimensional structured data. The AC thus recommends acceptance.
The paper received two accept and two marginally accept recommendations. All reviewers find value in the proposed supervised semantic segmentation methodology (making self supervised representation learning towards dense prediction tasks like segmentation or clustering without explicit manual supervision) and appreciate the experimental gains, but had (mostly practical) criticism that was reasonably well addressed in the rebuttal.
This paper is accepted, however, it could be much stronger by addressing the concerns below.  The theoretical analysis of the proposed methods is weak. * As far as I can tell, the proposition has more to do with the compatible feature assumption than their method. Furthermore the compatible feature assumption is very strong and not satisfied in any of their experiments. * Sec 4.2 does not provide strong support for their method. R2 points out issues with their statements about variance and the next subsection argues from an overly simplistic diagram.  The experimental results are promising, however, R3 brought up important issues in the private discussion: * Their implementation of SAC systematically produces results worse than reported in the original paper (they use a version of SAC with automatically tuned temperature https://arxiv.org/pdf/1812.05905.pdf); 1a) Their SAC gets average returns of 2.5k at 500k steps while the original implementation gets 3k at 500k steps; 1b) Their SAC on HalfCheetah 10k at 1M steps, original paper   11k at 1M steps; 1c) The same applies to Humanoid, there is no improvement with respect to the original SAC; * Their approach degrades performance on Hopper.  * They use non standard hyper parameters for SAC. 0.98 instead of 0.99 for the discount and 0.01 instead of 0.005 for the soft target updates. That might be the main reason why their SAC works worse than the original implementation.  * The authors use the hyper parameters suggested for HalfCheetahBulletEnv for all continuous control tasks. For HalfCheetah, however, the authors of the stable baselines repository (which this paper uses) suggest to use the hyper parameters from the original SAC paper (https://github.com/araffin/rl baselines zoo/blob/master/hyperparams/sac.yml#L48). Nonetheless, the results for the unmodified SAC reported in this work for HalfCheetah/Hopper/Walker/Ant are subpar to the original results, suggesting that the hyper parameters for HalfCheetahBulletEnv are suboptimal for these tasks.  Given the simplicity of the change and the promising experimental results (with some caveats), I believe the community will find this paper interesting and will lead to followup work that can patch the theoretical gaps.
Dear authors,  The reviewers all appreciated your work and agree that this a very good first step in an interesting direction.
This paper proposes an idea to address the non IID issue that is well known in federated learning. After the discussion with the authors, there are still some concerns remained about the proposed approach. First and foremost, the training of local GAN at each client can be demanding computationally and statistically, which limits the practicality of their approach. Secondly, there has been other work that aims to study the non IID issues in federated learning, as suggested by the reviewers. The authors should consider citing some of the work in this literature and compare the prior approaches with their GAN based approach. Thirdly, there is a lack of a formal statement about the privacy guarantee in this paper. In particular, it seems that the privacy guarantee would only make sense in the cross silo setting, in which each client has many users  data. If each client corresponds to a single user, it does not make sense to train a local GAN. The authors should consider elaborating on the privacy guarantee in the next revision.
This paper proposes a new method to learning heuristics for quantified boolean formulas through RL. The focus is on a method called backtracking search algorithm. The paper proposes a new representation of formulas to scale the predictions of this method.  The reviewers have an overall positive response to this paper. R1 and R2 both agree that the paper should be accepted, and have given some minor feedback to improve the paper. R3 initially was critical of the paper, but the rebuttal helped to clarify their doubt. They still have one more comment and I encourage the authors to address this in the final version of the paper.  R3 meant to increase their score but somehow this is not reflected in the current score. Based on their comments though, I am assuming the scores to be 6,8,6 which makes the cut for ICLR. Therefore, I recommend to accept this paper.
This work presents extensions of dialogue systems to simultaneously capture speakers  "personas" (in the framing of Li et al s work) and adapt to them. While the ideas are interesting, reviewers note that the incremental contribution compared to previous work is a bit too limited for ICLR s expectation, without being offset by strongly convincing experimental results. Authors are encouraged to incorporated their ideas into future submissions after having combined them with other insights to provide a stronger overall contribution.
This submission proposes an interesting experiment/modification of CNNs. However, it looks like this contribution overlaps significantly with prior work (that the authors initially missed) and the comparison in the (revised) manuscript seem to not clearly delineate and acknowledge the similarities and differences.  I suggest the authors improve this aspect and try submitting this work to next venue. 
This paper presents Convolutional Conditional Neural Process (ConvCNP), a new member of the neural process family that models translation equivariance. Current models must learn translation equivariance from the data, and the authors show that ConvCNP can learn this as part of the model, which is much more generalisable and efficient. They evaluate the ConvCNP on several benchmarks, including an astronomical time series modelling experiment, a sim2real experiment, and several image completion experiments and show excellent results. The authors wrote extensive responses the the reviewers, uploading a revised version of the paper, and there was some further discussion. This is a strong paper worthy of inclusion in ICLR and could have a large impact on many fields in ML/AI. 
This paper proposes to pre train hierarchical document representations for use in downstream tasks. All reviewers agreed that the results were reasonable.  However, the methodological novelty is limited. While I believe there is a place for solid empirical results, even if not incredibly novel, there is also little qualitative or quantitative analysis to shed additional insights.  Given the high quality bar for ICLR, I can t recommend the paper for acceptance at this time.
Novel way of analyzing neural networks to predict NN attributes such as architecture, training method, batch size etc. And the method works surprisingly good on the MNIST and ImageNet.
This paper addresses the tasks of video generation and prediction and shows impressive results on the datasets such as Kinetics 600. There is a reviewer disagreement on this paper. AC can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. The reviewers have raised the following concerns that were viewed as critical issues when making the final decision: R1 and R3 expressed the concerns regarding limited technical novelty of the proposed approach in light of the prior works, e.g. MoCoGAN and TGANv2. R3 suggests, that the proposed method shows advantage that might be due to the large computational resources available to train the model. Providing a comparison of the proposed model and the relevant baselines on the Kinetics dataset is desirable to access the benefits of the proposed approach (R1).  AC also agrees with the R2 about the potential impact this work could have in the community. However, given that the reviewers have raised important concerns and have given suggestions, the paper needs too many revisions for acceptance at this time. We hope the reviews are useful for improving and revising the paper.
Thank you for submitting you paper to ICLR. The revision improved the paper e.g. moving Appendix A3 to the main text has improved clarity, but, like reviewer 3, I still found section 4 hard to follow. As the authors suggest, shifting the terminology to "posterior shifting” rather than “sharpening" would help at a high level, but the design choices should be more carefully explained. The experiments are interesting and promising. The title, although altered, still seems a misnomer given that the experimental evaluation focusses on RNNs.  Summary: There is the basis of a good paper here, but the rationale for the design choices should be more carefully explained.
The paper proposes hybrid discriminative + generative training of energy based models (HDGE) building on JEM. By connecting contrastive loss functions to generative loss, HDGE proposes an alternative loss function that reduces computational cost of training EBMs.  The reviewers agree that this is an interesting idea and that the empirical results look promising. However, multiple reviewers raised concerns that the theoretical justification was incomplete and felt that some of the claims about the equivalence between the two, as well as some of the practical approximations introduced, need more justification.   I encourage the authors to revise the paper and resubmit to a different venue. 
The paper studies nonconvex strongly concave min max optimization using  proximal gradient descent ascent (GDA), assuming Kurdyka Łojasiewicz (KŁ) condition holds. The main contribution is a novel Lyapunov function, which leads to a clean analysis. The main downsides of the paper as discussed by the reviewers are the lack of experiments and somewhat stringent assumptions needed in the analysis. Nevertheless, the paper was overall viewed favorably by the reviewers, who considered it a worthwhile contribution to the area min max optimization.  
As several reviewers pointed out, the contribution is  too incremental from previous work.
Although the submission studies an interesting question, many parts are not clear enough and the presentation needs to be improved. I encourage authors to revise the paper accordingly and resubmit in future venues.
The paper received scores of WR (R1) WR (R2) WA (R3), although R3 stated that they were borderline. The main issues were (i) lack of novelty and (ii) insufficient experiments. The AC has closely look at the reviews/comments/rebuttal and examined the paper. Unfortunately, the AC feels that with no one strongly advocating for acceptance, the paper cannot be accepted at this time. The authors should use the feedback from reviewers to improve their paper. 
The paper explores surprise minimization in multi agent learning by using free energy across all agents in a multi agent system. A temporal EBM represents an estimate of surprise which is minimized over the joint agent distribution. Empirical studies on the proposed method are conducted. This paper builds in an interesting direction around surprise minimization in multi agent learning by using the energy based framework, but the presentation of the method seems to need more efforts to be improved to avoid confusion.   The discussion between authors and reviewers is summarized below: The major concerns of Reviewer doix include that: (i) the empirical results are not compelling, (ii) qualitative results are missing, and (iii) the motivation of surprise minimization in multi agent RL is unclear. After the rebuttal, the authors addressed the concerns of Reviewer doix, who changed his/her score from 5 to 6. The major concern of Reviewer zFND comes from the understanding and justification of the paper. After the rebuttal, the concerns of Reviewer zFND  have been partially addressed by clarification on what measures of surprise can be used and how these would be estimated. Reviewer zFND eventually changed his/her score from 5 to 6.  Also, most of the concerns about theory and experiments from Reviewer 8Wiu have been addressed after the rebuttal. Reviewer 8Wiu accordingly changed the rating from 5 to 6. Reviewer g9cM is still not satisfied with the authors  answers, and his/her concerns regarding some technical issues remain and points out that the current paper has many inconsistencies across the writing that make it hard to evaluate the soundness and correctness of the results.    After the rebuttal, the author successfully addressed most of the concerns from 3 of 4 reviewers, but the overall rating of the paper is on a borderline level. Given the fact that the paper still has some unaddressed concerns from Reviewer g9cM, and other reviewers actually do not champion the paper. The AC tends to recommend rejecting the paper at the current stage. AC urges the authors to improve their paper by including all the suggestions provided by the reviewers, and then resubmit it to a future venue.
The reviewers reached a unanimous consensus that the paper could not be accepted for publication in its current form. There were a number of concerns raised regarding (1) the clarity of the writing; (2) the comparisons, especially to prior work; (3) the details of the experimental setup.
a major issue or complaint from the reviewers seems to come from perhaps a wrong framing of this submission. i believe the framing of this work should have been a better language model (or translation model) with word discovery as an awesome side effect, which i carefully guess would ve been a perfectly good story assuming that the perplexity result in Table 4 translates to text with blank spaces left in (it is not possible tell whether this is the case from the text alone.) even discounting R1, who i disagree with on quite a few points, the other reviewers also did not see much of the merit of this work, again probably due to the framing issue above.   i highly encourage the authors to change the framing, evaluate it as a usual sequence model on various benchmarks and resubmit it to another venue.
This paper presents a counterfactual approach to interpret aspects within a  sequential decision making setup. The reviewers have reacted to each others  comments as well as the authors  response to their views. I am recommending acceptance of this paper, as it targets an interesting problem and presents an intriguing approach. I think the community would appreciate further discussing this paper at the conference.
The paper proposes the idea of searching parameterized activation functions, in contrast to the previous handcraft or learnable ones. It may be a counterpart of neural architecture search.  Pros: 1. The idea is very interesting. 2. The paper is well written. 3. The experiments show improvements over baseline activation functions.  Cons: 1. The AC fully agreed with Reviewer #4 that the whole literature of learnable activation function is neglected (Reviewer #2 also alluded to this issue). Although the authors added experiments with learnable baseline activation functionss, the literature review on learnable activation function was not included accordingly. 2. Although the idea of searching activation functions is interesting, the AC doubted the necessity. Since the rich literature of learnable activation functions is already there (note that it is more than introducing parameters to handcrafted ones), can we simply learn piecewise linear activation functions with more pieces so that it can approximate complex enough functions? This can be much more easily implemented (can go along with weight training on the standard deep learning platform) and the computation cost will be much lower. Such a comparison is absolutely necessary. 3. The AC was actually worried about the activation functions founded as they may be too complex, so the generalization issue (even numerical stability issue) may be a concern. More thorough testing is necessary (currently only tested on CIFAR 100 and three CNNs; and Reviewers #3 and #2 also concerned about this issue).  Although Reviewer #2 raised his/her score, the final average score is still below threshold. So the AC decided to reject the paper.
The reviewers and the AC acknowledge the paper contains interesting ideas on using an incremental sequence of multiple generators to capture the diversity of the examples. However, the reviewers and the AC also note that the potential drawback of the paper is the lack of evaluation with other metrics such as inception score, FID score, etc. Therefore the paper is not quite ready for acceptance right now, but the AC encourages the authors to submit to other top venues with more thorough experiments. 
This work investigates a metacognition model for object detection. Reviewer RHiF wrote the best summary for this work:  The paper proposes a new method to incorporate Spelke s principles of object perception as constraints to improve the performance of an out of the box object detector. This is done via defining a hierarchical generative model which defines "metacognitive" priors over the a set of observations. Through joint inference over these metacognitive priors and new unobserved states, the method outputs better object detections. The authors show improved performance on a synthetic dataset which contains scenes rendered in a virtual environment.  All reviewers agree that this is a really novel and interesting approach of enforcing consistency constraints in object detection, but had various issues with the experiments. At its current state, I believe it would make a very strong workshop paper, but not read for the ICLR 2020 conference. The authors found the reviews to be helpful, in particular, advice about the dataset construction and metric definitions, and I believe that future versions of this work will be significantly improved. We look forward to reading a revised version of this work in a high impact journal or future ML conference, good luck!
This work provides additional insights into a class of generative models that is rapidly gaining traction, and extends it by potentially providing a faster sampling mechanism, as well as a way to meaningfully interpolate between samples (an ability which adversarial models, currently the most popular class of generative models, also have). The revised manuscript includes an extension to discrete data, which could potentially amplify the impact of this work. The authors have also run additional experiments in response to the reviewers  comments.  Reviewer 1 raised several concerns about the choice of language (i.e. referring to the proposed model as a diffusion model, and the precise meaning of  implicit  in the context of generative models). This is a fair point, as the authors introduce changes that affect the Markovian nature of the "diffusion" process, and a diffusion process is supposed to be Markovian by definition.  However, I think there is something to be said for the authors  argument of using the word  diffusion  to clearly link this work to the prior work on which it is based. Given that technically speaking, the original DDPM work already  abuses  the term to refer to a discrete time process, it is difficult to argue compellingly that  diffusion  should not feature in the name of the proposed model. Referring to  non Markovian diffusion processes  however seems more problematic, as this is a direct contradiction. If the authors wish to use this phrase, adding a few sentences to the introduction that justify this use would be helpful, and personally I feel this would be sufficient to address the issue (I noted that Section 4.1 already acknowledges that the forward process is no longer a diffusion). Plenty of work in our field abuses notation and this is justified simply with the phrase "with (slight) abuse of notation..."; I don t think this would be any different.  Reviewer 1 is technically correct that  stochastic  is an absolute adjective, i.e. something can only be stochastic or deterministic, there is nothing in between, and there are no degrees/levels of stochasticity or determinism. In practice however, it is quite often used in a comparative sense, and I believe I have in fact been guilty of this myself! I do not feel that it causes any ambiguity in this case. Indeed, the phrase  degree of stochasticity  seems to be in relatively common use in literature. While there may be more correct terms to use, I subscribe to the descriptivist view on language, and I do not think the comparative use of  stochastic  is a major issue here. The alternatives I can think of seem potentially more cumbersome (e.g. I wager that  more/less entropic  would be more poorly understood than  more/less stochastic ). Still, I recommend that the authors consider potential alternatives in the future, to avoid any confusion.  Overall, I think the reviewers  major concerns have been addressed in the revised manuscript. Given that all reviewers consider the idea worthwhile, I will join them in recommending acceptance.
### Summary  The paper proposes a technique that enables inference directly on a compressed model without decompressing the model.  ### Discussion    Strengths     An important problem as well as a compelling direction, namely inference without decompression.     Weaknesses:      The reviewers provided a number of both broad and specific criticisms of the work.    The most salient point is the lack of comparison to modern baselines.  Notably, the primary comparison is to a 2015 technique that, while seminal, has since been followed by significant related work (e.g, that identified by Reviewer eHWE, R8Un, and G6tm). In concert, the evaluation should consider at least one more contemporary network in the domain, such as a ResNet.  ### Recommendation  I recommend Reject.  At current, this work is the first step in a strong, compelling direction. However, the work needs to be contextualized within a more modern context of contemporary results
This paper explores a network that has a parvo (fine, detailed, slow) and magno (low res, quick) stream.  The ideas are interesting and the results intriguing, and one reviewer is in favor of acceptance. Several reviewers criticized the clarity of the paper. and the lack of details for, explanations of, and critical evaluation of, the design decisions.  For example, how do the results depend on certain design decisions?  I think that with a bit more work, this paper has potential to be a very impactful paper.  I would encourage the authors to follow the  detailed suggestions and resubmit the work to a high impact conference or  journal.  
The paper introduces a notion of distributional generalization, which aims at characterizing aspects of underlying distribution that are learned by a trained predictor. Authors make several interesting conjectures and support them with empirical evidence. Reviewers agreed on the novelty of the ideas; however, the work seems to be preliminary in its current form. Unfortunately, I cannot recommend acceptance at this time. 
This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5). The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough. Some of these issues are addressed in the rebuttal, though. Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper.
The paper proposes an extension to reinforcement learning with self imitation (SIL)[Oh et al. 2018]. It is based on the idea of leveraging previously encountered high reward trajectories for reward shaping. This shaping is learned automatically using an adversarial setup, similar to GAIL [Ho & Ermon, 2016]. The paper clearly presents the proposed approach and relation to previous work. Empirical evaluation shows strong performance on a 2D point mass problem designed to examine the algorithms behavior. Of particular note are the insightful visualizations in Figure 2 and 3 which shed light on the algorithm s learning behavior. Empirical results on the Mujoco domain show that the proposed approach is particularly strong under delayed reward (20 steps) and noisy observation settings.  The reviewers and AC note the following potential weaknesses: The paper presents an empirical validation showing improvements over PPO, in particular in Mujoco tasks with delayed rewards and with noisy observations. However, given the close relation to SIL, a direct comparison with the latter algorithm seems more appropriate. Reviewers 2 and 3 pointed out that the empirical validation of SIL was more extensive, including results on a wide range of Atari games. The authors provided results on several hard exploration Atari games in the rebuttal period, but the results of the comparison to SIL were inconclusive. Given that the main contribution of the paper is empirical, the reviewers and the AC consider the contribution incremental.  The reviewers noted that the proposed method was presented with little theoretical justification, which limits the contribution of the paper. During the rebuttal phase, the authors sketched a theoretical argument in their rebuttal, but noted that they are not able to provide a guarantee that trajectories in the replay buffer constitute an unbiased sample from the optimal policy, and that policy gradient methods in general are not guaranteed to converge to a globally optimal policy. The AC notes that conceptual insights can also be provided by motivating algorithmic or modeling choices, or through detailed analysis of the obtained results with the goal to further understanding of the observed behavior. Any such form of developing further insights would strengthen the contribution of the submission.
This paper proposes a method for class imbalanced data based on meta learning. The technical contribution of the proposed method is limited as it is a reasonable but straightforward extension of the existing method. In addition, as commented by the reviewers, the comparison with existing methods is not enough, it is unclear why it is meta learned with balanced test data, and hyperparameter tuning details are not given.
This paper introduces a method for hierarchical classification with deep networks. The idea is interesting, and as far as I know novel: namely, the authors add a regularizer to the last layer in order to enforce a hierarchical structure onto the classifiers. The idea of placing spheres (with a fixed radius) around each classifier and forcing the child classifiers to lie on these spheres is quite clever.  The reviewers have pointed out some concerns with this paper. Some had to do with terminology (which the authors should fix but which is no big deal), but the main weakness are the experimental results and the ablation study. The reviewers were not convinced that the optimization in the Euclidean space wouldn t be sufficient. A more thorough ablation study could help here.   This is the kind of paper that I really want to see published eventually, but right now isn t quite ready yet. If you make one more iteration (in particular adding a stronger ablation study) it should be a strong submission to the next conference. Good luck!
R1 was neutral on the paper: they liked the problem, simplicity of the approach, and thought the custom pooling layer was novel, but raised issues with the motivation and design of experiments. R1 makes a reasonable point that training a CNN to classify time series, then throw away the output layer and use the internal representation in 1 NN classification is hard to justify in practice. Results of the reproducibility report were good, though pointed out some issues around robustness to initialization and hyper parameters. R2 gave a very strong score, though the review didn’t really expound on the paper’s merits. R3 thought the paper was well written but also sided with R1 on novelty. Overall, I side with R1 and R3. Particularly with respect to the practicality of the approach (as pointed out by both these reviewers). I would feel differently if the metric was used in another application beyond classification.
This paper provides an approach to jointly localize and repair VarMisuse bugs, where a wrong variable from the context has been used. The proposed work provides an end to end training pipeline for jointly localizing and repairing, as opposed to independent predictions in existing work. The reviewers felt that the manuscript was very well written and clear, with fairly strong results on a number of datasets.  The reviewers and AC note the following potential weaknesses: (1) reviewer 4 brings up related approaches from automated program repair (APR), that are much more general than the VarMisuse bugs, and the paper lacks citation and comparison to them, (2) the baselines that were compared against are fairly weak, and some recent approaches like DeepBugs and Sk_p are ignored, (3) the approach is trained and evaluated only on synthetic bugs, which look very different from the realistic ones, and (4) the contributions were found to be restricted in novelty, just uses a pointer based LSTM for locating and fixing bugs.   The authors provided detailed comments and a revision to address and clarify these concerns. They added an evaluation on realistic bugs, along with differences from DeepBugs and Sk_p, and differences between neural and automated program repair. They also added more detail comparisons, including separating the localization vs repair aspects by comparing against enumeration. During the discussion, the reviewers disagree on the "weakness" of the baseline, as reviewers 1 and 4 feel it is a reasonable baseline as it builds upon the Allamanis paper. They found, to different degrees, that the results on realistic bugs are much more convincing than the synthetic bug evaluation. Finally, all reviewers agree that the novelty of this work is limited.  Although the reviewers disagree on the strength of the baselines (a recent paper) and the evaluation benchmarks, they agreed that the results are quite strong. The paper, however, addressed many of the concerns in the response/revision, and thus, the reviewers agree that it meets the bar for acceptance.
This paper proposes a novel pruning method for use with transformer text encoding models like BERT, and show that it can dramatically reduce the number of non zero weights in a trained model while only slightly harming performance.  This is one of the hardest cases in my pile. The topic is obviously timely and worthwhile. None of the reviewers was able to give a high confidence assessment, but the reviews were all ultimately leaning positive. However, the reviewers didn t reach a clear consensus on the main strengths of the paper, even after some private discussion, and they raised many concerns. These concerns, taken together, make me doubt that the current paper represents a substantial, sound contribution to the model compression literature in NLP.  I m voting to reject, on the basis of:    Recurring concerns about missing strong baselines, which make it less clear that the new method is an ideal choice.   Relatively weak motivations for the proposed method (pruning a pre trained model before fine tuning) in the proposed application domain (mobile devices).   Recurring concerns about thin analysis.
This paper explores the effect on decoding accuracy (predicting hidden representations from fMRI datasets) from fine tuning models by injecting structural bias.  This paper specifically focuses the attention of BERT on syntactic features of the text, which (for one dataset) appears to improve the decoding performance.  The paper s motivation is strong, and complex concepts are communicated clearly.  The review period was very productive.  There were some questions about analyses, and the validity of the statistical tests, but through some very thorough back and forth with the reviewers, this seems to have been resolved.  There is a good amount of analysis done on the resulting language models to try and determine the impact of finetuning or attention on the models. However, the results on the fMRI two datasets appear to be very different, and it s unclear why (and isn t clearly related back to the extensive language model analyses).  We would have liked to have seen a more thorough analysis of the stark difference in performance, and some convincing explanations for the difference based on the analyses.     P.s. A minor point, but the Wehbe paper uses Chapter 9 of Harry potter, not chapter 2. 
This paper proposes a method for time series imputation modelling the spatial dependencies with graphs, focusing on spatio temporal data, where the spatial dimensions are represented by a graph. The reviewers find the approach novel. The paper is well written and clear. Related work is adequately discussed. The experiments are convincing. The reviewers agree that the paper should be accepted.
The paper takes a creative step in the theory of tournaments, and it seems plausible that this could lead to interesting follow ups. The reviewers made many excellent comments and I highly encourage the authors to take ALL of them into account in the revision, it will make the paper much stronger.
The paper proposed an approach to search image augmentation policies. The paper formulates this problem as a cooperative multi agent decision making problem, which is interesting. The paper received 3 borderline accept and 1 borderline reject ratings. The reviewers originally had multiple concerns regarding the necessity of RL based approach, lacking references, and additional experiments, and the authors responded to some of the concerns of the reviewers reasonably. However, none of the reviewers ended up strongly supporting the paper, staying with their ratings.  The RL formulation of the problem is interesting, but it requires multiple rounds of the target network training due to its nature (i.e., it is not an end to end approach). The paper misses some details on how exactly the patch wise RL based augmentation works and it requires additional hyperparameters for the selection of patch size and shape. It is also unclear how this RL based method is conceptually superior to previous augmentation approaches and the empirical results are not strong enough, as some of the reviewers also pointed out.  Although the paper has interesting ideas and the AC also think the paper has some merit, the senior AC finds the technical contribution of the paper weaker than the others. We unfortunately need to recommend the rejection of the paper.
The authors aim to improve policy gradient methods by denoising the gradient estimate. They propose to filter the transitions used to form the gradient update based on a variance explains criterion. They evaluate their method in combination with PPO and A2C, and demonstrate improvements over the baseline methods.  Initially, reviewers were concerned about the motivation and explanation of the method. The authors revised the paper by clarifying the motivation and providing a justification based on the options framework. Furthermore, the authors included additional experiments investigating the impact of their approach on the gradient estimator, showing that with their filtering, the gradient estimator had larger magnitude.  Reviewers found the justification via the options framework to be a stretch, and I agree. The authors should explain how the options framework leads to dropping gradient terms. At the moment, the paper describes an algorithm using the options framework, however, they don t connect the policy gradients of that algorithm to their method. Furthermore, the authors should more clearly verify the claims about reducing noise in the gradient estimate. While the additional experiments on the norm are nice, the authors should go further. For example, if the claim is that the variance of the gradient estimator is reduced, then that should be verified. Finally, there are many approaches for reducing the variance of the policy gradient (Grathwohl et al. 2018, Wu et al 2018, Liu et al. 2018) and no comparisons are made to these approaches.  Given the remaining issues, I recommend rejection for this paper at this time, however, I encourage the authors to address these issues and submit to a future venue. 
 The paper presents a new annotation of the CIFAR 10 dataset (the test set) as a distribution over labels as opposed to one hot annotations. This datasets forms a testbed analysis for assessing the generalization abilities of the state of the art models and their robustness to adversarial attacks.   All the reviewers and AC acknowledge the contribution of dataset annotation and that the idea of using label distribution for training the models is sound and should improve the generalization performance of the models. However the reviewers and AC note the following potential weaknesses: (1) the paper requires major improvement in presentation clarity and in depth investigation and evidence of the benefits of the proposed framework – see detailed comments of R3 on what to address in a subsequent revision; see the suggestions of R2 for improving the scope of the empirical evaluations (e.g. distortions of the images, incorporating time limits for doing the classifications) and the requests of R1 for clarifications; (2) the related work is inadequate and should be substantially extended – see the related references suggested by the R2; also R1 rightly pointed out that two out of four future extensions of this framework have been addressed already, which questions the significance of findings in this submission. The R2 raised concerns that the current evaluation is missing comparisons to a) the calibration approaches and b) cheaper/easier ways of getting soft labels   see R2’s suggestion to use the Brier score for model calibration and to use a cost matrix about how critical a misclassification is (cat < > dog, versus cat < > car) as soft labels. Among these, (2) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (3) makes it very difficult to assess the benefits of the proposed approach, and was viewed by the AC as a critical issue.   There is no author response for this paper. The reviewer with a positive view on the manuscript (R3) was reluctant to champion the paper as the authors have not addressed the concerns of the other reviewers (no rebuttal).  
This paper proposes an extension of the search space of neural architecture search to include dynamic convolutions, teacher nets among others. The method is evaluated on CIFAR 10 and Imagenet with a similar setup as other architecture search methods. The reviewers found that the results did not convincingly show that the proposed improvements were better than other ways of improving neural architecture search such as rroxylessNAS.
This paper proposes an adversarial attack method based on feature contributive regions. In generating adversarial perturbations, Grad CAM heatmaps are used as constraints for the perturbation. The overall idea is interesting and straightforward. However, as several reviewers raised, similar methods have been proposed in prior literature (Yao et al. CVPR 2019 and Xu et al. ICLR 2019), therefore making the novelty questionable.   All three reviewers have indicated rejection, and there is no author response. The paper is therefore rejected. 
Both reviewers (we apologize for the lack of a 3rd review) did not feel the paper should be accepted. The rebuttal offered did not change the reviewer scores. So the paper cannot be accepted unfortunately. But the authors should use the feedback to improve their paper and resubmit.
The paper proposes a new defense against backdoor attacks utilizing an improved version of defenses against noisy label attacks. The connection between these two problems is interesting and novel, which is acknowledge by all reviewers. The main drawback of the paper is, however, its experimental evaluation. The experiments are carried out only on one benchmark and the considered attack ratios are indicative for indiscriminative poisoning attacks rather than targeted backdoors. The AC addressed the summarized response and is not convinced that the reviews are biased. Despite the scarce response of the reviewers to the author rebuttal, the limitations of the experimental evaluation seem to persist in the revised version of the paper. While acknowledging the novelty and the overall good quality of the paper, the weakness of its experimental evaluation puts at in the position marginally below the acceptance threshold. The AC encourages the authors to revise the paper and improve on the pointed out weaknesses and is confident that this work will be well accepted by the scientific community.
The paper studies the convergence of a primal dual algorithm on a special min max problem in WGAN where the maximization is with respect to linear variables (linear discriminator) and minimization is over non convex generators. Experiments with both simulated and real world data are conducted to show that the algorithm works for WGANs and multi task learning.  The major concern of reviewers lies in that the linear discriminator assumption in WGAN is too restrictive to general non convex mini max saddle point problem in GANs. Linear discriminator implies that the maximization part in min max problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a non convex optimization instance and proves its first order convergence with descent lemma. This technique however can’t be applied to general non convex saddle point problem in GANs. Also the experimental studies are also not strong enough. Therefore, current version of the paper is proposed as borderline lean reject.  
The paper proposes an approach to sparse CCA with deep neural nets, performing simultaneous feature selection with stochastic gating and canonical correlation maximization.  The reviewers think that there is merit in defining an objective function that optimizes the goals jointly throughout the networks. However, the paper has not clearly presented the novelty in methodology. In particular, the reviewers agree that the paper needs to clearly distinguish itself from the two building blocks (Andrew et al. 2013 and Louizos et al. 2017), and demonstrate the significance of combining the two techniques theoretically and/or experimentally. Also, there is a large literature in sparsifying classical method. Sufficient discussions and comparisons with prior work can better position the current work in the literature.
A well written paper that proposes an original approach for leaning a structured prior for VAEs, as a latent tree model whose structure and parameters are simultaneously learned. It describes a well principled approach to learning a multifaceted clustering, and is shown empirically to be competitive with other unsupervised clustering models.  Reviewers noted that the approach reached a worse log likelihood than regular VAE (which it should be able to find as a special case), hinting towards potential optimization difficulties (local minimum?). This would benefit form a more in depth analysis.  But reviewers appreciated the gain in interpretability and insights from the model, and unanimously agreed that the paper was an interesting novel contribution worth publishing. 
This paper presents an approach to high quality waveform synthesis using multi band decomposition. The resulting synthesis speed is substantially faster the past work on both CPU and GPU   a feature that all reviewers viewed as a significant strength. However, the majority of reviewers raised concerns about discussion of and contextualization within past work, as well the novelty of the proposed approach. Finally, one reviewer pointed out a potential concern with experimental evaluation (sample rate of proposed system outputs vs baseline s). Author response clarified the relationship with some past work but did provide evidence to mitigate the concerns about experimental evaluation. Overall, this paper could still benefit from another round of review.
In this paper, data augmentation for graph contrastive learning (GraphCL) is studied. Most reviewers agree that the problems addressed in this paper are interesting and important for unsupervised graph representation learning literature. However, many reviewers were not fully satisfied with the novelty and the claim of the main contribution of this paper, a theoretical analysis of the conditions under which data augmentation works in GraphCL, due to the lack of clear explanation and evidence. Unfortunately, no reviewer has suggested acceptance of this paper at this time.
Motivated by (1) the problem of scaling up optimal transport to high dimensional problems and (2) being able to tolerate noisy features, this paper introduces a new optimization problem that they call feature robust optimal transport where they find a transport plan with discriminative features. They show that the min max optimization problem admits a convex formulation and solve it using a Frank Wolfe method. Finally they apply it to the layer selection problem and show that it achieves state of the art performance for semantic correspondence datasets.   The reviews were mixed for this paper. The main negative, which was brought up in all the reviews, is the lack of novelty compared to earlier methods like SRW which already combine dimensionality reduction and optimal transport. The new method in this paper still does have value since it can scale up to larger dimensional problems. It would have been nice to have a wider range of experiments, which would present a more compelling case for its applicability. Another reviewer brought up a correctness issue, however it is not clear if this is actually a bug or merely a misunderstanding about how the pieces in the overall proof fit together. In any case, the reviewers pointed out various places where the writing could be improved. 
The paper proposes adversarial flow based neural network architecture with adversarial training for video prediction. Although the reported experimental results are promising, the paper seems below ICLR threshold due to limited novelty and issues in evaluation (e.g., mechanical turk experiment). No rebuttal was submitted.
Most reviewers were positive about the paper, seeing that the proposed method is practical and has convincing experimental performances. One reviewer was a bit negative and raised questions about clarity. After the authors responded, the negative reviewer didn t respond further. After reviewing all the comments, the AC feels that there is enough support from reviewers to accept this paper.
The paper considers a new linear algebraic problem motivated by applications such as metagenomics which requires the algorithm to partition the coordinates of a long noisy vector according to a few known subspaces. A number of theoretical questions were asked (e.g., identifiability;  efficient algorithms and their error bounds; etc).    The reviewers generally liked the paper for what it does. Specific suggestions were raised by the reviewers, including how the paper went into length about the motivating applications but did not end up evaluating the proposed algorithms on any motivating applications; and that the main theoretical results were not technically challenging / nor surprising (although the authors provided a fair justification in their rebuttal).  The AC finds the paper an outlier in terms of the topics among papers typically received by ICLR, but liked the paper precisely because it is different.  The authors are encouraged to discuss the connections of the specific problem to the context of representation learning and machine learning in general.  Overall, I believe the paper is a solid borderline accept.    
The paper shows a causal perspective to the adversarial robustness problem. Based on a causal graph of the adversarial data creation process, which describes the perceived data as a function of content and style variables, the authors identified that the spurious correlation between style and label is the main reason for adversarial examples. Based on this observation, they propose a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Experiments on MNIST, CIFAR 10, and CIFAR 100 datasets show that the proposed method is better than two baselines, Madry and TRADES.  Overall, the paper contains interesting ideas and tackles an important problem. Due to some concerns regarding the clarity and motivation of the paper, we strongly recommend the authors take the reviewers  comments to heart and incorporate their thoughts in preparing the camera ready version of their manuscript.
This paper proposes a new unsupervised learning approach based on maximizing the mutual information between the input and the representation. The results are strong across several image datasets. Essentially all of the reviewer s concerns were directly addressed in revisions of the paper, including additional experiments. The only weakness is that only image datasets were experimented with; however, the image based experiments and comparisons are extensive. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
This paper presents a steganographic approach called Variable Length Variable Quality Audio Steganography (VLVQ) that encodes variable length audio data inside images with varying quality trade offs. However, according to the reviewers, the proposal made in this paper is not novel enough, there are many details missing in the paper, and the experimental study is far from comprehensive and conclusive. Afte the reviewers provided their comments, the authors did not submit their rebuttals. Therefore, as a result, we do not think the paper is ready for publication at ICLR.
There has been significant discussion in the literature on the effect of the properties of the curvature of minima on generalization in deep learning.  This paper aims to shed some light on that discussion through the lens of theoretical analysis and the use of a Bayesian Jeffrey s prior.  It seems clear that the reviewers appreciated the work and found the analysis insightful.  However, a major issue cited by the reviewers is a lack of compelling empirical evidence that the claims of the paper are true.  The authors run experiments on very small networks and reviewers felt that the results of these experiments were unlikely to extrapolate to large scale modern models and problems.  One reviewer was concerned about the quality of the exposition in terms of the writing and language and care in terminology.  Unfortunately, this paper falls below the bar for acceptance, but it seems likely that stronger empirical results and a careful treatment of the writing would make this a much stronger paper for future submission.
The authors suggest a VAE model for causal inference. The approach is motivated by CEVAE (Louizos et al., 2017) which uses a VAE to learn a latent representation of confounding between the treatment, target, and covariates. This paper goes beyond this approach and tries to design generative model architectures that encourage learning disentangled representations between different underlying factors of variation inspired by Hassanpour & Greiner (2019).   The reviewers agreed that the topic will be of interest to a large group of readers. While the first version of the papers raised questions about the experimental design, several questions on the architecture design were addressed during the rebuttal period (e.g., deeper architectures). Other improvements were suggested and not adopted (e.g., alternative methods to achieve better disentanglement). The ablation studies seem to suggest that some of the loss terms are not actually needed and that non probabilistic autoencoders (beta 0) also work well. We recommend aiming at improving the writing quality and coverage of more background material on the proposed architectures and causal factors.  
The authors propose a process to leverage the memorization effect of deep learning models to filter out examples at the boundary (hard) that the models are confident on, and argue that identifying those hard confident examples help improve the accuracy when learning under noisy data. The process essentially alternates between confident example selection and classifier updating, where the two parts are expected to help each other to form a positive cycle. Experiments demonstrate superior results over other self purifying approaches.  The reviewers have a very diverse opinion about the paper. On the positive side, everyone agrees that the superior experimental results to be very impressive. The authors have addressed some concerns well, such as the running time. One reviewer pointed out that the current study has not been combined with semi supervised learning yet, but during the discussion, most agreed that it is not a crucial negative point of the current paper. On the actual negative points, there are issues that were not cleared even after the rebuttal, such as whether re initialization in the process helps escape local optimal, and the key difference between the "small loss trick" and "memory momentum trick." While the authors argued the novelty with respect to SELF, more illustrations and experiments are needed to highlight the novelty aspect.  Given the diverse opinions, the AC read the paper in detail, and assessed the reviewer s opinions and the authors rebuttal. Overall a serious concern is the leap of faith that the proposed process is indeed (a) "leveraging the memorization effect" to (b) "extract hard confident examples" to (c) "improve accuracy in noisy learning". For (a), it is mentioned that deep learning models "learns simple patterns on majority of data" first, where the majority in this work is argued is the clean ones. But there is no validation of this claim in the experiments. For instance, there is no figure/discussion that shows how much "clean data" has been correctly captured/memorized by the earlier deep learning models. For (b), the terminology of "hard but confident" is ill defined. If examples being hard means them to be around the boundary, one can argue that they could never be "confident" as one measure of the confidence is the margin. The authors may want to mean "hard but clean", but then more illustrations are needed to analyze whether the extracted examples are really the clean ones, or if there are noisy ones being "confident" from the proposed process as well. For (c), it is then unclear whether the improved performance is caused by noise removal (as the authors hope to argue), or by just zooming in to the boundary (regardless of whether the extracted examples are clean or noisy). The authors are encouraged to not just look at the superior performance on accuracy, but analyze more on what actually happened behind the scenes to understand the proposed process better.  Some more comments from the AC that could help the authors (1) Are the examples kept by the proposed approach similar to the ones kept by SELF? Why or why not? Are there empirical studies on this? (2) For the competitors  approaches, what would their Figure 4 look like? In particular, for approaches that use "small loss trick", what would Figure 4 look like? How would Figure 4 be different for the proposed process (i.e extracting hard "confident" examples) and others (like extracting just hard examples without considering confidence, or just confident examples without considering hardness)? (3) Are there studies that deliberately initialize the process with noisy data, to see how sensitive the process is before the "positive cycle" begins? 
Understanding the quality of the solutions found by gradient descent for optimizing deep nets is certainly an important area of research. The reviewers found several intermediate results to be interesting.  At the same time, the reviewers unanimously have pointed out various technical aspects of the paper that are unclear, particularly new contributions relative to recent prior work. As such, at this time, the paper is not ready for ICLR 2018 acceptance.
The paper proposes a new reinforcement learning actor critic type algorithm for parameterized policy spaces. The actor builds gradient estimates derived from perturbations of the policy (in the spirit of simultaneous perturbation stochastic approximation (SPSA) or Flaxman Kalai McMahan s "Gradient Descent without a Gradient" idea), while the critic is based on standard temporal difference (TD) learning. The algorithm is benchmarked, along with other well known techniques, on Mujuco based environments where it is seen to often perform well.  There were several concerns raised by the reviews initially, including the validity of the value function obtained by the rather non standard perturbation of the behavior policy suggested in the paper, the necessity of the zeroth order scheme, the impact of the hyperparameter N, the lack of clarity about the overall algorithmic flow, and the lack of more contemporary baselines such as SAC, A3C and TD3.  Most concerns appear to have been addressed by the author(s) in their detailed responses, and new explanations have been added with significant effort, to the credit of the author(s). While the paper breaks new ground in the conceptual sense, and the reviewers are borderline positive about the paper, I am afraid that parts of the paper, especially relating the the soundness of the algorithm, are still unclear and not concretely motivated. This, coupled with the low confidence levels expressed in the reviewers  evaluations, renders the paper s form too preliminary at this stage to merit acceptance.  For instance, I notice upon a careful reading of the paper the following issues:  (a) Equation (7) is derived by claiming that $V^\beta(s_t)$ is uncorrelated with the Gaussian noise $\epsilon$. However, I fail to see why this should hold, since the paper mentions, in the paragraph before equation (6) that $\beta   \pi_{\theta + \sigma \epsilon}$, so $\beta$ ostensibly clearly depends on $\epsilon$.  (b) The motivation behind the objective $J_{ZOAC}$ in (6), and the quantities involved in its definition, is rather opaque. For instance, the right side of (6) suggests an infinite horizon discounted reward criterion, whereas the expectation is taken with respect to $d^\beta$, the "stationary distribution" of the policy $\beta$. How/why is this justified? I would expect the use of the discounted occupancy measure here, instead of the (long term) stationary measure which washes out any near term trajectory effects.  (c) The paper mentions that $\epsilon$ is a sequence of random perturbations *per time step* in (6) as opposed to the usual ES perturbation of a one time perturbation. However, the size of the covariance matrix $I$ in (6) and (3) are not explicitly distinguished, leading to much confusion in the mind of the keen reader.  I hope that the author(s) can utilize the feedback from the reviews in order to put up a significantly clearer and solidly motivated paper in the next round, so that its conceptual merits can be proven without doubt. Thanks and best wishes.
This paper proposes an extension of MPO for on policy reinforcement learning. The proposed method achieved promising results in a relatively hyper parameter insensitive manner.  One concern of the reviewers is the lack of comparison with previous works, such as original MPO, which has been partially addressed by the authors in rebuttal. In addition, Blind Review #3 has some concerns with the fairness of the experimental comparison, though other reviews accept the comparison using standardized benchmark.  Overall, the paper proposes a promising extension of MPO; thus, I recommend it for acceptance. 
The paper introduces a simple yet effective technique for supervised pre training based on kNN lookup from a MoCo memory queue . Initially, the reviewers raised concerns about limited novelty with respect to neighborhood component analysis, baseline results lower than the original papers, and several other questions such as how many positive samples fall in and out of kNN. The author response was strong, adequately addressing the reviewer’s comments with additional experiments and clarifications. After the discussion period, three reviewers recommended borderline acceptance. One reviewer maintained score 5, suggesting a more exhaustive search for hyper parameters, but indicated he/she was on the fence and would be ok if the paper is accepted. The AC considers the response of the authors regarding hyper parameter search (and the small gap from other reported results) is reasonable, and agrees with the majority that the paper passes the acceptance bar of ICLR.
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. However, the novelty of this paper is rather marginal and given the  high competition at ICLR2020, this paper is unfortunately below the bar. We hope that the reviewers  comments are useful for improving the paper for potential future publication. 
This paper attracted strong praise from the reviewers, who felt that it was of high quality and originality.  The broad problem that is being tackled is clearly of great importance.  This paper also attracted the attention of outside experts, who were more skeptical of the claims made by the paper. The technical merits do not seem to be in question, but rather, their interpretation/application. The perception by a community as to whether an important problem has been essentially solved can affect the choices made by other reviewers when they decide what work to pursue themselves, evaluate grants, etc. It s important that claims be conservative and highlight the ways in which the present work does not fully address the broader problem of adversarial examples.  Ultimately, it has been decided that the paper will be of great interest to the community. The authors have also been entrusted with the responsibility to consider the issues raised by the outside expert (and then echoed by the AC) in their final revisions.  One final note: In their responses to the outside expert, the authors several times remark that the guarantees made in the paper are, in form, no different from standard learning theoretic claims: "This criticism, however, applies to many learning theoretic results (including those applied in deep learning)." I don t find any comfort in this statement. Learning theorists have often focused on the form of the bounds (sqrt(m) dependence and, say, independence from the # of weights) and then they resort to empirical observations of correlation to demonstrate that the value of the bound is predictive for generalization. because the bounds are often meaningless ("vacuous") when evaluated on real data sets. (There are some recent examples bucking this trend.) In a sense, learning theorists have gotten off easy. Adversarial examples, however, concern security, and so there is more at stake. The slack we might afford learning theorists is not appropriate in this new context. I would encourage the authors to clearly explain any remaining work that needs to be done to move from "good enough for learning theory" to "good enough for security". The authors promise to outline important future work / open problems for the community. I definitely encourage this.     
This clearly written paper develops a novel, sound and comprehensive mathematical framework for computing low variance gradients of expectation based objectives. The approach generalizes and encompasses several previous approaches for continuous random variables (reparametrization trick, Implicit Rep, pathwise gradients), and conveys novel insights.  Importantly, and originally, it extends to discrete random variables, and to chains of continuous random variables with optionally discrete terminal variables. These contributions are well exposed, and supported by convincing experiments. Questions from reviewers were well addressed in the rebuttal and helped significantly clarify and improve the paper, in particular for delineating the novel contribution against prior related work. 
This paper addresses the problem of goal navigation in unseen environments by learning to build a local, then a registered, global occupancy and semantic map of object categories from reprojected RGB+D observations, while extrapolating (hallucinating) unseen observations from contextual semantic priors (e.g., "tables are usually surrounded by chairs"). It then uses a measure of epistemic uncertainty on different estimations (realisations) of that map as a navigation goal selection policy to perform active exploration, and controls the agent using a local goal driven policy; different information gain metrics are investigated. Essentially, the policy accumulates the predicted semantic maps and uses the uncertainty of the semantic mapping to select informative goals. The semantic map predictor is implemented as three U nets for occupancy extrapolation from depth projection, semantic segmentation of RGB, and semantic map inference from the ground projection and extrapolated depth projection maps. The whole method is evaluated on the Matterport3D environment.  Reviewers praised the well written and comprehensively evaluated study, the active learning formulation and the idea of epistemic information gain as a measure of uncertainty for goal selection, and the code availability. Reviewers  major concerns included the computational cost of ensemble based uncertainty estimation (Nmyk), and a missing submission to an active learderboard of the habitat challenge (Nmyk, CF2f). Reviewers CF2f and qy8x had a longer list of issues that have been addressed in the rebuttal.  Reviewers engaged in a discussion with the authors, and the scores are 5, 6 (though not updated) and 8. I believe that the paper just meets the conference acceptance bar and would advocate for its inclusion in the conference.
In this paper, the authors extend the performative prediction framework of Perdomo et al. (2020) to a multi agent, game theoretic setting, and they examine how and when multi agent performative learning may lead to performative stability/optimality.  The authors  results and contributions can be summarized as follows:   They consider a multi agent location scale distribution map with parameters constrained in a simplex, and they study the dynamics of an exponentiated gradient descent algorithm (EGDA for short) inspired by Kivinen and Warmuth (1997).   If the learning rate is small enough, the authors show that EGDA converges to a performatively stable point (under the same assumptions that guarantee existence of a convex potential).   On the other hand, if the learning rate is large, the algorithm behaves chaotically.  The reviewers  initial assessment was mixed, but after the authors  rebuttal, some concerns were partially addressed and the scores of the paper were upgraded to borderline positive. On a point by point basis, the authors  result on the convergence of EGDA with a small learning rate was appreciated by the reviewers, but it was not otherwise deemed significant enough relative to existing convergence results for gradient methods. Instead, most of the discussion centered on the authors  result on chaos (Theorem 4.6), which was viewed as the most significant contribution of the paper. However, continued discussion between committee members revealed that this result follows directly from Theorem 3.11 and Corollary 3.12 of the arxiv preprint "A family of chaotic maps from game theory" by T. Chotibut, F. Falniowski, M. Misiurewicz, and G. Piliouras <https://arxiv.org/abs/1807.06831>, which is not discussed in the paper. [As was pointed out, the update map (7) of the paper coincides with the update rule (7) of the arxiv preprint, and the proof techniques are likewise identical.]  This overlap with previous work was considered a "big omission" and it pushed the paper below the acceptance threshold. In the end, the paper was not supported by any of the reviewers, so a "reject" recommendation was made.
This paper proposes using RL to solve PDEs, with application to solving conservation laws. It is quite borderline, with one reviewer weakly recommending acceptance, one finding the paper interesting but the application not sufficiently novel, and one confessing they have not understood the paper.  I concur with R2 this is a difficult subject matter, but the other reviewers seem satisfied with the clarity of the presentation. R3 seems to believe the paper sufficiently proves the concept to warrant publication. I confess I do not understand R1 s argument for lack of novelty, despite my pushing for further detail. I see this as a novel application of RL methods, and R1 admits this will be seen as novel for a PDE conference. I am in favour of a certain degree of interdisciplinarity at ICLR, and believe this paper could bring a bit of subject matter diversity to the programme. However, due to the number of high quality submissions in my area, I m afraid this one must be rejected due to limited space. The authors are encouraged to submit to another ML conference after addressing (or having addressed) some of the action items from the more sympathetic reviewers.
All the reviewers agree that this paper was poorly written, which I agree upon my own reading of this paper. Section 1 is rather telegraphic and difficult to comprehend. Section 2 is cryptic in several respect, including what space of probability distributions the authors consider the Wasserstein distance, what QR task the objective function (5) for the discriminator corresponds to, especially after letting $a +\infty$ and $b  \infty$, and so on. The numerical experiment results do not seem convincing enough to demonstrate advantage of the proposal over existing methods. The authors did not respond to the reviews, so that many concerns raised by the reviewers have not been resolved. I would thus recommend rejection of this paper.
This paper studies the behavior of SGD for linear models fit with the squared Euclidean loss. There are three main results:  The first result (Sec. 4) studies the behavior where instead of regularizing the objective, Gaussian noise is added to the inputs. The main result is a sufficient condition for how the learning rate and noise can jointly change over time in order for SGD on the MSE error with noisy input to asymptotically converge to the same solution as regular gradient descent without noisy input.  The second result (Sec. 5) is slightly more general in that is considers the case where the noise can be an isotropic Gaussian where the variance changes over time. Again, a result is given for how the learning rate interacts with the data in order to asymptotically converge to the unregularized solution. This is first studied in Thm 5.1 then assuming power law decay in the noise in Thm. 5.2. It should be emphasized that though these are asymptotic guarantees, the results give asymptotic *rates* of convergence. In my opinion this is a significant strength of the results that was not emphasized by the reviewers.  The third result (Sec. 6) studies SGD for least squares linear models where the stochasticity is due to data subsampling only. The fraction of subsampled data may change over time.  The primary sentiment from reviewers was that the mathematical complexity of the paper meant that they could not understand it or give a fair review. (More on this below.) For this reason, and because the overall reviews are somewhat borderline, I read the paper in detail. A specific concern raised by two reviewers was that the paper first presents a very general framework but then studies very restricted specific problems. Some reviewers felt that the paper was very well written, while others felt it was poorly written. There are no experiments.  For my part, I mostly concur that the paper is well written (albeit quite technical). However, I agree with the concern from reviewers that the technical results all concern extremely restricted settings, and it s not clear what value the extremely general setup brings. I also find the title of the paper a bit puzzling. For specifics of the results, the practical value of Sections 4 and 5 is unclear. It s well known that adding data noise is exactly equivalent to adding ridge regularization when doing linear regression. But ridge regularized linear regression would be a non stochastic problem. So what is the value of studying the convergence rates in this case? The paper never makes this clear.  I have concerns about the exponential convergence rate in Thm. 6.1. The paper claims that an exponential convergence rate for SGD has been extensively studied. I do not believe this is true. In general SGD does not have an exponential convergence rate. There are modified methods like SAG that achieve this on finite data sets, but that s not what s studied here. The paper cites two papers: The first is Bottou et al. (2018). This is a lengthy review, with no specific reference given. I am familiar with it and also spent time searching but could not find a specific result. Ma et al. (2018) is also cited. This indeed gives an exponential convergence rate but assuming that at the optimum the loss for all datapoints is zero! No such assumption is made in the submitted paper, and the issue is not further discussed. This is cause for grave concern.
All four reviewers expressed significant concerns on this submission during review. None of them is willing to change their evaluations and supports this work during discussions. Thus a reject is recommended.
In this paper, the authors provide a model based approach for combining experimental and observational data in reinforcement learning, specifically in POMDPs.  The paper was not received very favorably by reviewers, with the main concerns revolving around: (a) writing quality, (b) validation, (c) extent of contribution given existing work on causal RL.  In preparing your revision, in addition to clarifying writing, and adding better validation, I would urge the authors to consult existing causal inference literature on point and partial identification in settings related to RL, such as off line policy learning.  This will help address issues of novelty by extending their approach to settings with more types of confounding.  In addition to useful references suggested by reviewers, another useful draft may be:  "Path Dependent Structural Equation Models." Srinivasan, R., Lee, J., Bhattacharya, R., and Shpitser, I.. In Proceedings of the Thirty Seventh Conference on Uncertainty in Artificial Intelligence.
R1 thought the proposed method was novel and the idea interesting. However, he/she raised concerns with consistency in the experimental validation, the trade off between accuracy and running time, and the positioning/motivation, specifically the claim about interpretability. The authors responded to these concerns, and R1 upgraded their score. R2 didn’t raise major concerns or strengths. R3 questioned the novelty of the work and the experimental validations. All reviewers raised concerns with the writing. Though I think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar. I think this is a paper that could make a good workshop contribution. 
The concept of increasing entropy in novel states to promote exploration appears to be quite interesting. I do appreciate this idea, and I would encourage the authors to study it further. I think the reviewers also agree with this. Unfortunately, the paper as written has a number of issues: (1) the theoretical motivation for this is quite weak   it intuitively makes sense, but for a published paper, we need more than intuition; (2) the empirical results are not especially compelling, it seems like the authors have to kind of thread the needle in arguing that they are concerned specifically with dense reward exploration   a less chartable view is that the method just doesn t work all that well compared to other exploration settings in problems that present a major exploration challenge. The reviewers generally found the evidence in favor of the method to be a bit questionable. Therefore, in the balance, while the paper presents what I think is a really nice idea, it still needs work to justify both theoretically and empirically. I would encourage the authors to flesh out this work more   I think with a bit more work, it could be a really nice paper, but for now it s probably not quite ready.  A few more comments (which did not influence the decision, but I recommend addressing in the future):    Presenting exploration and efficiency results in a table, like Table 1, is not good. It hides actual patterns in performance, especially if you are talking about exploration. To the authors  credit, it appears that this trend was started by prior work (e.g., Kimin Lee et al.), but it was bad scholarship then, and it s bad scholarship now   it s quite easy to pick a checkpoint where a given method looks better than all the other methods (which might be why some prior work opted for this format), but it s misleading to the reader and should not be done.    I don t agree with Reviewer 2 s comments about random seeds. Certainly it s better to have more random seeds than less, but this doesn t appear out of line with the standard in the field. That said, the results in Figure 4 do look quite close, so trying an actual statistical significance test might be a good idea (again, this didn t influence my decision   the standard in RL holds that 4 6 seeds is plenty, and we have to review work by the current standard in the field).
The reviewers had a number of concerns which seem to remain after the authors response. In particular, the reviewers were concerned about the validity of the paper s assumptions in real world applications and lack of experimental results. Also, while the reviewers acknowledge the novelty in technical contributions, they suggested that the authors explain more clearly how the results of this paper are distinguishable from prior art.
This paper tries to address the uncertainty calibration problem in meta learning by weighting the gradient from different tasks according to class wise similarity. There have been many concerns raised by the reviewers and most of them either are still not properly addressed after the rebuttal period.   The main concerns are as follows:   The problem the paper tries to address is not clear. The use of weighting in meta update is motivated from distributional uncertainty, but it is not clear how that will improve the task calibration at meta testing time.   The proposed update runs into the risk of focusing on simple tasks and down weighting hard tasks that could be improved with more learning to get more discriminative features. That might hurt the classification performance even though it gets better calibration quality.   Novelty is limited. The proposed method is a fairly small modification on the original MAML algorithm.   More comprehensive empirical evaluation is required to support the superiority of the proposed method to other baselines.  I suggest the authors take all the reviewers  comments seriously and improve their work for a better revision. 
This paper proposes a bidirectional joint image text model using a variational hetero encoder (VHE) randomized generative adversarial network (GAN). The proposed VHE GAN model encodes an image to decode its associated text. Three reviewers have split reviews. Reviewer #3 is overall positive about this work. Reviewer #1 rated weak acceptance, while request more comparison with latest works. Reviewer  #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work. During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns. Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance.
The paper addresess an important problem of neural net robustness verification, and presents a novel approach outperforming state of art; author provided details rebuttals which clarified their contributions over the state of art and highlighted scalability; this work appears to be a solid and useful contribution to the field.  
This work investigates the relationship between adversarial robustness and shape bias of neural networks. Reviewers pointed out that one of the primary questions being investigated "(a) how adversarially robust ImageNet classifiers (R classifiers) generalize to out of distribution examples;" has already been a primary focus of several prior works, and that many of the findings are already well established, or expected given known connections between adversarial robustness and corruption robustness. I recommend the authors rework the paper to focus more on building upon these prior results. As a possible example, the work would be strengthened if the authors compared adversarial training to other data augmentation strategies known to directly improve shape bias and corruption robustness, does adversarial training provide any unique ood robustness properties distinct from these other methods?
Dear authors,  Despite the desirable goal, that is to move away from regularization in parameter space toward regularization in function space, the reviewers all thought that the paper was not convincing enough, both in the choice of the particular regularization and in the experimental section.  While I appreciate that you have done a major rework of the paper, the rebuttal period should not be used for that and we can not expect the reviewers to do a complete re review of a new version.  This paper thus cannot be accepted to ICLR.
Meta score: 7  This paper presents a novel architecture for neural network based TTS using a memory buffer architecture.  The authors have made good efforts to evaluate this system against other state of the art neural TTS systems, although this is hampered by the need for re implementation and the evident lack of optimal hyperparameters for e.g. tacotron.  TTS is hard to evaluate against existing approaches, since it requires subjective user evaluation.  But overall, despite its limtations, this is a good and interesting paper which I would like to see accepted  Pros:    novel architecture    good experimentation on multiple databases    good response to reviewer comments    good results  Cons:    some problems with the experimental comparison (baselines compared against)    writing could be clearer, and sometimes it feels like the authors are slightly overclaiming  I take  the point that this might be more suitable for a speech conference, but it seems to me that paper offers enough to the ICLR community for it to be worth accepting.  
The reviewers thought this paper tackles an interesting question around whether MaxEnt RL already provides an important form of robustness. Such work helps us better understand the intersection between generalization, regularization and robustness. The reviewers had a number of comments, questions and clarifications and were generally satisfied with the detailed responses provided by the authors. There was some concern over the strength of the experiments and the authors also ran additional experiments. These addressed one reviewer’s concerns, though the other still thought the existing experiments were a bit too simple.
All three reviewers agree that the paper provide an interesting study on the ability of generative adversarial networks to model geometric transformations and a simple practical approach to how such ability can be improved. Acceptance as a poster is recommended.
This paper present a functional extension to NPI, allowing the learning of simpler, more expressive programs.  Although the conference does not put explicit bounds on the length of papers, the authors pushed their luck with their initial submission (a body of 14 pages). It is clear, from the discussion and the reviews, however, that the authors have sought to substantially reduce the length of their paper while improving its clarity.  Reviewers found the method and experiments interesting, and two out of three heartily recommend it for acceptance to ICLR. I am forced to discount the score of the third reviewer, which does not align with the content of their review. I had discussed the issue of length with them, and am disappointed that they chose not to adjust their score to reflect their assessment of the paper, but rather their displeasure at the length of the paper (which, as stated above, does push the boundary a little).  Overall, I recommend accepting this paper, but warn the authors that this is a generous decision, heavily motivated by my appreciation for the work, and that they should be careful not to try such stunts in future conference in order to preserve the fairness of the submission process.
This paper presents a distributed memory architecture based on a generative model with a VAE like training criterion. The claim is that this approach is easier to train than other memory based architectures. The model seems sound, and it is described clearly. The experimental validation seems a bit limited: most of the comparisons are against plain VAEs, which aren t a memory based architecture. The discussion of "one shot generalization" is confusing, since the task is modified without justification to have many categories and samples per category. The experiment of Section 4.4 seems promising, but this needs to be expanded to more tasks and baselines since it s the only experiment that really tests the Kanerva Machine as a memory architecture. Despite these concerns, I think the idea is promising and this paper contributes usefully to the discussion, so I recommend acceptance.
This paper is an easy accept   three reviewers have above threshold scores, while one reviewer is slightly below threshold, but based on the submitted manuscript.  It appears that the paper has substantially improved based on reviewer comments.  Pros:  All reviews had positive sentiment: "very elegant and general idea" (Reviewer4); "idea is interesting and potentially very useful" (Reviewer2); "method is novel, the explanation is clear, and has good experimental results" (Reviewer3); "a good way to learn a policy for resetting while learning a policy for solving the problem.  Seems like a fairly small but well considered and executed piece of work." (Reviewer1)  Cons:  One reviewer found that testing in only three artificial tasks was a limitation.  The initial reviews noted several issues where clarification of the text and/or figures was needed.  There were also a bunch of statements where the reviewers questioned the technical correctness / accuracy of the discussion.  Most of these points appear to have been adequately addressed in the revised manuscript.
The paper proposes an improvement to graph based neural network, by improving their attention mechanism (introducing recursive attention and jumping knowledge attention) to flexibly attend to its neighborhood. The paper shows solid experimental results over competitive baselines, as acknowledged by reviewers. The reviewers agree that the paper is clearly written, but overall have issues with the novelty of the approach. The paper combines multiple components (last residual connection module, improved attention mechanism) to show gains, but none of the pieces are very new.
sadly, none of the reviewers seem to have been able to fully appreciate and check the proofs.  but in the words of even the least positive reviewer: In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.  i think we can all gain from fresh perspectives of LSTMs and DL for NLP :) 
The paper revisits the traditional bias variance trade off for the case of large capacity neural networks. Reviewers requested several clarifications on the experimental setting and underlying results. Authors provided some, but it was deemed not enough for the paper to be strong enough to be accepted. Reviewers discussed among themselved but think that given the paper is mostly experimental, it needs more experimental evidence to be acceptable. Overall, I found the paper borderline but concur with the reviewers to reject it in its current form.
The paper makes overly strong claims, too weakly supported by a hard to follow and insufficiently rigorous mathematical argument. Connections with a large body of relevant prior literature are missing.
This paper aims at making a deep RL policy interpretable and verifiable by distilling the policy represented by a deep neural network into an ensemble of decision trees. This should be done without hurting the performance of the policy. The authors achieve this by extending the existing Viper algorithm. The resulting approach can imitate the deep policy better compared with Viper while preserving verifiability. Experiments show that the proposed method improves in terms of cumulative reward and error rate over Viper in four benchmark tasks.  The amount of improvement over the original Viper is not convincing given the presented results. Moreover, reviewers uniformly agree that the contribution of this work is incremental. I therefore recommend to reject this paper.
The paper contributes to the community by introducing an approximation to distribution Q functions, based on the epistemic and aleatoric uncertainty. The reviewers believe the ideas make sense. However the presentation and its experiment results make it hard for them to understand some important details. For example, the reviewers are confused about why the empirical results show the proposed methods are better.     The majority of the reviewers are negative about the paper. After rebuttal, the reviewers are not convinced. Based on this, the meta reviewer recommends rejection. Authors can strengthen paper by improving its presentation and addressing the concerns from the reviewers.   
Pros:   compelling idea to use VAEs to reduce the dimensionality of the space in which to run evolution   non trivial benchmark results   clearly written, solid background  Cons:   moderate novelty (as compared to [1])   performance results are sup par   no rebuttal, despite constructive and detailed review comments (and an explicit willingness to raise scores by multiple points!)  The reviewers agree that the paper should be rejected in its current form, but would plausibly have been willing to reassess their scores for a major revision   which did not materialize.
To perform self supervised graph representation learning that is scalable to large graphs, the authors propose Bootstrapped Graph Latents (BGRL) that learns its graph representation by predicting alternative augmentations of the input, avoiding the need to construct negative examples. The weakness of the paper lies in its novelty, as it can be considered as a direct adaptation of the BYOL method, whose success has been demonstrated on self supervised visual representation learning, to learn graph node representations. While the novelty is limited, the paper has shown how to appropriately apply BYOL to graph representation learning, achieving state of the art results on graph node representation learning on large scale graphs. The overall assessment of the reviewers is that the empirical significance of the paper outweighs its shortcoming in novelty. The AC agrees with this assessment and hence recommends acceptance.
This work proposes algorithms for solving ERM with continuous losses satisfying the PL condition. The first algorithm achieves that by using a chainging noise variance and thus the paper frames the contribution in terms of the advantages of non constant noise rate.  The problem is a well studied one and the result is a nice if relatively modest improvement over Wang et al. However, as pointed out in reviews, in the context of convex optimization the same rate has already been established (Feldman,Koren,Talwar STOC 2020). This work is cited and briefly discussed but the discussion only includes one of the algorithms in the paper (that does have an additional log N factor). The overall assumptions in this paper are not comparable (weaker in some ways and stronger since they only require PL instead of strong convexity) but still the overall the contribution appears to be incremental.
This paper tackles an open set setting where new classes (with few labeled examples) are introduced after the initial pre training on different categories. A simple approach is proposed based on a normalized softmax classifier and feature averaging to generate a classifier for the new categories. Results are shown on a few standard datasets as well as the Pl@ntnet dataset.   While reviewers found the topic and setting (as well as Pl@ntnet dataset) interesting, they had significant concerns on the novelty (t3Uk, Tp5p, AHvJ), contribution, and rigor of the empirical evaluation. Since the method is simple and largely leverages from prior works, the latter is especially important; reviewers pointed out that some of the latest in metric learning is ignored (e.g. Proxy Anchor, Tp5p and AHvJ), and no comparison is made to other classes of methods that (by the authors  admission) are very close to the setting such as open set recognition (especially those that seek to classify new categories) and incremental learning.  Unfortunately, no rebuttal was provided by the authors, so these significant concerns remain and the paper cannot be accepted as is. Since the reviewers did appreciate the setting and dataset, I recommend refining the paper and significantly beefing up the empirical evaluation for future resubmissions.
The authors provide a new analysis of generalization in deep linear networks, provide new insight through the role of "task structure". Empirical findings are used to cast light on the general case. This work seems interesting and worthy of publication.
  + Original regularizer that encourages discriminator representation entropy is shown to improve GAN training.   + good supporting empirical validation     While intuitively reasonable, no compelling theory is given to justify the approach     The regularizer used in practice is a heap of heuristic approximations (continuous relaxation of a rough approximate measure of the joint entropy of a binarized activation vector)     The writing and the mathematical exposition could be clearer and more precise
All three reviewers found this to be an interesting exploration of a reasonable topic—the use of ontologies in word representations—but all three also expressed serious concerns about clarity and none could identify a concrete, sound result that the paper contributes to the field.
This paper gives a new theoretical framework to characterize the expressive power of graph neural networks that describes GNN by tensor language (TL) and then makes it possible to analyze its expressive power through the lens of TL. The authors connect the expressive ability of TL to the color refinement algorithms and (vertex/graph) k WL algorithms. By doing so, the several existing results can be recovered in a unifying manner. In addition to that, the function approximation ability is also investigated.   The paper gives a novel theoretical framework that gives a clear perspective to the problem of expressive power of GNN, which would be quite beneficial to the community and open up a new research direction. The reviewers have raised several questions on the paper, but the authors addressed all the concerns properly. Therefore, I recommend acceptance to ICLR2022.
This paper presents a new method for training GAN by adding a precondition Layer. All reviewers are positive about the empirical results. However, some concerns were raised about the justification: (1) Only linear networks are considered, which is a bit impractical; (2) Existing work has shown the importance of controlling the whole spectrum instead of the condition number. There should be some connection missing between the proposed result and existing results; (3) The computational cost is a bit high. The paper would be much stronger if these concerns could be addressed.
This paper was a very difficult case. All three original reviewers of the paper had never published in the area, and all of them advocated for acceptance of the paper. I, on the other hand, am an expert in the area who has published many papers, and I thought that while the paper is well written and experimental evaluation is not incorrect, the method was perhaps less relevant given current state of the art models. In addition, the somewhat non standard evaluation was perhaps causing this fact to be masked. I asked the original reviewers to consider my comments multiple times both during the rebuttal period and after, and unfortunately none of them replied.  Because of this, I elicited two additional reviews from people I knew were experts in the field. The reviews are below. I sent the PDF to the reviewers directly, and asked them to not look at the existing reviews (or my comments) when doing their review in order to make sure that they were making a fair assessment.   Long story short, Reviewer 4 essentially agreed with my concerns and pointed out a few additional clarity issues. Reviewer 5 pointed out a number of clarity issues and was also concerned with the fact that d_j has access to all other sentences (including those following the current sentence). I know that at the end of Section 2 it is noted that at test time d_j only refers to previous sentences, but if so there is also a training testing disconnect in model training, and it seems that this would hurt the model results.  Based on this, I have decided to favor the opinions of three experts (me and the two additional reviewers) over the opinions of the original three reviewers, and not recommend the paper for acceptance at this time. In order to improve the paper I would suggest the following (1) an acknowledgement of standard methods to incorporate context by processing sequences consisting of multiple sentences simultaneously, (2) a more thorough comparison with state of the art models that consider cross sentential context on standard datasets such as WikiText or PTB. I would encourage the authors to consider this as they revise their paper.  Finally, I would like to apologize to the authors that they did not get a chance to reply to the second set of reviews. As I noted above, I did try to make my best effort to encourage discussion during the rebuttal period.
This paper proposes PAC Bayes bounds for meta learning. The reviewers who are most knowledgeable about the subject and who read the paper most closely brought up several concerns regarding novelty (especially a description of how the proposed bounds relate to those in prior works (Pentina el al. (2014), Galanti et al. (2016) and Amit and Meir (2018))) and regarding clarity. The reviewers found theoretical analysis and proofs hard to follow. For these reasons, the paper isn t ready for publication at this time. See the reviewer s comments for details.
Reviewers are in full agreement for rejection.
In this paper, the authors motivate the paper well by the gap between the upper bound of the popular offline RL algorithm and the lower bound of the offline RL. By exploiting the special linear structure, the authors designed a variance aware pessimistic value iteration, in which the variance estimation is used for reweighting the Bellman loss. Finally, the upper bound of the proposed algorithm in terms of the algorithm quantity is proposed, which is more refined to reflexing the problem dependency. These results are interesting to the offline RL theoretical community.   As the reviewers suggested, several improvements can be made to further refinement, e.g.,     The intuition about the self normalization in the algorithm exploited to improve the upper bound should be introduced.    The discussion in Sec 3.3t about the insight of the improvement of the upper bound is not sufficient.    The extra computational cost about the variance should be discussed.
Paper proposes a method for active learning on graphs. Reviewers found the presentation of the method confusing and somewhat lacking novelty in light of existing works (some of which were not compared to). After the rebuttal and revisions, reviewers minds were not changed from rejection. 
This paper introduces T NAS, a neural architecture search (NAS) method that can quickly adapt architectures to new datasets based on gradient based meta learning. It is a combination of the NAS method DARTS and the meta learning method MAML.  All reviewers had some questions and minor criticisms that the authors replied to, and in the private discussion of reviewers and AC all reviewers were happy with the authors  answers. There was unanimous agreement that this is a solid poster.   Therefore, I recommend acceptance as a poster.
This work addresses the problem of understanding how pre trained language models are encoding semantic information, such as WordNet structure. This is evaluated by recreating the structure of WordNet from embeddings. The study also shows evidence about the limitations of current pre trained language models, demonstrating that all of them have difficulties to encode specific concepts.  pros:   good idea to reveal how well the pre training models encode the underlying knowledge graph   detailed understanding on how language models incorporate semantic knowledge and where this knowledge might be located within the models   experiments show that models coming from the same family are strongly correlated   the paper shows how individual layers of the language models contribute to the underlying knowledge   analysis of the different semantic factors (9 different factors, including number of senses, graph depth etc.)    paper is clearly written and understandable and includes enough details to understand the implementation of the semantic probing classifier.   cons:   weakly connected goals, response from reviewers is string around 3 main topics, which is seen as many for a single scientific paper. It would be easier to focus only on one topic and make a clear conclusion,   single word concepts while CE models are powerful in context,   lack of a profound analysis of the experimental results       hard to understand which semantic category the pre trained methods work well or not well,       clarification about the improvement of the semantic learning abilities based on these results.  Several of the identified issues have been answered in the author s rebuttal, however, the paper would still need more work to be accepted. Note also that the bar a this year ICLR conference is high and we encourage the authors to submit their updated work again at the next conference.
This paper studies the roots of the existence of adversarial perspective from a new perspective. This perspective is quite interesting and thought provoking. However, some of the contributions rely on fairly restrictive assumptions and/or are not properly evaluated.   Still, overall, this paper should be a valuable addition to the program. 
The authors’ present a precise definition of deployment efficient RL, where each new update of the policy may be costly, and theoretically analyze this for finite horizon linear MDPs. The authors include an information theoretic lower bound for the number of deployments required. The reviewers found this an important setting of interest and appreciated the theoretical contributions. The authors’ carefully addressed the raised points and also addressed questions about deployment complexity and sample complexity in their revised work. One weakness of the paper is that it does not provide empirical results and the linear MDP assumption, while quite popular in theoretical RL over the last few years, is quite restrictive. However,the paper still provides a very interesting theoretical contribution for an important topic and I recommend acceptance.
This paper presents a multi view generative model which is applied to multilingual text generation. Although all reviewers find the overall approach is important and some results are interesting, the main concern is about the novelty. At the technical level, the proposed method is the extension of the original two view KERMIT to multiviews, which I have to say incremental. At a higher level, multi lingual language generation itself is not a very novel idea, and the contribution of the proposed method should be better positioned comparing to related studies. (for example, Dong et al, ACL 2015 as suggested by R#3). Also, some reviewers pointed out the problems in presentation and unconvincing experimental setup. I support the reviewers’ opinions and would like to recommend rejection this time. I recommend authors to take in the reviewers’ comments and polish the work for the next chance. 
This paper studies memorization properties of convnets by testing their ability to determine if an image/set of images was used during training or not. The experiments are reported on large scale datasets using high capacity networks.   While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) more formal justifications are required to assess the scope and significance of this work contributions   see very detailed comments by R2 about measuring networks capacity to memorize and the role of network weights and depth as studied in MacKay,2002. In their response the authors acknowledged they didn’t take into account network weights and depth but strived at an empirical evaluation scenario.  (2) writing and presentation clarity of the paper could be substantially improved – see very detailed comments by R3 and also R2;  (3) empirical evaluations and effect of the negative set used for training are not well explained and analysed (R2, R3).  AC can confirm that all three reviewers have read the author responses and have contributed to the final discussion. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This paper proposes to interpret point cloud data in Euclidean space as samples from some underlying probability distribution. Thus a set of point cloud data can be given the structure of a statistical manifold with a Riemannian metric structure, namely the Fisher Information Metric. Applications to point cloud autoencoders are then studied.  Reviewers generally agree that the idea of equipping point cloud data with the Fisher information metric is interesting and has potential. However, there are concerns that the theoretical properties of the proposed framework have not been explored in depth. Furthermore, the experimental work should be enhanced to demonstrate the practical utility of the proposed formulation.
Three reviewers are mildly positive, while one is negative. The substantive comments of the reviewers are consistent with each other; it is merely their evaluations that differ.   One contribution of the paper is that it shows how using temperature tuning can yield similar accuracy to using batch normalization; this is useful because batch normalization is not always possible. The revised paper shows improvements, and we appreciate the engagement of the authors with the reviewer comments. However, there are remaining weaknesses such as a weak argument based on the empirical.results.  This paper can be improved based on the comments made by the reviewers. We encourage the authors to resubmit to a future venue.
This paper studies the problem of mode collapse in GANs. The authors present new metrics to judge the model s diversity of the generated faces. The authors present two black box approaches to increasing the model diversity. The benefit of using a black box approach is that the method does not require access to the weights of the model and hence it is more easily usable than white box approaches. However, there are significant evaluation problems and lack of theoretical and empirical motivation on why the methods proposed by the paper are good. The reviewers have not changed their score after having read the response and there is still some gaps in evaluation which can be improved in the paper. Thus, I m recommending a Rejection.
The paper presents an unsupervised method for graph representation, building upon Loukas  method for generating a sequence of gradually coarsened graphs. The contribution is an "encoder decoder" architecture trained by variational inference, where the encoder produces the embedding of the nodes in the next graph of the sequence, and the decoder produces the structure of the next graph.   One important merit of the approach is  that this unsupervised representation can be used effectively for supervised learning, with results quite competitive to the state of the art.   However the reviewers were unconvinced by the novelty and positioning of the approach. The point of whether the approach should be viewed as variational Bayesian, or simply variational approximation was much debated between the reviewers and the authors.   The area chair encourages the authors to pursue this very promising research, and to clarify the paper; perhaps the use of "encoder decoder" generated too much misunderstanding.  Another graph NN paper you might be interested in is "Edge Contraction Pooling for Graph NNs", by Frederik Diehl.  
This submission is an interesting case...  The method it presents appears to work quite well, achieving state of the art quantitative reconstruction results (though qualitatively, the reconstructed surfaces are locally noisy).  The method is quite complex, which different reviewers saw as either a strength or a weakness ("a mix of SoA techniques creatively woven together in a fairly sophisticated model" vs. "bulky and ad hoc").  Most critically: it appears that the reasons for the method s significant (14%) improvement over the prior art for this problem (Pixel2Mesh++) are not due to the novel contributions that the paper focuses on (multi headed attention, contrastive depth loss). Rather, it is other system design choices that are not novel research contributions that make up all but 1% of this difference (primarily, using a voxel grid predictor to get the initial mesh, as opposed to an initial ellipsoid mesh).  It might be possible for the authors to write a systems paper supporting these design decisions and showing how they lead to better results. However, this is not the paper the authors have written (the majority of the technical detail in the paper is focused on method components that make minimal impact). I would also argue that this hypothetical paper would not necessarily be appropriate for ICLR, since it does not focus on any new representations. It would be better suited to a venue such as CVPR, ICCV, or 3DV.  p.s. Reviewer 5 deserves all of the credit for noticing this major issue with the paper.
This paper studies how layer wise representation and task semantics affect catastrophic forgetting in continual learning. It presents two findings: 1. the higher layers contribute more to forgetting than lower layers, 2. intermediate level similarity between tasks causes the maximal forgetting. It also indicates that existing methods employ either feature reuse or orthogonality to mitigate forgetting.  Pros:   The layer wise analysis of catastrophic forgetting and investigation of different mitigating forgetting methods are important and interesting.   The paper is well motivated and well written.   The results can potentially help to suggest new approaches for developing and measuring mitigation methods.  Cons before rebuttal:   The paper misses discussion on and takeaways from the findings.   How general are the findings? There is a different observation by Kirkpatrick et al. 2017.   Limited diversity of experiments, because the experiments are only done on image classification tasks with CIFAR10 and CIFAR100.  The authors conducted more experiments and updated the paper with added explanations and results. The reviewers found the new evidence and arguments in the rebuttal to be convincing and the authors addressed most concerns.  In summary, the findings from this paper will help researchers better understanding and addressing catastrophic forgetting, and will be of interest to the community. Hence, I recommend acceptance. 
This paper studies the role of pooling in the success underpinning CNNs. Through several experiments, the authors conclude that pooling is neither necessary nor sufficient to achieve deformation stability, and that its inductive bias can be mostly recovered after training.   All reviewers agreed that this is a paper asking an important question, and that it is well written and reproducible. On the other hand, they also agreed that, in its current form, this paper lacks a  punchline  that can drive further research. In words of R6, "the paper does not discuss the links between pooling and aliasing", or in words of R4, "it seems to very readily jump to unwarranted conclusions". In summary, the AC recommends rejection at this time, and encourages the authors to pursue the line of attack by exploring the suggestions of the reviewers and resubmit.  
This paper was borderline, based on the reviews. The paper points out an interesting connection (somewhat known but not in this specific version) and good experimental results. However, numerous reviewers raised concerns that the paper was lacking a comparison to prior work connecting unsupervised learning and meta learning, most notably, Hsu et al. (2019).  After reading the revised version of the paper, the authors address this issue and also all the other reviewer comments. In relation to prior work they clarify that they focus on the contrastive unsupervised case and also do a good job in answering other reviewer concerns relative to novelty and results.   I would also like to point out, as reviewers also did that the previous title was a bit aggressive and provocative. Gladly the authors agree to change it to a more scientific `The Close Relationship Between Contrastive Learning and Meta Learning”.   Overall I think the authors have done a good effort on addressing the reviewer concerns and I think the paper would be interesting for ICLR readers.
This paper does two things. First, it proposes an approach to estimating the mutual information between the input, X, or target label, Y, and an internal representation in a deep neural network, L, using MINE (for I(Y;L)) or a variation on MINE (for I(X;L)) and noise regularization (estimating I(X;L+ε), where ε is isotropic Gaussian white noise) to avoid the problem that I(X;L) is infinite for deterministic networks and continuous X. Second, it attempts to validate the information bottleneck theory of deep learning (Tishby and Zaslavsky, 2015) by exploring an approach to training DNNs that optimizes the information bottleneck Lagrangian, I(Y;L) − βI(X;L+ε), layerwise instead of using cross entropy and backpropagation. Experiments on MNIST and CIFAR 10 show improvements for the layerwise training over cross entropy training. The penalty on I(X;L+ε) is described as being analogous to weight decay. The reviewers raised a number of concerns about the paper, the most serious of which is that the claim that the layerwise training results validate the information bottleneck theory of deep learning is too strong. In the AC s opinion, R1 s critique that "[i]f the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?" is critical, and the authors  reply that "this quantity is in fact a more appropriate measure for “compactness” or “complexity” than the mutual information itself" undermines their claim that they are validating the information bottleneck theory of deep nets because the information bottleneck theory claims to be using mutual information. The AC also suggests that if the authors wish to continue this work and submit it to another venue, they (1) discuss the fact that MINE estimates only a lower bound that may be quite loose in practice and (2) say in their experimental section whether or not the variance of the regularizing noise was tuned as a hyperparameter, and if so, how results varied with different amounts of noise. Finally, the AC regrets that only one reviewer participated in the discussion (in a very minimal way), despite the reviewers  receiving several reminders that the discussion is a defining feature of the ICLR review process.
The paper provides theoretical justification for a previously proposed method for uncertainty estimation based on sampling from a prior distribution (Osband et al., Burda et al.).  The reviewers initially raised concerns about significance, clarity and experimental evaluation, but the author rebuttal addressed most of these concerns.  In the end, all the reviewers agreed that the paper deserves to be accepted.
While novelty is not the main strength of this paper, there is consensus that presentation is clear and the experimental results are convincing. Given the practical importance of designing and benchmarking methods to compactify deep nets, the paper deserves to be presented at ICLR 2018.
The authors study the problem of augmenting embedding based entity alignment in knowledge graphs (KG) through the use of joint alignment with deduced neural ontologies (more specifically, alignment of the KG  neural  axioms). Motivated by the observation that the representation between two potentially aligned entities must be bound by a minimal margin, which can be problematic when there are many potential alignments, they propose aligning neural axioms by Wasserstein distance based loss between learned entity embeddings conditioned on the relation embeddings. Experiments are conducted on OpenEA against multiple strong baselines   showing that adding the ontology alignment to these baselines improves the results.    Pros   + The addition of aligning (conditional) ontologies is ostensibly novel. + For KGs with sufficient entity/relation overlap, the proposed NeoEA method is applicable.  + NeoEA has been shown empirically to improve many SoTA methods.    Cons      While the theoretical justification is a welcome motivation, the reviewers did not find the theoretical arguments significant nor convincing.   Overall, the narrative needs work to make the paper more self contained and approachable for a broader range of readers. The reviewers (and myself) found many concepts and statements somewhat confusing and needing clearly context and contrast with existing works.  Evaluating along the requested dimensions:   Quality: Conceptually, the core idea is interesting, well motivated, original, and ostensibly effective. Empirically, NeoEA is shown able to improve upon several strong baseline (underlying) methods.  I believe that all of the reviewers find the work is interesting and promising. However, there were continuing concerns the strength/value of the described theory; it isn t clear if stronger theory isn t possible or if this just hasn t been fleshed out.    Clarity: Most of the reviewers (and myself) found the paper difficult to follow as a self contained work in terms of concepts, clear definitions (e.g., \mathcal T isn t defined early on) and the actual applicability of the theory. The figures help, but even these need some work. A related work section (or more structured presentation of related work) might be clarifying along with running examples and a more unifying math presentation that captures existing and proposed work. After thinking about this more, it is actually a relative simple (in a good way) and clever idea. However, it took several readings and readings of related work to get there. Additionally, the fact that all of the reviewers were concerned about different limitations is concerning wrt clarity. Appendix B helps a bit and I believe can also be put into the main paper.   Originality: As best as the reviewers and I can tell, we haven t seen this method applied to entity alignment despite this being a relatively mature subfield.   Significance: The consensus seems to be that the approach could be a notable contribution to an important area. However, it also appears that most of the reviewers don t feel the paper is ready for publication at a top tier venue yet.  As stated throughout this meta review, there are several aspects to like about this work including the originality of the idea, strong motivation, and good empirical results. However, we all agreed that the paper isn t quite ready in its current form   thus, I presently recommend reject for this submission.
The paper suggests that robust overfitting could be viewed as the early part of a double descent phenomenon for adversarial training. The authors identify implicit label noise, i.e. the label distribution mismatch between the true example and the generated adversarial example as a possible explanation for this phenomenon in adversarial training. This claim is empirically supported by experiments using static adversarial examples. The authors propose a method using temperature scaling and interpolation to mitigate the effects caused by implicit label noise for robust overfitting. This method is evaluated on CIFAR 10/100 and tiny Imagenet. Concerns have been raised in the reviews about sufficient justification for the claim that implicit label noise leads to adversarial overfitting. The rebuttal answers this question to some extent. Concerns have also be raised about the writing and whether sufficient details of the experimental setup are present in the main paper. While I acknowledge the difficulty of fitting all details within page limits, I would think that these details are crucial given that primary support for the claims made are from empirical observations.
The reviewers agreed that the paper presents interesting ideas but the presentation of the paper needs be improved. Also, the experiments and the related work section need be improved.
This paper studies the behavior of gradient descent on deep neural network architectures with spatial locality, under generic input data distributions, using a planted or "teacher student" model.   Whereas R1 was supportive of this work, R2 and R3 could not verify the main statements and the proofs due to a severe lack of clarity and mathematical rigor. The AC strongly aligns with the latter, and therefore recommends rejection at this time, encouraging the authors to address clarity and rigor issues and resubmit their work again.   
The authors propose a method for few shot learning for graph classification. The majority of reviewers agree on the novelty of the proposed method and that the problem is interesting. The authors have addressed all major concerns.
This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, i.e., it assigns higher probability to inputs that are out of the training distribution. This phenomenon is also shown to occur for several other dataset pairs. This finding is surprising and interesting, and the exposition is generally clear. The authors provide empirical and theoretical analysis, although based on rather strong assumptions. Overall, there s consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication.
This paper presents a guide for setting hyperparameters when fine tuning from one domain to another. This is an important problem as many practical deep learning applications repurpose an existing model to a new setting through fine tuning.  All reviewers were positive saying that this work provides new experimental insights, especially related to setting momentum parameters. Though other works may have previously discussed the effect of momentum during fine tuning, this work presented new experiments which contributes to the overall understanding. Reviewer 3 had some concern about the generalization of the findings to other backbone architectures, but this concern was resolved during the discussion phase. The authors have provided detailed clarifications during the rebuttal and we encourage them to incorporate any remaining discussion or any new clarifications into the final draft. 
This paper was assessed by three reviewers who scored it as 6/3/6.  The reviewers liked some aspects of this paper e.g., a good performance, but they also criticized some aspects of work such as inventing new names for existing pooling operators, observation that large parts of improvements come from the pre processing step rather than the proposed method, suspected overfitting.  Taking into account all positives and negatives, AC feels that while the proposed idea has some positives, it also falls short of the quality required by ICLR2020, thus it cannot be accepted at this time. AC strongly encourages authors to go through all comments (especially these negative ones), address them and resubmit an improved version to another venue.  
This work proposes a new framework that can learn the object centric representation for video. The authors did a good job during rebuttal and turned one slightly negative reviewer into positive ones. The final scores are 6,6,8,8. AC agrees that this work is very interesting and deserves to be published on ICLR. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are also encouraged to make other necessary changes.
The paper introduces a meta learning approach for re weighting samples for better adversarial robustness. Specifically, they parameterize the weights using an additional module and learn it with the MAML objective. I have read the paper and reviews carefully by myself and found that the paper has several weaknesses that are not well addressed in the rebuttal.   1) limited novelty. The proposed approach is a direct adaptation of the classical MAML algorithm to adversarial training, which is of limited technical novelty as pointed by serveral reviewers.   2) Adaptive attack experiments are incomplete. The proposed BiLAW relies an additional reweighting module in the training stage, though it will not be used in the test stage. But we can still use an independently learned reweighting head for adaptive attack, which is should be considered in the white box attacks. We do not want to see the new proposed defense will be defeated by other attacks quickly.   3) The true performance for BiLAW is problematic. Table 1 on MNIST is not representative for current development in adversarial community. On Table 2, comparing BiLAW with TRADES, for AA (0.031), 45.3% vs 51.7% on small CNN and 51.4% vs 52.1% on WRN 32 10. The performance of BiLAW is lower than TRADES, while when combine the two together, the results is very natural higher than TRADES and BiLAW but we are sure which component benefit the gains. For example, we can say TRADES benefits BiLAW because BiLAW+TRADES (52.6%) is much higher than BiLAW (45.3%). Also, the author did not show the results of single BiLAW on CIFAR 100. Considering the around two times running time, this performance is not acceptable in adversarial training methods.   Due to the above reasons, I cannot recommend acceptance in the current verison to ICLR.
The paper demonstrates that Gradient Descents generally operates in a regime where the spectral norm of the Hessian is as large as possible given the learning rate.   The paper presents a very thorough empirical demonstration of the central claim, which was appreciated by the reviewers.  A central issue to me in accepting the work was its novelty. Prior work has shown very closely related effects for SGD. The reviewers appreciated in discussions the novelty of the precise claim about the spectral norm hovering at around $\frac{2}{\eta}$. R4 and R2 also raised the issue that the related work discussion is not sufficient. Please make sure that you discuss very carefully related work in the paper, including a more detailed discussion in the Introduction.  The two key issues raised by R3, who voted for rejection, were that (1) the work studies Gradient Descent (rather than SGD), and (2) lack of theory. I agree with these concerns. Perhaps the Authors should address (1) by citing more carefully prior work that shows that a similar phenomenon does seem to happen in training with SGD. As for (2), I agree here with R1,R2 and R4 that empirical evaluation is a key strength of the paper.   Based on the above, it is my pleasure to recommend the acceptance of the paper. Thank you for submitting your work to ICLR, and please make sure you address all remarks of the reviewers in the camera ready version.
This paper tackles a really interesting and realistic problem: how does contradictory (potentially) fake information affect QA systems? The authors try to approach this problem by building a new dataset, starting with the widely used SQuAD and adding contradictory information. This is quite interesting, but the rest of the paper does not follow through. Reviewers ask a critical question: how would you distinguish the information that is fake, as opposed to valid, truthful information? Without this distinction, how would you train a language model to detect the fakeness and answer the question using the valid information? Unfortunately, the authors did not reply to this critical question, so it is difficult to judge the validity and contributions of this paper. There are also serious ethical implications which are discussed in the ethics review.
Manipulating deformable objects is an up and coming topic in robotics and machine learning, and it creates many interesting scientific and real world challenges. The paper looks into long horizon tasks of manipulation of deformable objects, using an interesting mix of more local trajectory optimization and differentiable physics. The reviewers agree on the interesting significance of the suggest work, all above acceptance threshold, but also a bit bimodal in terms of “just above” vs. “solidly good”. Thus, the paper appears an useful and discussion provoking accept for ICLR.
The paper studies gradient descent for matrix factorization with a learning rate that is large relative to the a certain notion of the scale of the problem. In particular, they show that the use of large learning rates leads to balancing between the two factors in the factorization.  The discussion between the authors and the reviewers was fruitful in dispelling some of the reviewers  doubts and at the same time improving the paper.  The paper seems to make some contribution on a relevant problem for the ICLR community. However, even in the restricted settings they consider, the problem does not appear to be completely solved. That said, I agree with the majority of the reviewers that the step forward seems enough to warrant the acceptance.  I would still encourage the authors to take into account the reviewers  comments in preparing the camera ready version. In particular, in the internal discussion it was suggested that the presentation of the paper could be improved by clearly stating the limitations of the current approach (e.g., the assumption of convergence in Theorem 5.1, a better discussion on large vs small learning rates w.r.t. the balancing effect).
The authors propose the use of Gaussian processes as the prior over activation functions in deep neural networks.  This is a purely mathematical paper in which the authors derive an efficient and scalable approach to their problem.  The idea of having flexible distributions over activation functions is interesting and possibly impactful.  One reviewer recommended acceptance with low confidence.  The other two found the idea interesting and compelling but confidently recommended rejection.  These reviewers are concerned that the paper is unnecessarily complex in terms of the mathematical exposition and that it repeats existing derivations without citation.  It is very important that the authors acknowledge existing literature for mathematical derivations.  Furthermore, the reviewers question the correctness of some of the statements (e.g. is the variational bound preserved?).  These reviewers agreed that the paper is incomplete without any empirical validation.  Pros:   A compelling and promising idea   The approach seems to be scalable and highly plausible  Cons:   No experiments   Significant issues with citing of related work   Significant questions about the novelty of the mathematical work
This paper proposes to do a fine grained analysis of how shape and texture play a role in the decisions made by CNNs. Lots of recent evidence suggests that CNNs exhibit a texture bias, and there has been considerable effort in understanding where this comes from and how to overcome it. The paper focuses in particular on understanding: (a) what fraction of the neurons are devoted to shape vs texture (roughly speaking), and (b) per pixel results using a convolutional readout function. The reviewers were divided at the time of submission and remained so at the end of discussion. At the end of discussion, the reviewers were split, with scores ranging from 4 (R1,R4), 7 (R2), and 8 (R3). The AC wants to thank and acknowledge the authors as well as all of the reviewers for their engagement in the discussion.    R2 and R3 are largely positive, driven by the extent of the experiments and the number of interesting results (e.g., how the fraction of the dimensions used for shape changes as a function of depth in the network). Both had smaller non critical concerns that were addressed (as far as the AC can tell) in the discussion.    R4’s most important concern, in the AC s view, is the question: could these results / different conclusions have been obtained via linear probe methods like Hermann et al.? The authors argue that analyzing the fraction of neurons used and at a per pixel is more fine grained than linear probes. This boils down to an intangible question of contribution, on which the AC is inclined to agree with the authors and R2 and R3: analyzing the dimensions contribute provides, at least to the AC, a complementary view to the linear probe and that this will be of interest to the ICLR community (although see final comment). R4 also had a number of smaller concerns that seem to be largely addressed (e.g., about correctness).   R1 argues primarily that the paper does not have a clear point or methodological contribution, for instance pointing out that readout modules were used in Hermann et al. or (as an example) arguing the readout function design is too simple.  The AC is inclined to agree with the authors’ response that the other reviewers seem to largely agree on the contribution (especially contributions via experiments rather than method) but disagree on how to weigh these contributions. The AC would also add that readout modules are a core idea for understanding neural representations that long predate Hermann and are by design (as the authors note) almost always as simple as possible.  At the end of the day, the AC is agrees with R2 and R3 for the contribution of the work and is inclined to accept. Given the other reviews, the AC does not agree with R1’s arguments, but would suggest that the authors think about how to sharpen their claims further. The AC is sympathetic to the concerns of R4, and urges the authors to think about a more concise and clean argument for R4’s concerns   many other readers will have similar concerns and as clean of an illustration will be helpful. Overall, the AC believes that the paper’s methods, experiments and analysis are of interest and value and is thus in favor of acceptance.
Thanks to the authors for the submission. This paper studies differentially private meta learning, where the algorithm needs to use information across several learning tasks to protect the privacy of the data set from each task. The reviewers agree that this is a natural problem and the paper presents a solution that is essentially an adoption of differentially private SGD. There are several places the paper can improve. For the experimental evaluation, the authors should include a wider range of epsilon values in order to investigate the accuracy privacy trade off. The authors should also consider expanding the existing experiments with other datasets. 
This paper concerns ensemble methods in deep reinforcement learning, examining several such methods, and proposes to address an important issue wherein ensemble members converge on a representation of approximately the same function, either by their parameters converging to an identical point or equivalent points that give rise to the same function. The authors propose a set of regularization methods aimed at improving diversity, and benchmark these augmentations on five ensemble methods and a dozen environments.  3 of 4 reviewers generally praised the method s simplicity and generality, and found the experiments convincing. Reviewer a9sA describes it as "clearly written and easy to follow", although others found clarity lacking in parts. There was agreement among these 3 reviewers that this was an interesting problem to tackle. Reviewer TfGq notes that this method lacks theoretical justification or guarantees, but that as a largely empirical paper this is perhaps of secondary importance. Reviewers 6miY and a9sA had questions about the precise choice of metrics, hyperparameters and seeds; the resulting discussion cleared up many of these concerns.  The most critical reviewer, i4M1, disputes the existence of the phenomenon at all, saying that "Neural networks converge to different solutions given the initialization is different and multiple local minima." The remainder of i4M1 s criticisms seem centered on the choice of environments and the number of seeds (also raised by other reviewers). The issue of seeds has been addressed partially and the authors have committed to strengthening their results in this regard.  Reviewer i4M1 s statement on the convergence of neural networks to different minima matches a bit of dated folk wisdom about neural networks, but the AC disputes this. The authors have cited a study from before the DL era properly began that identifies this issue and Section 5 addresses these criticisms directly. In practice, modern neural networks, especially with non saturating activations, tend to be surprisingly consistent across random seeds when trained against the same data stream, and more recent work posits that the loss landscape is less riddled with local minima than with saddle points (see e.g. Dauphin et al, 2014). _Equivalent_ minima are of course common due to scaling and permutation symmetries but SGD has a well documented preference for low norm solutions in the former case, and the authors  have chosen methods that would at least conceivably overcome these issues, by focusing on summary statistics of the representations rather than their precise values (and indeed, CKA is designed with these concerns in mind).  Despite i4M1 s incredulity I am inclined to agree with the majority of reviewers and view the paper as a worthwhile contribution to the body of knowledge (purely empirical though it may be) on both NN ensemble methods and DRL ensembles in particular. The introduction of measures from economics is clever and original, and the results are promising. A more exhaustive study on the entire Atari57 benchmark but can appreciate the resource problem this poses, and find that the suite of considered environments, combined with the augmentation of 5 different DRL ensemble methods, strikes a good balance. I concur on the issue of seeds and would encourage authors to include as many as possible for the camera ready, but on balance would recommend acceptance.
This paper proposed a semi supervised few shot learning method, on top of Prototypical Networks, wherein a regularization term that involves a random walk from a prototype to unlabeled samples and back to the same prototype.  SotA results were obtained in several experiments by using this method.  All reviewers agreed that the novelty of the paper is not such high compared with Haeusser et al. (2017) and the analysis and the experiments could be improved.
The authors derive a novel, unbiased gradient estimator for discrete random variables based on sampling without replacement. They relate their estimator to existing multi sample estimators and motivate why we would expect reduced variance. Finally, they evaluate their estimator across several tasks and show that is performs well in all of them.  The reviewers agree that the revised paper is well written and well executed. There was some concern about that effectiveness of the estimator, however, the authors clarified that "it is the only estimator that performs well across different settings (high and low entropy). Therefore it is more robust and a strict improvement to any of these estimators which only have good performance in either high or low entropy settings." Reviewer 2 was still not convinced about the strength of the analysis of the estimator, and this is indeed quantifying the variance reduction theoretically would be an improvement.  Overall, the paper is a nice addition to the set of tools for computing gradients of expectations of discrete random variables. I recommend acceptance.  
Learning on Riemannian manifolds can be easily done  with this Python package.  Considering the recent work on these in latent variable models, the package can be quite a useful approach.  But its novelty is disputed.  In particular Pymanopt is a package that does mostly the same, even though that may be computationally more expensive.  The merits of Geomstats vs. Pymanopt is not clarified.  But be that as it may, there is interest amongst the reviewers for the software package.  In the end, too, it s not uniformly agreed upon that a software describing paper fits ICLR.
This paper introduces an evolution strategy for solving the MAML problem. Following up on some other evolutionary methods as alternatives for RL algorithms, this ES MAML algorithm appears to be quite stable and efficient. The idea makes sense, and the experiments appear strong.  The scores of the reviews showed a lot of variance: 1,6,8. Therefore, I asked a 4th reviewer for a tie breaking review, and he/she gave another 8. The rejecting reviewer mostly took objection to the fact that learning rates / step sizes were not tuned consistently, which can easily change the relative ranking of different ES algorithms. Here, I agree with the authors  rebuttal: the fact that even a simple ES algorithm performs well is very promising, and further tuning would only strengthen that result. Nevertheless, it would be useful to assess the algorithm s sensitivity w.r.t. its learning rate / step size.  In summary, I agree with the tie breaking review and recommend acceptance as a poster.
The output kernel idea for lifelong learning is interesting, but insufficiently developed in the current draft.
This paper proposes a very interesting alternative to feed forward network layers, based on Quaternion methods and Hamilton products, which has the benefit of reducing the number of parameters in the neural network (more than 50% smaller) without sacrificing performance. They conducted extensive experiments on language tasks (NMT and NLI, among others) using transformers and LSTMs.   The paper appears to be clearly presented and have extensive results on a variety of tasks. However all reviewers pointed out that there is a lack of in depth analysis and thus insight into why this approach works, as well as questions on the specific effects of regularization. These concerns were not addressed in the rebuttal period, instead leaving it to future work. My assessment is that, with further analysis, ablation studies, and comparison to alternative methods for reducing model size (quantization, etc), this paper has the potential to be quite impactful, and I look forward to future versions of this work. As it currently stands, however, I don’t believe it’s suitable for publication at ICLR. 
The paper proposes a new recurrent unit which incorporates long history states to learn longer range dependencies for improved video prediction. This history term corresponds to a linear combination of previous hidden states selected through a soft attention mechanism and can be directly added to ConvLSTM equations that compute the IFO gates and the new state. The authors perform empirical validation on the challenging KTH and BAIR Push datasets and show that their architecture outperforms existing work in terms of SSIM, PSNR, and VIF. The main issue raised by the reviewers is the incremental nature of the work and issues in the empirical evaluation which do not support the main claims in the paper. After the rebuttal and discussion phase the reviewers agree that these issues were not adequately resolved and the work doesn’t meet the acceptance bar. I will hence recommend the rejection of this paper. Nevertheless, we encourage the authors improve the manuscript by addressing the remaining issues in the empirical evaluation.
The authors propose an alternative to batch norm, which they call POP norm, and provide theoretical justification for POP norm in nonconvex optimization on the basis of variance reduction. They then present empirical arguments.  One of the most cogent reviewers believed the theoretical results were known and the empirical arguments unconvincing because the method is similar to batch norm up to a change in learning rate and some minor differences.  Unfortunately, the reviewers did not engage with the author rebuttals at all. The authors seem to have addressed most points. However, if the reviewers are unwilling to engage, despite multiple emails, there s not much I can do, short of redoing the whole process from scratch. And I ll take the lack of engagement as lack of interest by the reviewers. Not being an expert in optimization myself, I m not going to override the scores. I do know enough to know that there are standard bounds for both convex and nonconvex optimization that improve with decreased variance. 
The focus of the submission is shape constrained regression, particularly the goal is to learn monotonic,  reasonably rich  functions. In order to tackle this task, the authors extend the monotonic regression framework (Gupta et al., 2016) which scales less benignly in the input dimension. They propose to use lattice functions with parameters having Kronecker product structure (, and their ensembles). The resulting function class can be (i) stored and evaluated in linear time (Proposition 1), (ii) characterized / checked from monotonicity perspective (Proposition 2). The efficiency of the approach is demonstrated in three real world examples.  Shape constrained regression is a central topic in machine learning and statistics. The authors propose a parametric family to learn monotonically constrained functions. The storage and the evaluation of the resulting functions are both fast (linear), and the numerical experiments are encouraging. The submission can be of definite interest to the ICLR community. 
VAE based sample selection for training NNs.  A well written experimental paper that is demonstrated through a number of experiments, all of which are minimal and from which generalization is not per se expected.  The absence of an underlying theory, and the absence of rigorous experimentation makes me request to extend either or, better, both.  
This paper concerns data augmentation techniques for NLP. In particular, the authors introduce a general augmentation framework they call CoDA and demonstrate its utility on a few benchmark NLP tasks, reporting promising empirical results. The authors addressed some key concerns (e.g., regarding hyperparameters, reporting of variances) during the discussion period. The consensus, then, is that this work provides a useful and relatively general method for augmentation in NLP and the ICLR audience is likely to find this useful.
Thank you for submitting you paper to ICLR. The consensus from the reviewers is that this is not quite ready for publication. The work is related to (although different from) Gu et al Neural Sequential Monte Carlo NIPS2015 and it would be useful to point this out in the related work section.
The paper proposes to use intra class mixup supplemented with angular margin to improve OOD detection.   Strengths: + Simple idea + Experiments on multiple datasets (although mostly focused on image benchmarks)  Weaknesses:   Justification for the idea could be improved. It d be nice to understand when we expect this to (not) work.   Differences from prior work "Angle based outlier detection in high dimensional data" could be better explained.  While the paper has some interesting contributions, the reviewers and I feel that the current version falls short of the acceptance threshold. I encourage the authors to revise and resubmit to a different venue.
This paper presents a new metric for disentanglement of learned representations, extending a prominent framework (DCI) to support object centric structured representations.  The reviewers agree on the importance of the question and find the metric a valuable contribution for addressing this problem. In the discussion, the reviewers identified some clarity issues that the authors have improved, leading to an overall much better writeup, as well as some deeper evaluation of learned matching agreements. The main remaining points that could be improved are    making the results more robust with thorough hyperparam tuning    connecting to other methods for inducing soft / probabilistic matchings, such as Sinkhorn or smooth&sparse optimal transport.  Please consider switching to the Times font as recommended by the ICLR style guide.
The paper proposes a method for data augmentation by cross modal data generation. While the reviewers agree that the paper addresses a relevant and important problem in medical imaging, they also agree on that the paper has limited novelty over the state of the art. Also the setup of experimental validation to comparison methods is questioned.
This paper presents work on action anticipation.  The reviewers appreciated the message passing based method.  However, concerns were raised regarding novelty, effectiveness, presentation, empirical results, and magnitude of impact for ICLR.  The reviewers considered the authors  response in their subsequent discussions but felt the concerns were not adequately addressed.  Based on this feedback the paper is not yet ready for publication in ICLR.
We have a very well informed reviewer who strongly feels that this paper is insufficiently novel and significant further discussion on how the paper might be raised to a publishable level with more empirical results.  I will have to side with the more engaged reviewers who feel that the paper should be rejected.
The paper considers the problem of knowledge grounded dialogue generation with low resources. The authors propose to disentangle the model into three components that can be trained on separate data, and achieve SOTA on three datasets.  The reviewers agree that this is a well written paper with a good idea, and strong empirical results, and I happily recommend acceptance.
This paper proposes an approach to handle the problem of unsmoothness while modeling spatio temporal urban data. However all reviewers have pointed major issues with the presentation of the work, and whether the method s complexity is justified. 
This paper explores a classification approach based on labeling pairs of inputs concurrently using a single network, rather than singletons. The authors test the approach on adversarial robustness (towards norm bounded perturbations), OOD detection next to basic standard accuracy calculations.  While the key idea is potentially interesting and the paper has received positive comments from the majority of reviewers, there were also some concerns that need to be addressed in a final manuscript:  * The paper does not motivate or explain theoretically why the joint classification framework is superior, beyond verbose arguments. These arguments need to be better clarified and linked with the experimental evaluation.  * While the empirical results are perceived as positive by the reviewers, one reviewer has raised the concern about the comparisons. The adversarial robustness and OOD comparisons are indeed basic. The adversarial attack used here is quite a weak PGD attack with a small radius and low iteration budget. Possibly include stronger attacks. The OOD comparisons are with standard baselines only. Please include further comparisons.    In its current form, the paper seems to be acceptable, and I strongly encourage the authors to improve both the theoretical justification, and empirical exploration in the final version.
The paper proposes an algorithm for zero shot generalization in RL via learning a scoring a function from.  The reviewers had mixed feelings, and many were not from the area. A shared theme was doubts about the significance of the experimental setting, and also the generality of the approach.  As this is my field, I read the paper, and recommend rejection at this time. The proposed method is quite laborious and requires quite a bit of assumptions on the environments to work, as well as fine tuning parameters for each considered task (number of regions, etc). I also agree that the evaluation is not convincing   stronger baselines need to be considered and the experiments to better address the zero shot transfer aspect that the paper is motivated by. I encourage the authors to take the review feedback into account and submit a future version to another venue.
The idea of using wavelet pooling is novel and will bring many interesting research work in this direction. But more thorough experimental justification such as those recommended by the reviewers would make the paper better. Overall, the committee feels this paper will bring value to the conference.
The paper presents an optimization technique for optical networks based on federated learning. The motivation for using federated learning stems from the privacy of datasets arising from different operators. The performance of the method is compared to the one based on centralized learning. Despite demonstrating an interesting and promising application of a federated learning, the paper is rather weak in its methodical contribution. Its experimental evaluation, however, is rather artificial with an FL problem generated by splitting the dataset for a centralized problem into parts. No response to the reviewers  comments was provided.
This paper extends recent work (Whittington & Bogacz, 2017, Neural computation, 29(5), 1229 1262) by showing that predictive coding (Rao & Ballard, 1999, Nature neuroscience 2(1), 79 87) as an implementation of backpropagation can be extended to arbitrary network structures. Specifically, the original paper by Whittington & Bogacz (2017) demonstrated that for MLPs, predictive coding converges to backpropagation using local learning rules. These results were important/interesting as predictive coding has been shown to match a number of experimental results in neuroscience and locality is an important feature of biologically plausible learning algorithms.  The reviews were mixed. Three out of four reviews were above threshold for acceptance, but two of those were just above. Meanwhile, the fourth review gave a score of clear reject. There was general agreement that the paper was interesting and technically valid. But, the central criticisms of the paper were:  1) Lack of biological plausibility The reviewers pointed to a few biologically implausible components to this work. For example, the algorithm uses local learning rules in the same sense that backpropagation does, i.e., if we assume that there exist feedback pathways with symmetric weights to feedforward pathways then the algorithm is local. Similarly, it is assumed that there paired error neurons, which is biologically questionable.  2) Speed of convergence The reviewers noted that this model requires many more iterations to converge on the correct errors, and questioned the utility of a model that involves this much additional computational overhead.  The authors included some new text regarding biological plausibility and speed of convergence. They also included some new results to address some of the other concerns. However, there is still a core concern about the importance of this work relative to the original Whittington & Bogacz (2017) paper. It is nice to see those original results extended to arbitrary graphs, but is that enough of a major contribution for acceptance at ICLR? Given that there are still major issues related to (1) in the model, it is not clear that this extension to arbitrary graphs is a major contribution for neuroscience. And, given the issues related to (2) above, it is not clear that this contribution is important for ML. Altogether, given these considerations, and the high bar for acceptance at ICLR, a "reject" decision was recommended. However, the AC notes that this was a borderline case.
This paper proposed MIDI DDSP, a structured hierarchical generative model which offers both detailed expressive controls (as in traditional synthesizers) as well as the realistic audio quality (as in black box neural audio synthesis). Overall the reviews are very positive. All the reviewers unanimously agree that the paper is very well written and presented a very convincing model and a meaningful step up from the earlier work of DDSP. The authors also presented a well documented website for the project and promised to release the source code. The reviewers raised some clarifying questions and minor corrections which the authors addressed during the response. Therefore, I vote for accept.
The authors admit the paper "was not written carefully enough and requires major rewriting."  This seems to be a frustratingly common phenomenon with work on the information bottleneck.  
This paper addresses the problem of poor generation quality in models for text generation that results from the use of the maximum likelihood (ML) loss, in particular the fact that the ML loss does not differentiate between different "incorrect" generated outputs (ones that do not match the corresponding training sequence).  The authors propose to train text generation models with an additional loss term that measures the distance from the ground truth via a Gaussian distribution based on embeddings of the ground truth tokens.  This is not the first attempt to address drawbacks of ML training for text generation, but it is simple and intuitive, and produces improvements over the state of the art on a range of tasks.  The reviewers are all quite positive, and are in agreement that the author responses and revisions have improved the paper quality and addressed initial concerns.  I think this work will be broadly appreciated by the ICLR audience.  One negative point is that the writing quality still needs improvement.
This paper builds on previous work on supermasks. It  proposes to replace binary masks by a signed supermask, i.e. a trainable, trashold based mask that can take values from { 1,0,1}. This change (in combination with the use of ELUS activation functions and an ELUS specific initialization strategy) leads to a significantly higher pruning rate while keeping competitive performance  in comparison to baseline models.   Most reviewers agreed that the paper is well written and that the proposed approach and the experimental findings are interesting. The motivation to improve interpretability was commonly perceived as misleading. Another  downside that was mentioned is the training time/efficiency. This however, should not be taken too much into account since the work focusses on finding the smallest possible subnetwork that still performs well (without changing the weight values) and  in line with work on the lottery hypothesis   aims at understanding more about the structure of the „winning tickets“ which is interesting for itself. The paper therefore should be accepted.
This paper studies the offline multi agent RL problem. The finding is that the dataset collected by one agent could be very different for other agents. The authors provide two solutions to this problem. Although being interesting, the reviewers found that the there are many imprecise math statements, and some of the methods are not well motivated. Hence, the overall recommendation is a reject.
This paper presents a probabilistic model for multitask learning with representation learning. The basic idea is to share information across tasks by making the prior over the model parameters of one task conditioned on a convex combination of the variational posteriors of the other tasks.  While some of the reviewers gave high scores and recommended acceptance, one of the reviewers (AnonReviewer3) had some pertinent concerns which lingered even after author response. In particular, AnonReviewer3 mentions that since the prior of one task is conditioned of the variational posteriors of the other tasks, the method is not a proper Bayesian approach. I also read the paper and agree with the assessment. Indeed, the common Bayesian way for multitask learning is to couple the tasks purely based on a prior that encouraging sharing across tasks instead of having task specific prior that depend on the variational posterior of other tasks as is being done in this paper.   I also read the reviews and the author response and have some other concerns as well:    There is a huge amount of prior work on multitask learning, both non Bayesian as well as Bayesian. Although the paper cites several of those it is disappointing that none of the baselines are Bayesian. Even the non Bayesian baselines aren t the state of the art recent methods, which is disappointing given the extensive body of prior work in this area.    The rebuttal wrongly claims MTRL and MRN to be Bayesian methods (included in Table 14 as baselines) whereas they only have a probabilistic formulation and only do point estimation. At a minimum, the paper should show comparison with some Bayesian multitask learning approaches (e.g., shared hierarchical priors, or task clustering, etc). The baselines such as MTRL and MRN aren t among the strongest ones out there.     The paper s title is way too generic. There are several multitask learning papers that use variational inference for a Bayesian model. Moreover, given that the basic formulation itself is a bit problematic to be called Bayesian, the title in some sense is also misleading.  Due to the above issues, I don t think the paper can be accepted in its current form.
 The reviewers have significantly different views, with one strongly negative, one strongly positive, and one borderline negative.  However, all three reviews seem to regard the NaaA framework as a very interesting and novel approach to training neural nets.  They also concur that the major issue with the paper is very confusing technical exposition regarding the motivation, math details, and how the idea works.  The authors indicate that they have significantly revised the manuscript to improve the exposition, but none of the reviewers have changed their scores.  One reviewer states that "technical details are still too heavy to easily follow."  My own take regarding the current section 3 is that it is still very challenging to parse and follow. Given this analysis, the committee recommends this for workshop.  Pros:         Interesting and novel framework for training NNs         "Adaptive DropConnect" algorithm contribution         Good empirical results in image recognition and ViZDoom domains  Cons:         Technical exposition is very challenging to parse and follow         Some author rebuttals do not inspire confidence.  For example, motivation of method due to "$100 billion market cap of Bitcoin" and in reply to unconvincing neuroscience motivation, saying "throw away the typical image of auction."
Meta score: 6  This is a thorough empirical paper, demonstrating the effectiveness of a relatively simple model for recommendations:  Pros:    strong experiments    always good to see simple models pushed to perform well    presumably of interest to practioners in the area  Cons:    quite oriented to the recommendation application    technical novelty is in the experimental evaluation rather than any new techniques  On balance I recommend the paper is invited to the workshop.
The authors did not respond to the concerns raised by all the reviewers. As the recommendation were on the edge, this lack of engagement seems odd, and it left the reviewers with little material to discuss and revise their recommendation. We recommend the authors carefully consider the reviews if they plan to resubmit.
The paper presents a method to learn inference mapping for GANs by reusing the learned discriminator s features and fitting a model over these features to reconstruct the original latent code z. R1 pointed out the connection to InfoGAN which the authors have addressed. R2 is concerned about limited novelty of the proposed method, which the AC agrees with, and lack of comparison to a related iGAN work by Zhu et al. (2016). The authors have provided the comparison in the revised version but the proposed method seems to be worse than iGAN in terms of the metrics used (PSNR and SSIM), though more efficient. The benefits of using the proposed metrics for evaluating GAN quality are also not established well, particularly in the context of other recent metrics such as FID and GILBO.  
The paper proposes a supernet learning strategy for NAS based on meta learning to tackle the knowledge forgetting issue. Forgetting happens when training a sampled sub model to optimize the shared parameters overrides the previous knowledge learned by the other sub models. The main idea of the paper is to consider training of each subnetwork as a task, and then apply MAML to ensure efficient cross task adaptivity. While the reviewers found the proposed method mainly an application of the existing meta learning strategies to one shot NAS, additional experimental results provided by the authors mostly convinced them about the effectiveness of the proposed method.
*Summary:* Study gradient flow dynamics of empirical and population square risk in kernel learning.   *Strengths:*    Empirical results studying several cases in MSE curves.   Explaining / solving certain phenomena in DL using kernels.   *Weaknesses:*    More motivations would be appreciated.    Technical innovation not so high.   *Discussion:*   Ud7D found that the main strength of this paper is the take home message rather than innovations. They concluded 7 might be appropriate for the evaluation. This opinion was seconded by WyHh who considered 7 the most appropriate rating. 5uQz also found that 7 would be the most appropriate rating. qXRH maintained concerns about the novelty of the work and rating 5. Nonetheless, they agreed the study is valuable and would not oppose acceptance.   *Conclusion:*   Three reviewers found this paper is definitely above the acceptance threshold (suggesting rating 7) and one more reviewer found it marginally below the acceptance threshold however not opposing acceptance. I found the general impressions from the discussion well described in a comment from Ud7D, who indicates that although this is not a breakthrough paper, it is a nice paper showing that a lot of DL phenomena are can be explained by Kernels. I conclude that the paper makes a sufficiently valuable contribution and hence I am recommending accept. I suggest the authors take the reviewers’ comments carefully into account when preparing the final version of the manuscript.
 The paper proposes a general framework for learn object centric abstractions represented using PPDDL (a probabilistic planning language).  The work assumes that objects and their attributes / features are identified.  The key contribution of the paper appears to be proposing to group individual objects into object types based on whether objects have the same outcome in planning. Using the learned object types, it would then be possible to transfer learned operators from one task to another.  The framework is demonstrated on block world (blocks are stacked on top of on another) and minecraft.     Review Summary: Initially, the submission received negative to borderline reviews with R4 being the most positive (score 6), and R1, R2, R3 being more negative (scores 4, 3, 5).  After discussion, R4 lowered their score to 4 and indicated that they felt the work was not ready for acceptance at ICLR.  Overall, there was limited discussion by the reviewers.  Reviewers (R2,R4) found the direction of the work promising and interesting.  After the author response, reviewers indicated that the revision and author response clarified some points, but believe that the work is not yet ready for acceptance, as 1) there is a significant amount of hand crafting required and 2) parts of the approach is still not clearly specified.    Clarity: As some reviewers note, the description of the framework is at a very high level, making it difficult to follow with missing details on specific details of how the object types are groups.  The specific aspect of the work that is novel is also not clearly stated, thus making it difficult to judge the originality and significance of the work.  After revision (the authors added brief paraphrase to the introduction to clarify the contribution, and additions to the appendices providing more details on how the difference steps work for the Minecraft scenario as well as failure cases), the manuscript is improved but the overall manuscript is still difficult to follow.  Pros:   Interesting and important problem (combining probabilistic/neural approaches with symbolic approaches) that is timely and deserves attention   The idea of clustering objects based on their effect is interesting (R4).     The framework proposed by the paper is interesting and potentially useful direction and can stimulate followup work   Cons:   The paper is difficult to follow with symbols/terms that are not clearly defined and missing details. (R1) The specific contributions of the work, wrt prior work, is also not clearly stated.   The novelty/contribution of the work on top of existing work (Konidaris et al 2018, Ugur & Piater 2015, etc) is not that clear (R2)   The experimental setup is weak with limited comparisons and no statistical results. Overall, reviewers felt the results are not convincing enough to support claims on transferability and learning efficiency.   Lack of baselines comparisons (R3).  In the rebuttal, the authors argue that there is no appropriate baselines.     The set of steps that is involved is fairly complex (R1), with many important details provided in the appendix   There are concerns about the generalization of the approach as many of the steps are handcrafted (R1, R4).  In the provided scenarios, many of the steps, including the set of provided options, and representation of objects, are manually designed.    Recommendation: The AC agrees with the reviewers that the paper is not ready for acceptance to ICLR.  It is the AC s opinion that the work addressing a very interesting problem and  would potentially be of interest to the community.  However, the exposition of the paper needs to be improved so that 1) the contribution of the work over prior work (Konidaris et al 2018, Ugur & Piater 2015, etc) is clearer 2) the assumptions and details of the proposed method is also clearer and easier to follow.  The authors are encouraged to improve their work and resubmit to an appropriate venue.  
This paper presents some insightful suggestions for researchers studying generalization in federated learning by separating two types of performance gaps between training and test performance, the participation gap (due to partial client participation) and the performance gap (due to data heterogeneity). They suggest that federated learning researchers use a three way split between participating clients  training data, participants clients  validation data, and non participating clients  data to measure the generalization performance of an FL model. The paper presents thorough experiments to support their conclusions. A common concern about the paper is that the authors  suggestions, although relevant and reasonable, are somewhat unsurprising and have been noted in different forms in other works in federated learning. Another concern is that the conclusions are purely based on experiments and are not supported by theoretical justification. Despite these concerns, the reviewers commended the overall insights presented in the paper.   There was a healthy post rebuttal discussion and some reviewers reevaluated the paper and raised their initial scores. Therefore, I recommend acceptance of the paper. I encourage the authors to take the reviewer s constructive suggestions into account when preparing the final version of the paper.
This paper proposes a hybrid RL algorithm that uses model based gradients from a differentiable simulator to accelerate learning of a model free policy.  While the method seems sound, the reviewers raised concerns about the experimental evaluation, particularly lack of comparisons to prior works, and that the experiments do not show a clear improvement over the base algorithms that do not make use of the differentiable dynamics. I recommend rejecting this paper, since it is not obvious from the results that the increased complexity of the method can be justified by a better performance, particularly since the method requires access to a simulator, which is not available for real world experiments where sample complexity matters more.
### Description  The paper demonstrates that efficient architectures such as transformers and MLP mixers, which do not utilize translational equivariance in the design, when regularized with SAM (sharpness aware minimization) can achieve same or better performance as convolutional networks, in the vision problems where the convolutional networks were traditionally superior (with data augmentation or not, regularized or not). The paper demonstrates it very thoroughly through many experiments and analysis of the loss surfaces.  ### Decision + Discussion  I find the paper to be very timely in its context. It has a remarkable coverage of experimental studies and different use cases: SAM + augmentation, +contrastive, +adversarial, +transfer learning; as well as ablation studies such as keeping first layers convolutional. The reviewers have asked further questions, and the authors were able to conduct respective experiments within the discussion period fully addressing all concerns and making the findings of the paper even more comprehensive and convincing.  After the rebuttal 3 reviewers were for "accept", one "marginally above" and one "marginally below". In the latter case the concern was that the paper is an experimental study of a known method, SAM. While I understand that many researchers are expecting theoretical and innovative results from ICLR papers, I find that it does not prevent acceptance. Indeed, the experimental findings in this paper are on a "hot" topic, could be of wide interest and could lead to a change of paradigm in designing models towards more generic ones. On the other hand, it could just indicate that CNNs are not fully exploiting their potential, e.g. not exploiting the context well enough in the hidden layers?  To get more insight, I am still wondering, how the predictions behave if the input is shifted by a few pixels in CNN and Transformers? It seems counterintuitive that making the first layers in ViT just an MLP of image patches is a good design. Furthermore, fully convolutional models allow to take input of an arbitrary size and average the predictions on the output if it happened to be larger than 1x1.   Since convolutions are also used for e.g. semantic segmentation and generative models, one should not (and the authors do not in the paper) discard them too fast. See also a recent work combining transformers and convolutional networks, Chen et al. (ICCV 2021) Visformer: The Vision friendly Transformer.
This paper is about representation learning for calcium imaging and thus a bit different in scope that most ICLR submissions. But the paper is well executed with good choices for the various parts of the model making it relevant for other similar domains.
This paper received two weak and one strong reject from the reviewers.  The major issues cited were 1) a lack of strong enough baselines or empirical results, 2) Novelty with respect to "Certified adversarial robustness via randomized smoothing" and 3) a limitation to Gaussian noise perturbations.  Unfortunately, as a result the reviewers agreed that this work was not ready for acceptance.  Adding stronger empirical results and a careful treatment of related work would make this a much stronger paper for a future submission.
This paper aims at improving the adoption of Bayesian NNs by providing a practical and user friendly variational inference method. The main ideas consist of two parts: 1. Warm start the variational inference from a pre trained deterministic NN. It takes advantage of existing deep learning library features for easy implementation including weight decay, batch matrix multiplication, etc. 2. Calibrating uncertainty estimation for out of domain detection using adversarial examples.  Pros: 1. A practical way of implementing DNN variational inference with reduced variance, without sacrificing classification accuracy of the pretrained NN model. 2. Significantly better OOD detection accuracy compared to other BNN approaches without taking OOD into account explicitly.  Cons: 1. During discussion, it becomes clear that most of the techniques have been proposed similarly in the literature. Krishnan, 2020 applied BNN starting from MAP of NN, Flipout (Wen et. al., 2018) applies instance wise sampling, Hendrycks et. al., 2018 and Hafner et. al., 2018 improves detection accuracy by training on OOD examples. The novelty of the proposed method is therefore limited. 2. There s not much benefit on the classification performance compared to the initial MAP and is inferior to MCMC based SOTA BNNs. One of the reviewers considers the SGLD type approach may be more appealing to ML practitioners with the overhead of VI in training additional variance parameters. 3. The authors argue MCMC based BNN methods cannot achieve good performance without temperature scaling. But the main performance improvement of the paper is in the OOD detection with uncertainty regularization that modifies the posterior as well. The method of training with OOD samples is orthogonal to applying Bayesian inference to NNs, and the detection performance is limited to the distribution close to examples during training.  This paper falls on the borderline for acceptance. With the goal of improving adoption of BNN in practice, it is not convincing yet making mean field VI easier to implement could realize it without achieving competitive performance. 
Scores ultimately point to accept. The one negative review is borderline and doesn t raise any red flags. Weaknesses in other reviews mainly point to minor improvements in the paper and are largely supportive. Rebuttal points are uncontroversial and seem to clarify several issues.
This is a well written paper proposing a promising a series of zero cost proxies for NAS. Overall, the reviewers were convinced that the approach is sound and the results overall support the use of zero cost proxies (although they are a bit weak in some cases, e.g. rank correlations in A.3). Despite some concerns amongst the reviewers around the technical novelty of the method, mostly due to the use of estimators from the pruning at init literature, this is promising work at the intersection of different sub communities in ML.
The paper proposes to study what information is encoded in different layers of StyleGAN.  The authors do so by training classifiers for different layers of latent codes and investigating whether changing the latent code changes the generated output in the expected fashion.  The paper received borderline reviews with two weak accepts and one weak reject.  Initially, the reviewers were more negative (with one reject, one weak reject, and one weak accept).  After the rebuttal, the authors addressed most of the reviewer questions/concerns.    Overall, the reviewers thought the results were interesting and appreciated the care the authors took in their investigations.  The main concern of the reviewers is that the analysis is limited to only StyleGAN.  It would be more interesting and informative if the authors applied their methodology to different GANs.  Then they can analyze whether the methodology and findings holds for other types of GANs as well. R1 notes that given the wide interest in StyleGAN like models, the work maybe of interest to the community despite the limited investigation.  The reviewers also point out the writing can be improved to be more precise.  The AC agrees that the paper is mostly well written and well presented.  However, there are limitations in what is achieved in the paper and it would be of limited interest to the community.  The AC recommends that the authors consider improving their work, potentially broadening their investigation to other GAN architectures, and resubmit to an appropriate venue.
The paper is on a new approach approach to transductive learning. Reviewers were a bit on the fence. Their most important objection is that the performance improvements that the authors report almost entirely come from the "online" version, which basically gets to see the test distribution.  That contribution is nevertheless, in itself, potentially interesting, but I was surprised not to see comparison with simple transductive learning from semi supervised learning, learning with cache, or domain adaptation, e.g., using knowledge of the target distribution to reweigh the training sample, or [0], on using an adversary to select a distribution consistent with sample statistics. I encourage the authors to add more baselines, analyze differences with existing approaches, and, if their approach is superior to existing approaches, resubmit elsewhere.   [0] http://papers.nips.cc/paper/5458 robust classification under sample selection bias.pdf
The authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. The paper is poorly written for an ICLR audience and the added value compared to extensive prior work in the area is not clearly demonstrated.
The paper studies an important problem of quantifying uncertainty (as measure by calibration) of predictions made by an ML algorithm in the presence of distribution drift. However, all reviewers point out a slew of concerns that went un rebutted by the authors. The reviewers concurred that the paper deserved to be rejected at the current stage, and I concur. I recommend that the authors take the critical and constructive feedback into account to improve the paper and perhaps resubmit to a different venue in 2022.
The paper proposes a classification method that improves model calibration using variational information bottlenecks and a noise contrastive loss.  Unfortunately, the authors  were not able to participate in the discussion of the paper with the reviewers. Given this, the reviewers raised several unaddressed concerns: First, it was argued that the different components of the proposed method required additional justification, in particular with regards to novelty. Second, reviewers argued that the paper required additional empirical validation, for example by testing if it works well with different convolutional methods such as ResNets.  Given these concerns, a consensus was reached that this paper should be rejected which is also my recommendation,
The authors propose to detect anomaly based on its representation quality in the latent space of the GAN trained on valid samples.  Reviewers agree that:   The proposed solution lacks novelty and similar approaches have been tried before.   The baselines presented in the paper are primitive and hence do not demonstrate the clear benefits over traditional approaches. 
The authors observe that batch normalization using the statistics computed from a *test* batch significantly improves out of distribution detection with generative models.  Essentially, normalizing an OOD test batch using the test batch statistics decreases the likelihood of that batch and thus improves detection of OOD examples.  The reviewers seemed concerned with this setting and they felt that it gives a significant advantage over existing methods since they typically deal with single test example.  The reviewers thus wanted empirical comparisons to methods designed for this setting, i.e. traditional statistical tests for comparing distributions.  Despite some positive discussion, this paper unfortunately falls below the bar for acceptance.  The authors added significant experiments and hopefully adding these and additional analysis providing some insight into how the batchnorm is helping would make for a stronger submission to a future conference.
the idea is interesting, but as pointed out the reviewers (and also agreed by the authors), the current manuscript lacks clear motivations, reasons underlying specific design choices and convincing empirical evaluation. 
This paper studies the problem of using oracle information that s only available during training in RL. The key contributions are 1) a variational Bayesian approach that models the oracle observation as latent variables; and 2) a Mahjong environment for benchmarking RL with oracle guiding. The novelty of the proposed approach is limited, but reviewers find the problem intriguing and agreed that it s a reasonable application of Bayesian approach to RL with latent oracle information. In addition, the Mahjong environment could benefit the community and spur new work in this direction. Therefore, I recommend this paper to be accepted as a poster.
The paper uses graph based neural networks to ensure constraint based simulation.  Even though the approach is a good one, it is only incremental w.r.t. the work published by Yang et al at NeurIPS in 2020; then, the experimental section is not convincing enough.  While the authors indicate their dissatisfaction with one of the reviewers  assessment, the overall reviews of the paper are not very positive.
This paper has been independently assessed by four expert reviewers. Two of them recommend acceptance (one straight, one marginal), and two rejection (both marginal). Among the main limitations of the presented work, found by the reviewers, was the limited reproducibility of the results due to the use of private data. The authors attempted to defend their experimental design choice to use only one private dataset by stating difficulty in obtaining public data that would be suitable for their method. I am personally in a strong disagreement with that statement, and with the sub statement of the authors that some publicly available ICU data may not be suitable. That either signals limited practical utility of the presented approach to non ICU settings only, or is simply incorrect. The presented approach nonetheless has an intriguing potential and I would be inclined to recommends its acceptance. Alas, I find the lack of reproducibility to be a significant drawback of the way this work is currently presented and this limitation could not be easily resolved. Therefore I am leaning towards recommending a rejection.
This paper proposes a simple modification of the Adam optimizer, introducing a hyper parameter  p  (with value in the range [0,1/2]) parameterizing the parameter update: theta_new   theta_old + m/v^p where p 1/2 falls back to the standard Adam/Amsgrad optimizer, and p 0 falls back to a variant of SGD with momentum.   The authors motivate the method by pointing out that:    Through the value of  p , one can interpolate between SGD with momentum and Adam/Amsgrad. By choosing a value of  p  smaller than 0.5, one can therefore use perform optimization that is  partially adaptive .    The method shows good empirical performance.   The paper contains an inaccuracy, which we hope will be solved before the final version. The authors argue that the 1/sqrt(v) term in Adam results in a lower learning rate, and the authors argue that the effective learning rate "easily explodes" (section 3) because of this term, and that a "more aggressive" learning rate is more appropriate. This last point is false; the value of 1/sqrt(v) can be smaller or larger than 1 depending on the value of  v , and that a decrease in value of  p  can result in either an increase or decrease in effective learning rate, depending on the value of v. The value of  v  is a function of the scale of loss function, which can really be arbitrary. (In case of very high dimensional predictions, for example, the scale of the loss function is often proportional with the dimensionality of variable to be modeled, which can be arbitrarily large, e.g. in image or video modeling the loss function tends to be of a much larger scale than with classification.)  The authors promise to include a comparison to AdamW [Loshchilov, 2017] that includes tuning of the weight decay parameter. The lack of this experiments makes it more difficult to make a conclusion regarding the performance relative to AdamW. However, the methods offer potentially orthogonal (and combinable) advantages.  [Loshchilov, 2017] https://arxiv.org/pdf/1711.05101.pdf 
This paper presents the neural stored program memory, which is a key value memory that is used to store weights for another neural network, analogous to having programs in computers. They provide an extensive set of experiments in various domains to show the benefit of the proposed method, including synthetic tasks and few shot learning experiments.  This is an interesting paper proposing a new idea. We discuss this submission extensively and based on our discussion I recommend accepting this submission.   A few final comments from reviewers for the authors:   Please try to make the paper a bit more self contained so that it is more useful to a general audience. This can be done by either making more space in the main text (e.g., reducing the size of Figure 1, reducing space between sections, table captions and text, etc.) or adding more details in the Appendix. Importantly, your formatting is a bit off. Please use the correct style file, it will give you more space. All reviewers agree that the paper are missing some important details that would improve the paper.   Please cite the original fast weight paper by Malsburg (1981).   Regarding fast weights using outer products, this was actually first done in the 1993 paper instead of the 2016 and 2017 papers.
This paper provides a unified view of some known methods for monotone operator inclusion problems like Forward Backward Forward (FBF) and OGDA, and provides new convergence results for the stochastic version of a variant of FBF called FBFp. All reviewers initially recommended rejection. The rebuttal and the manuscript update addressed several concerns from the reviewers, though the general consensus after rebuttal was still that the paper lacked in significance for the ICLR community. The AC thinks that the paper could make an interesting overview paper in a more optimization / theoretically minded venue.
This paper proposes a method to improve the training of sparse network by ensuring the gradient is preserved at initialization. The reviewers found that the approach was well motivated and well explained. The experimental evaluation considers challenging benchmarks such as Imagenet and includes strong baselines. 
The paper present a new learning based approach` to solve the Maximum Common Subgraph problem. All the reviewers find the idea of using GCN and RL to guide the branch and bound interesting although, even after reading the rebuttal, there are some important concerns about the paper.  The main issue raised by many reviewers are on scalability of the methods and motivation of the problem. It would be nice to add a scalability experiments on large networks(>1M nodes) to show that the method could potentially scale. In fact, the original motivation based on drug discovery, chemoinformatics etc. application is a bit weak because in those area domain specific heuristic should work better.  Overall, the paper is interesting but it does not meet the high publication bar of ICLR.
This paper proposes Hindsight Foresight Relabeling (HFR), an approach for reward relabeling for meta RL. The main contribution is a measure of how useful a given trajectory is for the purpose of meta task identification as well as the derivation of a task relabeling distribution based on this measure.  Reviewers agreed that the paper tackles an interesting problem and found the main insight to be simple and intuitive. While the initial reviews raised some concerns regarding novelty, the performance gap, and using the learned Q function to estimate post adaptation returns the rebuttal did a good job of addressing these concerns. Overall, the paper proposes a non trivial extension of hindsight relabeling to meta RL and while the results could be stronger I think the paper provides useful ideas and insights so I recommend acceptance as a poster.
This paper presents a method which selects feasible data augmentations suitable for contrastive time series representation learning. The topic in this paper is timely and interesting. One of 4 reviewers did not complete the review, not responding to a few reminders. So, one emergency reviewer, who is an expert in meta learning was added. While there is one review that strongly supports this work, two reviews remained unsupportive after the discussion period ended. I appreciate the authors for making efforts in responding to reviewers’ comments. However, after the discussion period, most of reviewers had concerns in this work, pointing out that the technical correctness needs further justification and experiments should be improved.  While the idea is interesting, the paper is not ready for the publication at the current stage. I encourage to resubmit the paper after addressing these concerns.
The paper introduces an approach to counterfactual fairness based on data pre processing, and compare it to other two counterfactual fairness approaches on the Adult and COMPAS datasets.  The reviewers are in agreement that, in its current state, the paper should not be accepted for publication at the venue. Their main concerns are around the metric used to measure fairness, and these were not resolved during the discussion. The reviewers would have also appreciated more experiments on real world datasets to get a more comprehensive comparison of the methods. Finally,  discussion and comparison with other methods to achieve counterfactual fairness from the literature were limited. 
This paper proposes a new method for speeding up convolutional neural networks. It uses the idea of early terminating the computation of convolutional layers. It saves FLOPs, but the reviewers raised a critical concern that it doesn t save wall clock time. The time overhead is about 4 or 5 times of the original model. There is not any reduced execution time but much longer. The authors agreed that "the overhead on the inference time is certainly an issue of our method". The work is not mature and practical. recommend for rejection. 
This paper proposes to discover causal mechanisms through meta learning, and suggests an approach for doing so. The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data. The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting. However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse. Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers  other concerns about the clarity, references, and experiments. Hence, it makes a worthwhile contribution to ICLR.
The reviewers all agreed that the paper represent thorough work but also is closely related to existing literature. (All referees point to other non overlapping literature so it is a crowded field the authors have entered.) The amount of novelty (needed) can always be discussed but given the referees unanimous opinion and knowledgable input it is better for this work to be rejected for this conference. Using this input can make this work a good paper for submission elsewhere. 
The reviewers appreciate the spatio temporal formulation of amortised iterative inference. However, the paper does not clearly state what is the end goal: if the end goal is video object segmentation, it should compared against other unsupervised object segmentation methods. If the goal is representation learning, it should evaluate the merit of the recovered representations, e.g. by fine tuning them on some downstream task. 
This work is well written and accurately covers the context and recent related work. It s a good example of how to apply self supervised training to the event sequence domain. However, the combination of a lack of technical originality (composing a set of previously explored ideas) and significant improvements in results (results with CoLES overlap in error bars with RTD results) limits the impact of this paper.  Pros:   Well written.   Extensive evaluation.   Well formulated problem.  Cons:   Lack of technical novelty. The method appears to be general to all sequences rather than specialized for event sequences so the motivation for this design is not crystal clear.   Minor improvement in results from using the method despite written claims that the method  significantly outperforms .   Limited analysis that shows the periodicity and repeatability in the data.
The paper studies the behavior of the intermediate ReLU like activations of trained neural networks and show empirically that the intermediate activation can be used as a hashing function for the examples with some key advantages, including almost no collisions and that there are desirable geometric properties (i.e. can use k means, k nn, logistic regression on these embeddings).  Pros:   The experimental analysis was solid and thorough investigating the effects of model size, training time, training set, regularization and label noise.   Cons:   An overall lack of novelty. It is already quite well known throughout the ML community that in many cases, using the intermediate embeddings serve as useful features to apply more classical methods such as kNN and clustering.  Overall, the reviewers appreciated the solid and thorough investigation into the hashing properties of neural network activation patterns, which convincingly confirms some intuitions about the behavior of activation patterns in neural networks. However, the reviewers also agreed that there was no significant new finding. There have already been many studies on clustering and kNN on the embeddings of a network. Thus, the core novelty of the paper appears to be the finding that almost every linear region has at most one datapoint after training (which does not seem too surprising given Hanin & Rolnick (ICML 2019)); however, without further novel implications of this finding, the impact of the paper is limited.
In this paper, the authors use a GP classifier to detect if the output of a NN classifier has been decided correctly. The GP takes as input the original input vector x and the output of the NN, i.e. the calibrated posterior probabilities given by the NN. It uses that as an input vector for the GP classifier to decide if the sample was correctly decided. The output of the GP will serve as confidence in the output of the NN. The results are comparable/superior with the state of the art and the authors have repeated the experiments with over 125 different datasets. The reviewers of this paper were all cautiously positive about the paper, but all of them pointed towards the reduced novelty of the paper. Also, none of the reviewers were willing to champion this paper as a must have at ICLR 2021.   For my reading of the paper, I would tend to agree with the reviewers’ comments. Also, I find that using the same NN, rather shallow, with the same configuration for all the datasets seems rather limited. Given that this method is independent of the underlying classifier and that the databases used are low dimensions and a low number of training examples, I would have liked to see what a random forest or a GP can accomplish. Also, I would have used bigger NNs that can be trained to overfit the sigmoid outputs for classification of higher accuracy. I believe that having a diversity of underlying classifiers is more relevant than having 125 datasets. We need to find the best classifier or ensemble and then apply the different mechanisms for estimating if the output is the correct one. Otherwise, the proposed method might only be workable for this specific NN configuration. In the tables, it can be hinted that this might be happening, as about 80% of the cases MCP and RED are indistinguishable in the AUROC values.   Also, for all of these datasets a GP could be used as an underlying classifier, and given the premises of this paper, the authors could check how well calibrate a GP classifier is. Also, there has been considerable work on calibrating NNs when they are trained to overfit. Comparing with those methods should be straightforward, as they provide more information than just a confidence score. This is probably the most influential paper: https://arxiv.org/abs/1706.04599 (1000+ references), but there are some recent papers too.     Finally, if the goal is to use a GP to detect if the classification done by the NNs is accurate, using a GP might be an overkill, as the complexity of the GP, especially for large datasets might end up being larger than the underlying classifier. 
The paper addresses an interesting problem, is novel and works. While the paper improved through reviews + rebuttal, the reviewers still find the presentation lacking. 
While prior work has shown the potential of using uncertainty to tackle catastrophic forgetting (e.g. by appropriate updates to the posterior), this paper goes further and proposes a strategy to adapt the learning rate based on the uncertainty. This is a very reasonable idea since, in practice, learning rate control is one of the simplest and most understood techniques to fight catastrophic forgetting.  The overall approach ends up being a well motivated strategy for controlling the learning rate of the parameters according to a notion of their "importance". Of course now the question is if this work uses a good proxy for "importance" so further ablation studies would help, but the current results already show a clear benefit.  
Reviewers generally appreciate the theoretical contribution of the paper, namely Accelerated Gradient Descent on the sphere and hyperbolic space with the same convergence rate as the Euclidean counterpart. However, there are several major concerns with the current work. From a theoretical standpoint, the geodesic map, which plays a crucial role in the algorithm and theoretical analysis, exists if and only if the manifold has constant sectional curvature (sphere and hyperbolic space). It is not at all clear how the current approach can be extended beyond this setting.  From an algorithmic viewpoint, the stated algorithm has not been experimentally validated. It is suggested that at least some synthetic experiments, e.g. on the sphere or Poincare disk, be carried out. Finally, the current presentation is quite dense and should be considerably improved.
The paper has some nice ideas for efficient exploration, but reviewers think more work is needed before it is ready for publication.  In particular, the paper should have an improved discussion of state of the art work on exploration, compare the difference and benefits of the proposed approach, and then conduct proper experiments to validate the claims.
This paper provides a new theoretical framework for domain adaptation by exploring the compression and adaptability.  Reviewers and AC generally agree that this paper discusses about an important problem and provides new insight, but it is not a thorough theoretical work. The reviewers identified several key limitations of the theory such as unrealistic condition and approximation. Some important points still require more work to make the framework practical for algorithm design and computation. The presentation could also be improved.  Hence I recommend rejection.
The paper based on cGAN developed a data augmentation GAN to deal with unseen classes of data. The paper developed new modifications to each component and designed network structure using ideas from state of the art nets. As pointed out by reviewer 1 & 2, the technical contribution is not sufficient. We hence recommend it to workshop publication.
This paper studies off policy learning of contextual bandits with neural network generalization. The proposed algorithm NeuraLCB acts based on pessimistic estimates of the rewards obtained through lower confidence bounds. NeuraLCB is both analyzed and empirically evaluated.  This paper received four borderline reviews, which improved during the rebuttal phase. The main strengths of this paper are that it is well executed and that the result is timely, considering the recent advances in pessimism for offline RL. The weakness is that the result is not very technically novel, essentially a direct combination of pessimism with neural networks. This paper was discussed and all reviewers agreed that the strengths of this paper outweigh its weaknesses. I agree and recommended this paper to be accepted.
This paper studies properties that emerge in an RNN trained to report head direction, showing that several properties in natural neural circuits performing that function are detected.  All reviewers agree that this is quite an interesting paper. While there are some reservations as to the value of letting a property of interest emerge as opposed to simply hand coding it in, this approach is seen as powerful and valuable by many people, in that it suggests a higher plausibility that the emerging properties are actually useful when optimizing for that function   a claim which hand coding would not make possible. Reviewers have also provided valuable suggestions and requests for clarifications, and authors have responded by improving the presentation and providing more insights. Overall, this is a solid contribution that will be of interest to the part of the ICLR audience that is interested in biological systems.
In this paper the authors show how to allow deep neural network training on logged contextual bandit feedback. The newly introduced framework comprises a new kind of output layer and an associated training procedure. This is a solid piece of work and a significant contribution to the literature, opening up the way for applications of deep neural networks when losses based on manual feedback and labels is not possible. 
This paper proposes to deal with task heterogeneity in meta learning by extracting cross task relations and constructing a meta knowledge graph, which can then quickly adapt to new tasks. The authors present a comprehensive set of experiments, which show consistent performance gains over baseline methods, on a 2D regression task and a series of few shot classification tasks. They further conducted some ablation studies and additional analyses/visualization to aid interpretation.  Two of the reviewers were very positive, indicating that they found the paper well written, motivated, novel, and thorough, assessments that I also share. The authors were very responsive to reviewer comments and implemented all actionable revisions, as far as I can see. The paper looks to be in great shape. I’m therefore recommending acceptance.  
Main content: BasiGAN, a novel method for  introducing stochasticity in conditional GANs Summary of discussion: reviewer1: interesting work and results on GANs. Reviewer had a question on pre defned basis but i think it was answered by the authors.  reviewer3: interesting and novel work on GANS, wel written paper and improves on SOTA. The main uestion is around bases again like reviewer 1, but it seems the authors have addressed this. reviewer4: Novel interesting work. Main comments are around making Theorem 1 more theoretically correct, which it sounds like the authors addressed. Recommendation: Poster. Well written and novel paper and authors addressed a lot of concerns. 
Overall the reviewers had various positive things to say about the paper, including that it was well written and easy to understand, topical, that the method was sensible, novel and interesting and that the computational efficiency (i.e. real time) was appealing.  However, all the reviewers thought it wasn t quite ready for acceptance, mainly citing concerns with the empirical evaluation.  It seems they had trouble interpreting the empirical results and placing the work with respect to other relevant methods.    It seems in the author response, the authors did much to add to the experiments, but ultimately the reviewers were not comfortable with acceptance.  Taking the reviewers  feedback into account and adding the desired empirical evaluation would make this a much stronger submission to a subsequent conference.
This paper describes situations whereby data augmentation (particularly drawn from a true distribution) can lead to increased generalization error even when the model being optimized is appropriately formulated. The authors propose "X regularization" which requires that models trained on standard and augmented data produce similar predictions on unlabeled data. The paper includes a few experiments on a toy staircase regression problem as well as some ResNet experiments on CIFAR 10.  This paper received 2 recommendations for rejection, and one weak accept recommendation.  After the rebuttal phase, the author who recommended weak acceptance indicated their willingness to let the paper be rejected in light of the other reviews.  The reviewer highlighted: "I think the authors could still to better to relate their theory to practice, and expand on the discussion/presentation of X regularization."  The main open issue is that the theoretical contributions of the paper are not sufficiently linked to the proposed algorithm.
The paper is somewhat borderline, though reviews mostly lean positive. Unfortunately after calibrating compared to other submissions, the work remains somewhat below the bar compared to higher scoring papers.  The reviewers praise the topic, the method, and the experiments (although some of this praise is a little mixed or lukewarm). The most negative review raises several specific concerns about the evaluation methodology, as well as some concerns about data leaks etc. While serious, the authors rebuttal to these claims seems reasonably convincing. While the remaining issues appear not to be dealbreakers, there are nevertheless some lingering concerns which ultimately put the paper slightly below the bar.  The AC notes that their initial inclination was to accept this paper, though it was suggested that the score be lowered after calibration compared to other submissions, mainly due to doubt regarding these lingering issues.
This is one of several recent parallel papers that pointed out issues with neural architecture search (NAS). It shows that several NAS algorithms do not perform better than random search and finds that their weight sharing mechanism leads to low correlations of the search performance and final evaluation performance. Code is available to ensure reproducibility of the work.  After the discussion period, all reviewers are mildly in favour of accepting the paper.   My recommendation is therefore to accept the paper. The paper s results may in part appear to be old news by now, but they were not when the paper first appeared on arXiv (in parallel to Li & Talwalkar, so similarities to that work should not be held against this paper).
The paper studies personalized federated learning, mixing a global model with locally trained models.  Reviewers agreed on the relevance of the problem and that the work contains valuable contributions, such as the generalization bounds. After discussion, unfortunately consensus remained that the paper remains narrowly below the bar in the current form.  Concerns remained on novelty over the Mapper optimization algorithm which also has adaptivity to the local/global combination of models, the dependence of the generalization bound on the mixing parameter as it converges to the global model,  as well as on the strength of the experimental findings compared to well known FedAvg and related method in a realistic benchmark environment (such as e.g. Leaf), since the dataset choice (and even more its partition among clients) is a crucial aspect for measuring personalization in a fair way. We hope the feedback helps to strengthen the paper for a future occasion.
We appreciate the authors for addressing the comments raised by the reviewers during the discussion period, which includes providing more experimental results to address the concerns. We believe the publication of this paper can contribute to the important topic of data augmentation.  The authors are highly recommended to consider all the comments and suggestions made by the reviewers when further revising their paper for publication.
This paper considers a new setting of robustness, where multiple predictions are simultaneously made based on a single input. Different from existing robustness certificates which independently consider perturbation of each prediction, the authors propose collective robustness certificate that computes the number of predictions which are simultaneously guaranteed to remain stable under perturbation. This yields more optimistic results. Most reviewers think this is a very interesting work and the authors present an effective method to combine individual certificate. The experimental results are convincing. I recommend accept.
The paper sheds light on issues with BN in continual learning and proposes a quite simple, which is a strength, solution to fix it.   The Authors first draw attention to the fact that using recalculated moments boosts performance and reduces forgetting, which serves as an argument that at least partially BN contributes to catastrophic forgetting in continual learning. Given that BN remains quite important in certain application areas such as vision, it is a strong motivation for the paper.  The experiments are thorough and clearly show that CN is a practically relevant alternative to BN in continual learning.  One weakness of the paper is that the method is poorly motivated, and relatedly, it has quite limited novelty. CN combines the strengths and weaknesses of BN and GN. Hence, it is not clear why it outperforms both, given that it still has the issue of BN that normalization statistics might become outdated. This is one of the weaknesses pointed out by 9jXz who recommended rejecting the paper. It would be also nice to compare to Mode Normalization https://openreview.net/forum?id HyN M2Rctm.   Other papers have suggested changing normalization for sequential learning. Changing batch normalization (to batch renormalization) was investigated in [1] in the context of continual learning. Relatedly, [2] proposes TaskNorm for meta learning.  Despite these issues, it is a solid contribution and it is my pleasure to recommend acceptance. In the camera ready, please describe more clearly the design principles behind CN.  [1] Rehearsal Free Continual Learning over Small Non I.I.D. Batches, https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Lomonaco_Rehearsal Free_Continual_Learning_Over_Small_Non I.I.D._Batches_CVPRW_2020_paper.pdf  [2] TaskNorm: Rethinking Batch Normalization for Meta Learning, https://arxiv.org/abs/2003.03284
The paper proposes contrastive learning for tabular data to improve anomaly detection.  Strengths:   Interesting and important problem.   Usage of contrastive learning for anomaly detection in general multi variate datasets is novel (as prior work mostly focuses on images)   Extensive experiments with comparisons to multiple baselines on multiple datasets   Well written paper  The reviewers raised some concerns about novelty (in particular, the relationship to the closely related paper "Neural Transformation Learning for Deep Anomaly Detection Beyond Images"), hyperparameter tuning and additional baselines. The authors did a great job of addressing the concerns and multiple reviewers raised their scores. During the discussion phase, the consensus decision leaned towards accept. I recommend acceptance and encourage the authors to address any remaining concerns in the final version.  Additional AC comments:   Please make sure that the camera ready version does not exceed page limits. https://iclr.cc/Conferences/2022/CallForPapers    " As far as we can ascertain, masking was not used for one class classification before.": There s some related work on pre training BERT for OOD detection (cf. https://arxiv.org/abs/2004.06100 or https://arxiv.org/abs/2106.03004) which might be worth discussing.
The paper gives a learning augmented algorithm for estimating the support size of a discrete distribution. The proposed algorithm is evaluated experimentally, showing significant improvements in the estimation accuracy. The reviewers unanimously agreed that the contributions are strong and relevant. I recommend accept.
The reviewers found a number of short comings in this work that would prevent it from being accepted at ICLR in its current form, both in terms of writing (not specifying the loss function),  experiments that are too limited, and inconclusive comparisons with existing regularization techniques. I recommend the authors take into account the feedback from reviewers in any follow up submissions.
The paper offers a direction for mult agent RL that builds on results for actor critic methods [Zhang, ICML 2018], extending that work to address deterministic policies.  The authors establish convergence under a number of assumptions.   Both on policy setting and off policy settings are treated.  The reviewers point out several concerns and the consensus seems to be that, while the direction looks promising, the paper deserves further work. 
This paper introduces a closed form expression for the Stein’s unbiased estimator for the prediction error, and a boosting approach based on this, with empirical evaluation. While this paper is interesting, all reviewers seem to agree that more work is required before this paper can be published at ICLR.  
This paper presents a novel method for general purpose supervised domain transfer that trains both generator and discriminator to compete in a minimax game in order to reconstruct data. This setup is meant to address a common issue in conditional GAN setups: they often ignore conditioning information. Results are positive and span two very different tasks: image to image translation and silent video to speech reconstruction. Overall reviewers were quite positive about this paper: they found the method to be novel and well motivated, and after rebuttal, found experimental results to be sufficiently convincing. Several concerns were brought up: (a) lack of emphasis that the approach is in fact supervised, (b) need for comparisons with stronger or task specific baselines, (c) lack of description of experimental details for reproducibility, and (d) lack of discussion of ethical implications. All of these concerns were satisfactorily addressed by authors in rebuttal and reviewers unanimously vote for acceptance. I agree, and recommend this paper be accepted. 
This paper investigates fast adversarial training methods as a bilevel optimization problem. The proposed algorithm compares well with the existing techniques in overall runtime (obtaining better clean test accuracy, which is not the goal, and) matching the robust accuracy of existing adversarial training methods. The proposed framework, however, is more general and flexible and is theoretically grounded. The problem studied here is exciting and the approach the authors take is interesting.   The current version, unfortunately, has some serious shortcomings. The empirical comparisons are a bit lacking — in general, the wall clock time is not a very good measure, it depends heavily on the implementation and various optimizations therein. A more suitable comparison would be in terms of floating point operations, or in terms of iteration complexity.   The paper reports other interesting findings such as how the proposed method avoids robust overfitting. However, there is little theoretical evidence or insight for how the proposed method avoids it.   The writing can be improved with more emphasis on the novelty and significance of the contributions — some of the statements regarding improvements over prior work are somewhat misleading given the incremental gains (e.g., see Table 1). I believe the comments from the reviewers have already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission.
This paper experimentally shows that the commonly used standard solvers such as Nesterov momentum or Adam can achieve the same performance as optimizers such as LARS and LAMB specially proposed for large batches training.  Large batch training is a very important topic, and if the author s argument is true, it might be an interesting discovery.  However, all reviewers were concerned about its limited technical contribution. Not only that, but above all, when tuning the optimizer using the same computational resource (for a new task), it seems unclear whether the standard optimizer can achieve as much performance as large batch optimizers (currently they tune the standard optimizers, fixing the hyperparameter for large batch solvers to match their performances). The authors did not answer the reviewer s questions about it, and they did not answer the reviewer s other questions in great detail. Through discussion among reviewers, all reviews agreed on this concern and agreed to reject this paper.  The quality of the paper will be greatly improved if this concern is resolved.
Meta score: 6  The paper approaches the problem of identifying out of distribution data by modifying the objective function to include a generative term.  Experiments on a number of image datasets.  Pros:    clearly expressed idea, well supported by experimentation    good experimental results    well written  Cons:    slightly limited novelty    could be improved by linking to work on semi supervised learning approaches using GANs  The authors note that ICLR submission 267 (https://openreview.net/forum?id H1VGkIxRZ) covers similar ground to theirs.
This paper proposes new target objectives for training random forests for better cross domain generalizability.   As reviewers mentioned, I think the idea of using random forests for domain adaptation is novel and interesting, while the proposed method has potential especially in the noisy settings. However, I think the paper can be much improved and is not ready to publish due to the following reviewers  comments:    This paper is not well written and has too many unclear parts in the experiments and method section. The results are not guaranteed to be reproducible given the content of the paper. Also, the organization of the paper could be improved.    The open set domain adaptation setting requires more elaboration. More carefully designed experiments should be presented.     It remains unclear how the feature extractors can be trained or fine tuned in the DNN + tree architecture. Applying trees to high dimensional features sacrifices the interpretability of the tree models, hampering the practical value of the approach.  Hence, I recommend rejection.
This work attempts to incorporate affect information from additional resources into word embeddings. This is a valuable goal, but the methods used are very similar to existing ones, and the experimental results are not quite convincing enough to make a strong enough case for accepting the paper.
The paper addresses the difficult problem of combining ILP in a meta interpretive framework with noisy inputs from a neural system.   The essential idea is to use MIL to "efficiently" search for constraints on the neural outputs (eg z1 + z2 + z3   7, or z2< z3) as well as logic programs, with a score related to program complexity as well as probability of the best constraint satisfying neural outputs.  It is interesting work for the right audience but it s clear from the reviews that the presentation was difficult for ICLR readers, even ones with appropriate background.   Some potential weaknesses of the approach include:  1   it s unclear how scalable the MIL framework is   presumably the intrinsic difficultly of the search means that programs and constraint sets must be small  2   it s unclear how general the approach is beyond the digits as separate inputs setting of the two experimental studies, and its unclear how accurate the perceptual layer needs to be   MNIST obviously being an example of a case where there is little noise with a modern classifier.  3   it s unclear how constraints can in general be used to backprop any information to the underlying neural system, and without this the joint training seems to be quite limited.  Overall the paper is judged as inappropriate for ICLR.
The paper presents a family of models for relational reasoning over structured representations. The experiments show good results in learning efficiency and generalization, in Box World (grid world) and StarCraft 2 mini games, trained through reinforcement (IMPALA/off policy A2C).  The final version would benefit from more qualitative and/or quantitative details in the experimental section, as noted by all reviewers.   The reviewers all agreed that this is worthy of publication at ICLR 2019. E.g. "The paper clearly demonstrates the utility of relational inductive biases in reinforcement learning." (R3)
The paper proposes few shot robust (FROB) model for classification and few shot OOD detection. While the paper has some interesting contributions, all the reviewers felt that the current version falls short of the ICLR acceptance threshold and the consensus decision was to reject. I encourage the authors to revise the paper based on the reviewers  feedback and resubmit to a different venue.  As Reviewer r838 pointed out, that this paper uses TinyImages dataset which has been since retracted. I appreciate that prior work used TinyImages, but please see "Why it is important to withdraw the dataset" https://groups.csail.mit.edu/vision/TinyImages/ and consider not using the TinyImages dataset for future revisions.
This review paper presents a way of comparative assessment of continual learning. Reviewers all agreed that this work is interesting, unique with comprehensive coverage of the CL space. The proposed categorization, CLEVA Compass, and its GUI have great potential to facilitate future CL work.
The authors address the problem of fine grained image classification. They propose a batch based regularizer, called the batch confusion norm (BCN), to encourage less over confident predictions. They also tackle the problem of class imbalance during training by adaptively weighting the BCN loss at the class level to take the imbalances in the underlying label distributions into account. Results are presented on four different fine grained datasets.    Overall, while the reviewers had some positive comments, there was not broad support for the paper. There are questions that need to be resolved related to the evaluation e.g. the best performing model uses GASPP, however there is no reported GASPP variant for the PC baseline. Similarly, it would be valuable to know how much PC would benefit from an additional class imbalance term in the iNaturalist2018 results. Given that the proposed regularizer builds on PC (Dubey et al.), it is very important that the authors provide a like for like comparison so that readers can better understand the merits of the proposed method.    There were also concerns with the presentation of the paper e.g. several typos (which can be easily fixed), issues with the clarity of the text (which require more work), and uninformative figures (e.g. Fig 2 should be revised to more clearly illustrate the differences between the three methods shown). The authors are encouraged to revise the text to resolve these problems.   While the paper has some strengths (e.g. the empirical performance on some of the tasks is promising and the method is conceptually simple), there are still a number of concerns from the reviewers e.g. a lack of a clear motivation as to why the proposed method works, and why it is conceptually better than existing alternatives (e.g. PC). Given this lack of support, it is not possible to recommend the paper in its current form.  
This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization. Conditioned on reviewers  judgements, this is a good submission but hasn t reached the bar of ICLR.
This paper propose a way to make minibatch Optimal transport (m OT) more efficient by computing an optimal assignment (in the OT sens) and us this assignment to compute instead a hierarchical OT loss (bomb OT ) that can be used instead of the m OT loss. The authors discuss how the equivalent OT plan with bomb OT is much more sparse, and how the proposed approach is actually not biased when the number of mini batches $k\rightarrow \infty$ . Numerical experiments  show that the proposed method allows a gain in performances in applications such as generative modeling, domain adaptation, color transfer and approximate Bayesian computation.  The paper originally got borderline negative scores from the reviewers. While the reviewers acknowledged that the idea is interesting, they had some concerns about the theoretical results strength, some missing baselines and discussions in the numerical experiments. The authors did a detailed reply that clarified some problems. the new numerical experiments with m UOT were also greatly appreciated by the reviewers but they also raised some questions about the paper. Some concerns detailed below about the comparison with m OT appeared during the reviewers discussion. Despite the new information,  the reviewers reached an agreement that this paper is interesting but needs more work and another round of reviews before acceptance. For theses reasons  the AC recommends a rejection for this paper.  More details and suggestions below:    While it is clearly not the objective of the paper a discussion about the proximity of the average plan to the exact OT plan is interested. Also a short numerical experiments showing that the bomb OT average plan is closer to the exact plan than m OT would be a good illustration of the better performance of bomb OT. This seems more important for the paper than the color transfer experiments that is kind of a toy problem.    After checking the definition in the paper and discussion between reviewers it appeared that the comparison with m OT is a bit unfair due to the reformulation of the problem in (1). indeed in the usual formulation, k pairs of independent  minibatches are used and the OT is done on those pairs (a sum of k OT) not on all the possible pairwise permutation as in definition of m OT in equation (1). In other  words in m OT the batches are supposed to be independent which is not the case  in the proposed formulation (it is equivalent in the population case though).  It means that in practical application, for the same computational complexity  (k^2 OT computed), m OT actually uses $k^2m$ independent samples on each distribution  whereas the bomb OD (and the m OT defined in equation (1) ) use $km$ samples . By implementing m OT as  in (1) they actually prevent m OT to explore the dataset as its original  formulation does. This means that all the experiments should be done either with the original m OT implementation of both the original and (1) in addition to bomb OT. The proposed method will proably work better but the current experiment do not allow this fair comparison.     The theoretical result need more discussion and justification.  For instance  m OT converges to its population value in   $O(m^{1/2}n^{ 1/2}+k^{ 1/2})$ that is independent from the dimensionality  $d$, but the authors prove the  concentration of bomb OT in  of $O(m^{1/2}n^{ 1/d})$  which is  clearly a problem for large $d$. Also the dependence on  $k$ of the convergence would be important since  bomb OT is well defined is true only in the population case where $k$ is  large. Note that the claim that it is well defined and hence better is also a  bit dubious because it is well defined for $k \infty$, which is also the case for m OT when $m \infty$. Both $m$ and $k$ large will lead to not practical optimization problems so they are comparable except that m OT converged to the true OT plan when $m\rightarrow \infty$ which is not the case for bomb OT.    While the contribution of the paper in indeed a methodological method and does not require to be state of the art on all applications the numerical experiments should be improved. First as discussed above the comparison with m OT is actually unfair an do not correspond to what in done in practice (where all mini batches are independent). m OT should be implemented with  $k^2$ truly independent minibatches.    Second , the authors use approximate W2 on two of  the GAN dataset and FID on the third. This is  problem because approximate W2 is not defined in the paper. FID is the standard performance measure and should be used for all dataset.     Third the novel experiments comparing also raises a lot of questions. m UOT is far better than BoMb OT suggesting that Unbalanced OT can compensate for the limits of m OT far better than bomb OT itself. Yes there is a slight increase in performance  for ebomb UOT over m UOT but is is so small (0.08 %) that it is hard to find them significant, especially since we have no variance. This result that is provided only for DA application actually  suggest that the competitor of bomb OT is m UOT and not m OT so it should also be part of the comparison in the other experiments. The authors talk in their replay about the limits of m UOT but stating that the experiences are not done in the paper is not an excuse for evaluating this clear competitor on other problems and showing numerically these limits.    Finally in the current version of the paper puts a lot of things in the annex that make the paper clearly not self content. Some experiments could go in annex/supp for instance the color transfer to make place for more details in the main paper.   Note that it is not one of those comments above that lead the the reject decision but the sum of them that clearly show that the paper needs more work.
The paper proposes a new approach to knowledge distillation by searching for a family of student models instead of a specific model. The key idea is that given an optimal family of student models, any model sampled from this family is expected to perform well when trained using knowledge distillation. Overall this is an interesting idea and an important direction of research. However, the reviewers raised several concerns regarding novelty and experimental evaluation. There was a clear consensus among the reviewers that the paper is not yet ready for publication. The specific reasons for rejection include the following: (i) the proposed method is somewhat incremental, and the paper s contributions should be adjusted accordingly; (ii) the experimental results in the paper do not provide a clear/fair comparison with existing approaches, and additional baselines should be considered. The reviewers have provided detailed feedback in their reviews, and we hope that the authors can incorporate this feedback when preparing future revisions of the paper.
This submission proposes a method for steganography, i.e. hiding "secret messages" in images. Specifically, the proposed approach implements a procedure similar to adversarial example generation, where a perturbation is found that a) is imperceptible and b) can be decoded by a fixed decoder. This approach results in the ability to hide a significant amount of information (up to 3 bpp) with essentially no decoding errors. While the resulting perturbations are sometimes detectable by existing methods, the authors nevertheless provide a compelling case that their approach is a significant step forward and demonstrate an interesting new application to face hiding. Reviewers generally found the paper easy to follow and clear and felt the proposed approach provided a new and effective way to perform steganography. However, there were many requests for clarification of the work s situation in the larger literature of steganography and adversarial ML. The authors have clarified their contribution in the rebuttal, and as long as these changes have made it back into the paper, I am happy to recommend acceptance.
The paper proposes a method for saving computation in surveillance videos (videos without camera motion) by re using features from parts of the image that do not change. The results show that this significantly saves computation time, which is a big benefit, given also the amount of surveillance video input available for processing nowadays. Reviewers request comparisons to obvious baselines, e.g., selecting a subset of frames for processing or performing a low level pixel matching to select the pixels to compute new features on. Such experiments would make this paper much stronger. There is no rebuttal  and thus no ground for discussion or acceptance. 
The paper introduces a variant of AMSGrad ("Optimistic AMSGrad"), which integrates an estimate of the future gradient into the optimization problem. While the method is interesting, reviewers agree that novelty is on the low side. The motivation of the approach should also be clarified. The experimental section should be made stronger; in particular, reporting convincing wall clock running time advantages is critical for validating the viability of the proposed approach.   
This paper presents a Actor Critic Hedge (ACH) method for 1 on 1 Mahjong. It is is an actor critic method for approximating Nash equilibrium strategies in large extensive form games. ACH extends the CFR family of algorithms that uses deep learning and model free training (not using full game traversal). The propose ACH agent defeats several human players, including a Mahjong champion. This is impressive.  The reviewers and authors have extensive discussions and the authors managed to address most of the concerns from the reviewers. The overall opinions from the the reviewers favor acceptance. Below are some of the strength and weakness summarized from the reviewers:  Strength: * Extensive experiments and impressive performance.  * New policy based algorithm for competitive environments. * Reviewers  questions are well addressed.  Weakness: Lack of more tabular theoretical analysis. Need experiments to compare to existing methods. Theory and experiment does not match.
The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
The paper introduces Value Iteration with Negative Sampling (VINS) algorithm as a method to accelerate RL using expert demonstrations. VINS learns an initial value function that has a smaller value at states not encounter during the demonstrations.  The reviewers raised several issues regarding the assumptions, theoretical results, and experiments. The method seems to be most natural for robotic control problems. Nonetheless, it seems that the rebuttal addressed most of the concerns, and two of the reviewers increased their scores accordingly. Since we have three Weak Accepts, I believe this paper can be accepted at the conference.
The authors were responsive to the comments of the reviewers, both in the rebuttal and in the revision to the manuscript.  However, the reviewers were still concerned about the lack of clarity of the manuscript, the motivation for the design decisions, and errors, present also in the rebuttal revisions. 
This paper considers initialization methods for the k means algorithm.  There is a lot of prior work in this area. The reviewers were mildly positive on the paper.  There were several concerns on how the results were presented as well as the comparison to prior work. Importantly, no reviewer felt that there was a lot of novelty in the paper over the line of work on k means initialization.
While the authors thought that the paper had some strong experimental comparisons, there were serious concerns with novelty and paper claims. For a stronger ML paper the authors would need to either: (a) design a new training methodology beyond pre training that is better suited for leveraging multiple datasets for Retrosynthesis, (b) design a new model for Retrosynthesis that is better able to leverage mutliple datasets, (c) design new evaluation metrics to describe how well current methods perform in Retrosynthesis and/or metrics that describe how well methods can use data from different sources. That said, if the authors were interested to submit to non ML venues then I agree with R2 that chemistry venues may be better suited to the paper in its current form. 
This paper points out methods to obtain sparse convolutional operators. The reviewers have a consensus on rejection due to clarity and lack of support to the claims.
Several reviewers thought the results were not surprising in light of existing universality results, and thought the results were of limited relevance, given that the formalization is not quite in line with real world networks for MIL. The authors draw out some further justifications in the rebuttal. These should be reintegrated. I agree with the general criticisms regarding relevance to ICLR. Ultimately, this work may belong in a journal.
No discussion or answers to concerns are offered by the authors. Given this, the current consensus remains the same as the initial review status, and AC s meta review cannot provide any additional information.  This leads to rejection
The reviewers in general did not seem to be strongly impressed by the contribution of the paper. As the authors noted, some reviewers seemed to misinterpret the claims of the paper   the paper is not to design new MORL algorithms that are significantly better on standard MORL benchmarks but is to apply MORL on offline RL and fine tuning. On the other hand, the AC suspects that the paper s exposition could be more centered around the applications, e.g., arguing why offline RL can be benefited from better MO training, and why the challenge of offline RL is to balance some given notions of risk and return computationally (instead of, e.g., developing the right notion/formula for quantifying the risk and return.) Moreover, I think the paper would be stronger if the evaluation for offline RL setting can be made stronger, e.g., including more tasks and algorithms on the D4RL dataset. If the paper s claim is that MORL is a great tool for offline RL, perhaps it s useful to demonstrate that MORL can achieve SOTA reliably when used on top of existing offline RL algorithms (which almost always have two parts in the objective). In summary, in the AC s opinion, the paper has a valuable contribution to the community but is somewhat boardline for ICLR in the current form, and the AC encourages the authors to resubmit to a top venue conference after addressing some of the reviewers  comments.
In this study, the authors propose a new graph transformer network for dynamic graph representation. To solve the challenges of static graphs learning and the temporal information aggregating, this paper introduces a Dynamic Graph Transformer (DGT) which contains three components: (1) a two tower Transformer based method, (2) temporal union graph construction (3) a complementary pre training task. Extensive experiments on the two datasets of link prediction and node classification demonstrate the superiority of the model. The ablation studies justify the effectiveness of each module in the DGT model. The reviewers has various technical issues with the paper which the authors mostly addressed (e.g., whether the nodes are static or dynamic, whether DGT is robust to noise, whether it scales to larger datasets). Overall, the contributions seem incremental. There is confusion among the reviewers as to whether the proposed model differs from prior art. It seems to me there actual  differences  but whether they major or minor is open to interpretation. Overall there reviewers were not particularly excited about model/results/contributions.
This paper proposes techniques to improve training with batch normalization. The paper establishes the benefits of these techniques experimentally using ablation studies. The reviewers found the results to be promising and of interest to the community. However, this paper is borderline due in part due to the writing (notation issues) and because it does not discuss related work enough. We encourage the authors to properly address these issues before the camera ready.
The authors present BASGD and asynchronous version of SGD that attempts to be robust against byzantine failures/attacks.  The papers is overall well written and clearly presents the results. Some novelty is present as there have been limited work in asynchronous algorithms for byzantine ML.   However, there have been several concerns raised by the reviewers, on which I agree, and they have not been fully addressed: 1) the tradeoff between asynchrony and robustness, as BASGD cannot handle the case of a buffer being straggler, which limits some of the novelty in this work 2) issues with the definition of privacy leakage has not been fully addressed 3) some reviewers mentioned the theoretical results being of limited importance, but arguably this is true for other related work in this area. Perhaps a general criticism is valid as to what is the operational value of the proposed guarantees. That is convergence does not exclude a model that has undesirable properties, eg has bad prediction accuracy for a small subset of tasks. 4) Finally, the motivation of the system model of the paper ( eg storing gradients as opposed to instances) paper is of unclear practical relevance, as was raised by multiple reviewers.   Overall the consensus was that the paper does have merits, however, some of the most major concerns were not properly addressed. This paper can potentially be improved for a future venue.  
Summary:  The authors propose a Bayesian approach to data cleaning, implemented via a variational auto encoder. They argue that a common problem in this context are posteriors that overfit by concentrating on a low dimensional subset and introduce an optimization target intended to discourage that behavior.  Discussion:  Arguably the main concern brought up in the reviews was how much novelty there is in addressing latent variable posterior collapse, solutions for which have been proposed. The authors were able to clarify that this was due to a misunderstanding (the collapse they address is not in latent space), and the reviewer considers the matter resolved.   Recommendation:  I recommend publication. The reviewers are all positive, agree that the method is interesting, and seems novel. The writing is clear, and remaining doubts have been addressed in the discussion. 
This paper proposes information theoretic quantification of epistemic uncertainty in autoregressive models.   This is a difficult problem that receives much less attention than the unstructured case. The paper is well written, contributes novel and tractable to estimate measures which are analysed formally and empirically with convincing experiments on ASR and NMT.   The reviewers and myself are overall pleased by this submission. The discussion phase went well and most concerns have been resolved.  
This paper presents neural architecture search for semantic segmentation, with search space that integrates multi resolution branches. The method also uses a regularization to overcome the issue of learned networks collapsing to low latency but poor accuracy models. Another interesting contribution is a collaborative search procedure to simultaneously search for student and teacher networks in a single run. All reviewers agree that the proposed method is well motivated and shows promising empirical results. Author response satisfactorily addressed most of the points raised by the reviewers. I recommend acceptance. 
This paper provides a new algorithm for learning fair representation for two different fairness criteria accuracy parity and equalized odds. The reviewers agree that the paper provides novel techniques, although the experiments may appear to be a bit weak. Overall, this paper gives new contributions to the fair representation learning literature.  The authors should consider citing and discussing the relationship with the following work: A Reductions Approach to Fair Classification., ICML 2018
This paper proposes a method for learning physics combining symbolic computation and learning in an interesting way, targeting sample efficiency. At the initial evaluation, it was on the fence but leaning towards acceptance, with 3 slightly positive and one slightly negative review.   The strengths lie in the combination between symbolic reasoning and statistical ML with a formulation around the classical EM framework. On the other hand, an important issue of the paper is its quite simplistic evaluation on now very easy problems and benchmarks. While benchmarks tend to be simple in the field of learning physics, current work does address more difficult problems than the problems tackled in this paper.  Another issue discussed was the simple trade off in injecting hand crafted inductive bias into a system leading to increased sample efficiency, which was perceived as unsurprising by some reviewers. While this is common in ML, and even strongly more so in learning physics from data synthetically generated with known physical laws, it was perceived to be particularly unsurprising in this paper where the benchmarks are indeed very simple and the laws directly encoded.  The AC discussed this paper with the PCs, and it was judged that the weaknesses in evaluation, in particular the simplicity of the tasks, cannot compensate for the interesting hybrid symbolic/ML formulation, and decided to reject the paper.
The paper studies the properties and advantages of temporal abstraction in hierarchical latent variable based video prediction approaches, producing interesting results on various simulated environments and the KTH action dataset.   The two key drawbacks of this paper are: limited visual complexity of datasets used for evaluation (a real video dataset like BAIR pushing would have really helped), and lack of comparison (conceptually or empirically) to relevant prior work including those raised by various reviewers, plus see more below.  Aside from this, does the paper claim to propose a new model, or perform an empirical study to evaluate an existing model class, or both? The answer to this question is not always clear from the paper and this confusion is reflected both in the reviews and reviewer discussions and also in the authors  own responses to these.  Some potentially relevant works that don t find mention in this paper (aside from those pointed out by reviewers):   NAOMI: Non Autoregressive Multiresolution Sequence Imputation https://arxiv.org/pdf/1901.10946.pdf   Long Horizon Visual Planning with Goal Conditioned Hierarchical Predictors https://arxiv.org/pdf/2006.13205.pdf   I do think these are all relatively easily fixed shortcomings, and I urge the authors to fix them in future versions.
The paper considers the saddle point problem of finding non convex/non concave minimax solutions. Building onEG+ of Diakonikolas et al., 2021 that works under weak MVI conditions, the work presents a new algorithm CurvatureEG+ that works for a larger range of weak MVI condition compared to previous work and also works for the constrained and composite cases. The authors show cases where this algorithm converges while the previous algorithms can be shown to reach limit cycles. Overall, this theoretical work seems strong. Most reviewers seem to agree that the contribution is good enough for publication. Compared to EG+ the additional contribution is to expand the range of weak MVI condition. While this seems like a slight improvement, looking beyond just the final convergence rate, the paper has some nice insights that provides a unifying view that captures past algorithms (like EG+ as special case). I recommend acceptance.
The paper considers the problem of learning to imitate behaviors from visual demonstrations, without access to expert actions. Consistent with recent approaches, the proposed method uses a neural network to measure the similarity between visual demonstrations and the agent s behavior, and employs this metric as a reward in RL. The primary contribution is the use of a recurrent siamese network that is trained to measure the distance between motions, as a means of better dealing with the challenges of imitation learning from a small number of (as few as one) noisy visual demonstrations. Experiments on a variety of simulated domains show that the proposed approach achieves reasonable results.  The paper was reviewed by four knowledgeable referees, who read the author responses and engaged in extensive discussion. The reviewers agree that learning to imitate behaviors from a small amount of noisy demonstrations is a challenging and important problem that is of significant interest. The proposed method nicely extends existing approaches to visual imitation learning, and the results reveal that the method performs well in a variety of continuous control domains. The reviewers raise several concerns regarding the clarity of the technical presentation and the sufficiency of the experimental evaluation. The authors have made a significant effort to address these concerns in their responses and updates to the paper, which the reviewers very much appreciate. However, some of the reviewers  primary concerns regarding clarity and the thoroughness of the experimental evaluation remain. This work has the potential to make a really nice contribution and the authors are encouraged to take this feedback into account for any future version of the manuscript.
This manuscript proposes a quantization approach to improve adversarial robustness. Reviewers agree that the problem studied is timely and the approach is interesting. However, note concerns about the novelty compared to closely related work, the quality of the presentation, the strength of the evaluated attacks compared to the state of the art, among other concerns. There is no rebuttal.
The main concerns from the reviewers is the novelty of the algorithm and analysis from the CIVR algorithm of Zhang and Xiao (2019b). The author rebuttal clarified the main contributions as reformulation of DRO as composite finite sum optimization, solving heavily constrained optimization problems through composite optimization, and extension to distributed algorithms. They indeed lead to meaningful contributions to the important topic of DRO and open new avenues for structured constrained optimization problems. The paper is written very clearly and the empirical results on realistic problems are much appreciated.
The paper gives a theoretical analysis highlighting the role of codimension on the pervasiveness of adversarial examples. The paper demonstrates that a single decision boundary cannot be robust in different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set.   The main concern with the paper is that most of the theoretical results might have a very restrictive scope and the writing is difficult to follow.   The authors expressed concerns about a review not being very constructive. In a nutshell, the review in question points out that the theory might be too restrictive, that the experimental section is not very strong, that there are other works on related topics, and that the writing of the paper could be improved. While I understand the disappointing of the authors, the main points here appear to be consistent with the other reviews, which also mention that the theoretical results in this paper are not very general, that the writing is a bit complicated or heavy in mathematics, and not easy to follow, or that it is not clear if the bounds can be useful or easily applied in other work.   One reviewer rates the paper marginally above the acceptance threshold, while two other reviewers rate the paper below the acceptance threshold. 
I agree with the majority of reviews that this paper is not sufficiently convincing. 
The paper proposes a new imitation learning algorithm that explicitly models the quality of demonstrators.  All reviewers agreed that the problem and the approach were interesting, the paper well written, and the experiments well conducted. However, there was a shared concern about the applicability of the method to more realistic settings, in which the model generating the demonstrations does not fall under the assumptions of the method. The authors did add a real world experiment during the rebuttal, but the reviewers were suspicious of the reported InfoGAIL performance and were not persuaded to change their assessment.  Following this discussion, I recommend rejection at this time, but it seems like a good paper and I encourage the authors to do a more careful validation experiment, and resubmit to a future venue.
Pros    Thorough analysis on a large number of diverse tasks   Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words)   The comparison can be useful when deciding which representations to use for a given task  Cons    Nothing serious, it is solid and important empirical study  The reviewers are in consensus.
This paper asks a simple question: Do extreme activating synthetic images for a CNN unit help a human observer to predict that unit’s response to natural images, compared with maximally/minimally activating natural images. They authors conducted well designed human studies and found that the synthetic images provide useful information for prediction, but that the benefit is smaller than that provided by simply presenting people with other natural images that maximally or minimally activate a unit.  The paper provides one reasonable metric for evaluating feature visualizations. Feature visualizations are widely used, but there are very few objective metrics for evaluating them. This methodological contribution is the main contribution of this work and could be impactful. Although the conclusion is not very surprising, the paper makes a potentially good contribution to the literature. 
This paper introduces a scalable method for FSP based on FBSDE. The method is theoretically derived then applied on two problems, one simple but with many (1000) agents, and one with only 2 agents but partial observability.  The main strength of this paper lies in the scalability and the time complexity of the proposed method. Computing Nash equilibriums for many agents is a difficult problem and this paper is interesting in this aspect.  However, the reviewers point out several weak points to this paper. The difference with a previous work by Han, Hu and Long needs to be highlighted. Some parts of the paper are not clear, and too much of the important results are pushed into the appendix. Maybe this work is not best fitted to a conference format, and should be submitted to a journal? Another concern raised by the reviewers is that the experimental section does not show significant enough results, and that it is surprising to see a 2 agents problem as an illustration of a method that is aiming at addressing scalability with respect to the number of agents.  Reviewers agree on rejection for this paper, although by a small margin. I therefore recommend rejection. I think that if the authors improve this paper by following the reviewer suggestions, it can be accepted in a future venue.
The paper studies the convergence of a generalized gradient descent ascent (GDA) flow in certain classes of nonconvex nonconcave min max setups. While the nonconvex nonconcave setups are computationally intractable and GDA is known to converge even in some convex concave setups, the paper argues that (generalized) GDA can converge on what is dubbed "Hidden Convex Concave Games" in the paper and argued that it contains GANs as a special case.  The reviewers all found the paper interesting and a worthy contribution to the literature on nonconvex nonconcave zero sum games. Main concerns expressed by the reviewers were w.r.t. the lack of convergence rate established for the considered dynamics, novelty compared to existing work, and practical usefulness of the considered scheme, as it involves preconditioning/matrix inversion. The authors made an effort to address all the concerns, to the extent possible.  Given the complexity of nonconvex nonconcave min max setups, their importance in GAN training, and the insightful perspective on hidden convexity/concavity in typical problem instances, I would like to see this paper published at ICLR. I would, however, strongly advise the authors to take all of the reviewers  comments into account when preparing a revision.
The paper presents a very interesting idea for estimating the held out error of deep models as a function of model and data set size. The authors intuit what the shape of the error should be, then they fit the parameters of a function of the desired shape and show that this has predictive power. I find this idea quite refreshing and the paper is well written with good experiments. Please make sure that the final version contains the cross validation results provided during the rebuttal.
#### Summary  The goal of this work is to reduce the costs of inference in ensembled models by ensembling sparse models. The paper also aims to reduce the costs of training these ensembles as well. The proposed techniques (DST and EDST) each these goals, respectively.   #### Discussion  As noted by the reviewers, the paper is interesting and timely. The authors provided significant clarifications in the response that satisfied the reviewers  concerns. There is still significant room to revise the remaining points and polish the text of the paper for the camera ready (I highly recommend proofreading from an individual who is not an author on the paper; there are still typos in the revised edits)  #### Recommendation.   I recommend Accept, due to the strengths above and the reasonably scoped remaining work to do going into the camera ready.
All reviewers are unanimous that the paper is below threshold for acceptance.  The authors have not provided rebuttals, but merely perfunctory generic responses.  I think the most important criticism is that the approach is "very ad hoc."  I would encourage the authors to consider more principled ways of automatically designing reward functions, like for example, Inverse Reinforcement Learning, in which you start with a good agent behavior policy, and then estimate a reward function for which the behavior policy maximizes the reward function.
Thank you for your submission to NeurIPS.  The reviewers are quite split on this paper, but some remain substantially negative even after discussion.  I m a bit more optimistic about the paper: the observed increase then decrease in ER during fine tuning _does_ strike me as a fundamentally interesting phenomenon, and I believe that papers that present such phenomena can be valuable contributions even without more fundamental "explanations" of the observations.  My recommendation, therefore, ultimately rests largely on the fact that I think (as is honestly evidenced by the reviews to a large degree), the presentation and contextualization of these results can be substantially improved in a future revision of the paper.  Specifically, the fact that several reviewers found the results obvious and/or not sufficiently substantiated suggests that the basic premises here are still failing to land.  I would strongly suggest revisions that clarified these points in a resubmission.
The reviewers all found that the Consensus method introduced seemed sensible and applauded the authors on their extensive experiments.  However, clearly they struggled to understand the paper well and asked for a clearer and more formal definition of the methods introduced.  Unfortunately, the highest scoring review was also the shortest and also indicated issues with clarity.  It seems like the authors have gone a long way to improve the notation, organization and clarity of the paper, but ultimately the reviewers didn t think it was ready for acceptance.  Hopefully the feedback from the reviewers will help to improve the paper for a future submission.
This paper attempts to present a causal view of robustness in classifiers, which is a very important area of research. However, the connection to causality with the presented model is very thin and, in fact, mathematically unnecessary. Interventions are only applied to root nodes (as pointed out by R4) so they just amount to standard conditioning on the variable "M". The experimental results could be obtained without any mention to causal interventions.
This paper presents a method to interpret neurons in the vision neural models by generating natural language description that specifies the activation selectivity of a given neuron. The proposed method first identifies an exemplar set of input image regions that corresponds to a neuron, then searches a natural language description by optimizing the point wise mutual information between descriptions and the exemplar set.  Strength:   Reasonable method design and clear writing   Important problem and broad applications   Extensive experiments for evaluation of the proposed method  Weakness:   Need more discussion on the limitations of the proposed method   Elaboration on the human inter annotation agreement   Analysis on method transferability across tasks.
Meta score: 7  The paper combined low precision computation with different approaches to teacher student knowledge distillation.  The experimentation is good, with good experimental analysis.  Very clearly written.  The main contribution is in the different forms of teacher student training combined with low precision.  Pros:    good practical contribution    good experiments    good analysis    well written Cons:    limited originality
The paper studies how to build predictive models that are robust to nuisance induced spurious correlations present in the data.  It introduces nuisance randomized distillation (NuRD), constructed by reweighting the observed data, to break the nuisance label dependence and find the most informative representation to predict the label. Experiments on several datasets show that by using a classifier learned on this representation, NuRD is able to improve the classification performance by limiting the impact of nuisance variables. The main concerns were about the presentation and organization of the paper, which was heavily focused on the theoretical justifications but fell short in explaining the intuitions and implementation details. The revision and rebuttal have addressed some of these concerns and improved the overall exposition of the paper, based on which two reviewers raised their scores to 8. While there is still room to further improve the paper by providing more detailed discussions about the proposed algorithms, the AC considers the paper ready for publication under its current form.
The paper examines neural architecture search for multi task networks, by associating model hyperparameters with a coding space and building an MLP predictor for mapping codes to task performance.  After the discussion phase, reviewers are marginally in favor or accepting the paper, pointing to the extensive experimental results as a convincing contribution.
This paper focuses on a segmentation of cell imagery (as opposed to the more commonly studied domain of "natural images"). Among its contributions are a novel metric for evaluation of results and a novel dataset. These are acknowledged by the reviewers as strengths. Multiple issues raised in the initial reviews were addressed in the revision (the reviewers agree on this and most of them raised their scores). On the other hand, the concerns remaining have to do with significance and impact. The final evaluation ratings are split, with only a single score clearly in favor of acceptance.   I tend to agree that the contributions, while without a doubt valuable, make this less of a fit to ICLR than to a more specialized venue focusing on biomedical data. 
This work was deemed interesting by the reviewers, but they highlighted the following weaknesses in this version of the paper:    Lack of comparison to other methods.    Lack of novelty compared to previous work.    Fundamental problem with training only on one dataset (MNIST), issue with possible overfitting.
This paper presents an adaptive gradient method for neural net training inspired by L BFGS. All of the reviewers recommend rejection. They raise concerns about the amount of novelty, the clarity of the writing, and the experimental comparisons. I encourage the authors to take the reviewers  comments into account and improve the submission for the next cycle.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  Concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition.
The authors propose a new methods for graph shortest distance embedding method called BCDR based on betweenness centrality. Then they show that the method is competitive both theoretically than experimentally with existing work.  After a discussion with the reviewers and after considering the nice changes in the paper and explanation in the rebuttal we agree that the paper contains some very interesting ideas but it is not probably ready for publication. The comparison with previous works is, in fact, still a bit limited and it should be extended. In addition the algorithm should also be tested on larger datasets.
The paper considers a relevant and interesting problem of protecting the intellectual property of data. The goal of the proposed method is to prevent unauthorized usage of the data, and the protection is attained when a model trained on the perturbed dataset will predict poorly and thus cannot be considered as a realistic inference model by the unauthorized attacker.   Technically, the paper tackles the problem of "unlearnable examples": to perturb the images of a labeled dataset to obtain perturbed dataset such that models trained on perturbed dataset have significantly lower performance, the perturbations are small, and one can approximately recover the original labeled dataset with the correct "secret key" (learnable parameters).  The authors propose two invertible transformations to craft adversarial perturbations: linear pixel wise transformation and convolutional functional transformation based on invertible ResNet. Numerous experiments demonstrate the effectiveness of the proposed transformations in both securing the data (making the data unlearnable when transformation is applied) and unlocking the transformation (making the data learnable when the transformation is inverted).  The paper is well motivated and exhibits competitive results. Although there are some concerns about the similarity of the work compared with [1], we believe the additional constraint of this work, that one can approximately recover the original labeled dataset with the correct "secret key",  justifies a significant contribution.   [1] "Unlearnable Examples: Making Personal Data Unexploitable" Huang et al., ICLR  21
Meta Review of Cross Trajectory Representation Learning for Zero Shot Generalization in RL  This work investigates a zero shot generalization method for RL based on an online clustering adapting it to RL. The intuition of this approach (called Cross Trajectory Representation Learning, CTRL) is that the self supervised objective used will encourage the encoder to map behaviorally similar observations to similar representations without the use of reward signals. The authors performed experiments on the 16 procgen tasks, and compared it against several baselines (DBC, PSE, CURL, Proto RL and DIAYN). The performance is generally better against baselines, but what I like about it is that a new approach to achieve such performance is proposed.  The scores were generally good (6, 6, 6, 3), and the 6 s are overall positive with the work (both in the writing, breadth of experiments). Reviewer Kekc, who gave a score of 3, maintained their score, despite acknowledging the authors  responses. The main outstanding issue from Kekc is that they believe the paper should stick with the original 25M step protocol (with a larger training budget), rather than 8M steps. If that s the main issue for this paper to not be accepted into ICLR 2022, I feel this can be adequately addressed for the camera ready version. (Please note that while I disagree with the final score of 3 that Kekc gave, I find their review to be highly informative and useful, and would like to acknowledge Kekc for their input and discussion).  Based on the discussion and the reviews, and with the context behind the score of 3, I would like to be on the side of recommending this paper for acceptance, and urge strongly for the authors to conduct the 25M experiments as reviewer Kekc suggested (as Kekc also noted, the training curves are still going up, so just train them for a longer time). Even if the final results are not as good as the 8M, that s fine, just include them in the final camera ready version, since I believe this work to meet the bar, offers good insights into RL generalization, has a good breadth of experiments and baselines, and will be of great interest to the broader RL community.
This paper addresses audio visual navigation tasks where a reinforcement learning agent perceives visual RGB and binaural audio inputs, rendered in a first person perspective 3D environment, and is tasked to navigate to the audio source. The authors propose to make the RL navigation policy robust, by training the agent with additional adversarial audio perturbations. These perturbations consist of an adversarial "ghost" agent (attacker) that emits noise perturbations volume, position and category determined by policies that are trained to maximise the negative rewards for the navigation agent in zero sum game. The agent is then evaluated on the simulated Replica and MatterPort3D environments and compared to a few baselines. The authors conduct a large number of ablation experiments.  The three reviewers were globally positive about the paper, regarding the motivation, joint training of the agent and attacker, and experimental evaluation. Reviewer TLMn had questions about specific results and ablations of existing baselines, whereas reviewer i5Vv had questions about random noise ablations   the authors provided responses for these questions. Outstanding requests were about proofreading.  After rebuttal and discussion, the scores for this paper are 6, 8 and a weak 8 (or 7), i.e., an average of 7, and thus I believe that the paper meets the conference acceptance bar.
In this paper, the authors claim to propose a distributed large batch adversarial training framework to robustify DNN. Although the authors made efforts to clarify reviewers  concerns,  it is clear that the authors still cannot convince some reviewers in several points after several rounds of discussion between reviewers and authors.  The reviewers were not in consensus on acceptance and some concerns were still not clearly addressed in the rebuttal phase. Hence, I recommend acceptance only if there is a room.  
The paper proposes a new architecture for unsupervised image2image translation. Following the revision/discussion, all reviewers agree that the proposed ideas are reasonable, well described, convincingly validated, and of clear though limited novelty. Accept.
The paper introduces a new dataset that contains multiple landings from the X plane simulator, and each includes readings from multiple sensors for aircraft landing. The  paper also trains a set of self supervised methods presented in previous works in order to learn sensory representations, and evaluates the learnt representations in terms of disentanglement and re purposing to a discriminative task.   Though the evaluations presented are interesting, they are not convincingly useful, as noted by the reviewers. Overall, it is not clear why this dataset is particularly well suited for representation learning. Furthermore, it is difficult to evaluate representation learning methods without relating them to an end task, e.g., that of landing the aircraft.  The paper writing would also benefit from restructuring and improving on English expressions. In particular, the conclusion section contains half finished sentences. 
The reviewers reached a consensus that the paper is preliminary and has a very limited contribution. Therefore, I cannot recommend acceptance at this time.
There was a healthy discussion with all the reviewers with a consensus that the results are somewhat expected and unlikely to shed light beyond the ntk  regime, yet within the confine of ntk there is a solid and nicely written technical contribution.
This paper introduces a dataset, based on preexisting standardized tests, of elimination/grid completion style logical reasoning puzzles expressed in text; available in both Chinese and English (with some of the text coming from semi automatic translation). The early pretrained MLMs BERT and RoBERTa perform poorly.   This paper is solidly borderline. Reviewers had some concerns about the motivation and novelty the work, but I think that there is a plausible enough story for where this data will have value that I m not comfortable rejecting it only on this basis. More worryingly, the initial submission had some fairly serious writing quality and clarity issues, which impacted both the paper *and* the data. It seems like the authors made significant progress on this in the revision and engaged substantially during the discussion, but reviewers were not fully satisfied that the paper was up to ICLR standards, either as an initial submission or after revisions. This is a small detail, but it s a bad sign for the carefulness of the work that the OpenReview abstract is still unreadable, even after a request from a reviewer.
This paper introduces a novel game theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner. The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large scale dataset (ResNet 200 activations). This paper is generally clearly written, and in particular Section 2 provides an easy to follow reasoning leading to the proposed game theoretic reformulation of PCA. I felt that the later sections are a bit condensed, including the figures. In the authors response major concerns raised by the reviewers have been appropriately addressed. I would thus recommend acceptance of this paper.  What I found particularly interesting in their game theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints $\hat{u}_j^\top\hat{u}_i 0$ have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality. Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable.  Pros:   Provides a novel game theoretic reformulation of PCA.   Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game theoretic reformulation.   Provides theoretical guarantee for the global convergence of the sequential algorithm.   Demonstrates that the proposed decentralized algorithm is scalable to large scale problems.  Cons:   The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high dimensional settings.   Significance of the proposed game theoretic formulation in the context of game theory does not seem to be well explored. 
This paper studies the problem called "Predict, then Optimize (PTO)", where the goal is to predict some unknown parameters in the objective function and then optimize the function. The paper focus on special case of PTO where the goal is to learn the unknown parameters corresponding to some combinatorial ordering of the data. The paper argues that traditional methods which perform the ranking after minimizing the objective can be very unstable, and proposes a new alternative loss called ATGP loss. The goal of the loss is to ensure that the total group preorder of the learned solution is as consistent with the observation as possible after fitting the data.   The paper shows some property of the new loss and claims that it can be better than naive regression then ranking approach on learning to rank with linear regression problem, however, the theorem only shows that the robustness bar of TGP loss is larger or EQUAL to that of the naive approach, not exactly higher   Thus, such an argument is not really convincing.  The authors also proposed a heuristic algorithm to minimize a smoothed version of the proposed loss.   However, the proposed objective is still interesting on its own, and the experimental result looks promising. Thus, this paper is a borderline accept paper at this conference.
This is a tricky one, hence my low confidence rating.  The reviewers seem to agree that the paper is well written, easy to follow, and that it tests a relevant hypothesis that is of interest to the community. There was some disagreement as to whether the experiments are comprehensive, complete and/or conclusive enough, although on balance it seems reviewers were overall satisfied barring a few additional requests which the authors addressed in their feedback.  However, no reviewers support the paper strongly (borderline accepts) while R5 remains unconvinced and has raised a technical point in their review about the estimator of the Trace of the Fisher information matrix. The question R5 has raised is central to the paper s methods, arguments and conclusions. In a message to ACs and PCs the authors raised concerns about R5. I personally thought that while R5 could have worded their review more carefully and respectfully (as I pointed out in my respose) the concerns raised were otherwise motivated, the reviewer engaged in a discussion, and the arguments were laid out clearly. I side with R5 and I think that the paper should be rewritten with more clarity on this question   the problem R5 found is likely to trip up others who read or build on the paper.  The authors have raised that there are two parallel submissions closely related to this one, complicating the decision making somewhat: [1] https://openreview.net/forum?id rq_Qr0c1Hyo [2] https://openreview.net/forum?id 3q5IqUrkcF 
This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i.e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4 way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation, obtaining a reasonable improvement, and makes a good case for using an equivariant seeking loss. The authors are strongly encouraged to release their code (including training details for reproducing ImageNet results) as the improvements they present are central to the acceptance.
This is a thought provoking paper that places GANs and VAEs in a single framework and, motivated by this perspective, proposes several novel extensions to them. The reviewers made several good suggestions for improving the paper and the authors are expected to make the revisions they promised. The current title of the paper is too general and should be changed to something more directly descriptive of the contents.
This paper proposes a new method for the important problem of semi supervised learning. This method relies on an auxiliary task, label observability prediction, to weight the examples according to the confidence in their pseudo labels, so as to avoid the propagation of errors encountered in self training. Limited experiments show that the proposed method can compete with other methods in terms of performance or training time. On the positive side, all evaluators agree on the potential value of the proposed approach, which is generic in nature. On the negative side, the experimental evaluation, although strengthened during the discussion, is not yet strong enough to have really convinced of the real merits of the method. In particular, comparisons with the state of the art still need to be improved. In addition, the paper would benefit from some rewriting, in particular of the mathematics (e.g. the d notation for task B should be avoided as suggested by one reviewer, there is a misplaced partial derivative in equation 6). The authors could also simplify their derivation by using the envelope theorem. I therefore recommend rejection, with an encouragement to strengthen the experimental part, and to improve the derivation of the proposed method.
The authors propose an algorithm to perform backprop in a feed forward neural network without the need to backpropagate errors. They hence claim that this algorithm is a biologically plausible variant of Backprop.  After a forward propagation phase, the method introduces a relaxation phase and they remark that at the equilibrium of this phase, the activity is equal to the derivatives. Some related algorithms have been proposed previously (predictive coding, equilibrium prop, target propagation). Advantages of the proposed algorithm are that it does not need multiple distinct backwards phases and that it only utilizes a single type of neuron instead of separate populiations (such as in predictive coding).  Their method is tested on MNIST and fashion MNIST using a 4 layer fully connected network. In a revised version after the initial reviews, the authors added preliminary results on CIFAR 10 with a 4 layer CNN.  The authors then study the impact of some unbiological constraints such as symmetric weights.  While the reviewers agreed that the work is interesting, there was some disagreement on the significance of the model. In particular, it was noted that while the learning rules are indeed local in space, they are not local in time (the network has to remember variables from the forward phase until the update at the relaxation phase), which was deemed questionable from the biological perspective.  In addition, it was criticized that the simulations are no sufficient to support the claims of the paper. The datasets are relatively simple, networks are shallow and performances of baseline models are not state of the art.  In a revised version after the initial reviews, the authors added preliminary results on CIFAR 10 with a 4 layer CNN. However, these results do not seem conclusive, as the baselines are far below SOTA and networks are still quite shallow (a study by Lillicrap found that biological approximations to backprop struggle especially when applied to deep networks).  In summary, the work looks promising but some questions remain about the locality of learning and its applicability to more demanding tasks.  I add that one reviewer gave a very good rating with a poor review and did not respond to any questions about the justification. Therefore I had to neglect this review.
* the proposed fine tuning of only the last layer is not novel enough * experiments are not sufficient to isolate the differences to support the benefit of post training 
This paper studies two techniques for handling high dimensional action spaces in deep RL, namely selecting action components independently or selecting components sequentially in an autoregressive approach.  The methods are developed for two deep RL algorithms (PPO and SAC) and tested on multiple domains.  The reviewers recognized the significance of this research topic but found significant problems in the presentation.  The reviewers raised concerns on the relationship to prior work in the literature (R2), baseline comparisons that are missing in the experiments (R2, R4), and a lack of clarity in the intended message of the experiments (R3).  The authors responded favorably to the reviews, answered clarification questions, and acknowledged the limitations of the submitted work.  The authors expressed their intent to release a stronger paper sometime in the future.  The reviewers acknowledged the author response and were in consensus that the submission needs more work.  Three reviewers have indicated reject for the reasons described above.  The paper is therefore rejected.
This paper describes a method for bounding the confidence around predictions made by deep networks. Reviewers agree that this result is of technical interest to the community, and with the added reorganization and revisions described by the authors, they and the AC agree the paper should be accepted. 
This paper proposes a method for predicting higher order structure in time varying graphs. The paper was reviewed by three expert reviewers, and while they expressed appreciation for the sensible solution, they have remaining concerns about the novel contributions and comparisons (analytical and empirical) with previous approaches. Also, the paper would be clearer if examples are used to illustrate the important points of the paper. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
Even though the authors revised the problem formulation, the paper seems not ready for publication. The assumptions are still too strong (The learning algorithm assumes knowledge of the sparsity mask). The proof technique also heavily relies on Zhong et al 17 without properly highlighting the difference. 
A novel dual memory system inspired by brain for the important incremental learning and very good results.
As identified by most reviewers, this paper does a very thorough empirical evaluation of a relatively straightforward combination of known techniques for distributed RL. The work also builds on "Distributed prioritized experience replay", which could be noted more prominently in the introduction.
This paper proposes a genetic algorithm to search neural network architectures with locally dense and globally sparse connections. A population based genetic algorithm is used to find the sparse, connections between dense module units. The local dense but global sparse architecture is an interesting idea, yet is not well studied in the current version, e.g. overfitting and connections with other similar architecture search methods. Based on reviewers’ ratings (5,5,6), the current version of paper is proposed as borderline lean reject.  
The authors made substantial  improvements to the originally submitted manuscript; however, reviewers initially remained reluctant to support the paper for acceptance based on the degree to which they were confident in the underlying arguments / position taken by the authors and the evidence provided to support their position and arguments. There are also concerns about the significance of the gains in performance afforded by the proposed approach.  During the author response period two reviewers became satisfied with the additions and modifications leading to an increase in the final score. It will be critical for the authors to try to add ImageNet results if possible in addition to other promises made to reviewers.  The AC recommends accepting this paper.
The paper proposes a new pseudometric for action representations.  The reviewers generally liked the work and the rebuttal helped to clarity may concerns.  However, the degree of novelty of the approach remains a concern.  In addition, a technical error was discovered by a reviewer in the revised paper.  Hence the paper is not ready for publication.
The paper proposes a variant of the max sliced Wasserstein distance, where instead of sorting, a greedy assignment is performed. As no theory is provided, the paper is purely of experimental nature.   Unfortunately the work is too preliminary to warrant publication at this time, and would need further experimental or theoretical strengthening to be of general interest to the ICLR community.
All reviewers believe that the paper is not ready for publication and clarity issue remain. All reviewers read the rebuttal responses, but they found that the paper wasn t revised during rebuttal, thus they retain their decisions.
This paper uses concepts from physics to make predictions about stochastic gradient descent. The reviews point to two issues. Firstly, the paper was not very accessible to those without a relevant background, and this is reflected in the low confidence rating reviewers gave. More importantly, two of the reviewers consistently pointed out  vague mathematics  and oversimplification in the mathematical arguments.  The authors  feedback did not successfully address the reviewer s concerns, both R3 and R4 indicated there were outstanding concerns.  I should note that despite giving low confidence scores and stating that some concepts from physics are beyond their field of expertise, reviewers gave high quality reviews with detailed comments and questions, and subsequently participated in the discussion revisiting their reviews. This suggests that the low confidence is not a symptom of insufficient reviewer effort, but perhaps a consequence of an inaccessible paper.
The reviewers really liked this paper. This paper presents a tweak to the LSTM cell that introduces sparsity, thus reducing the number of parameters in the model.  The authors show that their sparse models match the performance of the non sparse baselines. While the results are not state of the art but vanilla implementations of standard models, this is still of interest to the community.
This paper describes a method for generating adversarial examples from images and text such that they maintain the semantics of the input.  The reviewers saw a lot of value in this work, but also some flaws.  The review process seemed to help answer many questions, but a few remain: there are some questions about the strength of the empirical results on text after the author s updates. Wether the adversarial images stay on the manifold is questioned (are blurry or otherwise noisy images "on manifold"?).  One reviewer raises good questions about the soundness of the comparison to the Song paper.  I think this review process has been very productive, and I hope the authors will agree.  I hope this feedback helps them to improve their paper.
The paper describes a new technique to generalize across different environments.  More precisely a new state similarity metric is defined with a contrastive learning embedding technique.  Unlike previous works that extend supervised learning techniques such as data augmentation and regularization to RL, the proposed approach takes into account the sequential nature of RL.    The reviewers unanimously praised the work in terms of theory, algorithm and empirical evaluation.  This is a novel and technically deep contribution that advances the state of the art for RL generalization.
The paper explores unsupervised domain adaptation when the output is structured. Here they focus experimentally on semantic segmentation in driving scenes and use the spatial structure of the scene to produce two losses for adaptation: one global and one patch based. The method tackles an important problem and proposes a first attempt at a new solution. While the the experiments are missing ablations and some comparisons to prior work as noted by the reviewers, the authors have provided comments in their rebuttal explaining the relation to the prior work and promising to include more in the revised manuscript.   The paper is borderline, but falls short on the necessary updates requested by reviewers.  The use of the structured output which is available in semantic segmentation of driving scenes is a useful direction. The paper is missing enough key results and analysis in it s current form to be accepted. 
This manuscript presents a novel approach to learning a shared language between multiple agents.  In general, reviewers had difficulty understanding the symbolic mapping component. For such a critical part of the manuscript, questions by multiple reviewers were extremely basic, asking what symbolic mapping even is. Authors did clarify this in the discussion and updated the manuscript, but further improvements to the manuscript are warranted.  Reviewers had concerns about the novelty of the approach. Including being confused about whether this is just an application of curriculum learning. Reviewers were also concerned about the lack of ablations.  Reviewers also had concerns about the fact that this is a toy domain. Symbolic mapping as defined in the manuscript appears to be possible only for such toy domains. It fundamentally wouldn t scale to simple language games with real images. This significantly limits the scope of the work. More broadly, reviewers wanted to see symbolic mapping exercised much more. If this is a useful idea, they wanted to see the authors apply it to other domains.  Reviewers were confused about many other details in the manuscript. For example, about the fact that refdis is later discarded as a metric, which the authors answered is due to redundant symbols ("the symbolic mapping is not a highly compositional representation here because of the redundant symbols"). Why redundant symbols lead to less compositional representations seem unclear.  With significant additional improvements to the clarity of the manuscript, a demonstration of how symbolic mapping is useful in another domain, and additional experiments suggested by multiple reviewers this could be a strong submission in the future.
The reviewers generally liked the paper but had several concerns. The rebuttal and revision of the paper could mitigate most concerns and the reviewers are now mostly positive towards the paper. Remaining concerns are mostly about the presentation of the paper which indeed has room for improvements but overall is good enough to accept the paper.  
This paper proposes a benchmark for assessing the impact of image quality degradation (e.g. simulated fog, snow, frost) on the performance of object detection models. The authors introduce corrupted versions of popular object detection datasets, namely PASCAL C, COCO C and Cityscapes C, and an evaluation protocol which reveals that the current models are not robust to such corruptions (losing as much as 60% of the performance). The authors then show that a simple data augmentation scheme significantly improves robustness. The reviewers agree that the manuscript is well written and that the proposed benchmark reveals major drawbacks of current detection models. However, two critical issues with the paper paper remain, namely lack of novelty in light of Geirhos et al., and how to actually use this benchmark in practice. I will hence recommend the rejection of this paper in the current state. Nevertheless, we encourage the authors to address the raised shortcomings (the new experiments reported in the rebuttal are a good starting point). 
This paper proposes ScaleGrad, a simple technique to encourage generating non repetitive tokens for text generation tasks. The key idea is to modify a language model s token level distributions by rescaling the softmax probability for certain words (in the novel set) by a factor of $\gamma$. Experiments show that ScaleGrad outperforms MLE and Unlikelihood Training (UT).  This paper receives 2 reject and 2 accept recommendations. Most of the reviewers have provided very detailed comments, and the authors have also provided very long and detailed responses. On one hand, all the reviewers agree that the experiments are comprehensive, and the motivation of the proposed method is clear.   On the other hand, several concerns still exist after the rebuttal, namely, hand wavy arguments and inconsistent experimental protocol. (i) The empirical evidence in the experiments is not convincing enough. It makes reviewers more reluctant about the approach after seeing more experimental results during the discussion. That is, for different tasks, different $\gamma$s are used, while the hyper parameters used for other methods seem to be default values. This makes reviewers hesitant that scaling novel tokens and renormalizing the model s output distribution is really significant. (ii) Another minor concern is that the discussion on the potential issue of UL (sec. 5.4) does not look convincing. (iii) After reading the paper, the AC also feels that the novelty of the proposed method might be a little bit limited.   The rebuttal unfortunately did not fully address the reviewers  main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper proposes a faster approximation to batch norm, which avoids summing over the entire batch by subsampling either random examples or random image locations. It analyzes some of the tradeoffs of computation time vs. statistical efficiency of gradient estimation, and proposes schemes for decorrelating the samples to make good use of smaller numbers of samples.  The proposal is a reasonable one, and seems to give a noticeable improvement in efficiency. However, it s not clear there is a substantial enough contribution for an ICLR paper. The idea of subsampling is fairly obvious, and various other methods have already been proposed which decouple the computation of BN statistics from the training batch. From a practical standpoint, it s not clear that the observed benefit is large enough to justify the considerable complexity of an efficient implementation.  
This paper proposes a variant of Hamiltonian Monte Carlo for Bayesian inference in deep learning.  Although the reviewers acknowledge the ambition, scope and novelty of the paper they still have a number of reservations regarding experimental results and claims (regarding need for hyperparameter tuning). The overall score consequently falls below acceptance.  Rejection is recommended. These reservations made by the referees should definitely be addressable before next conference deadline so looking forward to see the paper published asap.
The paper introduces an augmentation technique that, given an image with a detected object, keeps the object and removes the background.  The reviewers expressed numerous valid concerns about the paper s novelty, the setting (assumption that there s a single object), the scalability of the approach and the experimental setup, including the baselines used.  The authors have not addressed these concerns.
I agree with the reviewers  comments. The technique proposed in the paper is very interesting, and although the method itself is not particularly surprising (it s "just" chaining two compressors), it s a really nice way of framing and studying the problem. On the other hand, the experiments _are_ relatively weak, and I think there is significant potential for improvement here (especially with an added 9th page of text). I encourage the authors to add some more convincing experiments in future versions of the paper.
The reviewers find the work interesting and well made, but are concerned that ICLR is not the right venue for the work.  I will recommend that the paper be accepted, but ask the authors to add the NMT results to the main paper (any other non synthetic applications they could add would be helpful).
This paper was close and also very polarizing with the reviewers. On the positive side, some reviewers found: 1. the results impressive 2. the proposed method to be novel, interesting, and produce good performance across several settings 3. the paper was well written  On the other hand, others found: 1. the motivation suspect 2. missing experiments to characterize the sensitivity to numerous hyper parameters  3. the baselines compared with weak and not representative 4. significant performance drop comparing the results in the original submission and the new ones added during discussion period 5. low number of seeds initially  In the end, multiple reviewers raised serious issues regarding the motivation for the approach and the quality and ultimately credibility of the results presented. One of the high scoring reviewers agreed the paper was a bit misleading (limitations relegated to the appendix). Unfortunately, none of the high scoring reviewers provided counters to this points.
 The paper presents a sensible algorithm for knowledge distillation (KD) from a larger teacher network to a smaller student network by minimizing the Maximum Mean Discrepancy (MMD) between the distributions over students and teachers network activations. As rightly acknowledged by the R3, the benefits of the proposed approach are encouraging in the object detection task, and are less obvious in classification (R1 and R2).   The reviewers and AC note the following potential weaknesses: (1) low technical novelty in light of prior works “Demystifying Neural Style Transfer” by Li et al 2017 and “Deep Transfer Learning with Joint Adaptation Networks” by Long et al 2017   See R2’s detailed explanations; (2) lack of empirical evidence that the proposed method is better than the seminal work on KD by Hinton et al, 2014; (3) important practical issues are not justified (e.g. kernel specifications as requested by R3 and R2; accuracy efficiency trade off as suggested by R1); (4) presentation clarity.   R3 has raised questions regarding deploying the proposed student models on mobile devices without a proper comparison with the MobileNet and ShuffleNet light architectures. This can be seen as a suggestion for future revisions.   There is reviewer disagreement on this paper and no author rebuttal. The reviewer with a positive view on the manuscript (R3) was reluctant to champion the paper as the authors did not respond to the concerns of the reviewers.  AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
The paper presents an interesting idea for increasing the robustness of adversarial defenses by combining with existing domain adaptation approaches. All reviewers agree that the paper is well written and clearly articulates the approach and contribution.  The main areas of weakness is that the experiments focus on small datasets, namely CiFAR and MNIST.  That being said, the algorithm is reasonably ablated on the data explored and the authors provided valuable new experimental evidence during the rebuttal phase and in response to the public comment. 
This paper proposes a new active learning algorithm based on clustering and then sampling based on an uncertainty based metric. This active learning method is not particular to deep learning. The authors also propose a new de noising layer specific to deep learning to remove noise from possibly noisy labels that are provided. These two proposals are orthogonal to one another and its not clear why they appear in the same paper.  Reviewers were underwhelmed by the novelty of either contribution. With respect to active learning, there is years of work on first performing unsupervised learning (e.g., clustering) and then different forms of active sampling.   This work lacks sufficient novelty for acceptance at a top tier venue. Reject
The paper addresses safe multi agent reinforcement learning and makes two key contributions. First is a safety concerned multi agent benchmark, which is an extension of MAMuJoCo. Second, is the formulation and two solution to safety MARL problem. The authors pose safe MARL, and MARL problem with safety constraints, as a constrained Markov game.  The safety constrained MARL is an important, difficult, and understudied problem. The problem is more difficult that the single agent safe RL because of the non stationarity in the MARL setting, which renders any theoretical guaranties conditioned on the assumptions of the behaviors of other agents. The authors are right to point out the lack of the benchmarks in the space.   That said, reflecting on the reviewers  feedback and my own reading of the paper, this paper is attempting to do too much (benchmark, problem formulation, and two methods), in too little space, and is falling short. For example, the benchmark is an important contribution, but it is barely mentioned in the main text of the paper. If this was fully safety benchmark paper, there is an opportunity to go beyond MAMuJoCo, which feels like a forced multi agent problem, and construct a safety benchmark with energy constraints, cooperative and competitive tasks etc... If this was fully methods paper, there would be an opportunity for more in depth analysis of the results that the reviewers  pointed out. In it s current form, the paper feels like proposing a benchmark not grounded in a real world problem, and then a method to solve the problem.  I would suggest the authors to either:   submit the paper to a journal where a space constraint would not be in a way, or   split it into two papers, a more comprehensive benchmark, and methods paper evaluated on more difficult problems.   Minor:   Please update the literature. Some of the papers have been published, and they are cited as Arxiv papers.
The paper proposes an approach to learn sparse embeddings for documents/labels which can be trained by using multiple GPUs in parallel, and are more amenable to nearest neighbor search.   The paper certainly seemed to have botched  comparison to SNRM and requires to fix the claims in  section 5.1.  But, the impressive performance on extreme classification tasks is quite convincing. Also, reviewers in general are quite enthusiastic about the paper. So we would recommend the paper for acceptance, but authors certainly need to take comments of reviewers into account (especially around baselines and comparison to SNRM).
The authors investigate the claim that agents in emergent communication games will converge to a symmetric homogenous state.  In particular, the authors show/argue for diversity in the population to close the gap between observed trends in neural agents and those expected when studying natural languages (e.g. around structure).  Reviewers were generally positive, though requested a number of rhetorical changes needed and additional literature.  These have been addressed.
This paper proposed a flow based approach FCause to Bayesian causal discovery that is scalable, flexible, and adaptive to missing data. Reviewers were split on this paper and could not reach a consensus during the discussion, and no reviewer pushed for acceptance. After taking a closer look myself, I agree with several of the reviewers that while the core ideas here are interesting and novel, there remain too many unresolved issues that require another round of revision.  I encourage the authors to carefully take in account the reviewers  comments and re submit this promising work to another ML venue.
Dear authors,  There was some disagreement among reviewers on the significance of your results, in particular because of the limited experimental section.  Despite this issues, which is not minor, your work adds yet another piece of the generalization puzzle. However, I would encourage the authors to make sure they do not oversell their results, either in the title or in their text, for the final version.
The paper presents a meta learning for Model based RL that introduces branched rollouts to improve sample efficiency of the learned model.  While the paper addresses an important topic of sample efficiency in RL, and provides theoretical analysis, the reviewers raised concerns with the novelty and clarity. The extension to POMDP setting is certainly important technological contribution, albeit a straightforward. To be suitable for publication the work needs to make stronger case for the significance of the method.  
This paper studies the problem of learning single layer neural networks under Gaussian marginals in the presence of outliers. The authors give a recovery algorithm in this setting. The consensus among the reviewers was that the paper lacks technical depth. Specifically, the algorithm is a minor tweak of the one in Wu et al. 2019 for the case without outliers. Another concern was that the algorithm does not recover the prior result when the fraction of uncorrupted points goes to 0. Overall, the paper is below the acceptance threshold.
The paper presents a simple but surprisingly effective data augmentation technique which is thoroughly evaluated on a variety of classification tasks, leading to improvement over state of the art baselines. The paper is somewhat lacking a theoretical justification beyond intuitions, but extensive evaluation makes up for that.
The authors propose a method to learn a neural network architecture which achieves the same accuracy as a reference network, with fewer parameters through Bayesian Optimization. The search is carried out on embeddings of the neural network architecture using a train bi directional LSTM. The reviewers generally found the work to be clearly written, and well motivated, with thorough experimentation, particularly in the revised version. Given the generally positive reviews from the authors, the AC recommends that the paper be accepted. 
The authors present a method to address off policy policy evaluation in the infinite horizon case, when the available data comes from multiple unknown behavior policies.  Their solution   the estimated mixture policy   combines recent ideas from both infinite horizon OPE and regression importance sampling, a recent importance sampling based method.  At first, the reviewers were concerned about writing clarity, feasibility in the continuous case, and comparisons to contemporary methods like DualDICE.  After the rebuttal period, the reviewers agreed that all the major issues had been addressed through clarifications, rewriting, code release, and additional empirical comparisons.  Thus, I recommend to accept this paper.
The authors propose implementing intrinsic motivation as a differentiable supervised loss coming from the error of a forward model, rather than the black box style of curiosity reward. The motivation is that this approach will lead to more sample efficient exploration for real robots. The use of a differentiable loss for policy optimization is interesting and has some novelty. However, the reviewers were unanimous in their criticism of the paper for poor baselines, unclear experiments and results, and unsupported claims. Even after substantial revisions to the paper, the AC and reviewers were unconvinced of the basic claims of the paper.
This paper proposes a new permutation invariant loss (where the order doesn t matter), motivated by set autoencoding settings. This is an important problem, and the authors  solution is interesting.  The reviewers, however, found the exposition to be unclear, in particular the explanation on how the loss function is derived was confusing for two of the reviewers. Reviewers also found the experimental results to be not convincing, even after the revision. This is a borderline paper: the idea is valuable and I d encourage the authors to develop it further, improving exposition and including additional experiments as suggested by the reviewers. 
The reviewers all agreed that this paper makes a strong contribution to ICLR by providing the first asynchronous analysis of a Nesterov accelerated coordinate descent method.
This paper is a follow up paper of Zhang et al. (2021), that proposed a new network architecture for adversarial robustness, l_\infty distance net. Although the l_\infty network is provably 1 Lipschitz w.r.t. the l_\infty distance, its training procedure exploits the l_p relaxation to overcome the non smoothness of the model but suffers from an unexpected large Lipschitz constant at the early training stage, an issue to be solved. This paper resolves this issue by a new loss design of scaled cross entropy loss and clipped hinge loss. Without using MLP on top of the l_\infty distance net backbone, the proposed new training method empirically outperforms the original one in Zhang et al. (2021) and improves over the state of the art by more than 5% for 8/255 and other radiuses. Moreover, the paper shows the theoretical expressive power of l_\infty distance net for well separated data.  There are some concerns about the moderate novelty and reproducibility of the results. Since the empirical results are indeed impressive, the paper could be accepted conditional on that the authors release their reproducible codes to the public.
This paper studies the important problem of time series anomaly detection using deep neural networks (DNNs). Unlike many other DNN models, it focuses on incorporating in its model architecture interpretable components that are inspired by previous studies based on both conventional statistical methods and more recent DNN models.  While the paper has merits as pointed out by the reviewers (esp. TtBt), a number of concerns have also been raised, including the choice of datasets (e.g., by reviewers rnBY and zX4p). We appreciate the authors’ effort by adding some preliminary results of further experiments, but addressing all the concerns thoroughly will need a lot more work to get a scholarly paper that is more ready for publication. We believe this work has potential to be accepted for publication in a reputable venue if the concerns are thoroughly addressed after substantial revision.
This paper presents a domain generalization method for semantic segmentation. The model is trained on synthetic data (source) and is tested on unseen real datasets (target). The authors propose a simple data augmentation method, AdvStyle, generating unconstrained adversarial examples for the training on the source domain.  There was no consensus on the method among the reviewers. Several issues have been raised. After rebuttal and discussion, no one really changed her/his mind. The motivation of why focus just on driving scenes is still questionable. Definitively, it could be interesting to investigate further why it is not straightforward to have gains on other kinds of scenes. Finally, we encourage the authors to address the raised concerns regarding the discussion with previous works and the comparisons for future publication.
The authors present a method for learning disentangled skill representation that uses weak supervision. The reviewers mentioned that the paper tackles an important problem, delivers interesting and novel visualizations of the learned skills, and positions the paper well in the context of related work. The reviewers also point out several points of criticism: the complexity of the method, lack of convincing comparisons to baselines that utilize the same amount of data and the quality of writing, among others. I encourage the authors to address these points in the future version of the paper.
This paper takes a step towards understanding the role of nonlinear function approximation  more specifically, function approximation via (two layer) neural nets in some variants of the policy gradient algorithms. The authors borrow the mean field analysis idea recently popularized in studying shallow neural nets, and investigate the mean field limits of the training dynamics in the current RL settings. The results and analyses are interesting as they nicely complement another line of linearization based analyses (i.e., the one based on neural tangent kernels) towards understanding non linear function approximation. As suggested by a reviewer, it would be nice to add discussions in the revised paper regarding when the dynamics can be guaranteed to converge to a stationary point.  
The paper points out pitfalls of existing metrics for in domain uncertainty quantification, and also studies different strategies for ensembling techniques.  The authors also satisfactorily addressed the reviewers  questions during the rebuttal phase. In the end, all the reviewers agreed that this is a valuable contribution and paper deserves to be accepted.   Nice work!
Solid work on extending AntisymmetricRNN and expanding its expressivity while controlling the global stability of the recurrent dynamics. It contributes to the growing interest in continuous time RNN formulations that can deal with exploding gradient problem, and worthy of ICLR poster presentation. Three reviewers were positive and one was slightly negative. Authors added additional experiments and strengthened the manuscript significantly during the review process.
Paper received mixed reviews: WR (R1), A (R2 and R3). AC has read reviews/rebuttal and examined paper. AC agrees that R1 s concerns are misplaced and feels the paper should be accepted.  
The paper proposed a multi hop machine reading method for hotpotqa and squad open datasets. The reviewers agreed that it is very interesting to learn to retrieve, and the paper presents an interesting solution. Some additional experiments as suggested by the reviewers will help improve the paper further. 
The paper presents new loss functions (which replace the reconstruction part) for the training of conditional GANs. Theoretical considerations and an empirical analysis show that the proposed loss can better handle multimodality of the target distribution than reconstruction based losses while being competitive in terms of image quality.
This paper investigates copying mechanisms and reward functions in sequence to sequence models for question generation. The key findings are threefold: (1) when the alignments between input and output are weak, it is better to use latent copying mechanism to soften the model bias toward copying, (2) while policy gradient methods might be able to improve automatic scores, their results poorly align with human evaludation, and (3) the use of adversarial objective also does not lead to useful training signals.       Pros: The task is well motivated and the paper presents potentially useful negative results on policy gradient and adversarial training.  Cons: All reviewers found the clarity and organization of the paper requires improvements. Also, the proposed methods are reletively incremental and the empirical results are not strong. While the rebuttal answered some of the clarification questions, it does not address major concerns about the novelty and contributions.  Verdict: Reject due to relatively weak contributions and novelty.
The paper proposes two methods for interactive panoptic segmentation (a combination of semantic and instance segmentation) that leverages scribbles as supervision during inference. Reviewers had concerns about the novelty of the paper as it applies existing algorithms for this task and limited empirical comparison with other methods. Reviewers also suggested that ICLR may not be a good fit for the paper and I encourage the authors to consider submitting to a vision oriented conference. 
The authors propose a technique called Autoencoder Adversarial Interpolation (AEAI). The key idea is to train autoencoder architectures that explicitly "shapes" trajectories in the encoder (latent) space to correspond to smooth geodesics between data points. This is achieved by a combination of several loss terms that are fairly intuitive. The authors empirically justify each term via ablation studies on simple datasets.  Initial review scores had wide variance. The reviewers liked the overall approach as well as the clarity with which the theory and experiments were presented, but raised several concerns. The authors provided succinct responses that seem to have satisfied the reviewers on average.  Unfortunately, after having carefully read this paper (and the authors  responses), I have to go against the wishes of the majority of the reviewers, and recommend a reject. My two main concerns are as follows: a) The synthetic pole dataset, as well as the COIL 100 dataset, are far too simplistic to evaluate performance. It is now standard to report results on considerably more challenging datasets. b) Echoing R2   the authors should articulate why a shaped latent space should actually matter in applications, beyond giving intuitive(I guess?) visualizations and reconstruction error curves. Results on downstream tasks may be one avenue to achieve this.
This paper proposes an online meta learning algorithm. 3 out of 4 reviews were borderline. The main concern during the discussion was that it is unclear what kind of online learning this paper does. For instance, in theory, the online learner competes with the best solution in hindsight. This is a regret minimizing point of view. The other online learning is streaming. In this case, there is no regret. The goal is a sublinear representation that is competitive with some baseline that uses all space.  After the discussion, I read the paper to understand the points raised by the reviewers. I agree that this paper is not ready to be accepted. My quick review is below:  The authors combine MAML and BOL to have online updates (not all tasks are required beforehand) and handle distribution shift. But the way of combining these is not well justified. In particular,  1) The distribution shift story is not convincing. The reason is that the proposed algorithm is posterior based. By definition, when you use posteriors as in (3) (5), you assume that the datasets are sampled i.i.d. given \theta. This means independently and identically. So no distribution shift. I am familiar with Kalman filtering. For that, you need p(\theta_t | \theta_{t   1}) in (3) (5), which would be sufficient for tracking stochastic distribution shifts.  2) I find the use of BOL unnatural. Since MAML is gradient based, it would be more natural to have a gradient based online learner. Gradient descent has online guarantees and does not require i.i.d. assumptions.  3) The authors should clearly state what the objective of their online algorithm is. In particular, the informal justification of (3) (5) as doing something similar to MAML (the paragraph around (6)) is highly confusing. I could not understand what the authors mean.
This paper proposes a technique to perform data imputation with normalizing flow defining a joint density between observed and unobserved variables. This is achieved by introducing a variational posterior over the missing variables which is parametrized in terms of the original model by using the Schur complement of the model s Jacobian over the hidden variables. The idea is interesting, but the proposed setup is quite complex and the experimental results are not conclusive. The quality of the results shown can likely be matched or surpassed with much simpler techniques. The paper would substantially benefit from more detailed experiments.
This is a meta learning approach to model compression which trains 2 policies using RL to reduce the capacity (computational cost) of a trained network while maintaining performance, such that it can be effectively transferred to a smaller student network. The approach has similarities to recently proposed methods for architecture search, but is significantly different. The paper is well written and the experiments are clear and convincing. One of the reviews was unacceptable; I am not considering it (R1).
While reviewers believe that the motivation of the paper is strong and the idea is interesting the ultimate execution of the paper is not up to the standards of ICLR. I believe the biggest concern is the precise privacy guarantee of the method. As pointed out, it is an extremely strong assumption that the model structure of the adversary is known (or even approximately known). Standard privacy guarantees are either information theoretic or based in computational hardness. This work does not provide such guarantees. While there has been recent work on using adversarial learning to learn models that are robust to such adversaries, they have been heavily criticized within privacy and security communities due to the lack of such guarantees. I was not convinced by the authors response to such questions: there are plenty of cryptographic/privacy preserving schemes that work in the honest but curious setting, and techniques that use the standard guarantee of differential privacy do not suffer from large slow downs.  Thus, I would urge the authors to modify this work so that it can leverage the guarantees of well known cryptographic/privacy preserving schemes. If done so, these arguments about privacy will go away and the paper will have a much better shot at acceptance.
The paper shows limitations on the types of functions that can be represented by deep skinny networks for certain classes of activation functions, independently of the number of layers. With many other works discussing capabilities but not limitations, the paper contributes to a relatively underexplored topic.   The settings capture a large family of activation functions, but exclude others, such as polynomial activations, for which the considered type of obstructions would not apply. Also a concern is raised about it not being clear how this theoretical result can shed insight on the empirical study of neural networks.   The authors have responded to some of the comments of the reviewers, but not to all comments, in particular comments of reviewer 1, who s positive review is conditional on the authors addressing some points.   The reviewers are all confident and are moderately positive, positive, or very positive about this paper. 
Overall I agree with the assessment of R1 that the paper touches on many interesting issues (deep learning for time series, privacy respecting ML, simulated to real world adaptation) but does not make a strong contribution to any of these. Especially with respect to the privacy respecting aspect, there needs to be more analysis showing that the generative procedure does not leak private information (noting R1 and R3’s comments). I appreciate the authors clarifying the focus of the work, and revising the manuscript to respond to the reviews. Overall it’s a good paper on an important topic but I think there are too many issues outstanding for accept at this point.
The reviewers agreed that this paper tackles an important problem, continual learning, with a method that is well motivated and interesting. The rebuttal was very helpful in terms of relating to other work. However, the empirical evaluation, while good, could be improved. In particular, it is not clear based on the evaluation to what extent more interesting continual learning problems can be tackled. We encourage the authors to continue pursuing this work.
This paper addresses the scale issue in Graph Neural Networks by proposing a “condensation” approach that produces a small synthetic graph from a large original graph such that GNNs trained on both graphs have comparable performance.   Reviewer cTj2 had concerns with novelty: they claimed the proposed method was close to gradient matching. However, they admitted that the graph setting was new. They suggested some clarity and experimental improvements.   Reviewer R5cV made a similar comment w.r.t. the similarity to gradient matching. Though overall they were more positive than R5cV and thought the idea was interesting and results were compelling.  Reviewer XqrK like the others, argued that the paper “lacked technical innovation in terms of technical contribution”. They pressed for a complexity or runtime analysis.   Reviewer peGb found the idea and problem “intriguing” though felt the solution fell short. They offered many suggestions for improving the quality of the experiments and the analysis.   In the discussion period, reviewer peGb raised their score, thanking the authors for answering their questions. They felt that the additional experiment for NAS was relevant and cleared up a key doubt. Reviewer cTj2 updated their score as well but stated it was critical that the author release the code for reproducing the new experimental results. I think that with most reviewers now on the “accept” side of the fence, I am more inclined to recommend acceptance because I do not see any critical flaws. I think that cTj2’s request for code is reasonable and strongly suggest that the authors do so.
This paper presents a dataset for measuring disentanglement in learned representations. It consists of MNIST digits, sometimes transformed in various ways, and labeled with a variety of attributes. This dataset is used to measure statistics of various learned models.  Measuring disentanglement is certainly an important problem in our field. This dataset seems to be well designed, and I would recommend its use for papers studying disentanglement. The experiments are well designed. While the reviewers seem bothered by the fact that it s limited to MNIST, this doesn t strike me as a problem. We continue to learn a lot from MNIST, even today.  But producing a useful dataset isn t by itself a significant enough research contribution for an ICLR paper. I d recommend publication if (a) it were very different from currently existing datasets, (b) constructing it required overcoming significant technical obstacles, or (c) the dataset led to particularly interesting findings.  Regarding (a), there are already datasets of similar complexity which have ground truth attributes useful for measuring disentanglement, such as dSprites and 3D Faces. Regarding (b), the construction seems technically straightforward. Regarding (c), the experimental findings are plausible and consistent with past findings (which is a good validation of the dataset) but not obviously interesting in their own right.  So overall, this seems like a useful dataset, but I cannot recommend publication at ICLR. 
Post rebuttal, the reviewers all recommend acceptance.
Techniques are introduced for improving representation learning capabilities of neural networks, and the result is interpreted in terms of random projections.  In further discussion, even the reviewer with the highest grade said that the paper does not yet have enough clarity to address the reviewers  comments. Particularly important would be to isolate the causal impact of the proposed components in the final result. But also several technical details would need to be clarified including comparing to simple l2 regularization and precise implications of Fig 1.  Positive aspects: The problem of learning representations and decorrelation is of course important. The authors have imagination, and the authors are encouraged to improve the ideas by taking the reviewer feedback into account. 
While generative model can be used to input data, this work propose to a novel discriminative learning approach to optimize this data imputation phase by deriving a discriminative version of the traditional variational lower bound (ELBO). The resulting bound can be estimated without bias with Monte Carlo estimation leads to a practical approach, leading to encouraging experimental performances.  The reviewers recognised the novelty and suggest that this approach, given its novelty and wide applicability, could be considered for an oral presentation.
The paper addresses the problem of non convex non concave min max optimization under the perspective of application of smoothed algorithms between two opponents. The paper examines a model where the max player applied a zero memory smooth (from differential perspective) algorithm and min player SGD/SNAG or proximal methods providing results similar with the state of art. Convergence guarantees proposed were sound and experimental results on generative adversarial networks and adversarial training demonstrate the efficiency of the proposed algorithms.
In this work, authors study query efficiency in the zeroth order setting of adversarial examples. Reviewers pointed out several weaknesses in the work. They mentioned the paper is not well organized and poorly written, experiments are not comprehensive and the practical significance of the proposed method is unclear. Although reviewers appreciated authors  efforts and responses in the discussion period, they felt that the paper is not above the accept threshold this round and still needs a bit more work.
This paper presents Layerdrop, which is a method for structured dropout which allows you to train one model, and then prune to a desired depth at test time. This is a simple method which is exciting because you can get a smaller, more efficient model at test time for free, as it does not need fine tuning. They show strong results on machine translation, language modelling and a couple of other NLP benchmarks. The reviews are consistently positive, with significant author and reviewer discussion. This is clearly an approach which merits attention, and should be included in ICLR.
This paper proposes to use randomly wired architectures [1] in the context of GNNs and introduces a method for sampling random architectures based on the Erdős–Rényi model. The authors further include a theoretical analysis and two methodological contributions: sequential path embeddings and DropPath, a regularizer. Results are reported on two graph datasets (ZINC and CLUSTER) and on GNN based CIFAR10 image classification.  The reviewers agree that the empirical results presented in the paper are compelling. The value of the contribution largely lies in this aspect, namely the empirical analysis of an existing technique (randomly wired architectures) in the context of GNNs, in addition to several smaller empirical methodological contributions. I agree with the reviewers in that the nature of the contribution and the otherwise limited novelty calls for a more extensive and detailed empirical evaluation (ideally incl. e.g. FLOPS, wall clock time, memory usage) across a wide range of datasets and careful ablation studies, and I encourage the authors to improve on this aspect in a future version of the paper. The theoretical analysis is interesting, but, as pointed out by the reviewers both during the reviews and the later discussion period, does not add sufficient value to the main empirical contribution of the paper to push the paper beyond the acceptance threshold and does not satisfactorily address the question of how the method addresses the oversmoothing problem in GNNs.  [1] Xie et al., Exploring randomly wired neural networks for image recognition (ICCV 2019) 
Reviewers like the simplicity of the approach and the fact that it works well.  
This paper proposes a learning framework for spiking neural networks that exploits the sparsity of the gradient during backpropagation to reduce the computational cost of training. The method is evaluated against prior works that use full precision gradients and shown comparable performance. Overall, the contribution of the paper is solid, and after a constructive rebuttal cycle, all reviewers reached a consensus of weak accept. Therefore, I recommend accepting this submission.
This paper addresses a meta learning method which works for cases where both the distribution and the number of features may vary across tasks. The method is referred to as  distribution embedding network (DEN)  which consists of three building block. While the method seems to be interesting and contains some new ideas, all of reviewers agree that the description for each module in the model is not clear and the architecture design needs further analysis. In addition, experiments are not sufficient to justify the method. Without positive feedback from any of reviewers, I do not have choice but to suggest rejection.  
The paper builds a transition based dependency parser for Amharic, first predicting transitions and then dependency labels. The model is poorly motivated, and poorly described. The experiments have serious problems with their train/test splits and lack of baseline. The reviewers all convincingly argue for reject. The authors have not responded. 
This paper proposed to defend against model stealing attacks by dataset inference. The paper received unanimous rating of "Good paper" and "accept". The reviewers praise this paper insightful and well written. There are active discussion between the reviewers and authors, which further clarify some of the issues. Given the positive review and overall rating, the AC recommends it to be an spotlight paper.
This paper would greatly benefit from some reorganization/rewriting since, as pointed out by some of the reviewers, it’s hard to follow in its current form. While a biologically inspired NAS algorithm could be an interesting direction to explore, the current paper falls short in providing evidence that the approach is well motivated or empirically strong.  In terms of empirics, too many details are missing on the search space/architecture, ablations and comparison with existing methods. For future submissions, it would be particularly useful for the authors to explicitly discuss why they don’t find competing methods applicable to their setting.
The paper considers the problem of using sparse coding to create better generalization in neural networks. The new generalization bound of the neural network only depends on the l1 norm of the weight, instead of the original \ell_2 version as in previous papers.    Well this direction is promising, the major concern about this work is that how they compare with existing generalization bounds empirically. There are definitely some hand crafted instances where this bound excel, but the authors did not provide enough evidence that this bound would actually be better than others for neural networks trained in practice: For example, would adding a relatively large \ell_1 regularizer resulted in a drastic decrement in test accuracy? How does the bound compare with compression based approach such as VC dimension + weight pruning (since the weights are somewhat sparse, so the VC dimension is lower)   One might argue that those pruning techniques do not have theoretical guarantees that they can work   Well this technique does not have theoretical guarantees either (whether this objective can be minimized efficiently): The theorem, at least in the current form, seems to only apply to networks that are the "global optimals" of some non convex training objective (MSE loss involving a non linear neural network +  ell_1 regularizer on its weights). It is also unclear whether such global optimals can be found efficiently in practice. At very least, the authors should devote some effort demonstrating the superiority of their bound empirically.   
This paper proposes a method to use Transformers with tabular data by sharing attention. Reviewers raise significant concerns about the motivation, writing and experimental results. Author s did not submit a response. Hence I recommend rejection.
This paper introduces a new clustering method, which builds upon the work introduced by Lee et al, 2019   contextual information across different dataset samples is gathered with a transformer, and then used to predict the cluster label for a given sample. All reviewers agree the writing should be improved and clarified. The novelty is also on the low side, given the previous work by Lee et al. Experiments should be more convincing. 
The reviewers found the paper to be well written, the work novel and they appreciated the breadth of the empirical evaluation.  However, they did not seem entirely convinced that the improvements over the baseline are statistically significant.  Reviewer 1 has lingering concerns about the experimental conditions and whether propensity score matching within a minibatch would provide a substantial improvement over propensity score matching across the dataset.  Overall the reviewers found this to be a good paper and noted that the discussion was illuminating and demonstrated the merits of this work and interest to the community.  However, no reviewers were prepared to champion the paper and thus it falls just below borderline for acceptance.
The paper shows that deep convolutional neural networks in the kernel regime restructure the eigenspaces of the inducing kernels, which leads to some insights regarding the range of space frequency combinations learned by such networks.  The reviewers identified a number of problems with the current submission. For instance, they found that the paper is hard to follow, it lacks clarity and the theorem statements are hard to understand. The authors also use a somewhat non standard experimental setup.  Despite an extensive discussion with the authors which cleared out a few minor problems, the bulk of the concerns of the reviewers were not successfully adressed. I am therefore not able to recommend acceptance. The authors need to improve the clarity of the paper and provide more discussions of the theorems in a resubmission, as well as potentially reconsider their experimental setup.
**Problem Significance**  This paper introduces an interesting taxonomy of OODs and proposed an integrated approach to detect different types of OODs. The AC agrees on the importance of a fine grained characterization of outliers given the large OOD uncertainty space.   **Technical contribution** The key idea of the paper is to combine the predictions from multiple existing OOD detection methods. While the AC recognizes the effort made by the authors to address the review comments, reviewers have several major standing concerns regarding limited contributions, insufficient analysis, and lack of clarity. The AC agrees with reviewers that the paper is not ready yet for ICLR publication, and can be further strengthened by:    (R1) reporting the computational cost for the integrated approach. The inference time for approaches such as Mahalanobis is typically a few times more expensive than the MSP baseline. The cumulative time for calculating all four scores may be non negligible. Authors are encouraged to analyze the performance tradeoff in a future revision.    (R2 & R3) discussing the effect of hyper parameters tuning, which be overly complicated in practice as it involves combinations of multiple methods that each have multiple parameters to tune.    (R3) comparing with more recent development on OOD detection and move the new results to the main paper. The AC also thinks it s worth discussing the connection and comparison to methods on quantifying uncertainty via Bayesian probabilistic approaches.   (R2 & R4) more rigorous analysis of the benefits of the proposed integrated approach, both empirically and theoretically. Based on Table 7, the performance of using Mahalanobis alone is almost competitive as the proposed approach (except for the CIFAR10 CIFAR100 pair). This may deem further careful examination to understand what value other components are adding, and in what circumstance.    (R2, R3 & R4) More discussion on the implication of the taxonomy and algorithm in the high dimensional space would be valuable. The 2D toy dataset might be too simple to reflect the decision boundary as well as uncertainty space learned by NNs. Moreover, it s important to justify further how aleatoric and epistemic uncertainty manifests in the current experiments using NNs. For example, epistemic uncertainty can arise due to the use of limited samples or due to the model uncertainty associated with the model regularization.   Recent work by Hsu et al. [2] also attempt to define a taxonomy of OOD inputs (based on semantic shift and domain shift), which can be relevant for the authors.   **Recommendation** Three knowledgeable reviewers have indicated rejection. The AC discounted R4 s rating due to the less familiarity in this area and lack of participation in the post rebuttal discussion.   [1] Richard Harang, Ethan M. Rudd. Towards Principled Uncertainty Estimation for Deep Neural Networks [2] Hsu et al. Generalized ODIN: Detecting Out of distribution Image without Learning from Out of distribution Data 
The initial reviews for this paper were 6,6,6, the authors have provided a rebuttal and after the rebuttal the recommendation stayed the same. The reviewers have reached the consensus that the paper is borderline but they have all recommended keeping it above the acceptance threshold. Following the recommendation of the reviewers, the meta reviewer recommends acceptance.
The paper studies the effect of different design choices related to learning a dynamics model. The reviewers uniformly agree that the topic of the paper, systematically studying different design choices, is important. Furthermore, the paper is very well written. However, there are a number of weaknesses as well, that limit the relevance of this work. Arguably, the main weakness is that the results are inconclusive: there is no single design choice that is better, a conclusion that provides little guidance for researchers working in this space. Another weakness is that the study focuses on only 4 domains. And while performing such a study on a much broader set of domains can be prohibitively expensive, that doesn t take away from the fact that it is hard to draw strong conclusions from such a small set of tasks. For these reasons, I recommend rejection.
This paper proposes a meta learning algorithm for reinforcement learning that incorporates expert demonstrations. The objective is to improve sample efficiency, which is an important problem.   The referees find the approach well motivated and pertinent, but the theoretical and practical contributions of the paper too slim. A concern was also raised in regard to reproducibility of the results, missing details about the implementation and comparisons with previous results.   The authors did not respond to the reviews.   The four referees are not convinced by this paper, with ratings from strong reject to ok, but not good enough. 
This paper leverages the piecewise linearity of predictions in ReLU neural networks to encode and learn piecewise constant predictors akin to oblique decision trees. The reviewers think the paper is interesting, and the idea is clever. The paper can be further improved in experiments. This includes comparison to ensembles of traditional trees or (in some cases) simple ReLU networks. Also  the tradeoffs other than accuracy between the method and baselines are also interesting. 
The paper proposes extracting multiple scale features using denoising score matching. Reviewers pointed out the limited novelty in the work and that it does not cite various previous work and how it connects to them.  The paper needs some further  polishing on the writing, and in making the use of lambda divergences more rigorous and principled as explained in the comment of Reviewer VdM1 .
The authors propose a new parameterization which (across multiple architectures) generalized hypercomplex multiplication and provides for small low dimensions strong performance at substantial parameter savings. All reviewers are happy with the theoretical contributions of the work, but would appreciate additional empirical evidence.
Thank you for your submission to ICLR.  This paper had somewhat dissenting reviews, but three of four reviewers felt negatively about the paper.  On the positive side, the reviewers noted good motivation for the problem, a good ablation study and, in some cases, good performance over standard cross entropy.  On the negative side, the reviewers noted limited novelty, missing discussion or comparison to prior work, and in some cases only marginal improvement over existing methods.  The positive reviewer still remained positive after discussion, but noted that their confidence was low on the paper and that they would defer to others  opinions.  The other reviewers still had several concerns after the discussion phase.  Ultimately, it seems that the paper could use some additional work before it is ready for publication.  I would strongly encourage the authors to keep the reviewer comments in mind when preparing a future version of the manuscript.
This is a creative piece of work wherein learning of what is normally family specific Potts models is turned into an amortized optimization problem across different families of proteins. The Potts models are learned with a pseudolikelihood approach, and the evaluation of the model against baselines is performed only on a contact prediction problem. This last point is problematic, because on the one hand, the authors use this "as a proxy for the underlying accuracy of the Potts model learned", and on the other hand, claim that "we do not want to claim our method is state of the art for contact prediction, it is certainly not".  Overall, the paper is promising, but is too preliminary on the empirics to warrant publication at this time.
This paper proposes a new model, the Routing Transformer, which endows self attention with a sparse routing module based on online k means while reducing the overall complexity of attention from O(n^2) to O(n^1.5). The model attained very good performance on WikiText 103 (in terms of perplexity) and similar performance to baselines (published numbers) in two other tasks.  Even though the problem addressed (reducing the quadratic complexity of self attention) is extremely relevant and the proposed approach is very intuitive and interesting, the reviewers raised some concerns, notably:    How efficient is the proposed approach in practice. Even though the theoretical complexity is reduced, more modules were introduced (e.g., forced clustering, mix of local heads and clustering heads, sorting, etc.)   Why is W_R fixed random? Since W_R is orthogonal, it s just a random (generalized) "rotation" (performed on the word embedding space). Does this really provide sensible "routing"?   The experimental section can be improved to better understand the impact of the proposed method. Adding ablations, as suggested by the reviewers, would be an important part of this work.   Not clear why the work needs to be motivated through NMF, since the proposed method uses k means.  Unfortunately several points raised by the reviewers (except R2) were not addressed in the author rebuttal, and therefore it is not clear if some of the raised issues are fixable in camera ready time, which prevents me from recommend this paper to be accepted.  However, I *do* think the proposed approach is very interesting and has great potential, once these points are clarified. The gains obtained in WikiText 103 are promising. Therefore, I strongly encourage the authors to resubmit this paper taking into account the suggestions made by the reviewers. 
This paper investigates the problem of uncertainty calibration under distribution shift. Based on a distributionally robust learning (DRL) framework, the paper estimates the density ratio between the source and target domains to achieve well calibrated predictions under domain shift.  As a plug in module, the proposed method benefits downstream tasks of unsupervised domain adaptation and semi supervised learning in experiments on Office31, Office Home, and VisDA 2017, demonstrating the superiority over empirical risk minimization (ERM) and the temperature scaling method measured by expected calibration error (ECE), Brier Score, and reliability plots.  After extensive interactions and discussions on the paper, the final scores were 6/5/5/5. AC considered the paper itself, and all reviews, author responses, and discussions, and reject the paper from the following concerns: + *Overclaimed Novelty*: This paper is mainly based on the well established competitive distributionally robust learning (DRL) framework. The contribution of the newly proposed regularization form that can further promote smoothed prediction and improve the calibration performance is relatively trivial. The designs of the resulting predictive form and the learning using new gradients, mentioned by the authors in the rebuttal, need further exploration and elaboration to verify its contributions. + *Lack of Clarifications*: Some key points mentioned by several reviewers are still not clear. For example, how well the density ratio is estimated, especially in high dimensions? Further, a positive correlation between HSF and density ratio is not enough to prove the main argument of the paper. + *Some statements are not well supported*: For example, the statement that "the harder the examples are, the farther away the examples are from the source domain", claimed by the author in the rebuttal, is untenable.  In summary, this paper studies a promising research direction of uncertainty estimation, but the work cannot be accepted before addressing the reviewers  comments. I suggest the authors to substantially revise their work by incorporating all rebuttal material as well as addressing the remaining concerns.
This paper tackles the problem of Unsupervised Environment Design to train more robust agents. The proposed method trains RL agents by generating a curriculum of training tasks to enable agents to generalize to many tasks. The key contribution is an algorithm to generate this curriculum by incremental edits of the grid world environments. The reviewers all agreed that the paper is well written and the method is intuitive. However, the weakness of this work is also obvious: the proposed method is only evaluated in grid worlds, and it s unclear how the editing approach can be easily generalized to more complex environments. This submission would benefit from more comprehensive evaluation in non grid world environments, especially given that the compared baselines have results in other environments too.
The authors propose a novel metric to detect distributional discrepancy for text generation models and argue that these can be used to explain the failure of GANs for language generation tasks. The reviewers found significant deficiencies with the paper, including:  1) Numerous grammatical errors and typos, that make it difficult to read the paper.  2) Mischarcterization of prior work on neural language models, and failure to compare with standard distributional discrepancy measures studied in prior work (KL, total variation, Wasserstein etc.). Further, the necessity of the complicated procedure derived by the authors is not well justified.  3) Failure to run experiments on standard banchmarks for image generation (which are much better studied applications of GANs) and confirm the superiority of the proposed metrics relative to standard baselines.   The reviewers were agreed on the rejection decision and the authors did not participate in the rebuttal phase.  I therefore recommend rejection.
This paper discusses an issue with decomposing a conditional generative model into an unconditional model and a separate classifier using Bayes  theorem, which is an approach that has recently received increased attention in the context of score based generative models. It explores several alternatives for mitigating this issue, including a novel one, which is to use a different loss function to train the classifier.  Reviewers praised the writing and the way this work draws attention to an issue that is underappreciated in the community. Although several weaknesses (clarity, scale of experiments, appropriateness of baselines, missing experiments) were also highlighted in the original reviewers, all reviewers agree after discussion that the authors have adequately addressed these for the paper to be considered for acceptance. I will follow their recommendation and recommend acceptance as well.
While there was disagreement on this paper, reviewers remained unconvinced about the scalability and novelty of the presented work. While it was universally agreed that many positive points exist in this paper, it is not yet ready for publication. 
The paper proposes an approach for finding an explainable subset of features by choosing features that simultaneously are: most important for the prediction task, and robust against adversarial perturbation. The paper provides quantitative and qualitative evidence that the proposed method works.  The paper had two reviews (both borderline), and the while the authors responded enthusiastically, the reviewers did not further engage during the discussion period.  The paper has a promising idea, but the presentation and execution in its current form have been found to be not convincing by the reviewers. Unfortunately, the submission as it stands is not yet suitable for ICLR.
While the general idea of the paper is certainly interesting and highly relevant, there is consensus that the paper cannot be published in the current form. There were serious concerns about   the correctness and generality of the method   clarity of presentation   experimental evaluation  The authors graciously accepted the feedback, we wish them all the best in thoroughly revising and resubmitting the paper.
Following a recent line of work on the implicit bias of learning algorithms, the authors consider optimization methods that incorporate momentum. The reviewers found the topic timely and interesting, and generally appreciated the novelty of the technical contributions in the work. However, several critical issues concerning the presentation quality and the positioning of the paper have been raised. In addition, some of the reviewers felt that parts of the paper were somewhat rushed and potentially misleading (mainly, those concerning deterministic\stochastic ADAM and the complexity of the models considered in the paper), and others believed that the experimental section should be made more solid to properly corroborate the theoretical analysis provided in the paper. The authors are encouraged to incorporate the instructive feedback provided by the reviewers in future revisions of the paper.
Authors developed a reparameterization scheme using QR decomposition to reveal symmetries in networks with radial activation. While I welcome new ideas and formalisms from other fields, the ideas presented in this manuscript are fairly straightforward under the radial activation assumption. Although the authors claim that the results may generalize, no evidence was provided. The practical contributions are marginal and lacks comparisons with related DNN compression schemes. Through the review process, the paper has been greatly improved, but unfortunately, this interesting paper does not meet ICLR s standard as is.
The paper proposes a GAN architecture with a ViT based discriminator and a ViT based generator. The paper initially received a mixed rating with two "slightly above the acceptance threshold" ratings and "three slightly below the acceptance threshold" ratings. Several concerns were raised in the reviews, including whether there are advantages of using a ViT based GAN architecture over the CNN based GAN and whether the proposed method can be extended to high resolution image synthesis. These concerns are well addressed in the rebuttal with most of the reviewers increasing their ratings to be above the bar. The meta reviewer agrees with the reviewers  assessments and would like to recommend acceptance of the paper.
Three out of the four reviewers raised various concerns on motivation clarify, result significance, and unclear writing. While the authors provided their rebuttals, unfortunately no reviewer seems to have changed their mind. AC reads the paper and agreed this paper perhaps needs major revision before publishing in a major venue. However, the technical ideas are still interesting and promising; the authors are suggested to carefully take into account reviewer comments during revision.
This paper proposes a new sampling mechanism which uses a self repulsive term to increase the diversity of the samples.  The reviewers had concerns, most of which were addressed in the rebuttal. Unfortunately, none of the reviewers genuinely championed the paper. Since there were a lot of good submissions this year, we had to make decisions on the borderline papers and this lack of full support means that this submission will be rejected.  I highly encourage you to keep updating the manuscript and to rebusmit it to a later conference.
This provides a new method, called DPAutoGAN, for the problem of differentially private synthetic generation. The method uses private auto encoder to reduce the dimension of the data, and apply private GAN on the latent space. The reviewers think that there is not sufficient justification for why this is a good approach for synthetic generation. They also think that the presentation is not ready for publication.
This paper proposes using non Euclidean spaces for GCNs, leveraging the gyrovector space formalism. The model allows products of constant curvature, both positive and negative, generalizing hyperbolic embeddings.   Reviewers got mixed impressions on this paper. Whereas some found its methodology compelling and its empirical evaluation satisfactory, it was generally perceived that this paper will greatly benefit from another round of reviewing. In particular, the authors should improve readability of the main text and provide a more thorough discussion on related recent (and concurrent) work. 
As the reviewers point out, this paper requires a major revision and fleshing out of the claimed contribution before it is suitable for conference presentation.
This paper proposes applying potential flow generators in conjunction with L2 optimal transport regularity to favor solutions that "move" input points as little as possible to output points drawn from the target distribution.  The resulting pipeline can be effective in dealing with, among other things, image to image translation tasks with unpaired data.  Overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms (e.g., GANs, etc.).  After the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance.  With these borderline/mixed scores, this paper was discussed at the meta review level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue.  As one important lingering issue, R1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space.  The rebuttal concedes this point, but suggests that the method still seems to work.  But as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue.  However, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology.  Additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision.  For example, the image to image translation experiments with CelebA were based on a linear (PCA) embedding and feedforward networks.  It would have been nice to have seen a more sophisticated setup for this purpose (as discussed in Section 5), especially for a non theoretical paper with an ostensibly practically relevant algorithmic proposal.  And consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes.
The work presents a novel and effective solution to learning reusable motor skills.  The urgency of this problem and the considerable rebuttal of the authors merits publication of this paper, which is not perfect but needs community attention.
This paper proposes to use evolutionary methods to learn auxiliary loss functions, demonstrating superior performance vs. typical auxiliary losses previously proposed in the RL literature.  Demonstrating that it is possible to learn auxiliary losses by evolution, both for pixel and state representations, that help train significantly faster (even on new environments) is definitely a meaningful contribution, as acknowledge by the majority of reviewers.  Although many of the original reviews  concerns were addressed by the authors during the discussion period, two major ones were only partially answered, both related to the limited empirical evaluation of the proposed approach (which is crucial for such a contribution that aims to demonstrate an improvement over existing related techniques): 1. The limited set of environments used for evaluation (and in particular the lack of partially observable environments) 2. The fact that the baseline being compared to was CURL, which the paper describes as "the state of the art pixel based RL algorithm", while reviewers mentioned DrQ and RAD as two more recent (and better) algorithms that were known well ahead of the ICLR submission deadline (note that the more recent DrQ v2 is now even better). Since the data augmentation techniques used by these algorithms help shape the internal representation, like auxiliary losses do, it would have been important to validate that the proposed technique could be useful when plugged on top of such baselines.  The authors did try their best to address these major concerns during the rebuttal period, but the discussion between reviewers and myself came to the conclusion that this wasn t quite convincing enough yet. I encourage the authors to investigate these points in more depth in a future version of this work so as to make the empirical validation stronger (NB: the links provided in the last comment by authors on Nov. 30th didn t work, but this wasn t the main factor in the decision).
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.     The method and justification are clear   The quantitative results are promising.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The contribution is minor   Analysis of the properties of the method is lacking. The first point was the major factor in the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Reviewer opinion was quite divergent but both AR1 and AR2 had concerns about the 2 weaknesses mentioned in the previous section (which remained after the author rebuttal).   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  No consensus was reached. The source of disagreement was on how to weigh the pros vs the cons. The final decision was aligned with the lower ratings. The AC agrees that the contribution is minor. 
Pros   Nice way to formulate domain adaptation in a Bayesian framework that explains why autoencoder and domain difference losses are useful.  Cons   Model closely follows the framework, but the overall strategy is similar to previous models (but with improved rationale).   Experimental section can be improved. It would interesting to explore and develop the relationship between the proposed technique and Tzeng et al.  Given the aforementioned cons, the AC is recommending that the paper be rejected. 
This paper focuses on scaling up neural theorem provers, a link prediction system that combines backward chaining with neural embedding of facts, but does not scale to most real world knowledge bases. The authors introduce a nearest neighbor search based method to reduce the time/space complexity, along with an attention mechanism that improves the training. With these extensions, they scale NTP to modern benchmarks for the task, including ones that combine text and knowledge bases, thus providing explanations for such models.  The reviewers and the AC note the following as the primary concerns of the paper: (1) the novelty of the contributions is somewhat limited, as nearest neighbor search and attention are both well known strategies, as is embedding text+facts jointly, (2) there are several issues in the evaluation, in particular around analysis of benefits of the proposed work on new datasets. There were a number of other potential weaknesses, such the performance on some benchmarks (Fb15k) and clarity and writing quality of a few sections.  The authors provided significant revisions to the paper that addressed many of the clarity and evaluation concerns, along with providing sufficient comments to better contextualize some of the concerns. However, the concerns with novelty and analysis of the results still hold. Reviewer 3 mentions that it is still unclear in the discussion why the accuracy of the proposed approach matches/outperforms that of NTP, i.e. why is there not a tradeoff. Reviewer 4 also finds the analysis lacking, and feels that the differences between the proposed work and the single link approaches, in terms of where each excels, are described in insufficient detail. Reviewer 4 focused more on the simplicity of the text encoding, which restricts the novelty as more sophisticated text embeddings approaches are commonplace.  Overall, the reviewers raised different concerns, and although all of them appreciated the need for this work and the revisions provided by the authors, ultimately feel that the paper did not quite meet the bar.
The paper proposes methods to deal with estimating classification confidence on unseen data distributions.   The reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) the authors  new comparison with Guo et al. (2017) asked by Reviewer 2 is not convincing enough.  AC thinks the proposed method has potential and is interesting, but decided that the authors need new ideas to meet the high standard of ICLR.
This paper presents an empirical study towards understanding the transferability of robustness (of a deep model against adversarial examples) in the process of transfer learning across different tasks.  The paper received divergent reviews, and an in depth discussion was raised among the reviewers.  + Reviewers generally agree that the paper makes an interesting study to the robust ML community. The paper provides a nice exploration of the hypothesis that robust models learn robust intermediate representations, and leverages this insight to help in transferring robustness without adversarial training on every new target domain.     Reviewers also have concerns that, as an experimental paper, it should perform a larger study on different datasets and transfer problems to eliminate the bias to specific tasks, and explore the behavior when the task relatedness increases or decreases.  AC agrees with the reviewers and encourages the authors to incorporate these constructive suggestions in the revision, in particular, explore more tasks with different task relatedness.  I recommend acceptance, assuming the comments will be fully addressed.
The authors have delivered an extensive examination of deep RL attacks, placing them within a taxonomy, proposing new attacks, and giving empirical evidence to compare the effectiveness of the attacks. The reviewers and AC appreciate the broad effort, comprising 14 different attacks, and the well written taxonomic discussion. However, the reviewers were concerned that the paper had significant problems with clarity of technical presentation and that the attacks were not well grounded in any sort of real world scenario. Although the authors addressed many concerns with their revision and rebuttal, the reviewers were not convinced. The AC believes that R1 ought to have increased their score given their comments and the resulting rebuttal, but the paper remains a borderline reject even with a corrected R1 score.
The paper proposes batch constrained approach to batch RL, where the policy is optimized under the constrain that at a state only actions appearing in the training data are allowed.  An extension to continuous cases is given.  While the paper has some interesting idea and the problem of dealing with extrapolation in RL is important, the approach appears somewhat ad hoc and the contributions limited.  For example, the constraint is based on whether (s,a) is in B, but this condition can be quite delicate in a stochastic problem (seeing a in s *once* may still allow large extrapolation error if that only observed transition is not representative).  Section 4.1 gives some nice insights for the special finite MDP case, but those results are a little weak (requiring strong assumption that may not hold in practice)   an example being the requirement that s  be included in data if (s,a) is in data and P(s |s,a)>0 [beginning of section 4.1].  In contrast, there are other more robust and principled ways, such as counterfactual risk minimization (CRM) for contextual bandits (http://www.jmlr.org/papers/v16/swaminathan15a.html).  For MDPs, the Bayesian version of DQN (the cited Azizzadenesheli et al., as well as Lipton et al. at AAAI 18) can be used to constrain the learned policy as well, with a simple modification of using the CRM idea for bandits.  Would these algorithms be reasonable baselines?
This work provides theoretical analysis for FedAvg, contributing better convergence rates than prior work. Moreover, the paper shows that setting E > 1 can reduce the number of communications.  The contribution is incremental. 
The paper proposes to combine the span level information into a phrase level representation in the fine tuning phrase for pre trained language models.  The phrases are pre defined in a dictionary.  Experiments show improvements in various downstream tasks in the GLUE benchmark.  It s a borderline paper.  Various concerns were raised by the reviewers, for example, the relation with the SpanBERT method in pre training phrase and the significance of the results.  The authors addressed most of the concerns but the reviewers were not fully convinced.  In general, I think it is an interesting paper with good motivation and results.  Hope it can be improved (e.g. more experiments on SpanBERT) and accepted in another conference.
The reviews were a bit mixed, with some concerns on the incremental nature of this work, which the AC concurs (after independently going through both the submission and Xie et al 2020). In a nutshell, the main contribution on the authors  side appears to be a simple linear interpolation of two masks so that it is possible to leverage attacks with varying strengths. Other claimed contributions are not substantiated. In particular:   (a) Fig 1 and its conclusion are a bit disturbing. It is suggested that the authors back up their claim with more empirical and theoretical evidence. For example, why can one conclude from the same mean and variance that there is no distribution mismatch between clean and adversarial examples (as claimed in Xie et al)? If the two sources have similar distribution, why is there a sharp difference in gamma for the two? When one claims different results from previous work, due diligence is required. For instance, did the authors reproduce the mean and variance on the same architecture and dataset of Xie et al? How about other BN layers (in addition to the first one)? How to explain the difference in gamma? The fact that you are using different masks for different epsilon is an indication that their distributions are probably different. The authors mentioned the joint effect between gamma and relu activation, which could be potentially insightful. However, this is a bit speculative in its current presentation. How about an ablation study with leaky relu or tanh/sigmoid? Without these careful comparisons, these claimed contributions are not appropriate to publish in their current form.  (b) As the reviewers pointed out, why BN, after all for models that do not use BN they still suffer from adversarial examples? Even when we restrict to models that use BN, why replicating BN for different sources helps generalization? Here, an excellent experiment point is to compare to fine tuning or replicating other layers in the network. During rebuttal, the authors only tried to fine tune ONE convolution layer and quickly concluded its ineffectiveness. Note that in contrast the authors fine tuned ALL BN layers. How about all convolution layers, some pooling layers, the last softmax layer? These experiments could help us understand if there is some magic in BN. Or maybe it is just more convenient to fine tune BN because of its small number of parameters? In any case these experiments would largely strengthen the findings of this work.  (c) As pointed out by the reviewers, Xie et al hinted at the advantage of using multiple BN masks and the authors proposed to linearly interpolate the masks. In the ablation study, what if we increase the number of BN masks in Xie et al, say we discretize p into 11 values p   {0, 0.1, ..., 0.9, 1} and have 1 mask for each value? Here an interesting experiment is to compare K   11 (basically AdaProp) with smaller K (such as 2 or 5). The authors seemed to suggest that a larger K does not seem to help, which would be clarified through the preceding experiment (and perhaps more). Note that using a uniformly random p is equivalent as adversarial training with a weaker (and varied) attack, and the better tradeoffs shown in the experimental section (e.g. Table 3) are perhaps expected.  (d) As pointed out by the reviewers, a head to head comparison against AdaProp (preferably with more masks) is desirable. The authors mentioned some difficulty in conducting this experiment fully. If it is only the software side, maybe check the sources here:  https://paperswithcode.com/paper/adversarial examples improve image  (e) Finally, a minor point: Algorithm 1 with k 2 and p 1 does not reduce to AdaProp as one will only train on adversarial examples and ignore all clean samples?   In the end this submission appears to be a bit incremental. However, the authors are strongly suggested to follow the reviewers  comments to further polish their work and address the concerns above. With proper revision this work can eventually become a solid contribution on top of AdaProp.
Strengths:  The paper introduces a novel constrained optimization method for RL problems. A lower bound constraint can be imposed on the return (cumulative reward),  while optimizing one or more other costs, such as control effort.  The method learns multiple  The paper is clearly written.  Results are shown on the cart and pole, a humanoid, and a realistic Minitaur  quadruped model.  AC: Being able to learn conditional constraints is an interesting direction.  Weaknesses:  There are often simpler ways to solve the problem of high amplitude, high frequency  controls in the setting of robotics.   The paper removes one hyperparameter (lambda) but then introduces another (beta), although beta is likely easier to tune. The ideas have some strong connections to existing work in  safe reinforcement learning. AC: Video results for the humanoid and cart and pole examples would have been useful to see.  Summary:   The paper makes progress on ideas that are fairly involved to explore and use  (perhaps limiting their use in the short term), but that have potential,  i.e., learning state dependent Lagrange multipliers for constrained RL. The paper is perfectly fine technically, and does break some new ground in putting a particular set of pieces together.  As articulated by two of the reviewers, from a pragmatic perspective, the results are not  yet entirely compelling. I do believe that a better understanding of working with constrained RL, in ways that are somewhat different than those used in Safe RL work.    Given the remaining muted enthusiasm of two of the reviewers, and in the absence of further calibration, the AC leans marginally towards a reject. Current scores: 5,6,7. Again, the paper does have novelty, although it s a pretty intricate setup. The AC would be happy to revisit upon global recalibration. 
The paper presents a computational model for transformer encoders in the form of a programming language (called RASP), shows how to use this language to "program" tasks solvable by transformers, and describes how to use this model to explain known facts about transformer models.  While the reviewers appreciated the novelty of the main idea, the evaluation and the exposition were found to be below the ICLR bar. As a result, the paper cannot be accepted this time around. I urge the authors to prepare a better new version using the feedback from the reviews and discussion.  In particular, the paper would be much stronger with a discussion of how the ideas here can help with improving the transformer model, and whether these ideas generalize to models other than transformers.
The paper presents two new methods for model agnostic interpretation of instance wise feature importance.   Pros: Unlike previous approaches based on the Shapley value, which had an exponential complexity in the number of features, the proposed methods have a linear complexity when the data have a graph structure, which allows approximation based on graph structured factorization. The proposed methods present solid technical novelty to study the important challenge of instance wise, model agnostic, linear complexity interpretation of features.   Cons: All reviewers wanted to see more extensive experimental results. Authors responded with most experiments requested. One issue raised by R3 was the need for comparing the proposed model agnostic methods to existing model specific methods. The proposed linear complexity algorithm relies on the markov assumption, which some reviewers commented to be a potentially invalid assumption to make, but this does not seem to be a deal breaker since it is a relatively common assumption to make when deriving a polynomial complexity approximation algorithm. Overall, the rebuttal addressed the reviewers  concerns well enough, leading to increased scores.  Verdict: Accept. Solid technical novelty with convincing empirical results.
This paper makes a substantial contribution to the understanding of the approximation ability of deep networks in comparison to classical approximation classes, such as polynomials.  Strong results are given that show fundamental advantages for neural network function approximators in the presence of a natural form of latent structure.  The analysis techniques required to achieve these results are novel and worth reporting to the community.  The reviewers are uniformly supportive.
The paper presents an approach to estimate the "effective path" of examples in a network to reach a decision, and consider this to analyze if examples might be adversarial. Reviewers think the paper lacks some clarity and experiments. They point to a confusion between interpretability and adversarial attacks, they ask questions about computational complexity, and point to some unsubstanciated claims. Authors have not responded to reviewers. Overall, I concur with the reviewers to reject the paper.
The paper addresses the problem of solving for the eigenpairs of a self adjoint differential operator. This problem, of course, is classical; the main innovations here are  a) the use a parametric form of the (pointwise) solution using a (shallow) neural network so as to avoid discretization, and  b) obtaining multiple eigenpairs simultaneously as outputs.  The techniques themselves (i.e., the architecture, the loss function, and the training procedure) are fairly standard.  There was some variance in the review scores. The reviewers appreciated the importance of the problem and the direction adopted by the authors. However, concerns were raised regarding: the limitations of the experimental evaluation; a lack of sufficient distinction from prior work; *very* limited comparisons with prior approaches; and the limited demonstration of generalizability. I agree with all these criticisms, and therefore vote to reject.  
This paper considers the problem of distributionally robust fair PCA for binary sensitive variables. The main modeling contribution of the paper is the consideration of fairness and robustness of the PCA simultaneously, and the main technical contribution of the paper is the provision of a Riemannian subgradient descent algorithm for this problem and proof that it reaches local optima of this non convex optimization problem. The results will be of interest to those working at the intersection of fair and robust learning.
The authors propose a method to train a neural network that is robust to visual distortions of the input image. The reviewers agree that the paper lacks justification of the proposed method and experimental evidence of its performance.
Thanks for your detailed responses to the reviewers, which helped us a lot to better understand your paper. However, given that the current manuscript still contains many unclear parts, we decided not to accept the paper. We hope that the reviewers  comments help you improve your paper for potential future submission.
The paper is proposing a new framework for understanding generalization in the deep learning. The main idea is considering the difference of stochastic optimization on a population risk and optimization on an empirical risk. The classical theory considers the difference of empirical risk and population risk. This basically translates the practical motivation from finding good function classes to finding good optimizers which can re use the data effectively. Although the paper provides no theoretical result, it provides an interesting empirical study. The paper somewhat demonstrates that SGD on deep networks is somehow good at re using the same data. I believe this angle is very novel and might hope to future theoretical discoveries. The paper is reviewed by four reviewers and two of them argue its acceptance and two of them argue rejection. After discussion, this status remained and I carefully read and reviewed the paper. Here are the major issues raised by the reviewers:    R#1: The paper is missing a theoretical study. The implications on the practical deep learning is not clear.   R#2: Choice of the soft error is particular to the task and how to go beyond soft max is not clear.   R#3: Finds the paper not novel as well as trivial or hard to understand.   R#4: The choice of soft error is ad hoc.  I believe the issues raised by R#3 are not justified. First of all, novelty is very clear and. appreciated by other reviewers. Moreover, the paper is rather easy to understand and the results are very farm from trivial. However, the other issues raised by other reviewers are valid. Specifically, soft error seems to be a limitation of the study. However, the authors respond to this concern and reviewer increases their score. I believe the theory is lacking but the paper is simply showing this novel approach and its empirical validity. A theory to explain this phenomenon would be amazing but not necessary for publication. Similarly, without a theory it is hard to expect any practical implication. Overall, I believe the paper is an interesting and novel one which will likely to lead additional work in the area. Considering we are still far from a satisfying theory of generalization for deep learning and the role of the optimization is clear, this angle worth sharing with the community. Hence, I decide to accept. However, I have some concerns which should be addressed by the camera ready.     Claims should be revised and authors should make sure they have enough evidence for them. For example, authors provide no satisfying evidence for random labels or very limited evidence for pre training. I strongly recommend authors to either remove some of these discussions or present in a fashion which is not a result but part of the discussion for future research.   A section about limitations should be added. Specifically, the soft error choice should be discussed in this limitation section.   Discussion section should be extended with the pointers to the relevant work on bootstrap literature as well as suggestions to the theoreticians. Not providing any theoretical result is always fine but authors should understanding why is it hard to make theoretical statements and where to search them.
This paper considers options discovery in hierarchical reinforcement learning. It extends the idea of covering options, using the Laplacian of the state space discover a set of options that reduce the upper bound of the environment s cover time, to continuous and large state spaces. An online method is also included, and evaluated on several domains.  The reviewers had major questions on a number of aspects of the paper, including around the novelty of the work which seemed limited, the quantitative results in the ATARI environments, and problems with comparisons to other exploration methods. These were all appropriately dealt with in the rebuttals, leaving this paper worthy of acceptance.
Reviewers are positive overall   the is a general consensus towards acceptance. Reviewers viewed the simplicity, novelty, and effectiveness of the propose pre training approach as strengths. Further, reviewers praised the draft as very clearly written, and viewed experimental ablations as relatively in depth   e.g. two reviewers found the additional analysis of impact of data size to be valuable. A few concerns about additional ablations and claims were brought up, but all were adequately addressed in author response.
This paper argues that NNs deployed to hardware needs to robust to additive noise and introduces two methods to achieve this.  The reviewers liked aspects of the paper and the paper is borderline. However, all in all sufficient reservations were raised to put the paper below the threshold. The criticism was constructive and can be used in an updated version submitted to next conference.  Rejection is recommended.
The paper describes knowledge distillation methods. As noted by all reviewers, the methods are very similar to the prior art, so there is not enough novelty for the paper to be accepted. The reviewers  opinion didn t change after the rebuttal.
This paper proposes one vertex attack for GNN, applied to spatiotemporal forecasting. The paper can be improved w.r.t novelty, incorporating graph topology and rigorous analysis. 
The paper proposes a technique to efficiently retrain a model when a small number of classes are required to be removed.  Reviewers in general like the paper, but the key issue is motivation for the problem. The motivating examples in the rebuttal are not very good because a. authors do not provide any evidence that such situations are critical or commonplace, b. the data points that are available for retraining might be very biased.  A more careful grounding of the work would be important to motivate the ICLR community and the ML community in general to further study this problem. But for now, unfortunately the paper does not seem ready for publication at ICLR.
All reviewers raise issues with the proposed method and whether it is a) applicable to non synthetic tasks/datasets; b) how the input could be broken down into intermediate subproblems in a principled way and whether this would substantially  make the proposal slower than the vanilla encoder/decoder framework; c) awareness of previous work. It is a same the authors did not provide a response, however the reviewers have provided useful feedback they could use to improve their submission.
The paper presents a decomposition of the value function in the context of CCDA.  Most reviewers find this paper clear and well written, although one reviewer suggests to change the paper structure.  The method presented in this paper is simple and well justified by a theoretical section. Experiments on several domains, including Starcraft 2 micro management tasks, are supporting the claims of that section. After some reviewers pointed out that the tabular setup is not useful in practice, the authors have extended the empirical and theoretical results to a more general setup.  Some reviewers point out that some theoretical results may not be directly related to the experimental findings. In particular, reviewer 3 does not support a central claim of the paper, and find that CDM is misleading and not provably representing the core problem. In general, reviewer 3 does not support acceptance of this paper, but I still believe this paper should be accepted based on the other reviews (clearly in favour of acceptance). I hope that the authors and reviewer 3 will be able to further discuss and reach understanding, which hopefully should lead to fruitful results.
This paper combines PEARL with HAC to create a hierarchical meta RL algorithm that operates on goals at the high level and learns low level policies to reach those goals. Reviewers remarked that it’s well presented and well organized, with enough details to be mostly reproducible. In the experiments conducted, it appears to show strong results.  However there was strong consensus on two major weaknesses that render this paper unpublishable in its current form: 1) the continuous control tasks used don’t seem to require hierarchy, and 2) the baselines don’t appear to be appropriate. Reviewers remarked that a vital missing baseline is HER, and that it’s unfair to compare to PEARL, which is a more general meta RL algorithm. The authors don’t appear to have made revisions in response to these concerns.  All reviewers made useful and constructive comments, and I urge the authors to take them into consideration when revising for a future submission.
Strengths: This paper is "thorough and well written", exploring the timbre transfer problem in a novel way. There is a video accompanying the work and some reviewers assessed the quality of the results as being good relative to other approaches. Two of the reviewers were quite positive about the work.  Weaknesses: Reviewer 2 (the lowest scoring reviewer) felt that the paper was a little too far from solving the problem to be of high significance and that there was:    too much focus on STFT vs. CQT    too little focus on getting WaveNet synthesis right    too limited experimental validation (too restricted choice of instruments)    poor resulting audio quality    feels too much of combining black boxes  AMT listening tests were performed, but better baselines could have been used. The author response addressed some of these points.  Contention:  An anonymous commenter noted that the revised manuscript added some names in the acknowledgements, thereby violating double blind review guidelines. However, the aggregated initial scores for this work were past the threshold for acceptance. Reviewer 2 was the most critical of the work but did not engage in dialog or comment on the author response.   Consensus: The two positive reviewers felt that this work is worth of presentation at ICLR. The AC recommends accept as poster unless the PC feel the issue of names in the Acknowledgements in an updated draft is too serious of an issue.  
The paper describes a framework that unifies several previous lines under hindsight information matching.  Within that framework, the paper also describes variants of the decision transformer (DT) called categorical DT and unsupervised DT.  The rebuttal was quite effective and the reviewers confirmed that their concerns are addressed.  The revised version of the paper is significantly improved and consists of an important contribution that should interested many researchers.  Well done!
This paper presents a method for merging a discriminative GMM with an ARD sparsity promoting prior.  This is accomplished by nesting the ARD prior update within a larger EM based routine for handling the GMM, allowing the model to automatically remove redundant components and improve generalization.  The resulting algorithm was deployed on standard benchmark data sets and compared against existing baselines such as logistic regression, RVMs, and SVMs.  Overall, one potential weakness of this paper, which is admittedly somewhat subjective, is that the exhibited novelty of the proposed approach is modest.  Indeed ARD approaches are now widely used in various capacities, and even if some hurdles must be overcome to implement the specific marriage with a discriminative GMM as reported here, at least one reviewer did not feel that this was sufficient to warrant publication.  Other concerns related to the experiments and comparison with existing work.  For example, one reviewer mentioned comparisons with Panousis et al., "Nonparametric Bayesian Deep Networks with Local Competition," ICML 2019 and requested a discussion of differences.  However, the rebuttal merely deferred this consideration to future work and provided no feedback regarding similarities or differences.  In the end, all reviewers recommended rejecting this paper and I did not find any sufficient reason to overrule this consensus.
This paper proposes an identifiable nonlinear ICA model based on volume preserving transformations. The overall approach is very similar to the GIN method published @ ICLR 2020. There is a weak consensus among the reviewers that this paper has some merit, although none pushed for acceptance. After reviewing the paper myself, I agree that the contributions here appear to be incremental, but the results do push this growing field of identifiable latent variable models forward.
Although one review is favorable, it does not make a strong enough case for accepting this paper. Thus there is not sufficient support in the reviews to accept this paper.  I am recommending rejecting this submission for multiple reasons.  Given that this is a "black box" attack formalized as an optimization problem, the method must be compared to other approaches in the large field of derivative free optimization. There are many techniques including: Bayesian optimization, (other) evolutionary algorithms, simulated annealing, Nelder Mead, coordinate descent, etc. Since the method of the paper does not use anything about the structure of the problem it can be applied to other derivative free optimization problems that had the same search constraint. However, the paper does not provide evidence that it has advanced the state of the art in derivative free optimization.  The method the paper describes does not need a new name and is an obvious variation of existing evolutionary algorithms. Someone facing the same problem could easily reinvent the exact method of the paper without reading it and this limits the value of the contribution.  Finally, this paper amounts to breaking already broken defenses, which is not an activity of high value to the community at this stage and also limits the contribution of this work. 
This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally.   The work is well written, and all of the reviewers appreciated the easy to read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.)   However, many of the reviewers also agreed that the theoretical assumptions   and, in particular, the random initialization of the weights   greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. 
This paper proposes Switchable Normalization (SN) that leans how to combine three existing normalization techniques for improved performance. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
The paper proposes intra class clustering as an indicator of generalization performance and validates this by extensive empirical evaluation. All reviewers have found this connection highly interesting. The author response has also duly addressed most of the reviewers  concerns. Given the importance of studying generalization performance of overparameterized deep models, the paper will potentially generate interesting discussion at the conference. 
This work tackles an important clinical application. It is experimentally solid and investigates novel deep learning methodologies in a convincing way.  For these reasons, this work is endorsed for publication at ICLR 2022.
Thank you for submitting your paper to ICLR. The reviewers agree that the idea of sharing the approximating distribution across sets of variables is an interesting one and that the Omniglot experiments are thorough. However, although the authors make the nice addition of some simple examples during the revision period and a new table of quantitative results on Omniglot, the consensus is that the experimental results are not quite persuasive enough for publication. Adding a second dataset, such as mini imagenet or the youtube faces dataset, would make the paper very strong.
This paper presents an ensembling approach to detect underdetermination for extrapolating to test points. The problem domain is interesting and the approach is simple and useful. While reviewers were positive about the work, they raised several points for improvement. The authors are strongly encouraged to include the discussion here in the final version.
This work proposes an approach to encourage within layer diversity in neuron activations, and derive a generalization bound meant to motivate their approach.  None of the reviewers support the acceptance of this work, despite the authors  detailed rebuttals, with the majority of reviewers confirming their preference for rejection following the author response. Many raised concerns regarding the value of the accompanying theory. The empirical results demonstrated by the proposed regularizer were also not judged to be sufficiently compelling to compensate for the shortfall on the theory side.  I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
This paper extends Lowe et al. 2020 to discover causal relations from nonstationary time series by assuming conditionally stationarity of the times series. Based on the assumption, a deep learning method based on VAE is proposed to learn the causal relations from data.  The paper is well motivated and well organized.  However, there are some concerns from the reviewers. 1) The presentation needs significant improvement, e.g., clarification of the notations.  2) The identifiability of the causal graph is not given. It is unclear under what conditions the proposed method can discover the true causal graph. 3) The capacity of the discrete states might not be able to handle complex real situations. 4) The experiments are limited to synthetic and low complexity cases. This further weakens the significance of the proposed method given that there are also no theoretical guarantees of the proposed method. 5) Discussions about some important relevant works are missing.  Overall, the paper studies an interesting problem. However, given the above concerns, the novelty and significance of the paper will degenerate. Both theoretical and empirical analysis of the proposed method need further improvement. Addressing the concerns needs a significant amount of work. Thus, I do not recommend acceptance of this paper.
The paper presents a novel graph convolutional network by integrating the curvature information (based on the concept of Ricci curvature). The key idea is well motivated and the paper is clearly written. Experimental results show that the proposed curvature graph network methods outperform existing graph convolution algorithms. One potential limitation is the computational cost of computing the Ricci curvature, which is discussed in the appendix. Overall, the concept of using curvature in graph convolutional networks seems like a novel and promising idea, and I also recommend acceptance.
This paper studies the role of “noise injection” in GANs with tools from Riemannian geometry, and derives a new noise injection approach that aims to learn a fuzzy coordinate system to model non Euclidean geometry. The new noise injection approach is shown to improve over StyleGANv2 noise injection on lower resolution 128x128 FFHQ, LSUN, and 32x32 CIFAR 10 images.    Some reviewers found the experimental results a “considerable improvement on DCGAN and StyleGANv2” (R3), “extensive and convincing” (R2), while others had concerns around the experimental setup using lower resolution images (R1, R4).  While reviewers were mostly positive about the experimental wins of the paper, there was confusion (R3) and several concerns (R4) around the theory and the relationship between the theory and the practical noise injection algorithm. I additionally had several concerns around the presentation and relation to prior work on generative models. Thus in the current state I cannot recommend this paper for acceptance. Below I highlight concerns that should be addressed in future revisions.   1. My biggest concern is the tremendous gap between the theoretical claims and the practical implementation. When training a GAN with the new form of noise injection, does it learn the skeleton and fuzzy equivalence relationships you claim? This paper is missing any kind of toy experimenting showing that training a GAN with fuzzy reparameterization discovers these relationships or coordinates. Such an experiment would greatly strengthen the paper and help to answer the question of why this new method works (i.e. it’s not just more parameters, a slightly better architecture, or better hyperprameters as mentioned by R3 and R4). There’s also no discussion of what happens theoretically when you have multiple layers of fuzzy reparameterization, and the claims that StyleGAN2’s noise injection limits to Euclidean geometry is false in this case (and thus StyleGAN2’s noise injection can also overcome the “adversarial dimension trap”).   2. Theoretical setting: As mentioned by R4, there is much prior work on the difficulties in fitting a lower dimensional model manifold to a higher dimensional data manifold (e.g. WGAN). Theorem 1 highlights the impossibility of exactly fitting the data manifold with (smooth) neural networks, but the resulting solutions of increasing the dimensionality of the latent space is well known and commonly used (e.g. StyleGAN). This paper also doesn’t discuss the alternative of *approximately* fitting the data manifold with a lower dimensional structure, which is what is often studied in practice.    3. Clarity: The term “noise injection” is overloaded in the literature, and the current presentation of the paper does not sufficiently describe the method. There’s also no discussion of “instance noise” that is another solution to this problem that adds noise to inputs of the discriminator to yield finite f divergences (Sonderby et al., 2016, Roth et al., 2017). The work on instance noise is very related to the approach here, but only adds noise to the output of the generator, not at all levels.  There s also no discussion of how adding noise is just expanding the generative model with additional latent variables, a standard approach that is often discussed in the context of hierarchical generative models. The authors mention the relation to reparameterization trick in VAEs, but argue it is doing something fundamentally different. However, modern VAE architectures (IAF VAE, Very Deep VAE), use a very similar form of modulation at multiple levels in the hierarchy.    4. Experiments: There are no error bars in experimental results, and many results are presented in a new experimental setting defined by the authors (lower resolution than prior work even if using prior code). Rerunning experiments in more standard settings on full resolution images would greatly improve the confidence that the new noise injection strategy is effective. 
This submission proposes a secondary objective when learning language models like BERT that improves the ability of such models to learn entity centric information. This additional objective involves predicting whether an entity has been replaced. Replacement entities are mined using wikidata.  Strengths:  The proposed method is simple and shows significant performance improvements for various tasks including fact completion and question answering.  Weaknesses:  The experimental settings and data splits were not always clear. This was sufficiently addressed in a revised version.  The paper could have probed performance on tasks involving less common entities.  The reviewer consensus was to accept this submission. 
This paper proposes a method to encourage diversity of Bayesian dropout method. A discriminator is used to facilitate diversity, which the method deal with multi modality. Empirical results show good improvement over existing methods. This is a good paper and should be accepted. 
The paper performs an empirical evaluation of deterministic methods for the quantification of epistemic uncertainty.  There is no new algorithm.  The main contribution is the empirical evaluation.  This empirical evaluation will be useful for the community. It is an independent evaluation that casts some doubts on the calibration of several existing deterministic techniques, which will spur additional research. However, the paper is not well written. As pointed out by the reviewers, the paper does not provide much background. It refers to many  concepts without defining them. The concepts are not new (references are provided for each concept), but since the paper does not describe any new technique it should do a good job at explaining those concepts. The authors added some explanations in the supplementary material, but some of those explanations should really be in the main paper.  The most important issue with the paper is that it does not explain why the deterministic techniques do not seem to be well calibrated. The authors added a "theoretical justification" in section 6.1, but it amounts to saying that deterministic methods make a point estimate, which is too general to explain much. An important factor for proper generalization and calibration is the inductive bias of the model. At the end of the day, if we generate data from a model, then that model will be better calibrated than the other models. So a discussion of the inductive bias of each model and how this inductive bias relates to the properties of each dataset would have been much more insightful.
While the paper studies an interesting and important problem, namely the language generation, it is poorly written, which makes it difficult to judge its value. The reviewers also expressed concern over the scope of the evaluation and the lack of comparison to SOTA.
This work starts with a decomposition of the adversarial risk into two terms: the first is the usual risk, while the second is a stability term, that captures the possible effect of an adversarial perturbation. The insight of this work is that this second term can be dealt with using unlabelled data, which is often in plentiful supply. Unfortunately, the same ideas was developed concurrently and independently by several groups of authors.  The reviewer all agreed that this particular version was not ready for publication. In two cases, the authors compared the work unfavorably with concurrent independent work. I will note that the main bound somewhat ignores the issue of overfitting that the second term deals with via the Rademacher bound. Unless one assumes one has unlimited unlabeled data, could one not get an arbitrarily biased view of robustness from the sample. Seems like a gap to fill.
This paper explores the idea that fixational drift of a sensor over an image (something that primate eyes do) could be used to achieve visual hyperacuity, i.e. image recognition with low resolution images equivalent to what would be achieved with high resolution images. The authors construct networks where the bottom of a deep convnet is replaced by recurrent networks and the network is then trained on low resolution versions of high resolution images that are sampled with fixational drift across the image. The authors show that this approach allows their system (dynamical recurrent classifier, or DRC) to get much better classification performance on CIFAR images than can be achieved without the early recurrence and drift. The authors also show that the most robust classification mandates drift trajectories with higher curvature, and they show that this matches some of the properties of visual drift trajectories in humans.   The reviews on this paper were highly divergent (ranging from 3 to 10). Three of the reviewers felt this paper should be rejected, but one felt very strongly it should be accepted. The primary concerns from the negative reviewers were lack of appropriate controls, lack of insight into why the system works, lack of appropriate references to past work, and lack of connection to biology. The authors made a very concerted effort to attend to all of the reviewers  comments. They ran all of the requested control experiments, updated the text to better reflect past literature, and included some comparison to psychophysics data. In the end, only one reviewer increased their score, though, leading to final scores of 3, 10, 5, and 3. Discussion did not lead to any more consensus.   Thus, this paper was still very much in the borderline zone, and required AC consideration. After reading through the paper, reviews, and rebuttals, the AC felt that the authors really had addressed the primary concerns as best as could be hoped for in the time frame for ICLR, and that the paper was sufficiently interesting and informative for ML and neuroscience to be worthy of publication. Some of the negative review points stand, e.g. there are still some mysteries as to why this works and there is certainly a lot more that could be done to make this paper informative for neuroscience. Nonetheless, in total, the AC felt that this paper deserved to be accepted, given that the authors did most of what the reviewers requested of them.
The authors propose stable rank normalization, which minimizes the stable rank of a linear operator and apply this to neural network training. The authors present techniques for performing the normalization efficiently and evaluate it empirically in a range of situations. The only issues raised by reviewers related to the empirical evaluation. The authors addressed these in their revisions. 
This paper is certainly on the way to be a solid contribution: it s an interesting research question, and we need more understanding papers (rather than yet another algorithmic trick paper).  The reviewers thought the paper was not yet ready. The reviewers suggested: (1) more motivation of why the proposed metrics were of interest, (2) clearer discussion and evidence of how the analysis better articulates the performance of PER, (3) missing empirical details like methodology for setting hyper parameters, why these 9 Atari games, undefined errorbars, unspecified number of runs, and (4) conclusions not supported by evidence in Atari: with missing experiment details, likely too few runs, and overlapping errorbars in most games few scientific conclusions can be drawn.  The work might be strengthen by developing the first part of the paper (and focussing on the reviewer s suggestions) and deemphasizing the novel algorithmic contribution part.
Given that the paper proposes a new evaluation scheme for generative models, I agree with the reviewers that it is essential that the paper compare with existing metrics (even if they are imperfect). The choice of datasets was very limited as well, given the nature of the paper. I acknowledge that the authors took care to respond in detail to each of the reviews.
Three reviewers have assessed this submission and were moderately positive about it . However, the reviewers have also raised a number of concerns. Initially, they complained about substandard experimentation which has been resolved to some degree after rebuttal (rev. believe more can be done in terms of unifying them, investigating backbones, attack methods, and experimental settings in light of recent papers).  A somewhat bigger criticism concerns the theoretical part:  1. Rev. remained unclear why using tensor decomposition techniques is a sound approach for designing robust network. 2. AC and rev. also noted during discussions that using low rank constraints (and other mechanisms) and i.e. encouraging smoothness (one important mechanism among many in robustness to attacks) have been extensively investigated in the literature, yet, the proposed idea makes scarce if any theoretical connection to such important theoretical tools.  Some references (not exhaustive) that may help authors further study the above aspects are: Certified Adversarial Robustness via Randomized Smoothing, Cohen et al. Local Gradients Smoothing: Defense against localized adversarial attacks, Naseer et al. Limitations of the Lipschitz constant as adefense against adversarial examples, Huster et al. Learning Low Rank Representations, Huster et al.  On balance, AC feels that despite the enthusiasm, this paper is not ready yet for the publication in ICLR as the key theory behind the proposed idea is missing. Thus, this submission falls marginally short of acceptance in ICLR 2020. However, the authors are encouraged to build up a compelling theory and resubmit to another venue (currently the paper feels like a solid workshop idea that needs to be investigated further).
The paper considers the question of identifying whether a model is bad from an OOD perspective or certifying that it is good. The reviews agree that there are interesting ideas in the paper, however, lack of sufficient experiments and presentation issues were pointed out which make the paper not ready for acceptance at this stage.
This paper proposes an approach for jointly learning a label embedding and prediction network, as a way of taking advantage of relationships between labels.  This general idea is well motivated, but the specifics of the proposed approach are not motivated or described well.  More discussion of relationship with prior work (e.g. other ways of "softening" the softmax) is needed.  The authors claim to have state of the art results, but reviewers point out that much better results exist.
The revisions made by the authors convinced the reviewers to all recommend accepting this paper. Therefore, I am recommending acceptance as well. I believe the revisions were important to make since I concur with several points in the initial reviews about additional baselines. It is all too easy to add confusion to the literature by not including enough experiments. 
This paper is proposed to investigate the robustness of self supervised learning (SSL) and supervised learning (SL) in both balanced (in domain) and imbalanced (out of domain) settings. It can be concluded that SL can regularly learn better representations than SSL, and representations are better from balanced than from imbalanced datasets. The SSL is more robust than SL in the imbalanced settings, which is the crucial of this paper. Expect the experimental results, the authors also provided theoretical analysis to support their claims. The authors also extend a well established method SAM into the Reweighted SAM as the technical contribution to better address the imbalanced setting. The paper is well written with clear logic to follow.
The paper proposes using a set of orthogonal bases that combine to form convolution kernels for CNNs leading to a significant reduction of memory usage. The main concerns raised by the reviewers were 1) clarity; 2) issues with writing and presentation of results; 3) some missing experiments. The authors released a revised version of the paper and a short summary of the enhancements. None of the reviewers changed scores following the author response. The reviews were detailed and came from those familiar with CNNs. I have decided to go with reviewer consensus. 
This paper proposes the notation of DB variability, which is essentially prediction variance. It is also closely related to algorithmic stability which is a theoretically more sound notation to derive generalization bounds. The paper is a mixed bag of empirical observations and "theory". However, looking at the "theoretical results" in the paper, it is clear that the authors lack adequate theoretical background.   I d be more positive if the paper has been focused more on the former, and could be judged by the empirical part only. While the reviews were positive, I looked at them and realized that similar to the authors, the reviewers also lack theoretical backgrounds.   First, the fact large variance implies a generalization lower bound is trivial, to the degree it is not worth stating as a "result". Second small variance implies a generalization upper bound isn t true. One can have a predictor that perfectly overfits the training data and predicts class 0 everywhere else. This has small prediction variance but poor generalization. In this context, the upper bound analysis of the paper is clearly misleading. Usually one compares training error to generalization error, where the estimator depends on the training set. In such case, one cannot use the simple argument of the convergence of the empirical mean of sum of independent random variables to the mean due to the dependency of estimator on the training set, e.g. in Thm 3. One needs to use uniform convergence and exponential probability (instead of Chebyshev) inequality to obtain such results. The right hand side of Thm 3 (the theorem itself is also very poorly stated. and shouldn t be allowed to be published) could not be interpreted as training error as should usually be the case for such bounds, but only as validation error. Such a result (comparing validation and test error when distribution isn t changed) has no value.   I would not elaborate on other similar issues.  My recommendation is to focus on the empirical study if the authors are not familiar with theoretical analysis.
All four reviewers expressed very significant and consistent concerns on this submission during review. No reviewer is willing to support this submission during discussion. It is clear this submission does not make the bar of ICLR.
The paper proposes an unsupervised representation (embedding) learning method for time series. Overall, the paper is well motivated, well written and easy to follow. As agreed by all reviewers, the idea is interesting. To further improve the paper, the authors are encouraged to justify the choice of encoder architectures and window size, and describe more clearly how the statistical test is incorporated.
This work proposes a novel Transformer Control Flow model and achieves near perfect accuracy on length generalization, simple arithmetic tasks, and computational depth generalization.  All reviewers give positive scores. AE agrees that this work is very interesting and has many potentials. It would be exciting if the author could extend this framework to more challenging tasks (e.g. visual reasoning [1. 2]).  Given the novelty of the proposed model, AC recommends accepting this paper!  [1]  CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. ICCV 2017.  [2] PTR: A Benchmark for Part based Conceptual, Relational, and Physical Reasoning, NeurIPS 21
We thank the authors for their detailed responses and the revised version, which addresses several of the questions raised by the reviewers.  The paper is correct and clearly written. All reviewers agree that the idea to add structural features in the message passing of graph neural networks is sensible. While different from previous work, the novelty is a bit incremental though, particularly given the previous work on colored graph neural network. The significance of the work is weak, given 1) the need to select "by hand" structural features that are passed as information, 2) the increased time complexity to compute the structural features compared to other GCNN, and 3) the experimental results that suggest that the benefit of the new approach is limited, particularly on challenging task.  To summarize, this is not a bad paper, but we consider it below the standard of ICLR in terms of originality and significance.
This paper proposes a regularization scheme for reducing meta overfitting. After the rebuttal period, the reviewers all still had concerns about the significance of the paper s contributions and the thoroughness of the empirical study. As such, this paper isn t ready for publication at ICLR. See the reviewer s comments for detailed feedback on how to improve the paper. 
This paper studies the following broad question: How can we predict model performance when the data comes from different sources? The reviewers agreed that the direction studied is very interesting. While the results presented in this work are promising, several reviewers pointed out some weaknesses in the paper, including a confusion between absolute loss and excess loss, and the limited scope of the experiments. Overall, this paper does not appear to be ready for publication in its current form. In my personal opinion, if the concerns raised by the reviewers are appropriately addressed, this work could be publishable in a high quality venue.
This paper proposes a kernel diffusion method to improve upon density based clustering methods. The reviewers found the empirical results quite promising and there is consensus that there are some good ideas in this work. However, their criticisms are strikingly consistent that the technical details are lacking and some of the claims are not fully supported, and these criticisms were not found to be fully addressed in the author responses. I agree with the assessment that this is promising in a major revision toward a future submission but it is currently not complete, especially in the theoretical and technical details.
This paper proposes a reinforcement learning approach that better handles sparse reward environments, by using previously experienced roll outs that achieve high reward. The approach is intuitive, and the results in the paper are convincing. The authors addressed nearly all of the reviewer s concerns. The reviewers all agree that the paper should be accepted.
Information bottleneck is a well known principle that is used for clustering, dimensionality reduction, and recently deep learning. It finds a compressed representation of input X while retaining most information on the response Y. This paper addresses an attempt to interpret the meta learning using the information bottleneck. In addition, a GP based meta learning method is also proposed.  The topic itself is interesting without any doubt. However, most of reviewers have serious concerns about this work, which is summarized below. First of all, two components of this paper (IB and GP based meta learning) do not provide a coherent message.  While the IB interpretation is emphasized in the beginning of this paper, the main point seems to that GP based methods can be more data efficient than gradient based meta learning. There does not much point to GP+MAML or IB interpretation of MAML.  Experiments are not strong enough, although a few ones are added during the author responses. During the discussion with reviewers, no one support this work, so I do not have choice but to suggest rejection. 
This paper proposes a tuning strategy for Hamiltonian Monte Carlo (HMC). The proposed algorithm optimizes a modified variational objective over the T step distribution of an HMC chain. The proposed scheme is evaluated experimentally.  All of the reviewers agreed that this is an important problem and that the proposed methods is promising. Unfortunately, reviewers had reservations about the empirical evaluation and the theoretical properties of the scheme. Because the evaluation of the scheme is primarily empirical, I cannot recommend acceptance of the paper in its current form.  I agree with the following specific reviewer concerns. The proposed method does not come with any particular guarantees, and particularly no guarantees regarding the effect of dropping the entropy term and using an SKSD training scheme to compensate. While guarantees are not necessary for publication, the paper should make up for this with comprehensive and convincing experiments. I agree with R1 that more careful ablation studies on toy models are needed, if nothing else to reveal the strengths and weaknesses of the proposed approach. I would also recommend a more careful discussion about the computational cost of this method and how it can be fairly compared to baselines. I don t agree that "deliberately wasteful" experiments reveal much, especially if running more realistic experiments reduces the relative impact of the proposed method.
This paper presents an attention based approach to transfer faster CNNs, which tackles the problem of jointly transferring source knowledge and pruning target CNNs.  Reviewers are unanimously positive on the paper, in terms of a well written paper with a reasonable approach that yields strong empirical performance under the resource constraint.  AC feels that the paper studies an important problem of making transfer learning faster for CNNs, however, the proposed model is a relatively straightforward combination of fine tuning and filter pruning, each having very extensive prior works. Also, AC has very critical comments for improving this paper:    The Attentive Feature Distillation (AFD) module is very similar to DELTA (Li et al. ICLR 2019) and L2T (Jang et al. ICML 2019), significantly weakening the novelty. The empirical evaluation should consider DELTA as baselines, e.g. AFS+DELTA.  I accept this paper, assuming that all comments will be well addressed in the revision.
This paper introduces a set of techniques that can be used to obtain smaller models on downstream tasks, when fine tuning large pre trained models such as BERT. Some reviewers have noted the limited technical novelty of the paper, which can be seen more as a combination of existing methods. This should not be a reason for rejection alone, but unfortunately, the results in the experimental section are also a bit weak (eg. see [1 4]), there are not very insightful analysis and it is hard to compare to existing work. For these reasons, I believe that the paper should be rejected.   [1] DynaBERT: Dynamic BERT with Adaptive Width and Depth  [2] Training with quantization noise for extreme model compression  [3] MobileBERT: a Compact Task Agnostic BERT for Resource Limited Devices  [4] SqueezeBERT: What can computer vision teach NLP about efficient neural networks?
The paper presents a reasonable idea, probably an improved version of method (combination of GAN and SSL for semantic segmentation) over the existing works. Novelty is not ground breaking (e.g., discriminator network taking only pixel labeling predictions, application of self training for semantic segmentation each of this component is not highly novel by itself). It looks like a well engineered model that manages to get a small improvement with a semi supervised learning setting. However, given that the focus of the paper is on semi supervised learning, the improvement from the proposed loss (L_semi) is fairly small (0.4 0.8%).
This  paper is on graph based semi supervised learning where the goal is to develop an approach to jointly the node labeling function together with the edge weights. A natural way to formulate this problem as a bi level optimization problem. However, the authors claim that this approach introduces two main difficulties: (a)  the "upper" objective function is itself the solution to the "lower" optimization problem (Eq. (2)), and (b) optimization is challenging (Eq. (3)). The AC disagrees. Firstly, there is a close connection between the constrained version and the regression version of the problem (e.g., Belkin, Matveeva and Niyogi)   the former is infact a special case of the latter for a certain choice of regularization parameter. The latter reduces to an linear system. The outer problem can be optimized using standard gradient descent using the implicit function theorem trick common in bilevel optimization. Reviewers have also raised concerns about clarity, and experimental support in this paper and comparisons with related work.  
The paper studies how the size of the initialization of neural network weights affects whether the resulting training puts the network in a "kernel regime" or a "rich regime". Using a two layer model they show, theoretically and practically, the transition between kernel and rich regimes. Further experiments are provided for more complex settings.  The scores of the reviewers were widely spread, with a high score (8) from a low confidence reviewer with a very short review. While the authors responded to the reviewer comments, two of the reviewers (importantly including the one recommending reject) did not further engage.  Overall, the paper studies an important problem, and provides insight into how weight initialization size can affect the final network. Unfortunately, there are many strong submissions to ICLR this year, and the submission in its current state is not yet suitable for publication.
The paper is proposing a novel representation of the GradNorm. GradNorm is presented as a Stackelberg game and its theory is used to understand and improve the convergence of the GradNorm. Moreover, in addition to the magnitude normalization, a direction normalization objective is added to the leader and a rotation matrix and a translation is used for this alignment. The paper is reviewed by three knowledgable reviewers and they unanimously agree on the rejection. Here are the major issues raised by the reviewers and the are chair:   The motivation behind the rotation matrix layers is not clear. It should be motivated in more detail and explained better with additional illustrations and analyses.   Empirical study is weak. More state of the art approaches from MTL should be included and more realistic datasets should be included.   The proposed method is not properly explained with respect to existing methods. There are MTL methods beyond GradNorm like PCGrad and MGDA (MTL as MOO). These methods also fix directions. Hence, it is not clear what is the relationship of the proposed method with these ones.  I strongly recommend authors to improve their paper by fixing these major issues and submit to the next venue.
While the reviewers feel there might be some merit to this work,  they find enough ambiguities and inaccuracies that I think this paper would be better served by a resubmission.
I found the setup for this paper a bit contrived. The tool is presented as a code translation tool, but it really functions more as a multi language code search tool. The Idea is that one has a program in language A, and a database that contains the same program in language B, so one can translate from A to B simply by searching for the right program in the database.   When evaluated as a language translation tool, it appears to outperform existing language translation schemes, but this is an unfair comparison, because iPTR is being given a database that contains the exact translation of the program in question. The performance is also compared with code search tools, but these are also apples to oranges comparisons, because the tools in question are operating from very high level queries. A much more comparable baseline would be the Yogo tool  recently published in PLDI (https://dl.acm.org/doi/abs/10.1145/3385412.3386001), or for compiled languages you could compare against statistical similarity tools for binaries (https://dl.acm.org/doi/10.1145/2980983.2908126).   The experiment in the appendix A5 is more fair to standard language translation, and it yields results that are much less impressive. I would be much more comfortable with this paper if it were written around this experiment, or alternatively if it were evaluated against a more comparable approach for semantic code search. 
Summary of reviews and discussions: Reviewers were overwhelmingly negative on this paper due to a variety of factors: unclear writing, heuristic motivation, overpromising in the title while underdelivering on results. Although the authors responded to the reviewers  feedback, and some reviewers increased their score to acknowledge the author s work, they remained unconvinced overall and no reviewer argued strongly for acceptance.
Dear authors,  Thank you for your submission. The reviewers all appreciated the direction of research and the message that GN can be a bad measure of generalization. That said, they all shared concerns regarding the strength of the conclusions that can be drawn from your work.  I encourage you to address their comments and submit a revised version to a later conference.    Reviewer 1 wanted to update their review but couldn t so here is the update:   Some more details on my original concerns  Thank you for your detailed responses. I wanted to add more details to the ones not discussed by other reviewers.    Regarding the speed of computing the gradient norm, I still don t agree that the computation cost is high. Figure 6 in the Backpack paper shows the cost of computing individual gradients at most 4x the cost of a single backprop not 100 1000x. In reference [2] that I gave, there is also a cheaper approximation discussed with computational costs detailed in Appendix B. As long as the computation of gradient norm is comparable with the cost of a single back prop it should be cheap enough to run all your experiments.    Regarding the conclusions in the paper. Thank you for giving more details. Adding those explanations to the paper would help. I personally missed some of those takeaway messages.  Overall, I strongly recommend either strengthening the link between GN and AGN or using better approximations. As well as better discussing the conclusions. Of course in addition to the suggestions by other reviewers.
This paper presents a generative model for geometric graphs.  The main contribution is to separate the representation and generation of geometry from that of graph structure and features.  Based on this idea the authors assembled a set of existing ideas and built an auto encoder style generative model for geometric graphs.  This paper sits on the borderline, with reviewers split on both sides.  I appreciate the clarifications from the authors during the rebuttal and the interactions with the reviewers.  The main concern is the novelty of this approach, as the main contribution is the idea of separating geometry from graph structure, and most other components of the pipeline already exist in the literature.  Because of this I think the paper can probably devote a bit more to this ablation study.  In particular the paper currently lacks detail about whether the size of the models were controlled when doing the ablation, which could be a confounding factor that explains why the joint model with both geometry and graph structure works better.  Also the different architecture choices may also factor into the difference, it would be more convincing if for example the same combination of multi head attention blocks and GINE networks are used for the ablated graph encoder (you can simply concatenate the features from both on all layers, or even at the end).  Based on this I would recommend rejection at this time but encourage the authors to improve the paper and send it to the next venue.
All three reviewers are in agreement that this paper is not ready for ICLR in its current state. Given the pros/cons, the committee feels the paper is not ready for acceptance in its current form.
There is insufficient support to recommend accepting this paper.  The reviewers unanimously recommended rejection, and did not change their recommendation after the author response period.  The technical depth of the paper was criticized, as was the experimental evaluation.  The review comments should help the authors strenghen this work.
The paper proposes a learning by teaching (LBT) framework to train an implicit generative model via an explicit one. It is shown experimentally, that the framework can help to avoid mode collapse. The reviewers commonly raised the question why this is the case, which was answered in the rebuttal by pointing to the differences between the KL  and the JS divergence and by showing a toy problem for which the JS divergence has local minima while the KL divergence has not. However, it still remains unclear why this should be generally and for explicit models with insufficient capacity the case, and if the model will be scalable to larger settings, therefore the paper can not be accepted in the current form.
This paper presents a method for learning more flexible prior distributions for GANs by learning another distribution on top of the latent codes for training examples. It s reminiscent of layerwise training of deep generative models. This seems like a reasonable thing to do, but it s probably not a substantial enough contribution given that similar things have been done for various other generative models. Experiments show improvement in samples compared with a regular GAN, but don t compare against various other techniques that have been proposed for fixing mode dropping. For these reasons, as well as various issues pointed out by the reviewers, I don t recommend acceptance. 
Two reviewers expressed clear concerns about the paper but the authors did not provide any response. 
Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal with one reviewer hesitating about the appropriateness of this submission to ML venues. The reviewers have raised a number of criticisms such as an incremental nature of the paper (HHL and LMR algorithms) and the main contributions lying more within the field of quantum computing than ML. The paper was discussed with reviewers, buddy AC and chairs. On balance, it was concluded that this paper is minimally below the acceptance threshold. We encourage authors to consider all criticism, improve the paper and resubmit to another venue as there is some merit to the proposed idea. 
This paper proposes a new method for domain generalization by adopting a single test example. Authors formulate the problem using a variational bayesian framework which ends up in an adaptation technique requiring a single feed forward computation. The provided empirical results indicate that the proposed method has comparable performance to techniques which require more data.  Reviewers all acknowledge the novelty and significance of this work. The paper is well written and the related work is adequately discussed. Moreover, the proposed method is computationally efficient and empirical results provide strong evidence in its favor. While I am recommending acceptance, I tend to agree with reviewer xA1m about the main weaknesses of this work and I recommend authors to improve them for the final version:    Lack of proper discussion or intuition about under what conditions the proposed method works well. This may be using theoretical analysis, using toy examples, trying to break the method, motivate using prior work or just simply providing intuitive arguments. Also, as reviewers pointed, Figure 1 is currently very confusing.   Lack of analysis or ablation study allows a better understanding of the proposed method
The paper contributes to the theoretical understanding of finite width ReLU networks. It contributes new ideas and constructions to investigate the representational power of such networks. In particular, the analysis works without skip connections. Referees found the paper refreshingly well written and pleasant to read.   There is a concern that the paper may be overstating the novelty and innovation of the results, as some of them are easy implications, and there are other previous works that have obtained results on finite width networks (see AnonReviewer4 s comments).  On the other hand, the authors were careful to cite when they reuse proof techniques from these and other works (AnonReviewer2). Another concern is that the considered target function space might be too narrow (see AnonReviewer2 s comments). The authors clarify that the choice was because the considered classes are known to be hard to approximate and there are no known classical methods that would yield exponential approximation accuracy. Another concern is that the results might not be suitable to ICLR, having an emphasis on approximation theory and less on learning (see AnonReviewer3 s comments).   The reviewers consistently rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below acceptance threshold ratings.   While this appears to be a well written paper with valuable new ideas in regard to the approximation properties of networks, the contributions were not convincing enough. I would suggest that developing a clearer connecting to learning and broader classes of target functions could increase the appeal of the paper. 
This paper describes a non uniformly weighted version of SGMCMC, combining aspects of SG methods and importance sampling. The idea is interesting and novel, but unfortunately the authors have not made a compelling case for the resulting algorithm being a practical addition to the literature. The experimental analysis is not particularly compelling, and there are key concerns raised about practical implementation, and about the validity of the approximations raised. I hope that the authors will continue along this interesting line of work and add additional explorations of the approximations and improved experimental analysis.
This paper presents a biologically plausible architecture and learning algorithm for deep neural networks.  The authors then go on to show that the proposed approach achieves competitive results on the MNIST dataset.  In general, the reviewers found that the paper was well written and the motivation compelling.  However, they were not convinced by the experiments, analysis or comparison to existing literature.  In particular, they did not find MNIST to be a particularly interesting problem and had questions about the novelty of this approach over past literature.  Perhaps the paper would be more impactful and convincing if the authors demonstrated competitive performance on a more challenging problem (e.g. machine translation, speech recognition or imagenet) using a biologically plausible approach. 
 The paper addresses an important problem of detecting biases in classifiers (e.g. in face detection), using simulation tools with Bayesian parameter search. While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (e.g., beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue.  While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range. We hope the suggestions and comments of the reviewers can help to improve the paper.     
This paper proposes an efficient method to compute the singular values of the linear map represented by a convolutional layer. It makes uses of the special block matrix form of convolutional layers to construct their more efficient method. Furthermore, it shows that this method can be used to devise new regularization schemes for DNNs. The reviewers did note that the diversity of the experiments could be improved, and R2 raised concerns that the wrong singular values were being computed. The authors should add a section clarifying why the singular values of a convolutional linear map are not found directly by performing SVD on the reshaped kernel   indeed the number of singular values would be wrong. A contrast with the singular values obtained by simple reshaping of the kernel would also be helpful.
This paper proposed a weight sharing method to speed up the pretraining of large language models. Basically, during the training, it first share weights across all the layers with the same architecture, and then untie the shared weights at some point later. The main advantage of weight sharing is that it can reduce the memory load. Our reviewers have many concerns on this work. The method is not well motivated or explained, and many experimental details are missing. In particular, there is no downstream task result presented for the so called 10T parameter model. The claim highlighted in the title remains unsupported. In addition, one of our reviewers pointed out that the proposed method is fairly similar to the method in a previous ICLR submission: https://openreview.net/forum?id jz7tDvX6XYR.
Overall, this paper receives negative reviews due to limited technical novelty and contributions. The authors  rebuttal does not address all the raised concerns. As such, the area chair agrees with the reviewers and does not recommend it be accepted at this conference
While the updated version of this manuscript did motivate one reviewer to give the paper a marginal accept rating, all other reviewers really felt that the paper could use more work along the lines of their suggestions. The aggregate view of the reviewers is just not positive enough at this time to warrant an accept recommendation by the AC at this time. The work does seem to have promise and the authors are encouraged to continue to improve the paper for another round of peer review elsewhere.
This paper investigates gradient sparsification using top k for distributed training. Starting with empirical studies, the authors propose a distribution for the gradient values, which is used to derive bounds on the top k sparsification. The top k approach is further improved using a procedure that is easier to parallelize.  The reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the rigor and novelty of the results, and perhaps issues with unstated assumptions. In reviews and discussion, the reviewers also noted issues with clarity of the presentation, some of which were corrected after rebuttal. In the opinion of the AC, the manuscript is not appropriate for publication in its current state. 
The authors propose a new perspective on active learning by borrowing concepts from subjective logic. In particular, they model uncertainty as a combination of dissonance and vacuity; two orthogonal forms of uncertainty that may invite additional labels for different reasons. The concepts introduced are not specific to deep learning but are generally applicable. Experiments on 2d data and a couple standard datasets are provided.  The derivation of the model is intuitive but it s not clear that it is "better" than any other intuitively derived model for active learning. With the field of active learning having such a long history, the field has moved towards a standard of expecting theoretical guarantees to distinguish a new method from the rest; this paper provides none. Instead anecdotal examples and small experiments are performed. Like other reviews, I am extremely skeptical about the use of KDE which is known to have essentially no inferential ability in high dimensions (such as in deep learning situations where presumably images are involved). It is hard not to feel as though deep learning is somewhat of a red herring in this paper.   I recommend the authors lean into understanding the method from a perspective beyond anecdotes and experiments if they wish for this method to gain traction. 
The reviewers have reached consensus that while the paper is interesting, it could use more time.  We urge the authors to continue their investigations.
The authors present a method for fine grained entity tagging, which could be useful in certain practical scenarios.  I found the labeling of the CoNLL data with the fine grained entities a bit confusing.  The authors did not talk about the details of how the coarse grained labels were changed to fine grained ones.  This detail is important and is missing from the paper.  Moreover, there are concerns about the novelty of the work, both in terms of the task definition and the model (see the review of Reviewer 1, e.g.).  There is consensus amongst the reviewers, in that, their feedback is lukewarm about the paper.  
The paper proposes adversarial sampling for pool based active learning.  The reviewers and AC note the critical potential weaknesses on experimental results: it is far from being surprising the proposed method is better than random sampling. Ideally, one has to reduce the complexity under keeping the state of art performance. Otherwise, it is hard to claim the proposed method is fundamentally better than prior ones, although their targets might be different.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The authors study the problem of reducing uplink communication costs in training a ML model where the training data is distributed over many clients.   The reviewers consider the problem interesting, but have concerns about the extent of the novelty of the approach.  As the reviewers and authors agree that the paper is an empirical study, and the authors agree that the novelty is in the problem studied and the combination of approaches used, a more thorough experimental analysis would benefit the paper.
The authors give an effective framework PRIME to tackle the challenges of automating hardware design optimization.  This problem is of importance to the community.  Overall, the reviewers thought the paper gave a nice clean approach to the problem and that the community would be interested with these results.
This paper proposes new heuristics to prune and compress neural networks. The paper is well organized. However, reviewers are concerned that the novelty is relatively limited. The advantage of the proposed method is marginal on ImageNet. What is effective is not very clear. Therefore, recommend for rejection. 
This paper provides a theoretically rigorous treatment of approximation properties and convergence analysis of LINEAR RNNs. The reviewers were divided in their evaluation. On the positive side, the presented relation between approximation error and required memory size is not obvious and interesting. On the less positive side, two of the reviewers raised the necessity of mathematical machinery that were invoked. Furthermore, its applicability is unclear in ML, since they aren t applicable to the usual nonlinear RNNs. However, given that the theoretical contributions are clear, the final decision was to accept.
This paper was pretty borderline, but ultimately I am recommending rejection, for the following reason:  The two most negative reviewers (in terms of original score) were concerned about both the quality of the evaluation and whether the evaluation metrics actually meant what the paper claimed they meant.  The authors did make a good faith effort to update the paper to respond to some of the concerns about quality,  but R4 (who has read the rebuttal) is still not convinced that the results of the evaluation are meaningful, and I think I agree with their concern (and I don t see any attempt to address that concern in the rebuttal?).  I view (maybe naively) the goal of this review process as being mostly a correctness check,  and I don t think this paper has passed the correctness check to my satisfaction or to the satisfaction of the majority of reviewers.   However! This is a fixable issue. The paper is definitely cool and interesting, and I would urge the authors to think harder about what sort of evaluation makes sense here and resubmit to the next suitable machine learning conference.
This paper explores the training of CNNs which have reduced precision activations. By widening layers, it shows less of an accuracy hit on ILSVRC 12 compared to other recent reduced precision networks. R1 was extremely positive on the paper, impressed by its readability and the quality of comparison to previous approaches (noting that results with 2 bit activations and 4 bit weights matched FP baselines). This seems very significant to me. R1 also pointed out that the technique used the same hyperparameters as the original training scheme, improving reproducibility/accessibility. R1 asked about application to MobileNets, and the authors reported some early results showing that the technique also worked with smaller network/architectures designed for low memory hardware. R2 was less positive on the paper, with the main criticism being that the overall technical contribution of the paper was limited. They also were concerned that the paper seemed to motivate based on reducing memory footprint, but the results were focused on reducing computation. R3 liked the simplicity of the idea and comprehensiveness of the results. Like R2, they thought the paper was limited novelty. In their response to R3, the authors defended the novelty of the paper. I tend to side with the authors that very few papers target quantization at no accuracy loss. Moreover, the paper targets training, which also receives much less attention in the model compression / reduced precision literature. Is the architecture really novel? No. But does the experimental work investigate an important tradeoff? Yes.
The reviewers are reasonably positive about this submission although two of them feel the paper is below acceptance threshold. AR1 advocates large scale experiments on ILSVRC2012/Cifar10/Cifar100 and so on. AR3 would like to see more comparisons to similar works and feels that the idea is not that significant. AR2 finds evaluations flawed. On balance, the reviewers find numerous flaws in experimentation that need to be improved.   Additionally, AC is aware that approaches such as  Convolutional Kernel Networks  by J. Mairal et al. derive a pooling layer which, by its motivation and design, obeys the sampling theorem to attain anti aliasing. Essentially, for pooling, they obtain a convolution of feature maps with an appropriate Gaussian prior to sampling. Thus, on balance, the idea proposed in this ICLR submission may sound novel but it is not. Ideas such as  blurring before downsampling  or  low pass filter kernels  applied here are simply special cases of anti aliasing. The authors may also want to read about aliasing in  Invariance, Stability, and Complexity of Deep Convolutional Representations  to see how to prevent aliasing. On balance, the theory behind this problem is mostly solved even if standard networks overlook this mechanism. Note also that there exist a fundamental trade off between shift invariance plus anti aliasing (stability) and performance; this being a reason why max pooling is still preferred over anti aliasing (better performance versus stability). Though, this is nothing new for those who delve into more theoretical papers on CNNs: this is an invite for the authors to go thoroughly first through the relevant literature/numerous prior works on this topic.
 This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage. While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing.
This paper studies the problem of choosing the best cloud provider for a task. The problem is formulated as a bandit and solved using algorithm CloudBandit. The algorithm is compared to several baselines, such as SMAC, and performs well. The evaluation is done on 60 different multi cloud configuration tasks across 3 public cloud providers, which the authors want to share with the public.  This paper has four borderline reject reviews. All reviewers agree that it studies an important problem and that the promised multi cloud optimization dataset could spark more research in the area of cloud optimization. The weaknesses of the paper are that it is not technically strong and that the quality of the new dataset is not clear from its description. At the end, the scores of this paper are not good enough for acceptance. Therefore, it is rejected.
This paper presents an analysis of using multiple generators in a GAN setup, to address the mode collapse problem. R1 was generally positive about the paper, raising the concern on how to choose the number of generators, and also whether parameter sharing was essential. The authors reported back on parameter sharing, showing its benefits yet did not have any principled method of selecting the number of generators. R2 was less positive about the paper, pointing out that mixture GANs and multiple generators have been tried before. They also raised concern with the (flawed) Inception score as the basis for comparison. R2 also pointed out that fixing the mixing proportions to uniform was an unrealistic assumption. The authors responded to these claims, clarifying the differences between this paper and the previous mixture GAN/multiple generator papers, and reporting FID scores. R3 was generally positive, also citing some novelty concerns similar to that of R2. I acknowledge the authors detailed responses to the reviews (in particular in response to R2) and I believe that the majority of concerns expressed have now been addressed. I also encourage the authors to include the FID scores in the final version of the paper.
This paper proposes monotonic graph neural networks (MGNNs) for the transformation of knowledge graphs. Specifically, MGNNs transform a knowledge graph into a colored graph where each node is represented by a numeric feature vector and each edge encodes the node relationship with different colors. The authors provide theoretical analysis showing that monotonic constraint can enable the model to derive logical inference rules in Datalog, and thus the trained model is explainable.    The authors addressed most of the concerns raised by the reviewers, such as motivation, runtime, and comparison with existing baselines. Three of the four reviewers are positive (with the scores of 6 or above) towards acceptance after rebuttal discussions, and the remaining reviewer gives a score of 5 (below acceptance threshold) thinks that this work still lacks novelty, but he/she is not against acceptance if other reviewers choose so. Considering this work makes a good exploration on explainable graph neural networks, which is an interesting and important research direction, we recommend for acceptance. We thank the reviewers and the authors for their active discussion.
This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto encoder framework. Significant gains are found through semi supervised learning.  The largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over stating the perceived utility.  Overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.
AR1 is concerned about lack of downstream applications which show that higher order interactions are useful and asks why not to model higher order interactions for all (a,b) pairs. AR2 notes that this submission is a further development of Arora et al. and is satisfied with the paper. AR3 is the most critical regarding lack of explanations, e.g. why linear addition of two word embeddings is bad and why the corrective term proposed here is a good idea. The authors suggest that linear addition is insufficient when final meaning differs from the individual meanings and show tome quantitative results to back up their corrective term.  On balance, all reviewers find the theoretical contributions sufficient which warrants an accept. The authors are asked to honestly reflect all uncertain aspects of their work in the final draft to reflect legitimate concerns of reviewers.
As the reviewers point out, the paper is below the acceptance standard of ICLR due to low novelty, unclear presentation, and lack of experimental comparison against the state of the art baselines.
The paper addresses the problem of large scale fine grained classification by estimating pairwise potentials in a CRF model. The reviewers believe that the paper has some weaknesses including (1) the motivation for approximate learning is not clear (2) the approximate objective is not well studied and (3) the experiments are not convincing. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account to improve the paper. 
All the reviewers unanimously agree that the paper should be rejected. The main concern is well summarized by comment by R1 s comment "While the problem is interesting, I found the paper difficult to read as the task is ill defined in section 3 where many notation definitions are missing and some notations are reused in different contexts with different definitions". Also, as R4 mentions the proposed method can be reduced to reward engineering and doesn t provide any scientific or methodological advancement to the problem of testing hypothesis. The authors did not provide any rebuttal. 
This paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low order and high order interaction patterns are easy to be learned, while middle order interaction patterns are difficult to be learned. They also propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle order interaction. All reviews are positive.
After revision, all reviewers agree that this paper makes an interesting contribution to ICLR by proposing a new methodology for unbalanced optimal transport using GANs and should be accepted.
The authors explore the forgetting behavior of large scale pre trained models in continual learning (CL). The authors find that forgetting is mitigated by scale (or models and of pre training datasets). The authors also make preliminary observations to try to explain why this happens. This manuscript is somewhat in line with recent results around scaling laws and in a sense, this paper extends the study of scaling laws to the CL setup with a focus on catastrophic forgetting, traditionally the main desiderata of CL.  The initial reviewer assessments indicated that this paper was likely below the acceptance threshold of the conference. The main perceived limitations were:  + CL experiments on sequences of two tasks are limiting (in CL it s common to look at sequences of at least 10 tasks) + The effect of the pre training data on the results was not properly assessed. In particular, it is likely that the pre training data and downstream/test data were very similar. + Other aspects such as missing hyperparameter values and empirical settings were also raised.  The authors really came through and obtained lots of additional empirical results. In particular, the authors show that their results mostly hold on longer task sequences and even if the downstream task was very different from the pre training tasks. Further, the authors provided precise answers to all the reviewer comments and also ran a few more studies to answer more specific reviewer questions (including a few to answer some excellent suggestions from reviewer ZtJq).   Overall, this is a good contribution and I imagine one that could have a significant impact in the field and give rise to follow up work. Congratulations!   In preparing the final version of the manuscript, I would strongly suggest that the authors incorporate all results discussed in their replies in their paper. Further, and as was suggested by reviewer Dwjt, I think it would be very useful to add the study of the longer task sequences and of the different downstream tasks to the main paper and not the appendix as is done currently.
The reviewers were quite unanimous in their assessment of this paper.  PROS: 1. The paper is relatively clear and the approach makes sense 2. The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks. 3. Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation. 4. The multi stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.  CONS 1. Lack of novelty e.g. wrt to Finn et al. in "Deep Spatial Autoencoders for Visuomotor Learning" 2. The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem. 3. The contribution on reward shaping would benefit from a more detailed description and investigation. 4. There is concern that results may be specific to the chosen task. 5. Experiments using real robots are needed for practical evaluation.
The rebuttal (revisions, and released code) very successfully addressed all the major concerns the reviewers had.  Pros: The dynamics distance function is a very neat, simple (which is good in this case) idea that is theoretically sound, has proven to perform well in thorough experimental results, and that can be broadly applied.  Cons: None
Thank you for submitting you paper to ICLR. The reviewers agree that the paper’s development of action dependent baselines for reducing variance in policy gradient is a strong contribution and that the use of Stein s identity to provide a principled way to think about control variates is sensible. The revision clarified an number of the reviewers’ questions and the resulting paper is suitable for publication in ICLR.
This paper looks at a natural application of robust learning for vehicle routing. The paper introduces some new ideas for this RL problem; although the problem has been considered before.  The paper gives a nice algorithm with extensive experimental contributions.    The paper has some shortcomings. The reviewers found there to be a lack of clarity in the mathematical definitions.  Moreover, there were modeling choices that the reviewers felt needed more thorough explanation.   For these reasons, this paper falls below the bar.  The authors are encouraged to revise the manuscript taking these concerns into consideration.  
The paper proposes a recurrent and autorgressive architecture to model temporal knowledge graphs and perform multi time step inference in the form of future link prediction. However, the reviewers feel that the papers are more of a straight application of current techniques. Furthermore, a better presentation of the experimental section will also help improve the paper.  
This paper introduces a new approach that consists of the invertible autoencoder and a reversible predictive module (RPM) for video future frame prediction.  Reviewers agree that the paper is well written and the contributions are clear. It achieves new state of the art results on a diverse set of video prediction datasets and with techniques that enable more efficient computation and memory footprint. Also, the video representation learned in a self supervised way by the approach can have good generalization ability on downstream tasks such as object detection. The concerns of the paper were relatively minor, and were successfully addressed in the rebuttal.  AC feels that this work makes a solid contribution with well designed model and strong empirical performance, which will attain wide interests in the area of video future frame prediction and self supervised video representation learning.  Hence, I recommend accepting this paper.
This paper suggests that datasets have a strong influence on the effects of attention in graph neural networks and explores the possibility of transferring attention for graph sparsification, suggesting that attention based sparsification retains enough information to obtain good performance while reducing computational and storage costs.   Unfortunately I cannot recommend acceptance for this paper in its present form. Some concerns raised by the reviewers are: the analysis lacks theoretical insights and does not seem to be very useful in practice; the proposed method for graph sparsification lacks novelty; the experiments are not thorough to validate its usefulness. I encourage the authors to address these concerns in an eventual resubmission. 
The paper deal with a mutual information based dependency test.   The reviewers have provided extensive and constructive feedback on the paper. The authors have in turn given detailed response withsome new experiments and plans for improvement.   Overall the reviewers are not convinced the paper is ready for publication.  
The reviewers agree that the paper studies an important and interesting problem and presents a good solution which is theoretically sound. The paper can be further improved by looking into more applications such as cold start recommendations.
This paper proposes a novel coding scheme for compressing neural network weights using Shannon style coding and a variational distribution over weights.  This approach is shown to improve over existing schemes for LeNet 5 on MNIST and VGG 16 on CIFAR 10, strictly dominating them in terms of compression/error rate tradeoffs. Comparing to more baselines would have been helpful. Theoretical analysis based on non trivial extensions of prior work by Harsha et al. (2010) and  Chatterjee & Diaconis (2018) is also presented. Overall, there was consensus among the reviewers that the paper makes a solid contribution and should be published. 
This paper proposes an architecture for learned surface parameterization, with application to image unwarping, which can be coupled with differentiable rendering, multi view data, and other modern objective terms.  The shape of the document is parameterized using an SDF technique, coupled with neural rendering and objective terms inspired by classical geometry processing.  This machinery is quite "heavy," leading to slow training times.  As pointed out by reviewer QH85, there were some experimental discrepancies rightfully acknowledged by the authors which make comparisons to DewarpNet less favorable for the new method, at least from a quantitative perspective.  Visual inspection makes the comparison more favorable, although it would be preferable for the quantitative quality metrics and qualitative examples to align.    Runtime measurements here are also not favorable and severely limit applicability of this technique in real world scenarios, as pointed out by reviewers hfPz and QH85.  While the mistaken quantitative results are forgivable, the AC agrees that the scope of this work is quite narrow; it is not clear where this architecture would be applied relative to the motivating application.
This paper makes significant advances in offline reinforcement learning by proposing a new approach of being pessimistic to deal with uncertainties in the offline data.  The proposed approach uses bootstrapped Q functions to quantify the uncertainty, which by itself is not new, and introduces additional data based on the pseudo target that is penalized by the uncertainty quantification.  The use of such additional data is the first of a kind, and the paper provides theoretical support for the case of linear MDP and empirical support with the D4RL benchmark.  The reviewers had originally raised concerns or confusions regarding theoretical analysis and experiments.  The authors have well responded to them, and no major concerns remain.
This paper describes Flashlight, a tool for ML researchers with specific design considerations for conducting systems research. The needs for such tool are significant, and recent advances in this topic have been relatively slow, so this research is timely and important.   Reviewers are positive about the importance of the problem and the nice design of Flashlight. It seems the tool has been used by researchers with positive feedback. At the time of the original submission, reviewers expressed some concerns about the novelty and the weak arguments for convincingly showing the advantages over other similar tools.   Authors provided nice replies including specific case studies, but with the short time period to reassess the proposed changes and additions, some reviewers remain hesitant, and thus this paper cannot be accepted at this time. I strongly encourage the authors to incorporate all of the proposed revisions and resubmit to a future venue.
This paper is motivated by figuring out what regularization do popular neural network reconstruction techniques correspond to. In particular, this paper studies a convex duality framework that characterizes the global optima of a two layer fully convolutional ReLU denoising network via convex optimization. The authors use this regularization to interpret the obtained training results. The reviewers raised a variety of concerns regarding the tractability of the optimization problem (seems to be exponential in number of constraints), the utility for interpretation etc, significance of the results compared to existing literature. Some of these concerns were alleviated but not fully resolved. One reviewer had concerns about the correctness of the proof that was resolved based on the authors’ response. I share many of the above concerns. However, I do think having a computationally feasible way to figure out the exact regularization in these simple settings (at least with small dimensions) could provide some insights to guide further theoretical development.  Therefore I am recommending acceptance. However, I strongly urge the authors to further revise the paper based on the above comments.
This paper proposes a new autoregressive flow model with autoencoders to learn latent embeddings from time series. The authors conducted extensive comparative experiments, and the experimental results are very encouraging. However, the proposed method, as a combination of the encoder/decoder structure and autoregressive flows on the latent space, does not seem novel enough.
This submission studies an interesting problem. However, as some of the reviewers point out, the novelty of the proposed contributions is fairly limited.
The authors design a framework for active learning on time series data. The framework, called Temporal Coherence based Label Propagation (TCLP) leverages temporal coherence to propagate expert labels to nearby points by a plateau model. In addition to describing the framework clearly with simple pseudocode, several experiments are carried out with careful analysis to validate the effectiveness of the framework.  The reviewers are mostly positive on the simple algorithm with strong empirical performance as well as the solid analysis of experimental results. They are also satisfied with most of the rebuttal feedback from the authors. Somehow there are joint concerns on the weaker theoretical results, especially in terms of their correctness. In particular, the unrealistic assumptions and over simplification make it hard to connect the theoretical results with the actual algorithm. Several reviewers suggest the authors to move theoretical analysis section to a supplementary section as a hypothesis, and the authors are also encouraged to clearly discuss what the theory can and cannot cover.
This paper provides a simple technique for stabilizing GAN training, and works over a variety of GAN models.  One of the reviewers expressed concerns with the value of the theory. I think that it would be worth emphasizing that similar arguments could be made for alternating gradient descent, and simultaneous gradient descent. In this case, if possible, it would be good to highlight how the convergence of the prediction method approach differs from the alternating descent approach. Otherwise, highlight that this theory simply shows that the prediction method is not a completely crazy idea (in that it doesn t break existing theory).  Practically, I think the experiments are sufficiently interesting to show that this approach has promise. I don t see the updated results for Stacked GAN for a fixed set of epochs (20 and 40 at different learning rates). Perhaps put this below Table 1.
The authors propose an invertible flow based model for molecular graph generation. The reviewers like the idea but have several concerns: in particular, overfitting in the model, need for more experiments and missing related work. It is important for authors to address them in a future submission
This paper aims at raising awareness of climate change by GAN projecting flooding images of popular places. This is an interesting case. While all reviews agree that this is an interesting direction, they also value the contributions differently. Two of them would like to see more methodological contributions, two focus more on the contribution to the (psychological) fight of climate change. Nevertheless, the rolling discussion helped to clarify several of the issues raised by the reviewers, and (in my opinion) "combining" existing methods to realize an important model that is one little step towards making people more aware how climate change may impact their own lifes is highly creative and useful. I therefor overall suggest strongly to accept the paper. As the ICLR CfP reads, ICLR is not just about methodological contributions. Societal considerations of representation learning are explicitly mentioned.
This paper proposes two contributions to improve uncertainty in deep learning.  The first is a Mahalanobis distance based statistical test and the second a model architecture.  Unfortunately, the reviewers found the message of the paper somewhat confusing and particularly didn t understand the connection between these two contributions.   A major question from the reviewers is why the proposed statistical test is better than using a proper scoring rule such as negative log likelihood.  Some empirical justification of this should be presented.
Quality: the paper takes an important question and analyzes it well from a theoretical angle; it also provides empirical evidence to back up its main message in more complex models. The proofs are non trivial. The paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non asymptotic regime.  Clarity: The motivation of studying the double descent phenomenon with optimal regularization is well explained in the introduction. Connections and comparisons with existing related works are discussed clearly. The paper is clearly written, and exposes the results in a clear and accessible fashion.   Originality: The presented theoretical results on the linear regression model are non asymptotic, which is new and different from existing works.   Significance: The proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings, which affects its significance.  Main Pros:   the paper takes an important question and analyzes it well from a theoretical angle. The proofs are non trivial; the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non asymptotic regime.  Main Cons:   Generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground truth parameters \beta^*.    The experiments need to be more extensive and better explained, especially for the CIFAR 100 experiments. It is important to discuss this difference clearly at the beginning. 
Meta Review for Learning Representations for Pixel based Control: What Matters and Why?  In this work, the authors presented large scale empirical evaluations and ablation studies to analyze various components (e.g. contrastive objectives, model based approaches, data augmentation) for pixel based control with distractors. Reviewer 7euW wrote a great summary for this paper:  This paper presents an approach for learning representations from pixel data that are amenable for control tasks. The proposed approach is a simple baseline that does not require data augmentation, world models, contrastive losses etc. but only contains two simple sub tasks that are supposed to contribute heavily towards an effective representation: reward prediction and state transition prediction. Along with evaluating this proposed baseline, the paper also compares it to several prior works on representation learning: i.e., several approaches such as data augmentation, distance metric losses, contrastive losses, relevant reconstruction etc. It is shown that the proposed simple baseline either outperforms several of these methods or at least is very close in performance. Finally, the paper presents an interesting discussion about how evaluating an algorithm is not just about the dataset and the chosen benchmark task, but requires a more nuanced point of view of several factors such as reward sparsity, action continuity/discreteness, relevance and irrelevance of features to the task, and so on. The findings of the paper are not just about the effectiveness of the proposed method, but a more overarching view of which types of representation learning methods work in what conditions.   Along with myself, most reviewers (including the critical 61FY) agree that there is great value in the large scale studies presented in this paper. Furthermore, I personally like how it links a large body of recent work in this topic together in one study. The reviews were mixed (6, 6, 3, 3), and the negative reviews (the 3 s) generally had issues with not the study or experiments, but the conclusions the authors drew from them. In the words of 61FY (who managed to have a good discussion with the authors):  *I m not convinced by conclusions as the authors try to generalize behavior of specific implementation to a family of methods. If I were to implement a new agent, I don t feel like I can believe these conclusions so that makes me question what knowledge this paper can add to the community. Furthermore, many details are either missing or not made clear, and the main story isn t very strong. Therefore, I don t think this paper is ready for publication in the current status.*  Although I really appreciate the effort and detail that went into this nice work, based on the current assessments from the 4 reviewers, I can t recommend it for acceptance in its current state. I feel though, that with a change of narrative, or even with a re examination of the experimental results, the authors can turn the paper around into a highly impactful paper. The description of all of the methods explored, and experiments performed alone makes a wonderful survey of the field with sufficient impact, so I think the authors are *almost* there in publishing a highly impactful work that can make the community look deeper into pixel based control methods (with distractors). I hope to read an updated version of this work in the future published at a journal or presented at a future conference. Good luck!
This paper introduces an RNN based approach to incremental domain adaptation in natural language processing, where the RNN is progressively augmented with the parameterized memory bank which is shown to be better than expanding the RNN states.  Reviewers and AC acknowledge that this paper is well written with interesting ideas and practical value. Domain adaptation in the incremental setting, where domains come in a streaming way with only the current one accessible, can find some realistic application scenarios. The proposed extensible attention mechanism is solid and works well on several NLP tasks. Several concerns were raised by the reviewers regarding the comparative and ablation studies, which were well resolved in the rebuttal. The authors are encouraged to generalize their approach to other application domains other than NLP to show the generality of their approach.  I recommend acceptance.
This paper studies the pruning problem of graph neural networks, i.e. finding lottery tickets for GNN. In particular, it generalizes UGS by Chen et al. (2021) from transductive setting to inductive setting where prediction on unseen graphs is possible. The main idea is: 1) learn a mask network to assign importance scores for edges using the embedding features of the nodes connected, that avoids the double parameter memory costs in UGS; 2) prune the edges according to the importance score and weights of GCN according to their magnitudes. Main concerns from reviewers are about the novelty, evaluation, and scalability. Despite that generalization to unseen graphs using the mask functions on embedding features is a new aspect, the evaluation is compared with relatively weak baselines and inference time scalability of is still an issue.
There is consensus among the reviewer that this is a good paper. It is a bit incremental compared to Gregor et al 2016. This paper show quite better empirical results.
In this paper  propose a novel approach for semi supervised domain adaptation based on the cyclic monotonicity property of optimal transport map. The main idea is to adapt (perturbed wrt a source classifier)  the labeled source samples  toward the target samples while preserving the known labels via the cyclical monotonicity. Then these perturbed samples can be used to perform classical OT domain adaptation. This pre processing of the data has been shown in the numerical experiments to lead to better performance in average.  The proposed method has been found intriguing by all reviewers but the writing of the paper has been found clearly lacking and several suggestions were proposed by the reviewers. The choice of the authors to call the perturbed samples adversaries for instance made the paper harder to understand and the (anti adversarial is also not a good choice of words). Another concern was that despite encouraging numerical results lack more baselines semi supervised Domain Adaptation methods discussed by the reviewers were not compared (with or without OT).   The authors provided a short but clear response that was appreciated by the reviewers. But the clarifications promised by the authors were not done in the PDF during the editing period which means that the paper clearly needs a new round of reviews.  For this reason the consensus during the discussions was that this paper should be rejected. The AC believes that this is an interesting research direction that should be investigated but that the paper needs some more work before reaching the threshold for acceptance in selective ML venues. The authors are strongly encouraged to take into account the comments form the reviewers before resubmitting their work.
This paper proposes a training algorithm for ConvNet architectures in which the final few layers are fully connected.  The main idea is to use direct feedback alignment with carefully chosen binarized (±1) weights to train the fully connected layers and backpropagation to train the convolutional layers. The binarization reduces the memory footprint and computational cost of direct feedback alignment, while the careful selection of feedback weights improves convergence. Experiments on CIFAR 10, CIFAR 100, and an object tracking task are provided to show that the proposed algorithm outperforms backpropagation, especially when the amount of training data is small. The reviewers felt that the paper does a terrific job of introducing the various training algorithms   backpropagation, feedback alignment, and direct feedback alignment   and that the paper clearly explained what the novel contributions were. However, the reviewers felt the paper had limited novelty because it combines ideas that were already known, that it has limited applicability because it will not work with fully convolutional architectures, that the baselines in the experiments were somewhat weak, and that the paper provided no insights on why the proposed algorithm might be better than backpropagation in some cases. Regrettably, only one reviewer (R2) participated in the discussion, though this was the reviewer who provided the most constructive review. The AC read the revised paper, and agrees with R2 s concerns about the limited applicability of the proposed algorithm and lack of insight or analysis explaining why the proposed training algorithm would improve over backpropagation.
Thanks for your submission to ICLR.  When the initial reviews were written, three of the four reviewers were positive about the paper.  Everyone felt it was overall a solid contribution, but there were some concerns about the clarity and presentation, as well as some suggestions for additional experiments.  During the rebuttal/response period, the authors did a very nice job in responding to the concerns of the reviewers.  Ultimately, all of the reviews were in agreement after discussion that the paper is strong and ready for publication.   I also like this paper a lot, and find it to be a nice way to combine LSH with NN training.  I am happy to recommend this paper for publication.
This paper received borderline scores but overall lean positive.   The reviewers point out that the paper presents interesting new ideas and an effective solution to the problem of automatically searching for loss functions. The empirical results are convincing, although the baselines are not the strongest possible in terms of absolute performance. Overall, the ACs find that the paper has sufficient novelty and technical contribution to be accepted. 
The paper addresses the problem of learning and exploiting common (latent) task structure in multi task reinforcement learning settings. The authors introduce a new formalism for capturing this type of structure and derive a gradient based learning algorithm. They provide novel theoretical insights and strong empirical results.  Reviewers initially raised several concerns, regarding assumptions and especially accessibility of the paper (and in particular theoretical discussions). The majority of these concerns have been addressed in the detailed rebuttal. The resulting consensus is to accept the paper. Authors are encouraged to continue to improve accessibility of the paper for the camera ready submission.
This paper studies improving continuous control. The paper suggests a practical, beneficial combination approach that does well in the presented experiments. It also provides some overview and comparison over several recent insights in RL. While both are valuable, multiple reviewers had concerns that the paper has some limitations on both. In particular, the proposed ensemble approach is quite simple though valuable, and that reviewers generally felt that raised their expectations as to the strength of the empirical results which was not yet there. The reviewers’ provided a lot of detailed feedback that may be useful in revising the contribution.
This paper studies spread divergence between distributions, which may exist in settings where the divergence between said distributions does not. The reviewers feel this work does not have sufficient technical novelty to merit acceptance at this time.
The reviewers had several concerns with the paper related to novelty and comparisons with other approaches. During the discussion phase, these concerns were adequately addressed.
While the problem of learning word embeddings for a new domain is important, the proposed method was found to be unclearly presented and missing a number of important baselines. The reviewers found the technical contribution to be of only limited value.
The paper tackles a very important problem. The formulation of the paper is sound as under lightweight assumptions, the supervised loss follows an f divergence formulation (see "Information, Divergence and Risk for Binary Experiments" by Reid and Williamson (JMLR 2011), in particular Section 4.7). It would make sense to dig in the loss in the context of label noise; the variational formulation provides an interesting direction along those lines. The rebuttal on the experimental concerns of reviewers is appreciated (Cf authors’ rebuttal summary). 
The paper focuses on NeuralODE and shows that for the implementation popular among ML community, one of the equations is not an ODE and can be replaced by an integral. This is implemented using "seminorm" (just assigning zero weight to the last equation).  Pros:    Well written   Useful to replace the "standard" implementation    Consistent benchmarking  Cons:    Contribution is too limited   Used in several "prior" codes without explicit ICLR submission.    (My personal) The title is not good: more on the "hype side" of the story, rather than progressing the field. I don t think we need to put every single minor fact into a ICLR submission.  For example, one can just compute the integral as an alternative by any suitable quadrature rule. That would add 10 15 function evaluations at most, since most of the functions in NeuralODEs are quite smooth.
This paper argues that the existing approaches for reducing power consumption do not model the precise power usage of each model. To remedy this an approximate power usage model is proposed using bit flips and a simple approach called PANN is introduced that relies on tricks such as unsigned arithmetics and implementation of multiplications with addition. The reviewers have found the overall direction of this paper in modeling power consumption important and have acknowledged the clarity of presentation. However, they have also raised serious concerns regarding (i) the efficacy of modeling power consumption with bit flips and ignoring memory power, (ii) its relevance to modern hardware, and (ii) the efficacy of replacing multipliers with repeated additions. Unfortunately, the paper in it is current form does not provide a compelling answer to these concerns. Given these criticisms, we don t believe that the paper is ready for publication at ICLR.
The paper describes a WaveNet like model for MIDI conditional music audio generation. As noted by all reviewers, the major limitation of the paper is that the method is evaluated on a synthetic dataset. The rebuttal and post rebuttal discussion didn t change the reviewers  opinion.
The paper has been actively discussed in the light of the authors’ response. Even though the paper was, overall, found quite clear with a solid theoretical support, the reviewers listed several concerns that remained unsolved after the rebuttal, e.g.,  * The proposed approach may not be properly scoped/positioned and evaluated as an HPO method, a concern unanimously shared across the reviewers. Although this is not a concern impossible to overcome, the reviewers believed it could not be achieved as part of a simple revision of the paper. * The lack of challenging baselines to fully assess the performance of the proposed method (e.g., see the list suggested by Reviewer 1)  * Along the lines of the previous point, the experiments focus on small scale settings, which does not make it possible to completely assess the performance of the approach * Some discrepancy between the theoretical analysis and the actual experimental settings (e.g., the assumption about bounded losses not valid with the squared loss)   As illustrated by its scores, the paper is extremely borderline. Given the mixed perspectives of pros and cons, the paper is eventually recommended for rejection.  This list, together with the detailed comments of the reviewers, highlight opportunities to improve the manuscript for a future resubmission. 
This paper opens the area of adversarial attack research on streaming data (e.g., real world settings such as self driving cars and robotic visual tasks for a robot). For instance, online adversaries can focus their attack on a small subset of the streamed/online data, but still cause much damage to downstream models. This work highlights the need for stateful defense strategies. Connections to online algorithms and the k secretary problem are made, along with improvements to some online algorithms work of Albers and Ladewig.    Overall, the attack model introduced is important, and the bridge to online algorithms would be useful for the ICLR community. I also believe this topic lends diversity to the typical set of ICLR papers.
This paper proposes a new dataset, called RainNet, obtained from gridded precipitation data, for training precipitation downscaling methods, as well as a new neural network based architecture for that task, which estimates the underlying dynamics of the local weather system, and new metrics for evaluating precipitation downscaling methods.  Reviewers praised the large, novel and useful dataset (D3tQ, szBD, ggKX) and novel metrics for evaluating statistical downscaling methods (D3tQ), along with evaluation on 14 baselines (szBD, ggKX).  There were however many issues highlighted by the reviewers. First, reviewer D3tQ raised concerns about the paper being resubmitted after rejection from NeurIPS (/pdf?id VVZZJiQB51l), with minimal changes (/pdf?id 6p8D4V_Wmyp), and noticed that the authors did not follow up on most reviewer recommendations. D3tQ noticed however that in the ICLR resubmission, the cross validation results were presented to provide a more robust comparison between models, and that the discussion of metrics in section 4 was much more thorough than in the previous version.  Other themes in the negative reviews included concerns about missing standard errors in the cross validation results (D3tQ, 5pVg) or measures of uncertainty in the upscaling (ggKX), lack of information about hyperparameter tuning (D3tQ), inadequate literature review about statistical downscaling (D3tQ), lack of information about the dataset (5pVg), missing discussion about applications (ggKX) and insufficient proofreading (D3tQ, 5pVg).  I will not take into consideration the criticism from szBD who "don t feel that ICLR is the right venue for this work" as I do not find such opinions to be much helpful.  The authors did not provide a rebuttal to the initial reviews and there was no discussion about this paper among the reviewers. Given the issues raised by the reviewers and the scores of 3, 3, 5 and 6, I believe that this paper does not meet the acceptance bar in its current form.  Sincerely, AC
This paper studies the effect of using unlabelled out of distribution (OOD) data in the training procedure to improve robust (and standard) accuracies. The main algorithmic contribution is a data augmentation based robust training algorithm to train a loss which is carefully designed to benefit from the additional OOD data. What s also interesting is that the OOD data is fed with random labels to the training procedure. As demonstrated in the theoretical results, this way of feeding OOD data helps to remove the dependency to non robust features and hence improves robustness.    As pointed out by all the reviewers (which I agree with), the idea of using unlabelled OOD data at training is novel/interesting, and the paper also shows how this can be done algorithmically. The numerical results also confirm the effectiveness of the proposed methods. 
This paper investigates TD based off policy policy evaluation. This topic is of interest as most SOTA DRL methods are built upon unsound algorithms, whereas more sound variants are difficult to use in practice and have not been widely adopted. This paper introduces a new variant of ETD that addresses the variance issue with the existing algorithm, along with theory characterizing sample efficiency. The paper includes a well done illustrative empirical study to support the theory. The reviewers all scored the paper highly.   The AC pointed out several minor issues in the presentation that the authors should address for camera ready.  In addition the grammar and word usage is rough in some places. Please take time to improve the text.
Pros:   The authors propose a novel method to perform MCMC in the condition where there is a distribution over models describing the data, rather than just a distribution over the parameters of a single consistent model (ie, in the theme of reversible jump MCMC). Sampling from the posterior of mixtures of parametric models is something current MCMC algorithms are very bad at, so this addresses an important need.   Reviewers believed the proposed technique was novel and technically correct.   The paper appears to build a bridge between the fields of system identification and Bayesian sampling techniques  Cons:   Major concerns were raised about lack of clarity     From a *lightweight* read, I also had difficulty understanding what the paper was proposing, or even the precise problem it was tackling   Experimental validation was limited, and missing baselines   During discussion, the paper had no strong advocate
Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission. Especially, the authors should take care to make this paper accessible (understandable) to the ML community as ICLR is a ML venue (rather than quantum physics one). Failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future.
This work proposes a dynamical systems model to allow the user to better control sequence generation via the latent z. Reviewers all agreed the that the proposed method is quite interesting. However, reviewers also felt that current evaluations were weak and were ultimately unconvinced by the author rebuttal. I recommend the authors resubmit with a stronger set of experiments as suggested by Reviewers 2 and 3.
The authors proposed to pre process the original input features into a low dimensional term and its corresponding residual term via SVD. The paper empirically demonstrated the neural networks trained on such factorized exhibit faster convergence in training. Several issues of clarity were addressed during the rebuttal period by the authors.   However, the reviewers still felt that there were some remaining fundamental issues with the paper,   1)  The motivation is not echoed in the experiments, namely most of the experiments on CIFAR and CatDog dataset using a low dimensional factorization of d 1 which is trivial and often part of the whitening preprocessing.   2) The proposed factorization via SVD will be difficult to scale up to high dimensional features, large training sets and higher d >> 1.    3) The empirical experiments show a marginal improvement in the training speed, especially in the image recognition tasks, yet there seems an early plateau in test performance when compared to the baselines.  4) The theoretical analysis in Section 2 studied linear models. Yet, the rest of the paper focuses on non linear neural networks. It is difficult to see the connection between the analysis and the rest of the paper.   Thus, I recommend rejection of the paper at this time as the current version of the paper needs further development, and non trivial modifications, to be broadly applicable.
 + interesting novel extension of equilibrium propagation, as a biologically more plausible alternative to  backpropagation, with encouraging initial experimental validation.    currently lacks theoretical guarantees regarding convergence of the algorithm to a meaningful result    experimental study should be more extensive to support the claims
The proposed ENorm procedure is a normalization scheme for neural nets whereby the weights are rescaled in a way that minimizes the sum of L_p norms while maintaining functional equivalence. An algorithm is given which provably converges to the globally optimal solution. Experiments show it is complementary to, and perhaps slightly better than, other normalization schemes.  Normalization issues are important for DNN training, and normalization schemes like batch norm, weight norm, etc. have the unsatisfying property that they entangle multiple issues such as normalization, stochastic regularization, and effective learning rates. ENorm is a conceptually cleaner (if more algorithmically complicated) approach. It s a nice addition to the set of normalization schemes, and possibly complementary to the existing ones.  After a revision which included various new experiments, the reviewers are generally happy with the paper. While there s still some controversy over whether it s really better than things like batch norm, I think the paper would be worth publishing even if the results came out negative, since it is a very natural idea which took some algorithmic insight in order to actually execute.  
This paper proposes an algorithm for end to end image compression outperforming previously proposed ANN based techniques and typical image compression standards like JPEG.  Strengths   All reviewers agreed that this a well written paper, with careful analysis and results.  Weaknesses   One of the points raised during the review process was that 2 very recent publications propose very similar algorithms. Since these works appeared very close to ICLR paper submission deadline (within 30 days), the program committee decided to treat this as concurrent work.  The authors also clarified the differences and similarities with prior work, and included additional experiments to clarify some of the concerns raised during the review process. Overall the paper is a solid contribution towards improving image compression, and is therefore recommended to be accepted. 
This paper proposes a scalable optimization method for multi task learning in multilingual models.   Pros: 1) Addresses a problem which has not been explored much in the past 2) Presents very good analysis to show the limitations of existing methods.  3) Good results.  4) Well written  Cons: 1) Some missing details about various choices made in the experiments (mostly addressed in the rebuttal)  This is a very interesting and useful work and I recommend that it should be accepted. 
This paper explores the Wasserstein natural gradient in the context of reinforcement learning. R5 rated the paper marginally below the acceptance threshold, but is not very confident about the correctness of his/her assessment. His/her main criticism was the experimental evaluation. This concern was shared by a confident R1. R1 found the paper well structured and that it contains encouraging empirical results, but low technical novelty and (initially) insufficient experiments. His/her initial recommendation was reject, but following an extensive discussion and improvements of the manuscript by the authors, he/she was more convinced about the empirical significance and applicability of the method, and raised his/her score to 6, indicating that the interpretation and presentation improved but that the paper might be interesting only to a moderate number of readers. A confident R2 found this paper very good, although only providing a short review. Two other unfinished or not sufficiently confident reports were not taken into account. Weighing the reports by contents, confidence, and participation in the discussion, the paper scores marginally above the acceptance threshold. In view of the authors  responses, I am discounting R5 s criticism about lack of comparison with the PPO baseline. I personally consider the paper very well written, that it presents a natural and potentially useful application of the Wasserstein natural gradient to the context of reinforcement learning, and enjoyed the discussion of behavioral geometry. I am recommending a borderline accept. However, I also appreciate the concern of the referees about the limited technical innovation and how some of the strengths of the method could be presented more convincingly. Please take these comments carefully into consideration when preparing the final version of the paper. 
Pros   Extends embeddings to use a richer representation; simple yet interesting improvement on Mikolov et al. work. Cons   All of the reviewers pointed out that the experimental evaluations needs improvement. The authors should find better ways to improve both quantitative (e.g., accuracy in analogies as in Mikolov et al., or by using the model for an external task if that’s the end goal) and qualitative (using functional similarity for the baseline) evaluations.  Given these comments, the AC recommends that the paper be rejected. 
There was quite a bit of discussion about this paper but in the end the majority felt that, though the paper is interesting, the results are too limited and more needs to be done for publication.  PROS: 1. Good comparison of state space model variations 2. Good writing (perhaps a bit dense in places) 3. Promising results, especially concerning speedup  CONS: 1. The evaluation is quite limited  
The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights. They show that various Bayesian deep learning algorithms tend to converge to layers of this variety. This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods. 
Well motivated and well received by all of the expert reviewers. The AC recommends that the paper be accepted.
The reviewers indicated a number of concerns (which I agree with) which have not been addressed by the authors as they have not provided any response.  Indeed, the paper would be significantly improved once these issues are addressed. 
The paper proposed a speech to speech translation (S2ST) model. The model is trained end to end from speech to speech, along with an auxiliary speech to phoneme task. Experiments firmly support multiple claimed improvements to the previous model.  However, most reviewers argue the novelty and clarity of this paper, making this paper cannot be accepted by ICLR 2022. We hope the authors can modify this paper accordingly.
The paper considers the problem of learning to carry out novel, multi task instructions specified via temporal logic using deep reinforcement learning. A specific focus of the paper is improving generalization to test time instructions that differ from those encountered during training. To facilitate this generalization, the proposed architecture encodes a latent specification of the goal according to the given instruction and environment state that is then combined with a task agnostic environment embedding. Experiments on grid like domains demonstrate that the proposed framework outperforms recent deep RL approaches to satisfying temporal logic based instructions.  The instruction following problem has long been of interest in the robotics, ML, and broader AI communities dating back several decades. The problem has received renewed attention in the last few years, largely as a target for neural network based multi view and RL learning architectures. The primary contribution of this paper is the proposed extension of existing deep RL approaches to reason over a learned, latent goal specification as a means of improving generalization to novel test time utterances. The approach is sound and several reviewers agree that the ablation studies together with comparisons to contemporary deep RL architectures support the advantage of these inductive biases. The reviewers raised initial concerns regarding the statistical significance of the results and the clarity of the presentation. The authors provided detailed feedback to the reviewers and updated the paper to address many of these concerns, largely satisfying two of the reviewers.  However, concerns remain that the paper doesn t adequately position this work in the context of the decades worth of research in instruction following. Early work in this area focused on interpreting highly structured instructions (e.g., formal logic based), first using rule based methods, and then parsers trained via supervised learning. Over the past decade, however, the field has largely moved towards learning to follow instructions conveyed in "natural" language, which brings with it a significant number of challenges, including the assumption that test time instructions will inherently be out of distribution. That is not to say that the contributions of the paper aren t interesting they are, but in the relatively narrow scope of deep RL based approaches to following structured, temporal logic instructions.
The paper proposes a layer wise or block wise distillation scheme, Neighbourhood Distillation, that aims to reduce the training time and to improve parallelism when distilling large teacher networks. By breaking down the end to end distillation objective into blocks, the proposed method enables faster distillation when applied to model compression and block wise architecture search. Several concerns of reviewers were addressed during the rebuttal period by the authors.  However, there are still some concerns among the reviewers after the discussion:  1) Computational cost benefit of the proposed KD method seems marginal in comparison to the baseline, given that we still have to train all the neighborhoods in parallel and potentially to fine tune in the end.   2) It was brought up by a couple of the reviewers that the experiments lack diversity. It would be great to a clearly defined metric applied to a wide variety of the model architectures and datasets. It will also strengthen the paper by providing more details on the experiments.  The basic idea is interesting, but the paper needs further development and modification for publishing.   
The reviewers raised a number of major concerns including the incremental novelty of the proposed (WGANs are applied to a new domain), and, most importantly, insufficient and unconvincing experimental evaluation presented (including the lack of comparative studies). The authors’ rebuttal failed to fully alleviate reviewers’ concerns. Hence, I cannot suggest this paper for presentation at ICLR.
The paper presents an architecture search method which jointly optimises the architecture and its weights. As noted by reviewers, the method is very close to Shirakawa et al., with the main innovation being the use of categorical distributions to model the architecture. This is a minor innovation, and while the results are promising, they are not strong enough to justify acceptance based on the results alone.
The reviewers all acknowledge the importance of the paper as it addressed the challenge of the insufficient data problem in conditional contrastive learning, feeling that the idea was novel, the experiments verified the effectiveness of the model well, and the paper is well written. Reviewers also raised some good questions, such as the computational complexity, comparison with Fair_InfoNCE in the experiments, and kernel ablations. These questions are well addressed in the rebuttal and the revised version. One reviewer raised the issue of similarity to [1]. After taking a close look at this paper and [1], the AC felt that the motivation and focus of this paper are quite different from [1]. The authors should incorporate all the rebuttal info into the final version.  [1] Jean Francois Ton et al. 2021.
The paper is concerned with learning transformation equivariant node representation of graph data in an unsupervised setting. The paper extends prior work in this topic by focusing on equivariance under topology transformations (adding/removing edges) and considering an information theoretic perspective. Reviewers highlighted the promising ideas of the approach, its relevance for the ICLR community, and the promising experimental results (although improvements over prior work are not necessarily significant on all benchmarks).  However, reviewers raised concerns regarding the novelty of the method and the clarity of presentation with respect to key parts of the method. These aspects connect also to further concerns raised, e.g., related to mathematical correctness as well as the significance of the proposed loss function, the benefits of motivating it from MI, and the improvements over GraphTER. The rebuttal didn t fully clarify these points. While the paper is mostly solid, I agree with the reviewers  concerns and   currently   the paper doesn t clear the bar for acceptance; it would require another revision to improve upon these points. However, I d encourage the authors to revise and resubmit their work with considering this feedback.
In this paper, the authors study the behavior of the Lookahead dynamics of Zhang et al. (2019) in bilinear zero sum games. These dynamics work as follows: given a base algorithm for solving the game (such as gradient descent ascent or extra gradient), the Lookahead dynamics perform $k$ iterations of the base algorithm followed by an exponential moving average step with weight $\alpha$. The authors then provide a range of sufficient conditions for the eigenvalues of the matrix defining the game under which the Lookahead dynamics become more stable and converge faster than the base method.  This paper received four reviews and generated a very lively discussion between the authors and reviewers. Reviewer 4 was enthusiastic about the paper; the other three initially recommended rejection. During the discussion phase, the authors revised their paper extensively, and Reviewer 3 increased their score to an "accept" recommendation as a result. In the end, the reviewers were evenly split, and I also struggled a lot to reach a recommendation decision.  On the plus side, the paper treats an interesting problem: prior empirical evidence suggests that the Lookahead dynamics can improve the training of some adversarial machine learning models, so a theoretical study is very welcome and of clear value. On the other hand, the setting treated by the paper (bilinear min max games) is somewhat restrictive, and the authors  theoretical conclusions do not always admit as clear an interpretation as one would like.  The issues that ended up playing the most important role in my recommendation were as follows: 1. The Lookahead dynamics with period $k$ involve $k$ gradient evaluations, so their rate of convergence should be compared at a $k:1$ ratio to GD and EG (with an additional $2:1$ ratio between GD and EG to put things on an even scale). To a certain degree, this $k:1$ ratio is present in the last part of Lemma 3; however, the exact acceleration achieved by the "shrinkage" of the spectral radius is not clear. This can also be seen in the semi log plots provided by the authors, where the corresponding slopes of GD/EGD methods should be multiplied by $k$ when compared to the respective LA variants. In this regard, a comparison with the values of $k$ provided in Appendix D reveal that the performance of the Lookahead variants in terms of gradient queries is very similar (if not worse) to the non LA variants. This is a cause of concern because, if LA does not accelerate convergence in simple bilinear games, it is not credible to expect faster convergence in more complicated problems. During the AC/reviewer discussion of this point, Reviewer 3 pointed out that this might be due to a suboptimal tuning of $\alpha$ (i.e., that it was not chosen "small enough"), and went out to note that this echoes the arguments of other reviewers that the characterization of acceleration may be problematic and not significant (even if it takes place). 2. Another major concern has to do with the stabilization provided by the Lookahead dynamics: using a benchmark game proposed in a recent paper by Hsieh et al. (2020), the authors showed that the Lookahead dynamics converge to a point which is unstable under GDA/EG (and hence avoided). This is fully consistent with the authors  theoretical analysis, but it also highlights an important problem with the Lookahead optimizer: if $k$ and $\alpha$ are tuned to suitable values for stabilization, the algorithm converges to a non desirable critical point (a max min instead of a min max solution). This is a major cause of concern because it shows that the algorithm may, in general, converge to highly suboptimal states.  The above create an inconsistency in the main story of the paper. In fact, it seems to me that the authors  results form more of a "cautionary tale in hiding": even in very simple bilinear problems, the lookahead step may not provide acceleration and, even worse, it could converge to highly undesirable critical points. I find this "negative" contribution quite valuable from a theoretical standpoint, and I believe that a thoroughly revised paper along these lines would be of interest in the top venues of the community (though a more theoretical outlet like COLT might be more appropriate). However, this would require a drastic rewrite of the paper, to the extent that it should be treated as a new submission.  In view of all this, I am recommending a rejection at this stage. I insist however that this should not be seen as a critique for the mathematical analysis of the authors (which was appreciated by the reviewers), but as a recommendation to reframe the paper s narrative to bring it in line with the algorithm s observed behavior. I strongly encourage the authors to resubmit at the next top tier opportunity.
The paper improves the previous method for detecting out of distribution  (OOD) samples.   Some theoretical analysis/motivation is interesting as pointed out by a reviewer. I think the paper is well written in overall and has some potential.  However, as all reviewers pointed out, I think experimental results are quite below the borderline to be accepted (considering the ICLR audience), i.e., the authors should consider non MNIST like and more realistic datasets. This indicates the limitation on the scalability of the proposed method.   Hence, I recommend rejection.
The authors propose a method for feature selection in non linear models by using an appropriate continuous relaxation of binary feature selection variables. The reviewers found that the paper contains several interesting methodological contributions. However, they thought that the foundations of the methodology make very strong assumptions. Moreover the experimental evaluation is lacking comparison with other methods for non linear feature selection such as that of Doquet et al and Chang et al.
The paper tackles a major problem of supervised ML, that of the minimisation of the risk of a set of classifiers. This problem has received attention in numerous work over the past decades, much of which spans the formal aspects of the problem. The paper tackles the problem from a “diversity” standpoint. My main concern is, for such a problem and exhaustive formal and experimental SOTA, one cannot just evacuate any formal understanding of a contribution to future work (Authors’ reply to R2). The argument is then a victim of its own content, ending up in a sloppy vocabulary where “speculation” and “intuition” are called forward as justification to the calls for “rigorous” (R1) and “theoretical” understanding (R2 + answer to R2). I am confident the authors can find formal merit to their contribution, but this needs to be addressed. R1 + R4 hint on avenues to understand the contribution.  
Thank you for submitting you paper to ICLR. The big picture idea is fairly simple, although the implementation is certainly challenging requiring a deep generative model to be trained as part of the final system. The experimental validation is not sufficient to warrant publication. A comparison to a larger number of competitors e.g. [1,2] on a greater range of tasks is required.  [1] Continual Learning Through Synaptic Intelligence Friedemann Zenke BenPoole SuryaGanguli, ICML 2017 [2] Gradient Episodic Memory for Continual Learning, David Lopez Paz and Marc’Aurelio Ranzato, NIPS 2017
This paper proposes a two stage distillation from pretrained language models, where the knowledge distillation happens in both the pre training and the fine tune stages.  Experiments show improvement on BERT, GPT and MASS.  All reviewers pointed that the novelty of the work is very limited.
This paper focuses on the limitations of the transformer architecture as an autoregressive model. The paper is relatively easy to follow. Though most reviewers find the paper interesting, the idea is not very novel. The introduction of sequential ness to Transformer is good, though it also slow things down especially as the sequence gets longer.  An extensive set of experiments are performed, though the results are not entirely convincing. The authors are encouraged to add more ablative experiments, efficiency analysis, and large scale results.
This paper proposes a regularization approach based on the second order Taylor expansion of the loss objective to improve robustness of the trained models against \ell_inf and \ell_2 attacks. It is interesting to explore the second order based regularization approach for network robustness. However, as pointed out by the reviewer, a major drawback of this approach is that SOAR is broken under a stronger attack   AutoPGD DLR.  In addition, the theoretical bound seems very loose in the \ell_inf case.  
In this paper, a network architecture search (NAS) problem in a changing environment is studied and an online adaptation (OA) algorithm for the problem is proposed. Many reviewers found that the OA NAS problem discussed in this paper is interesting and practically important. However, many reviewers (including those with high review scores) recognize that the weakness of this paper is the lack of sufficient theoretical verification. Furthermore, although extensive experiments are conducted, it is still not clear whether the experimental setups discussed in the paper are generally applicable to other practical problems. Overall, although this is a nice work in that a new practical problem is considered and a workable algorithm for the problem is demonstrated in an extensive simulation study, I could not recommend the acceptance in its current form because of the lack of theoretical validity and evidence of general applicability.
This paper proposes an "embedding layer" in which points on a model are mapped into a feature space, trained using a reconstruction based pretext task.  Then, the resulting embedding layer can be applied to shape data before using different learning architectures for modalities like meshes and point clouds.  The work is particularly interesting in its attempt to derive a learned shape representation that is agnostic to modality.  Some questions remained about experiments (e.g. baselines), but these are relatively minor and partially addressed in the rebuttal phase; also, sometimes the improvement seems to be marginal in practice.    Two reviewers championed this work during the discussion phase.  The AC tends to agree this work is an interesting direction for future work and contains insight that the vision/learning communities might be able to use in other settings.
The paper analyzes the space of mixed sample data augmentation approaches, and proposes a new variant, FMix, based on a new masking strategy. Reviewers point to the fact that FMix is only marginally better than previous approaches, that the experimental setup is unconvincing, and that the proposed analysis might not be grounded. This is a really borderline paper but I see the issues as more important than the benefits, so I recommend rejection.
The authors propose a novel approach for imitation learning in settings where demonstrations are unaligned with the task (e.g., differ in terms of state and action space). The proposed approach consists of alignment and adaptation steps and theoretical insights are provided on whether given MDPs can be aligned. Reviewers were positive about the ideas presented in the paper, and several requests for clarification were well addressed by the authors during the rebuttal phase. Key evaluation issues remained unresolved. In particular, it was unclear to what degree performance differences were purely caused by issues in alignment, and reviewers did not see sufficient evidence to support claims about performance on the full cross domain learning setting.
The authors propose a recurrent model of self position, with a handcrafted expression of the rotational structure in terms of a matrix Lie group. As noted by the reviewers, this work strongly builds upon Gao et al (ICLR 2019). This really is mentioned too late and not prominently enough in the manuscript, and furthermore, the difference to this work is not clearly explored in the paper (there are just two sentences immediately prior to the conclusion and no experimental comparison). The reviewers pointed out that the phenomena observed here are handcrafted into the structure of the model, rather than being emergent. The reviewers raised concerns that it is not clear what conclusion to draw from this work. For these reasons, I recommend rejection this stage.
This paper studies margin maximization in linear and ReLu networks. The reviewers appreciate the technical contributions of this paper, especially the simple counterexamples. However, reviewers also found the new results seem not to give enough conceptual insights or an important "main result". The meta reviewer agrees and thus decides to reject this paper.
This paper proposes a training scheme to enhance the optimization process where the outputs are required to meet certain constraints. The authors propose to insert an additional target augmentation phase after the regular training. For each datapoint, the algorithm samples candidate outputs until it find a valid output according the an external filter. The model is further fine tuned on the augmented dataset.   The authors provided detailed answers and responses to the reviews, which the reviewers appreciated. However, some significant concerns remained, and  due to a large number of stronger papers, this paper was not accepted at this time.
Three experts reviewed the paper and gave mixed reviews. Reviewer BBZL raised their score to 6 in the discussion phase. Reviewer dv5k was not fully convinced by the rebuttal and remained negative. Reviewer oUrr also remained negative. The reviewers were not excited by the proposed method in general and raised questions about both experiments and theoretical results. AC found clear merits in the paper, but the reviewers  comments suggested the work could be strengthened in both experiments and presentation. Hence, the decision is *not* to recommend acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper tackles the neural contextual bandit problem, for which existing approaches consists rely on bandit algorithms based on deep neural networks to learn reward functions. In these existing strategies, exploration takes place over the entire network parameter space, which can be inefficient for the large size networks typically used in NTK based approaches. In this work, the authors address this by building on an existing technique of shallow exploration, which consists in exploring over the final layer of the network only, allowing to decouple the deep neural network feature representation learning from most of the exploration of the network parameters. More specifically, they propose a simple and effective UCB based strategy using this shallow exploration scheme, for which they provide a theoretical analysis. The proposed approach builds on several ideas for previous works, including borrowing proof techniques and theoretical arguments. Although this limits the novelty of the work, connecting these ideas together is not obvious and constitutes a significant contribution. Moreover, the proposed approach fixes an important known issue due to the matrix inversion in LinUCB, which could have a strong impact on the bandit community.
This paper presents a meta learning algorithm that represents uncertainty both at the meta level and at the task level. The approach contains an interesting combination of techniques. The reviewers raised concerns about the thoroughness of the experiments, which were resolved in a convincing way in the rebuttal. Concerns about clarity remain, and the authors are *strongly encouraged* to revise the paper throughout to make the presentation more clear and understandable, including to readers who do not have a meta learning background. See the reviewer s comments for further details on how the organization of the paper and the presentation of the ideas can be improved.
This paper proposed a long term object based memory system for robots.  The proposed method builds on existing ideas of data association filters and neural net attention mechanisms to learn transition and observation models of objects from labelled trajectories.  The proposed method was compared with baseline algorithms in a set of experiments.  The initial reviews raised multiple concerns about the paper. Reviewers nrGQ and  V7qP commented on the conceptual gap between the problem proposed in the introduction and the extent of the experiments. Reviewer qPet understood the paper to be a form of object re identification and was concerned about the limited comparisons with related work.  The author response clarified their goal of estimating the states of the objects in the world, which they state is different from the goals of long term tracking and object reidentification mentioned by the reviewers.  The authors also clarified the relationship to other work in slot attention and data association filters.    The ensuing discussion among the reviewers indicated that the paper s contribution remained unclear even after the author response. Two reviewers noted the paper did not clearly communicate the problem being solved (all reviewers had a different view of the problem in the paper).  These reviewers wanted a better motivation for the problem being addressed in this paper.  The third reviewer remained unconvinced that the problem in the paper was different from long term object tracking.  Three knowledgeable reviewers indicate reject as the contributions of the paper were unclear to all of them. The paper is therefore rejected.
This paper studies the problem of devising optimal attacks in deep RL to minimize the main agent average reward. In the white box attack setting, optimal attacks amounts to solving a Markov Decision Process, while in black box attacks, optimal attacks can be trained using RL techniques. Empirical efficiency of the attacks was demonstrated. It has valuable contributions on studying the adversarial robustness on deep RL. However, the current motivation and setup needs to be made clearer, and so is not being accepted at this time. We hope for these comments to help improve a future version.
The paper addresses various improvements in visual continuous RL, based on a previous RL algorithm (DrQ). As the reviewers point out, the main contribution of the paper is of empirical nature, demonstrating how several different choices relative to DrQ significantly improve data efficiency and wall clock computation, such that several control problems of the DeepMind control suite can be solved more efficiently. The average rating for the paper is above the acceptance threshold, and some reviewers increased their rating after there rebuttal. While a mostly empirically motivated papers is always a bit more controversial, the paper may nevertheless stimulated an interesting discussion at ICLR that will be beneficial for the community, and should thus be accepted.
This paper exposes a simple recipe to manipulate the latent space of generative models in such a way to minimize the mismatch between the prior distribution and that of the manipulated latent space. Manipulations such as linear interpolation are commonplace in the literature, and this work will be helpful to improve assessment on that front.  Reviewers found this paper interesting, yet unpolished and incomplete. In subsequent iterations, the paper has significantly improved on those fronts, however the AC believes an extra iteration will make this work even more solid. Thus, unfortunately this paper cannot be accepted at this time. 
The paper introduces a new and convincing method for video frame prediction, by adding prediction uncertainty through VAEs.  The results are convincing, and the reviewers are convinced.  It s unfortunate however that the method is only evaluated on simulated data.  Letting it loose on real data would cement the results and merit oral representation; in the current form, poster presentation is recommended.
The paper proposes a Bayesian approach for time series regression when the explanatory time series influences the response time series with a time lag. The time lag is unknown and allowed to be non stationary process. Reviewers have appreciated the significance of the problem and novelty of the proposed method, and also highlighted the importance of the application domain considered by the paper. 
This paper proposes a decentralized attribution method to distinguish the generative models trained on the sample dataset. The key theoretic result is the derivation of the sufficient conditions for decentralized attribution and the design of keys following these conditions. Results are validated on two datasets with several generative models. The work is very interesting. R1: Overall, I am more positive. I am willing to raise my score to 6. However, the paper is still somewhat borderline. R3: Given the rebuttal, I am willing to raise my score to a 6 due to the added StyleGAN, PGAN, and other experiments and improved paper layout / clarity. The added experiments are welcome addition to the paper and demonstrate this technique. The lip and eyestaining are interesting results and I do hope this direction gets explored in the future. 
The paper presents a good analysis on the use of different linear maps instead of identity shortcuts for resnet. It is interesting to the community but the experimental justification is insufficient. 1) As pointed out by the reviewer that this work shows "that on small size networks Tandem Block outperforms Residual Blocks, since He at. al. (2016) in Tab 1 showed a contrary effect, does it mean that the observations do not scale to higher capacity networks?", the paper would be much stronger if with experiments justify this claim. 2) "extremely deep networks take much longer to train" is not a valid reason to not conduct such exps.
This paper proposes an attention based technique to focus on relevant entities in multi agent reinforcement learning.  While the effectiveness of the proposed method is demonstrated on some tasks, there remain major concerns including the following: 1. It is not sufficiently convincing that the proposed method performs well in more complex domains 2. Novelty over Agarwal et al. and MAAC is rather minor
This paper proposes a phrase based attention method to model word n grams (as opposed to single words) as the basic attention units. Multi headed phrasal attentions are designed within the Transformer architecture to perform token to token and token to phrase mappings. Some improvements are shown in English German, English Russian and English French translation tasks on the standard WMT 14 test set, and on the one billion word language modeling benchmark.  While the proposed approach is interesting and takes inspiration in the notion of phrases used in phrase based machine translation, with some positive empirical results, the technical novelty of this paper is rather limited, and the experiments could be more solid. While it is understandable that lack of computational resources made it hard to experiment with larger models (e.g. Transformer big), perhaps it would be interesting to try on language pairs with fewer resources (smaller datasets), where base models are more competitive.
The authors propose to decompose control in a POMDP into learning a model of the environment (via a VRNN) and learning a feed forward policy that has access to both the environment and environment model. They argue that learning the recurrent environment model is easier than learning a recurrent policy. They demonstrate improved performance over existing state of the art approaches on several PO tasks.  Reviewers found the motivation for the proposed approach convincing and the experimental results proved the effectiveness of the method. The authors response resolved reviewers concerns, so as a result, I recommend acceptance.
This paper proposes a simple change to Transformer architecture to improve efficiency. While the reviewers appreciate the writing, all the reviewers agree that the novelty and contributions of the paper are limited both in the problem being solved by the paper and the level of experiments in it. Authors did not respond to reviewer s comments. Hence I recommend rejection.
The work extends the line of work based on value iteration networks. The main goal is to extend VINS to continuous and partially observable state spaces. The approach combines self supervised contrastive learning and graph representation learning with VINs to address these issues. Reviewers liked the premise of the paper and had several follow up clarifications.  The authors provided the rebuttal and addressed some of the concerns. However, upon post rebuttal discussion, the reviewers decided to maintain their score. While everyone recommended weak acceptance, no one championed the paper. This was primarily due to the concerns in the empirical analysis. It is not clear that XLVINs are clearly outperforming VINs and Graph VINs in all settings. All baselines are not present in all the environments, so it is difficult to draw a consistent conclusion. The paper is in a good state but not fully polished to infer clear conclusions about the effectiveness of the proposed approach. Please refer to the feedback below for more details. We believe strengthening the experimental results section will turn this paper into a very strong submission.
The paper looks at the soft constrained RL techniques and proposes a meta gradient approach. One of the biggest problems with the Lagrange Optimization based CMDP algorithms is that the optimization of the Lagrange multiplier is tricky The proposed solution and empirical results have promise. The reviewers broadly agree on their evaluation and the major concerns on comprehension, additional experiments and as well as comparison with baselines have been addressed in the rebuttal.     Convergence rate and quality of fixed point reached. The authors mention convergence to local optima but omit the quality of this solution from perspective of safety. It would be useful to include a discussion on the topic, with potential references to concurrent work.  Other relevant and concurrent papers to potentially take note of:   Risk Averse Offline Reinforcement Learning (https://openreview.net/forum?id TBIzh9b5eaz)   Distributional Reinforcement Learning for Risk Sensitive Policies (https://openreview.net/forum?id 19drPzGV691)    Conservative Safety Critics for Exploration (https://openreview.net/forum?id iaO86DUuKi)  I would recommend acceptance of the paper based on empirical results, conditional on release of sufficiently documented and easy to use implementation.  Given the fact that the main argument is empirical utility of the method, it would be limit the impact of this work if readers cannot readily build on this method. 
There are two main contributions in this paper. First, the use of NN from the same cluster as “views” of the data as understood in classical contrastive learning. Second, the use of additional augmentation techniques, namely cutMix and multi resolution. The reviewers noted that the paper is written well and easy to understand, that the ablation study is conducted well and that the model shows good empirical performance on several tasks.   At the same time, the somewhat limited novelty of the paper was also discussed. As noted by R4, all aspects of the present paper have been discussed in previous work. The difference with previously published clustering based SSL methods was also not very clear. This was discussed in the rebuttal but without strong evidence supporting the claims. Moreover, the ablation study is conducted on models that are trained for 200 epochs. While this is understandable from a pragmatic point of view, the conclusions may be completely different when the model is fully optimised.   Because of all the points raised in the discussions, this paper is a too close to borderline to be accepted. We recommend the authors improve the manuscript given the feedback provided in the reviews and discussion and resubmit to another venue.
The paper tries to find better semi positive definite (SPD) manifold networks using neural architecture search. However, as pointed out by the reviewers, the paper has a few weaknesses: (a) it lacks in novelty, (b) it lacks in experiments that are mentioned in the SOTA papers, (c) the experiments should be performed with the same model complexity for fairness.  
The paper proposes learning a latent embedding for image manipulation for PixelCNN by using Fisher scores projected to a low dimensional space. The reviewers have several concerns about this paper: * Novelty * Random projection doesn’t learn useful representation * Weak evaluations Since two expert reviewers are negative about this paper, I cannot recommend acceptance at this stage. 
This paper studies the loss landscape of domain adversarial neural networks for domain adaptation. First, the authors show that smooth minima with respect to adversarial loss leads to sub optimal generalization on the target domain. Then, they suggest to enforce smoothness only with respect to the task loss. 3 reviewers are on negative sides, and 1 reviewer is on a positive side. All negative reviewers interacted with authors in the discussion period. After the discussion period, even the positive reviewer agreed negative comments of other reviewers and declined to champion this paper.    AC thinks that this is a borderline paper; the proposed claims (both theoretic and empirical) are interesting and there is no critical weakness of this paper. However, the results are not super excited, as evidenced by 3 negative reviewers. In particular, AC agrees with negative comments of reviewers on limited novelty and lack of strong theoretical motivation for the proposed scheme. AC also thinks the performance improvements in experiments are not that significant. Furthermore, the problem scope is narrow, i.e., the authors study a certain property (smoothness) of a special algorithm (adversarial training) for domain adaptation that is a particular way for domain generalization. Hence, the impact to the community can be not significant as well. Considering all aspects, AC is a bit toward to suggesting rejection.
This paper proposed a new method improving online reinforcement learning using offline datasets. Three reviewers suggested (borderline) acceptance and two did rejection. The main concerns of reviewers are (a) limited/incremental novelty (from all reviewers) and (b) limited experiments (from three reviewers). AC also agrees that the authors  response for novelty beyond the prior works, e.g., AWR (and CRR), is not convincing enough (although their goals/settings are different). AC also thinks that more discussion, analysis and results when offline datasets are poor (e.g., far from experts) are necessary to meet the high standard of ICLR (the authors provided some, but AC thinks it is not convincing enough). Hence, AC recommend rejection. 
This paper proposed an augmentation construction to mitigate the double descent. For any pairs of data points, the constructed input is simply concatenation of two inputs and the constructed label is the average of their corresponding labels. The authors further empirically show that this would mitigate double descent.  Reviewers unanimously like the main idea of the paper but they have other major concerns about this work. The main concern is that we already know double descent is not a practical issue since it can be mitigated by early stopping or proper regularization (Nakkiran 20 ). Therefore, the main benefit from this paper could come from a better understanding of double descent using the observations from this construction. However, the paper does not provide us with insightful theoretical or empirical findings beyond the main observation. There are a couple of other concerns as well about discussions around #samples and the fact that the proposed construction is not i.i.d. and also lack of proper discussion about the relationship between the proposed construction and regularization techniques.  Unfortunately, authors did not responded to reviewers concerns. Nonetheless, I encourage authors to read reviewers  specific feedbacks, incorporate them and resubmit their work.  Given the above concerns, I recommend rejecting the paper.
The authors study dropout for matrix sensing and deep learning, and show that dropout induces a data dependent regularizer in both cases. In both cases, dropout controls quantities that yield generalization bounds.   Reviewers raised several concerns, and several of these were vehemently rebutted. The rhetoric of the back and forth slid into unfortunate territory, in my opinion, and I d prefer not to see this sort of thing happen. On the one hand, I can sympathize with the reviewers trying to argue that (un)related work is not related work. On the other hand, it s best to be generous, or you run into this sort of mess.  In the end, even the expert reviewers were unswayed. I suspect the next version of this paper may land more smoothly.  While many of the technical issues are rebutted, one that caught my attention pertained to the empirical work. Reviewer #4 noticed that the empirical evaluations do not meet the sample complexity requirements for the bounds to be valid (nevermind loose). The response suggests this is simply a fact of making the bounds looser, but I suspect it may also change their form in this regime, potentially erasing the empirical findings. I suggest the authors carefully consider whether all assumptions are met, and relay this more carefully to readers.
This paper introduces a graph neural network (GNN) based on the finite element method (FEM) for learning partial differential equations from data. The proposed finite element network is based on a piecewise linear function approximation and a message passing GNN for dynamics  prediction. The authors also propose a method to incorporate inductive bias when learning the dynamical model, e.g. including a convection component.  The paper received three clear accept and one weak accept recommendations. The reviewers discussed the possible extensions of the method, and also raise several concerns regarding experiments, e.g. the added value of a synthetic dataset, implementation tricks or hyper parameter settings. The rebuttal did a good job in answering reviewers  concerns: after rebuttal, there was a consensus among reviewers to accept the paper.  The AC s own readings confirmed the reviewers  recommendations. The paper is well written and introduces solid contribution at the frontier of GNNs and finite elements methods, especially a pioneer graph based model for spatio temporal forecasting derived from FEM. Therefore, the AC recommends acceptance.
This paper analyzes mathematically why weights of trained networks can be replaced with ternary weights without much loss in accuracy. Understanding this is an important problem, as binary or ternary weights can be much more efficient on limited hardware, and we ve seen much empirical success of binarization schemes. This paper shows that the continuous angles and dot products are well approximated in the discretized network. The paper concludes with an input rotation trick to fix discretization failures in the first layer.  Overall, the contribution seems substantial, and the reviewers haven t found any significant issues. One reviewer wasn t convinced of the problem s importance, but I disagree here. I think the paper will plausibly be helpful for guiding architectural and algorithmic decisions. I recommend acceptance. 
The authors develop a novel framework for certifying the robustness of RL agents against data poisoning attacks. They obtain lower bounds on the cumulative reward for several benchmark tasks.  Reviewers had concerns about certain organizational and technical aspects of the paper, but these were addressed well in the discussion phase and author responses. Hence, I recommend acceptance. However, I would urge the authors to incorporate points from the discussion phase into the revised version, in particular the discussion with reviewers xuEG and RQX2.
Pros:   Provides a practical technique which can dramatically speed up PDE solving   this is an important and widely applicable contribution.   Paper is simultaneously clearly written and mathematically sophisticated.   The experimental results as impressive.  Cons:   There were concerns that the paper lacks novelty compared to Li et al 2020b, where the underlying theoretical framework was developed. The primary novelty would seem to be:     using Fourier transforms as the specific neural operator     the strength of the experimental results  Overall, I recommend acceptance. I believe the techniques in this paper will be practically useful for future research.
This paper introduces a meta learning approach to "amalgamate" optimizers. The reviewers all found the idea interesting and unanimously found it to be acceptable for publication. In particular, I appreciate that the authors expanded their results to include more larger problems. One of the outstanding questions that would be interesting to address in future work is the use of tuned learning rate schedules.
This paper proposes a method of risk sensitive multi agent reinforcement learning in cooperative settings.  The proposed method introduces several new ideas, but they are not theoretically well founded, which has caused many confusions among the reviewers.  Although some of the confusions are resolved through discussion, there remain major concerns about the validity of the method.  
This paper proposes a layer wise adaptive aggregation method for federated learning that seeks to reduce the communication cost. The frequency of aggregation is adjusted separately for each layer of the model that is being trained. The number of iterations $\tau$ after which each layer s parameters are averaged across clients is multiplied by a factor $\phi$ depending on the magnitude of changes to the parameters at each layer. The paper gives a convergence analysis of the proposed method and provides experimental results to demonstrate its effectiveness in reducing communication without compromising accuracy.    Reviewer 7ks6 found some errors in the convergence analysis that were fixed by the authors in the discussion period. Reviewer YGZf increased their score to 5 after the discussion with the authors. The reviewers also gave the following suggestions to improve the paper: 1) Showing convergence curves to demonstrate that the communication reduction does not come at the cost of a slowdown in convergence. 2) The results presented in the paper shows only a small communication reduction for many of the layers. Perhaps the strategy can be improved in order to boost the communication reduction. 2) Use a more realistic model to calculate the communication cost so as to account for network delays and other costs rather than just considering the number of parameters communicated.  The scores are split on this paper. While one of the reviewers recommends acceptance, three others say that the paper is below the acceptance threshold. So I recommend a rejection while noting that the paper is close to the borderline.
The paper proposes to deep neural network models with elements of the weight from algebras, and considers a wide range of algebras and large scale promising experiments. The paper raised a heated discussion.  Pros:     Using algebras, one can hope for more efficient architectures     Numerical experiments on a wide range of problems  Cons:     The theoretical grounding provided in the current version of the paper is not sufficient. The study is empirical (nothing wrong about it), but there is no clear understanding/explanation of why particular choice is better than another, and also why it works in the particular setup.     The title does not reflect the content of the paper. It is too broad, and also in some sense “provocative”. The reader expects something much more significant from it.    Experiment setup: the resulting flops/accuracy figure (main result, Figure 1) does not contain error bars.  I.e., the accuracies should be averaged over several random seeds in order to guarantee the resulting metrics. Also, this figure does not show a clear advantage over the ResNet 50 baseline.
Summary of the paper: This work considers the problem of generating sets and graphs conditioned on a latent representation (a.k.a. one shot set generation) and makes two contributions.  First, it provides sufficient conditions for a learning algorithm to be able to handle permutation equivariance (the (F, l  ) equivariance). Second, it proposes Top n, an approach for set generation that builds on the set generation method proposed by [1]. Top n first generates an "angle vector" from a latent representation of the query vector, and then uses cosine similarity to select the closest n elements from a learnable reference set.   The authors compare their method with competing methods for set generation, including MLP based generation, random iid generation, and First n creation. Their approach improves over these baselines on SetMNIST, synthetic molecule like 3D structures, and the QM9 dataset.   Summary of discussions: The authors engaged extensively with the reviewers during the response period and were able to address significant reviewer concerns. While the reviewers are overall positive about the paper, it is expected that the authors will address some major concerns in the final camera ready version. These include the lack of experiments on tasks used by [1], comparison with recursive methods, and discrepancies in TSPN results. Finally, the utility of the (F, l) equivariance is unclear, as most existing generative models already satisfy these conditions (as mentioned in authors  discussions with reviewer tgJk). Thus, the authors should adjust their claims accordingly and add necessary clarifications.
The reviewers are unanimous in finding the work in this paper highly novel and significant.  They have provided detailed discussions to back up this assessment.  The reviewer comments surprisingly included a critique that  "the scientific content of the work has critical conceptual flaws" (!)  However, the author rebuttal persuaded the reviewers that the concerns were largely addressed.
Multiple reviewers had concerns about the clarity of the presentation and the significance of the results. 
The reviewers agree that this is a reasonable paper but somewhat derivative. The authors discussed the contribution further in the rebuttal, but even in light of their comments, I consider the significance of this work too low for acceptance.
This work presents a principled objective function for large margin learning. Specifically, it introduces class margin and sample margin, both of which it aims to promote. It also derives a generalized margin softmax loss which to draw general conclusions on the existing margin based losses. The effectiveness of the proposed theory is empirically verified in visual classification, imbalanced classification, person re identification, and face verification.  The reviewers initially raised some concerns, but most of them were well addressed in the rebuttal and convinced the reviewers. Specifically, pU1u was satisfied by authors  reply on Theorem 3.2 and the practical methods. pGzf appreciated clarifications around the evaluation metric used on IJB C and believes this work can improve our understanding of margin based face recognition. Finally, 3YiD had some reservations about number of parameters which got clarified by the authors.   In sum, all post rebuttal ratings fall in the accept zone, and the reviewers find the paper interesting and insightful. In concordance with them, I recommend this paper for publication. Please make sure to include suggestions made by reviewers in the camera ready version.
This paper extends neural processes (NPs) to the multi task setting (MTNPs). The approach uses a hierarchical Bayesian construction, where the latent variables of an NP are conditioned on a set of global task specific context variables. This allows the NP to share knowledge across related tasks.  There were a few issues raised in the reviews. Consistently, the reviewers noted that the writing could be improved. There were variables, like the context variables M, that lacked explanation. There was also confusion between the use of a Gaussian likelihood for classification vs regression. These were resolved with the author’s response and updated draft.  There were also requests for additional experiments and baselines: 1) a synthetic task, to which the authors included a 1D regression task. 2) More baselines against other multi task methods, to which the authors included a comparison to Guo et al., 2020, MTAN, and multi task Gaussian Processes.  Finally, there were questions around whether MTNPs are valid stochastic processes. This has been proven theoretically by the authors, albeit in the appendix.  Currently, this paper remains borderline. The main remaining criticisms are a) A desire for more experiments and analysis to highlight the particular strengths of the approach. b) That the approach is a straightforward extension of NPs, and may not be sufficiently novel. c) That the authors include more baselines from the recent multi task literature (Yu et al. and Sun et al.). In the end it was determined that the paper does not quite meet the bar for acceptance. I think in future submissions, it would be worthwhile to further highlight MTNP’s performance in the low data regime, where it particularly seems to do well, and to complete the full set of comparisons (e.g., Sun et al. and Yu et al.) that were requested by the reviewers.
This submission proposes "Mako", which enables continual learning when only a limited amount of labeled data is available (along with a good deal of unlabeled data). Reviewers shared concerns about difficulty in understanding which components of the proposed system were novel, especially given that the most important components seemed to be proposed in past work. Reviewers also had difficulty getting insight on which parts of the system were most useful, and further requested additional experiments on harder benchmarks. There consensus was therefore to reject the paper.
The paper studies robust learning in the presence of noisy labels and proposes a new loss function called the golden symmetric loss (GSL) combining both regular cross entropy and reverse cross entropy and leveraging an estimate of the corruption matrix. The paper appears to be well written.  Pros:   Good range of application domains (both vision and text).   Learning with noisy labels is an important practical problem.   Theoretical guarantees for the procedure using framework from recent work.  Cons:   Limited novelty as the method appears to be a weighted combination of two existing ideas.    Concerns about the baselines used: why are the same baselines not being used throughout?   Having at least a few trials with mean and standard deviation for the experiments would make the conclusions stronger.  Overall, the limited novelty combined with the concerns about the empirical analysis was a key reason for rejection. 
The paper proposes an approach and specific training algorithm to defend against membership inference attacks (MIA) in machine learning models. Existing MIA attacks are relatively simple and rely on the test loss distribution at the query point and therefore the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it (in addition to the standard gradient descent step). The submission gives extensive experimental results demonstrating advantage over existing defense methods on several benchmarks. The primary limitation of the work is that it defends only against rather naive existing attacks which do not examine the model (but rely only on the loss functions).
This paper proposes Adam and Momentum optimizers, where the optimizer state variables are quantized to 8bit using block dynamics quantization. These modifications significantly improve the memory requirements of training models with many parameters (mainly, NLP models). These are useful contributions which will enable training even larger models than possible today. All reviewers were positive.
The authors present a Bayesian approach for context aggregation in neural processes based models. The article is well written, and provides a nice and comprehensive framework. The reviewers raised some issues regarding the lack of comparisons to proper baselines. The authors provided additional comparisons in the revised version. The comparisons were found satisfactory by some some reviewers, who increased their scores. Based on the revised version, I recommend acceptance.
This paper is concerned with learning in the context of so called Byzantine failures. This is relevant for for example distributed computation of gradients of mini batches and parameter updates. The paper introduces the concept and Byzantine servers and gives theoretical and practical results for algorithm for this setting.  The reviewers had a hard time evaluating this paper and the AC was unable to find an expert reviewer. Still, the feedback from the reviewers painted a clear picture that the paper did not do enough to communicate the novel concepts used in the paper.  Rejection is recommended with a strong encouragement to use the feedback to improve the paper for the next conference.
This paper proposes an interesting unified framework for meta learning with commentaries, which contains information helpful for learning about new tasks or new data points. The authors present three kinds of different instantiations, i.e., example weighting, example blending, and attention mask, and show the effectiveness with the extensive experiments. The proposed method has a potential to be used for a wide variety of tasks.
This paper propose a reparametrization approach for pruning residual networks. The proposed approach replace the skip layer connections with feedforward layers, and show the equivalence to the original network. However, the current presentation is not very clear on the advantage of the proposed approach for pruning. As two networks are equivalent, pruning the reparameterized network can be transferred to pruning the residual network. The authors need to clarify how their reparametrized network is different from the residual network when being pruned. More ablation studies are also need to better justify their claim.
The paper proposes to incorporate unsupervisedly extracted emotion related tokens/embeddings to improve sentiment classifiers trained on top of BERT. The strengths of the paper, as identified by reviewers, are in the importance of the problem, a relatively easy to reproduce method, and a clear write up. However, all the reviewers identify several major weaknesses, including the lack of a clear research question, unclear contribution, limited novelty of the method, missing baselines, and relatively small gains in the downstream task of sentiment analysis. In sum, all the reviewers agree that the draft is not yet ready for publication.
The reviewers found the aim of the paper interesting (to connect representation quality with adversarial examples). However, the reviewers consistently pointed out writing issues, such as inaccurate or unsubstantiated claims, which are not appropriate for a scientific venue. The reviewers also found the experiments, which are on simple datasets, unconvincing.
This paper proposes a method to solve regression without correspondence.  The problem is well motivated, and the proposed method is technically sound. The motivation, organization, and presentation of the paper are very clear.  Reviewers’ suggestions to further improve the paper (e.g., clarifications on initialization, comparison and discussion with with EM, AD, etc) were adequately incorporated to the revised manuscript. 
The paper is tackling an important open problem.  AnonReviewer3 identified some technical issues that led them to rate the manuscript 5 (i.e., just below the acceptance threshold). Many of these issues are resolved by the reviewer in their review, and the author response makes it clear that these fixes are indeed correct.  However, other issues that the reviewer raises are not provided with solutions.  The authors address these points, but in one case at least (regarding w_infinity), I find the new text somewhat hand waivy. Regardless, I m inclined to accept the paper because the issues seem to be straightforward. Ultimately, the authors are responsible for the correctness of the results.
Though some concepts discussed in the submission are interesting, there are many major concerns: there is a lack of literature review, comparison experiments with the state of the art methods are missing, the technical novelty of the proposed method is very limited.  In the rebuttal, the authors agreed with reviewers  comments and did not provide responses to address reviewers  concerns.  Therefore, based on its current form, this submission does not meet the standard of publication at ICLR.
This paper has conflicting reviews with no strong advocate.  One of the positive reviewers states the caveat that paper is "very dense to read and needs to be improved".  Having looked at the paper myself I would agree with this criticism.  One of the negative reviewers states that the paper gives "an incremental variant of the NLM model".  I am less confident in this judgement.  However, I find the density of the paper and the use of synthetic data to be significant drawbacks. With the lack of any real champions for the paper I do not see a path to acceptance.
This paper comprehensively evaluated 18 different performance predictors on ten combinations of metrics, devices, network types, and training tasks for NAS. While evaluating and comparing different prediction models is not itself novel, the authors provided many insights that are potentially interesting to future NAS developments.   Reviewer reactions to this paper are rather mediocre and lukewarm. It is in general consensus that this work gives a good empirical analysis on hardware metric predictors for NAS, but the novelty is low and it is perhaps a bit incremental (e.g., nothing "shockingly new" was revealed, and observations are mostly "as expected"). Despite the authors improving the paper during rebuttal with new plots/tables, there remain to be unaddressed comments, e.g., adding experiments that run BO / evolution / etc with different hardware predictors and comparing the quality of the Pareto front. Those missed points were also raised in the private discussion.   After personally reading this paper, AC sides with most reviewers that this paper lacks true novelty nor technical excitement. While the empirical study is valuable, it perhaps suits venues other than ICLR, e.g., the NeurIPS benchmark track.
This paper claims to present a model agnostic continual learning framework which uses a queue to work with delayed feedback. All reviewers agree that the paper is difficult to follow. I also have a difficult time reading the paper.   In addition, all reviewers mentioned there is no baseline in the experiments, which makes it difficult to empirically analyze the strengths and weaknesses of the proposed model. R2 and R3 also have some concerns regarding the motivation and claim made in the paper, especially in relation to previous work in this area.  The authors did not respond to any of the concerns raised by the reviewers. It is very clear that the paper is not ready for publication at a venue such as ICLR at the current state, so I recommend rejecting the paper.
The submission studies the problem of geolocalizing a city based on geometric information encoded in so called "lean" images.  The reviewers were unanimous in their opinion that the submission does not meet the threshold for publication at ICLR.  Concerns included quality of writing, novelty with respect to existing literature (in particular see Review #2), and limited validation on one geographic area.  No rebuttal was provided.
A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented. The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games.   Reviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers.  This is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new. It should be accepted for poster presentation. 
Adversarial defense is a tricky subject, and the authors are to be commended for their novel approach to this problem. The reviewers all agree that there is promise in this approach. However, after reviewing the discussion, they have all come to the conclusion that the robustness of your generative model needs to be more thoroughly explored. Regarding gradient masking, there are other attacks like a manifold attack that use gradients that can be explored as well. Regarding SPSA, it would be helpful perhaps to also include other numerical gradient attacks to ensure that SPSA is stronger and working as intended.  Essentially, the reviewers would all like to see a more streamlined version of this paper that removes any doubt about the efficacy of the generative approach. Once that is established, additional properties and features can be explored.  Also note that for the purposes of these reviews and discussion, Schott et al. was considered as concurrent work and not prior work. 
This paper proposes an autoregressive framework that combines RNN and local linear component for the problem of meta forecasting of time series. The linear model can domain adapt to different time series while the RNN component is shared across series. Reviewers thought the problem was important, the paper was generally clear and the experiments extensive. However they found the significance to be limited and all took issue with some of the ways that the comparisons were done. FZbR also raised the issue of complexity of the matrix inversion component of the method.  I believe this paper does fall on the rejection side of the fence due to the issues of complexity, significance and evaluations. With some development, the paper could certainly be ready for acceptance.
Dear Authors,  The paper was received nicely and discussed during the rebuttal period. The current consensus suggests the paper be accepted, but could have another round of revisions before it gets published    Definition of sparsity within theoretical results + clarity of results. This seems to be the main concern by one of the reviewers.    The reviewers acknowledged that some of the concerns raised could be found somewhere in the appendix, which raises further the concern of the presentation of the results: reviewers suggest a more focused and proper dissemination of the results (main theorems in main text + explanation of the results obtained, etc), which requires another round of revisions and reviewing.   Best AC
The paper presents a layered image generation model  (e.g., foreground vs background) using GANs. The high level idea is interesting, but novelty is somewhat limited. For example, layered generation with VAE/GAN has been explored in Yan et al. 2016 (VAEs) and Vondrick et al. 2016 (GANs). In addition, there are earlier works for unsupervised learning of foreground/background generative models (e.g., Le Roux et al., Sohn et al.). Another critical problem is that only qualitative results on relatively simple datasets (e.g., MNIST, SVHN, CelebA) are provided as experimental results. More quantitative evaluations and additional experiments on more challenging datasets will strengthen the paper.  * N. Le Roux, N. Heess, J. Shotton, J. Winn; Learning a generative model of images by factoring appearance and shape; Neural Computation 23(3): 593 650, 2011. ** Sohn, K., Zhou, G., Lee, C., & Lee, H. Learning and selecting features jointly with point wise gated Boltzmann machines. ICML 2013. 
The paper extends previous work on asymmetric self play by introducing a novel behavior cloning loss (referred to as ABC). The zero shot results are impressive and demonstrate that the proposed curriculum learning approach pushes the state of the art. The reviewers acknowledge these contributions. The pros of the paper are well summarized by R2,     The experimental results are very encouraging.   The analysis of the method helps to understand which components are important.   The evaluation on the hold out tasks is very impressive and pushes the state of the art.   The paper is well written and very easy to follow, the illustrations are informative and appealing.   Although this approach is based on previous work on asymmetric self play, the authors clearly describe the contributions of this work (training clearly from self play, zero shot generalization).  R1, R2, R5 recommend accepting the paper with scores of 7, 7, 6. R1 expressed that he is not confident about the paper. R4 recommends acceptance with a score of 6. However, R4 also expresses the concern for real world applicability, "the sim real gap with respect to applicability to robotics is still a major concern IMO. I have updated my score accordingly." The sim to real gap is a concern due to knowledge of perfect state information and the assumption of resets.   Based on confident reviews of R2, R4, and R5, and the impressive zero shot results, ordinarily, I would recommend the paper to be accepted. However, unfortunately, both the authors and reviewers missed a comparison to prior work, which I detail below. While the current paper makes a good case for zero shot generalization, it does not compare to previous approaches that also exhibits zero shot generalization. For instance, Li et al. ICRA 2020 (https://arxiv.org/abs/1912.11032) show that using a simple curriculum that depends on the number of manipulated objects + Graph Neural Networks can generalize very well to unseen tasks. E.g., the results reported in the paper demonstrate that a policy trained on 2/3 blocks generalizes and can stack many more blocks. Their policy learns to stack 6 7 blocks, whereas the paper under review can only stack up to 3 blocks (Figure 8).   The authors mention in Section 5.2 of their paper, "The curriculum:distribution baseline, which slowly increases the proportion of pick and place and stacking goals in the training distribution, fails to acquire any skill. The curriculum:full baseline incorporates all hand designed curricula yet still cannot learn how to pick up or stack blocks. We have spent a decent amount of time iterating and improving these baselines but found it especially difficult to develop a scheme good enough to compete with asymmetric self play."  This is at odds with results in Li et al.   This reason for dissonance is that good generalization can be achieved by improving two separate components   the neural network architecture or the learning curriculum. This paper shows good generalization with weak neural net architectures + a good curriculum learning method. It is unclear to me how critical the self play method would be with a stronger architecture such as a graph network which is arguable more appropriate for the set of tasks presented in the paper. I would like to see if the curriculum is necessary (i.e., complements a stronger architecture) or is it just a replacement for alternate neural network architecture. Without such a study, this paper should not be accepted, because it will add to more noise rather than advancing the field of robotic manipulation. 
This paper studies various graph measures in depth.  The paper was reviewed by three expert reviewers who complemented the ease of understanding because of clear writing. But they also expressed concerns for limited novelty, theoretical justification, and unrealistic setting. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
This paper demonstrates that for deep RL problems one can construct adversarial examples where the examples don t really need to be even better than the best opponent. Surprisingly, sometimes, the adversarial opponent is less capable than normal opponents which the victim plays successfully against, yet they can disrupt the policies. The authors present a physically realistic threat model and demonstrate that  adversarial policies can exist in this threat model.  The reviewers agree with this paper presents results (proof of concept) that is "timely" and the RL community will benefit from this result. Based on reviewers comment, I recommend to accept this paper. 
Overall, the reviewers thought this paper suggested an important problem.  However, there were many concens.  Particularly, the multiple reviewers felt it was unclear when the new approach is better than prior work. The reviewers had difficulty connecting the experiments to the paper s main claims.
The reviewers are in consensus that this manuscript falls just short of the bar. I recommend that the authors take their recommendations into consideration in revising their manuscript, with a particular focus on comparison to the state of the art.
This paper proposes a framework for privacy preserving training of neural networks within a Trusted Execution Environment (TEE) such as Intel SGX. The reviewers found that this is a valuable research directions, but found that there were significant flaws in the experimental setup that need to be addressed. In particular, the paper does not run all the experiments in the same setup, which leads to the use of scaling factor in some cases. The reviewers found that this made it difficult to make sense of the results. The writing of this paper should be streamlined, along with the experiments before resubmission.
This paper has several issues: (1) The empirical results were incomplete and hard to interpret. 1.a The paper uses non standard benchmark domains making comparisons with results in the literature very difficult. The paper does not use the same environments as related baselines they build on. Some progress on this last point was made during the discussion period well done authors! 1.b The experiments did not sweep key hyperparameters of the TrajeDi baseline, and generally did not comment on nor ablate several other potentially important hyperparameters and design choices (2) the proposed method is very similar to another called TrajeDi and the paper and results don t clearly show why the modification of TrajeDi is significant (see #1). The paper initially claimed the TrajeDi was a concurrent submission but one reviewer pointed out the work was published in May  (3) writing and structure could be improved. In addition some inaccurate statements could be cleaned up (4) The algorithm is more generally applicable beyond human AI coordination and the reviewers found it odd the paper did not focus on this  In addition, the authors did not respond to several of the reviewers responses. This made it difficult for the reviewers to increase their scores. Several reviewers found the work intriguing, but its not ready yet
This paper provides the first convergence analysis for convex model distributed training with quantized weights and gradients. It is well written and organized. Extensive experiments are carried out beyond the assumption of convex models in the theoretical study.  Analysis with weight and gradient quantization has been separately studied, and this paper provides a combined analysis, which renders the contribution incremental.   As pointed out by R2 and R3, it is somewhat unclear under which problem setting, the proposed quantized training would help improve the convergence. The authors provide clarification in the feedback. It is important to include those, together with other explanations in the feedback, in the future revision.  Another limitation pointed out by R3 is that the theoretical analysis applies to convex models only. Nevertheless, it is nice to show in experiments that deep networks training is benefitted from the gradient quantization empirically.
A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper. The results are solid and impactful. Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways. All four reviewers overwhelmingly recommended accept. I recommend that this paper be selected as an oral presentation.
The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory. The authors systematically compare three types of initialization strategies for training the recurrent models. The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to "anyone using recurrent networks on RL tasks". Empirical results on Atari and DMLab are impressive.  The reviewers noted several weaknesses in their original reviews. These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup. A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights.  The authors carefully addressed all concerns raised during the reviewing and rebuttal period. They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers. The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors.  The reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted.
The paper proposed learning partial and full equivariances from data in an end to end way. The problem studied is an important issue of existing equivariant neural networks which always assume full equivariance. However, the paper only got 3 "marginally below threshold" and 1 "marginally above threshold" even after rebuttal. The major challenges include technical parts being hard to follow due to multiple reasons, unsatisfactory paper writing (the updated version has undergone restructuring), the issue of "breaking equivariance" after multiple layers, some important experiments (such as comparing with Steerable G CNNs) missing, etc. After rebuttal only reviewer RivC raised his/her score to "marginally above threshold". Such scores are difficult to justify acceptance. The AC appreciated the authors for making great efforts on rebuttal and revising the manuscript. However, from the review comments it is clear that the paper still needs further revision (many of those clarified for the reviewers, e.g., explaining distribution for the discrete groups and adding more experiments, should be included in the manuscript). So the AC deemed that the paper is not ready for publication.
The paper presents a deep learning network architecture for (semi) supervised tabular data classification and regression problems based on a new attention mechanism between samples (rows) and features (columns). The model is compared to 10 sota methods, studied on 30 diverse datasets (10 for binary classification, 10 for multiclass classification and 10 for regression). contrastive learning approach for pre training on unlabeled data and fine tuning on a small number of labels Explainability capabilities are not presented in a very convincing way.  While the reviewers find the problem relevant, they criticise novelty and, in particular, the experimental comparison. Concerns about hyperparameter tuning of own vs. comparison methods voiced by the reviewers. While these concerns have partially been addressed in the author response, the reviewers still doubt the fairness of comparison.
Existing methods for graph clustering usually use node/edge information, but ignore graph level information. This paper proposes incorporating graph level labels into graph clustering and formulating the new problem as weakly supervised graph clustering.  The paper further proposes Gaussian Mixture Graph Convolutional Network (GMGCN) framework for the task.  Experimental results on several datasets demonstrate the effectiveness of the method.  The authors are very active in answering questions by the reviewers.  They have successfully addressed some of the issues. However, there are still questions that remain unaddressed. The submission is not of the quality of ICLR papers.  Strength * A new method is proposed. * The proposed methods outperform baseline models on the given datasets and synthetic datasets.  Weakness * The explanations are not clear enough.  Although the authors provide detailed responses to the reviews,  the problems indicated by the reviewers are still not well addressed. * The proposed method seems to be too complicated. * It is not clear why the proposed method works. * The problem studied might not be realistic.    Here is a summary of the reviewers  final comments.  * Reviewer oDis slightly increased the score。  * Reviewer r2ym says“I read responses to my concerns and others, but except for some clarifying statements and notations, authors  responses are not convincing enough. Also, while I now understand the concept of proposed work better than before, I do not think that it is explained and presented well enough.”  * Reviewer inpd says “would like to keep my original score”.
The reviewers have all recommended accepting this paper thus I am as well. Based on the reviews and the selectivity of the single track for oral presentations, I am only recommending acceptance as a poster.
Three reviewers recommend rejection and there is no rebuttal.
The reviewers unanimously agree that this paper is a strong accept; it makes important progress in developing our ability to query relational embedding  models.
This paper proposes a new approach called CoAE MLSim as a faster alternative to PDE solvers.  According to the authors, the main strength of the method is that it needs fewer training data, however, as pointed out by the reviewers, the factor is not significant enough in the current results. Another concern raised by the reviewers is that the iterative algorithm is much more expensive than only one forward pass of other ML methods. Furthermore, the reviewers criticized that the comparisons with baselines are not sufficient, and some claims in the paper were not backed up. The authors provided their rebuttals and had a long discussion with most of the reviewers. Some clarifications have been made, and some reviewers increased their scores accordingly. However, overall speaking, the major problems with the paper still remain, and the rebuttal has not successfully convinced the reviewers to turn to the positive side. Therefore, we cannot give a green light to this paper yet.
While the main idea of the paper (using a Max Ent objective on the states of an MDP) was considered interesting, all reviewers raised the problem of clarity of the paper which needs to be drastically improved. While the writing could be improved by the revsion, these concerns could also not be fully alleviated by the rebuttal of the authors. The reviewers agreed that the paper needs rewritting in order to clarify the contribution before the paper can be published.
The authors develop a framework for off policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain. Reviewers were uniformly impressed by the work, and satisfied by the author response. Congratulations!  
This paper presents an interesting approach for training generative autoencoders with a latent space that lies on a hyperspherical subspace. However, the reviewers have raised concerns regarding the similarity of this work with several prior works and have questioned the experimental setup. Without the authors  response, we cannot situate this paper among the prior work properly. Thus, I recommend Reject at this point. 
This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions. In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure. The paper shows good performance in inducing compositional structure in two datasets.  Summarizing the reviewers  doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up. The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see. If anything, this is probably the most original paper in my pool.  Concerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time.  Finally, the authors added new text and new experiments strenghtening their conclusion during the discussion.  I am strongly in favour of accepting this paper. 
This paper explores meta learning of local plasticity rules for ANNs. The authors demonstrate that they can meta learn purely local learning rules that can generalize from one dataset to another (though with fairly low performance, it should be noted), and they provide some data suggesting that these rules lead to more robustness to adversarial images. The reviews were mixed, but some of the reviewers were very positive about it. Specifically, there are the following nice aspects of this work:  A) The meta learning scheme has interesting potential for capturing/learning biological plasticity rules, since it operates on binary sequences, which appears to be a novel approach that could help to explain things like STDP rules.  B) It is encouraging to see that the learning rules can generalise to new tasks, even if the performance isn t great.  C) The authors provide some interesting analytical results on convergence of the rules for the output layer.  However, the paper suffers from some significant issues:  1) The authors do not adequately evaluate the learned rules. Specifically:    The comparison to GD in Fig. 2 is not providing an accurate reflection of GD learning capabilities, since a simple delta rule applied directly to pixels can achieve better than 90% accuracy on MNIST. Thus, the claim that the learned rules are "competitive with GD" is clearly false.    The authors do not compare to any unsupervised learning rules, despite the fact that the recurrent rules are not receiving information about the labels, and are thus really a form of unsupervised learning.    There are almost no results regarding the nature of the recurrent rules that are learned, either experimental or analytical. Given positive point (A) above, this is particularly unfortunate and misses a potential key insight for the paper.  2) The authors do not situate their work adequately within the meta learning for biologically plausible rules field. There are no experimental comparisons to any other meta learning approaches herein. Moreover, they do not compare to any known biological rules, nor papers that attempt to meta learn them. Specifically, several papers have come out in recent years that should be compared to here:  https://proceedings.neurips.cc/paper/2020/file/f291e10ec3263bd7724556d62e70e25d Paper.pdf https://www.biorxiv.org/content/10.1101/2019.12.30.891184v1.full.pdf https://proceedings.neurips.cc/paper/2020/file/bdbd5ebfde4934142c8a88e7a3796cd5 Paper.pdf https://openreview.net/pdf?id HJlKNmFIUB https://proceedings.neurips.cc/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222 Paper.pdf  And, the authors should consider examining the rules that are learned and how they compare to biological rules (e.g. forms of STDP), if indeed biological insights are the primary goal.  3) The paper needs to provide better motivation and analyses for the robustness results. Why explore robustness? What is the hypothesis about why these meta learned rules may provide better robustness? There is little motivation provided. Also, the authors provide very little insight into why you achieved better robustness and insufficient experimental details for readers to even infer this. This section requires far more work to provide any kind of meaningful insight to a reader. What was the nature of the representations learned? How are they different from GD learned representations? Was it related to the ideas in Theorem 4? Note: Theorem 4 is interesting, but only applies to a specific form of output rule.  4) In general, the motivations and clarity of the paper need a lot of work. What are the authors hoping to achieve? Biological insights? Then do some analyses and comparisons to biology. More robust and generalisable ML? Then do more rigorous evaluations of performance and comparisons to other ML techniques. Some combination of both? Then make the mixed target much clearer.  5) The authors need to tidy up the paper substantially, and do better at connecting the theorems to the rest of the paper, particularly for the last 2 theorems in the appendix. Also, note, Theorems 2 & 4 appear to have no proofs.  Given the above considerations, the AC does not feel that this paper is ready for publication. This decision was reached after some discussion with the reviewers. But, the AC and the reviewers want to encourage the authors to take these comments on board to improve their paper for future submissions, as the paper is not without merit.
The authors have proposed a method for imitating a given control trajectory even if it is sparsely sampled. The method relies on a parametrized skill function and uses a triplet loss for learning a stopping metric and for a dynamics consistency loss. The method is demonstrated with real robots on a navigation task and a knot tying task. The reviewers agree that it is a novel and interesting alternative to pure RL which should inspire good discussion at the conference.
This paper establishes high probability generalization bounds of the order O(1/n) for a range of stochastic minimax problems. The reviewers agreed that results are of broad interest and the techniques are non trivial. I recommend acceptance.
This paper presents yet another scheme for weight tying for compressing neural networks, which looks a lot like a less Bayesian version of recent related work, and gets good empirical results on realistic problems.  This paper is well executed and is a good contribution, but falls below the bar on 1) Discovering something new and surprising, except that this particular method (which is nice and simple and sensible) works well.  That is, it doesn t advance the conversation or open up new directions. 2) Potential impact (although it might be industrially relevant)  Also, the title is a bit overly broad given the amount of similar existing work.
This paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form. Reviewers found the paper to be clearly written, saying it "nice introduction to the topic" and noting that they "enjoyed reading this paper". In general though there was a feeling that the "substance of the work is limited". One reviewer complained that experiments were limited to small English datasets PTB and Wikitext 2 and asked why they didn t try "machine translation or speech recognition". (The author s note that they did try the Linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial) . Another felt that the "multipop model" alone was not too large a contribution. The actual experiments in the work are well done, although given the fact that the models are known there was expectation of "more "in depth" analysis of the different models". Overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area.
This paper offers flow based alignment methods for alignment of distributions in a domain adaptation setting.  While there are many positive aspects of the submission, the experimental results only weakly support the results.  The AC agrees with the critical comments mentioned by reviewer sZ2C, and in particular observes that the experimentation is not state of the art with regard to current domain adaptation literature. Unfortunately the submission is not acceptable in present form.
The initial reviews for this paper were very borderline. The authors provided detailed responses as well as a few additional results and observations. The authors  responses answered the reviewers  questions and addressed their main comments (including in the discussion of related works as well as with more in depth analysis in a new Section 5.1). Unfortunately, the reviewers did not come to a consensus.  Overall, this paper extends some current methodology for emotional classification, is well executed, and provides a reasonably thorough study. The results are somewhat in line with previous results from other fields (and notably NLP), but the authors demonstrate the efficacy of using primary multi task learning for multimodal conversational analysis.   Unfortunately, this paper also has some flaws as highlighted by the initial reviews. As stated above, the authors did provide a strong rebuttal, but given the different comments raised by the reviewers that spanned many aspects of the paper including motivation, possibly limited contribution and novelty, missing related work, somewhat shallow analysis of the results, I find that another full round of reviewing would be useful to assess the paper.  As a result, this remains a very borderline paper, and given the strong competition at this year s conference, I cannot recommend acceptance at this stage.  I suggest that the authors incorporate some of the discussions from this forum (and especially with respect to related work, new findings, and clearly defining the motivation and contribution of this work) into the next version of their paper.
The authors design a deep model architecture for learning word embeddings with better performance and/or more efficient use of parameters.  Results on language modeling and machine translation are promising.  Pros:  Interesting idea and nice results.  New model may have some independent value beyond NLP.  Cons:  Empirical comparisons could be more thorough.  For example, it is not clear (to me at least) what would be the benefits of this approach applied to whole words versus a competitor using subword units.
This is a well written paper that contributes a clear advance to the understanding of how gradient descent behaves when training deep linear models.  Reviewers were unanimously supportive.
The reviewers have provided thorough reviews of your work. I encourage you to read them carefully should you decide to resubmit it to a later conference.
Current meta learning algorithms suffer from the requirement of a large number of tasks in the meta training phase, which may not be accessible in real world environment. This paper addresses this bottleneck, introducing a cross task interpolation in addition to the existing intra task interpolation. The main idea is very simple, which can be viewed as an incremental adding up to existing augmentation methods. However, the method is well supported by nice theoretical results which highlight the relation between task interpolation and the Rademacher complexity. In fact, this is not a trivial extension of existing work.  Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. All reviewers agree to champion this paper. Congratulations on a nice work.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready.
This paper proposes a method to learn representations in MBRL by exploiting sparsity in the model to improve data efficiency. The key idea is to build a representation for which the model is invariant. The idea is quite interesting, but one weakness of the current draft is that there is a disconnect between the presented theory (linear case) and the relevant experimental setup (non linear). The paper is overall well written but would still benefit from a revision to improve clarity as pointed out by the reviewers. The experimental results are inconclusive due to the choice of weak baselines.
This paper evaluates the extent to which disentangled representations can be recovered from pre trained GANs with style based generators by finding an orthogonal basis in the space of style vectors, and then training an encoder to map images to coordinates in the resulting latent space. To construct the orthogonal basis, the authors consider 3 recently proposed methods for controllable generation, along with a newly developed generalization of one of these methods. The authors evaluate metrics for disentanglement for 4 datasets, consider an abstract visual reasoning task, and compute unfairness scores.  Reviewers expressed diverging opinions on this paper. R2 is in support of acceptance,  R3 finds the paper borderline but is leaning towards acceptance, whereas R4 is critical. R2 and R4 engaged in a relatively detailed discussion, but maintained their scores.   Having read the paper, the metareviewer feels this submission indeed has strengths and weaknesses. On the one hand, the main results are notable; it is worth reporting that disentangled representations can be recovered from pretrained GANs is a relatively straightforward manner. In this context, the metareviewer feels that some comments by R4 are more critical than is warranted. The authors do not necessarily have to show that GAN based methods uniformly improve upon VAE based methods, either in terms of disentanglement metrics or in terms of sensitivity to hyperparameters. The main claim in this submission is that GAN based methods are mostly comparable to VAE based methods, and this claim is both sufficiently notable and sufficiently supported by experimental results.   At the same time, this submission is not without flaws.  The writing is on the rough side, and as R4 notes the authors have removed all white space between paragraphs. The metareviewer also feels it is not satisfactory to show a box plot for GAN based methods in Figure 2 and ask the reader to compare these plots to the violin plots for VAE based methods in the Locatello paper. The authors need to find a way to make a more direct comparison here. R4 s comments about the comparison in the abstract reasoning setting are also well taken –– here the baseline employs standard (entangled) models, so it is unclear what conclusions we should draw from this experiment. Similarly the unfairness results once again appeal to an indirect comparison to results in the  Locatello paper on this topic.  On balance, the metareviewer is inclined to say that this submission, in its current form, falls just below the threshold for acceptance. These results are clearly of note to the community and worth reporting, but the presentation has enough flaws that another round of reviews is warranted based on a revised manuscript. The metareviewer hopes to see this paper appear a conference in the (near) future. 
The paper studies the effect of explicitly introducing stochastic label noise into SGD updates, showing both theoretically and empirically that this can improve model performance on datasets with "inherent" label noise. The intuition is that this helps the model escape sharp local minima, where predictions may be overconfident.   Reviewers broadly found the work to be conceptually and theoretically interesting, and the empirical results are promising. The paper is thus well posed to be of broad interest to the community.
Improving the expressiveness of GNN is an important problem in the current graph learning community. Its key idea is to generate subgraphs from the original graph, then encode the subgraphs into the message passing process of GNN. The proposed method is proven to be strictly more powerful than 1 WL. The authors also quantize how design choices such as the subgraph selection policy and equivariant neural architecture can affect the architecture’s expressive power.  After the rebuttal, all reviewers are glad to accept this submission.  During the discussion, while reviewer B3oK has shown some concerns on the concurrent works in NeurIPS 2021, it should not affect the decision of the submission once the authors have discussed them in the main text. The authors have done this in their revision, thus an acceptance (spotlight) is suggested.
This paper introduces a new layer for graph neural networks that aims to reduce the oversmoothing issue common to this model type. The reviewers find the paper well organized and easy to follow, and they recognize the importance of the problem that is addressed. However, they also identify critical errors in the mathematical derivations: the authors did not provide a response to the reviews, and hence these errors remain unaddressed. In addition, multiple reviewers indicate they find the experimental evaluation insufficient. For these reasons I m recommending rejecting this paper.
The authors propose a method—"Proto Trex"—that incorporates prototype networks into text classification architectures to facilitate model explanations via presentation of similar training examples. There was agreement that the direction here is promising and the work contains some nice ideas and a good initial set of evaluations. However, the presentation can be improved substantially to better situate the contribution with respect to related work (and clarify the specific contributions on offer here), and to clarify the key technical details of the proposed approach.
This paper proposes an algorithm for achieving disentangled representations by encouraging low mutual information between features at each layer, rather than only at the encoder output, and proposes a neural architecture for learning. Empirically, the proposed method achieves good disentanglement metric and likelihood (reconstruction error) in comparison to prior methods. The reviewers think that the methodology is natural and novel to their knowledge, and are happy with the detailed execution. The authors are encouraged to improve the presentation of the paper, by providing rigorous formulation of the "Markov chains" to avoid confusions, justification of the independence assumptions behind them, and more in depth discussions of the learning objectives.
This paper addresses the problem of unsupervised domain adaptation and proposes explicit modeling of the source and target feature distributions to aid in cross domain alignment.   The reviewers all recommended rejection of this work. Though they all understood the paper’s position of explicit feature distribution modeling, there was a lack of understanding as to why this explicit modeling should be superior to the common implicit modeling done in related literature. As some reviewers raised concern that the empirical performance of the proposed approach was marginally better than competing methods, this experimental evidence alone was not sufficient justification of the explicit modeling. There was also a secondary concern about whether the two proposed loss functions were simultaneously necessary.   Overall, after reading the reviewers and authors comments, the AC recommends this paper not be accepted. 
This paper studies the problem of training tiny networks, by proposing a new training method called Network Augmentation (NetAug). The main challenge for training tiny networks lies in underfitting, which data augmentation and dropout etc. regularizations may suffer from for tiny networks. To overcome this hurdle, the proposed method first embeds or augments the tiny network as a subnet into a larger network, mostly by enlarging the width; then the gradients from the larger network are used as additional or auxiliary supervision. With this training strategy, the tiny model can perform better than the conventional training scheme on ImageNet and several downstream tasks. The proposed method is simple to implement and complementary with other techniques such as knowledge distillation and pruning. While there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. Despite that there are existing works sharing a similar idea of NetAug for large model training, which slightly hurts the novelty of this work, the majority of reviewers still like the idea and suggest to accept the paper.
`This paper tackles the problem of learning with one hidden layer non overlapping conv net for XOR detection problem. For this problem the paper shows that over parametrized models perform better, giving insights into why larger neural networks generalize better   an interesting question to study. However reviews opined that the setting considered in this paper is too specific to this XOR problem and the simplified network architecture,  and the techniques are not generalizable to other models. Generalizing these results to more complex architectures or other learning problems will make the paper more interesting. 
This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in ICLR 2020. 
This paper proposes a novel technique for matrix completion, using graphical neighborhood structure to side step the need for any side information.  Post rebuttal, the reviewers converged on a unanimous decision to accept. The authors are encouraged to review to address reviewer comments.
The paper introduces a new scheme for compressing gradients in distributed learning which is argued to exploit temporal correlation.  The paper received very detailed reviews and generated a lot of discussions (thank you to the reviewers for the amazing job).  Many reviewers acknowledge that this is interesting work, a simple and potentially useful algorithm but pointed out many problems with discussion, theoretical analysis, and experiments (e.g., it was not clear to R4 and R3 that these are temporal correlations which are beneficial rather  lossiness ). Some of these issues were addressed and the current version is currently much stronger than the initial submission (and stronger than the low average scores suggest). Still, the reviewers do not believe that the paper is ready for publication and I share this sentiment. I would strongly encourage the authors to invest more effort in addressing the reviewers  comments and resubmit the work to one of the upcoming top conferences.
The paper introduces a new federated learning algorithm that ensures that the objective function optimized on each device is asymptotically consistent with the global loss function.    Both theoretical analysis and empirical results, evaluating communication efficiency, demonstrate the advantages of the proposed FedDyn method over the baselines.   All the reviewers recommend accepting the paper. To summarize the discussion:    R1 mentioned a very recent (NeurIPS 20) related paper and asks several questions. I believe that the authors nicely answered the questions and discussed the relation to the previous paper in detail.      R2 mentioned that the paper focuses solely on minimizing communication costs, ignoring costs of local computations. The authors argued that the local computation costs are comparable to those of the baselines, and, in general, communication costs are the main source of computation energy costs (pointing to previous work), and, thus, are a natural objective to optimize.  I believe that this adequately addressed this (and other) reviewer s concerns and the reviewer kept their score unchanged.    R3 had several concerns, which according to the reviewer were addressed in the rebuttal (they increased the score).    R4 points out several limitations of the method and theoretical analysis and believes that the rebuttal did not quite address the concerns. Nevertheless, remains positive about the paper, and believes that the shortcomings can be addressed in follow up work.  We share the reviewers  sentiment: it is a very nice and interesting paper, and should be accepted. 
This paper proposes a GAN for video generation based on stagewise training over different resolutions, addressing scalability issues with previous approaches. Reviewers noted that the paper is clearly written, proposes a method that improves upon the DVD GAN architecture by reducing training time and memory consumption, and has competitive quantitative results.  On the other hand, the more negative reviewers are concerned that the empirical improvements demonstrated are somewhat incremental, and that there is not much novelty as the proposed approach is similar to other methods that decompose the generation process into multiple stages at different temporal window lengths and/or spatial resolutions. The authors argue that these criticisms are subjective and non actionable. I sympathize with their frustration, but an acceptance decision for a competitive conference like ICLR does involve some subjective judgment as to whether the method and/or results meet a high bar beyond mere correctness. For this submission that s a close call, but between the novelty/incrementality concerns and the other more minor issues raised by reviewers (e.g., missing frame conditional evaluation) I believe this paper could benefit from another round of revisions and improvements and recommend rejection.  I hope the authors will consider improving the submission based on the reviewers  feedback and resubmitting to a future venue, as the paper certainly has merit. To this end I have a few concrete recommendations for the authors which could have flipped my recommendation to an accept if implemented:  * Report results in the frame conditional setting for comparison with DVD GAN and other methods that operate in this setting. * Proofread the paper more thoroughly. I noticed several typos while skimming the paper, e.g. in the theory section, the second term of eq. 6 confusingly uses $\rho$ instead of $\log$. (Relatedly, given that appendix B.1 reports that the hinge loss is used, I m not sure whether $\log$ is correct in the first place   this probably deserves further explanation or correction.) * Demonstrate/argue more convincingly (in one way or another) that SSW GAN s improved efficiency really expands the frontier of what was possible before. It is true that the 128x128/100 video samples contain 2x as many total pixels as DVD GAN s 256x256/12 samples, but this isn t a *strict* improvement as the spatial resolution is smaller, and a 2x difference leaves space for reviewers to reasonably wonder whether previous methods really couldn t have matched this if pushed. Some possible examples of this: show that SSW GAN can generate longer 256x256 videos (a strict improvement over what was possible with DVD GAN), or orders of magnitude longer (e.g., 1 minute) but still temporally coherent videos at 128x128, or videos with substantially improved subjective sample quality at the same (or higher) resolution. * The paper notes that "DVD GAN models do not unroll well and tend to produce samples that become motionless past its training horizon". If this were quantified, e.g. by additionally reporting IS/FID/FVD separately for different timestep ranges, it could make a more compelling argument in favor of SSW GAN.
This paper proposes a novel approach to include graph information into Transformers. Reviewers expressed concerns on 2 main issues    1) The exact architecture proposed in the paper is not well motivated. In words of one of the reviewers  I still do not understand why the authors learn the spectral GCN filter weights from the attention matrix of the transformer, which can have a completely different sparsity pattern than the input graph, instead of learning the filter weights from the graph itself, e.g., by using a GNN.  . Authors tried to provide an explanation in the response however I think it needs to be made much more rigorous for it to be well motivated.  2) The interplay between existing position encoding schemes and the proposed method. This point also confused couple of reviewers as the empirical results seem to be strongly influenced by the choices of position encoding. Authors, I think did a great job in addressing this concern by providing additional results during the response period.   Given the weak experimental results and lack of clear motivations I think the paper is not currently ready for acceptance.
The paper discusses the relevant topic of unsupervised meta learning in an RL setting. The topic is an interesting one, but the writing and motivation could be much clearer. I advise the authors to make a few more iterations on the paper taking into account the reviewers  comments and then resubmit to a different venue.
Dear authors,  all reviewers found many interesting contributions in your paper and also pointed out some minor/major issues. In your rebuttal discussions, you addressed most of them to their satisfaction and I hope you will incorporate them carefully also in your final submission.  I hence recommend accepting this paper  
This paper develops an effective model free algorithm that achieves high sample efficiency. The empirical performance is appealing, which is comparable to model based policy optimization and significantly outperforms SAC. The paper is well written, and contains rigorous ablation studies. Weakness: the theoretical analysis is Section 3.1 is not thorough yet,  and it would be helpful to include more numerical comparisons with the Maxmin approach by Lan et al. 
The paper proposes a novel method for (diverse) client selection at each round of a federated learning procedure with the aim of improving performance in terms of convergence, learning efficiency and fairness. The main idea is to  introduce a facility location objective to quantify how representative/informative is the gradient information of a given set of clients is, and then choose a subset that maximizes this objective.  Given the monotonicity and submodularity of the proposed facility location objective, the authors have been able to provide theoretical guarantees. Experimental results on two data sets (FEMNIST and CelebA) show the effectiveness to the proposed approach and algorithm.   The reviewers had a number of concerns most of which were addressed in the authors response. The reviewers believe that the theoretical results of the paper are incremental given the prior work (see the reviews for more details); however, the reviewers (as well as myself) agree that the proposed method is novel and can provide significant practical advantage. Utilizing sub modular objectives for diverse selection is a well known (and effective approach), but I am seeing it in the context of federated learning for the first time.   My suggestion to the authors: (i) Improve the experimental section by adding a few more common data sets (such as CIFAR when data is distributed in a heterogeneous manner). CelebA and FEMNIST are not really the best data sets to try in FL (although they are commonly used). (ii) One of the reviewers had several critical comments about the theoretical results, please address those in the updated version. (iii) Please clarify in more detail how the theoretical and algorithmic contributions of there paper go beyond the recent work of (mirzasoleiman et el. 2020); (iv) iIt seems to me that the paper is missing some references on client selection in federated learning. Please revise the related work accordingly.
The paper primary theoretical contribution claim is to establish the constant size SGD converges linear to the optimal solution in non convex settings. This is shown in the interpolation regime for over parametrized situations when starting from points nearby to the optimum. The paper s empirical claim is to use relatively larger learning rates for SGD in common deep learning settings and claim that they can do well.   My recommendation is based on the overall low scores provided by the reviewers   which did not change post rebuttal. The concerns raised by the reviewers amounting to my decision recommendation is summarized below     Overall the reviewers found the connection between the theoretical results and the overall claims of the paper unconnected. The reviewers found the theoretical contribution of the local convergence weak   particularly in the context of an analysis of constant learning rates and taking into account existing work on the convex case for such results. Furthermore, the experimental contribution of the paper is incremental as the proposed algorithm is standard with just a larger than typical initial learning rates. This factor is usually searched over during Hyper Parameter sweeps in all the large scale learning setups. In this context, SGDL performing favorably, is an interesting observation but not enough of a contribution. Further the reviewers objected to the fact that SGDL does not connect with the theory presented as SGDL in experiments still uses learning rate schedules.  
The authors offer theoretical guarantees for a simplified version of the deep Q learning algorithm. However, the majority of the reviewers agree that the simplifying assumptions are so many that the results do not capture major important aspects of deep Q Learning (e.g. understanding good exploration strategies, understanding why deep nets are better approximators and not using neural net classes that are so large that can capture all non parametric functions). For justifying the paper to be called a theoretical analysis of deep Q Learning some of these aspects need to be addressed, or the motivation/title of the paper needs to be re defined. 
Three of four reviewers are in favour of accepting the paper. Some reviewers raised valid criticism regarding the derivations, interpretation of the mathematical analysis and experimental results. So clearly some aspects of the paper could and should be clarified in accordance with the points raised by the reviewers. However, all in all the paper contains enough contributions to warrant publication.   
The paper studies a robust GNN against adversarial attacks on both graph structure and node features.  The reviewers agree that the paper need to improve in terms of novelty and more technical details to meet ICLR standard.
The reviewers are in agreement that this paper could benefit further improvement. There are several areas: novelty of the proposed approach and evaluation on real world datasets (beyond just CLEVR).
This paper considers multi task RL from the perspective of an unsupervised clustering of different tasks with an EM like algorithm. The idea is evaluated on several simple and ATARI domains. We thank the reviewers for their detailed responses and revision. This work still seems a little preliminary in its current form. While the empirical results seem promising, it is generally felt that it would benefit from more extensive experiments, including further comparisons to other approaches and exploring the effects of the hyperparameters on tasks with much larger numbers of clusters. It would also be beneficial to provide some theoretical results, particularly with respect to negative transfer.
This paper presents a framework to test the accuracy and robustness of different machine learning algorithms in classifying the COVID 19 spike sequences. After reading the paper and taking into consideration the reviewing process, here are my comments:   The work is aligned to the efforts on understanding the COVID 19 pandemic.   Many concepts are not novel.   Sequences errors are not modeled in a realistic way.   The benchmark is very limited and nonlinear machine learning approaches are presented.    Many typos are presented. From the above, the paper is not suitable for aacceptance in ICLR.
This paper proposed a new approach to jointly model text and stock price information and fuse them for stock market forecasting. It encodes text and stock price information in parallel and then fuses them using a co attention transformer. According to the reviewers, the design of the model is not very well justified and seems to be a little ad hoc. The authors spent quite a few pages introducing background knowledge and the novelty of the proposed model is not sufficiently described. Some details in the experiments are missing, and it is not clear whether the results could be easily reproduced.  There are many writing issues too. As a result, we do not think the paper is ready for publication at ICLR in its current form. BTW, after the reviewers posted their comments, the authors did not submit their rebuttals.
this submission demonstrates an existing loop hole (?) in rushing out new neural language models by carefully (and expensively) running hyperparameter tuning of baseline approaches. i feel this is an important contribution, but as pointed out by some reviewers, i would have liked to see whether the conclusion stands even with a more realistic data (as pointed out by some in the field quite harshly, perplexity on PTB should not be considered seriously, and i believe the same for the other two corpora used in this submission.) that said, it s an important paper in general which will work as an alarm to the current practice in the field, and i recommend it to be accepted.
The paper proposes a data augmentation technique for image classification which consists in averaging two input images and using the label of one of them. The method is shown to outperform the baseline on the image classification task, the but evaluation doesn’t extend beyond that (to other tasks or alternative augmentation mechanisms); theoretical justification is also lacking.
This paper focuses on understanding the role of model architecture on convergence behavior and in particular on the speed of training. The authors study the gradient flow of training via studying an ODE s coefficient matrix H. They study the effect of H in terms of possible paths in the network. The reviewers all agreed that characterizing the behavior in terms of path is nice. However, they had concerns about novelty with respect to existing work on NTK. Other comments by reviewers include (1) poor literature review (2) subpar exposition and (3) hand wavy and rack of rigor in some results. While some of these concerns were alleviated during the discussion. Reviewers were not fully satisfied.  I general agree with the overall assessment of the reviewers. The paper has some interesting ideas but suffers from lack of clarity and rigor. Therefore, I can not recommend acceptance in the current form.
In the paper, it introduces a forget and relearn framework to the iterative learning algorithm. It provides serval new insight that forgetting could be favorable to learning and validates the insights via image classification and language tasks. The idea is novel and inspiring. Although there are some debates on the experiment and the generality of the proposed method, I think authors answered those questions decently and many researchers would be interested in this direction.
With scores of 7 7 6  and the justification below the AC recommends acceptance.  One of the reviewers summarizes why this is a good paper as follows:  "This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances:   This gives a more unified way of understanding, and implementing the methods.   The paper points out situations when the methods are equivalent   The paper analyses the methods  sensitivity to identifying single and joint regions of sensitivity   The paper proposes a new objective function to measure joint sensitivity" 
This paper proposes to mitigate mode collapse in GANs by encouraging distribution matching in the latent space. Reviewers 1 and 3 expressed concerns that the methodology is too incremental in the context of the existing literature (VEEGAN, VAE GAN, AAE). This, combined with the lack of up to date baselines, makes it difficult to access the significance of the proposed modifications. The quality and precision of the writing can also be improved to meet the standards of publication at a top tier conference.  
The paper proposed an attention forcing algorithm that guides the sequence to sequence model training to make it more stable. But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable. The solution to address that is using another teacher forcing model, which can be expensive.   The major concern about this paper is the experimental justification is not sufficient: * lack of evaluations of the proposed method on different tasks; * lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc; * lack of comparisons to related existing supervised attention mechanisms.  
The paper introduces a compression method for distributed Split Learning for better communication efficiency, by compressing the intermediate output between client and server model by vector quantization. Convergence analysis and experimental results are provided.  Unfortunately consensus among the reviewers remained that it remains slightly below the bar after the discussion phase. Main remaining concerns were the variety of baselines and benefits from split learning setup in experiments, compared to other FL approaches, quantization approaches, architecture splits. Reviewers also missed a discussion of latency requirements of model parallel training in FL as opposed to data parallel which allows less frequent communication compared to here (e.g. discussing the split layer size vs latency trade off, here of quantized intermediate layers compared to regular FL). The newly added Figure 6 does not specify or vary the number of local steps (or batch size) in FedAvg. We hope the detailed feedback helps to strengthen the paper for a future occasion.
This paper explores methods for pruning binary neural networks. The authors provide algorithms for developing sparse binary networks that perform okay on some basic ML benchmarks. They frame this as providing insights into synaptic pruning in the brain, and potentially providing a method for more efficient edge computing in the future.  All four reviews placed the paper below the acceptance threshold. The reviewers noted that the paper was hard to follow in several places and were unsure as to the motivations. The authors attempted to address these concerns in their replies, but the Area Chair felt that these were insufficient.   As well, the Area Chair notes that some of the claimed contributions of the paper are questionable. Specifically:  (1) The claim that there is anything biologically plausible about the algorithms presented here is very suspect. The brain cannot use a search and test system for synaptic pruning like the algorithms proposed here. Thus, it is unclear how this paper provides any insight for neuroscience. In fact, the authors do not even really try to provide any neuroscience insights in the results or discussion. Moreover, they don t actually appear to use any neuroscience insights to develop their algorithms, other than the stochasticity of the pruning (though note: it is not actually clear in neuroscience data whether pruning is stochastic). Given the ultimately very poor performance on ML tasks, the paper doesn t seem to provide anything particularly useful for application in ML either.  (2) The claim that the provide, "The demonstration that network families with common architectural properties share similar accuracies and structural properties." is odd. Surely this is the null hypothesis anyone would have about ANNs? It would be surprising if networks with common connectivity profiles (which is what the authors mean by "architecture") didn t share similar performance!  (3) The claim that searching in architecture space like this leads to "architecture agnostic networks" is odd... As noted by Reviewer 2, the authors are really just specifying algorithms for sparsifying binary neural networks, which they frame as being "architecture agnosticism" according to a rather strained definition. There are other ways of approaching the sparsification of neural networks, and of doing architecture optimization, but the paper is not framed as contributing to this literature.  Altogether, given these considerations, and the four reviews, a "Reject" decision was delivered.
The paper considers adaptive stochastic optimization methods and shows that they can be re interpreted as first order trust region methods with an ellipsoidal trust region, they consider a related second order method, and they show convergence properties and empirical results.  The results are of interest, but the significance of some of the results is not clear.  Part of this has to do with substance, and part of this has to do with presentation that can be improved.  Empirical results are weak, including appropriate baselines and details of the empirical results.
This paper is on the right track to be accepted after a revision but it is not ready yet. The reviewers were mostly puzzled by different details in the evaluation process, however, as far I can see, most of them should be possible to address. My impression is also that the authors might be more used to a slightly different style of paper writing, more similar to what is typically accepted at MICCAI, ML4H, MIDL etc. I think that a lot can be improved in this respect just by carefully analyzing the reviews.
This paper proposes a contribution aiming at understanding the cause of errors in few shot learning. The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose. The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work. Hence, I propose rejection.
Overall, the paper makes some interesting and intuitive observations regarding the autoencoders with a cycle consistency, and aims at achieving controllable synthesis via a disentangled representation. However, the overall consensus was that the manuscript needs further iterations:  In particular: The ideas should be made more precise using mathematical arguments, as it stands some ideas are (e.g. DEAE and UDV) disconnected.  The scope needs to be clarified, e.g. respective contributions of GSL AE and DEAE, use of label information   More numerical/quantitative evaluations, the current experimentation is not convincing enough, needed for better justification (spurious and not convincing experimentations)  The English of the manuscript could be improved as it occasionally hampers the flow. 
Dear Authors,  Thanks for your detailed feedback to and even communications with the reviewers. Your additional input certainly clarified some of the concerns raised by the reviewers and also improved their understanding of your work.  However, we still think that the notion of sequential bias is unclear, and the authors overclaim what they have done. For these reasons, this paper cannot be recommended for acceptance. I hope that the detailed feedback from the reviewers will help improve this work for future publication. 
Summary (from reviewer uzT5): This paper analyzes adversarial domain learning (DAL) from a game theoretical perspective, where the optimal condition is defined as obtaining the local Nash equilibrium. From this view, the authors show that the standard optimization method in DAL can violate the asymptotic guarantees of the gradient play dynamics, thus requiring careful tuning and small learning rates. Based on these analyses, this paper proposed to replace the existing optimization method with higher order ordinary differential equation solvers. Both theoretical and experimental results show that the latter ODE method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations.  All reviewers appreciated the contributions of this paper and recommended acceptance. While the methods themselves are not novel, the game perspective applied to this problem appears to be and the use of higher order solves yield interesting theoretical and empirical improvements.    Additional comments    1) For the comparison vs. game optimization algorithms (Figure 3), it would be nice to normalize the x axis so that one "epoch" yields comparable computational cost among the different methods (as RK4 and RK2 is much more expensive than EG or GD per mini batch). Given that EG had such bad performance there, it would not change the conclusions; but the current scaling is still quite misleading. Same comments for Figure 2.  2) Note that modern approaches for stochastic extragradient recommend to use different step sizes for the extrapolation step and the update step (see e.g. Hsieh et al. NeurIPS 2020 "Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling") I suspect that much bigger step sizes could be used in this case while maintaining convergence, and this version should be added to Figure 3.  3) In "Related Work | Two Player Zero Sum Games"  > note that Gidel et al. 2019a provided all their convergence theory and methods for stochastic variational inequalities and thus it also applies to three player games, unlike seems to be implied by this paragraph. In particular, all the algorithms they investigated (Extra Adam amongst others) could also be applied to DAL. While I can see that the specifics of the objective in DAL might be different than for GAN optimization, it would be worthwhile to acknowledge these alternative approaches more clearly, and I encourage the DAL community to investigate their performance more exhaustively for DAL than what was done in this paper.
PAPER: This paper addresses the problem of learning methods for general speech restoration which generalizes across at least 4 tasks (additive noise, room reverberation, low resolution and clipping distortion).  The proposed approach is based on a two stage process, which includes both analysis and synthesis stages.  DISCUSSION: The reviewers wrote very detailed reviews which ask some important questions and point to some potential issues. The authors responded to all reviews, but only addressed a subset of the issues and questions mentioned by the reviewers. Novelty and comparison with previous approaches was one of the issues mentioned by reviewers. SUMMARY: While reviewers are supportive of this line of research, reviewers were also concerned with the novelty of the proposed approach and details of the experiments. In its current form, the paper may not be ready for publication.
The reviewers agree the paper is not ready for publication. 
The paper considers differentially private federated learning   a well motivated problem. The proposed algorithm is a simple modification to existing methods, e.g., DP FedAvg, but uses a different DP mechanism for noise adding.  The reviewers liked the motivation but criticized the work for its incremental nature and for somewhat overselling the contributions.  Pros:    The paper used advanced Renyi DP accounting to get a stronger privacy utility tradeoff.    The experimental results improve over cp sgd that uses Binomial mechanisms  Cons:     It is a bit incremental in its contribution.  The main contribution is to applying "discrete Gaussian mechanism" to the federated learning problem for the interest of reducing the communication cost.   Discrete Gaussian mechanism and its RDP analysis are both from existing work.    The improvement in privacy utility tradeoff over cp sgd seems to be due to that the discrete Gaussian mechanism has an RDP bound, which plugs right into the subsampling bound and moments accountant.    It is unclear whether the improvement is coming from the different noise or a stronger privacy accounting.  Notice that the privacy accounting of Binomial mechanism in the initial cp sgd paper was rather crude, thus a fair comparison would be to also conduct an RDP analysis for the Binomial mechanism.    Overall, there weren t sufficient support among the reviewers and the experimental results alone are not so groundbreakingly strong to carry the paper single handedly. 
This is a nicely written paper proposing a reasonably interesting extension to existing work (e.g. VPN). While the Atari results are not particular convincing, they do show promise. I encourage the authors to carefully take the reviewers  comment into consideration and incorporate them to the final version.
This paper proposes a dynamic programming strategy for faster approximate generation in denoising diffusion probabilistic models.  All reviewers appreciated the paper, but they are not overly excited.   Two reviewers are focused on the log likelihood not being the objective for image quality. This AC does not really buy this argument.   The method and story around are well rounded and finished. So it is hard to think of any major modifications that will change the overall story a lot. One could therefore argue for acceptance as it stands. On the other hand this is difficult to argue for given the below acceptance level scores.   So the final recommendation is reject with a strong encouragement to submit to the next conference. Updating the paper with preemptive arguments on why the ELBO and not FID is the right thing to consider.
The paper proposes a general method to enhance the performance of first order optimizers. The main idea is to use a memory buffer to maintain a limited set of critical gradients from recent history. Namely, gradients with large l2 norm. The paper includes a convergence proof on strongly convex smooth objectives. Experimental results are reported for several architectures in vision and language tasks. When integrated with several commonly used optimizers (SGD, SGDM, RMSProp, Adam), the method shows an improvement in terms of learning speed as well as improved performance (in almost all cases). Several ablations are performed to show strong robustness to hyperparameters introduced.  The paper is very well written and easy to follow. The proposed method is simple and effective. The empirical evaluation is strong and the ablation studies exhaustive and convincing, as pointed out by all four reviewers.  The authors provided a solid rebuttal addressing many questions raised by the reviewers.  Reviewer KE2X is the only reviewer recommending to reject the paper, pointing out that the paper lacks a clear motivation on why the critical gradients are selected based on the l2 norm. In their response the authors provided a connection with recent methods designed to find important examples when training neural networks. In the discussion (not visible to the authors). Reviewer v5GJ stated that this criterion is intuitive and acceptable given the strong empirical performance reported. The AC agrees with this view.  Reviewer KE2X also points out that the theoretical results do not provide a convincing improvement over vanilla SGD. The authors acknowledge this point, adding that the convergence results apply to a wide variety of critical gradient methods and aggregation strategies, so it is natural to expect that improvements will depend on the specific conditions. While the AC agrees that having stronger theoretical results would improve the paper, as it stands it is certainly above acceptance threshold.  Overall the paper makes a solid contribution with a method that is simple and effective. It will likely inspire other alternative methods in the future. The AC recommends accepting the work.
The paper proposes a framework for distilling deep directed graphical models where the teacher and student models have the same number of latent variables z. The key idea is to reparameterize both models in terms of standardized random variables epsilon with fixed distributions and train the student to match the conditional distributions of the observed variables/targets given the values of the standardized RVs epsilon. The approach aims to avoid error compounding that affects the local distillation approach, where the student is trained to match conditional distributions of the teacher model (without the above reparameterization). To deal with discrete latent variables and vanishing gradients the authors augment the target matching loss with the latent distillation loss that matches the local distribution for each z_i given the standardized variables epsilon it depends on.  Positives  The paper tackles an important problem.  The idea of using reparameterization for distillation in this way makes a lot of sense for continuous latent variables and could be impactful.  The experiments provide some evidence in support of the idea.  Negatives  There are considerable issues with the clarity of writing: for example, it is really not clear how (and why) the method is supposed to work for discrete latent variables. The explanation provided by the authors in their response to the reviewers was helpful but still not clear enough.  The fact that the teacher and student models need to have the same number of latent variables (and perhaps even the same structure) is a big limitation of the method given the claim of its generality, and thus needs to be clearly acknowledged and discussed. For example, the method cannot be used to train a student model with fewer latent variables than the teacher, which seems like a very common use case.  The experimental evaluation is extensive but insufficient, in large part due to the evaluation metrics. Given that VAEs are trained by maximizing the ELBO (and distilled by minimizing a sum of KLs), it makes sense to also evaluate them based on the ELBO rather than solely on the FID, is done in the paper. The VRNN experiment would be much more informative if it included a quantitative evaluation (e.g. based on ELBO).  In summary, the paper has considerable potential but needs to be substantially improved before being published.
 Interesting idea, reviewers were positive and indicated presentation should be improved. 
There was substantial disagreement between reviewers on how this paper contributes to the literature; it seems (having read the paper) that the problem tackled here is clearly quite interesting, but it is hard to tease out in the current version exactly what the contribution does to extend beyond current art.
The paper presented a unified framework for constructing likelihood based generative models for raw audio. It demonstrated tradeoffs between memory footprint, generation speech and audio fidelity. The experimental justification with objective likelihood scores and subjective mean opinion scores are matching standard baselines. The main concern of this paper is the novelty and depth of the analysis. It could be much stronger if there re thorough analysis on the benefits and limitations of the unified approach and more insights on how to make the model much better. 
Thank you for your submission to ICLR.  The paper proposes a simple method for improving calibration performance using a loss based upon a Dirichlet KDE.  The method is appealing in its simplicity, but several reviewers (and myself) have concerns simply about the fact that the method ultimately seemed to give rather marginal improvement over the standard cross entropy baseline.  The authors attempted to address this point in the rebuttal, with their additional example on the Kather domain.  And while this is a nice addition, I m still not fully convinced that the improvement here is _that_ significant, to the point where I think it would be important to consider much broader sweeps of hyperparameters, etc, for all methods (which I believe should be reasonable here given the data set sizes).  I believe this has the potential to be a nice contribution, and its simplicity can be a positive, but ultimately I think a bit of additional effort is required to show the full empirical advantages of the method.
There is a general consensus on the fact that the paper is not yet ready for publication. I encourage the authors to carefully address the detailed concerns raised by the reviewers, which include  among other: i) the incompleteness of the literature overview, which should include the references provided by the reviewers, ii) poor (or bias towards the proposed approach) experimental evaluation, and iii)  a vague treatment of key terms in the interpretability literature  like feasibility (e.g., to make sure that the counterfactual lie in high data density regions). 
Reviewers could not reach consensus here and legitimate concerns are raised on novelty and on empirical results, although this can be attributed to the important computation times required to run experiments on 3D MRI volumes. The authors have provided a comprehensive response to the reviews, the general feedback is that the work has merit but it fails to convince on its real contribution to the state of the art. At this stage, I fear this work cannot be recommended for acceptance.
Paper proposes an approach for scene autoregressive layout generation. Four expert reviewers evaluated the paper outlining the following pros/cons of the work.   > Pros:   Good performance across different domain [R1,R2,R3,R4]   Formulation is general [R1,R2]   Clever separation of different attributes [R1]   The idea of using transformers is interesting [R4]  > Cons:   Missing related works [R3]   Unclear comparison with baselines that [R2]   Lacks of  hyper parameter tuning on the baselines [R2]   The quantitative results do not outperform the state of the art models consistently across all metric [R4]  Authors have addressed some of the concerns in the rebuttal and generally reviewers are more convinced after the rebuttal than before. The fairness of comparison to baselines remains an issue for two of the reviewers, and quality of results for one. AC acknowledges and agrees with these concerns. As such, given the large number of highly qualified submissions to ICLR and in comparison to those submissions, the paper fell slightly bellow the acceptance threshold.   That said, AC believes the approach, overall, is interesting and warrants re submission after the appropriate revisions are implemented. 
This paper provides an active learning approach to improve the performance of an existing differentially private classifier with public labeled data. Where the paper provides a new approach, there is a consensus among the reviewers that the paper does not provide a strong enough contribution for acceptance. The authors can potentially improve the submission by including a more comprehensive comparison with the PATE framework and improving its overall presentation.
This paper develops an interesting new angle on the behavior of large width neural networks by elucidating the connection between the NNGP and noisy gradient descent and by examining finite width corrections through an Edgeworth expansion. While these contributions are important, the paper would better serve the community if its presentation were significantly improved before publication. The main issue is not one of presentation style   papers with physics style prose are welcomed and appreciated at ICLR   but rather one of presentation substance. In addition to the various specific points raised by the reviewers, I would add that the figures and captions are difficult to interpret, the experiments need a more in depth discussion, and the notations should all be defined at the time of their introduction, among other things. For these reasons, I cannot recommend accepting the paper in its current form, but I hope to see a more polished version of the manuscript at a subsequent conference.
This paper was reviewed by 4 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper proposes an MLP based approach for data without known structure (such as tabular data). At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block. The results are then aggregated recursively to produce the final output.   Pros: 1. Handling less structured data is surely an important problem in machine learning and is much less explored.  2. The paper is well written, easily understandable even with a fast browsing.  3. The experimental results show some improvement.   Cons: 1. The approach is somewhat trivial, and the framework could be improved, see, e.g. Reviewers #3&#4.  2. By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc. Maybe should even compare with deep random forest. Although the comparison with MLP etc. is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4 s comment on why using imagery data, which do not fit the theme of the paper). Reviewers #3&#4 also had some concerns with the experiments. Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental.  Although the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline. Due to the limited acceptance rate, the area chair has to reject the paper.
The main contribution of the paper is in providing an additional layer to standard certainty equivalent control for LQR dynamics, that essentially prevents the state from exploding exponentially, via forcing a "descent" to a small bounded state space in growing epoch sizes. Theoretically, this is shown to ensure a notion of boundedness which is termed as "bounded cost safety".  Overall, the reviewers raised several concerns in their initial reviews. These included the relevance of the proposed new approach to modeling "safety" here, whether it is actually novel given the assumptions about open loop stability of the original plant, the fact that the learning algorithm could take the state to arbitrarily large states durign the learning process, the significance of the contribution of the paper wherein a "kill switch" is effectively being designed depending on the state norm, whether least squares is an arguably simpler system estimation method compared to the impulse response estimator here, whether existing approaches based on an input failure probability parameter can be used to yield the same result by iteratively taking it down to zero, and other concerns about the technical exposition.  The author(s) provided detailed responses to the reviewers  concerns. Specifically, the safety/stability notion was clarified as being distinct from standard input to state stability, that the learning algorithm could, during its operation, drive the system state to arbitrarily large sizes (although asymptotically almost surely this is supposed to not happen), and how this paper s assumptions are different (and lighter) than other related work that achieves almost sure guarantees.  In the discussion that ensued after the author response period, it was clear that the author responses had helped to address many of the reviewers  concerns. However, the overall impression has still not been convincing enough to recommend acceptance. This is primarily on two fronts, which I hope that the author(s) can address in the future to strengthen the submission: (1) The attempt at expressing "safety of learning" is found to be not satisfactory, in view of the observation that the proposed scheme does not guarantee classical notions of stability while in the process of learning. It also makes the main message of the paper confusing   the reviewers noted that the words "safety" and "safe learning" still recur in the revised manuscript, creating scope for misinterpretation. (2) The algorithmic contribution appears to be incremental   its essence is to "apply the brakes when the car runs too fast". This is not to take away from the hard work put in to analyzing the algorithm and deriving guarantees. (3) The experiments benchmarking the proposed strategy are not comprehensive   more relevant baselines drawn from existing work, such as the ones that have emerged from the author response discussion, could be compared against. In fact, it would be compellingly in favor of this submission for the author(s) to show that other approaches fail to offer the same kind of "safe" performance that is expected.  Upon a more careful reading by myself in the recent past, I would also like to bring up a fundamental technical criticism about Theorem 1 and Defn. 2 ("bounded cost safety"), which I believe must be fixed before the paper s conclusions can be accepted. Defn. 2 states that a learning process if bounded cost safe if for all times $k$, $\pi_k$ is not destabilizing in the sense that its value function is finite. However, the sequence of policies $\pi_k$, $k 1, 2, \ldots$ is a sequence of *random* objects. So in what sense is Defn. 2 to be interpreted? If the author(s) mean(s) to say that the event {$\forall k 1, 2, ...: \pi_k$ is not destabilizing} occurs w.p. 1, then this is a very strong requirement and cannot be guaranteed (random noise can cause a  bad  controller to be learnt and applied at some time t with positive probability). Basically my contention is that Defn. 2 is incomplete for a theory oriented paper like this, and as a consequence I do not see why Theorem 1 should hold (or more precisely, in what sense it should hold). The proof of Theorem 1 is not clear as well: The expectation in the third sentence of Sec A.2 ought to be taken for a *fixed* controller $\hat{K}_k$ *always applied* to a system from time zero until infinity. I do not see why a fixed controller s (standard infinite horizon average) cost should always be finite, leading me to suspect an irregularity in the proof argument. I wish this point could be discussed and resolved earlier in the author response phase, but it is unfortunately too late.
This paper introduces a deep neural network sequence to sequence framework for modifying the length of a speech sequence.  It employs a convolutional encoder decoder architecture optimized under a Bayesian formulation with variational inference.  The proposed framework is evaluated on a voice conversion task and three emotion conversion tasks. The results show that it can successfully change the duration of an utterance without accessing the target utterance.  Almost all reviewers raised concerns with some strong or inaccurate claims made by the authors in the paper.  The literature review on related work also needs to be significantly improved.  Another major concern is on experiments. Other than the DTW compared in the work, the proposed method should also be compared with existing duration modification techniques. The MOS evaluation seems to be limited and needs further improvement to make the results stronger and more convincing.  Since the authors did not provide a rebuttal, all these major concerns remain unanswered.
This paper describes a method for adapting an RL policy in a deployment environment that does not provide a reward signal.  This concern arises commonly when a task reward is available in a robot simulator but not on the physical robot where the policy is eventually deployed.  The proposed solution is to learn an inverse dynamics model as an auxiliary prediction task on an internal state embedding that is shared with the policy.  The policy is adapted during deployment by modifying the state embedding using this auxiliary task (with the assumption that the main objective remains unchanged).  The proposed method is tested with transfer between simulated domains and also on transfer from a simulator to a physical robot.  The experiments showed the method had consistently higher performance than alternatives.  The reviewers found many positive contributions in the presented paper. These include the problem s importance (R1, R2,R4), extensive experiments (R1, R2, R3), clear writing (R1,R4), simplicity and effectiveness in comparison to ablations (R3, R4).  The reviewers saw a weakness in the method s limitation to perceptual adaptation instead of dynamics adaptation (R1 4) and the lack of novelty (R4).  The author response addressed both concerns.  They stated that the method is novel for adapting to continuously changing environments in a self supervised fashion without rewards.  The authors modified the paper to clarify how the method demonstrates robustness to changes in the system dynamics.  The reviewers found the author response addressed their major concerns.  Four reviewers indicate accept for the contributions stated above and expressed no remaining concerns.  The paper is therefore accepted. 
The paper explores "Astuteness of explainer", to measure reliability of the explanations. There were concerns about the overlap of the proposed work with existing literature.  It was felt that both theory and experiments need more development
The paper gives a framework for learning temporal logic rules from noisy unlabeled data. The key novelty is a formulation of combinatorial rule search as an end to end differentiable problem. The method is evaluated on a video dataset and a healthcare dataset.   The reviewers liked the high level ideas behind the paper. However, the conclusion was that the experimental results, while interesting, are still somewhat preliminary (in particular, the baselines are weak).  I agree with this point and am recommending rejection this time around. However, I urge the authors to develop the paper further and submit to the next deadline.
The submission introduces an algorithm for structured pruning of fully connected ReLU layers using ideas from tropical geometry. The paper begins with a very accessible overview of key concepts from tropical geometry, and shows how ReLU networks can be thought of as  tropical polynomials. It gives an efficient K means based algorithm for pruning units in a way that approximately minimizes the Hausdorff distance between certain polytopes. Experiments show that the method outperforms other methods based on tropical geometry and is competitive with SOTA methods from a few years ago.  I think the reviewers, authors and I all agree on the following points: tropical geometry is a mathematical topic not commonly used in our field and for which it is difficult to find expert reviewers (notice that most of the citations aren t from ML venues). The paper is well written, and the authors have taken pains to present the required concepts in an accessible way. Nobody has raised any concerns about correctness. While this isn t the first pruning method that uses tropical geometry, the algorithm is novel and interesting. It s expensive, but not unreasonably so. The experiments are a proof of concept: they use small networks by today s standards, and the baselines aren t the current SOTA.   The average scores are slightly below the usual cutoff. The reviewers are concerned about whether this method is useful, given that is based on different principles from current methods and can t quite compete with current SOTA. But my own sense is that this is a paper that we d like to have at ICLR. It gives a clear, accessible introduction to tropical geometry and demonstrates its usefulness for practical deep learning. It demonstrates competitiveness with fairly strong baselines, which is all we should expect from methods that haven t benefited from years of hill climbing on the same handful of ideas. I recommend acceptance.
This paper analyzes the behavior of VAE for learning controllable text representations and uses this insight to introduce a method to constrain the posterior space by introducing a regularization term and a structured reconstruction term to the standard VAE loss. Experiments show the proposed method improves over unsupervised baselines, although it still underperforms supervised approaches in text style transfer.  The paper had some issues with presentation, as pointed out by R1 and R3. In addition, it missed citations to many prior work. Some of these issues had been addressed after the rebuttal, but I still think it needs to be more self contained (e.g., include details of evaluation protocols in the appendix, instead of citing another paper).   In an internal discussion, R1 still has some concerns regarding whether the negative log likelihood is less affected by manipulations in the constrained space compared to beta VAE. In particular, the concern is about whether the magnitude of the manipulation is comparable across models, which is also shared by R3. R1 also think some of the generated samples are not very convincing.   This is a borderline paper with some interesting insights that tackles an important problem. However, due to its shortcoming in the current state, I recommend to reject the paper.
The paper develops an approach to training generative models with binary weights. The reviewers are split. Two reviewers regard binary networks and the general theme of reducing the computational cost of training as important, and the presented work as a solid contribution. Two reviewers raise concerns about the motivation and the quality of the results. The authors  responses somewhat alleviated the quality concerns of R3, but not the concerns of R4. Overall, the reviewers lean on the positive side. There is disagreement on the importance of the problem, but there is clearly a non trivial subset of the community that welcomes research in this direction. The AC supports acceptance.
This paper provides a generalization analysis for graph embedding methods concluding with the observation that the norm of the embedding vectors provides an effective regularization, more so than dimensionality alone. The main theoretical result is backed up by several experiments.  While the result appears to be correct, norm control, dimensionality reduction and early stopping during optimization are all very well studied in machine learning as effective regularizers, either operating alone or in conjunction. The regularization parameters, iteration count, embedding dimensionality is typically tuned for an application. The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods.   Both reviewer 2 and 3 have also raised points on the need for stronger empirical analysis.
This paper combines two recently proposed ideas for GAN training: Fisher integral probability metrics, and the Deli GAN. As the reviewers have pointed out, the writing is somewhat haphazard, and it s hard to identify the key contributions, why the proposed method is expected to help, and so on. The experiments are rather minimal: a single experiment comparing Inception scores to previous models on CIFAR; Inception scores are not a great measure, and the experiments don t yield much insight into where the improvement comes from. No author response was given. I don t think this paper is ready for publication in ICLR. 
This paper proposes augmentation of the state exploration strategy that is interesting and has a potential to lead to improvement. However, the current presentation makes it difficult to properly assess that. In particular, the way the authors convey both the underlying intuition and its implementation is fairly vague and does not build confidence in the grounding of the underlying methodology.
This paper is a solid contribution to researchers in this field, as it provides a new idea for the basic problem of determining the direction of causality between two variables, using the functional causal model as a dynamical system and optimal transport.
The paper proposed using stochastic AUC for dealing with imbalanced data. This paper provides useful insights and experiments on this important problem. I recommend acceptance.
This paper proposes an algorithm for learning linear SEMs via the Cholesky factorization and provides a detailed theoretical analysis of the algorithm. After an extensive discussion and clarification from the authors, there was a consensus that the theoretical results are incremental compared to existing work and many of the claims need additional context in light of existing work. In particular, I recommend the authors pay careful attention to the presentation of the sample complexity bounds, which were revealed to be substantially weaker than initially claimed, and to validate these bounds with careful experiments.
The paper presents an architecture for conditional video generation tasks with temporal self supervision and temporal adversarial learning. The proposed architecture is reasonable but looks somewhat complicated. In terms of technical novelty, the so called "ping pong" loss looks interesting and novel, but other parts are more or less some combinations of existing techniques. Experimental results show promise of the proposed method against selected baselines for video super resolution (VSR) and unpaired video to video translation tasks (UVT). In terms of weakness, (1) the technical novelty is not very high; (2) the final loss is a combination of many losses with many hyperparameters; (3) experimentally the proposed method is not compared against recent SOTA methods on VSR and UVT.   The proposed method should be compared against more recent SOTA baselines for VSR tasks (see examples of references below):  EDVR: Video Restoration with Enhanced Deformable Convolutional Networks https://arxiv.org/abs/1905.02716  Progressive Fusion Video Super Resolution Network via Exploiting Non Local Spatio Temporal Correlations ICCV 2019  Recurrent Back Projection Network for Video Super Resolution CVPR 2019  The same comment would apply for baselines for UVT tasks:  Mocycle GAN: Unpaired Video to Video Translation https://arxiv.org/abs/1908.09514  Preserving Semantic and Temporal Consistency for Unpaired Video to Video Translation https://arxiv.org/abs/1908.07683  Particularly for UVT, the evaluated dataset seems limited in terms of scope as well (i.e., evaluations on more popular benchmarks, such as Viper would be needed for further validation). Overall, given that the contribution of this work is an empirical performance with a rather complex architecture/loss, more comprehensive empirical evaluations on SOTA baselines are warranted. 
The authors propose to provide fast convergence results for the OGDA and OMWU algorithms based on a reinterpretation of the metric subregularity in the saddle point problem setting. During the rebuttal period, the paper improved significantly, not only due to the diligence of the authors but also due to reactive reviewers that provided extremely constructive comments. The technical developments are quite nice: Lemma 2 allows constant step size parameter as compared to Daskalakis and Panageas, followed by Theorem 3, which establishes the first linear rate under the saddle point metric subregularity. The numerical demonstrations are also helpful in driving the point home. Although it is not surprising that the shape of the polytope matters, it is still impactful to see the linear rate.    ps. The authors should consider including a related work comparison to the reflected FB algorithm in [1] since it reduces to the FoRB and it also provides convergence analysis for the sequence in the general monotone inclusions.   [1] Cevher and Vu, "A reflected forward backward splitting method for monotone inclusions involving Lipschitzian operators,   https://arxiv.org/pdf/1908.05912.pdf
This paper tackles the open set recognition problem, specifically the subset that looks at rejecting test data that with unknown classes that are related to the training data. The proposed approach uses an existing distance based classifier (based on LDA) combined with a new background class regularizer. Results, comparing to a few prior OSR methods, are shown across image/text datasets.     The reviewers gave a mixed set of scores, with concerns about visualization/ablation studies and the lambda parameter with affect on classification accuracy (1wRX, ujMG, rop6), computation complexity and efficiency (1wRX), limited novelty and discussion of relationship to prior works (Mkdh, rop6), and limited comparison to state of art as only a few algorithms are compared to the proposed approach (Mkdh), and initialization method. Notably, the authors make a strong claim for the latter point that the method should only be compared to previous BCR methods (as opposed to softmax based classifiers, for example); this seems to ignore whole classes of different methods that can approach the OSR problem. While it is true that comparing to previous BCR methods can directly show your approach is superior to them under similar class of algorithms (thereby showing that it is an improvement), putting the method within the context of the entire literature is absolutely necessary to discuss relative impact to the field. For example, the improvements in AUROC are not that great (and in some cases worse) than even the methods you compare to, while OSCR is improved significantly, so it is not clear how it stacks up with respect to the current state of art. Even if it doesn t beat it, you could argue your contribution, but not presenting it all prevents a holistic perspective that is necessary.     The authors provided thorough rebuttals, including additional ablations and experiments. However, after the review period the scores remain mixed (5,5,6,8) and the reviewers expressed remaining concerns about novelty and comparison to the current state of art (not just BCR based methods). As a result of these remaining concerns, I recommend rejection at this time.
The authors propose an alternative fine tuning procedure by introducing a projection head and two new losses to be combined with the vanilla cross entropy loss. The authors introduce and jointly optimize the standard cross entropy loss, the contrastive cross entropy loss for classifier head and the categorical contrastive learning loss for projector head in an end to end fashion. The authors empirically confirmed that this setup compares favorably to existing baselines.  The reviewers found the setting challenging and worth investigating. The idea of exploring the intrinsic structure of the downstream task to help with fine tuning was deemed useful. The reviewers appreciated the thorough empirical validation. While the proposed approach was not yet explored in this specific context, most reviewers were concerned with the lack of novelty. In addition, there seems to be a large gap between the quality of exposition in the introduction and results section with respect to the rest of the paper which introduces confusion.   As it currently stands, the paper is not yet ready for publication and I will recommend rejection. To improve the manuscript the authors should incorporate the received feedback and significantly improve the exposition and justification of the proposed loss. In terms of empirical results, the authors should also explore alternative neural architectures to validate whether the proposed approach is general and whether the need for hyperparameter tuning arises. 
Reviewers found the new framework interesting. However, reviewers are unsatisfied with empirical evaluations. More experiments and discussion are needed.
This paper attempts to address a problem they dub "inverse" covariate shift where an improperly trained output layer can hamper learning. The idea is to use a form of curriculum learning. The reviewers found that the notion of inverse covariate shift was not formally or empirically well defined. Furthermore the baselines used were too weak: the authors should consider comparing against state of the art curriculum learning methods.
The authors investigate theoretical properties of meta learning. In particular, the train validation split to tackle the linear centroid meta learning problem is investigated in asymptotically regimes (a non asymptotic analysis of the train train estimator has been added later on). It is shown that the train validation method has statistical consistency, while the train train method has a statistical bias to the centroid. Yet, in the noise free setting both methods have statistical consistency. Furthermore, the train train method is superior to the train validation method in the sense of the asymptotic MSE. Based on the asymptotic analysis the optimal ratio of the data splitting for the train validation method is derived. The theoretical findings are corroborated by some numerical experiments. The main theoretical result suggests that the train train method + optimally tuned regularization is a strong alternative to the train validation split, hence the authors  recommendation that for meta learning train train method should be preferred.  The reviewers and the area chair appreciated the theoretical findings and the subsequent effort to improve the paper the authors put in place during the discussion period. Unfortunately, the tight competition among papers in this year s edition of the conference makes this specific submission not compelling enough for publication.  I would like to encourage the authors to sumbit to another ML venue in the near future, while considering improvements in their experimental validation (as also suggested by some reviewers). 
From the positive side the problem addressed by the paper could be of potential interest in the case there is noise in the features associated to each node of the graph. The paper is mostly well written and clear. The proposed approach is based on solid mathematical grounds.  On the other hand there are concerns about:  i) motivation: it is not clear how significant the proposed approach is since the authors were not able to clearly highlight the advantages with respect to the standard approach where already the weight matrix (via learning) can play the role of a low pass filter for node’s features. Maybe the main advantage is given by the fact that the network does not have to learn a low pass filter, however this needs a better clarification;  ii) suggested approach: the authors are using an approach that seems to be more complex with respect to simpler ones already proposed in literature and not mentioned in the paper. In addition to that, the simpler approaches have convergence guarantees that have not been proved for the proposed approach;  iii) significance of the experimental results: the obtained experimental results are obtained by using a model with more parameters with respect to the baselines. Comparisons versus baselines with a similar number of parameters are necessary to have a fair assessment of the merits of the proposed approach. 
This paper presents an elegant and effective approach to knowledge transfer in RL by learning a policy prior from expert data. The paper is generally well structured and well written. Generally, all the reviewers were favourable about this paper, with its simple idea and convincing results. It was thought that the paper would benefit from the addition of more discussion around related work, and more experimental results, but it remains a strong paper.
This paper proposes a new metric, AUL, for classification problems with unbalanced data. The paper proves that it is unbiased and it does not need to estimate the mixture proportions, a traditional approach. Empirical results show improvement. While the results are novel, the referees pointed out several concerns 1. Positioning of the results in the context of existing work. It would be helpful if the paper can establish theoretical links to existing metrics such as  AUC, make a very precise statement about what is lacking in those approaches 2. Some of the results, may have bugs in the proof. 3. More ablation studies are needed   In summary the paper has good ideas but maybe premature for publication. 
The manuscript describes a novel technique predicting metal fatigue based on EEG measurements. The work is motivated by an application to driving safety. Reviewers and the AC agreed that the main motivation for the proposed work, and perhaps the results, are likely to be of interest to the applied BCI community.   The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and breadth of empirical evaluation. In particular, only a few baselines were considered. As a result, for the non expert, it is also unclear if the proposed methods are compared against the state of the art. There was also a particular concern that this work may not be a good fit for an ICLR audience. 
This paper presents an approach for the long tailed image classification, where the class frequencies during (supervised) training of an image classifier are heavily skewed, so that the classifier underfits on under represented classes. The authors  responses to the reviews clarified most of their  concerns, although some reviewers pointed out that some of the details regarding experiments such as the construction of the validation set and the selection of balanced/imbalanced sets remain unclear. Overall, we believe this paper contains interesting observations to be shared.
The paper presents an KL divergence minimisation approach to the action–perception loop, and thus presents a unifying view on concepts such as Empowerment, entropy based RL, optimal control, etc.  The paper does two things here: it serves as a survey paper, but on top of that puts these in a unifying theory.  While the direct merit of that may not be obvious, it does serve as a good basis to combine the fields more formally.  Unfortunately, the paper suffers from the length restrictions.  With more than half of the paper in the appendix, it should be published at a journal or directly at arXiv.   Not having a page limit would improve the readability much.  ICLR may not be the best venue for review papers.
Pros: + Clearly written paper. + Good theoretical analysis of the expressivity of the proposed model. + Efficient model update is appealing. + Reviewers appreciated the addition of results on the copy and adding tasks in Appendix C.  Cons:   Evaluation was on less standard RNN tasks.  A language modeling task should have been included in the empirical evaluation because language modeling is such an important application of RNNs.  This paper is close to the decision boundary, but the reviewers strongly felt that demonstration of the method on a language modeling task was necessary for acceptance. 
While there are some potentially interesting aspects to this work, it doesn’t acknowledge a significant amount of relevant literature, and there are some unsupported claims. All reviewers believe the paper is not ready for acceptance. Reviewers provided some good thorough reviews and suggestions, but the authors did not choose to respond or engage in discussions to improve the paper. 
This work provides an empirical investigation on the adversarial attacking problem in deep neural networks. While it contains some interesting ideas, the work is still in the preliminary stage, lacking substantial support for the main points. Many of the ideas discussed in the paper have been explored in the past and hence more discussions on previous works would be needed. We encourage the authors to keep improving the work for future submission.
Despite the new ideas in this paper, reviewers feel that it needs to be revised for clarification, and that experimental results are not convincing.  I have down weighted the criticisms of Reviewer 2 because I agree with the authors  rebuttal.  However, there is still not enough support among the remaining reviews to justify acceptance. 
This paper heavily modifies standard time series VAE models to improve their representation learning abilities.  However, the resulting model seems like an ad hoc combination of tricks that lose most of the nice properties of VAEs.  The resulting method does not appear to be useful enough to justify itself, and it s not clear that the same ends couldn t be pursued using simpler, more general, and computationally cheaper approaches.
The main novelty of the paper lies in using multiple noise vectors to reconstruct the high resolution image in multiple ways. Then, the reconstruction with minimal loss is selected and updated to improve the fit against the target image. The most important control experiment in my opinion should compare this approach against the same architecture with only with m 1 noise vector (i.e., using a constant noise vector all the time). Unfortunately, the paper does not include such a comparison, which means the main hypothesis of the paper is not tested. Please include this experiment in the revised version of the paper.  PS: There is another high level concern regarding the use of PSNR or SSIM for evaluation of super resolution methods. As shown by "Pixel recursive super resolution (Dahl et al.)" and others, PSNR and SSIM metrics are only relevant in the low magnification regime, in which techniques based on MSE (mean squared error) are very competitive. Maybe you need to consider large magnification regime in which GAN and normalized flow based models are more relevant.
This paper proposes two techniques for improving self supervised learning with a vision transformer. The first improvement is using a multi stage ViT, which is very similar to Swin transformer and authors recognized this is not a major contribution. The authors further found that using a multi stage ViT does not produce discriminative patch representation, thus proposing the second improvement with a region level loss. While both improvements are not particularly novel by themselves, combining both leads to a strong empirical result. However, It does looks like the multi scale vision transformer is the major improvement as removing the regional loss only leads to less than 1% decrease in performance in most cases. In general this is a good "engineering" paper with a practical approach for improving self supervised learning with vision transformation and obtained strong results, thus it s worthy of publication.
This paper explores the practice of using lower dimensional embeddings to perform Bayesian optimization on high dimensional problems.  The authors identify several issues with performing such an optimization on a lower dimensional projection and propose solutions leading to better empirical performance of the optimization routine.  Overall the reviewers found the work well written and enjoyable.  However, the reviewers were concerned primarily about the connection to existing literature (R2) and the empirical analysis (R1, R3).  The authors claim that their method outperforms state of the art on a range of problems but the reviewers did not feel there was sufficient empirical evidence to back up this claim.   Unfortunately, as such the paper is not quite ready for publication.  The authors claim to have significantly expanded the experiments in the response period, however, which will likely make it much stronger for a future submission. 
Following the unanimous vote of the four submitted reviews, this paper is not ready for publication at ICLR. Among other concerns raised, the experiments need significant work.
This work proposes to reduce memory use in network training by quantizing the activations during backprop. It shows that this leads to only small drops in accuracy for resnets on CIFAR 10 and Imagenet for factors up to 8. The reviewers raised concerns about comparison to other approaches such as checkpointing, and questioned the technical novelty of the approach.  The authors were able to properly address the concerns around comparisons, but the issue around novelty remained. This could be compensated by strengthening the experimental results and leveraging the memory saving for instance to train larger networks. Resubmission is encouraged.
This paper adopts an idea from 1990 for reducing reliance on texture, and shows that this idea improves the quality of visual representations in a variety of tasks. Initially reviewer scores were 7/5/4 but those improved slightly to 7/6/4 (changed in comment, not final review) after the rebuttal stage  thus, one accept, one borderline, and one reject score. Reviewers have concerns about the great simplicity of the approach, where the only contribution is from prior work. Some reviewers request comparisons in a proper domain adaptation setting. While the large number of experimental settings somewhat balance out the concerns, overall, support for acceptance is not strong enough at this stage.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
This paper proposes the use of the determinantal point process to introduce the diversity in the prosodic features, including intonation, stress, and rhythm, in text to speech synthesis.  The proposed approach is certainly new, but the experimental support is of critical importance for this work.  One of the major points of discussion was the reliability of the experimental results.  In the original submission, the mean opinion score (MOS) of the proposed approach was inferior to the baseline.  The authors updated the experiments, which significantly (more than the confidence interval) lowers the MOS of a baseline.  This however makes the experimental results questionable.
Before the discussion phase nearly all reviewers had doubts about the comparison of the current work with state of the art works (notably Yan et al., 2020, RetroXpert, and GraphRETRO). The authors then compared with these works and emphasized that these works rely on hand crafted features. They argue that the fairest comparison is the one where each method uses the same sort of features during train/test time. This is because in certain real world settings we may not have accurate estimates of such features (e.g., atom mappings, templates, reaction centers). However, in the revised version of the paper the authors did not adhere to this concept of fair comparison in Table 4 of Appendix A.4. Here their method uses reaction centers as input while baselines do not. While the authors claimed that the comparison here was designed to show how reaction centers provided as input improved performance, this doesn t seem like a good way to show it: to isolate the improvement due to reaction center inputs you should fix everything else, i.e., the rest of the method.   Apart from the above contradiction, I buy the arguments of reviewers that distinguishing between methods that use hand crafted features and those that do not is not a meaningful distinction. One can apply atom mapping or reaction center discovery algorithms as data preprocessing before applying other methods. Ablation studies where such preprocessing is added or removed are interesting, but it is completely fair for any method to use such preprocessing before applying their method, it is up to the modeller.   I would have argued for acceptance had the authors either (a) just included results from SOTA methods (one, RetroXpert was published 1 month after the ICLR submission deadline), and/or (b) reran their approach with such preprocessing. However the authors ended up hurting the submission by emphasizing a difference between using handcrafted features and not, then contradicting their experimental setup in Table 4.  This is a good paper, but I agree it is not ready to be accepted at ICLR. I recommend the authors do the following: (a) use any preprocessing they want for their method and compare with the state of the art, (b) if they want they can run their method without any preprocessing as an interesting ablation study, (c) remove Table 4 (as (b) already does this type of an ablation study), (d) describe recent work through the lense of EBM, (e) resubmit to a strong ML conference. The new submission will be much stronger.
The paper proposes a new way of learning image representations from unlabeled data by predicting the image rotations. The problem formulation implicitly encourages the learned representation to be informative about the (foreground) object and its rotation. The idea is simple, but it turns out to be very effective. The authors demonstrate strong performance in multiple transfer learning scenarios, such as  ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR 10 classification.
This paper introduces a method for approximating real time recurrent learning (RTRL) in a more computationally efficient manner. Using a sparse approximation of the Jacobian, the authors show how they can reduce the computational costs of RTRL applied to sparse recurrent networks in a manner that introduces some bias, but which manages to preserve good performance on a variety of tasks.  The reviewers all agreed that the paper was interesting, and all four reviewers provided very thorough reviews with constructive criticisms. The authors made a very strong effort to attend to all of the reviewers  comments, and as a result, some scores were adjusted upward. By the end, all reviewers had provided scores above the acceptance threshold.  In the AC s opinion, this paper is of real interest to the community and may help to develop new approaches to training RNNs at large scale. As such, the AC believes that it should be accepted and considered for a spotlight.
Dear Authors,  The paper was received nicely and discussed during the rebuttal period. However, the current consensus suggests the paper requires another round of revisions before it gets accepted.   In particular:    it is not clear if the new method with randomization improves over the deterministic methods, either in theory and practice.   it is not clear how the assumptions made in this work compare to the existing ones and what the implications are, in terms of applications.  Reviewers were not satisfied by the replies received during the rebuttal period.  One reviewer stated that the argument "first coordinate method for this setting" is valid, but not sufficient to justify publication at this stage.   Best AC
This paper introduces a two level hierarchical reinforcement learning approach, applied to the problem of a robot searching for an object specified by an image.  The system incorporates a human specified subgoal space, and learns low level policies that balance the intrinsic and extrinsic rewards.  The method is tested in simulations against several baselines.  The reviewer discussion highlighted strengths and weaknesses of the paper.  One strength is the extensive comparisons with alternative approaches on this task.  The main weakness is the paper did not adequately distinguish between which aspects of the system were generic to HRL and which aspects are particular to robot object search.  The paper was not general enough to be understood as a generic HRL method. It was also ignoring much relevant background knowledge (robot mapping and navigation) if the paper is intended to be primarily about robot object search.  The paper did not convince the reviewers that the proposed method was desirable for either hierarchical reinforcement learning or for robot object search.  This paper is not ready for publication as the contribution was not sufficiently clear to the readers.  
Although all reviewers acknowledge that the paper has some merit, the work lacks in novelty. The idea of using normalisation techniques to reduce the domain discrepancy in UDA is well established in the DA and DG community. The theoretical analysis is an interesting first step in providing some insights on this class of approaches, but it still does not support an understanding on why some of these methods work better than others. Given all these considerations, the work is not ready for acceptance at ICLR.
This work proposes a new architecture for solving Ravens Progressive Matrices (RPM), a well known form of visual reasoning problem. The method relies on an operation that directly compares the final row and column (completed with different candidate answers) with the first two rows and columns. Doing this allows the network to perform better than previous approaches when measured on an in distribution test set on two RPM datasets, RAVEN and PGM.   As the reviewers pointed out, a strength of this work is the strong performance on the neutral split of these datasets and the fact that the methods do not (unlike some other approaches) require access to any annotations from the dataset other that knowledge of the structure of the RPM task and access to the candidate answers and the correct answer.   However, a noted weakness is the fact that the network reflects the structure of the task more directly than other approaches, which means that the insight is specific to the problem of solving RPMs. Another weakness is that the authors focus on the neutral (in distribution) splits. Reading the PGM paper, it is clear that the neutral split is not really the main focus of that dataset (it accounts for only 1/7 of the dataset), which seems to have been specifically developed as a benchmark for out of distribution generalisation. Indeed, the whole point of the RPM task is to measure the ability to induce abstract rules and principles from pixels, but without measuring out of distribution generalisation, can we really claim that any model has induced a  rule ?   The authors mitigate this issue to a small degree during the rebuttal by adding scores on the  interpolation  and  extrapolation  splits of the PGM dataset, but still do not consider the other splits where rule application is most clearly tested.   I note that the weakness described above also applies to lots of other published work involving PGM and RAVEN datasets.   In summary, this is a well executed, neat piece of work that shows a better way to fit a large dataset by incorporating knowledge of the structure of the data into the task. Because it does not consider the full benchmarks, only the in distribution splits, it falls short of showing that this enables better induction of abstract principles or rules. On the majority opinion of the reviewers, and because there are no scientific flaws in the work, I recommend acceptance with weak confidence pending wider calibration across the program. 
The paper appears unfinished in many ways: the experiments are preliminary, the paper completely ignored a large body of prior work on the subject, and the presentation needs substantial improvements. The authors did not provide a rebuttal.  I encourage the authors to refrain from submitting unfinished papers such as this one in the future, as it unnecessarily increases the load on a review system that is already strained.
The paper provides a way for explaining the reasoning of a neural network to humans in the form of a class specific structural concept graph (c SCG). The c SCG can be modified by humans. The modified c SCG can be incorporated in training a new student model. Experiments show that the new model performs better on classes that their corresponding c SCG have been modified. While all the reviewers agree that the paper puts forth an interesting idea, some concerns have been raised by reviewers about the scale of experiments and the lack of theoretical guarantee on the fidelity of the SCG. The authors have added two large scale experiments which confirm their previous results as part of their rebuttal. This paper is borderline and needs to be discussed further.
The reviewers were generally in agreement that the paper presents a valuable contribution and should be accepted for publication. However, I would strongly encourage the authors to carefully read over the reviews and address the suggestions and concerns insofar as possible for the final.
This paper introduces the Filtered CoPhy method, an approach for learning counterfactual reasoning of physical processes in pixel space. The approach enables forecasting raw videos over long horizons, without requiring strong supervision, e.g. object positions or scene properties.   The paper initially received one strong accept, one weak accept, and one weak reject recommendations. The main reviewers  concerns relate to clarifications and consolidations in experiments, including stronger baselines, experiments on real data, or more diversity on the datasets. The rebuttal did a good job in answering reviewers  concerns, especially by providing new experimental results and analysis. Eventually, all reviewers recommended a clear acceptance after authors  feedback.   The AC s own readings confirmed the reviewers  recommendations. The proposed approach is a meaningful extension of CoPhy for the unsupervised prediction at the pixel level. The proposed approach is solid, clearly described, and overcomes important limitations of previous methods. The dataset is also an important outcome for the community. Causality and counterfactual reasoning are of primary importance for the design of effective and explainable AI prediction models: this paper brings therefore an important contribution to the ICLR community.
This is a deep theoretical paper with results that I consider very interesting. I have *not* had time to check them myself, but I have background in these theoretical matters and the results seem reasonable to me   the hardness of even checking the quality of a solution is well known for partition functions (as well as hardness of any reasonable approximation), but the undecidability seems new   I assume it comes naturally and it is a very interesting result   I have seen similar decidability issues for #P: general probabilistic polynomial time Turing machines (it is unclear if a connection was sought here). Reviewers are all positive about the content, and authors have acknowledge some points for improvement.
The AC, the reviewers, and the authors had many discussions about the results in the paper during the discussion period. Below is a brief summary.   1. The paper shows that with $O(N^{⅔})$ parameters, a feedforward neural network can memorize $N$ inputs with arbitrary labels if the inputs satisfy some mild assumptions.   2. AC brought up in the discussion phase two central questions (one of which has been raised by R3 as well)   a. The results rely on using the infinite precisions of real values in the neural networks. The results wouldn’t hold if the precision of the neural nets is finite. The subtlety about the infinite precision was not prominently discussed in the paper.   b. It’s unclear to the AC what’s the practical implication of the results to generalization or optimization. In particular, it’s unclear to the AC what a finite sample memorization result within infinite precisions would entail. The AC thinks there is a fundamentally big difference between expressivity and finite sample expressivity. Expressivity is a very important topic to study, whereas the motivation for studying finite sample expressivity with infinite precision is unclear. (This is raised by R3 in the reviews as well.)  3. R1 supports the paper with the following main points (The AC rephrased these with some approximations, and might misinterpret to a certain degree.)  a. The paper’s result is surprising and mathematically non trivial.  b. Memorization is an important question to study. Many prior works study it, e.g., for showing tight VC dimension bound. It can be considered as an established setting.   c. Relying on the infinite dimension is not uncommon in ML theory.   4. R2 does not object R1’s point 3a, but seems to have a reservation to strongly recognize the technical significance of the results because it seems potentially likely to obtain the results by combining existing methods. Both R2 and the AC had some (partial) arguments to obtain the results of the paper with non standard architecture or non standard activations (which doesn’t subsume the paper’s results because the paper uses standard activations and feedforward net). This does make the AC unwilling to strongly recognize the technical significance of the result, but the AC doesn’t think the results are trivial either. In any case, this issue is not among the main concerns of the AC.   5. Regarding 3b, the AC thinks that unlike the prior work, the memorization results in this paper do not have an implication to the VC dimension (and in return, the dependency on $N$ is better), and this makes the significance and impact of the result in this paper somewhat unclear.  6. In summary, because the paper’s average score is somewhat borderline and because the AC has the concern 2a and 2b and was not quite convinced by the R1’s points or the authors’ responses, the AC is recommending rejection for the paper. The AC personally thinks the paper’s result has a strong potential and with additional clarification for the subtlety in 2a and additional results on the connections to generalization or optimization, the paper can be a strong one for future ML venues.  
Though the method suggested in this paper is interesting, theoretically motivated, and resulted in some practical improvement, the reviewers ultimately had low scores. The reasons for this are: 1) The improvements obtained by this method were rather small, especially on the standard datasets (CIFAR, Imagenet). 2) In the main results presented in the paper, it seems that a proper validation/test split was not done (which seems quite important for demonstrating the validity of this method). In some of the results, presented in supplementary, such a split was done, but this seems to decrease the performance of the method even more. 3) The method requires that features in the last hidden layer approximately span a low dimensional manifold. This seems like a major limitation for the accuracy of this method, which becomes approximate in datasets where the number of datapoints is larger than the size of the last hidden layer (which is the common case).  Therefore, I suggest the authors try to improve all of the above issues and re submit. For example, one simple way to address issue 3 and potentially improve the results (issue 1) is to use the same method on all the features in all the layers, instead of just the last layer. In other words, concatenate all the features and all the layers, and then add a linear layer from this concatenated feature vector directly to the network output, in a direction that is orthogonal to the data.   
This paper proposes methods to estimate how informative a single training data is wrt the weights and output of the neural network. All reviewers think this is an interesting problem and the proposed method is easy to implement. On the other hand, the reviewers also raise a few questions:  1.	There is a large body of work analyzing the informativeness of a feature wrt the model. The authors should compare their work to the feature importance analysis. 2.	The derived informativeness of a data depends not only on the network architecture, but also depends on the training algorithm, such as initialization and number of epochs. This makes the notion of data informativeness less general. 3.	The writing should be substantially improved. 
Many papers have been written on calibrating neural networks recently.  This paper presents a definition of calibration that is more robust than the popular ECE measure while also being more discerning than the Brier score.  Then it proposes a practical spline based method of post editing the output softmax scores to make them more calibrated.  The method is shown to be better than existing methods both on their measure and established measure (thanks to reviewer s questions on that.). The paper should be of much interest to the community.
The paper studied an interesting yet challenging problem in active learning and provided an intuitive heuristic for selecting informative subset(s) of training examples. The reviewers generally find the paper well presented and highlight that the clarity of the exposition of the issues of existing query heuristics, especially for training deep models with class imbalance data.  However, there are shared concerns among all reviewers in whether the existing experiments sufficiently justify the practical significance of the proposed heuristic (Reviewer 4ATq: Missing comparison against important baselines such as Gal et al 2017; Reviewer Cp2k: ablations of class and boundary balancing; Reviewer yngU: lack of comparison to SOTA and ablation for important hyperparameters; Reviewer oEcZ: lack comparison against SOTA). Reviewers also point out that the approximation guarantee is against an algorithm that is optimal wrt a somewhat ad hoc objective, which makes the theoretical components of the paper not as significant. Given the above concerns, the paper does not appear to be ready for acceptance at the current stage.
The paper aims to improve our understanding of GNNs for relational reasoning. In this regard, authors develop a conceptual framework unifying popular models (GNNs, Transformers, etc.) for analyzing their expressiveness and learning capacity. We thank the reviewers and authors for engaging in an active discussion. Based on author comments, the goal of the paper was more of a conceptual exposition, however this did not come across to the reviewers from the manuscript at first. Thus, a better presentation would definitely make the paper much more accessible and useful to the community. Moreover, there were some concerns about the significance of the exposition and better positioning would help (e.g., how the results help improve our understanding of GNNs). Thus, unfortunately I cannot recommend an acceptance of the paper in its current form.
The paper addresses the challenging and important problem of exploration in sparse rewards settings. The authors propose a novel use of contingency awareness, i.e., the agent s understanding of the environment features that are under its direct control, in combination with a count based approach to exploration. The model is trained using an inverse dynamics model and attention mechanism and is shown to be able to identify the controllable character. The resulting exploration approach achieves strong empirical results compared to alternative count based exploration techniques. The reviewers note that the novel approach has potential for opening up potential fruitful directions for follow up research. The obtained strong empirical results are another strong indication of the value of the proposed idea.   The reviewers mention several potential weaknesses. First, while the proposed idea is general, the specific implementation seems targetted specifically towards Atari games. While Atari is a popular benchmark domain, this raises questions as to whether insights can be more generally applied. Second, several questions were raised regarding the motivation for some of the presented modeling choices (e.g., loss terms) as well as their impact on the empirical results. Ablation studies were recommended as a step to resolving these questions Reviewer 3 questioned whether the learned state representation could be directly used as an additional input to the agent, and if it would improve performance. Finally, several related works were suggested that should be included in the discussion of related work.  The authors carefully addressed the issues raised by the reviewers, running additional comparisons and adding to the original empirical insights. Several issues of clarity were resolved in the paper and in the discussion. Reviewer 3 engaged with the authors and confirmed that they are satisfied with the resulting submission. The AC judges that the suggestions of reviewer 1 have been addressed to a satisfactory level. A remaining issue regarding results reporting was raised anonymously towards the end of the review period, and the AC encourages the authors to address this issue in their camera ready version.
This paper proposes a tool to visualizing the behaviour of deep RL agents, for example to observe the behaviour of an agent in critical scenarios. The idea is to learn a generative model of the environment and use it to artificially generate novel states in order to induce specific agent actions. States can then be generated such as to optimize a given target function, for example states where the agent takes a specific actions or states which are high/low reward. They evaluate the proposed visualization on Atari games and on a driving simulation environment, where the authors use their approach, to investigate the behaviour of different deep RL agents such as DQN.  The paper is very controversial. On the one hand, as far as we know, this is the first approach that explicitly generates states that are meant to induce specific agent behaviour, although one could relate this to adversarial samples generation. Interpretability in deep RL is a known problem and this work could bring an interesting tool to the community. However, the proposed approach lacks theoretical foundations, thus feels quite ad hoc, and results are limited to a qualitative, visual, evaluation. At the same time, one could say that the approach is not more ad hoc than other gradient saliency visualization approaches, and one could argue that the lack of theoretical soundness is due to the difficulty of defining good measures of interpretability and that apply well to image based environments.  Nonetheless, this paper is a step in the good direction in a field that could really benefit from it. 
This paper considers the problem of learning models for NLP tasks that are less reliant on artifacts and other dataset specific features that are unlikely to be reliable for new datasets. This is an important problem because these biases limit out of distribution generalization. Prior work has considered models that explicitly factor out known biases. This work proposes using an ensemble of weak learners to implicitly identify some of these biases and train a more robust model. The work shows that weak learners can capture some of the same biases that humans identify, and that the resulting trained model is significantly more robust on adversarially designed challenge tasks while sacrificing little accuracy on the test sets of the original data sets.  The paper s method is useful, straightforward, and intuitively appealing. The experiments are generally well conducted. Some of the reviewers raised questions about evaluating on tasks with unknown biases. The authors addressed these concerns in discussion and we encourage them to include this in the final version of the paper using the additional page.
This paper tackles the problem of detection out of distribution (OoD) samples. The proposed solution is based on a Bayesian variational autoencoder. The authors show that information theoretic measures applied on the posterior distribution over the decoder parameters can be used to detect OoD samples. The resulting approach is shown to outperform baselines in experiments conducted on three benchmarks (CIFAR 10 vs SVNH and two based on FashionMNIST).  Following the rebuttal, major concerns remained regarding the justification of the approach. The reason why relying on active learning principles should allow for OoD detection would need to be clarified, and the use of the effective sample size (ESS) would require a stronger motivation. Overall, although a theoretically informed OoD strategy is indeed interesting and relevant, reviewers were not convinced by the provided theoretical justifications. I therefore recommend to reject this paper.
### Summary  This paper builds on previous work on sparse training that shows the many modern sparse training techniques do no better than a random pruning technique that selects layer wise rations, but otherwise randomly selects which weights within a layer to remove.  The key difference in this work is to take these existing results and scale the size of the network to show that as the size of the network increases, the smaller   as measured in pruning ratio   a matching subnetwork becomes.  ### Discussion   #### Strengths  Places an emphasis on simple techniques  #### Weakness  Significant overlap with previous work. Prior already demonstrated the equivalence of random pruning and contemporary pruning at initialization techniques.  ### Recommendation  I recommend Accept (poster). However, I do want to stress that there is a significant overlap with previous work. The paper does appropriately attribute observations to previous work. However, there is some risk that readers may misinterpret the title and claim results as a wholly new observation about random pruning, where the reality is instead much more nuanced. Given that the work points to new methodological directions on considering scaling the network as an additional parameter to consider in pruning observations, I do believe these results   even if narrower in scope that can be interpreted   provide value to the community.
The reviewers found the paper insightful and the authors explanations well provided. However the paper would benefit from more systematic empirical evaluation and corresponding theoretical intuition.
This paper proposes a method to quantify the uncertainty for RNN, which is an important problem in various applications. It provides results in a variety of domains demonstrating that the proposed method outperforms baselines. However, these experiments would benefit greatly from a comparison with SOTA methods for the specific tasks in addition to the considered baselines (e.g. covariance propagation, prior network, and orthonormal certificates). The paper could also be improved by adding a theoretical justification to explain how the Gumbel softmax function is able to capture the underlying data and model uncertainty.
This paper introduces an approach for structured exploration based on graph based representations.  While a number of the ideas in the paper are quite interesting and relevant to the ICLR community, the reviewers were generally in agreement about several concerns, which were discussed after the author response. These concerns include the ad hoc nature of the approach, the limited technical novelty, and the difficulty of the experimental domains (and whether the approach could be applied to a more general class of challenging long horizon problems such as those in prior works). Overall, the paper is not quite ready for publication at ICLR.
The paper proposes two approaches to topic modeling supervised by survival analysis. The reviewers find some problems in novelty,  algorithm and experiments, which is not ready for publish.
This paper proposes a representation learning approach from cardiac signals, which adopts contrastive learning to incorporates knowledge on patient specificity. This problem is highly motivating because of potential application to medicine and healthcare and large amounts of accumulating unlabeled physiological data. The presentation needs to be substantially improved – e.g., lack of clear description of the contribution, the details such as input data and network architecture, a clear description of the downstream tasks, missing explanations of equations, etc – some of which were addressed in the revision. Major concerns include lack of comparison with relevant prior methods developed for cardic signals, and the need for further refinement of domain knowledge based intuitions. 
This work proposes a simple and intuitive way to improve how to learn a communication protocol off policy in the non stationary situation in which messages received in the past do not reflect an agent s current policy. The authors introduce a communication correction that relabels the received message adjusting it to the current policy. The authors show that this method, besides being simple, is effective in a number of experiments. As observed by some reviewers, an issue with the method is that it is not clear how it would scale up to more complex environments than those considered. However, the authors addressed the concerns during the response phase, both adding new experiments, and with a clear statement of what are the outstanding issues. The paper is certainly a clever and solid contribution to the area of multi agent communication learning, and I am strongly in favour of accepting it. 
The paper studies the setting of group based fairness under the so called demographic shift, where the marginal distribution of the data remains the same conditional on the subgroup but the subgroup distribution can change. It provides a class of algorithms which give high confidence guarantees under demographic shift in both the known and unknown shift setting.  Overall the paper is a worthwhile contribution: it provides a new angle to the important problem of group based fairness with good theoretical and empirical results.
The paper proposes a modification to the Transformer network, which mostly consists in changing how the attention heads are combined. The contribution is incremental, and its novelty is limited. The results demonstrate an improvement over the baseline at the cost of a more complicated training procedure with more hyper parameters, and it is possible that with similar tuning the baseline performance could be improved in a similar way.
This paper proposes a combined method to address stragglers and adversaries in federated learning. Stragglers are overcome by allowing staleness in model aggregation. Adversaries are handled by using a public dataset to identify poisoned devices and adjusting their weights when doing model aggregation. However, the reviewers raised concerns about: * The correctness of Theorem 1 * The novelty of the paper given that there has been significant previous work on straggler mitigation and robust aggregation in distributed learning.   As a result, I am unable to recommend the acceptance of the paper. However, the idea is certainly promising, and if built upon more rigorously can result in a nice and impactful paper. I hope that the authors can take the reviewers  feedback into account when revising the paper for a future submission!
This paper performs a comprehensive investigation on self supervised pre training with streaming data. Reviewers agreed that the task studied in this paper is highly practical and important, and the analysis is insightful. Meanwhile, reviewers raised some concerns such as empirical setups and insights. In the revised paper, the authors provided more justifications and added more analysis such as few shot evaluation and uniformity analysis. After the discussion period, most reviewers are positive about this paper.  Overall, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version.
All four knowledgeable referees have indicated reject due to many concerns. In particular, reviewers pointed out that the novelty of this paper is not clear because the difference from related work is very limited (i.e., the difference from Z. Wang and S. Ji is not clear, other than using one additional layer),  and they were concerned that the results of the experiment are not convincing (For example, the results reported in this paper are significantly inferior to those reported in other papers, the GNN architecture used is limited, and the performance difference especially in the additional experiments in the revision, is very marginal). No reviewers were convinced by the authors  claims even through the author s rebuttal and revision.  One important note: Reviewers have stated that they did not explicitly check the identity of the author and did not pose a problem on this, but if we follow the link specified in the original submission, we can see the identity of the author, which may be considered as a violation of the double blind policy. This is a small and regrettable mistake, but it can be a serious problem in the review process. In this review process, reviewers unanimously suggested rejection even ignoring this issue, but it seems that you need to pay attention in your future submissions. 
This paper introduces a new structured bandit problem called congested bandits, where the expected reward for an arm is a decaying function of how many times it has been played recently. This model aims to address problems such as route recommendation, in which recommended routes tend to get congested (hence yield lower rewards). Different from prior work on bandits with non stationary reward distributions, the effect of congestion in this model resets after Delta time steps. The authors show that this problem can be formulated as a structured MDP and propose a variant of UCRL2 that learns to recommend the optimal arm for each congestion state. They show that the proposed algorithm achieves a policy regret bound that significantly improves upon UCRL2. They also propose a variant of their algorithm tailored for the linear stochastic contextual bandit setup with the associated analysis.  Unfortunately, this is a rather niche problem formulation and it fails to truly capture congestion models for traffic routing platforms (or other practical routing problems) which serve as the main motivation for the paper.  Moreover, the novelty is limited: the setting is very close to existing non stationary bandit models and the proposed algorithms are straightforward extensions of existing strategies. A possible way for supporting the novelty of the setting could be to improve its theoretical understanding through a lower bounds analysis, which is currently lacking from the paper. Although this paper contains interesting and well articulated ideas, contributions are not sufficient.
The consensus among all reviewers was to reject this paper, and the authors did not provide a rebuttal.
This paper proposes “Continual Federated Learning (CFL)” to study time evolving heterogeneous data. To do this the authors introduce time drift to capture data heterogeneity across time. The authors also present some preliminary convergence results. Finally, the authors carryout numerical experiments in time varying and heterogeneous settings. The reviewers identified the following strengths: (1) combining FL and CL is interesting, (2) the development of a new algorithm and providing some initial analysis is a good step. They also identified weaknesses as follows: (1) limited technical novelty as the use of replay buffer is quite standard, (2) cumbersome and not easy to interpret results, (3) lack of time evolving patterns with a common component (4) lack of different metrics that demonstrate how the algorithm is able to maintain accuracy as time shifts occur, (5) lack of questionable assumptions. The reviewers had a very bimodal view advocating acceptance with a score of 8 and 2 advocating a rejection and neither group changed their opinion. Although the authors thorough responses did alleviate the concerns IMO. My own reading of the paper is that this is an interesting paper working on an emerging area. However, I must agree with some of the reviewers that the final conclusions are not easy to interpret, and the assumptions are not fully motivated. After this is carried out, I think the novelty of the paper can also become much clear. Therefore, I cannot strongly advocate acceptance of the paper in its currently state given the scores. However, I very strongly encourage the authors to submit to a future ML venue after addressing the remaining comments of the reviewers. I would also like to commend the authors for a very strong rebuttal sorry the final decision couldn’t be more favorable given the borderline ratings and the aforementioned issues.
I join all five reviewers in recommending acceptance.  There was some discussion about a comparison with WaveGrad (Chen et al., 2020), a contemporaneous work that explores a similar modelling approach for speech generation. While I agree that such a comparison is a useful addition to the manuscript, I do not think it is reasonable to request anything beyond an acknowledgement and citation of the work from the authors as a condition for acceptance. Further discussion and comparison experiments could be valuable, but I believe that should not factor into the final decision. My position is most similar to Reviewer 4 s in this sense. The current version of the manuscript briefly discusses the differences between WaveGrad and DiffWave, which I think is more than sufficient. (As an aside, another difference potentially worth discussing is that the "noise schedule" for WaveGrad can be adapted at inference time, enabling a trade off between inference speed and sample quality, which I believe is not possible for DiffWave in its current form.)  There was some debate about the weakly conditioned generation results; I believe they are a nice addition to the paper, although it would have been suitable for publication without them. They certainly do not detract from it, and might inspire further work in weakly conditioned audio generation (e.g. music). There were also concerns about the clarity of writing, which I believe the authors have addressed in the current version of the manuscript.  This work stands out because it applies a relatively fresh idea in generative modelling to a domain of great practical importance, which has long been dominated by traditional likelihood based models, with compelling results. While this implies a limited degree of technical novelty, I do not think that is grounds for rejection, and in fact I would argue that making new ideas work well for practical problems is just as important.
The paper was praised for being clearly written, well motivated, and for addressing an important problem: measuring intrinsic robustness. It improves the previous results on intrinsic robustness based on concentration of data distribution, by incorporating the constraint on the label uncertainty of the models. This requires information on label uncertainty for each data sample rarely available (here CIFAR 10H is considered), but could open new directions for future work on adversarial robustness, confidence calibration or label noises.
The work brings little novelty compared to existing literature. 
The paper presents an interesting idea but all reviewers pointed out problems with the writing (eg clarity of the motivation) and with the motivation of the experiments and link to the contest. The rebuttal helped, but it is clear that the paper requires more work before being acceptable to ICLR.
The authors propose a method for automatic tuning of learning rates. The reviewers liked the idea but felt that there are much more extensive experiments to be done especially better baselines. Also, clarifying what aspect is automated is important, because no method can be truly automatic: they all have some hyperparameters. 
This paper proposes a new framework which combines pruning and model distillation techniques for model acceleration. The reviewers have a consensus on rejection due to limited novelty.
This paper proposes an innovative method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continual learning benchmarks. It is a clear accept. There was good discussion between the reviewers and authors that addressed a number of minor issues, including clarifying that the method has the same computational complexity as backpropagation. The authors are encouraged to make sure that these points are addressed in the final version of the paper.
The submission applies architecture search to object detection architectures. The work is fairly incremental but the results are reasonable. After revision, the scores are 8, 6, 6, 3. The reviewer who gave "3" wrote after the authors  responses and revision that "Authors  responses partly resolved my concerns on the experiments. I have no object to accept this paper. [sic]". The AC recommends adopting the majority recommendation and accepting the paper.
All three reviewers initially recommended reject.  The main concerns were: 1) weak technical contribution and insight [R1, R2, R3, R4]; 2) incremental novelty (another variation of SiamFC) [R1, R2, R3]; 3) unconvincing experiment results against missing SOTA [R1, R2, R3];  The author s response did not assuage these concerns.
The paper discusses the problem of how to augment cross modal retrieval for the task of multi modal classification   it uses image caption pairs to improve downstream multimodal learning, and shows improvement in the task of visual question answering. However, the paper has the following weaknesses: (a) lack of novelty, (b) lack of thorough empirical evaluation, (c) the complex model did not give significant gains.
The paper tackles the Q value overestimation problem by proposing a regularization technique to maximize diversity in representation space, preventing ensemble "collapse", in order to improve the efficacy of techniques such as Maxmin and Ensemble Q learning. Reviewers praised the originality of the method and the interesting connections drawn to economic theory, and seemed to agree that the method is somewhat effective. On the other hand, R3 pointed out that hyperparameters are tuned per domain, the number of domains considered is small (echoed by R4), and criticized the paper for failing to truly validate its central hypothesis experimentally (echoed by R1).  R4 raised the issue of unfair comparisons in between ensemble and non ensemble methods, while R1 raised a multitude of criticisms ranging from ahistorical attributions to confusing figures, which I will not exhaustively repeat here. Based on the reviews, and the fact that the majority of reviewers  concerns remain entirely unaddressed (the authors only responded to R2), this manuscript is not a candidate for acceptance at this time. 
In this paper, the authors extend the FLAMBE to the infinite horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community.   As the reviewers suggested, there are still several minors to be addressed:    The extension of the proposed algorithm for finite horizon MDP should be added.    The directly comparison between the sample complexity of FLAMBE and the proposed algorithm in infinite horizon MDP is not appropriate. The authors should clarify the difference here.    The organization of the proof is not clear. As reviewer suggested, the one step back trick should be emphasized for better significance of the submission.
The paper considers an important problem in medical applications of deep learning, such as variability/stability of  model s predictions in face of various perturbations in the model (e.g., random seed), and evaluates different approaches to capturing model uncertainty. However, it appears to be little innovation in terms of machine learning methodology, so ICLR might not be the best venue for this work, while perhaps other venues focused more on medical applications might be a better fit.   
This paper considers the challenge of sparse reward reinforcement learning through intrinsic reward generation based on the deviation in predictions of an ensemble of dynamics models. This is combined with PPO and evaluated in some Mujoco domains.  The main issue here was with the way the sparse rewards were provided in the experiments, which was artificial and could lead to a number of problems with the reward structure and partial observability. The work was also considered incremental in its novelty. These concerns were not adequately rebutted, and so as it stands this paper should be rejected.
The paper is rejected based on unanimous reviews.
This paper proposes enhancing contextualized word embeddings learned by Transformers by modeling long range dependencies via a deep topic model, using a Poisson Gamma Belief Network (PGBN). The experimental results show incorporating topic information can further improve the performance of Transformers. While this is an interesting idea, reviewers pointed out some weaknesses:    GLUE evaluation is not a test of long term dependencies, it remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task.   The improvement over the baseline does not seem to be significant.   The ablation study could be improved and more experiments could be done to understand the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer.   A comparison of the model performance for different lengths of input sequences would be helpful.   There are many recent methods for long.range transformer transformer variants, it would be interesting to compare them against the proposed latent topic based method.  Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection.
The paper needs more revisions and better presentation of empirical study.
This paper proposes the algorithm which they call DEO* SGD, which is a combination of the ideas of the generalized DEO scheme, denoted by DEO*, to facilitate exploration (Section 3.1), adoption of stochastic gradient descent (SGD) in the exploration chains (i.e., those chains except the one with the lowest temperature) (Section 4), and use of adaptive tuning of learning rates (Section 4.2). The proposal is applied experimentally in Section 5 to demonstrate superiority of the proposal over existing approaches.  The initial review scores of the four reviewers were one positive and three negatives. Most reviewers positively evaluated the proposal, including the proposal of DEO* and its theoretical analysis, as well as its empirical usefulness in deep learning for a computer vision task. On the other hand, some reviewers showed concern about soundness of the proposal. Upon reading the reviews and the author responses, as well as the paper itself, I think that this paper lacks a clear statement on its objective.  * **What does "uncertainty approximation" mean?:** The paper title would imply that the objective of the proposal in this paper is for "uncertainty approximation," but I could not find any concrete description on what it exactly is. * **Sampling versus optimization:** The methods of Langevin dynamics, or more generally Markov chain Monte Carlo methods, have been used for two distinct purposes: sampling and optimization. In any case fast relaxation towards equilibrium would be of practical importance. For sampling purposes it is also important to assure that the stationary distribution of the Markov chain corresponds to the target distribution (In Langevin dynamics the target distribution would be the canonical ensemble defined by the energy $U(\cdot)$ and the temperature $\tau$). For optimization purposes, however, the assurance of the stationary distribution to be equal to the target distribution would be less of concern. It seems that the authors  interest would be in optimization rather than in sampling, but it is not clearly stated. * **Soundness issue:** As Reviewer mbau pointed out, DEO* does not have a guarantee of convergence to the target distribution. I thought that if the objective of this paper would be in optimization rather than in sampling, the existence of approximation already in DEO* might be thought of as a minor problem, as the proposal already has other approximations introduced in Section 4. The authors claim that this problem does not affect the main body of the paper, but I feel that it would affect the overall organization of the paper, as the current organization seems to presume that approximation only resides in the adoption of the SGD based exploration kernels with deterministic swap. In any case, this problem has been acknowledged by the authors themselves, as well as Reviewer ofJx.  In particular, the detailed discussion between the authors and Reviewer mbau has been very fruitful in clarifying technical subtleties in this manuscript, including the soundness issue mentioned above. At the same time, it would imply that this paper still has room for improvement.  An additional point I would like to mention is that this paper is not really self contained, in the sense that several key notions and quantities are not defined or only defined in the Supplementary Materials ($\tilde{U}$ is not explicitly defined at all, the terms "swap time" and "round trip time" are defined in Appendix A.5, $\sigma_p$ in Corollary 1 and Lemma 2 is defined in Appendix A.1).   All these weaknesses make me to think that another round of revision would be appropriate to properly judge the quality of this paper, whereas there is no such option within the review procedure of ICLR. I therefore cannot recommend acceptance of this paper at least in its current form.  Minor points (page and line numbers refer to the revised version):   Abstract, line 5: "given sufficient many $P$ chains" would be better phrased as "given $P$ chains", as the big O notation usually assumes the large P asymptotic.   In several places, there are periods after "Figure" and "Table", which are not needed.   Page 3, line 32: In Lemma 2 there is apparently no such term found as "the second quadratic term". It should appear only after having assumed the equi acceptance/rejection rates in equation (4), so that the sum becomes proportional to $P$.   Theorem 1: "the maximal round trip time" should certainly be "the minimal round trip time". / is the ceiling function(. T  > , t)he round trip time   Table 1: I did not really understand what "non asymptotic" / "asymptotic" mean, as the big O notation used here should by definition be asymptotic.   Corollary 1: the optimal (number of) chains   Page 4, line 34: The abbreviation SGLD is not defined in this paper.   Page 4, line 36: similar(ly) to   Equation (6): The sign of the last term should be " ".
This paper receives positive reviews. The authors provide additional results and justifications during the rebuttal phase. All reviewers find this paper interesting and the contributions are sufficient for this conference. The area chair agrees with the reviewers and recommends it be accepted for presentation.
This paper extends the idea of successor representations. Typically the reward is compute linearly on top of states in this setting but the authors relax it to have a quadratic form.   ${\bf Pros}$: 1. A novel formulation of the successor representation where the reward does not follow the linearity assumption 2. The idea of using a second order term for the reward branch is interesting and could have meaningful implications for learning and exploration.   ${\bf Cons}$: 1. All authors agree that the experimental results do not clearly validate the advantage of this method. More work is needed to establish the effects of using this particular reward structure on a wide variety of tasks  2. Both R2 and R4 were unconvinced of the limitations of the linearity assumptions in the original successor representation formulation   especially in the case when the state is represented by a non linear function approximator.  The ideas presented in this paper are quite interesting and promising. But more experimental work is needed to show the benefits of this approach. 
While considerable effort went into improving the paper during the author response period, the concerns outlined by reviewer 2 remain and the aggregate score across reviewers reflects this issue. The AC recommends rejection with strong encouragement to resubmit this work to another high quality venue upon further revision to the work.
I thank authors and reviewers for discussions. Reviewers found the paper (specially the CAT r method proposed in the rebuttal period) interesting but there are some remaining concerns about the significance of the results and experiments. Given all, I think the paper still needs a bit of more work before being accepted. I encourage authors to address comments raised by the reviewers to improve their paper.    AC
In this paper, the authors propose a new layer by layer training approach for GNN in particular for a large graph. The proposed approach can be easily parallelizable and scale well to a large graph. Reviewers are concerned about the novelty of the approach and the lack of theoretical analysis, and it is not well addressed by the rebuttal. Therefore, this paper is below the acceptance threshold of ICLR. I encourage the authors to revise the paper based on the reviewer s comments and resubmit it to a future venue.
This paper proposes a method for improving training of text generation with GANs by performing discrimination between different generated examples, instead of solely between real and generated examples.  R3 and R1 appreciated the general idea, and thought that while there are still concerns, overall the paper seems to be interesting enough to warrant publication at ICLR. R2 has a rating of "weak reject", but I tend to agree with the authors that comparison with other methods that use different model architectures is orthogonal to the contribution of this paper.  In sum, I think that this paper would likely make a good contribution to ICLR and recommend acceptance.
The paper attempts to develop a method for learning latent representations using deep predictive coding and deconvolutional networks. However, the theoretical motivation for the proposed model in relation to existing methods (such as original predictive coding, deconvolutional networks, ladder networks, etc.), as well as the empirical comparison against them is unclear. The experimental results on the CIFAR10 dataset do not provide much insight on what kind of meaningful/improved representations can be learned in comparison to existing methods, both qualitatively and quantitatively. No rebuttal was provided. 
The authors have extended previous publications on curiosity driven, intrinsically motivated RL with this broad empirical study on the effectiveness of the curiosity algorithm on many game environments, the merits of different feature sets, and limitations of the approach. The paper is well written and should be of interest to the community. The experiments are well conceived and seem to validate the general effectiveness of curiosity. However, the paper does not actually have any novel contribution compared against prior work, and there are no great insights or takeaways from the empirical study. Therefore, the reviewers were somewhat divided on how confident they were that the paper should be accepted. Overall, the AC agrees that it is a valuable paper that should be accepted even though it does not deliver any algorithmic novelty.
The paper proofs that reinforcement learning (using two different algorithms) converge to iterative dominance solutions for a class of multi player games (dominance solvable games).   There was a lively discussion around the paper. However, two of the reviewers remain unconvinced of the novelty of the approach,  pointing to [1] and [2], with [1] only pertaining to supermodular games. The exact contribution over such existing results is currently not addressed in the manuscript.  There were also concerns about the scaling and applicability of the results, as dominance solvable games are limited.   [1] http://www.parisschoolofeconomics.eu/docs/guesnerie roger/milgromroberts90.pdf [2] Friedman, James W., and Claudio Mezzetti. "Learning in games by random sampling." Journal of Economic Theory 98.1 (2001): 55 84.
Joint optimization of dimensionality reduction and temporal clusters. Results suggest performance improvement in a variety of scenarios versus a baseline of a recent state of art clustering method.  Pro:   Joint optimization may be new and results suggest performance improvement when done on NASA Magnetospheric Multiscale (MMS) Mission.  Con:   Small datasets evaluated, impact unclear   Breadth of possible applications unclear   Similarities exist to prior works. Significance of novelty not clear.   Unanimous consensus among reviewers that work is not in a state to be accepted.
The reviewers generally agreed that the technical novelty of the work was limited, and the experimental evaluation was insufficient to make up for this, evaluating the method only on relatively simple toy tasks. As much, I do not think that the paper is ready for publication at this time.
This paper proposes a new class of divergences that are also sensitive to the variance of the estimator. The proposed additional variance penalty term introduces a bias term and acts directly on each component of the statistical estimator. By choosing the penalty parameter one can trade bias versus variance.   The results on synthetic examples look promising and suggest that with this technique, it is feasible to decrease the estimation error relative to the baseline statistical estimator. This is demonstrated to be particularly pronounced for certain Renyi divergences in the large order parameter alpha regime. Two applications (detection of subpopulations and disentangled representation learning in speech) are provided.  The opinions about the work were fairly divided. Both positive reviews have lower confidence and are rather short and do not fully justify the high rating. Two high confidence reviews are negative and raise several critical points. In a nutshell, reviewer JtE9 complains mainly about the insufficient experimental evaluation while 3SLV raises several concerns regarding readability, mathematical notation, lacking details of the proofs, as well as technicalities regarding the consistency. The authors have partially answered the concerns.  While there seems to be a consensus that the paper is interesting and makes a valid contribution, the introduction of the VP term defines a new estimation problem and both the choice and interpretation of lambda becomes critical. In particular, the key question is understanding the effect of lambda for various tasks where divergence estimation is crucial and I am not fully convinced if the chosen applications are the best for convincingly demonstrating the utility as these require somewhat application specific motivation. I would rather see result of standard benchmark datasets (such as estimating the divergence between two subsets of MNIST images to detect subtle distribution shifts).  The synthetic experiments are good but this section could be improved as well to get the message accross. Rather than delving directly to the findings, this section could first justify what needs to be measured and what are the control variables (number of samples, Renyi order etc)  In light of the comments raised by the reviewers, I feel that this paper can benefit from a further iteration and clarification of the experimental section before being accepted to a venue like ICLR.
This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor. The paper is well motivated and well explained, easy to follow. This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant. The experimental section is comprehensive, with several benchmarks, and show clear improvements. The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns. This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend this paper to be presented as a spotlight.
This paper aims to present a new representation learning framework for supervised learning based on finding a representation such that the input is conditionally independent given the representation, the components of the representations are independent and the representation is rotation invariant. While there were both positive and negative assessments of this paper by the reviewers, there are 3 major concerns that lead me to recommend rejecting this paper: 1. Most importantly, experiments do not seem to be conclusive as they do not properly ablate the specific aspects of this method. More specifically, the authors compare their deep learning based approach with non deep learning approaches but do not compare against deep learning baselines. This makes it impossible to assess the merit of the proposed approach (which also appears to be complicated) over much simpler baselines. 2. The required properties of the representations do not seem to be properly motivated.  3. The paper refers to their produced representations as disentangled representations. As pointed out by AnonReviewer4, this appears not to be consistent with prior uses of that word in the community. 
This paper proposes a new algorithm called Continuous Sparsification (CS) to search for winning tickets (in the context of the Lottery Ticket Hypothesis from Frankle & Carbin (2019)), as an alternative to the Iterative Magnitude Pruning (IMP) algorithm proposed therein. CS continuously removes parameters from a network during training, and learns the sub network s structure with gradient based methods instead of relying on pruning strategies. The papers shows empirically that CS finds lottery tickets that outperforms the ones learned by ITS with up to 5 times faster search, when measured in number of training epochs.  While this paper presents a novel contribution of pruning and of finding winning lottery tickets and is very well written, there are some concerns raised by the reviewers regarding the current evaluation. The paper presents no concrete data on the comparative costs of performing CS and IMP even though the core claim is that CS is more efficient. The paper does not disclose enough detail to compute these costs, and it seems like CS is more expensive than IMP for standard workflows. Moreover, the current presentation of the data through "pareto curves" is misleadingly favorable to CS. The reviewers suggest including more experiments on ImageNet and  a more thorough evaluation as a pruning technique beyond the lottery ticket hypothesis. We recommend the authors to address the detailed reviewers  comments in an eventual ressubmission. 
The paper aims to devise a distributed multi task privacy preserving framework for image processing. In this regard, author propose partitioning neural network models into task specific heads/tails and a common task agnostic feature backbone (body). A training procedure is designed which is claimed to be privacy preserving wherein the head and tail is trained locally on the client or using federated learning when multiple clients share a task, while the main backbone/body is trained in a centralized manner by collecting appropriate gradients from the clients. Making easy to follow code is also highly appreciated. We thank the reviewers and authors for engaging in an active discussion and also updating the paper. While the new version is definitely resolves some of the concerns of the reviewers, some still remain. Privacy preserving in title and in main body of the paper seems misleading. Proposed method doesn t provide any guarantees for privacy (also pointed out by many reviewers). The author response doesn t seem to be convincing and other federated learning papers do not claim privacy unless having some specific mechanism like adding noise, secure aggregate, etc. Also, the reviewers are in consensus that novelty as well as large scale empirical evaluation is limited.
This paper studies the expressivity, complexity and unpredictability of emergent languages in referential games. The authors defined measures of complexity and unpredictability and empirically showed that the expressivity of emergent languages is a trade off between the complexity and unpredictability of the context that the languages are used in. They introduced a contrastive loss based training method that alleviates the collapse of message types seen using standard referential loss functions.  The paper is controversial among the reviewers. On the positive side, most liked how the paper has a clearly stated hypothesis and extensive evaluations which makes a clear contribution to the field of emergent languages. On the negative side, the paper only shows the results in an artificial setting where the key variables are highly simplified (e.g. size of candidates). The main negative review argue the authors used an inappropriate definition of unpredictability and that the batch size is actually the key independent variable instead of what is claimed. The paper does somewhat equate batch size with the candidate size that is so important to their results (after eq (1)), but they seem to measure candidate size in the key figures. Perhaps an experiment controlling for batch size independently of the candidates size can address this issue. On the point of defining unpredictability, the other reviewers and I find the given definition to be reasonable and at least defensible. However, the reviewer remained unconvinced. More generally, the paper relies on one definition of the concepts measured in one setting to make a general claim, which is at risk of missing other important variables. Overall, most reviewers found the scope to be sufficient, and two improved their scores after the discussion.  Recommendation: accept
This paper uses the interpolation property to design a new optimization algorithm for deep learning, which computes an adaptive learning rate in closed form at each iteration. The authors also analyzed the convergence rate of the proposed algorithm in the stochastic convex optimization setting. Experiments on several benchmark neural networks and datasets verify the effectiveness of the proposed algorithm. This is a borderline paper and has been carefully discussed. The main objection of the reviewers include: (1) The interplay between regularization and the interpolation property is not clear; and (2) the proposed algorithm  is no better than SGD in any of the benchmarks except one, where SGD s learning rate is set to be a constant. After the author response, this paper still does not gather sufficient support. So I encourage the authors to improve this paper and resubmit it to future conference.
This paper studies the training of multi branch networks, i.e., networks formed by linearly combining multiple disjoint branches of the same architecture.  The four reviewers seem to reach a consensus that the paper is not ready for publication for ICLR. 
The authors have presented an empirical study of generalization and regularization in DQN. They evaluate generalization on different variants of Atari games and show that dropout and L2 regularization are beneficial. The paper does not contain any major revelations, nor does it propose new algorithms or approaches, but it is a well written and clear demonstration, and it would be interesting to the deep RL community. However, the reviewers did not feel that the paper met the bar for publication at ICLR because the experiments were not more comprehensive, which would be expected for an empirical study. The AC will side with the reviewers but hopes that the authors will expand their study and resubmit to another venue in the future.
All reviewers were clear in their opinion that the paper deserves to be accepted. One reviewer also indicated a wish to increase the score from 6 to 7 but was not able to do that, so it isn t reflected in the final score. The reviewers appreciated the methodological contribution made by the paper.
There was some discussion on this paper, both with the authors and between reviewers. On the one hand, there is a general agreement that the empirical results suggesting that spectral clustering based method can be competitive with SOTA methods on node classification benchmark is an interesting result. One the other hand, reviewers did not find a significantly novel contribution in the methodology proposed, and found that the empirical evaluation lacks depth and details to be really informative (eg, to understand why some methods work or not on some benchmarks). There is therefore a consensus that the paper is not ready for ICLR in its current form, but we hope that the reviews and discussion will help the authors prepare a revised version in the future.
Well written paper that motivates through theoretical analysis new memory writing methods in memory augmented neural networks. Extensive experimental analysis support and demonstrate the advantages of the new solutions over other recurrent architectures. Reviewers suggested extension and clarification of the analysis presented in the paper, for example, for different memory sizes. The paper was revised accordingly. Another important suggestion was considering ACT as a baseline. Authors explained clearly why it wasn t considered as a baseline, and updated the paper to include references and explanations in the paper as well.
This paper has received reviews from multiple experts who raise a litany of issues. These have been addressed quite convincingly by the authors, but I believe that ultimately this work needs to go through another round of reviewing, and this cannot be achieved in the context of ICLR s reviewing setup. I look forward to reading the final version of the paper in the near future.
This paper considers transferability measures both in the supervised and unsupervised domain. It identifies instabilities in the way that H score is computed and proposes to correct the issue with a shrinkage based covariance estimations. The proposed fix results in 80% absolute gain over the original H score and makes it competitive with state of the art LogME metric. The new shrinkage based H score is much faster to compute.  Reviewers agree that the paper makes interesting and important contributions. In particular, the reviewers appreciate that the paper takes a deeper look at existing metrics and propose valuable fixes instead of proposing yet another new metric. The paper demonstrates depth of statistic knowledge and proposes shrinkage operators to estimate high dimensional covariance.   There are a few shortcomings of the paper, however, that suggests that the paper can benefit of another round of improvement. In particular, the paper is very dense with little motivation. Some of the choices in the paper can be motivated better. For instance, the hypothesis of lack of robustness in estimating H score is not demonstrated empirically. The reviewers also felt that the paper should extend experiments to other domains beyond image.
This paper is board line but in the end below the standards for ICLR. Firstly this paper could use significant polishing. The text has significant grammar and style issues: incorrect words, phrases and tenses; incomplete sentences; entire sections of the paper containing only lists, etc. The paper is in need of significant editing.  This of course is not enough to merit rejection, but there are concerns about the contribution of the new method, experiment details, and the topic of study. The results are reported from either a single run or unknown number of runs of the learning system, which is not acceptable even if the we suspect the variance is low. The proposed approach relies on pre training a feature extractor which in many ways side steps the forgetting/interference problem rather than what we really need: new algorithms that processes the training data in ways the mitigate interference by learning representations. In general the reviewers found it very difficult to access the fairness of the comparisons dues do differences between how different methods make use of stored data and pre training. The reviewers highlighted the similarity between the propose approach and recent work in angle of generative modeling / out of distribution (OOD) detection which suggests that the proposed approach has limited utility (as detailed by R1) and that OOD baselines were missing. Finally, the CL problem formulation explored here, where task identifiers are available during training and data is i.i.d, is of limited utility. Its hard to imagine how approaches that learn individual networks for each task could scale to more realistic problem formulations.  All reviewers agreed the paper s experiments were borderline and the paper has substantial issues. There are too many revisions to be done.
This paper proposes a mathematical framework to theoretically understand and quantify the benefit of self supervision on the downstream tasks. The theoretical analyses in this paper are concrete and the authors conducted experiments to support their claims. However, the current version still has the following weaknesses.      This paper would benefit from incorporating the reviewers  comments (which was complained by multiple reviewers) and the authors  responses if any.   The authors need to make it clear in the abstract and introduction that (1) this paper only considers the *reconstruction based* SSL, instead of the general SSL, and (2) addresses the discrepancy between the practice and the proposed framework.     In the post rebuttal phase, Reviewer 2 pointed out "Lemma 3 seems much more meaningful after the clarifications. I also notice that R5 concerned about the discrepancy in downstream task setup and may doubt the mathematical framework in this paper. In fact, I agree with R5. There is indeed a gap between the proposed mathematical model and the practical SSL algorithms. There is still some work need the authors to complete, i.e., $\mathbb{E}[Y|X_1]\approx \mathbb{E}[Y|X_1,X_2]$."
The paper aims to improve image and video compression keeping in mind computation cost. In this regard, authors propose variants of Swin Transformer for image and video coding. The experimental results shows that Transformer based transforms can replace Conv based transforms in image and video compression, and simultaneously achieving better rate distortion performance at much faster decode times, i.e. resulting in a better rate distortion computation trade off. We thank the reviewers and authors for engaging in an active discussion. The reviewers found some results to be surprising (in a good way) and are in a consensus that the empirical results are strong across datasets for image and video compression. For completeness, the authors should provide FLOPs or CPU runtimes in the final version so that one can compare to methods like VTM even if CPU is not the desired hardware for proposed method.
Summary of discussions: R1 was positive on the paper in their initial evaluation, and although dissatisfied with the author s feedback, continued to support the paper. I agree with R1 s assessment that other reviewers  call for more theory is somewhat unfair, considering the fact that very similar papers don t usually include theoretical justification beyond intuitive motivation.  By contrast, R3 is the most negative on the paper, leaning towards rejection. The main concern is that open questions remain as to whether the reported performance can be attributed to the architecture, or the loss function proposed. This is an important point to clarify, and further ablation studies would make the paper stronger.  After considering the strengths and weaknesses of this work, the final decision was to reject. Authors are encouraged to improve this promising work and resubmit to a future venue.
There is no author response for this paper. The paper presents a multi task learning framework as a unified view on the previous methods for tackling catastrophic forgetting in continual learning. In light of this framework, the authors propose to minimize the KL divergence between the predictions of the previous optimal model and the current model using some stored samples from the previous tasks.   The consensus among all three reviewers and AC is that the paper lacks (1) novelty, as the proposed approach is similar if not identical to Learning without forgetting (LwF)[Li&Hoiem 2017] with the difference that the KL divergence is computed on samples kept from the previous tasks (and LwF uses samples from the current task). Methodological and experimental comparison to LwF is crucial to assess the benefits and novelty of the proposed approach.   Also the reviewers address other potential weaknesses and give suggestions for improvement: (2) empirical evaluations can be substantially improved with sensitivity analysis of the hyper parameters on the validation data (R3), indicating errors and error bars for all results (R3 and R2), using more challenging and realistic experimental setting where the data comes from different domains (R1), justifying the results better   see R2’s questions; (3) lack of clarity and motivation in Section 3.1   see R2’s and R1’s suggestions for how to improve clarity and potentially take advantage of the current task to probably correct the previous models prediction when it was wrong.  AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This paper proposes an interesting analysis of the limitations of WGANs as well as a solution to these limitations. I am not too convinced by the experimental part as, as some of the reviewers have mentioned, it relies on hyperparameters which can be hard to tune.  The more theoretical part, even if it could be written with more care as pointed out by reviewer 2, is nonetheless interesting and could stir discussion. I think it would be a good addition to ICLR as a poster.
The paper presents some interesting insights, but all reviewers have agreed that it does not meet the bar of ICLR. The theoretical results require revision as several issues have been indicated in the reviews. The authors have tried to correct them during the rebuttal, but the reviewers remain unconvinced.  Also the novelty is limited as re ranking is a well known concept and decoupling of head and tail labels is an approach often used in practice across many applications.  The authors should also clarify the way the RankNet method is used and implemented to clarify the issue raised by Reviewer 1. Finally, let me notice that adjusting thresholds for labels has been considered in the XMLC literature, in the context of optimization of the macro F measure (Extreme F measure Maximization using Sparse Probability Estimates, ICML 2016).  
This work proposes a framework to search for the topology of an artificial neural network jointly with the network training, via a genetic algorithm that can decide structural actions, such as addition or removal of neurons and layers. An extra heuristic based on Bayesian information criterion helps the optimization process decide on its decisions about the topology. They demonstrate improvements over baseline fully connected networks on SVHN and (augmented) CIFAR 10.  Reviewers and myself agree that this is an interesting idea, and that the paper is easy to follow. While I may not agree that we need to achieve SOTA on these datasets, or see large scale ImageNet type experiments for novel ideas, I agree with the reviewers, esp R1 s point that the current experiments are not satisfactory to meet the bar for acceptance at ICLR.  CIFAR 10 and SVHN are well established tasks, and showing baseline accuracy of 75%/48% on them respectively doesn t seem to do them justice, especially when most methods (even with low compute requirements) can get > 95% on both, for the past few years. For this work to be of interest to the broader community, it needs to be improved to incorporate at least respectable baselines on these small datasets, and perhaps be improved to work beyond fully connected networks.  At this stage, we need to see a revision of the method and see improvements before an acceptance decision can be made.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    strong qualitative and quantitative results   a good ablative analysis of the proposed method.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    clarity could be improved (and was much improved in the revision).   somewhat limited novelty.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
This paper presents an idea for interpolating between two points in the decision space of a black box classifier in the image space, while producing plausible images along the interpolation path. The presentation is clear and the experiments support the premise of the model. While the proposed technique can be used to help understanding how a classifier works, I have strong reservations in calling the generated samples "explanations". In particular, there is no reason for the true explanation of how the classifier works to lie in the manifold of plausible images. This constraint is more of a feature to please humans rather than to explain the geometry of the decision boundary. I believe this paper will be well received and I suggested acceptance, but I believe it will be of limited usefulness for robust understanding of the decision boundary of classifiers.
The paper proposes multiresolution and equivariant generative models.  Experimental results for several applications are shown.  Pros:   A first hierarchical generative model with multiresolution and equivariance.   Extensive experiments  Cons:   Marginal novelty (multiresolution and permutation equivalence each is not novel for graph neural networks.   State of the art methods are not compared as baselines.   Some standard metrics are not evaluated, and the used metrics are questionable (some generated molecules might not be stable although the chemical validity is 100%).   Time/space complexity evaluation is missing.  The authors did not address some of the serious concerns in the rebuttal.
The authors address the challenge of sample efficient learning in multi agent systems. They propose a model that distinguishes actions in terms of their semantics, specifically in terms of whether they influence the acting agent and environment or whether they influence other agents. This additional structure is shown to substantially benefit learning speed when composed with a range of state of the art multi agent RL algorithms. During the rebuttal, technical questions were well addressed and the overall quality of the paper improved. The paper provides interesting novel insights on how the proposed structure improves learning.
The paper provides an interesting combination of existing techniques (such as GCN and and the Bernoulli Poisson link) to address the problem of overlapping community detection. However, there were concerns about lack of novelty, evaluation metrics, and missing comparisons with previous work. The authors did not provide a response to address these concerns.
The reviewers are split about this paper and did not come to a consensus: on one hand they agreed that the paper has valuable theoretical contributions and addresses an important problem in current ML literature, on the other hand they would have liked to see empirical results on a real world problem setting. After going through the paper and the discussion I have decided to vote to reject for the following reason: I believe the reviewers  concerns about empirical results is not just a request for applying this to more datasets (which is easy to satisfy and I don t think is grounds for rejection), but is actually for a clearer connection for how this work would be used in the machine learning problems described in the introduction and related work sections. What would really help this paper is a real world running example, in place of the blue plus example, in Figure 1 (I think the blue plus problem is still a useful experimental tool and should be evaluated, but it doesn t clarify the real world use cases of this work. This led the reviewers to look to the experimental section for clarification on this, but this wasn t clarified there either. The authors  response to these concerns was an out of scope argument: the goal of this paper is to derive/test theoretical results, and there are a number of possible use cases we could apply this to. The authors argue that the current work sends  a strong signal to the ICLR community that the Prover Verifier Game is interesting and promising . I m sorry but I disagree here: the authors need to do more to convince the ICLR community that this is a framework that will solve outstanding problems in ML. This is solved if the authors (a) run their approach on a real world dataset in a paper they cite in the related work, (b) they include baselines in this experiment, and (c) if they add this as a running example throughout with a figure that explains this real world example. With these additions the paper will be a much stronger submission.
The reviewers have pointed out several major deficiencies of the paper, which the authors decided not to address.
The paper presents a model for learning spiking representations. The basic model is a a deep autoencoder trained end to end with a biophysical generative model and results are presented on EMG and sEMG data, with the aim to motivate further research in self supervised learning.  The reviewers raised several points about the paper. Reviewer 1 raised concerns about lack of context on surrounding work, clarity of the model itself and motivating the loss. Reviewer 2 pointed out strengths of the paper in its simplicity and the importance of this problem, but also raised concerns about the papers clarity, again motivations on the loss function and sensibility of design choices. The authors responded to the feedback from reviewer 1, but overall the reviewer did not think their scores should be changed.  The paper in its current form is not yet ready for acceptance, and we hope there has been useful feedback from the reviewing process for their future research.
This paper considers the generalized target shift setting for domain adaptation and proposes an optimal transport map based approach to it. The considered setting for domain adaptation is rather general and of practical use. The proposed method seems sensible, as supported by the theoretical identifiability and empirical results.   It is worth noting that the way to cite previous work seems to be improved. For instance, in the first paragraph of Introduction, the authors reviewed various settings for domain adaptation. For model shift, the authors cited previous work. However, when discussing covariate shift, target shift, and generalized target shift, the authors did not cite the original work that provides the categorization. For completeness, the authors may want to consider including the setting of conditional shift as well, which has received a number of applications in domain adaptation in computer vision. I believe the categorization of target shift, conditional shift, and generalized target shift was provided by Zhang et al. (2013). This work should also be cited when the authors give the problem definition in Section 2.1. The quality of the paper will be even better if the authors cite previous work in all the right places this may also make the authors  contribution clearer.
This paper takes steps towards a theory of convergence for TD(0) with non linear function approximation.  The paper provides two theoretical results.  One result bounds the error when training the sum of linear and homogenous parameterized functions.  The second result shows global convergence when the environment dynamics are sufficiently reversible  and the differentiable function approximation is sufficiently well conditioned.  The paper provides additional insight using a family of environments with partially reversible dynamics.  The reviewers commented on several aspects of this work.  The reviewers wrote that the presentation was clear and that the topic was relevant.  The reviewers were satisfied with the correctness of the results.  The reviewers liked the result that state value function estimation error is bounded when using homogeneous functions. They also noted that the deep networks in common use are not homogeneous so this result does not apply directly. The result showing global convergence of TD(0) with partial reversibility was also appreciated. Finally, the reviewers liked the family of examples.  This paper is acceptable for publication as the presentation was clear, the results are solid, and the research direction could lead to additional insights.
This paper proposed a LBPNet for character recognition, which introduces the LBP feature extraction into deep learning. Reviewers are confused on implementation and not convinced on experiments. The only score 6 reviewer is also concerned "Empirically weak, practical advantage wrt to literature unclear". Only evaluating on MNIST/SVHN etc is not convincing to demo the effectiveness of the proposed method.
The paper presents a variational Bayesian approach for quantising neural network weights and makes interesting and useful steps in this increasingly popular area of deep learning.
This paper proposes a learning based approach for solving combinatorial optimization problems such as routine using continuous optimizers. The key idea is to learn a continuous latent space via conditional VAE to represent solutions and perform search in this latent space for new problems at the test time. The approach is novel and experiments showed good results including ablation analysis.   Reviewer comments are adequately addressed during the response phase and I find the changes satisfactory. Overall, this is a good paper and I recommend accepting it.  One last comment: It would be a great addition if the paper could add discussion about the applicability of this approach to arbitrary combinatorial optimization problems and what design choices are critical to come up with an effective instantiation.
The authors propose to use pruning to study/interpret learned CNNs. The reviewers believed the results were not surprising and/or had no practical relevance. Unlike in many cases, two of the reviewers acknowledged reading the rebuttals, but were unswayed.
This paper proposes Dirichlet Neural Architecture Search (DrNAS), a new NAS algorithm that formulates NAS as a distribution architecture search problem. The paper shows theoretically that DrNAS implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search) and presents very strong empirical results on several benchmarks, including the tabular benchmark NAS Bench 201.   The reviews and discussion put this paper very close to the acceptance threshold, so I read it in detail myself to act as a tie breaker. I see a lot of positive aspects of this paper: + it tackles a very important and timely problem + the method implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search) + empirical results are very strong + the paper includes insightful ablation studies for the most important parts of the algorithm + the progressive architecture learning approach is a general contribution that reduces the memory complexity and also improves basic DARTS + the paper uses a tabular benchmark to yield results that are directly comparable to those of other papers + the method s hyperparameters are kept fixed across the different benchmarks, which underlines its robustness.  There are also some negative aspects:   The method was not originally derived from a Bayesian point of view. Per request of the reviewers, the relation to variational inference has been added to the appendix, but the difference between the L2 regularization and using the explicit KL regularization that falls out of the Bayesian treatment remains. Nevertheless, the new appendix helps to clear up the relationship.    The paper does not mention availability of the code. This is a must in modern NAS research, as many papers have exposed the poor reproducibility of research in NAS. Fortunately for the authors, I have already seen (independently of this submission) that the code is available on github, but I urge the authors to provide an anonymous repo for review in future submissions, since it was purely by chance that I saw it this time.    Regarding the point of exploration vs. exploitation, the authors emphasize that in contrast to Gumbel softmax based methods, such as GDAS and SNAS, with DrNAS there is no need for a cooling schedule. While it is nice to keep the number of hyperparameters small, this also appears to give up control of when the method switches from exploration to exploitation. In practical applications of AutoML, there will be a time budget, and while a cooling schedule can be adapted to fit this budget, it would be suboptimal if DrNAS is still in the exploration phase by the end of the budget, or has already switched to exploitation after, e.g., 5% of the budget. It would be good if the authors could briefly discuss this issue in their final version (if only by acknowledging that this can be a problem).    Minor negative points   * The paper sometimes uses jargon, and I believe not even always correctly: e.g., even by googling I did not find such a thing as the "iregularized incomplete beta function", it s also not in the original reference by Jankowiak. I only found the "incomplete beta function" (and the regular one).    * The author names for several references are garbled. This is likely due to not replacing a comma between the authors with an "and" in the bibtex file.   * The paper lacks citations for several of the methods it uses, e.g., Adam, cutout, cosine annealing, label smoothing, auxiliary towers, etc. There is no limit on references, and it is standard to cite these concepts to remain more self contained.  * The experimental results of GDAS on NB201 CIFAR 100 do not seem to align with the numbers in the NB201 paper. Did you use the numbers from the paper or rerun this method yourself? Please clarify and check this for the final version. The point of tabular benchmarks is to have comparability and consistency across papers!   * Please have the paper proofread for Grammar again, there are several avoidable errors. E.g., in the first sentence, "lots of attentions"  > "lots of attention". Also things like "alone"  > "along", "down" >"done" etc.  Overall, I think this is a very nice paper, introducing an empirically very strong NAS method that is also theoretically shown to implicitly regularize the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search). I am therefore recommending acceptance. I would like to ask the authors to go through all the reviews again and fix any remaining points in the paper for the final version.
This paper proposes an end to end approach for abstractive summarization of on line discussions. The approach is contrary to the previous work that first disentangles discussions, and the summarizes them, and aims to tackle transfer of disentanglement errors in the pipeline. The proposed method is a hierarchical encoder   hierarchical decoder architecture. Experimental results on two corpora demonstrate the benefits of the proposed approach. The reviewers are concerned about the synthetic nature of the datasets, limited novelty given the previous work, lack of clear explanation of whether disentanglement is actually needed for summarization, and simpler baselines in comparison to the state of the art. Hence, I recommend rejecting the paper.
The paper presents a framework named Wasserstein bounded GANs which generalizes WGAN. The paper shows that WBGAN can improve stability.  The reviewers raised several questions about the method and the experiments, but these were not addressed.  I encourage the authors to revise the draft and resubmit to a different venue.
The authors propose a novel model based reinforcement learning algorithm. The key difference with previous approaches is that the authors use gradients through the learned model. They present theoretical results on error bounds for their approach and a monotonic improvement theorem. In the small sample regime, they show improved performance over previous approaches.  After the revisions, reviewers raised a few concerns: The results are only for 100,000 steps, which does not support the claim that the models achieves the same asymptotic performance as model – free algorithms would. The results would be stronger as the experiments were run with more than 3 random seats. In the revised version of the text, it s unclear if the authors are using target networks.  Overall, I think the paper introduces some interesting ideas and shows improved performance over existing approaches. I recommend acceptance on the condition that the authors tone down their claims or back them up with empirical evidence. Currently, I don t see evidence for the claim that the method achieves similar asymptotic performance to model free algorithms or the claim that their approach allows for longer horizons than previous approaches.
The authors propose a method for attacking neural NLP models based on individual word importance ("WordsWorth" scores).  This is an interesting, timely topic and there may be some interesting ideas here, but at present the paper suffers from poor presentation which makes it difficult to discern the contribution. Presentation issues aside, it seems that the experimental setup is missing key baselines (an issue not sufficiently addressed by the author response). 
The paper presents a quite rigorous analysis of approximate implicit differentiation with warm starts applied to strongly convex upper level/strongly convex lower level and nonconvex upper level/strongly convex lower level bilevel optimization algorithms in a very general yet also very practical framework. They allow for stochastic errors in the algorithms solving the upper and lower level problems, making their work practical and applicable to real problems in machine learning (hyperparameter optimization), while analyzing in a way that is agnostic towards which algorithms are specifically used for the lower and upper level problems.  Three out of four reviewers were rather positive of the paper (scores: 6, 6, 8). One reviewer was very negative (score: 3). To my knowledge, the authors have convincingly answered all the points raised by the reviewer. Unfortunately, the reviewer did not follow up.  Similarly to reviewers, I found sections 1 3 to be extremely well written and to give a nice overview of the field. Section 4 had slight clarity issues (dense notation) that were addressed in the revision. Reviewer 6zLQ partially proof read proofs.  Overall, I recommend acceptance as a poster, as this paper is advancing stochastic implicit differentiation and should be of interest to many at the ICLR conference.
This paper presents a systematic breakdown and evaluation of several assumptions and algorithmic choices for pruning algorithms. As covered in the reviews, the evaluation and its conclusion offers a timely contribution to the broader community.  In particular, this paper uncovers the observation that precisely modeling the loss (and hence minimizing the drop in loss after pruning) may not in fact yield improvements in pruning. This is an important observation as the community continues to propose new techniques with the justification that their improved performance results from improved loss modeling.  A significant concern on the part of the reviewers is the limited practical prescription offered by the paper. Specifically, the paper does not propose a new algorithm. It also doesn’t necessarily identify why this interesting phenomenon emerges.  For example, to the latter, it doesn t articulate what features of the network or loss landscape is indicative of this property.  Ultimately, the decision for this paper is very challenging given the reviews. Whether or not a phenomena is interesting is an inherently subjective consideration. Moreover, without a clear technical prescription or path forward that can be evaluated on its merits, the reviews fall into two categories of either 1) those that   by my estimation   felt personally inspired by the work and 2) those that could not intuit the impact of the observation.    A significant complication is that the narrative of the paper includes claims around addressing locality and convergence which, if not read with the understanding that contributions here are simply a synthesis of current work, appear as claims to novelty (when these techniques have no or limited novelty). This is a source of contention in at least one review.  Given this partition, my recommendation is Reject.  For future versions of this paper, I recommend that the authors narrow the claimed contributions to exclusively focus on the final observation that modeling the loss may not be as important as thought.   The work in this paper on developing the ideas around convergence and locality can, instead, be cast as efforts to provide best available baselines for the topline claim.  I believe these changes will eliminate a significant source of distraction, enabling readers (and reviewers) to avoid any attempt to evaluate the novelty of  the locality and convergence narratives, which have indeed been considered in other work in various ways.  An additional step that I highly recommend for this paper to unambiguously clear the bar is to identify with what the performance of pruning does correlate. Appendix C.4 provides an evaluation of two recent gradient preservation methods. Unfortunately, the paper did not present if, instead, the preservation of the gradient correlated with additional performance.   In essence, the paper need not solve the mystery by providing a SoTA algorithm that exploits the right features of the problem for pruning. However, it would be valuable to provide a roadmap for future directions along with an articulation of the challenges down those directions. 
This paper presents a new multi document summarization task of trying to write a wikipedia article based on its sources. Reviewers found the paper and the task clear to understand and well explained. The modeling aspects are clear as well, although lacking justification. Reviewers are split on the originality of the task, saying that it is certainly big, but wondering if that makes it difficult to compare with. The main split was the feeling that "the paper presents strong quantitative results and qualitative examples. " versus a frustration that the experimental results did not take into account extractive baselines or analysis. However the authors provide a significantly updated version of the work targeting many of these concerns, which does alleviate some of the main issues. For these reasons, despite one low review, my recommendation is that this work be accepted as a very interesting contribution.  
 pros:   The paper is clear and easy to read   Both Reviewer 1 and Reviewer 2 found the empirical evaluation to be good  cons:   Some of the reviewers felt that the proposed approach lacked novelty (e.g. with respect to Nogueira and Cho)   Some of the architecture choices seem complicated and it was not fully clear to the reviewers (even after the rebuttal) how and why things were working better in this approach than in other similar ones.  I think this is a good paper but it doesn t quite meet the bar for acceptance at this time.   
This paper proposes a method which patches/edits a pre trained neural network s predictions on problematic data points. They do this without the need for retraining the network on the entire data, by only using a few steps of stochastic gradient descent, and thereby avoiding influencing model behaviour on other samples. The post patching training can encourage reliability, locality and efficiency by using a loss function which incorporates these three criteria weighted by hyperparameters. Experiments are done on CIFAR 10 toy experiments, large scale image classification with adversarial examples, and machine translation. The reviews are generally positive, with significant author response, a new improved version of the paper, and further discussion. This is a well written paper with convincing results, and it addresses a serious problem for production models, I therefore recommend that it is accepted. 
All reviewers agreed that the paper proposes some interesting and novel ideas on the use of OT for pooling. It also provides some nice insights and strong experimental results. As suggested by one of the reviewer, a discussion about the impact of the number of references may be of interest though.  
All Reviewers point out that the paper, although having some strong points, does not meet the bar for a highly selective machine learning conference like ICLR. Hence, my recommendation is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Well written paper.   Ambitious task.   Code will be released.  Cons:   Unclear task terminology (music production; misleading title).   Mixed results.   Experimental design could be improved.   Exposition could be improved (technical details missing).   Lack of comparison (for instance with other CycleGAN variants; more experimental setups).   Lack of discussion on the use of a source algorithm for pre processing data.
This paper proposes a method to reduce the instability issues of off policy deep reinforcement learning.  The proposed solution constructs a simple MDP from the experience in the agent s replay memory.  This graph is used to compute a lower bound for the values from the original problem. Incorporating this bound can make the learning system less prone to soft divergence.  The reviewers appreciated the motivation of the paper and the direction of this research.  However, the reviewers were not convinced that the formulation was sufficiently complete.  There were concerns that the method makes additional assumptions about the data distribution (the presence of state aggregation and the absence of repeated states in continuous spaces).  Reviewers found related work was missing.  The reviewers also found multiple aspects of the presentation unclear even after the author response.    This paper is not ready for publication as the generality of the proposed method was not sufficiently clear to the reviewers after the author response.
Reviewers recognize the proposed method of hierarchical extension to ALI to be potentially novel and interesting but have expressed strong concerns on the experiments section. The paper also needs to have comparisons with relevant hierarchical generative model baselines. Not suitable for publication in its current form.
This paper studies self supervised video representations with a multi modal learning process that the authors then use for performance on a variety of tasks. The main contribution of the paper is a successful effort to incorporate BERT like models into vision tasks.  Reviewers acknowledged the extensive empirical evaluation and the good performance of the approach. However, they raised some concerns about the lack of clarity and the absence of analysis and interpretation of the results. The AC shares this view, and recommends rejection at this time, encouraging the authors to revise their work addressing these analysis and clarity questions.  
 The paper proposes an method for investigating robustness of graph neural nets for node classification problem; training time attacks for perturbing graph structure are generated using  meta learning approach. Reviewers agree that the contribution is novel and empirical results support the validity of the approach.  
This paper studies the problem of adversarial training for graph neural networks. The proposed method is build on the free training approach, and more specifically FreeLB, with some additional tricks including bias perturbation (for node classification) and unbounded attacks.  While these additions are potentially useful, there are only limited investigation into their effect.  Putting aside the technical distinctions of the method with prior work, this paper can also be viewed as an empirical study of adversarial training techniques on graph data with various GNN architectures.  It is worth noting that overall the conclusions on "adversarial training" are positive, we do see consistent improvement over a variety of architectures and tasks.   The issues however, is that it is unclear whether these improvements can be similarly achieved using prior technique like FreeLB (oblation is only done on one single task, where biased perturbation is shown to lead to minor improvement).   The paper also provides some results showing the effect of depth of the network as well as different training strategies such as batch norm, dropout with general adversarial training.  These results are interesting to see but do seem to be limited in both scope and depth.  It appears that the authors have two goals in mind, one is to propose FLAG and demonstrate its usefulness, and the other is to provide a better understanding of how adversarial training works for GNNs in general.  Given the limited novelty of FLAG compared to prior methods, the main contribution actually comes from the later part, which unfortunately is somewhat underdeveloped.   
This paper presents a simple, reasonable, alternative to target networks.  Given the effectiveness of target networks, and the fact that they are still somewhat poorly understood, this is a good topic for consideration.  It is unfortunate that the paper did not have more depth, in terms of analysis and/or analytical experiments that expose the properties of the suggested approach, and the mechanism still seems heuristic (and inspired by the success of target networks, and similar) more than principled.  That said, the proposed mechanism does seem somewhat effective (even if performance differences are not very pronounced), and is clean and simple to implement.  This version of the paper is rejected because we believe the paper could be a lot better than it currently is.  If the proposed regularisation mechanism is really as good as the authors argue, then it should be possible (and hopefully even easy) to demonstrate this clearly in more settings (e.g., in more algorithms).  Alternatively or additionally, the authors could consider digging deeper into the understanding of the method.  For instance, the paper often argues that target networks slow down learning, but (naively?) one could argue the exact same point (in general) for regularisation: this will trade off stability for speed.  It could be that the proposed mechanism is indeed a better way to achieve this trade off, but this is currently argued heuristically and not really proven (either theoretically, or with sufficient empirical evidence) (For what it is worth, I personally did not find Section 3.2 particularly enlightening, because it is known these TD algorithms are not actually gradient algorithms, and hence considering  losses  and  gradients  in this way does not convince me we are getting at an actual deeper understanding of the dynamics of these algorithms.)  I wholeheartedly encourage the authors to take the comments and suggestions to heart and use these to improve the paper (as they have already started to do during this reviewing cycle), because I believe that there could be quite a good paper on this topic.  I hope the authors can convince themselves and their readers more convincingly that this idea is an actual, lasting contribution to the literature.  Ultimately, if they can, this will make the paper more impactful.  So although I appreciate this decision will come as a disappointment, I hope the authors also see this as an opportunity to make a larger research impact.  In particular, I would encourage considering: 1) comparing to our current theoretical understanding of target networks (see, e.g., [1]); 2) considering the effect of multi step updates (shown in, e.g., [2] and [3] to be quite effective); and 3) considering whether the proposed approach (or a variation thereof) could be understood as a more fundamental idea: could this update be derived from first principles?  [1] Shangtong Zhang, Hengshuai Yao, Shimon Whiteson (2021). Breaking the Deadly Triad with a Target Network.  [2] Hessel et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning.   [3] van Hasselt et al. (2018). Deep Reinforcement Learning and the Deadly Triad.
This paper proposes a model for learning using ensemble clustering. The reviewers found the general idea promising. However, while promising, all reviewers noted that in its curent form the paper is not fit for publication. The reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work. Because of all these reasons, this paper does not meet the bar of acceptance. I recommend the authors take into account the feedback provided in the reviews and discussion and resubmit to another venue.
The reviewers agreed that this paper is not quite ready for publication at ICLR.  One of the reviewers thought the paper was well written and easy to follow while the two others said the opposite.  One of the main criticisms was issues with the composition.  The paper seems to lack a clear formal explanation of the problem and the proposed methodology.  The reviewers in general weren t convinced by the experiments, complaining about the lack of a required baseline and that the proposed method doesn t seem to significantly help in the experiment presented.  Pros:   The proposed idea is interesting   The problem is timely and of interest to the community   Addresses multiple important problems at the intersection of ML and RL in sequence generation  Cons:   Novel but somewhat incremental   The experiments are not compelling (i.e. the results are not strong)   A necessary baseline is missing   Significant issues with the writing   both in terms of clarity and correctness.
The paper is interesting, and the update to the paper and additional experiments has already improved it in many ways, but the paper still does still not have as much impact as it could, by further strengthening the comparisons and usefulness in many of situations of current practice.
The paper proposes a platform for benchmarking, and in particular hardware agnostic evaluation of machine learning models. This is an important problem as our field strives for more reproducibility.  This was a very confusing paper to discuss and review, since most of the reviewers (and myself) do not know much about the area. Two of the reviewers found the paper contributions sufficient to be (weakly) accepted. The third reviewer had many issues with the work and engaged in a lengthy debate with the authors, but there was strong disagreement regarding their understanding of the scope of the paper as a Tools/Systems submission.  Given the lack of consensus, I must recommend rejection at this time, but highly encourage the authors to take the feedback into account and resubmit to a future venue.
This paper present a learning method for speeding up of LP, and apply it to the TSP problem.  Reviewers and AC agree that the idea is quite interesting and promising. However, I think the paper is far from being ready to publish in various aspects:  (a) much more editorial efforts are necessary (b) the TPS application of small scale is not super appealing   Hence, I recommend rejection.
this submission has two results; (1) it defines what it means for the optimal representation is, although this is rather uninteresting that it simply says that if the representation from a model is going to be used based on some given metric, the cost function should directly reflect it, and (2) it shows that different choices of encoding and decoding have different implications. as with most of the reviewers, i found these to be a rather weak contribution.
The authors study the limiting dynamics of a simple linear regression model. They use an underdamped Langevin equation which is quite common in the literature. Although the reviewers welcome the direction and the attempt to understand the dynamics of a simple model, the novelty of the paper is limited. As an example, the paper shows that the key ingredient driving these dynamics is not the original training loss, but a modified loss. As pointed out by the reviewers, this has already been pointed out in multiple papers. One important problem is the tendency of the paper to oversell the results (including the title), which makes it difficult to clearly separate the contributions made in the paper. After a discussion with the reviewers, the overall feeling did not change.  I can therefore not recommend acceptance. I strongly recommend the authors do a significant rewrite of the paper in order to clearly separate what contributions are truly novel and also improve the discussion of prior work.
The paper proposes the use of contrastive learning to learn patient specific representations from medical data. The authors show how their method can be used to find similar patients within and across datasets.   The paper has some issues, as indicated by the reviewers:   similarity to past work; in the response to R1, the authors specify differences to related papers; however, experimental comparisons should still be performed against CLOCS and DROPS   the evaluation is not fully convincing (the follow up comments of Reviewer 3), including the retrieval of similar patients
The paper investigates a new approach to classification of irregularly sampled and unaligned multi modal time series via set function mapping. Experiment results on health care datasets are reported to demonstrate the effectiveness of the proposed approach.   The idea of extending set functions to address missing value in time series is interesting and novel. The paper does a good job at motivating the methods and describing the proposed solution. The authors did a good job at addressing the concerns of the reviewers.   During the discussion, some reviewers are still concerned about the empirical results, which do not match well with published results (even though the authors provided an explanation for it). In addition, the proposed method is only tested on the health care datasets, but the improvement is limited. Therefore it would be worthwhile investigating other time series datasets, and most important answering the important question in terms of what datasets/applications the proposed method works well.   The paper is one step away for being a strong publication. We hope the reviews can help improve the paper for a strong publication in the future. 
This paper proposes a type of adaptive dropout to regularize gradient based meta learning models. The reviewers found the idea interesting and it is supported by improvements on standard benchmarks. The authors addressed several concerns of the reviewers during the rebutal phase. In particular, revisions added results against other regularization mthods. We recommend that further attention is given to ablations, in particular the baseline proposed by Reviewer 1.
This paper addresses an important topic and was generally well written. However, reviewers pointed out serious issues with the evaluation (using weak or poorly chosen attacks), and some conceptual confusions (e.g. conflating adversarial examples with out of distribution examples, unsubstantiated claim that adversarial examples lie off the data manifold).
The presented paper introduces a method to represent neural networks as logical rules of varying complexity, and demonstrate a tradeoff between complexity and error. Reviews yield unanimous reject, with insufficient responses by authors.  Pros: + Paper well written  Cons:   R1 states inadequacy of baselines, which authors do not address.   R3&4 raise issues about the novelty of the idea.   R2&4 raise issues on limited scope of evaluation, and asked for additional experiments on at least 2 datasets which authors did not provide.  Area chair notes the similarity of this work to other works on network compression, i.e. compression of bits to represent weights and activations. By converting neurons to logical clauses, this is essentially a similar method. Authors should familiarize themselves with this field and use it as a baseline comparison. i.e.: https://arxiv.org/pdf/1609.07061.pdf 
The paper presents adversarial "attacks" to maze generation for RL agents trained to perform 2D navigation tasks in 3D environments (DM Lab).  The paper is well written, and the rebuttal(s) and additional experiments (section 4) make the paper better. The approach itself is very interesting. However, there are a few limitations, and thus I am very borderline on this submission:     the analysis of why and how the navigation trained models fail, is rather succinct. Analyzing what happens on the model side (not just the features of the adversarial mazes vs. training mazes) would make the paper stronger.    (more importantly) Section 4: "adapting the training distribution" by incorporating adversarial mazes into training feels incomplete. That is a pithy as giving an adversarial attack for RL trained navigation agents would be much more complete of a contribution if at least the most obvious way to defend the attack was studied in depth. The authors themselves are honest about it and write "Therefore, it is possible that many more training iterations are necessary for agents to learn to perform well in each adversarial setting." (under 4.4 / Expensive Training).  I would invite the authors to submit this version to the workshop track, and/or to finish the work started in Section 4 and make it a strong paper.
The paper shows that the transfer attack is query efficient and the success rate can be kept high with the zeroth order score based attack as a backup.  Experiments show state of the art results.  Pros:    Simple method based on a simple idea.   State of the art performance.  Cons:    Proposal is a straightforward combination of two methods, and therefore technical contribution is marginal.   The threat model is easy (surrogate can be trained on the same datasets and use the same loss function) and questionable.  Most of the experimental evidence shows that the research for this threat model is almost saturated (and the problem seems almost solved).  This paper got a borderline score with reviewer s concerns above.  I agree with the authors that the simplest method is best among those performing similarly, but the threat setting considered might be not very realistic as the authors admitted.  I see the proposed method a kind of egg of Columbus in a negative sense.  Namely, the authors found a shortcut to win a game that was created and adopted by the community.  Perhaps this paper would give an impact on the small community and would make the community change the game.  But to give an impact to a general audience, the authors should convince that there are some situations where the analyzed thread model is realistic and therefore the proposed method is really useful.  Or, the authors could adjust the thread model to be more realistic.  Serious discussion on the thread model would be a big plus to the marginal technical contributions.  After discussion with SAC, and PC, our conclusion is that this paper effectively tells the community that the benchmark they are using is too simple, which alone is worthwhile publishing because this may move the community forward (even if the community is small).
This paper proposes replacing the softmax of deep NNs with a kernel based Gaussian mixture model, to allow for per class multi modality.  Results show that the method is competitive with other output modifications such as the large margin softmax.    The  two primary concerns of the reviewers were the lack of large scale image classification results and theoretical guarantees.  The authors have added CIFAR 100 results.  Moreover, the authors agree that theoretical results would be nice to have, but such results are non trivial and likely require a PAC Bayes treatment.  I find the method to be well motivated and that the paper demonstrates sufficient experimental rigor.  Given the popularity of the softmax throughout deep learning, this paper will likely be of interest or at least, be of potential use to a large part of the ICLR community.  I encourage the authors to add the ImageNet results to the final version. 
The reviewers are enthusiastic about this work, and the few comments that they had were appropriately addressed by the reviewers.
This paper proposes using a lightweight alternative to Transformer self attention called Group Transformer. This is proposed in order to overcome difficulties in modelling long distance dependencies in character level language modelling. They take inspiration from  work on group convolutions. They experiment on two large scale char level LM datasets which show positive results, but experiments on word level tasks fail to show benefits. I think that this work, though promising, is still somewhat incremental and has not shown to be widely applicable, and therefore I recommend that it is not accepted. 
The paper contains an interesting way to do online multi task learning, by borrowing ideas from active learning and comparing and contrasting a number of ways on the arcade learning environment.  Like the reviewers, I have some concerns about using the target scores and I think more analysis would be needed to see just how robust this method is to the choice/distribution of target scores (the authors mention that things don t break down as long as the scores are "reasonable", but that s not a particularly empirical nor precise statement).  My inclination is to accept the paper, because of the earnest efforts made by the authors in understanding how DUA4C works. However, I do agree that the paper should have a larger focus on that: basically Section 6 should be expanded, and the experiments should be rerun in such a way that the setup for DUA4C is more "favorable" (in terms of hyper parameter optimization).  If there s any gap between any of the proposed methods and DUA4C, then this would warrant further analysis of course (since it would mean that there s an advantage to using target scores).  
The paper addresses a well motivated problem of evaluating the accuracy of a black box classifier A(x) using actively selected set of labeled examples.  They predict two additional classifiers   one to predict if an example will be corrected by A(x) and the second a Bayesian NN to assign a distribution over likely labels of unlabeled examples.  The final accuracy estimate is obtained   from the labeled data and the probabilistic labels on the unlabeled data. Reviewers rightly pointed out that the paper only compares with conventional active learning methods, and skips comparison with methods specifically designed for active evaluation.  A list is attached below.  Also, the overall technical contribution seems limited in terms of both empirical accuracy gains it obtains and novelty of ideas exposed.  Related papers: A lazy man s approach to benchmarking: Semisupervised classifier evaluation and recalibration P Welinder, M Welling, P Perona   Proceedings of the IEEE …, 2013   cv foundation.org  Active evaluation of classifiers on large datasets N Katariya, A Iyer, S Sarawagi   2012  Adaptive Stratified Sampling for Precision Recall Estimation. A Sabharwal, Y Xue   UAI, 2018  Online Active Model Selection for Pre trained Classifiers Mohammad Reza Karimi, Nezihe Merve Gürel, Bojan Karlaš, Johannes Rausch, Ce Zhang, Andreas Krause  Towards Efficient Evaluation of Risk via Herding Z Xu, T Yu, S Sra
the aim of this work is to produce an open vocabulary detector.  The approach is via knowledge distillation from existing large scale V+L models, and the evaluation is based on novel classes with LVIS.  The reviewers were generally happy with the work (approach and results), but there were substantial points of clarification during discussion that need to be properly integrated into the final manuscript.
The paper presents a bidirectional pooling layer inspired by the classical Lifting scheme from signal processing. LiftDownPool is able to preserve structure and details in different sub bands, whereas LiftUpPool is able to generate a refined up sampled feature map using the detail sub bands. This is very useful for image to image translation tasks and all tasks that involve up scaling. This is a solid contribution with extensive and thorough experiments and direct practical usage, clear accept. 
The authors propose two new variants of (projected) gradient descent for attacking a classifier and a detector simultaneously. Using these two new variants they are able to break four recent detection methods for adversarial samples.  Strength:   All the reviewer acknowledge that breaking these four defenses is a valuable contribution.  Weakness:   From a technical perspective the paper is rather simple and no theoretical support for the suggested variants is provided. The justification is rather handwavy. From an optimization perspective it is unclear why not a simple penalty based approach would have given the same results or would even work better. The choices maded in this paper seem a bit arbitrary and are mainly justified by the fact that they work for the four detectors   the honeypot defense was already broken in  A Partial Break of the Honeypots Defense to Catch Adversarial Attacks, Nicholas Carlini, arXiv:2009.10975  Minor:   the authors should update the references, several papers have appeared in the meantime  While I appreciate the contribution of the broken defenses, in terms of technical contribution and discussion of the methods this paper is borderline.
Description of paper content:  The paper addresses the problem of credit assignment for delayed reward problems. Their method, Randomized Return Decomposition, learns a reward function that provides immediate reward. The algorithm works by randomly subsampling trajectories and predicting the empirical return by regression using a sum of rewards on the included states. The method is compared to a variety of existing methods on Mujoco problems in “episodic reward” settings, where the reward is zero except for the final step of the episode, where it is the sum of rewards from the original task. Theoretical argument suggests the method is an interpolation of return decomposition (regress based on all states, not a subsample) and uniform reward distribution (send episodic reward to all states equally). By regressing with a subset of states, the method reduces compute for longer problems and is suggested to be more scalable.  Summary of paper discussion:  The reviewers largely commended the simplicity of the method, the simplicity of the presentation, the novelty of the algorithm, and the quality of the empirical results. The negative reviewer maintained their initial review’s score on account of a bias introduced by the algorithm.
During the discussion phase, although the reviewers acknowledge superior empirical performance of the proposed method, they shared the two major concerns: 1. Lack of theoretical or empirical justification/proof for the key statement: "the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima". 1. Lack of comparisons with newer methods from e.g. ECCV2020 etc.  In particular, the first point is crucial. As the reviewers pointed out, since it is the main contribution and the key message of this paper, it should be carefully examined theoretically and/or empirically. However, in its current state, there is no theoretical analysis, and empirical evaluation is not convincing.   About the second point, although I think it cannot be a solo reason for rejection, at least it is better to cite and discuss it fo the completeness.  Overall, the contribution of this paper it not significant enough for publication. Hence I will reject the paper.
Although the paper studies a relevant and important topic, which is about learning of hierarchy of concepts in an unsupervised manner, the reviewers raised several critical concerns. In particular, although the hierarchical structure of concepts is the key idea in this paper, the concept of hierarchy itself is not well explained. How to define the hierarchical level of concepts should be carefully and mathematically discussed. In addition, empirical evaluation is not thorough as reviewers pointed out. Although we acknowledge that the authors addressed concerns by the author response, newly added results are still confusing and more careful treatment is needed before publication. I will therefore reject the paper.   This work reminds me the the topic called "formal concept analysis" (e.g. see [1]), which mathematically defines concepts as closed sets and constructs a hierarchy of concepts in an unsupervised manner. This method can be viewed as co clustering and also has a close relationship to closed itemset mining. This approach is used in machine learning (e.g. [2]). I think it is beneficial for the authors to refer such existing and well established approaches to elaborate this work further.  [1] Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order, Cambridge Univ. Press (2002)   [2] Yoneda, et al., Learning Graph Representation via Formal Concept Analysis, 	arXiv:1812.03395 
This paper focuses on two new characteristics of adversarial examples from the channel wise activation perspective, namely the activation magnitudes and the activated channels. The philosophy behind sounds quite interesting to me, namely, suppressing redundant activations from being activated by adversarial perturbations. This philosophy leads to a novel algorithm design I have never seen, i.e., Channel wise Activation Suppressing (CAS) training strategy.  The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all comments in the final version.
The authors present several theorems bounding the generalization error of a class of conv nets (CNNs) with high probability by             O(sqrt(W(beta + log(lambda)) + log(1/delta)]/sqrt(n)),   where W is the number of weights, beta is the distance from initialization in operator norm, lambda is the margin, n is the number of data, and the bound holds with prob. at least 1 delta. (They also present a bound that is tighter when the empirical risk is small.)  The bounds are "size free" in the sense that they do not depend on the size of the *input*, which is assumed to be, say, a d x d image. While there is dependence on the number of parameters, W, there is no implicit dependence on d here.  The paper received the following feedback:  1. Reviewer 3 mostly had clarifying questions, especially with respect to (essentially independent) work by Wei and Ma. Reviewer 3 also pressed the authors to discuss how the bounds compared in absolute terms to the bounds of Bartlett et al. The authors stated that they did not have explicit constants to make such a comparison. Reviewer 3 was satisfied enough to raise their score to a 6.  2. Reviewer 1 admitted they were not experts and raised some issues around novelty/simplicity. I do not think the simplicity of the paper is a drawback. The reviewers unfortunately did not participate in the rebuttal, despite repeated attempts.  3. Reviewer 2 argued for weak reject, despite an interaction with the authors. The reviewer raised the issue of bounds based on control of the Lipschitz constant. The conversation was slightly marred by a typo in the reviewers original comment. I don t believe the authors ultimately responded to the reviewer s point. There was another discussion about simultaneously work and compression based bounds. I would agree with the authors that they need not have cited simultaneous work, especially since the details are quite different. Ultimately, this reviewer still argued for rejection (weakly).  After the rebuttal period ended, the reviewers raised some further concerns with me. I tried to assess these on my own, and ended up with my own questions.  I raise these in no particular order. Each of them may have a simple resolution. In that case, the authors should take them as possible sources of confusion. Addressing them may significantly improve the readability of the paper.  i. Lemma A.3. The order of quantification is poorly expressed and so I was not confident in the statement. In particular, the theorem starts \forall \eta >0 \exists C, .... but then C is REINTRODUCED later, subsequent to existential quantification over M, B, and d and so it seems there is dependence. If there is no dependence, this presentation is sloppy and should be fixed.  ii. Lemma A.4, the same dependence of C on M, B and d holds here and this is quite problematic for the later applications. If this constant is independent of these quantities, then the order of quantifiers has been stated incorrectly. Again, this is sloppy if it is wrong. If it s correct, then we need to know how C grows.  Based on other claims by the authors, it is my understanding that, in both cases, the constant C does not depend on M, B, or d. Regardless, the authors should clarify the dependence. If C does in fact depend on these quantities, and the conclusions change, the paper should be retracted.  iii. Proof of Lemma 2.3. I d remind the reader that the parametrization maps the unit ball to G.   iv. The bound depends on control of operator norms and empirical margins. It is not clear how these interact and whether, for margin parameters necessary to achieve small empirical margin risk, the bounds pick up dependence on other aspects of the learning problem (e.g., depth). I think the only way to assess this would be to investigate these quantities empirically, say, by varying the size and depth of the network on a fixed data set, trained to achieve the same empirical risk (or margin).  I ll add that I was also disappointed that the authors did not attempt to address any of the issues by a revision of the actual paper. In particular, the authors promise several changes that would have been straightforward to make in the two weeks of rebuttal. Instead, the reviewers and myself are left to imagine how things would change. I see at least two promises:  A. To walk back some of the empirical claims about distance from initialization that are based on somewhat flimsy empirical evaluations. I would add to this the need to investigate how the margin and operator norms depend on depth empirically.  B. Attribute Dziugate and Roy for establishing the first bounds in terms of distance from initialization, though their bounds were numerical. I think a mention of simultaneously work would also be generous, even if not strictly necessary. 
In this manuscript, the authors study the relatively unexplored problem of how to characterize and assess the adversarial vulnerability of classification models with categorical input. Even certifying the robustness of such classification models is intrinsically an NP hard combinatorial problem, the authors show that the robustness certification can be solved via an efficient greedy exploration of the discrete attack space for any measurable classifiers with a mild smoothness constraint.  Overall, the theoretical analyses in this paper are rigorous, and reviewers seem to be satisfied with the responses from the authors. Based on the three positive reviewers, this manuscript is recommended to be accepted.
This paper explores strategies for scaling vision transformers that can be transferable across hardware devices and ViT variants. While it presents some interesting observations as well as a useful practical guide, multiple reviewers expressed major concerns over the novelty and significance of the methods and findings. Besides novelty and significance, there are also some concerns about comparison with existing work as well as clarity of the presentation.
The authors use a Tucker decomposition to represent the weights of a network, for efficient computation. The idea is natural, and preliminary results promising. The main concern was lack of empirical validation and comparisons. While the authors have provided partial additional results in the rebuttal, which is appreciated, a thorough set of experiments and comparisons would ideally be included in a new version of the paper, and then considered again in review. 
The paper proposes two simple generator architecture variants enabling the use of GAN training for the tasks of denoising (from known noise types) and demixing (of two added sources). While the denoising approach is very similar to AmbientGAN and could thus be considered somewhat incremental, all reviewers and the AC agree that the developed use of GANs for demixing is an interesting novel direction. The paper is well written, and the approach is supported by encouraging experimental results on MNIST and Fashion MNIST. Reviewers and AC noted the following weaknesses of the paper: a) no theoretical support or analysis is provided for the approach, this makes it primarily an empirical study of a nice idea.  b) For an empirical study, the experimental evaluation is very limited, both in terms of dataset/problems it is tested on; and in terms of algorithms for demixing/source separation that it is compared against.  Following these reviews, the authors added the experiments on Fashion MNIST and comparison with ICA which are steps in the right direction. This improvement moved one reviewer to positively update his score, but not the others. Taking everything into account, the AC judges that it is a very promising direction, but that more extensive experiments on additional benchmark tasks for demixing and comparison with other demixing algorithms are needed to make this work a more complete contribution. 
Adversarial training has quickly become important for training robust neural networks.  However this training generally results in poor generalization behavior. This paper proposes using margin loss with adversarial training for better generalization. The paper provides generalization bounds for this adversarial training setup motivating the use of spectral regularization. The experimental results using the spectral regularization with adversarial training are very promising and all the reviewers agree that they show non trivial improvement. Even though the spectral regularization techniques have been tried in different settings, hence of limited novelty, the experimental results in the paper are encouraging and I believe will motivate further study on this topic. Reviewers also opined that the writing in the paper is currently not that great with limited explanation of the theoretical results. More discussions interpreting the theoretical results and their significance can help the readers appreciate the paper better.
Thanks for the discussion, all. This paper proposes an attack strategy against federated learning. Reviewers put this in the top tier, and the authors responded appropriately to their criticisms. 
This paper introduces a probabilistic data subsampling scheme that can be optimized end to end.  The experimental evaluation is a bit weak, focusing mostly on toy scale problems, and I would have liked to see a discussion of bias in the Gumbel max gradient estimator.    It s also not clear how the free hyperparameters for this method were chosen, which makes me suspect they were tuned on the test set.  However, the overall idea is sensible, and the area seems under explored.
This paper introduces a new algorithm to solve game, more or less similar (in the general idea, yet differences are interesting) than CFR. The concept is to sample from past policies to generate trajectories and update sequentially (via regret matching).  The three reviewers gave rather lukewarm reviews, with possible suggestions of improvements (that were more or less declined by the authors for those proposed by Rev3 and Rev4; the added material focuses more on the clarity of the text than on the content itself).  I have also read the paper, and find it quite difficult to assess. At the end, it is not clear to a reader whether ARMAC is the new state of the art, or just a "variant" of CFR that will be soon forgotten. The performances do not seem astonishing (at least against NSFP) and even though DREAM might not be satisfactory to the authors (EDIT POST DISCUSSION: actually, DREAM is a valid competitor and must be included in the comparative study), it would have been nice to provide some comparison. Maybe the issue is the writing of the paper that could and should be improved so that it is clearer what are the different building blocks of ARMAC (and their respective importance).  If ARMAC is the new state of the art, then I am sure the authors will be able to clearly illustrate it in a forthcoming revision (maybe with more experiments, as suggested by Rev2). Unfortunately, for the moment, I do not think this paper is mature enough for ICLR.
This work builds on MAML by (1) switching from a single underlying set of parameters to a distribution in a latent lower dimensional space, and (2) conditioning the initial parameter of each subproblem on the input data. All reviewers agree that the solid experimental results are impressive, with careful ablation studies to show how conditional parameter generation and optimization in the lower dimensional space both contribute to the performance. While there were some initial concerns on clarity and experimental details, we feel the revised version has addressed those in a satisfying way.
The paper looks into performance of a single network vs ensemble CNN networks of similar no. of parameters, through lens of accuracy, training time, memory used, inference time.  the authors show that after some threshold, the ensemble model starts to outperform a single model and make better use of its capacity. although this is not the first paper to look into this question and there are two other earlier results from this year, the current paper looks into more measures and not just accuracy. Although initially the paper only looked at over parameterized regime, the authors added experiments on under parametrized case as well. moreover, the authors address the issue of only looking into small and medium sized datasets by adding two more ImageNet experiments.  I thank the authors for engaging with the reviewers, addressing their comments and updating the paper accordingly.  It s of interest for follow up work to consider large data regime and transformer style models as well.
This work proposes to use a transformer model and language model inspired self supervised training techniques to generate local modifications of organic molecules. The use of IUPAC names coupled with language inspired pre training is indeed an interesting idea worthy of exploration. The paper has a lot of promises in this regard but needs more work to deliver it through the finish line. In the rebuttal, the authors have provided strong arguments toward the advantages of using IUPAC representation. While these arguments make sense, they are more or less conceptual and better and more clear empirical evidences are required to back them up.
The problem as formalized in this paper is essentially a domain adaptation problem. There is a training distrinution P and a test distribution P*. the learner gets training data generated by P and aims to minimize the loss of its hypothesis w.r.t. P*. How is it relate dto fairness? The authors add the assumption "we assume the unbiased Bayes decision rule is algorithmically fair in some sense and hope that enforcing the correct notion of fairness allows us to recover h∗ from P". Under such an assumption, almost by definition "enforcing fairness may improve accuracy". By a similar logic, if we assume the unbiased byes decision rule is biased against a certain group, then enforcing bias against that group will imporve accuracy ....
This paper tries to improve the training of adversarial deep neural networks by avoiding fitting the “harmful” atypical samples and fitting more “benign” atypical samples.   Overall, the main concerns are  1. The current presentation can easily cause some misunderstandings on the observations made in Section 3, especially [1] and [3] mentioned by the reviewer iXiX.    The authors may consider moving "related work" to the first half of the submission, and organize existing findings with rare/hard/atypical in a more principle manner.    Besides, as author mentioned in Section 3.1: "it is equivalent to a classification task based on an extremely small dataset, with one or a few training samples given". Such findings are natural and not novel to the deep learning community. Authors may consider shorten Section 3.1 and elaborate more in Section 3.2.  2. Theorem 1 and 2 do not help much.    It does not talk about the training algorithm and models, which over simplifies the learning problem.    Besides, the authors can consider some theoretical results how BAT improve the performance of typical samples, but still preserve the ability to fit those "useful" atypical samples.  This helps to bridge the gap between motivation behind BAT and its algorithm design (raise by reviewer  ytJj and sm19).  3. It is also suggested to make observations more convincing.    Since authors want to claim their findings are universal, it is better to consider more adv training methods and datasets; it is also better to change the ratio of "normal samples" v.s. "atypical samples". In this way, the effect of atypical samples in adversarial training can be more carefully quantized.
This paper studies a variational formulation of the loss minimization to study the solution that generalizes the most. An expectation of the loss wrt a Gaussian distribution is minimized to find the mean and variance of the Gaussian distribution. As the variance goes to zero, we recover the original loss, but for a higher value of variance, the loss may be convex. This is used to study the generalizability of the landscape.  Both objective and solutions of the paper are unclear and not communicated well. There is not enough citation to previous work (e.g., Gaussian homotopy exactly considers this problem, and there are papers that study the convexity of the expectation of the loss function). There are no experimental results either to confirm the theoretical finding.  All the reviewers struggle to understand both the problem and solutions discussed in this paper. I believe that the paper could become useful if reviewers  feedback is taken seriously to improve the paper.
All reviewers agreed to reject.
The authors present a new approach to improve performance for retro synthesis using a seq2seq model, achieving significant improvement over the baseline. There are a number of lingering questions regarding the significance and impact of this work. Hence, my recommendation is to reject. 
In recent years, artificially trained RNNs have been used for studying systems and behavioral neuroscience in terms of their learned representations, dynamics, computation, and the learning process itself. This paper contributes to further identify learning principles that may be revealed by curricula. The proposed approach for finding signatures of different loss functions is a novel and interesting idea which is very well fit for the neuro oriented ICLR audience and has potential impact in other fields.
This work presents a new sentiment representation method with the use of affect control theory and BERT. Reviewers pointed out several major concerns towards the insufficient experiments and results, as well as the lack of ablation studies and related work discussion. I would like to encourage the authors to take into account the comments from reviewers to further improve their work for a stronger version for future submissions.
This paper presents a new view of latent variable learning as learning lattice representations.  Overall, the reviewers thought the underlying ideas were interesting, but both the description and the experimentation in the paper were not quite sufficient at this time. I d encourage the authors to continue on this path and take into account the extensive review feedback in improving the paper!
This paper proposed a novel Adversarial Deep Metric Learning approaches. The reviews pointed out the paper proposes an interesting idea and it is among the rare works that address directly robust metric learning which an important topic for efficient metric learning.  Some concerns were raised about the analysis and the lack of comparisons notably with other types of adversarial attacks.  The authors provide a rebuttal where they addressed some concerns raised by reviewers with some precisions on the work, its positioning with respect to other related papers and additional comparisons notably with other types of attacks.  A minor remark: there is a typo in Eq(13), where the $z$ in the loss function is actually not defined and should be included in the max function. That being said, the contribution is still limited in considering only the infinite norm, analysis and comparisons to prior work remain weak. The paper does not meet the requirements for acceptance to ICLR in its current form. I have then to propose rejection. 
This paper proposed graph neural networks based approach for subgraph detection. The reviewers find that the overall the paper is interesting, however further improvements are needed to meet ICLR standard:  1. Experiments on larger graph. Slight speedup in small graphs are less exciting.   2. It seems there s a mismatch between training and inference.  3. The stopping criterion is quite heuristic.  
This work proposes an observational (not counterfactual) link prediction method based on clustering and data augmentation. The total score went down during the rebuttal phase. One of the challenges is that the title confused reviewers. The authors proposed other titles. But in the end all proposed titles had variations of "counterfactual graph", which is what makes the title confusing.   I think if the work had been more empirically focused and stayed away from a causal model, it would have been accepted. The idea is ingenious and it is likely that it indeed gives the empirical improvement in observational models that the authors claim. The authors have very scant citations to relevant works in the intersection of causality and representation learning anyways. But if the causal model must stay for future submissions, the authors should: (a) more clearly define and differentiate the causal and non causal aspects of the task (and properly contextualize the work), and (b) more clearly specifies the causal model that is being utilized (and be more mathematically rigorous in general).  Unfortunately, the original description of the causal model in the paper was both sloppy and incorrect. I had to ask a total number of five (5) long clarifying questions (visible to authors and reviewers only) in order to understand the causal assumptions. In my back and forth with the authors, the authors proposed a total of 7 different causal DAGs to explain their method. Some of these DAGs contradicted each other and most of the answers were vague. My first advice to the authors is to always be mathematically precise for the benefit of reviewers and readers. Causality requires very clear assumptions and precise notation. For instance, the variables in the causal DAG given in Figures 2 and 3 had little to do with the variables in the actual graph model. The causal DAG must have the variables $A_{ij}$, $T_{ij}$, $C_i$, $C_j$, $z_i$, $z_j$, for all $i,j \in V$. The final causal DAG proposed by the authors in the discussion was:   $z_i \to A_{ij}$   $z_j \to A_{ij}$   $z_i \to T_{ij}$ (may not be needed)   $z_j \to T_{ij}$ (may not be needed)   $C_i \to T_{ij}$   $C_j \to T_{ij}$   $T_{ij} \to A_{ij}$  Regarding the nature of $z_i$ and $z_j$: They need to be formally described as structural node embeddings of the graph, otherwise the DAG is incorrect. After digging and reading some of the references the authors use, I came to the conclusion that the empirical method indeed relies on structural embeddings (and the authors later confirmed it). This should be made **very** clear in the SCM description.  This final causal model after discussion looks reasonable. And one challenge that remains, however, is sampling or MAP point estimates of the posterior $P(C_i,C_j|A)$. All standard clustering methods that can use this DAG rely on node embeddings $z_i, z_j$ being positional embeddings, not structural ones. So, how are we actually obtaining $P(C_i,C_j|A)$? The authors would need to prove that standard clustering methods can perform this task (I have my doubts this is even true). I fear this issue could again derail the paper in a future submission if the counterfactual justification is used again. One of the reviewers raised concerns about the quality of the embedding already.  The proposed method is observational and does not need a causal description. But if a causal description is provided, it must be correct and the method must be theoretically sound. The causality part of the paper needs a substantial revision, unfortunately. I suggest rejection.
The paper proposes a new goal conditioned hierarchical RL method aimed at improving performance on sparse reward tasks. Compared to prior work the novelty lies in a new way of improving the stability of goal representation learning and in an improved exploration strategy for proposing goals while taking reachability into account.  The paper does a good job of motivating the main ideas around stability and combining novelty with reachability. Reviewers found the quantitative evaluation and the choice of baselines to be good with the exception of not including Feudal Networks which the authors explained was due to poor performance on the hard exploration tasks (something that has been observed in prior work). Reviewers also found the thoroughness of the ablations and insightful visualizations to be highlights. Overall, reviewers were unanimous in recommending acceptance, which I support.
The paper studies the problem of finding an optimal memory less policy for POMDPs.  This work makes an important theoretical contribution.  The reviewers are unanimous in recommending the acceptance of the paper.  Well done!
Overall, reviewers are positive. The majority praised the approach as novel and viewed the results as quite strong. Further, reviewers valued the provided ablations and analysis that helped motivate the proposed method. A few concerns were raised about overall clarity (though some reviewers praised the clarity of presentation), the use of hand crafted filters, and certain experimental comparisons. The majority of these concerns have been adequately addressed in author response.
This paper received divergent ratings (7, 3, 3). While there is value in thorough evaluation papers, this manuscript has significant presentation issues. As all three reviewers point out, the way it is currently written, it misrepresents the claims made by Mirowski et al 2016 and over reaches in its findings. Unfortunately, we cannot make a decision on what the manuscript may look like in future once these issues are fixed, and must reject. 
The work proposes a method to learn graph representations based on subgraphs that are invariant to spurious subgraphs. The reviewers found the paper easy to read and the theory interesting, well explained and justified. The reviewers seem happy with the existing and new experiments that came during the rebuttal phase. I too found the paper interesting and mostly well written.   Besides the corrections done during the rebuttal, in further discussion with the authors, I raised a concern that the work must make additional assumptions about the support of the induced subgraph distributions that were not clearly stated in the paper: The work makes the assumption that there is enough training data such that all spurious induced subgraph patterns $S$ that are smaller than the truly correlated induced subgraph $C$ can be identified as spurious. The authors promised to make this into a clearly demarcated assumption since it a key requirement for the method to work.
This paper proposes a new architecture for point cloud processing, with good empirical results. All reviewers recommended accept. AC does not see a reason to overturn the consensus.
**Overview**: This paper provides a new clustering based method to predict future probability density of a policy. It provides comparable performance to prior Q learning based methods, but without careful hyper parameter tuning.  **Pro**: The method of using clustering to estimate future density is novel. Both theory and experiments appear solid. In the rebuttal phase, the authors convinced all the reviewers by addressing their concerns. The reviewers unanimous tend to acceptance.  **Con**: The reviewers had many concerns before the rebuttal. But these were addressed by the authors.   **Recommendation**: The C learning method proposed in this paper is novel and can be potentially useful in practice. Both theory and experiments are solid and convincing. Hence the recommendation is accept.     
The authors present an approach to large scale bitext extraction from Wikipedia. This builds heavily on previous work, with the novelty being somewhat minor efficient approximate K nearest neighbor search and language agnostic parameters such as cutoffs. These techniques have not been validated on other data sets and it is unclear how well they generalise. The major contribution of the paper is the corpus created, consisting of 85 languages, 1620 language pairs and 135M parallel sentences, of which most do not include English. This corpus is very valuable and already in use in the field, but IMO ICLR is not the right venue for this kind of publication. There were four reviews, all broadly in agreement, and some discussion with the authors.  
Reviewers agree that the idea of layer wise regularization is interesting and is in line with many efforts in the optimization realm to specialize in the training procedure and the learning rate to each layer.  Given the depth of some state of the art neural networks, efficiency is at stake and the idea brought up in this paper naturally falls into that.  While the theoretical result in Theorem1 is sound and clear, an extended result on the impact of such « merge » and « layer skipping » on the overall predictions of the algorithm can be well appreciated. The overall goal of network compression should remain to reduce drastically the network size, and thus the training time (energy consumption etc...), while keeping a relatively good prediction accuracy (at least of the same order). Being able to back this with theory (and of course experiments) is crucial.   Reviewers also pointed out that the empirical evaluations were not sufficient for ICLR. For example, there are no enough comparisons with existing algorithms and there should be more experimental results based on real datasets. Although the rebuttals did help clarify some of the issues raised by the reviewers, overall this paper does not seem to meet the bar to be accepted.   
The paper presents an off policy actor critic scheme where i) a buffer storing the trajectories from several agents is used (off policy replay) and mixed with the on line data from the current agent; ii) a trust region estimator is used to select trajectories that are sufficiently close to the current policy (e.g. in the sense of a KL divergence).  As noted by the reviews, the results are impressive.   Quite a few concerns still remain: * After Fig. 1 (revised version), what matters is the shared replay, where the agent actually benefits from the experience of 9 other different agents; this implies that the population based training observes 9x more frames than the no shared version, and the question whether the comparison is fair is raised; * the trust region estimator might reduce the data seen by the agent, leading it to overfit the past (Fig. 3, left); * the influence of the $b$ hyper parameter (the trust threshold) is not discussed. In standard trust region based optimization methods, the trust region is gradually narrowed, suggesting that parameter $b$ here should evolve along time.   
The paper proposes a simple approach to quantizing neural network weights with encouraging empirical results. The authors did work hard to improve the paper and address reviewers  concerns during the discussion period. I believe the presentation of results can improve by adding a discussion of inference time. I am not sure if all of the baselines (e.g., in Figure 4) have the same inference cost.  PS1: The method does seem to unroll the iterative optimization process (ie. EM) of a Gaussian mixture model (GMM) and differentiates through the unrolled iterations. The paper makes the connection to attention, but does not seem to make a clear connection with GMM and EM. If this connection is correct, adding a discussion can be helpful.  PS2: I am not a big fan of using differentiable k means as the method name. Differentiable k means is confusing partly because k means is differentiable, i.e., one can optimize k means centers using gradient descent. The proposed approach seems more relevant to meta learning, where one differentiate though one optimization process to optimize a secondary objective.
This paper is concerned with the ongoing research program of mapping the approximation power of different GNN architectures. It provides significant advances in the study of equivariant GNNs and nice extensions in the invariant case by closing existing gaps between distinct GNN families.  All reviewers agreed that this is a strong submission with substantial new theoretical results. The AC recommends a strong acceptance. 
This paper proposes a method to automatically generate corpora for training program synthesis systems.  The reviewers did seem to appreciate the core idea of the paper, but pointed out a number of problems with experimental design that preclude the publication of the paper at this time. The reviewers gave a number of good comments, so I hope that the authors can improve the paper for publication at a different venue in the future.
Although the reviewers acknowledge some contributions of the paper, it has some limitations on both theoretical results and numerical experiments. It is still unclear about the effectiveness of the proposed method. The authors should consider the following issues for the future submission:   1) The justification of $\tau$ is not clear in federated learning with respect to communication efficiency (please see Reviewer 1’s comments).  2) The bounded stochastic gradient assumption is not applicable in the strongly convex case. This issue has been discussed clearly in [Nguyen et. al, “SGD and Hogwild! Convergence Without the Bounded Gradients Assumption”, ICML 2018]. Therefore, the constant G in Section 3.2. would damage the complexity bound since it could be arbitrarily large.   3) Although the goal is to illustrate the benefits of the proposed quantization approach, the numerical experiments and the theoretical contributions are limited. The theoretical results are incremental from the existing optimization theory (both strongly convex and non convex). Moreover, network architecture and data sets are not enough to justify the efficiency of the method.  
The paper presents a method for learning from expert state trajectories using a similarity metric in a learned feature space. The approach uses only the states, not the actions of the expert. The reviewers were variously dissatisfied with the novelty, the theoretical presentation, and the robustness of the approach. Though it empirically works better than the baselines (without expert demos) this is not surprising, especially since thousands of expert demonstrations were used. This would have been more impressive with fewer demonstrations, or more novelty in the method, or more evidence of robustness when the agent s state is far from the demonstrations.
This paper focuses on finding universal adversarial perturbations, that is, a single noise pattern that can be applied to any input to fool the network in many cases. Further more, it focuses on the data free setting, where such a perturbation is found without having access to data (images) from the distribution that train  and test data comes from.   The reviewers were very conflicted about this paper. Among others, the strong experimental results and the clarity of writing and analysis were praised. However, there was also criticism of the amount of novelty compared to GDUAP, on the strong assumptions needed (potentially limiting the applicability), and on some weakness in the theoretical analysis.   In the end, the paper seems in current form not convincing enough for me to recommend acceptance for ICLR.  
The paper makes a solid contribution to understanding the convergence properties of policy gradient methods with over parameterized neural network function approximators.  This work is concurrent with and not subsumed by other strong work by Agarwal et al. on the same topic.  There is sufficient novelty in this contribution to merit acceptance.  The authors should nevertheless clarify the relationship between their work and the related work noted by AnonReviewer2, in addition to addressing the other comments of the reviewers.
This paper presents a method for ensembling light fine tuning methods and full fine tuning methods to achieve better performance both in domain and out of domain distributions. As authors agree, similar idea has been explored in the computer vision literature. The reviewers like the overall idea of the paper, but they all had some concerns regarding the experiments. The reviewers provide valuable feedback on how to improve the experiments, potentially running the same idea on more datasets and tasks, provide more analyses and discussions on how to understand the results.
While the reviews of this paper were somewhat mixed (7,6,4), I ended up favoring acceptance because of the thorough author responses, and the novelty of what is being examined.  The reviewer with a score of 4, argues that this work is not a good fit for iclr, but, although tailoring new metrics may not be a common area that is explored, I don t believe that it s outside the range of iclr s interest, and therefore also more unique.
Reviewers have commented on the lack of novelty of the paper as it reads only as applying the variational inference framework of Blundell et al. (2015) to deep metric learning (R2 and R4). Furthermore, the paper has not properly positioned itself when compared to previous works on "Deep variational metric learning" and "Deep adversarial metric learning" (R1) and other previous literature that have studied robustness for metric learning. The argument on robustness to noisy labels needs to be expanded and better fleshed out in a future version of the paper. 
The paper proposes a new method to learn OT maps, and reframes it in the GAN literature. The initial method works when computing maps between equal dimensions, through duality and an identity (10   11, amply discussed in the reviewing process). Lemma 4.1 provides the main result. While the discussion right below on the fact that several functions (non OT maps) might maximize that criterion is not completely satisfactory, the result provides an interesting characterization. The second contribution adds a method to compute OT maps between spaces of unequal dimensions. Overall the contribution sounds a bit ad hoc, and one wonders whether this does really work (comments such as "we add small gradient penalty (Gulrajani et al., 2017) on potential ψω for better stability. The penalty in not included in Algorithm 1 to keep it simple." are strange and point to instability) but the overall creativity and new ideas in the paper seem to have garnered enough support from reviewers to push for an accept.
In this paper the authors propose an approach to improving the accuracy of the classification problem based on deep neural networks by detecting the in domain data from background/noise.  The strategy is designed in such a way that the detector and the classifier share the bottom layers of the network.  Theoretical proof is given and experiments are conducted on a variety of datasets.  The novelty of the work is to come up with a better estimate the pdf of the data and use it to help the classification based on the deep neural networks.   There are concerns raised by the reviewers regarding the related work, the exposition and the experimental design.  After the rebuttal from the authors, which is meticulous, some of the issues unfortunately still stand.  The paper needs to make a stronger case in order to be accepted, especially, for instance, the theoretical and empirical comparison with the existing techniques sharing the similar idea. 
This paper presents a comparison and analysis of continual learning methods for pretrained language models. The authors categorise continual learning methods into three categories, those that use cross task regularisation, those that employ some form of experience replay of previous training examples, and those that dynamically alter the network architecture for each task. Evaluation results from representative examples of these three paradigms are then presented and analysed. In general methods that incorporate experience reply appear to perform the best, while analysis of the predictive power of individual layers of the pretrained models suggests that some network layers are more robust to catastrophic forgetting than others, and that this also varies across architectures (BERT, ALBERT, etc.).  In general the reviewers agree that this is a well conducted study that provides an interesting contribution to an important area of research. They also generally agree that the many of the results are unsurprising given the properties of the algorithms explored and prior work in this area. The main point of difference then becomes how valuable it is to present a thorough study of existing algorithms that confirms our assumptions. I believe that the current work raises enough interesting questions to make it a useful contribution to researchers working in continual learning. In particular the results analysing the relative differences in catastrophic forgetting across different layers in models suggests interesting avenues for follow on work.
This paper identifies a limitation with current attention in transformers where they scoring with query key pairs is strongly tied to retrieving the value and proposes a more flexible configuration that subsumes the previous setup but provides more flexibility. The authors shows this leads to improvements in various settings.  Overall, all reviewers seem to agree there is interesting insight and results in this paper and it merits publication. Also the discussion helped stress important points regarding weight sharing and more. One concern is that the model was not evaluated on standard NLP/vision datasets (I assume alluding to GLUE/SuperGlue/SQuAD, etc.), and authors seem to hint that pre training this is an issue for them computationally. This leaves open whether this indeed can and should replace the standard attention mechanism across the board, but is still very worthy of publication.
This paper presents work on efficient video analysis.  The reviewers appreciated the clear formulation and effective methodology.  Concerns were raised over empirical validation.  The authors  responses added additional material that assisted in clarifying these points.  After the discussion the reviewers converged on a unanimous accept rating.  The paper contains solid advances in efficient inference for video analysis and is ready for publication.
The authors proposed a new loss function for end to end edge detection to overcome the label imbalance and edge thickness problems. Overall, the proposed VT appears to be a useful representation for boundary detection. Though similar to DT, VT outperforms DT by a large margin and is more robust to noise. One reviewer recommends acceptance, two others recommend marginal acceptance. The main issues have been adequately addressed in the rebuttal.
This paper proposes an approach for improving MultiTaskLearning by providing a way of incorporating task specific information.  Pros: 1) All reviewers agreed that the paper is clearly written 2) Interesting to see a single model for AST, STS (speech to speech translation) and MT   Cons: 1) The work is not adequately compared with related work (some important references are also missing)   The authors did perform some additional experiments with T5  and pointed out some drawbacks but this needs to be explored a bit more. 2) The answers about scalability are not very convincing and need more empirical results.   Overall, none of the reviewers were very positive about the paper and felt that while this is a good first attempt, more work is needed to make it suitable for acceptance. 
To improve the generative adversarial nets, the paper proposes to add an implicit transformation of the Gaussian latent variables before the top down generator. To further obtain better generations with respect to quality and diversity, this paper introduces targeted latent transforms into a bi level optimization of GAN.  Experiments are conducted to verify the effectiveness of the proposed method. The paper is highly motivated and well written, but the experiment part still needs to be strengthened because the goal of the paper is to improve the GAN training, comprehensive and thorough evaluation of the proposed method is necessary.   After the first round of review, in addition to the clarification issue and missing reference issue, two reviewers point out that the method is only tested in small scale datasets, and suggest authors evaluate the performance of the proposed method in more complex datasets. Two reviewers point out that the experimental validation and comparison to prior approaches are insufficient. During the rebuttal, the authors provide extra experiment results to partially address some issues.  However, most of the major concerns from other reviewers, such as (i) how are the performance of the method in large scale datasets that have complex latent space manifolds, (ii) non convincing performance gain, and unclear problem setup, still remain. After an internal discussion, AC agrees with all reviewers that the current paper is not ready for publication, thus recommending rejecting the paper. AC urges the authors to improve their paper by taking into account all the suggestions provided by the reviewers, and then resubmit it to the next venue.
The paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a DNN. The idea is interesting and quite useful as is showcased in the experiments. The reviewers feel that the paper is also quite well written and easy to follow.
The reviewers were in general lukewarm about the paper, not convinced by why realistic augmentation mean more robust features in SSL, had concerns over the szie of the datasets (up to ~100k), and the success depends on the relevance of color for classification. The AC agrees with the reviewers. While the paper sounds interesting, there are many questions remain unanswered   it s unclear that the rebuttal addressed the concerns shared by the reviewers.  In addition to the comments by the reviewers, the AC also feels that the overall design is adhoc and it s unclear that the proposed augmentation can generalize to larger, more practical problems.
This paper considers black box adversarial attacks based on perturbations of the intermediate layers of a neural network classifier, obtained by training a binary classifier for each target class.  Reviewers were happy with the novelty of the approach as well as the presentation, described the presentation as rigorous and were pleased with the situation of this method relative to the literature. R3 had concerns about evaluation, success rate, and that the procedure was "cumbersome".  Some of their concerns were addressed in rebuttal, but remained steadfast that the method was too cumbersome to be practical.  I agree with R1 & R2 that this approach is novel and interesting and disagree with R3 that it is too impractical. The paper could be stronger with the addition of adversarial training experiments (and I disagree with the authors that "there are currently no whitebox attacks that do well at attacking AT models", this is very much not the case), but I concur with R1 & R2 that this is interesting work that may stimulate further exploration, enough so to warrant acceptance.
### Paper summary  This paper investigates theoretically and empirically the effect of increasing the number of parameters ("overparameterization") in GAN training. By analogy to what happens in supervised learning with neural networks, overparameterization does help to stabilize the training dynamics (and improve performance empirically). This paper provides an explicit threshold for the width of a 1 layer ReLU network generator so that gradient ascent training with a linear discriminator yields a linear rate of convergence to the global saddle point (which corresponds to the empirical mean of the generator matching the mean of the data). The authors also provides a more general theorem that generalizes this result to deeper networks.  ### Evaluation The reviewers had several questions and concerns which were well addressed in the rebuttal and following discussion, in particular in terms of clarifying the meaning of "overparameterization". After discussing the paper, R1, R2 and R4 recommend acceptance while R3 recommends rejection. The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator  distribution (produced from a *fixed* set of latent variables z_i) and the empirical mean of the data. R3 argues that this is not sufficient to "understanding the training of GANs". At least two aspects are missing: how the distribution induced by the generator converges according to other notion of divergence (like KL, Wasserstein, etc.); and what about the true generator distribution (not just its empirical version from a fixed finite set of samples z_i)? While agreeing these are problematic, the other reviewers judged that the manuscript was useful first step in understanding the role of overparameterization in GANs and thus still recommend acceptance. And importantly, this paper is the first to study this question theoretically.  I also read the paper in more details. I have a feeling that some aspects of this work were already developed in the supervised learning literature; but the gradient descent ascent dynamic aspect appears novel to me and the important question of the role of overparameterization here is both timely, novel and quite interesting. I side with R1, R2 and R4: this paper is an interesting first step, and thus I recommend acceptance. See below for additional comments to be taken in consideration for the camera ready version.  ### Some detailed comments   Beginning of section 2.3: please be clearer early on that you will keep V fixed to a random initialization rather than learning it. The fact that this is standard in some other papers is not a reason to not be clear about it.   Theorem 2.2: in the closed form of the objective when $d$ is explicitly optimized, we are back to a more standard supervised learning formulation, for example (5) could look like regression. The authors should be more clear about this, and also mention in the main text that the core technical part used to prove Theorem 2.2 is from Oymak & Soltanolkotabi 2020 (which considers supervised learning). This should also a bit more clear in the introduction   it seems to me that the main novelty of the work is to look at the gradient descent dynamic, which is a bit different than the supervised learning setup, even though some parts are quite related (like the full maximization with respect to $d$).   p.6 equation (8): typo   the  $ \mu d_t$ term is redundant and should be removed as already included from $\nabla_d h(d,\theta)$.   p.7 "numerical validations" paragraph: Please describe more clearly what is the meaning of "final MSE". Is this a global saddle point (and thus shows the limit of the generator to match the empirical mean), or is this coming from slowness of convergence of the method (e.g. after a fixed number of iterations, or according to some stopping criterion?). Please clarify. 
This paper proposing a framework for augmenting classification systems with explanations was very well received by two reviewers, and on reviewer labeling themselves as "perfectly neutral". I see no reason not to recommend acceptance.
This paper proposes a new way of learning tensors representation with ring structured decompositions rather than through Tensor Train methods. The paper investigates the mathematical properties of this decomposition and provides synthetic experiments. There was some debate, with the reviewers, about the novelty and impact of this method, where overall the feeling was this work was too preliminary to be accepted. The idea, from my understanding, is interesting and would benefit from discussion at the workshop track, but the authors are investigated to make a stronger case for the novelty of this method in any further work and, in particular, to consider showing empirical improvement on "real" data where TT methods are currently applied.
The reviewers found this to be an interesting and clearly written paper, but broadly agreed that it is not yet ready for acceptance. In particular, multiple reviewers felt that the experiments don t show clear benefits of the proposed SVAE approach when compared to the VAEVAE and other baselines; nor do they sufficiently back up the central claim regarding relative benefit of PoE vs MoE for either "AND" or "OR" relations. Hopefully the comments and suggestions from the reviewers, particularly regarding framing and experimental validation, will help in revising the paper.
This paper presents an empirical study of different efficient ways to estimate the performance of architectures in NAS, focussing on weight sharing and performance prediction methods. Most reviewers appreciated the paper s goal of performing a careful, controlled study of different factors that can affect the ranking of architectures. However, all reviewers also had substantial concerns. Many of these could in principle be fixed by additional experiments, but the short time window of the author response period did not allow for this.  As a result, all reviewers voted for rejection, and I will follow that recommendation. Nevertheless, I would like to encourage the authors to continue this work, as I believe that the NAS community needs more careful controlled studies of this type. For their next version, I encourage the authors to take into account the many points mentioned by the reviews.
The paper had three borderline reviews. While the idea of posterior sampling of a neural network is potentially useful and Langevin dynamics are a way to attempt to address that, the reviewers did not appear convinced by the experiments and what the MCMC sampling was doing wasn t really front and center there.
This paper presents a novel family of probabilistic approaches to computing the similarities between two sentences using bag of embeddings representations, and presents evaluations on a standard benchmark to demonstrate the effectiveness of the approach. While there seem to be no substantial disputes about the soundness of the paper in its current form, the reviewers were not convinced by the broad motivation for the approach, and did not find the empirical results compelling enough to serve as a motivation on its own. Given that, no reviewer was willing to argue that this paper makes an important enough contribution to be accepted.  It is unfortunate that one of the assigned reviewers—by their own admission—was not well qualified to review it and that a second reviewer did not submit a review at all, necessitating a late fill in review (thank you, anonymous emergency reviewer!). However, the paper was considered seriously: I can attest that both of the two higher confidence reviewers are well qualified to review work on problems and methods like these.
The idea of using the determinant of the covariance matrix over inputs to select experiments to run is a foundational concept of experimental design.  Thus it is natural to think about extending such a strategy to sequential model based optimization for the hyperparameters of machine learning models, using recent advances in determinantal point processes.  The idea of sampling from k DPPs to do parallel hyperparameter search, balancing quality and diversity of expected outcomes, seems neat.  While the reviewers found the idea interesting, they saw weaknesses in the approach and most importantly were not convinced by the empirical results.  All reviewers thought that the baselines were inappropriate given recent work in hyperparameter optimization (and classic work in statistics).  Pros:   Useful to a large portion of the community (if it works)   An interesting idea that seems timely  Cons:   Only slightly outperforms baselines that are too weak   Not empirically compared to recent literature   Some of the design and methodology require more justification   Experiments are limited to small scale problems
This paper proposes  architectural modifications to transformers, which are promising for sequential tasks requiring memory but can be unstable to optimize, and applies the resulting method to the RL setting, evaluated in the DMLab 30 benchmark.  While I thought the approach was interesting and the results promising, the reviewers unanimously felt that the experimental evaluation could be more thorough, and were concerned with the motivation behind of some of the proposed changes. 
This paper proposes a new black box adversarial attack approach which learns a low dimensional embedding using a pretrained model and then performs efficient search in the embedding space to attack target networks. The proposed approach can produce perturbation with semantic patterns that are easily transferable and improve the query efficiency in black box attacks. All reviewers are in support of the paper after author response. I am very happy to recommend accept. 
This paper develops the notion of the arrow of time in MDPs and explores how this might be useful in RL. All the reviewers found the paper thought provoking, well written, and they believe the work could have significant impact. The paper does not fit the typical mold: it presents some ideas and uses illustrative experiments to suggest the potential utility of the arrow without nailing down a final algorithm or make a precise performance claim. Overall it is a solid paper, and the reviewers all agreed on acceptance.  There are certainly weaknesses in the work, and there is a bit of work to do to get this paper ready. R2 had a nice suggestion of a baseline based on simply learning a transition model (its described in the updated review) please include it. The description of the experimental methodology is a bit of a mess. Most of the experiments in the paper do not clearly indicate how many runs were conducted or how errorbars where computed or what they represent.  It is likely that only a handful of runs were used, which is surprising given the size of some of the domains used. In many cases the figure caption does not even indicate which domain the data came from. All of this is dangerously close to criteria for rejection; please do better.  Readability is also known as empowerment and it would be good to discuss this connection. In general the paper was a bit light on connections outlining how information theory has been used in RL. I suggest you start here (http://www2.hawaii.edu/~sstill/StillPrecup2011.pdf) to improve this aspect. Finally, the paper has a very large appendix (~14 oages) with many many more experiments and theory. I am still not convinced that the balance is quite right. This is probably a journal or long arxiv paper. Maybe this paper should be thought of as a nectar version of a longer standalone arxiv paper.  Finally, relying on effectiveness of random exploration is no small thing and there is a long history in RL of ideas that would work well, given it is easy to gather data that accurately summarizes the dynamics of the world (e.g. proto value, funcs). Many ideas are effective given this assumption. The paper should clearly and honestly discuss this assumption, and provide some arguments why there is hope.
The paper studies federated learning with various sketching techniques used for communication.   The main concerns from the reviewers are:   1) the presentation can be improved;   2) the novelty and related work section is not satisfactory since there have been papers on sketched federated learning;   3) there is no numerical study to validate the efficacy of the method.   I suggest the authors to take into consideration the feedback from the reviewers in the revision of the paper.
It is common in imitation learning to measure and minimize the differences between the agent’s and expert’s visitation distributions. This paper proposes using Wasserstein distance for this, named PWIL, by considering the upper bound of its primal form and taking it as the optimization objective. The effectiveness of the approach is demonstrated by an extensive set of experiments.   Overall, reviewers reached general agreement that this paper makes a good contribution to the conference, and given the overall positive reviews, I also recommend accepting the paper. 
The paper considers an alternative to the standard MDP formulation, motived by the novo drug design problem.  The formulation is meant to optimize a notion of expected maximum reward along the trajectory rather than the expected sum of rewards.  The formulation is presented through a variation of the Bellman equation.  Thus mode of presentation does not make it entirely clear what the fundamental problem is and whether it is the right formulation for the application.  The reviewers point out some problems with the analysis.   Experiments compare the proposed max Q algorithm to Q learning and demonstrate that it achieves higher maximum reward.  Experiments involving novo drug design show promise.  This looks like an interesting idea and direction, but the consensus view is that the project deserves further work and polish.
The submission proposes to use CNN for Amharic Character Recognition.   The authors used a straight forward application of CNNs to go from images of Amharic characters to the corresponding character.  There was no innovation on the CNN side. The main contribution of the work is the Amharic handwriting dataset and the experiments that were performed.  The reviewers indicated the following concerns: 1. There was no innovation to the method (a straight forward CNN is used) and is likely not of interest to the ICLR community 2. The dataset was divided into train/val split and does not contain a held out test set.  Thus it was impossible to determine the generalization of the model. 3. The paper is poorly written with the initial version having major formatting issues and missing references. The revised version has fixed some of the formatting issues.  The paper still need to having more paragraph breaks to help with the readability of the paper (for instance, the introduction is still one big long paragraph).  The terminology and writing can also be improved.  For instance, in section 2.3, the authors write that "500 dataset for each character were collected".  It would be clearer to say that "500 images for each character were collected".  The submission received low reviews overall (3 rejects), which was unchanged after the rebuttal.  Due to the general consensus, there was limited discussion.  There were also major formatting issues with the initial submission.  The revised version was improved to have proper inclusion of Amharic characters in the text, missing figures, and references.  However, even after the revision, the paper still had the above issues with methodology (as noted by R4) and is likely of low interest for the ICLR community.    The Amharic handwriting data and experiments using a CNN can be of interest to the different community and I would recommend the authors work on improving their paper based on reviewer comments and submit to different venue (such as a workshop focused on character recognition for different languages). 
This paper offers a possibly novel approach to regularizing policy learning to make it suitable for large scale divergence in the underlying domain.  Unfortunately all the reviewers are unanimous that the paper is not acceptable in present form.  Insufficient clarity regarding the contribution relative to several references, some of which were missing from the submitted version, is perhaps the most significant issue in the view of the AC.
After carefully reading the reviews and the rebuttal I feel the paper fails slightly short.   Unfortunately some of the issues that I have are aligned with the feedback from reviewer 6YwU and pULY.  A significant part of the paper is the formalism and theory introduced by this work, followed then by the empirical evaluation. The theory I feel is not sufficiently well formulated. I understand this is a complex topic, and one can only make minimal statements about a system (particularly when learning is involved). And I understand that the authors are looking at a slightly different phenomena, and not the traditional vanishing/exploding gradient problem, where they consider a per unit scenario. And I believe one can make a case that this alternative definition holds value and should be investigated.   However, I believe being more explicit of this alternate view, and make sure that one does not go into the theory with the wrong preconception of what these results are about is important. And secondly making sure the claims are adequate is important and not overly strong (or over claiming). I think this is important particularly in such works, dealing with systems that do not allow a full mathematical analysis.  In particular, just to give some examples:  1. Thm 3, pointed out by the reviewers as well. I don t understand the point of this thm. It basically says that around initialization things are well behaved. The same can be said or proven for many other methods. You argue that this is different, as in other models beside initialization forgetting is not controlled, while you could potentially control it by a forgetting gate. However this is not a theoretical, precise argument. The forgetting gate is learned as well. If we go back to the LSTM scenario, LSTM suffer for vanishing gradients. Also Gers et al. paper does not prove that trying to preserve error has to harm learning (it provides some empirical evidence that is the case, but there have been many other things that affected this results). The point here is not that forget gates are not useful, nor that the gating mechanism proposed by LSTM are not extremely useful. They are. Is that the Thm 3 can not prove or show that using mmRNN is a better way of mitigating (and trading of) vanishing gradient than another model. You do that through your empirical evidence, and I think that is how most of ML works. But is not clear what the point of the theorem is.   2. I do not understand how one reasons theoretically about epsilon in Def 1. I don t see how an empirical observation by Gers et al resolves this. It justifies maybe why vanishing gradients are not always problematic, but that should not affect the definition of what vanishing now means. In the current form, if T goes to infinite, even if technically the network does not suffer from vanishing gradients, the gradients go to 0. Or at least T and epsilon should somehow be tied together to make the definition work.   The issue of defining the vanishing / exploding gradient per unit is also that now is not clear what is problematic or not. Probably having exploding gradient for any given unit is bad, as it might affect the overall gradient. But having a few units suffering from vanishing gradient, is that problematic?  This things need to be quantified better.  I think overall to me the problem is that some of this mathematical statements do not seem to be strong enough or contextualized enough to be properly understood by the reader. I would have understood the formalism if it was trying to correct some misconception in the community, case in which it is important to formalize just to be precise. But I don t think this is what is happening here. As in stands it just feels sloppy.  And I think this retracts considerably from the empirical side of the work and reduces the space you had to give it enough attention. Which should have played main stage. I think the empirical work would have benefited from more analysis (showcasing some of the arguments you were making using the theory), which would have made for a much stronger and convincing paper. The current framing of the paper is unfortunately not the right one.
In this paper, the author present a method for learning a shared latent space between the fMRI activity of multiple individuals processing the same stimulus. The method consists of an auto encoder with a single encoder and subject specific decoders which is specifically regularized to decouple common and shared representations. This paper generated a lot of discussion between the reviewers and the authors, as well as between the reviewers. In light of these discussion, I cannot recommend acceptance at this point, as the paper is not ready. The main concerns were (1) about how the results and improvement are evaluated statistically, (2) that the baselines chosen were not strong enough and did not include existing approaches (neural or non neural) and relatedly (3) that the paper was not framed correctly within the existing literature on finding shared spaces between participants, which would help with determining and understanding the novelty of the proposed approach. Some other smaller points were made by the reviewer can also strengthen the paper for a future submission in a neuroscience or machine learning venue.
### Summary  This paper presents a technique to reduce the worst case latency of inference. The key idea is to use a combination of early exit and filter selection to achieve its results. The filter selection predicts the top k classes for the input and, using that indication, uses the filters that are the most relevant  (using DeepLIFT) to refine the result.  ### Strengths (from Discussion)    The idea is interesting. Early exit, mixtures of experts (one potential interpretation of the filter selection here), as well as pruning are interesting mechanisms for neural network efficiency.  There may be new opportunities to find synergies in their combination.  ### Weaknesses (from Discussion)    The clarity of writing could be significantly improved, particularly in the description and illustration of the constituent techniques. Figures, such as those in https://arxiv.org/abs/2008.13006, that clearly present the constitution of various layers, in particular, would help.    There are relevant and applicable baselines that a comparison would contextualize the strength of the approach (as per Reviewer vKUc examples)    ImageNet experiments appear to be within reach of this experimental apparatus (i.e., without extreme cost). Hence, such experiments would validate the applicability of this approach to practice.     A small point arose that longest path inference was not motivated. Work on optimizing tail latency (https://research.google/pubs/pub40801/) may be helpful contextualization here.  ### Recommendation   My recommendation is Reject. The work here is a very promising start for a new idea. Though requests for additional experimentation and baselines can be ill defined recommendations. Here, scaling of the results to ImageNet as well as comparing against baselines in the literature (as per Reviewer vKUc s examples) would provide much stronger scoping for this work.
Ricci flow is a central topic in geometric analysis. It has had stunning applications in mathematics, most notably the proof of the Poincare conjecture. The major issue is that it, while it can be used to make a manifold more well behaved, it frequently develops singularities. The main contribution fo this paper is in introducing linearly nearly Euclidean metrics. They give a proof of convergence in both short and infinite time, under the Ricci DeTurk flow, and exploit connections to information geometric and mirror descent to develop methods for approximating the gradient flow. The paper is confusingly written (compounded by poor organizational structure and many grammatical mistakes). Perhaps the biggest issue is that it does not have any clear relevance to machine learning. Some sections mention connections to neural networks, but the reviewers found these sections to be indecipherable.
It seems to be an interesting contribution to the area. I suggest acceptance.
This paper proposes a new framework for improved nearest neighbor search by learning a space partition of the data, allowing for better scalability in distributed settings and overall better performance over existing benchmarks.  The two reviewers who were most confident were both positive about the contributions and the revisions. The one reviewer who recommended reject was concerned about the metric used and whether comparison with baselines was fair. In my opinion, the authors seem to have been very receptive to reviewer comments and answered these issues to my satisfaction. After author and reviewer engagement, both R1 and myself are satisfied with the addition of the new baselines and think the authors have sufficiently addressed the major concerns. For the final version of the paper, I’d urge the authors to take seriously R4’s comment regarding clarity and add algorithmic details as per their suggestion. 
This work addresses a novel and important real world setting for semi supervised learning – the open world problem where unlabeled data may contain novel classes that are not seen in labeled data.  The paper provides an approach by combining three loss functions: a supervised cross entropy loss, a pairwise cross entropy loss with adaptive uncertainty margin, and a regularization towards uniform distribution.    The authors were responsive to reviewers’ comments and have respectively improved their paper by adding experiments, including an ablation study of each component of the objective function, study of the effect regularization on unbalanced class distributions, reporting accuracy on pseudo labels.  While two reviewers have slightly increased their scores, some concerns still remain.  This is a borderline paper, and after some discussion and calibration, we decided that the work in its current form does not quite meet the bar for acceptance.
The reviewers appreciated this contribution, particularly its ability to tackle nonstationary domains which are common in real world tasks.   
This paper proposes a method to improve the transfer of visual control policies between robots.  The method extends a visual foresight approach using a learned robot agnostic world dynamics model and a (potentially analytic) robot specific robot dynamics module.  A key aspect of the method is to form a blocky mask over the robot s body in the visual image, thus allowing the learned dynamics to depend less on appearance attributes of the robot.  Planning with these dynamics models operates in the visual observation space.  The method is tested for zero shot transfer on multiple physical robots and also with simulated robots, with positive results across multiple experiments.  The reviews raised multiple concerns on the details of the method and the clarity of the presentation that were largely addressed in the author response.  The authors refined their claims to a demonstration of zero shot transfer of visual skills across real world robots.  The evaluation of the proposed transfer method on real world robots is a notable strength of the paper.  The core limitation, raised by one reviewer, is that the generality of the invariance is only provided by a visual mask on the color and appearance of the robot.  This serves as a limited form of invariance to the robot s dynamics.    The discussion phase did not yield a consensus on the merits of the paper, with the proposed method seen as useful but still limited.  Three knowledgeable reviewers indicate to accept the paper and one indicates to reject.  The formal description does not make the world dynamics explicitly robot agnostic, as the learned model $(P_w(o_w |o_w, r, r , a)$ in Equation 2) still explicitly depends on the actions of the robot ($a$) and thus the robot s dynamics.  Despite some limitations in the formalism, the practical utility of the method is convincingly demonstrated in multiple robot experiments.  This is a clear contribution to the literature, which is supported by three reviewers.  The paper is therefore accepted.
This paper proposes an lightweight method for cross domain few shot learning, using a meta learning approach to predict batch normalization statistics. After the extensive paper revisions and discussion, the reviewers all agreed that this paper is above the bar for acceptance, assuming that the authors will include results for both the standard and expanded target set size in the final version of the paper. The authors are strongly encouraged to include these results in the camera ready version of the paper.
This is an interesting direction but multiple reviewers had concerns about the amount of novelty in the current work, and given the strong pool of other papers, this didn t quite reach the threshold.  
This paper aims to model adversarial noise by learning the transition relationship between adversarial labels and natural labels. In particular, an instance dependent transition matrix to relate adversarial labels and natural labels. Reviewers agreed that the paper is well motivated and well written, and the proposed method is novel. Meanwhile, reviewers raised some concerns about experiments and paper presentation. During discussion, the authors provided a lot of additional results that partially addressed the reviewers  concerns. However, the reviewers still think the experimental part of this paper should be further strengthened before acceptance.   Thus, I recommend to reject this paper. I encourage the authors to take the review feedback into account and submit a future version to another venue.
This paper has been evaluated by four reviewers who overall hesitated between borderline reject/accept. In general, as Rev. 4 points out, this paper appears to cope with over oscillation rather than over smoothing aspect of GCN modeling (something worth clarifying). Rev. 3 also rightly points out that the connection between the heat kernel and GCN in fact was established in previous works. Also, the connection between SGC (polynomial filter) and  the heat diffusion (the spectral filter matrix) is hard to overlook (the impression that this work builds heavily on SGC). Therefore, while AC sympathizes with the idea, it is also difficult to overlook the incremental nature of the paper and therefore the paper cannot be accepted in its current form.
In this work, the authors develop a method for providing frequentist confidence intervals for a range of deep learning models with coverage guarantees.  While deep learning models are being used pervasively, providing reasonable uncertainty estimates from these models remains challenging and an important open problem.  Here, the authors argue that frequentist statistics can provide confidence intervals along with rigorous guarantees on their quality.  They develop a jack knife based procedure for deep learning.  The reviews for this paper were all borderline, with two weak accepts and two weak rejects (one reviewer was added to provide an additional viewpoint).  The reviewers all thought that the proposed methodology seemed sensible and well motivated.  Among the cited issues, major topics of discussion were the close relation to related work (some of which is very recent, Giordano et al.) and that the reviewers felt the baselines were too weak (or weakly tuned).  The reviewers ultimately did not seem convinced enough by the author rebuttal to raise their scores during discussion and there was no reviewer really willing to champion the paper for acceptance.  Unfortunately, this paper falls below the bar for acceptance.  It seems clear that there is compelling work here and addressing the reviewer comments (relation to related work, i.e. Robbins, Giordano and stronger baselines) would make the paper much stronger for a future submission.
This work describes a system for collaborative learning in which several agents holding data want to improve their models by asking other agents to label their points. The system preserves confidentiality of queries using MPC and also throws in differentially private aggregation of labels (taken from the PATE framework). It provides expriments showing computational feasibility of the system. The techniques use active learning to improve the models.  Overall the ingredients are fairly standard but are put together in a new (to the best of my , admittedly limited, knowledge of this area). This seems like a solid attempt to explore approaches for learning in a federated setting with strong limitations on data sharing.
This is interesting work, but not yet sufficiently mature for publication.  Although the authors propose an novel algorithm and provide an analysis, the reviewers raised several criticisms about the comparison to previous work, the lack of any empirical evaluation, the strength and unnaturalness of the assumptions used to establish convergence.  After discussion, the reviewers remained largely unsatisfied with the author responses to these questions, and none recommended accepting this paper.
The paper proposes a filtration based on the covers of data sets and demonstrates its effectiveness in recommendation systems and explainable machine learning. The paper is theory focused, and the discussion was mainly centered around one very detailed and thorough review. The main concerns raised in the reviews and reiterated at the end of the rebuttal cycle was lack of clarity, relatively incremental contribution, and limited experimental evaluation. Due to my limited knowledge of this particular field, I base my recommendation mostly on R1 s assessment and recommend rejecting this submission.
This paper proposes to apply regularizers such as weight decay or weight noise only periodically, rather than every epoch. It investigates how the "non regularization period", or period between regularization steps, interacts with other hyperparameters.   Overall, the writing feels somewhat scattered, and it is hard to identify a clear argument for why the NRP should help. Certainly one could save computation this way, but regularizers like weight decay or weight noise incur only a small computational cost anyway. One explicit claim from the paper is that a higher NRP allows larger regularization. There s a sense in which this is demonstrated, though not a very interesting sense: Figure 4 shows that the weight decay strength should be adjusted proportionally to the NRP. But varying the parameters in this way simply results in an unbiased (but noisier) estimate of gradients of exactly the same regularization penalty, so I don t think there s much surprising here.  Similarly, Section 3 argues that a higher NRP allows for larger stochastic perturbations, which makes it easier to escape local optima. But this isn t demonstrated experimentally, nor does it seem obvious that stochasticity will help find a better local optimum.  Overall, I think this paper needs substantial cleanup before it s ready to be published at a venue such as ICLR. 
The paper proposes a new framework for out of distribution detection, based on variational inference and a prior Dirichlet distribution.  The reviewers and AC note the following potential weaknesses: (1) arguable and not well justified choices of parameters and (2) the performance degradation under many classes (e.g., CIFAR 100).  For (2), the authors mentioned that this is because "there are more than 20% of misclassified test images". But, AC rather views it as a limitation of the proposed approach. The out of detection detection problem is a one or two classification task, independent of how many classes exist in the neural classifier.  In overall, the proposed idea is interesting and makes sense but AC decided that the authors need more significant works to publish the work.
The paper proposes a feature smoothing technique as a new and "cheaper" technique for training adversarially robust models.   Pros:  * the paper is generally well written and the claimed results seem quite promising  * the theory contribution are interesting  Cons:  * the main technique is fairly incremental  * there were concerns regarding the comprehensiveness of evaluations and baselines used
This paper initially received three negative reviews: 4,4,4. The main concerns of the reviewers included limited methodological novelty and an oversimplistic experimental setup. The authors did not submit their responses. As a result, the final recommendation is reject.
This paper incorporates tree structured information about a sentence into how transformers process it. Results are improved. The paper is clear. Reviewers liked it. Clear accept.
This paper presents a dataset to evaluate the quality of embeddings learnt for source code. The dataset consists of three different subtasks: relatedness, similarity, and contextual similarity. The main contribution of the paper is the construction of these datasets which should be useful to the community. However, there are valid concerns raised about the size of the datasets (which is pretty small) and the baselines used to evaluate the embeddings   there should be a baselines using a contextual embeddings model like BERT which could have been fine tuned on the source code data. If these comments are addressed, the paper can be a good contribution in an NLP conference. As of now, I recommend a Rejection.
Four reviewers have reviewed this manuscript and two found it borderline leaning towards acceptance, two other reviewers scored it below the acceptance threshold. While the authors *identify the key challenges and bottlenecks in 3D point cloud model*, the most positive two reviewers notice that the depthwise kernel and the attention mechanism (and similar tools) are well known in the literature and that this work is *more of an engineering improvement than a technical contribution*, which erodes the novelty of the proposed idea on that front. While authors noticed some discrepancies in the numbers quoted by Rev. 3, the model gains are also nonetheless modest compared to other models. Overall, the feeling amongst the reviewers was that  the presentation of NK attention could be further improved and that the paper uses very heavy machinery to achieve results comparable with SOTA.  On this occasion, the manuscript is below the acceptance threshold with even the borderline positive reviewers having their doubts about clear cut technical novelty.
This paper presents a certified defense method for adversarial patch attacks. The proposed approach provides certifiable guarantees to the attacks, and the reviewers particularly find its experiments results interesting and promising. The added new experiments during the rebuttal phase strengthened the paper. There still is a remaining concern that is novelty is limited as this paper could be viewed as the application of the original IBP to patch attacks, but the reviewers believe in that its empirical results are important.
The paper focuses on characterizing the expressiveness of graph neural networks. The reviewers were satisfied that the authors answered their questions suffciiently and uniformly agree that this is a strong paper that should be accepted.
This paper address the increasingly studied problem of predictions over long term horizons. Despite this, and the important updates from the authors, the paper is not yeat ready and improvements identified include more control over the fair comparisons, improved clarity in exposition.
This paper introduces a biologically inspired locally sensitive hashing method, a variant of FlyHash. While the paper contains interesting ideas and its presentation has been substantially improved from its original form during the discussion period, the paper still does not meet the quality bar of ICLR due to its limitations in terms of experiments and applicability to real world scenarios.
A simple but sensible idea to improve VAE with good experimental results.
The paper considers the problem of distributed optimization in the Federated Learning setting in particular when the data in the clients is non i.i.d. The paper points to the problem of catastrophic forgetting during the local update stages to be a cause for the bad training of models and proposes to fix it via introducing a pseudo loss, which are based on adversarial examples of the global model in the previous step and the adversarial examples of the local model at current step.   The paper s core idea was generally appreciated by the reviewers as well as the favorable evaluation of the proposed method when compared with other existing algorithms. The authors also provided further additional information during the rebuttal period and addressed author comments adding enough justification to the paper to resolve issues in the submission. One lingering issue that remains is the hyper parameter tuning on test set results that was performed. The authors have promised to redo experiments based on validation sets.   Overall the paper is borderline but I am recommending accept based on novelty of the idea and strong experimental results.
This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written.   However, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions.   Furthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model.   This concern is further validated by the fact that Black box attacks are often the best performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step.   The paper thus requires significantly stronger baselines and attacks.
This paper was reviewed by 3 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. 
The paper proposes to recalibrate predictive models by fitting a normalizing flow on top of the predictive model on a held out validation set using side information. At a high level this idea has some potential, especially in the multivariate setting, but there are several directions for improvement:    Comparison with a broader set of baselines as suggested by the reviewers     Clarity on why recalibrate with a normalizing flow especially in the 1 d case     Why not any other model with explicit density? Are there other important desiderata?     A motivating experiment that makes the potential value clear
This paper was reviewed by three experts. Initially, the reviews were mixed with several concerns raised. After the author response, there continue to be concerns about need for significantly more experiments. If this were a journal, it is clear that recommendation would be "major revision". Since that option is not available and the paper clearly needs another round of reviews, we must unfortunately reject. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. 
This paper proposes to use contrastive predictive coding for self supervised learning.  The proposed approach is shown empirically to be  more effective than existing self supervised learning algorithms.  While the reviewers found the experimental results encouraging, there were some questions about the contribution as a whole, in particular the lack of theoretical justification.
The new non linearity proposed in this paper present interesting observations and improvements on image and text datasets. However, reviewers point out that there should’ve been more comparisons to other efficient transformers and on more datasets. The speed improvements are also not clear. I’d encourage the authors to revise and submit in the future.
This work presents a practical unsupervised pretraining strategy that does not require layer wise training stages. Clearly this is an area that has lot of potential and the work seems to head in the right direction.  However, despite a very positive review, I share the same concerns raised by the remaining 3 reviewers. Better motivation and clarity is needed and, considering the proposed approach, a much more thorough comparison and analysis of the theoretical advantages and guarantees of such an approach. A main argument is that the method can handle arbitrary networks due to the way it is implemented, it is however not clear how practical that would be and how that would work in presence of non linearities.  Experiments require also additional work to be presented with clear and standard baselines, not by presenting SOTA on arbitrary tasks. This seemed to be a main concern of the reviewers.
This paper proposes a method for incentivizing exploration in self supervised learning using an inverse model, and then uses the learned inverse model for imitating an expert demonstration. The approach of incentivizing the agent to visit transitions where a learned model performs poorly. This relates to prior work (e.g. [1]), but using an inverse model instead of a forward model. The results are promising on challenging problem domains, and the method is simple. The authors have addressed several of the reviewer concerns throughout the discussion period. However, three primary concerns remain: (A) First and foremost: There has been confusion about the problem setting and the comparisons. I think these confusions have stemmed from the writing in the paper not being sufficiently clear. First, it should be made clear in the plots that the "Demos" comparison is akin to an oracle. Second, the difference between self supervised imitation learning (IL) and traditional IL needs to be spelled out more clearly in the paper. Given that self supervised imitation learning is not a previously established term, the problem statement needs to be clearly and formally described (and without relying heavily on prior papers). Further, the term self supervised imitation learning does not seem to be an appropriate term, since imitation learning from an expert is, by definition, not self supervised, as it involves supervisory information from an expert. Changing this term and clearly defining the problem would likely lead to less confusion about the method and the relevant comparisons. (B) The "Demos" comparison is meant as an upper bound on the performance of this particular approach. However, it is also important to understand what the upper bound is on these problems in general, irrespective of whether or not an inverse model is used. Training a policy with behavior cloning on demonstrations with many (s,a) pairs would be able to better provide such a comparison. (C) Inverse models inherently model the part of the environment that is directly controllable (e.g. the robot arm), and often do not effectively model other aspects of the environment that are only indirectly controllable (e.g. the objects). If the method overcomes this issue, then that should be discussed in the paper. Otherwise, the limitation should be outlined and discussed in more detail, including text that outlines which forms of problems and environments this approach is expected to be able to handle, and which of those it cannot handle.  Generally, this paper is quite borderline, as indicated by the reviewer s scores. After going through the reviews and parts of the paper in detail, I am inclined to recommend reject as I think the above concerns do not outweigh the pros.  One more minor comment is that the paper should consider mentioning the related work by Torabi et al. [2], which considers a similar approach in a slightly different problem setting.  [1] Stadie et al. https://arxiv.org/abs/1507.00814 [2] Torabi et al. IJCAI  18 (https://arxiv.org/abs/1805.01954)
This paper studies tradeoffs in the design of attention based architectures. It argues and formally establishes that the expressivity of an attention head is determined by its dimension and that fixing the head dimension, one gains additional expressive power by using more heads.  Reviewers were generally positive about the question under study here, but raised important concerns about the significance of the results and the take home message in the current manuscript. The AC shares these concerns, and recommends rejection, while encouraging the authors to address the concerns raised during this discussion. 
The paper considers ensambling of smooth classifiers to improve certified robustness. Theoretical results are provided showing that taking ensambles of a large number of models is useful, while experiments show that combining only a small number of models improves performance. On the negative side, the experiments are somewhat inconclusive, as the base models are not state of the art, and the combined results do not achieve state of the art performance. In  this respect, further studies would be necessary to explore the effectiveness of the proposed technique.  In summary, while the topic of the paper is interesting and timely, the proposed ensambling technique is not especially exciting (as it is what one would naturally expect). On the other hand, the problem is reasonably well investigated (e.g., details are worked out well, both theoretical and experimental results are presented), although further experiments are needed (as recommended by the reviewers) to properly assess the potential and limitations of the approach. Accordingly, all reviewers agreed in the discussion that this is a borderline paper. Therefore, unfortunately, it cannot be accepted this time due to the heavy competition at the conference. The authors are encouraged to resubmit a revised version to the next venue, taking into consideration the reviewers  recommendations.
The reviewers overall appreciated the efforts of the authors in making NAS more computationally efficient. The paper could greatly benefit from further editing/restructuring with the goal of improving clarity, as it’s currently hard to navigate and understand in places. Future submissions of this work would benefit from more extensive empirical validation that motif networks mimic the original network. The reviewers also agreed that for the method to be appealing/useful, a general way to generate motif networks is needed. Overall, the outcome was that this is a very interesting idea but needs further development along the directions outlined above.
Inspired by BERT and the corresponding masked language modeling objective, this paper proposes masked image modeling as a pre training technique for vision transformer. More precisely, the image is tokenized using a pre trained tokenizer, and the goal is to predict the token indices corresponding to masked patches of the image. As noted by the reviewers, the proposed method is simple, works very well in practice and the paper is well written. Since this work potentially opens a whole new research direction, my recommendation is to accept with oral presentation.
The consensus view was that the reviewers were not convinced that the analysis done in the paper was sufficient motivated.
This paper presents a method to improve search engines; the method is designed based on the BM25 retrieval method and is evaluated on NQ open dataset. The reviewers agree that the motivation is interesting and implementation is reasonable, but the authors have only showed the impact of their approach over one retrieval method and one dataset, which is limited and does not show if the method is general enough or not.
This paper presents a sensible, but somewhat incremental, generalization of neural architecture search.  However, the experiments are only done in a single artificial setting (albeit composed of real, large scale subtasks).  It s also not clear that such an expensive meta learning based approach is even necessary, compared to more traditional approaches.  If this paper was less about proposing a single new extension, and more about putting that extension in a larger context, (either conceptually or experimentally), it would be above the bar. 
The paper proposes a new method to learn representation and community structure of a network jointly. The reviewers agree that the paper contains some interesting ideas but they raise also some important concerns. For example:    even after considering the authors  rebuttal, the paper seems not too novel. In particular the results seem a bit incremental over vGraph.    the notion of community is not formalized by the authors neither in the paper or in the rebuttal. The paper would benefit greatly by having such formal definition   Overall, the paper is interesting but it does not meet the high acceptance bar of ICLR
The paper shows (nearly) matching upper and lower bounds on dynamic regret for non stationary finite horizon reinforcement learning problems. The paper studies an important problem and the results are interesting. Some reviewers are concerned that there is not enough algorithmic and theoretical innovations in light of prior results. Authors need to improve the presentation and add a more detailed discussion on related works, the novelty and the originality of the paper, and the new algorithmic and theoretical contributions. Finally, authors can improve the submission by implementing the proposed method and adding experiments.
This paper proposes a novel recurrent network called RIMs for improving generalization and robustness to localized changes. The network consists of largely independent recurrent modules that are sparsely activated and interact through soft attention. The experiments on a range of diverse tasks show that RIMs generalizes better than LSTMs.  The overall feedback from reviewers is positive: the paper is well written, the idea is interesting, and the experiments cover a wide range of diverse tasks.  The main concerns of most reviewers are the fairness of comparison, the limited novelty, and lacking details on how and why the system works. The authors pointed out that RIMs are not a straightforward combination of attention and RNN, and it has fewer parameters than LSTMs. They also conducted ablation study to demonstrate the benefits of RIMs and provided the missing details in the revised version.  In summary, this paper presents an important research direction for systematic generalization using modularized network. The paper is well written, the idea is novel and interesting, and the experiments cover a wide range of diverse tasks. Hence, it makes a worthwhile contribution to ICLR and I am recommending acceptance of this paper. 
The paper proposes using a mixture of Gaussian models for transformer keys (MGK) so that the posterior distribution of key given query matches the attention scores in the transformer architecture under some assumption. Similarly but in reverse, the query given key under a MoG also matches transformer attention score. The paper proposes that this formulation can learn more diverse attention heads and reduce the computation by replacing some heads with the Gaussian mixture heads which are easier to compute. Moreover, the authors show that it is straightforward to transfer their formulation to linear transformers. The authors perform experiments on the Long Range Arena benchmark and on Wikitext 103 where they show some improvements with less attention heads, while using up to 20% less FLOPs for softmax transformers and around mostly 20% but up to 80% less parameters for the worse linear transformer.  Reviewers generally find that the interpretation of transformers as mixture of Gaussian interesting, although it is based on normalization assumptions. The main objections from both positive and negative reviewers is the weakness of the empirical results. In summary, all the improvements in perplexity are less than 0.5, and accuracy improvements are less than 1. In the discussions, the authors claim that their comparisons are using fewer heads vs baselines with more heads, which is how the tables are presented, but upon closer look I find this unconvincing since the same head count comparisons can be reconstructed and still show very weak results. For example in table 1, acc(MGK4) 61.85 vs. acc(baseline4) 61.23, or in table 2 acc(LinearMG4) 55.7 vs acc(LK4 baseline) 55.61. To begin with, it would be better to compare the same number of heads for more head to head comparisons, so I find the whole argument to be misleading. The claims about FLOPS and memory has a similar flavor where the differences are small but the claims were big. A visual evaluation of figure 3 show that most reductions are around 10% in FLOPs or parameters for the better softmax model (note the axis does not start at 0). The retrieval task for the worse linear transformer is the only case where the FLOPs reduction seems significant. Given that EM is now required during training, I d interpret the results as a negative for MGK given the marginal improvements achieved.  Several reviewers are unhappy with the strength of the empirical results while most reviewers gave favorable scores after the discussion where the authors insisted that some valid points are not valid. The authors requested the AC to look further into the one negative review due to the lack of a reviewer s response. After taking some time to look at the substance of the paper and the review, I recommend rejection due to the weakness of the results, the misleading presentation and discussions.
This paper follows the recent line of work of theoretically analyzing the Neural Collapse phenomenon, by making certain simplifying assumptions on the problem setup. In this case, the authors use cross entropy loss on an unconstrained model where second to last representations become free variables. Their main results characterise the NC as the only global minimiser.  Reviewers were positive about this work, and concluded it presents a valuable addition to the growing analysis of NC. They also pointed out several clarity issues that should be addressed in the final revision, including a more objective comparison to prior work. Ultimately this work will be an interesting addition to the conference, and therefore the AC recommends acceptance.
The reviewers are unanimous that the paper is not sufficiently clear and could be improved with better empirical results.
This paper investigates the performance of various second order optimization methods for training neural networks. Comparing different optimizers is worthwhile, but as this is an empirical paper which doesn t present novel techniques, the bar is very high for the experimental methodology. Unfortunately, I don t think this paper clears the bar: as pointed out by the reviewers, the comparisons miss several important methods, and the experiments miss out on important aspects of the comparison (e.g. wall clock time, generalization). I don t think there is enough of a contribution here to merit publication at ICLR, though it could become a strong submission if the reviewers  points were adequately addressed. 
The paper seems technically correct and has some novelty, but the relevance of the paper is questionable. Considering the selectiveness of ICLR, I cannot recommend the paper for acceptance at this point.   In more detail: the authors propose a technique for estimating density rations between a target distribution of real samples and a distribution of samples generated by the model, without storing samples. The method seems to be technically well executed and verified. However, there was major concerns among multiple reviewers that the addressed problem does not seem relevant to the ICLR community. The question addressed seemed artificial, and it was not considered realistic (by R2 and also by R1 in the confidential discussion). R3 also expressed doubts at the usefulness of the method.   Furthermore, some doubts were expressed regarding clarity (although opinions were mixed on that) and on the justification of the modification of the VAE objective to the continual setting. 
This paper studies the problem of producing distribution free prediction sets using conformal prediction that are robust to test time adversarial perturbations of the input data. The authors point out that these perturbations could be label and covariate dependent, and hence different from covariate shift handled in Tibshirani et al 19, the label shift handled in Podkopaev and Ramdas 21, and the f divergence shifts of Cauchois et al 2021.   The authors propose a relatively simple idea that has appeared in other literatures like optimization but appears to be new to the conformal literature: (i) use a smoothed (using Gaussian noise on X, and inverse Gaussian CDF) nonconformity score function, in order to control its Lipschitz constant, (ii) utilize a larger score cutoff than the standard 1 alpha quantile of calibration scores employed in conformal prediction. The observation that point (i) alone lends some robustness to adversarial perturbations of the data is interesting. As several experiments in the paper and responses to reviewers show, this comes at the (apparently necessary) price of larger prediction sets.   I read through all the comments and also the supplement. The authors have responded very well to all the reviewers questions/concerns, adding significant sections to their supplement as a result. Three reviewers are convinced, but one remaining reviewer requested additional experiments to compare with Cauchois et al (in addition to all the others already produced by the authors originally and in response to reviewers). However, the authors point out that the code in the aforementioned paper was not public, but they were able to privately get the code from the authors during the rebuttal period. At this point, I recommend acceptance of the paper even without those additional experiments, since it is not the authors  fault that the original code was not public. Nevertheless, I suggest to the authors that, if possible, they could add some comparisons to the camera ready since they now have the code.  I congratulate the authors on a nice work, a very solid rebuttal, and also the astute reviewers on pointing out various aspects that could be improved.   Minor point for the authors (for the camera ready): I would like to comment on the Rebuttal point 4.4 in the supplement, which then got further discussed in the thread. The reviewer points out four references [R1 R4]. I will add one more to the list [R5] https://arxiv.org/pdf/1905.10634.pdf (Kivaranovic et al, appeared in 2019, published in 2020). I think the literature reviews in this area are starting to be messy, and all authors need to do a better job. Clearly, the original paper of Vovk et al already establishes various types of conditional validity (and calls it PAC style guarantee), produces guarantees that others in this area produce, and it appears that much recreation of the wheel is occurring. For eg, [R2, R4] do not cite [R5], despite [R5] appearing earlier and being published earlier, and having PAC style guarantees and experiments with neural nets, etc. However, in turn, [R5] do not cite Vovk [R1], but [R2, R4] do cite [R1]. (And [R3] does not seem to be relevant to this discussion of conditional validity?) In any case, I am not sure any of these papers need citing since the current paper does not deal with conditional validity. If at all, just one sentence like "Conditional validity guarantees, of the styles suggested by Vovk [2012], would be an interesting avenue for future work".
This is a tough choice as it is a reasonably strong paper. I am similar to another reviewer quite confused how this graph matching can "only focus on important nodes in the graph" This seems counter intuitive and the only reason given in the rebuttal is that other people have done it also..  Relatedly: "In graph matching, we not only care about the overall similarity of two graphs but also are interested in finding the correspondence between the nodes of two graphs"  I am sorry for the authors and hope they will get it accepted at the next conference.
The paper explores the application of generative adversarial networks as posterior models in simulation based inference. A new method is proposed, and its connections with related work are studied. The proposed method is empirically evaluated on joint inference of up to 784 parameters.  The reviews are borderline, with one weak reject, two weak accepts, and one strong accept. Overall, the paper is well written and well executed. Its main strength is the promising performance of the proposed method in high dimensional parameter spaces, which are out of reach for many existing approaches. The main weakness of the paper is its lack of novelty: the proposed method is only marginally different from already existing ones, while the paper could have explored the differences to a greater extent.  On balance, I m leaning towards recommending the paper for acceptance. Despite the lack of novelty, the paper is well executed with potential impact in high dimensional simulation based inference.
All the reviewers reach a consensus to reject the current submission.   In addition, there are two assumptions in the proof which seemed never included in Theorem conditions or verified in typical cases.   1) Between Eq (16) and (17), the authors assumed the  extended restricted strong convexity’ given by the un numbered equation.   2) In Eq. (25), the authors assume the existence of \sigma making the inequality true.  However those assumptions are neither explicitly stated in theorem conditions, nor verified for typical cases in applications, e.g. even the square or logistic loss. The authors need to address these assumptions explicitly rather than using them from nowhere.
The authors propose bringing lexicase selection from evolutionary computation and applying it to the optimisation of gradient descent. This is done by training a set of p networks and using their performance to select this set of p networks as training progresses on random subsets of the training data. The reviewers felt, and I agree, that the paper was well written and its method now well described. Concerns raised during review include: additional computational cost, novelty, and marginal performance improvements. Nonetheless after discussion, while the computational cost is indeed higher, it is a novel application, and the reviewers were all in agreement with acceptance after further discussion around experiments.
This work proposes an instance specific label smoothing method, which is formulated as a two stage optimization problem for finding the optimal label smoothing. The authors show that the proposed approach can be equivalent to an efficient variant of self distillation techniques (i.e. no need to store the parameters or the output of a trained model). Experiments on image classification (CIFAR 10 and CIFAR 100) and natural language understanding datasets (the GLUE benchmark) demonstrate that our method is competitive against strong baselines.  The reviewers find the proposed approach reasonable, and the presentation clear. However, they all rated the paper as borderline, due to some concerns that the submission has in its current form. These include limited novelty (by CUiW) [the link between label smoothing and knowledge distillation is largely based on previous research findings (e.g., Yuan et al., 2020)], nonconvincing results on the effectiveness of the proposed method (by bwuZ) [Improvement of Pseudo KD in practice is not significant in terms of test accuracy gains], and lack of comparison with some recent related methods (by JBRd as well as other reviewers). The authors responded to these (and other concerns), but this did not convince the reviewers about the concerns listed above. I recommend the authors to resubmit after addressing these issues.
The paper aims to generate molecules with desired properties using a variant of supervised variational auto encoders. Disentanglement is encouraged among the style factors.  The reviewers point out that the idea is nice, but authors avoid quantitative comparison with SotA Graph based generative models.  Especially, the JT VAE is acknowledged as a strong baseline widely in the community and is a VAE based model, it is important to do these comparisons. 
This paper is very interesting and timely, but as the reviewers note there is significant room for improvement in the clarity of the presentation and evaluation. In addition to the references mentioned by the reviewers, some other relevant references are the following:   [1] Evan Rosenman, Nitin Viswanathan, "Using Poisson Binomial GLMs to Reveal Voter Preferences," https://arxiv.org/abs/1802.01053  [2] Law, H. C. L., Sutherland, D., Sejdinovic, D., & Flaxman, S. (2018, March). "Bayesian approaches to distribution regression." In International Conference on Artificial Intelligence and Statistics (pp. 1167 1176). 
The paper proposes a new offline RL technique to generalize across domains. The paper was initially confusing (i.e., MDP vs POMDP) and weak empirically.  The authors greatly improved the paper.  However, a the end of the day, it is still not clear why the proposed approach performs better than existing techniques.  We can think of the cumulant function with the discrete labels as essentially computing some statistics of future actions, observations and rewards.  This is what every self supervised technique does.  They differ in terms of their particular choice of statistics and architecture.  The paper does not sufficiently motivate the particular architecture.  Interestingly, in the experiments, the best statistics are cumulative rewards, which are closely related to the Q values. In that case, it is even less clear why the approach should be beneficial since RL techniques that generalize across domains by learning state representations to predict Q values seem very closely related.    Despite the updates to the paper, the POMDP references are still confusing.  The issue is that the paper embeds observations as if they were sufficient to predict future observations and rewards.  This corresponds to the memoryless approach where a policy is optimized based on the last observation instead of the history of past actions and observations.  Memoryless strategies are effective only when the last observation is a sufficient statistic, meaning that we really have a (near) fully observable MDP.  The paper should discuss this and acknowledge that the approach will suffer in domains where memory of past actions and observations is critical.
This paper adapts the idea of progressive growing of GANs to time series synthesis. The reviewers thought that the idea was well motivated. DRP7 initially expressed concern w.r.t. novelty. They were also concerned with the lack of certain baselines. The authors responded, highlighting its contributions w.r.t. Evaluation (Context FID score) and extensiveness of the evaluation. The authors also added missing references but pushed back on the additional baselines. DRP7 raised their score.  Reviewer pbaT was also positive about the work though had some questions and suggestions for improving clarity. They had initially given a low score for “correctness” but raised this, indicating they were satisfied their clarity concerns were addressed. Reviewer RLDM (whose code name happens to match a ML conference) thought the work was novel and appreciated the introduction of a new metric for evaluating the quality of generated time series data. They remarked on the thoroughness of the experiments and the quality of the presentation. They asked some clarifying questions to which the authors provided a response. Reviewer 4v5L also had a concern with novelty, felt the loss function was “heuristic” and didn’t see the utility of the FID based score. They also presented several clarifying questions. The authors provided a lengthy response to that reviewer’s concerns, having run additional analysis, and the reviewer upgraded their score in response. With all reviewers on the accept side of the fence I am inclined to recommend acceptance. Please note 4v5L’s comment that “the paper still needs significant edits to reflect the points in the reviewer responses”.
This paper develops a linear quadratic model predictive control approach for safe imitation learning.  The main contribution is an analytic solution for the derivative of the discrete time algebraic Riccati equation (DARE).  This allows an infinite horizon optimality objective to be used with differentiation based learning methods.  An additional contribution is the problem reformulation with a pre stabilizing controller and the support of state constraints throughout the learning process.  The method is tested on a damped spring system and a vehicle platooning problem.  The reviewers and the author response covered several topics. The reviewers appreciated the research direction and theoretical contributions of this work.  The reviewers main concern was the experimental evaluation, which was originally limited to a damped spring system.  The authors added another experiment for a substantially more complex continuous control domain.  In response to the reviewers, the authors also described how this work relates to non linear control problems.  The authors also clarified the ability of the proposed method to handle state based constraints that are not handled by earlier methods.  The reviewers were largely satisfied with these changes.  This paper should be accepted as the reviewers are satisfied that the paper has useful contributions.
This paper proposes an architecture of a policy network (WaveCorr) that is particularly effective for portfolio management tasks.  A key observation that leads to the design of WaveCorr is that the dependency across asset should be treated differently from the dependency across time.  The proposed WaveCorr has the property that it is "permutation invariant" with respect to assets, which means that the class of functions that can be represented by WaveCorr is invariant to permutation of assets.  WaveCorr is shown to achieve the state of the art performance in a portfolio management task.  A major point of discussion was the definition of "permutation invariance".  The reviewers and AC understood the difference between the permutation invariance defined in this paper and that studied in the prior work (the output of a network is insensitive to the permutation of the particular values of the input).  With the definition in this paper, however, a fully connected layer is permutation invariant, but the Corr layer proposed in the paper appears to have more structure.  It is unclear exactly what properties of the Corr layer leads to the performance improvement.
This paper derives a generalization bound on target loss based on training loss and reverse KL divergence between source and target representation distributions. Then, proposes an algorithm for DA using inverse KL on representations. they show that inverse KL term can be estimated efficiently without the need for additional networks and minimax objective. The experiments show the efficiency of the proposed algorithm in terms of improving target accuracy. The paper touches an important problem and the proposed idea is simple and effective.   There were several concerns regarding the paper that were addressed during rebuttal period, such as strength of assumptions, experiments with different values of beta, experiments on office31 dataset, novelty of theoretical results, significance of derived bounds and comparison to [3]. The remaining concern is on comparing the proposed method to recent work in domain adaptation.  I ask the authors to add the following to the camera ready (1) visualizations they have promised that depicts their method leading to better alignment and (2) add the points raised in defending the novelty of theoretical results.
This paper attempts to decouple two factors underlying the success of GANs: the inductive bias of deep CNNs and adversarial training. It shows that, surprisingly, the second factor is not essential. R1 thought that comparisons to Generative Moment Matching Networks and Variational Autoencoders should be provided (note: this was added to a revised version of the paper). They also pointed out that the paper lacked comparisons to newer flavors of GANs. While R1 pointed out that the use of 128x128 and 64x64 images was weak, I tend to disagree as this is still common for many GAN papers. R2 was neutral to positive about the paper and thought that most importantly, the training procedure was novel. R3 also gave a neutral to positive review, claiming the paper was easy to follow and interesting. Like R1, R3 thought that a stronger claim could be made by using different datasets. In the rebuttal, the authors argued that the main point was not in proposing a state of the art generative model of images but to provide more an introspection on the success of GANs. Overall, I found the work interesting but felt that the paper could go through one more review/revision cycle. In particular, it was very long. Without a champion, this paper did not make the cut.
This paper proposes a meta learning approach for few shot text classification. The main idea is to use an attention mechanism over the distributional signatures of the inputs to weight word importance. Experiments on text classification datasets show that the proposed method improves over baselines in 1 shot and 5 shot settings.  The paper addresses an important problem of learning from a few labeled examples. The proposed approach makes sense and the results clearly show the strength of the proposed approach.  R1 had some questions regarding the proposed method and experimental details. I believe this have been addressed by the authors in their rebuttal.  R2 suggested that the authors clarified their experimental setup with respect to prior work and improved the clarity of their paper. The authors have made some adjustments based on this feedback, including adding new sections in the appendix.  R3 had concerns regarding the contribution of the approach and whether it trades variance for bias. The authors have addressed most of these concerns and R3 has updated their review accordingly.  I think all the reviewers gave valuable feedbacks that have been incorporated by the authors to improve their paper. While the overall scores remain low, I believe that they would have been increased had R1 and R2 reassessed the revised submission. I recommend to accept this paper. 
This well written paper introduces an improved exploration strategy by exploiting knowledge about sequences of actions that lead to the same state. The idea is straightforward and easy to understand and apply, which makes it potentially interesting. An important downside is the limited applicability of the method, as there mainly seems to be an advantage in (mostly deterministic) grid like MDPs. In addition, priors about action sequence equivalences have to be available. Overall, the contribution of the paper is not deemed significant enough for publication at a top tier conference like ICLR by the majority of the reviewers as well as myself. For these reasons, I recommend rejection.
The paper studies an interesting problem motivated by VLSI design. The reviewers agree that there are interesting aspects of the RC algorithm. Nevertheless, the paper could be improved by a clearer characterization/apples to apples comparison to baselines, particularly regarding computation cost, use of parallelism, as well as a more thorough contrast to state of the art in general. Given the contribution is experimental, and this is a well studied problem, it is important to establish whether the solution is indeed best in class; cost due to training should be taken into account, and minimized to the extend possible. Going beyond the baselines considered here, as well as reviewing possible theoretical connections to other problems and guarantees, would also strengthen the paper.
This paper invariantizes distribution based deep networks by using pairwise embedding of the set’s elements.  The idea is inspired from De Bie et al. (2019), which allows invariance to be incorporated through the interaction functional.  Although the paper is well executed with solid theoretical analysis and solid response to the reviewers  comments, the novelty is limited, and reviewers have concerns with experiments and presentation.  
This paper proves that fully connected wide ReLU NNs trained with squared loss can be decomposed into two parts: (1) the minimum complexity solution of an interpolating kernel method, and (2) a term depends heavily on the initialization. The main concerns of the reviewers include (1) the contribution are not significant at all given prior work; (2) flawed proof,  and (3) lack the comparison with prior work. Even the authors addressed some of the concerns in the revision, it still does not gather sufficient support from the reviewers after author response. Thus I recommend reject.
The paper analyses the loss landscape induced by AUC loss. Reviewers found critical issues with the paper, and the Authors have not provided feedback. As such I have to recommend rejecting the paper. I thank the Authors for submitting the paper to the ICLR conference. I hope the reviews will be helpful in improving the paper.
The paper propose a value aware transformer for sparse multivariate time series data. While to approach is well motivated and the problem well motivated from a clinical viewpoint, the comparison with related work brought up by reviewer qFRi and  reviewer ph4X would really make it clear where this paper stands. The authors attempt to diffuse this issue in their replies, but empirical comparisons in the paper would guide practitioners more. This is especially important as the paper is motivated by a real world problem.
Reviewers agree that the paper excels in providing a principle pipeline that combines CNNs and GPs with a Poisson Gamma distribution to provide a generic approach for multiresolution modelling of tumour mutation rates. As a whole such combination of techniques addresses a key challenge in computational biology that also scales to large datasets. 
This work shows that the source filter model of speech production naturally arises in the latent space of a variational autoencoder (VAE). It is interesting that the fundamental frequency and formant frequencies are encoded in orthogonal subspaces of the VAE latent space   this opens up a possible way of easily controlling these.  The key motivation/goal of the paper has caused some confusion. The abstract highlights an observation about VAE’s learned representation. In retrospection, some reviewers have not found the findings very surprising. On the other hand, the authors also do not attempt at developing and evaluating a speech generation method. As is, the paper seems to be much more suitable to a specialized workshop on speech. Alternatively, the paper could be extended to other modalities to show steerability of a representation using a synthetic dataset. However, the current scope seems to be somewhat limited hence I am not able to recommend the current manuscript for acceptance.
This paper generalize the idea of Mixup based data augmentation for regression. Compared to classification for which Mixup was used, the paper argues that in regression the linearity assumption only holds within specific data or label distances. The paper thus proposes MixRL to select suitable pairs using k nearest neighbor in a batch for mixup. The selection policy is trained with meta learning by minimizing the validation set loss. The approach provides consistent but small improvement over mixup on several datasets. Reviewers have also suggested discussion and comparison with more baselines, such as respective method using other (lower variant) gradient estimators (e.g., gumbel softmax), and using local input/output kernels for data selection, etc.
The scores here are bimodal.  The low scoring reviewers have problems with the evaluation, and I agree it could be improved.  The high scoring reviewers seem to mostly agree with those complaints, but think that the paper is interesting enough  to be accepted anyway.  One of the low scoring reviewers has some complaints about novelty that I don t find super convincing. The other low scoring reviewer has suggested that they d be OK with a decision of Accept.   Part of me thinks that I should reject this paper with a message of "come back later with the experiments improved", and that that would be the best thing for the field, because the paper can already be publicized on arXiv anyway.  But the other part of me thinks: what if they do that and get unlucky with a bad batch of reviews the next time (the current reviewers were great and had a really thorough discussion)? With some amount of trepidation, I m recommending accept, but *please* reward my faith in you (the authors) and make an effort to fix the things reviewers complained about before the camera ready.
The authors propose a system for asynchronous, model parallel training, suitable for dynamic neural networks.  To summarize the reviewers:  PROS: 1. Paper contrasts well with existing work. 2. Positive results on dynamic neural network problems. 3. Well written and clear  CONS: 1. Some concern about extrapolations/estimates to hardware other than that on CPU. 2. Comparisons with Dynet seem to suggest auto batching results in a dynamic mode aren t very positive.  For 1) the AC notes the author s objections to reviewer 1 s views on the value of estimation/extrapolation to non CPU hardware.  However, reviewer 3 voiced  a similar concern and  both still feel that there is more to be done to be convincing in the experiments.
This paper presents a method for curriculum learning based on extracting parallel sentences from comparable corpora (wikipedia), and continuously retraining the model based on these examples. Two reviewers pointed out that the initial version of the paper lacked references and baselines from methods of mining parallel sentences from comparable corpora such as Wikipedia. The authors have responded at length and included some of the requested baseline results. This changed one reviewer s score but has not tipped the balance strongly enough for considering this for publication. 
This paper proposes an algorithm called LightWaveS to improve the ROCKET (and mini ROCKET) algorithm for multivariate time series classification, by using wavelet scattering instead of the kernel function. More than the usual number of reviewers were invited to provide independent reviews on the paper.  A concern was raised regarding the lack of hyperparameter search in the paper. The authors responded that this was intentional to avoid overfitting the solution to the tested datasets. This response is not convincing. Note that other important reasons to vary the hyperparameter values (as commonly adopted by ML researchers) are to study the sensitivity of the proposed method to hyperparameter settings and to perform more holistic performance comparison with other methods.  Other concerns on both novelty and significance have also been raised.  Although 2 of the 7 reviews show a weak support for acceptance, other reviewers have pointed out legitimate concerns that make this paper not ready for publication in ICLR in its current form. We appreciate the authors for clarifying some points in their responses and discussions and even including further results, but addressing all the concerns raised really needs a more substantial revision of the paper. We hope the comments and suggestions made by us can help the authors prepare a revised version that will be more ready for publication.
The paper proposes a new graph based regularizer to improve the robustness of deep nets. The idea is to encourage smoothness on a graph built on the features at different layers. Experiments on CIFAR 10 show that the method provides robustness over very different types of perturbations such as adversarial examples or quantization. The reviewers raised concerns around the significance of the results, the reliance on a single dataset and the unexplained link between adversarial examples and the regularization. Despite the revision, the reviewers maintain their concerns. For this reason this work is not ready for publication.
The paper presents an original approach to replace inefficient discrete autoregressive posterior sampling by a parallel sampling procedure based on fixed point iterations reminiscent of normalizing flow, but for discrete variables. All reviewers liked the idea, and found that it was an original and promising approach. But all agreed the paper was poorly written and very unclear.  All also found the experimental section lacking, in clarity and scope.  Authors did not provide a rebuttal.  Overall a potentially really promising idea, but the paper is not yet ripe. 
Summary: The paper studies RL and bandits in the conservative setting where the performance of the new, learnt policy should never be significantly worse than that of a baseline.   Discussions: The main concern of the reviewers was about novelty, and specifically what new techniques and ideas were brought in this work compared to (Wu et al. 2016) and (Garcelon et al 2020). The authors have addressed these concerns and updated their draft accordingly. The reviewers have now all reached a consensus and recommend to accept this work.   Recommendation: Accept
This article studies the identifiability of architecture and weights of a ReLU network from the values of the computed functions, and presents an algorithm to do this. This is a very interesting problem with diverse implications. The reviewers raised concerns about the completeness of various parts of the proposed algorithm and the complexity analysis, some of which were addressed in the author s response. Another concern raised was that the experiments were limited to small networks, with a proof of concept on more realistic networks missing. The revision added experiments with MNIST. Other concerns (which in my opinion could be studied separately) include possible limitations of the approach to networks with no shared weights nor pooling. The reviewers agree that the article concerns an interesting topic that has not been studied in much detail yet. Still, the article would benefit from a more transparent presentation of the algorithm and theoretical analysis, as well as more extensive experiments. 
This paper proposes a novel method for class supervised disentangled representation learning. The method augments an autoencoder with asymmetric noise regularisation and is able to disentangled content (class) and style information from each other. The reviewers agree that the method achieves impressive empirical results and significantly outperforms the baselines. Furthermore, the authors were able to alleviate some of the initial concerns raised by the reviewers during the discussion stage by providing further experimental results and modifying the paper text. By the end of the discussion period some of the reviewers raised their scores and everyone agreed that the paper should be accepted. Hence, I am happy to recommend acceptance.
The authors present a new learning based algorithm for constructing index structure. Existing learned index algorithm use a fixed value, in contrast the authors show that a more refined methods can be used to obtain higher quality solutions for the problem.  The reviewers, after discussion, found the paper interesting and the experimental results promising but they feel that the paper in the current form is not yet ready for publication.  In particular,   in the current form the theoretical motivation and the experimental results are a bit detached  Overall, the paper is interesting and the results are promising but it probably would benefit from significant re writing before being accepted.
This paper proposes to automatically learn the form of the non linearities of neural networks in deep neural networks, which the reviewers noted to be an interesting albeit significantly studied direction.   Overall, this paper falls just below the bar, with no reviewer really willing to champion for acceptance.  Reviewer 3 found the paper to be marginally above the acceptance threshold and found the insights provided in the paper (in Section 2) to be a neat and strong contribution.  Reviewers 1 2, however, found the paper marginally below the bar and seemed confused by the presentation of the paper.  They seemed to believe in the motivation and idea, but they found the paper hard to follow and not particularly clearly written.  It would seem that the paper could significantly benefit from careful editing and restructuring to disambiguate contributions from motivation and existing literature.  Also, the authors should provide clear justification for their design choices and modeling assumptions. 
The paper proposes an entropic coding approach for sentence embedding.   Reviewers have spent good efforts in reviewing. They generally feel the problem is important/interesting, but also found it difficult to understand the paper. Thus, the authors are encouraged to thoroughly revise the paper according to the reviews provided, and another round of review is needed to better determine the merits of this paper.
This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. The paper is well written, and is overall easy to follow. The proposed algorithm is well motivated, and easy to apply. The method can be expected to perform well empirically, which the experiments verify, and to have potential impact. On the other hand, the novelty is not very high, though this paper uses these existing techniques in a different setting.
This paper presents a theoretical analysis of self attention modules, using Lipschitz conditions.  It suffers from two main weaknesses: the clarity of the presentation, and the weak experimental section.
This paper presents methods for telegraphic summarization, a task that generates extremely short summaries.  There are concerns about the utility of the task in general, and also the novelty of the modeling framework.  There is overall consensus between reviewers regarding the paper s assessment the feedback is lukewarm.
Thank you for your submission to ICLR.  The reviewers were split on this paper, with more favoring acceptance but with relatively low confidence.  After reading through the paper and reviews, I tend to agree slightly more with the more critical comments.  The paper is very much on the borderline, but ultimately 1) the rather incremental nature of the work compared to [Bhagoji, 2021], and 2) the rather small scale evaluations in the current version, which the field has largely moved on from as they often give overly optimistic impressions of certified robustness.  Ultimately a lot of the extensions (which at this point are fairly standard in most methods for deep network verification), seem like they should really be taken into account in the current paper.  For these reasons I lean slightly towards not accepting the paper in its current state.
The reviewers generally reached a consensus that the work is not quite ready for acceptance in its current form. The central concerns were about the potentially limited novelty of the method, and the fact that it was not quite clear how good the annotations needed to be (or how robust the method would be to imperfect annotations). This, combined with an evaluation scenario that is non standard and requires some guesswork to understand its difficulty, leaves one with the impression that it is not quite clear from the experiments whether the method really works well. I would recommend for the authors to improve the evaluation in the next submission.
This paper builds upon existing works to prove that learning (correlated) equilibrium can be fast, i.e., faster than \sqrt{n} even in extensive form games.  Three reviewers are rather lukewarm, and one reviewer is more positive (but seems less confident in his score). The two major criticisms is that this paper is very difficult to read and that the results might seem rather incremental with respect to the literature.  I tend to agree with both points but the paper still as merits: the reason is that extensive form games are intrinsically way harder than normal form games and they more or less all have a burden of notations. We agreed  that the authors actually did some efforts to make it fit within the page limit. but another a conference or a journal would have been better suited than ICLR.  Our final conclusion is that the result is interesting yet maybe not breathtaking for the ICLR community; we are fairly certain that another venue for this paper will be more appropriate and that it will be accepted in the near future (I can only suggest journals based on the large amount of content and notations, such as OR, MOR, or GEB   yet, conferences such as EC should be more scoped too) . It does not, unfortunately, reach the ICLR bar.
This paper is far more borderline than the review scores indicate. The authors certainly did themselves no favours by posting a response so close to the end of the discussion period, but there was sufficient time to consider the responses after this, and it is somewhat disappointing that the reviewers did not engage.  Reviewer 2 states that their only reason for not recommending acceptance is the lack of experiments on more than one KG. The authors point out they have experiments on more than one KG in the paper. From my reading, this is the case. I will consider R2 in favour of the paper in the absence of a response.  Reviewer 3 gives a fairly clear initial review which states the main reasons they do not recommend acceptance. While not an expert on the topic of GNNs, I have enough of a technical understanding to deem that the detailed response from the authors to each of the points does address these concerns. In the absence of a response from the reviewer, it is difficult to ascertain whether they would agree, but I will lean towards assuming they are satisfied.  Reviewer 1 gives a positive sounding review, with as main criticism "Overall, the work of this paper seems technically sound but I don’t find the contributions particularly surprising or novel. Along with plogicnet, there have been many extensions and applications of Gnns, and I didn’t find that the paper expands this perspective in any surprising way." This statement is simply re asserted after the author response. I find this style of review entirely inappropriate and unfair: it is not a the role of a good scientific publication to "surprise". If it is technically sound, and in an area that the reviewer admits generates interest from reviewers, vague weasel words do not a reason for rejection make.  I recommend acceptance.
This paper presents an approach that uses ASR based scores to guide masking high confident blocks for speech representation learning. As most of the reviewers mentioned, it is an incremental improvement over baseline systems with limited novelty. About the use of confidence scores which is a key factor of the method, it lacks enough discussion on its quality and sensitivity.
mnist and small picture variants are not that impressive. it is a minor extension of VAEs which also are not common in sota systems.
This paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive soft max inference scales linearly with the vocabulary size. The proposed approach is based on a clustering step combined with per cluster, smaller soft maxes. It retains differentiability with the Gumbel softmax trick. The experimental results are impressive. There are some minor flaws, however there s consensus among the reviewers the paper should be published. 
Viewing language modeling as a matrix factorization problem, the authors argue that the low rank of word embeddings used by such models limits their expressivity and show that replacing the softmax in such models with a mixture of softmaxes provides an effective way of overcoming this bottleneck. This is an interesting and well executed paper that provides potentially important insight. It would be good to at least mention prior work related to the language modeling as matrix factorization perspective (e.g. Levy & Goldberg, 2014).
The authors propose a framework for estimating "global robustness" of a neural network, defined as the expected value of "local robustness" (robustness to small perturbations) over the data distribution. The authors prove the the local robustness metric is measurable and that under this condition, derive a statistically efficient estimator. The authors use gradient based attacks to approximate local robustness in practice and report extensive experimental results across several datasets.  While the paper does make some interesting contributions, the reviewers were concerned about the following issues: 1) The measurability result, while technically important, is not surprising and does not add much insight algorithmically or statistically into the problem at hand. Outside of this, the paper does not make any significant technical contributions. 2) The paper is poorly organized and does not clearly articulate the main contributions and significance of these relative to prior work. 3) The fact that the local robustness metric is approximated via gradient based attacks makes the final results void of any guarantees, since there are no guarantees that gradient based attacks compute the worst case adversarial perturbation. This calls into question the main contribution claim of the paper on computing global robustness guarantees.  While some of the technical aspects of the reveiwers  concerns were clarified during the discussion phase, this was not sufficient to address the fundamental issues raised above.  Hence, I recommend rejection.
Four reviewers acknowledged the author s response and did not change their largely negative scores.  The one enthusiastic reviewer did not respond to the more negative reviewers and has not worked in the theorem proving area. The main problem with the paper seems to be that the reviewers were not convinced by the empirical results.  They felt that results should have been presented on more widely used benchmark datasets.
This work addresses the issue of learning reward functions that overfit less/are invariant to irrelevant features of expert demonstrations.  The proposed algorithm builds on top of adversarial imitation learning (AIRL) and proposes to include a regularization principle that is based on invariant risk minimization. The proposed algorithm is evaluated both in grid worlds as well as continuous control tasks. Both zero shot policy transfer, as well as transfer of the reward function to learn out of distribution tasks from scratch.  **Strenghts** This work is well motivated and addresses an important problem The proposed method is well motivated, and provides theoretical foundations   **Weaknesses** The manuscript had many missing details/no appendix  only one baseline is provided, while many relevant IRL algorithms exist The evaluation is very limited in actually evaluating the invariance properties of the learned reward function  poor alignment between how the proposed algorithm is motivated (learning invariant reward functions), and on what most of the experimental evaluation is focussed (zero shot transfer of policy)**(more details on this below).    **Rebuttal** the authors have updated the manuscript to include an appendix and were able to address most structural issues and provided many of the missing details.  No additional baselines were provided, and the experimental evaluation remains limited/poorly aligned with the initial motivation  **Summary** This manuscript addresses an important problem and proposes a promising algorithm. My major remaining concern is the  experimental evaluation that seems not well aligned with the main contribution of this paper. As the authors state in their rebuttal the main supporting evidence for their claim is provided in Section 5.3, with only one set of experiments on using the reward function to learn policies on OOO tasks and very little analysis (< quarter of a page). While the majority of the evaluation (Section 5.2) is focussed on zero shot transfer of the learned policy (which is trained during the IRL training phase). These zero shot transfer experiments are not motivated in the context of the "learning invariant reward functions", so it s unclear what these results show. If these results are still relevant in showing that the proposed algorithm learns "invariant rewards", then this needs to be explained. Furthermore, more baselines would have been required (e.g algorithms that are focussed on learning a good policy by learning a "pseudo" reward   such as GAIL).  Because of this, my recommendation is that this manuscript is not quite ready yet for publication.
This paper analyzes the convergence of SGD with a biased yet consistent gradient estimator. The main result is that this biased estimator results in the same convergence rate as does using unbiased ones. The main application is on learning representations on graphs (e.g., GCNs), and FastGCN is a closely related work. I agree that this paper has valuable contributions, but it can be further strengthened by considering the review comments, such as on the key assumptions.
The paper evaluates several different strategies for labeling of missing data, and recommend the best strategy in practice based on the empirical results on six data sets.  The reviewers agree that empirical evaluation is important for providing a good guideline for this practical problem. The concerns of the reviewers include the lack of motivation on the chosen strategies, the lack of novelty (the tools in the strategies are all pretty standard in the literature), the lack of reproducibility (by referring to the authors  own anonymous work for parameters), and the lack of breadth (e.g #data sets) and depth (e.g. metrics explored) in the experiments. 
While some of the reviewers find that the paper proposes a solid contribution to a problem, I will tend to agree with other ones that the proposed approach has limited novelty and limited potential for improvement over baselines. In addition,  simulations are pretty weak due to lack of comparisons to strong baselines and to lack of clarity.
Three reviewers recommended rejection and there was no rebuttal to overturn their recommendation.
I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.  A paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:  1. Show that the errors found can be used to meaningfully improve the models.   This requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).  2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non obvious to a researcher in the field.  This is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non obvious and it seems to work fine, making the Gumbel method unnecessary.  3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.  I do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).  4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models  Given that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326 However, I believe the paper would need to be rethought and rewritten to make this sort of contribution.   Ultimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea. 
This paper explores the use of sequential information to improve imitation learning, essentially using recurrent networks (LSTM) instead of a simple NN in several existing imitation learning models (BC, GAIL, etc.). On the positive side, the empirical results are good, showing improvement in terms of attained rewards, convergence speed and stability. There are however some significant issues with the way the way the approach is motivated and positioned with respect to existsing work. In particular, the issue described in the paper is due to the fact they consider POMDPs (not MDPs): this should have been more clearly explained. There are also issues with the Related Work section. For these reasons, the paper is not quite ready for publication. 
This paper provides a privacy preserving method to boost the sample quality after training a GAN. The reviewers were unanimous that this paper should be presented at ICLR, with an important contribution to privacy preserving GANs.
The authors propose using a noisy channel formulation which allows them to combine a sentence level target source translation model with a language model trained over target side document level information. They use reranking of a 50 best list generated by a standard Transformer model for forward translation and show reasonably strong results.  The reviewers were concerned about the efficiency of this approach and the limited novelty as compared to the sentence level noisy channel research Yu et al. 2017. The authors responded in depth, adding results with another baseline which includes backtranslated data. I feel that although this paper is interesting, it is not compelling enough for inclusion in ICLR. 
The paper provides a novel attack method and contributes to evaluating the robustness of neural networks with recently proposed defenses. The evaluation is convincing overall and the authors have answered most questions from the reviewers. We recommend acceptance. 
This paper presents the Order Memory Policy Network (OMPN), an architecture for modelling a hierarchy of sub tasks and discovering task decompositions from demonstration data. Results are presented on a compositional grid world task (Craft) and on a simulated robotics task (Dial).  The reviewers agree that the proposed method is novel and interesting, that the paper addresses an important problem, and that it is well written. One main criticism by the reviewers, the lack of experimental evaluation of different hyperparameter choices, such as the depth of the memory stack and the expected number of subtasks, has to a large part already been addressed in the revision by the authors. The total number of hyperparameters that need to be tuned, however, is quite large and the authors are encouraged to revise their claim "Our central message is that OMPN is a general off the shelf model for task decompositions" in this light. The paper is borderline, and could clearly benefit from a revised, stronger presentation and more extensive experimental evaluation, but I am confident that the authors can use the time until the camera ready version is due to address some of the remaining feedback by the reviewers, and hence I think that this paper can be accepted.  The authors are further encouraged to take the following additional reviewer feedback into account, which was brought up during the internal discussion period: 1) The complexity of the proposed method could be better justified by more thoroughly investigating the effectiveness of using a multi level hierarchy (e.g., by running experiments on more complicated and hierarchical tasks with multiple branches). 2) Further strengthening down stream performance evaluation, such as in imitation learning (in addition to the already presented behavioral cloning results) and/or reinforcement learning, would further strengthen the paper and demonstrate that the discovered decomposition is indeed useful. 
The authors describe an approach to modeling data via an implicit representation that lives on a union of linear subspaces.  While the reviewers consider the authors  approach as novel and having potential, they (and myself) consider that the exposition and notation could be improved, and that the paper as it is is hard to understand and contextualize.
The authors propose a methodoloy for dynamic feature selection. They use differentiable gates with  an RNN architecture to select different subsets of features at each time point thus resulting in dynamic selection.  The reviewers agree that the idea is interesting and the method could be useful and I share their opinion.  The majority vote is towards rejection. The overarching mwssage of the reviews is that the manuscript raises confusion in a number of points. I see this work as one with good potential for impact but its current presentation is confusing. The vivid discussion that it raised is also an indication of it. The authors have done a good job replying to the concerns and the questions raised. However, the reviewers were still unsatisfied with the authors response to their concerns.  I recommend rejection at this time, while encouraging the authors to take seriously the reviewers  requests for a clearer presentation of their approach s contribution in order to strengthen their paper for future submission.  
This paper tackles the problem of detecting out of distribution (OoD) samples. To this end, the authors propose a new approach based on typical sets, i.e. sets of samples whose expected log likelihood approximate the model s entropy. The idea is then to rely on statistical testing using the empirical distribution of model likelihoods in order to determine whether samples lie in the typical set of the considered model. Experiments are provided where the proposed approach show competitive performance on MNIST and natural image tasks.  This work has major drawbacks: novelty, theoretical soundness, and robustness in settings with model misspecification. Using the typicality notion has already been explored in Choi. et al. 2019 (for flow based model), which dampers the novelty of this work. The conditions under which the typicality notion can be used are also not clear, e.g. in the small data regime. Finally, the current experiments are lacking a characterization of robustness to model misspecification. Given these limitations, I recommend to reject this paper. 
In an attempt to understand generalization, this paper aims at understanding the dynamics of functions presented by the network for different images in the training set. Authors look at activation patterns (whether a ReLU activation is on or off) as a way of characterizing the active paths in the network and approximating the function presented by the network for each image. Authors study different related statics (eg. correlation) and how they evolve during training including.  Pros:    Understanding the dynamics of training, how diversity is encouraged by the training procedure and its relationship to generalization is an important problem.   This paper takes an empirical approach and tries to make interesting empirical observations about the dynamics of the training.  Cons:   The paper is poorly written in terms of structure, making clear arguments with enough evidence, notation, etc.   Some empirical trends are shown but their connections to the main claim of the paper about generalization is very weak. The main attempt to connect the observations to generalization is Fig. 7 which shows model accuracy correlated with the ratio of early to mid overlap. This is problematic both because it only has 6 data points and also because a simple correlation analysis is not enough to establish this claim which is more about the cause of generalization.  Reviewers have pointed to various concerns including but not limited to clarity of the paper, lack of rigorous arguments, not providing enough evidence for the arguments, etc. Unfortunately, authors did not participate in the discussion period.  Given the above concerns, I recommend rejecting the paper.
This paper proposes to improve MT with a specialized encoder component that models roles. It shows some improvements in low resource scenarios.  Overall, reviewers felt there were two issues with the paper: clarity of description of the contribution, and also the fact that the method itself was not seeing large empirical gains. On top of this, the method adds some additional complexity on top of the original model.  Given that no reviewer was strongly in favor of the paper, I am not going to recommend acceptance at this time.
This paper proposes to use data driven deep convolutional architectures for modeling advection diffusion. It is well motivated and comes with convincing numerical experiments. Reviewers agreed that this is a worthy contribution to ICLR with the potential to trigger further research in the interplay between deep learning and physics. 
The paper proposes a new method to approximate the nonlinear value function by estimating it as a sum of linear and nonlinear terms. The nonlinear term is updated much slower than the linear term, and the paper proposes to use a  fast least square algorithm to update the linear term. Convergence results are also discussed and empirical evidence is provided.   As reviewers have pointed out, the novelty of the paper is limited, but the ideas are interesting and could be useful for the community. I strongly recommend taking reviewers comments into account for the camera ready and also add a discussion on the relationship with the existing work.   Overall, I think this paper is interesting and I recommend acceptance. 
This paper proposes an attention mechanism that works at the phrase level for semantic parsing. Reviewrs agree that the idea has been previously explored outside semantic parsing, that the gains should be shown on less saturated datasets, and that there are issues in the experimental design (observing test set results for many experiments). Thus, at this point I recommend that the paper is rejected.
## A Brief Summary Recent works in deep learning have shown that it is possible to solve [[combinatorial optimization]] problems (COP) with neural networks.  However, generalization beyond the examples seen in the training set is still challenging, e.g., generalizing to TSP with more cities than the ones seen in the training set. This paper proposes the GANCO approach, where a separate generative neural network based on GAN generates new hard to solve training instances for the optimizer. The optimizer and the generative network are trained in an alternating fashion. The authors have run experiments with the attention model (AM) and POMO with their GAN based data augmentation approach. The authors provide experimental results on several well known COPs, such as the traveling salesman problem.  ## Reviewers  Feedback  Below, I will summarize some reviewers  feedback and would like the authors to address the cons noted below. ### Reviewer sEuD  *Pros:*   Paper is well written.   Task is important and well motivated.   Good experimental results. *Cons:*   The paper s core contribution on the necessity of adversarial entities is not well motivated.   Missing baselines:   RL agent trained on all target distributions. To figure out how far GANCO is from the optimal policy.   The performance of an agent trained on a curriculum.   Figure 2 is unnecessary/redundant in the paper.    ### Reviewer tjCH *Cons:*   The paper is reasonably written. However, it would be much easier to follow with a few changes. For example, section 3.1 explains the architecture and, in related works, a more comprehensive overview of the methods to improve the robustness of RL methods.   It is widely known that data augmentation helps in deep learning. The paper s claims would be more convincing if it provided some crucial baselines, such as comparing different data augmentations methods and carefully ablating them.    ### Reviewer N945 *Pros:*   Well written   Good evaluation   Simple model with good results *Cons:*   Missing citation to the PAIRED paper.   How important are the adversarial entities generated? Is it possible to achieve similar results by just training on more samples?   Missing baselines: Instead of training in stages, alternate optimizer and generator network per step basis.  ### Reviewer mumN *Pros:*   The proposed approach is novel.   Comprehensive and extensive experiments.   Figure 1 provides a good summary of the approach.  *Cons:*   Motivation is for the GANCO is not very convincing.   Concerns about the capacity of the neural nets used in the paper.   Concerns on forgetting the original distribution.   Concerns about experimental evaluation protocol.   Including experiments on routing problems to show the generality of the proposed approach.   Request for improvements in the writing and the formatting of the paper.  ## Key Takeaways and Thoughts I think this paper attacks an interesting problem. As far as I am aware of the approach is novel. However, generative adversarial networks have been used in the machine learning literature for data augmentation and RL for augmenting the environment (see the PAIRED paper.) GAN type of approaches hasn t been used to improve the generalization of the deep learning approaches for COP. The results look promising. However, as pointed out by Reviewer mumN and tjCH, this paper would benefit more from further ablations, particularly the necessity of adversarial generation part to make the arguments more convincing. As it stands now, it is not clear where exactly the improvements are coming from. Reviewer mumN also raised some concerns about the poorly configured LHK3 baseline in the discussion period. Furthermore, I agree with the reviewer mumN and tjcH that this paper would benefit from restructuring to make it flow better. I do think that this paper needs another round of reviews. I would recommend the authors go over the feedback provided here and address them for future submission.## References
This paper studies gradient descent with weight decay and momentum for scale invariant networks. While this is an interesting research direction, the clarity of the paper suffers from poor writing. In its current form, I doubt this paper would have a large impact in the community. In addition, some reviewers pointed out that the analysis is not rigorous (some steps are missing, the authors re use some unjustified steps from previous papers, ...). The authors claim these steps are obvious, I certainly don t think that s the case and at the very least, one would expect that detailed and rigorous derivations in the appendix if the lack of space is the real issue in the main paper.  I think the feedback provided by the reviewers is valuable and I strongly encourage the authors to take advantage of this feedback to improve their work and submit to another venue.  
This paper received all negative reviewers, and the scores were kept after the rebuttal. The authors are encouraged to submit their work to a computer vision conference where this kind of work may be more appreciated. Furthermore, including stronger baselines such as Acuna et al is recommended.
This work tackles sparse or delayed reward problem in reinforcement learning. The key idea is to build a classifier to detect states that will lead to high rewards in the future and provide a bonus to those states. All the reviewers liked the idea but had issues with the execution of empirical results. The approach is evaluated only in a few Atari games skipping many sparse reward games and missing comparison to many exploration baselines. Furthermore, many reviewers found the writing confusing and hard to follow. The authors provided the rebuttal and addressed some of the concerns. However, upon discussion post rebuttal, the reviewers decided to maintain their score. Reviewers believe that the paper will immensely benefit with improved writing, evaluation on all atari games, and comparison to exploration baselines. Please refer to the reviews for final feedback and suggestions to strengthen the future submission.
The paper aims at developing mechanisms for adversarial attack and defense towards combinatorial optimization solvers, where the solver is treated as a black box function and the original problem’s underlying graph structure is attacked under a given budget. While the reviewers found the problem novel and interesting, they are not convinced by the problem formulation and the proposed solutions, as well as the experimental setup. Some of the points that the reviewers brought up during the discussion include: (i) the attack to the TSP does not follow the main paper s attack principle of adding and deleting edges, (ii), in general, it has not been explained why all these modification are really "relaxations", (iii) the notations are very confusing, and (iv) while authors  response on loosening the constraints makes sense, but the experiments (i.e., the TSP problem setting) in this work are not consistent with such clarification. Addressing the above points will significantly improve the manuscript.
This paper adapts the ideas around universal successor features for decentralised multi agent environments, with a particular emphasis on deriving better exploration from them. Like most of the reviewers, I think this is indeed a promising research direction. Given the complexity of the endeavour however, it may take a few more steps until the empirical evidence can back up the authors  ambition: the reviewers  consensus on the current version of the paper is that it is not ready for publication yet.
In this work, a central idea introduced by CycleGAN is extended from 2D convolutions to 3D convolutions to ensure better consistency of style transfer across time. Authors demonstrate improvements on a variety of datasets in comparison to frame by frame style transfer.   Reviewer Pros: + Seems to be effective at enforcing improved consistency over time + Proposed medical dataset may be good contribution to community.  + Good quality evaluation  Reviewer Cons:   All reviewers felt the technical novelty was low.   Some questions arose around quantitative results, left unanswered by authors.   Experiments missing some baseline approaches   Architecture limited to fixed length video segments  Reviewer consensus is to reject. Authors are encouraged to continue their work and take into account suggestions made by reviewers, including adding additional comparison baselines 
This paper investigates the practical and theoretical consequences of speeding up training using incremental gradient methods (such as stochastic descent) by calculating the gradients with respect to a specifically chosen sparse subset of data.   The reviewers were quite split on the paper.   On the one hand, there was a general excitement about the direction of the paper. The idea of speeding up gradient descent is of course hugely relevant to the current machine learning landscape. The approach was also considered novel, and the paper well written.   However, the reviewers also pointed out multiple shortcomings. The experimental section was deemed to lack clarity and baselines.  The results on standard dataset were very different from expected, causing worry about the reliability, although this has partially been addressed in additional experiments. The applicability to deep learning and large dataset, as well as the significance of time saved by using this method, were other worries.  Unfortunately, I have to agree with the majority of the reviewers that the idea is fascinating, but that more work is required for acceptance to ICLR.  
This paper proposes a novel method for learning Hamiltonian dynamics from data. The data is obtained from systems subjected to an external control signal. The authors show the utility of their method for subsequent improved control in a reinforcement learning setting. The paper is well written, the method is derived from first principles, and the experimental validation is solid. The authors were also able to take into account the reviewers’ feedback and further improve their paper during the discussion period. Overall all of the reviewers agree that this is a great contribution to the field and hence I am happy to recommend acceptance.
This paper presents the problem of robust inaccuracy (model predictions being robust to perturbations but inaccurate on datapoints), and present methods to maximize robustness while avoiding robust inaccuracy. Furthermore, they develop an abstention mechanism based on robustness to prevent prediction on points where the model is not robust. Results show improvement in adversarial robustness to standard attacks with only small reduction in natural accuracy.   Reviewers were mixed on the clarity and importance of this submission. A major concern raised was on the importance of robust inaccuracy, motivation for avoiding it, and novelty of the proposed method. Other abstention mechanisms are available and one does not solely need to rely on robustness. Additionally, results are often presented on a pareto front and the method does not strictly dominate prior approaches. Authors addressed many of the clarity concerns in their updated revision, and reviewers commented on the high quality of analysis performed in the experiments. But several reviewers still found the draft and description of the robust inaccuracy problem insufficiently motivated and the methodology not well explained. Given lingering concerns over clarity and motivation (in spite of a revision that exceeds the page limit), I cannot recommend this paper for acceptance.
As an empirical paper, this paper studies uncertainty estimations with respect to various architectures and learning schemes. Three reviewers suggested acceptance based on the strength of the paper (fairly extensive experiments were conducted, and some new observations were discovered, such as the superiority of ViT). On the other hand, two reviewers proposed rejection due to lack of rigor in writing and lack of novelty. No consensus was reached through additional discussion. In particular, the reviewer s point that the experiment was not well controlled different models were trained with different hyperparameters etc  seems quite important, and it weakens the significance of the contribution of the paper.   All reviewers agreed that it is a potentially interesting and important paper. I encourage the authors to resubmit in the future after carefully addressing the reviewers  concerns.
Introducing an adversarial agent that re configures the rendered scenes of CLEVR to demonstrate that models that appear achieve super human performance are actually easily fooled due to their lack of ability to reason, provides a nice insight into limitations with existing approaches and correspondingly how we evaluate on some benchmarks.  There is a persistent concern that the results are only on CLEVR, and that the adversarial examples are not really disproving reasoning but rather issues with vision.  However, overall reviewers were generally positive about the aims the work.
The paper introduces some good ideas, but I don t think it is quite there in terms of a method to be recommended for publications. I think it is mostly reasonably written (I do not agree with the comment of a  complete rewrite ) but there are indeed some passages for improvement (for instance, an equation as y   σ−1[Q0(t, x)] + εH(t, x)], Section 2, needs comments, as the left hand side is discrete and the right hand side is continuous, unbounded).  My main concern is the disregard for identification. Some citations are unclear (the second to last paragraph in Section 4 cites a few papers in identification that have little to do with the problem here, which is proxy based. The papers cited don t even mention latent variables at all). As stated, the split in three sets of variables as suggested by Figure 1 is just an idealization: there is no reason at all they can be identified, and actually the theory where just Zc is considered impose a lot of restrictions on when we can possible identify Zc (see e.g., Miao et al. 2018, Biometrika, https://arxiv.org/pdf/1609.08816.pdf ). I know that some papers like Louizos et al. play fast and loose with identification too, but at least their Z_c structure they aim at has been studied elsewhere (like the Miao et al. paper), while here, like the Zhang et al. paper cited, may be leading researchers to an unfruitful path. This, combined with the relative modesty of the novelty, is the primary reason for my recommendation. I do think the paper can be improved in a productive way by investigating it from the point of view of either i) the theoretical justification for identification; ii) or from a more empirical direction with much experimentation on the different ways the structured latent space is capturing confounding (the target learning aspect of it is pretty much orthogonal to this).
This paper presents an extension of an existing topic model, DocNADE. Compared to DocNADE and other existing bag of word topic models, the primary contribution of this work is to integrate neural language models into the topic model in order to address two limitations of the bag of word topic models: expressiveness and interpretability. In addtion, the paper presents an approach to integrate external knowledge into the neural topic models to address the empirical challenges of the application scenarios where there might be only a small training corpus or limited context available.   Pros:  The paper presents strong and extensive empirical results. The authors went above and beyond to strengthen their paper during the rebuttal and address all the reviewers  questions and suggestions (e.g., the submitted version had 7 baselines, and the revised version has 6 additional baselines per reviewers  requests).  Cons: The paper builds on an earlier paper that introduced the DocNADE model. Thus, the modeling contribution is relatively marginal. On the other hand, the extended model, albeit based on a relatively simple idea, is still new and demonstrates strong empirical results.  Verdict: Probably accept. While not groundbreaking, the proposed model is new and the empirical results are strong. 
After a bit of discussion, all reviewers are for accepting the paper.  Strengths: + Clarity (agreed by R4, R2, R1). The paper is easy to read and follow the core contributions. R3 had a concern about the correctness of a derivation, which was resolved in the discussion. + The work solves a core problem of generative models over multimodal applications, building on prior work with mixture and product of expert models. As R1 notes: "By combining MVAE and MMVAE under one framework, this may provide insights to researchers in this area." + On the datasets studied, the details for reproducibility are transparent, and multiple metrics and uncertainty over the metric results are reported.  Weaknesses: + Multi modality of the benchmarks. The experiments evaluate on MNIST SVHN, "PolyMNIST", and CelebA. The latter two benchmarks are fairly arguable in whether they re really multimodal as R4 notes: e.g., CelebA has two "modalities" of image and attribute pairs. It seems arguable whether you even need multimodal approaches. + Scale of the benchmarks. Language models (especially with Transformer architectures) have been studied quite a bit over multiple modalities, and these works scale significantly better applying simple strategies. It remains to be seen empirically what the utility of multimodal latent variable models really are.
The paper proposes learning of 3D object representation from images. The pretraining used assumes it can generate implicit 3D models for the objects, and then objects are detected in multi object scenes without further supervision. Reviewers raised concerns regarding experiments being conducted only on synthetic data. Authors are encouraged to try out their approach on real data, to demonstrate the benefits of their solution.
The reviewers point out several important issues to be addressed, including comparing to other methods that can address the "combinatorial generalization" problems studied (one reviewer points out the crucial difference from "compositional generalization" studied before), addressing the gap between the proposed dataset (simple and has the value of diagnosing/model debug/research algorithm development) and real datasets/problem settings.     As such the AC recommends Reject and encourages the authors to take the constructive feedback to improve. 
I enjoyed reading the paper myself and agree with some of the criticisms raised by the reviewers, but not all of them. In particular, I don t think it s a major issues that this work studies an explicit regularization scheme BECAUSE the state of our understanding of generalization in deep learning is so embarrassingly poor!!   Unlike a lot of work, this work is engaging with the *approximation* error and developing risk bounds (called "generalization error" here ... not my favorite term for the risk!) rather than just controlling the generalization gap. The simple proof in the bounded noiseless case was nice to see.  On the other hand, not being familiar with the work of Klusowski and Barron (2016), I m not willing to overrule the reviewers on judgments that this work is not novel enough. I would suggest the authors take control of this and paint a more detailed picture of how these two bodies of work relate, including how the proof techniques and arguments overlap.  Some other comments:   1. your theorem requires \lambda > 4, but then you re using \lambda   0.1. this seems problematic to me.  2. your "nonvacuous upper bound" is path norm/sqrt(n) ... but do the numbers in the table include the constants? looking at the constants that are likely to show up, (4Bn sqrt(2 log 2d), they are easily contributing a factor greater than 10 which would make these bounds vacuous as well.  you need to explain how you are calculating these numbers more carefully.  3. several times Arora et al and Neyshabur et al are cited when reference is being made to numerical experiments to show that existing bounds are vacuously large. But Dziugaite and Roy, who you cite for the term "nonvacuous", made an earlier analysis of path norm bounds in their appendix and point out that they are vacuous.   4. the paper does not really engage with the fact that you are unlikely to be exactly minimizing the functional J. any hope of bridging this gap?  5. the experiments in general are a bit too vaguely described. also, you control squared error but then only report classification error. would be interested to see both.
The authors make an experimental case that dropout aids generalization by promoting "flatter minima".  The reviewers felt that the work reported in this paper makes a useful step forward on a question of central interest.  The consensus view was that the total weight of evidence presented was not sufficient for publication in ICLR.  The paper could be strengthened was more extensive and varied experiments and/or theoretical analysis.
This paper introduces a method for building interpretable classifiers, along with a measure of "concept accuracy" to evaluate interpretability, and primarily applies this method to text models, but includes a proof of concept on images in the appendix.  The main contributions are sensible enough, but the main problems the reviewers had were: A) The performance of the proposed method B) The lack of human evaluation of interpretability, and  C) Lack of background and connections to other work.  The authors improved the paper considerably during the rebuttal period, and might have addressed point C) satisfactorily, but only after several back and forths, and at this point it s too late to re evaluate the paper.  I expect that a more polished version of this paper would be acceptable in a future conference.  I mostly ignored R1 s review as they didn t seem to put much thought into their review and didn t respond to requests for clarifications.
The authors propose a distillation based approach that is applied to transfer knowledge from a classification network to non classification tasks (face alignment and verification). The writing is very imprecise   for instance repeatedly referring to a  simple trick  rather than actually defining the procedure   and the method is described in very task specific ways that make it hard to understand how or whether it would generalize to other problems.
meta score: 5  This paper gives a thorough experimental comparison of convolutional vs recurrent networks for a variety of sequence modelling tasks.  The experimentation is thorough, but the main point of the paper,  that convolutional networks are unjustly ignored for sequence modelling, is overstated as there are several areas where convolutional networks are well explored. Pros:  clear and well written  thorough set of experiments Cons  original contribution is not strong  it is not as radical to consider convolutional networks for sequence modeling as the authors seem to suggest 
The authors derive and experiment with quaternion based recurrent neural networks, and demonstrate their effectiveness on speech recognition tasks (TIMIT and WSJ), where the authors demonstrate that the proposed models can achieve the same accuracy with fewer parameters than conventional models. The reviewers were unanimous in recommending that the paper be accepted.
The reviewers of this paper unanimously agreed that this paper adds an interesting theoretical and practical discussion to discrete flows. The paper has improved from the first version to the final one, in which the comments and suggestions by the reviewers have been followed.   The paper is still incremental with respect to the previous paper and the reviewers all recommended a poster presentation.
This paper examines the correspondence between topological similarity of languages (correlation between the message space and object space) and ability to learn quickly in a situation of emergent communication between agents.  While this paper is not without issues, it does seem to present a nice contribution that all of the reviewers appreciated to some extent. I think it will spark further discussions in this area, and thus can recommend it for acceptance.
The paper analyses several approaches to pruning at initialization, compared to after training. There was a large gap in reviewers appreciation of the paper, but I think that the pros outdo the cons as the paper show a lot of insights overall. I recommend accepting the paper.
The authors propose a new type of compositional embedding (with two proposed variants) for performing tasks that involve set relationships between examples (say, images) containing sets of classes (say, objects).  The setting is new and the reviewers are mostly in agreement (after discussion and revision) that the approach is interesting and the results encouraging.  There is some concern, however, that the task setup may be too contrived, and that in any real task there could be a more obvious baseline that would do better.  For example, one task setup requires that examples be represented via embeddings, and no reference can be made to the original inputs; this is justified in a setting where space is a constraint, but the combination of this setting with the specific set query tasks considered seems quite rare.  The paper may be an example of a hammer in search of a nail.  The ideas are interesting and the paper is written well, and so the authors can hopefully refine the proposed class of problems toward more practical settings.
All three reviewers expressed concerns about the writing of the paper. The AC thus recommends "revise and resubmit".
The authors propose a conditional normalizing flow approach to learning likelihoods. While reviewers appreciated the paper, in its present form it lacked a clear champion, and there were still some remaining concerns about novelty and clarity of presentation. The authors are encouraged to continue with this work and to account for reviewer comments in future revisions. Following up on the author response, a reviewer adds: "Thanks for your clarification. I still disagree that the conditional flow architecture proposed should be considered as a novel contribution. The reason why I mentioned [1] or [2] was not because they follow the exact setting (coupling based conditional flow model) discussed in this paper. I wanted to highlight that the idea to use conditioning variables as an input to the transforming network (whether it is an autoregressive density function, autoregressive transforming network, or coupling layers) is quite universal (as we all know many of the existing codes implementing flow based models includes additional keyword arguments  context  to model conditioning). I m not sure why the fact that the proposed framework is conditioning on high dimensional variables makes a contribution. There seems to be no particular challenge in doing that and novel design choices to circumvent that (i.e., we can just use existing architectures with minor modifications).  I agree that the binary dequantization should be considered as a contribution, but as significant as to change my decision to accept. Thanks for the clarification on experiments. Considering this, I raise my rating to weak reject...  Another previous work I forgot to mention in the initial review is "Structured output learning with the conditional generative flow", Lu and Huang 2019, ICML 2019 invertible neural network workshop. This paper discusses the conditional flow based on a similar idea, and attacks high dimensional structured output prediction. I think this should be cited in the paper." 
Description of paper content:  A mixed theoretical and experimental paper that investigates the robustness of distributional RL to perturbations of state observations as compared to expectation based value function learning. They provide sufficient conditions for TD’s convergence and prove the Lipschitz continuity of the loss of a histogram based KL version of distributional RL with respect to the state features, whereas this is not true for expected RL. This continuity indicates a certain robustness of the loss with respect to perturbations of the state. The theory’s tie to experiment is weak in the sense that it is not predictive of the actual performance of any algorithm. The theoretical methods are based on a previously published paper SA MDP.  Summary of paper discussion: The reviewers raised concerns about the statistical significance of the experimental results, the clarity and organization of the writing, the novelty of the theoretical setting, and its usefulness for describing a real problem setting. The majority of reviewers rejected the paper and did not lift the scores after the rebuttal.   (I personally wonder if the community would not benefit from conducting some of these kinds of theoretical analyses and experiments on LQR systems rather than Atari (etc.) environments.)
PAPER: This paper presents analysis of cross modal interactions in multimodal models and propose a method to help balance the multimodal learning process. The cross modal analysis is based on measures related to conditional utilization rate and the proposed approach is related to conditional learning speed. DISCUSSION: The reviewers showed support for this line of research, as a way to better understand the learning process for multimodal models. The discussion allowed to identify points that needed to be clarify and concerns about the experimental results. The authors addressed many of these issues in their response. All reviewers took the time to read these responses as well as other reviews. There are many reviewers who are still expressing concerns with the experimental results.  SUMMARY: This is an important line of research, and the authors should continue this research endeavor. While the paper presents some interesting research hypotheses about multimodal learning, it seems that more experiments are needed to properly address these hypotheses. In its current form, the paper may not yet be ready for publication.
The authors propose a sample reweighting scheme that helps to learn a simple model with similar performance as a more complex one. The authors contained critical errors in their original submission and the paper seems to lack in terms of originality and novelty of the proposed method.
This paper describes a new library for forward propagation of binary CNNs. R1 for clarification on the contributions and novelty, which the authors provided. They subsequently updated their score. I think that optimized code with permissive licensing (as R2 points out) benefits the community. The paper will benefit those who decide to work with the library.
By the scores, this submission is quite borderline. This paper introduces stochastic weight averaging into a few shot learning setting, The reviewers all agreed the work was sound; discussion after the author response focused on the theoretical justifications, degree of novelty and potential impact, and the empirical support.   The primary concerns were that the work was slightly too incremental to obviously merit publication at this stage: though the empirical results were sound, they mostly follow the existing observation that SWA tends to be beneficial for generalization in other settings; apparently in few shot learning as well. The positives would be that this is simple enough that it could become a general "best practice" in few shot learning baselines, and as such communicating this is important.  The other discussion focused around the theoretical justification relating SWA to low rank solutions. While empirically it does seem that the solutions found by SWA lead to low rank representations, this is not really adequately explored, and it s not clear enough why this should be expected to happen. I think if this relationship between SWA and low rank representations were more clearly explored then the paper would be a strong accept.  As it stands, it is quite borderline. Based on the scores (5,5,6), the recommendation is to reject, but it certainly could be included as well, as it has solid execution and is a clear topical fit for ICLR.
The paper provides an "asynchronous" method for multi agent actor critic with macro actions. A major contribution of this paper is the integration of the macro action value from the Q value based macro action MARL method into multi agent policy gradient. Although it appears an interesting contribution, reviewers found that several parts of the paper were not clear enough and there is a lack of fair comparison with previous works.
The paper proposes a novel differential way to output brush strokes, taking a few ideas from model based learning. The method is efficient in that one can train it in an unsupervised manner and does not require paired data. The strengths of the paper are the qualitative results that demonstrate nice interpolations among other things, on a number of datasets (esp. post rebuttal).  The weaknesses of the paper are the writing (which I think is relatively easy to improve if the authors make an honest effort) and some of the quantitative evaluation. I would encourage the authors to get in touch with the SPIRAL paper authors in order to get access to the SPIRAL generated MNIST test data and then perhaps the classification metric could be updated.  In summary, from the discussion, the major points of contention were the somewhat lacking initial evaluation (which was fixed to a large extent) and the quality of writing (which could be fixed more). I believe the submission is genuinely novel, interesting (esp. the usage of world model like techniques) and valuable for the ICLR audience so I recommend acceptance.
Paper received reviews of A, WA, WR. AC has carefully read all reviews/responses. R1 is less experienced in this area. AC sides with R2,R3 and feels paper should be accepted. Interesting topic and interesting problem. Authors are encouraged to strengthen experiments in final version. 
This paper on  anarchic  federated learning (FL) envisions an FL framework where edge clients can act independently instead of their participation being controlled by a central server. The idea is certainly promising, however, the reviewers pointed out the following main issues: 1) Technical gaps in the theoretical analysis need to be addressed 2) Bounded delay assumptions are too strong and are mismatching with the  anarchic  goal of the framework 3) The linear speed up claim should be better explained and justified. The paper generated lots of post rebuttal discussions. However, the concerns about the theoretical analysis still remain, and therefore I recommend rejection. I hope the authors will take these constructive comments into account when revising the paper.
The reviewers, including me, agreed that considering sampling diversity is interesting and reasonable when designing GNNs. However, the proposed method is too heuristic and empirical. Without the authors  feedback, I tend to reject this work.
This work considers the problem of how to predict on sensitive user points while preserving their privacy. It proposes a fairly straightforward way to create a local randomizer that optimizes loss for a given model subject to preserving LDP. The work also gives theoretical analysis of the randomizer for least squares linear regression.  The problem formulation is different from the standard LDP framework where privacy of training data points needs to be preserved. The submission does not motivate this setting and I don t see a good motivation for this problem either. More importantly, it does not sufficiently emphasize that the problem is entirely different from prior work. Indeed all reviewers were confused about various aspects of comparison with previous work. Therefore, in my opinion, the submission is not sufficiently well motivated and clearly presented to be accepted.
The majority of reviewers suggest rejection, pointing to concerns about design and novelty. Perhaps the most concerning part to me was the consistent lack of expertise in the applied area. This could be random bad luck draw of reviewers, but more likely the paper is not positioned well in the ICLR literature. This means that either it was submitted to the wrong venue, or that the exposition needs to be improved so that the paper is approachable by a larger part of the ICLR community. Since this is not currently true, I suggest that the authors work on a revision.
All authors agree the paper is well written, and there is a good consensus on acceptance.  The last reviewer was concerned about a lack of diversity in datasets, but this was addressed in the rebuttal.
The paper proposes to learn embeddings into complex hyperbolic space. This is an extension of the popular hyperbolic space embeddings which have shown success on graph like and tree like data. Reviews and discussion mostly centered around the lack of clear motivation for the work (why complex hyperbolic spaces?) and the lack of a clear advantage over other manifold embedding methods that have varying curvature. The reviewers mentioned many questions and points that they thought the work should cover. There was also concern about the baselines against which the method was compared. There was not a consensus that the paper should be accepted, and no reviewer argued strongly for acceptance, even after the author response. As a result, I recommend that this paper not be accepted at this time. I expect a new version of this paper, incorporating this reviewer feedback and especially improving the explanation of the motivation, will be a good submission for a future conference.
Reviewers agreed that taking into account the secondary structure in addition to the amino acid sequence, although not new in bioinformatics, may be a good idea in the context of deep generative models of peptides. On the other hand, all reviewers also agreed that the experimental results do not allow concluding about the potential benefit of the method, i.e., whether it is likely to produce potent AMPs (and whether it does it better than existing methods). Indeed, the proposed computational criteria can not replace a proper experimental validation, and it is not clear whether a "better method" on the computational criteria will be "better" in the real world. Second, the results on the computational criteria are not convincing: regarding the physical properties, it remains debatable to claim that a method is good if it outputs many AMPs that fulfill the criterion, while less than 7% of the true AMPs do; and regarding the computational prediction of being an AMP, the proposed method is outperformed by existing ones. In conclusion, we consider that the paper is not ready for publication at ICLR, since there is no significant methodological novelty nor significant experimental results if this is an application paper, and we encourage the authors to consider a publication with wet lab experiments to demonstrate the relevance of the method.
The authors provide an empirical study of the recent 3 head architecture applied to AlphaZero style learning. They thoroughly evaluate this approach using the game Hex as a test domain.  Initially, reviewers were concerned about how well the hyper parameters for tuned for different methods. The authors did a commendable job addressing the reviewers concerns in their revision. However, the reviewers agreed that with the additional results showing the gap between the 2 headed architecture and the three headed architecture narrowed, the focus of the paper has changed substantially from the initial version. They suggest that a substantial rewrite of the paper would make the most sense before publication.  As a result, at this time, I m going to recommend rejection, but I encourage the authors to incorporate the reviewers feedback. I believe this paper has the potential to be a strong submission in the future.  
This paper introduces a novel quality diversity algorithm, "Evolutionary Diversity Optimization with Clustering based Selection (EDO CS)", and applies it to reinforcement learning. A bandit approach (UCB) is used to select which cluster to sample parents from. The QD algorithm can be evaluated on its own, outside of the RL context, and if so it should be compared to the several approaches to niching and other standard diversity preservation approaches in evolutionary computation that rely on clustering. (And the authors should make an effort to connect to the niching literature in particular.) However, the use of the algorithm for RL makes it possible to use behavioral features as the space in which to cluster, separating it from standard diversity preservation methods. The resulting algorithm is relatively simple and the empirical results are good.  Some of the main concerns for reviewers included the bibliography, which the authors promptly acted on by citing several suggested papers and comparing their approach where relevant. There was also discussion about the exact novelty of the paper, for example as compared to the CVT MAP Elites algorithm, but this was clarified by the authors. Reviewers agree that the paper is easily to follow and well written.  Based on this, it seems that the paper makes a clear contribution to QD methods for RL, and is worth accepting.
All three reviewers are consistently positive on this paper. Thus an accept is recommended.
This paper studies the effect of the discount mismatch in actor critics: the discount used for evaluation (often 1), the discount used for the critic and the discount used for the actor. There’s notably a representation learning argument supported by a series of experiments. The initial reviews pointed out that this paper addresses very relevant research questions, sometimes in a quite original way, with a large set of experiments. However, they also raised concerns about the organization/clarity of the paper, and possible weaknesses about the experimental studies. The authors provided a rebuttal and a revision, that clarified some points and triggered additional discussions. However, if the revision improved the initial submission, the shared assessment is that the clarity and experiments themselves are still somewhat lacking. As such, the AC cannot recommend accepting this paper. Yet, this work does have interesting ideas, and the problem considered is of interest for the community and under studied. The authors are strongly encouraged to submit a revised version to a future venue. 
The paper introduces an object detection method that integrates vision and detection transformers through a novel Reconfigured Attention Module (RAM). Among other questions, the reviewers raised concerns about fair comparison with baselines, limited novelty of the RAM module, completeness of experiments, and missing details. The rebuttal adequately addressed these concerns with clarifications and additional experiments. R1 remained unconvinced that a simple modification to YOLOS could not be devised to improve the speed similar to the proposed method, but stated he/she wouldn’t argue strongly for rejection. While this is a legitimate concern, the AC agrees with R2 and R3 that the paper has enough merits to be accepted at ICLR, as the results are strong and are likely to have significant practical value.
The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice.  I recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion. 
The paper analyzes the sample complexity of convolutional architectures, proving a gap between it and that of fully connected (fc) networks. The approach builds on certain invariances of fc nets. The reviewers appreciated the technical content and its contribution to understanding the relative advantages of different architecture, as well as the role of invariance. 
The paper investigates how the geometrical compactness of in distribution examples affects OOD detection performance and proposes architectural modifications to enable compact in distribution embeddings. All the reviewers agreed that the paper has several interesting contributions. I agree with the authors that simplicity is a strength, not a weakness.   My main concern is that the paper s contributions feel a bit scattered. For instance, the paper does a detailed evaluation of normalization and compactness, but makes a few other minor contributions (as detailed by  the authors at https://openreview.net/forum?id 7VH_ZMpwZXa&noteId m 1y5byLbwS​). However, the latter contributions feel a bit narrow to specific methods and are not as comprehensively tested as the claims around normalization.  Overall, the reviewers and I think that the current version falls below the acceptance threshold. I encourage the authors to revise the draft and resubmit to a different venue.
This paper presents several analyses on the geometry of GAN generators through the lens of Riemannian geometry: showing interpretability of the leading eigenvectors of the Hessian, homogeneity of the space, and more efficient latent space inference through preconditioning. Reviewers found the (revised) paper well written and clear, with a thorough set of experiments to support their main claims. While there were several concerns around the generality of the approach, the authors performed several experiments in the rebuttal period to address many of the reviewer’s concerns (robustness of findings with different image distance functions, inversion on additional GANs and datasets, user study of perceptual properties of axes, and comparison to previous methods for intepretable axes discovery). I found these experiments extensive and convincing, supporting the claims around robustness of the approach to different image distance metrics, GAN architectures, and interpretability of the axes.   There were also strong concerns around similarity with recent work (Chiu et al., SIGGRAPH 2020 and Peebles et al., ECCV 2020), but both of these papers were published at most 1 month before the ICLR submission deadline, and thus should be considered as concurrent work.   Given the strong set of additional experiments and interesting empirical observations, I recommend accepting this paper.  There remain concerts around the extent to which the findings “unify” previous approaches on interpretable axes, and we encourage the authors to update the paper before the camera ready to address these and additional reviewer concerns (especially expanding the discussion of the relationship with concurrent work in Chiu et al. and Peebles et al.).
The presented work is a good attempt to expand the work of Li and Malik to the high dimensional, stochastic setting. Given the reviewer comments, I think the paper would benefit from highlighting the comparatively novel aspects, and in particular doing so earlier in the paper.  It is very important, given the nature of this work, to articulate how the hyperparameters of the learned optimizers, and of the hand engineered optimizers are chosen. It is also important to ensure that the amount of time spent on each is roughly equal in order to facilitate an apples to apples comparison.  The chosen architectures are still quite small compared to today s standards. It would be informative to see how the learned optimizers compare on realistic architectures, at least to see the performance gap.  Please clarify the objective being optimized, and it would be useful to report test error.  The approach is interesting, but does not yet meet the threshold required for acceptance.
**Overview** This paper performs detailed ablation studies over different dynamics prediction methods for MBRL. It proposes metrics for models to evaluate how different types of uncertainty impact predictions. The paper also measures control performance with random shooting MPC. The paper further implements a new hyper parameter schedule to achieve new SOTA performance on the acrobat task.  **Pro**    The paper is well written.   The analysis in this paper is very warranted.    The paper provides a very detailed ablation study.   The authors do a great job defining and arguing for evaluation metrics.   The seven properties and metrics are mostly well motivated and well defined.   The authors discussed the results clearly with implications.   The result of the necessity of probabilistic vs. deterministic models in different scenarios is a good contribution to this field.  **Con**   The methodology might be hard generalizable, i.e., there is difficulty in matching the paper to the literature based on its own defined metric.   The scope might be limited.  **Recommendation** The paper provides a significant contribution to MBRL by providing a detailed empirical study. During the rebuttal phase, the authors addresses many reviewers  concerns in a satisfactory way.  The paper is well written and easy to read. The recommendation is an accept. 
This paper proposes a solid (if somewhat incremental) improvement on an interesting and well studied problem. I suggest accepting it.
This manuscript proposes an information fusion approach to improve adversarial robustness. Reviewers agree that the problem studied is timely and the approach is interesting. However, note concerns about the novelty compared to closely related work, the quality of the presentation, the strength of the evaluated attacks compared to the state of the art, among other concerns. There is no rebuttal.
This is an empirical paper that proposed a few different settings for applying GNNs on temporal data, including what context window to use, code start vs warm start, incremental training vs static.  This paper also proposed and released a few more temporal graph datasets, which could be useful.  The consensus assessment of the reviewers is that the contributions of this paper are incremental, and the results are expected and not exciting enough.  I want to in particular point out that the results highlighted in the paper, that a GNN with window size 1 is sufficient to recover 90% of the performance of the model on full graph, is probably not the correct message to communicate.  This either indicates that the data and task used in the benchmarks do not require sophisticated long horizon temporal information (which makes the comparison between any methods uninteresting), or it indicates that the metric is not sensitive enough to sufficiently distinguish models trained with different settings.  I would recommend rejection and encourage the authors to improve this paper.
The paper received mixed reviews with scores of 5 (R1), 5 (R2),  7 (R3).  All three reviewers raise concerns about the lack of comparisons to other methods. The rebuttal is not compelling on this point. There are quite a few methods that could be used for this application available (often with source code) and should be compared to, e.g. DenseNets (Huang et al.). Given that the proposed method isn t in of itself hugely novel, a thorough experimental evaluation is crucial to the justifying the approach. The AC has closely looked at the rebuttal and the paper and feels that it cannot be accepted for this reason at this time. 
The paper presents improvements to AlphaZero and MuZero for settings where one is restricted in the number of rollouts. The initial response from reviewers was generally favorable  but the reviewers wanted more details and clarifications of multiple parts of the paper, and further intuition about the Gumbel distribution.  The authors’ responses were detailed and convinced or maintained strong positive support of most reviewers. The authors also stated that they plan to provide a release of the code and also provided a policy improvement proof. Overall this is an interesting approach that is likely to be of significant interest to many.
This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance. 
The paper aims to model fake news by drawing tools from multi agent reinforcement learning. After the discussion period, there is a consensus among the reviewers that the paper lacks novel technical contributions. The reviewers also acknowledge that paper also doesn t quite deliver a practical solution as claimed by the authors.
This paper proposes discriminability distillation learning (DDL) for learning group representations. The core idea is to learn a discriminability weight for each instance which are a member of a group, set or sequence. The discriminability score is learned by first training a standard supervised base model and using the features from this model, computing class centroids on a proxy set, and computing the iter and intra class distances. A function of these distance computations are then used as supervision for a distillation style small network (DDNet) which may predict the discriminability score (DDR score). A group representation is then created through a combination of known instances, weighted using their DDR score. The method is validated on face recognition and action recognition.  This work initially received mixed scores, with two reviewers recommending acceptance and two recommending rejection. After reading all the reviews, rebuttals, and discussions, it seems that a key point of concern is low clarity of presentation. During the rebuttal period, the authors have revised their manuscript and interacted with reviewers. One reviewer has chosen to update their recommendation to weak acceptance in response. The main unresolved issues are related to novelty and experimental evaluation. Namely, for novelty comparison and discussion against attention based approaches and other metric learning based approaches would benefit the work, though the proposed solution does present some novelty. For the experiments there was a suggestion to evaluate the model on more complex datasets where performance is not already maxed out. The authors have provided such experiments during the rebuttal period.  Despite the slight positive leanings post rebuttal, the ACs have discussed this case and determine the paper is not ready for publication.
The submission proposes triangular dropout training to provide adaptive capacity of the network at inference time. The proposed approach is simple and sound. However, the experiments are lacking in terms of complexity of the task and up to date architectures (e.g., transformers or convolutional layers) to demonstrate the effectiveness of the method. Therefore I recommend this paper for rejection.
This paper introduces the idea of a counterfactually augmented dataset, in which each example is paired with a manually constructed example with a different label that makes the minimal possible edit to the original example that makes that label correct. The paper justifies the value of these datasets as an aid in both understanding and building classifiers that are robust to spurious features, and releases two small examples.  On my reading, this paper presents a very substantially new idea that is relevant to a major ongoing debate in the applied machine learning literature: How do we build models that learn some intended behavior, where the primary evidence we have of that behavior comes in the form of datasets with spurious correlations/artifacts.  One reviewer argued for rejection on the grounds that dataset papers are not appropriate for publication at a main conference. I don t find that argument compelling, and I m also not sure that it s accurate to call this paper primarily a dataset paper. We could not reach a complete consensus after further discussion. The other reviews raised some additional concerns about the paper, but the revised manuscript appears to have address them to the extent possible.
This paper proposes an approach to improve cross domain generalization in few shot learning, using an objective that attempts to fight overfitting on the observed domain at any given iteration while maintaining the general learned information so far from all domains. The approach uses a domain cycling procedure, where each iteration sees a single domain and, pseudo labels coming from predictions of a previous iterate of the model and from a parameter averaged general model are used in a combined training objective.  Three of the reviewers support acceptance (one strongly), while the fourth leans weakly towards rejection, despite an extensive response from the authors that include new results. One concern was a lack of comparison on Meta Dataset, which the authors went some way towards addressing during the rebuttal, though they also argued Meta Dataset couldn t really support the kind of cross domain evaluation they were targeting. The reviewer was not convinced by the authors  argument, and I too am not, in particular when you consider that Meta Dataset evaluations now often include evaluations on MNIST and CIFAR 10/100, in addition to MS COCO and TrafficSigns (all not included in the training split of Meta Dataset). That said, the experimental protocol favored in the authors  experiments certainly is sound and challenging for cross domain generalization, so I d hesitate to penalize them for that alternative choice.  Overall, I find the ideas behind this work neat, interesting and well motivated. Even if the basic ideas aren t completely novel, I found their combination thought provoking and creative.  Therefore, in the end, I feel this work will be beneficial to the body of literature on few shot learning and would merit to appear at ICLR.
The paper computes an "approximate" generalization bound based on loss curvature. Several expert reviewers found a long list of issues, including missing related work and a sloppy mix of formal statements and heuristics, without proper accounting of what could be gleaned from some many heuristic steps. Ultimately, the paper needs to be rewritten and re reviewed. 
The paper has been actively discussed, both during and after the rebuttal phase. I enjoyed, and I am thankful for, the active communication that took place between the authors and the reviewers.  On the one hand, the reviewers agreed on several pros of the paper, e.g., * Clear, well presented manuscript * The presentation of practically relevant setting * A work that fosters reproducible research (both BO data and algorithms are made available) * Careful experiments  On the other hand, several important weaknesses were also outlined, e.g., * _Novelty_: While the authors claim they “introduce a practically relevant and fundamentally novel research problem”, existing commercial HPO solutions already mention, and propose solutions for, the very same problem, e.g., [AWS](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic model tuning warm start.html) (section “Types of Warm Start Tuning Jobs”) and [Google cloud](https://cloud.google.com/blog/products/gcp/hyperparameter tuning on google cloud platform is now faster and smarter) (section “Learning from previous trials”). The reviewers all agreed on the fact that this down weights the novelty aspect (claimed many times in the rebuttal and the manuscript): The paper formalizes an already existing framework rather than introducing it. * In the light of the weakened "novelty" contribution (see above), the reviewers regretted the absence of a novel transfer method _tailored to HT AA_, which would have certainly strengthened the submission. * _“Dynamic range” of the benchmark_: It is difficult to evaluate the capacity of the benchmark to discriminate between different approaches (e.g., see new Fig. 3 showing the violin plot with all three methods for transfer, as suggested by Reviewer 1: the improvements over "best first" seem marginal at best). To better understand the benchmark, it would be nice to illustrate its “dynamic range” by exhibiting a more powerful method that would substantially improve over “best first”.  As illustrated by its scores, the paper is extremely borderline. Given the mixed perspectives of pros and cons, we decided with the reviewers to recommend the rejection of the paper. 
Quoting from Reviewer2: "The paper considers the problem of optimizing convex functions under metric constraints. The main challenge is that expressing all metric constraints on n points requiries O(n^3) constraints. The paper proposes a “project and forget” approach which is essentially is based on cyclic Bregman projections but with a twist that some of the constraints are forgotten."  The reviewers were split on this submission, with two arguing for weak acceptance and one arguing for rejection.  Purely based on scores, this paper is borderline.  It was pointed out by multiple reviewers that the method is not very novel.  In particular it effectively works as an active set method.  It appears to be very effective in this setting, but the basic algorithm does not differ in structure from any active set method, for which removal of inactive constraints is considered standard (see even the wikipedia page on active set methods).
This paper proposes a method for out of distribution (OOD) detection by introducing a K+1 abstention class for outliers, in addition to the in distribution classes. While the method has shown promising performance compared to the Outlier Exposure (OE), the novelty is limited given the idea is almost identical to an AAAI 20 paper (Mohseni et al. 2020). In addition, several reviewers have raised concerns regarding the lack of fairness in the experimental setting. Authors are encouraged to address them in a future submission.   The AC believes an interesting and valuable contribution to the community would be showing conceptual and theoretical reasoning for the pros and cons of using K+1 class vs. entropy regularization. Currently, the tradeoff between these two types of training objectives is not well understood.   Overall, three knowledgeable reviewers in this area have indicated rejections. The AC discounted R2 s rating due to the less familiarity in this area and lack of participation in the post rebuttal discussion among reviewers.   Lastly, the AC would like to greatly thank R1, R3, and R4 for the active engagement and participation in the paper discussion period. It was very helpful for the decision making process.      
The paper provides methods for training generative models by combining federated learning techniques with differentiable privacy. The paper also provides two concrete applications for the problem of debugging models. Even though the method in the paper seems to be a standard combination of DP deep learning and federated learning, the paper is well written and presents interesting use cases.
The paper proposes an aggregation algorithm for federated learning that is robust against label flipping, backdoor, and Gaussian noise attacks. The reviewers agree that the paper presents an interesting and novel method, however the reviewers also agree that the theory was difficult to understand and that the success of the methodology may be highly dependent on design choices and difficult to tune hyperparameters. 
All reviewers recognized the contribution of performing a theoretical study to investigate how the two technique lines (detection based methods and integral regression based methods) work for pose estimation. The study in this paper could be valuable for the researchers in the pose estimation domain, for the further research. The AC agrees with the reviewers and recommends accept for this paper.
This paper proposes to learn self explaining neural networks using a feature leveling idea.  Unfortunately, the reviewers have raised several concerns on the paper, including insufficiency of novelty, weakness on experiments, etc. The authors did not provide rebuttal. We hope the authors can improve the paper in future submission based on the comments.   
The paper identifies a subtle gradient problem in adversarial robustness  imbalanced gradients, which can cause create a false sense of adversarial robustness. The paper provides insights into this problem and proposes a margin decomposition based solution for the PGD attack.  Pros:   Novel insights into why some adversarial defenses may make some versions of PGD overestimate robustness.   Proposes a method that is motivated by such findings of imbalanced gradients.   The proposed attacks are shown effective across a wide range of defenses.  Cons:   The proposed gradient imbalance ratio could be better motivated: i.e. how is it connected to the scheme of margin decomposition?   Limited novelty in the attacks: i.e. variant of the existing PGD and MT attacks with some proposed changes.   Various concerns with experiments (i.e. stepsize tuning, choice of hyperparameters).  Overall, the reviewers felt that there were some interesting ideas and directions presented in the paper; however, the reviewers also felt that the contribution was of marginal significance and more confidence in the various components (i.e. how the proposed metrics measure the imbalanced gradient effect and various concerns in the experiments) would have made the paper more convincing.
The paper proposes matching the distribution of biases for an LSTM to estimates of long range mutual information from analyzing the statistics of languages. The authors shows empirical evidence that LSTMs seem indeed to be following such a distribution, using natural language and Dyck 2 grammar. They show that explicitly enforcing the distribution of biases in learning can actually help LSTM language models. The reviewers had slight concern about some of the baseline numbers reported, but the authors took the time to address those concerns. Overall, it was an interesting and thought provoking paper that can provide a useful angle to consider when building recurrent models for a problem   namely that of matching the properties / inductive bias of the model to that of the data.
This paper carries out extensive experiments on Neural Tangent Kernel (NTK)  kernel methods based on infinitely wide neural nets on small data tasks. I recommend acceptance.
The paper extends an existing work with three different frequency representations of audios and necessary network structure modifications for music style transfer. It is an interesting study but does not provide "sufficiently novel or justified contributions compared to the baseline approach of Ulyanov and Lebedev". Also the revisions can not fully address reviewer 2 s concerns.  
The authors present a framework for deriving distributional robustness certificates for smoothed classifiers under perturbations of the input distribution bounded under the Wasserstein metric.   Several authors raised concerns regarding the correctness of results presented in the initial version of the paper. While these were addressed during the rebuttal, the reviewers remain concerned about the novelty of the work relative to prior work, in particular the following papers: https://arxiv.org/abs/1908.08729 https://arxiv.org/pdf/2002.04197.pdf https://doi.org/10.1287/moor.2018.0936 and the author responses during the rebuttal did not sufficiently address these concerns.  Hence, I recommend rejection. 
This paper and reviews makes for a difficult call.  The reviewers appear to be in agreement that Value Propagation provides an interesting algorithmic advance over earlier work on Value Iteration networks.  AnonReviewer1 gives a strong rationale why the advance is both original and significant.  Their experiments also show very nice results with VProp and MVProp in 2 D grid worlds.  However, I also fully agree with AnonReviewer2 that testing in other domains beyond 2 D grid world is necessary.  Earlier work on VIN was also tested on a Mars Rover / continuous control domain, as well as graph based web navigation task.  The authors  rebuttal on this point comes across as weak.  In their view, they can t tackle real world domains until VProp has been proven effective in large, complex grid worlds.  I don t buy this at all   they could start initial experiments right away, which would perhaps yield some surprising results. Given this analysis, the committee recomments this paper for workshop.  Pros: significant algorithmic advance, good technical quality and writeup, nice results in 2 D grid world.  Con: Validation is only in 2 D grid world domains. 
The reviewers wondered about the practical application of this method, given that the performance was lower.  The reviewers were also surprised by some of your claims and wanted you to explore them more deeply.    On the positive side, the reviewers found your experiments to be very thorough.  You also performed additional experiments during the rebuttal period.  We hope that those experiments will help you to build a better paper as you work towards publishing this work.
This paper presents a gradient alignment approach to alleviate negative transfer and catastrophic forgetting for multitask and cross lingual learning. Experiments on many domains and datasets demonstrate the efficacy of the proposed approach.  All reviewers agree that the simplicity of the proposed method is a strength of the paper and the experiments are promising. They have suggestions to improve the experiments section, which I believe the authors have addressed in their rebuttal by adding GLUE, image classification, and statistical significance tests, among others.   I recommend accepting this paper for ICLR.
This paper considers the RL problems where actions and observations may be delayed randomly. The proposed solution is based on generating on policy sub trajectories from off policy samples. The benefits of this approach over standard RL algorithms is clearly demonstrated on MuJoCO problems. The paper also provides theoretical guarantees.  This paper is well written overall and technically strong. The majority of the reviewers find that this paper would constitute a valuable contribution to the ICLR program. 
The authors present a method using a VAE to model segmentation masks directly. Errors in reconstruction of masks by the VAE indicate that the mask may be outside the distribution of common mask shapes, and are used to predict poor quality segmentation scenarios that fall outside the distribution of common segmentations.   Pros: + R2: Technical idea is interesting, and a number of baselines used to compare.  + R1 & R4: Method is novel.   Cons:   R3 & R4: The method ignores the original input in its prediction, making the method wholly reliant on shape priors. In situations where the shape prior is weak, the method may be expected to fail. Authors have confirmed this, but not added any experiments to quantify its effect.    R4: The baseline regressor method is missing key details, which makes it impossible to judge if the comparison is fair (i.e. at minimum, number of learned parameters for each model, number of convolutional layers, structure of network, etc.). Authors have not provided these details. Authors have not investigated datasets with weak shape prior to see how methods compare in this setting.   R2: GANs can be used as a baseline. Authors confirmed, but did not supply results.   Reviewers generally agree that the idea is novel, but the value of the approach cannot be determined due to missing baseline experiments, and missing details of baselines. Recommend reject in current form, but encourage authors to complete experiments. 
This paper provides a novel theoretical analysis of epoch wise double descent for a linear model and a two layer non linear model in the constant NTK regime. Some reviewers noted that these models may be too simple to offer a full explanation for the phenomenon in state of the art practical models, for which the NTK is known to change significantly. While this may be true, I believe that the detailed understanding derived in these simple settings provides an important first step and will surely be of interest to the community. I therefore recommend acceptance.
This paper argues that each layer of a network may have some channels useful for and some not useful for transfer learning. The main contribution is an approach which identifies the useful channels through an attention based mechanism. The reviewers agree that this work offers a valuable new approach that offers modest improvements over prior work.   The authors should take care to refine their definition of behavior regularization, including/expanding on the discussion from the rebuttal phase. The authors are also encouraged to experiment with other architecture backbones and report both overall performance as well as run time for learning with the larger models.  
This paper proposes a modification of the training objective of non autoregressive MT which claims most of the improvements that other approaches obtain only through knowledge distillation (KD) from an autoregressive teacher.   The strategy has been largely appreciated as simple and the results suggest that it s rather effective. One of us was not ready to accept certain aspects of the comparisons in the paper, and challenged the paper also from a speed of generation point of view. While I see that NAT MT is very much concerned with speed, removing the dependency on an autoregressive teacher is an important step in the NAT MT agenda (as KD has various drawbacks, e.g., it corrupts the statistics of the training data), and, in my view, disentangling the two desiderata (e.g., faster models, and no KD) is okay at this stage. I hope the authors will not take this recommendation as a reason to ignore the comments in that review, rather, that they take it as an opportunity to address those comments as well as possible (for example, by positioning the work more carefully wrt speed and the possibly negative impact of KD).  On style: Table 1 should fit within the margins of the paper, please fix it. Also, avoid boldfacing model names and avoid vertical bars (please check this nice guide on making [tables look nice](https://people.inf.ethz.ch/markusp/teaching/guides/guide tables.pdf)).
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The paper is clear and well motivated.   The experimental results indicate that the proposed method outperforms the SOTA   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.     The novelty is somewhat minor.   An interesting (but not essential) ablation study is missing (but the authors promised to include it in the final version).  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.   There were no major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
This paper considers a variant of adversarial weight perturbations / sharpness aware minimization for graph (convolutional) neural networks for node and graph classification. In particular, they make two adjustments: “truncating”, i.e., limiting the weight perturbation to specific layers, and weighting the sharpness aware loss with the regular loss during training. The reviewers found that the theoretical justifications (characterization of vanishing gradient and understanding of non iid setting which was added during rebuttal) are interesting, but several reviewers also found the solution/empirical results not convincing enough. I recommend the authors to either shift the focus to the theoretical results or to strengthen the empirical results (and their connections with theory) following the comments of the reviewers.
This paper presents and evaluates a technique for semantic parsing, and in particular proposes a model to re rank the candidates generated by beam search. The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions. The reviews identified strengths of the paper but also significant concerns, mostly centered around the experimental evaluation (including choice of datasets, lack of direct comparison to baselines, need for more methodical and quantitative analysis, need for additional analysis, etc.) and some questions about the design of the technical approach. The authors submitted responses that addressed some of these concerns, but indicated that additional experimentation would be needed to address all of them. In light of these reviews, we are not able to recommend acceptance at this time, but I hope authors use the detailed, constructive feedback to improve the paper for another venue.
This paper proposes a novel method for the single shot domain adaptation with the help of Generative Adversarial Nets. The proposed method is interesting, novel, and versatile. Moreover, the performance is impressive and better than the existing methods. However, the writing needs some improvement for better readability. More quantitive results should be provided in the revision for completeness.
The paper proposes Diversity Regularized Training (DRT), a new training method for an ensemble classifier to improve its certified robustness when randomized smoothing is applied. Specifically, it trains a set of base classifiers to diversify their input gradients while maximizing the confidence margin of each. The method is backed up with a theoretical observation on robustness of ensembles of smooth classifiers.  After the discussion phase, the reviewers unanimously ended up with supporting acceptance of this paper, and the authors were quite responsive to address the reviewers  concerns. Overall, the reviewers appreciated its strong empirical results with a theoretical support   AC also agrees on that, and thinks the paper presents a promising and under explored direction to boost certified robustness of randomized smoothing.
The manuscript develops a new and simple graph neural network architecture. The proposal make use of only O(V) (number of vertices) rather than O(E) (number of edges, meaning that it may be useful for scaling to larger problems. The didactic figures are especially clear, and as is shown in Fig 1 the proposed architecture passes messages based only on the source vertex rather than based on source and target. This challenges common ideas in the field that passed messages ought to reflect a function of both source and target. In spite of this introduced simplification, the architecture performed better than or as well as a set of strong baselines on a set of 6 datasets. The manuscript also examines latency and memory consumption, showing that the methods comes out favourably in this regard. One of the reviewers worries that the paper does not directly provide a solution to scaling network training to very large graphs; they note that several of the datasets that are examined do not contain large graphs. This is true, but the paper does not overclaim in this regard, and I agree with the majority of reviewers that the manuscript is worth publishing on the basis of having developed a well performing approach that challenges the accepted assumptions in the field. While it may not be a direct solution, the counterintuitive results may help point the direction toward development of simple, effective approaches that do scale up.
This manuscript proposes a ranking approach to identify Byzantine agents in federated learning. Distinct from existing methods, the mitigation is implemented by computing ranks for each gradient, then computing rank statistics across agents. The primary intuition is that adversarial agents can be identified by examining these rank statistics.  There are three reviewers, all of whom agree that the method addresses an interesting and timely issue   giving the growing interest in both Byzantine robust learning and federated learning in the community. However, reviewers are mixed on the paper score   with a strong accept a weak accept, and a strong reject. Common issues raised include the generality of the approach beyond the outlined attacks,  Other issues brought up, but addressed in the rebuttal include some weaknesses in the evaluation and comparison to additional baselines. There is also an interesting discussion of using higher order statistics, which does not seem to help the methods when evaluated by the authors. Nevertheless, after reviews and discussion, the reviewers are mixed at the end of the discussion.  The area chair finds, first, that the paper is much improved, and much more applicable in the updated form than in the original version. However, the area chair agrees with the reviewer who notes that the moniker "Byzantine robust" implies the methods should be provably robust to worst case adversaries, not only to a selected set of adversaries with pre selected attacks. The specified setting may be too narrow for interest by the community. To this end,  the area chair suspects that the method may be robust to a more general set of attacks than noted   working to outline sufficient conditions for robustness would significantly strengthen this work. The asymptotic nature of the robustness guarantees is also of concern.  An additional concern of the area chair is that the system setting investigated assumes gradient communication and IID data across devices. While this is not an issue on its own, the setting is closer to distributed learning than federated learning, where one generally communicates model updates, or model differences after multiple local updates, and not gradients. This difference can have a significant effect on robustness methods that depend on identifying benign vs. adversarial statistics of parameters. Non IID data is also common in the federated setting, though this is less concerning, as robust methods for non IID settings are only now emerging. A simple fix for this issue would be to rename the setting from "Federated" to "Distributed."  Authors are encouraged to address the highlighted technical concerns in any future submission of this work. The primary concern may simply be a naming issue (i.e., removing "Byzantine" might fix this concern. Nevertheless, taken together, the opinion of the area chair is that the manuscript is not ready for publication. Again, the area chair believes that many of the issues noted can be fixed, the paper can be strengthened, and this paper may be publishable with limited additional work.
The reviews are a bit mixed. While all the reviewers feel that the paper proposed an interesting mechanism to train conditional generators from a single image and demonstrated good image editing results in the experiments, there are also common concerns about the practicality of the proposed method for interactive image editing. All the reviewers asked for the computation time, and some expressed the concerns about technical contributions. While these concerns were (somewhat) addressed in the rebuttal, the AC feels that it’s a hard sell to bet on the dramatic increase of computational capacity to make the computing time from an hour to realtime. Concerns about novelty also remained. Given the drawbacks, the final decision was to not accept. However, this work is promising and can be made stronger for publication in a later venue.
Reviewers agreed that this work is well motivated and presents a novel approach for data augmentation around the adaptive augmentation policies. There were some concerns around the lack of ablation studies and unclear performance improvements, which were addressed well by the authors’ responses. Thus, I recommend an acceptance.
The paper presents a new way to train the prediction of implicit 3D scene representations from a single view. The main innovations are a novel numerically stable and memory efficient formulation of the derivatives of a loss function based on the spatial gradients of the implicit field, and focusing the training on regions near the surfaces of objects. The method leads to good performance, especially when training on imperfect ground truth scan data.  Concerns were raised about the novelty of the approach and its significance. These were adequately addressed in the author response and revisions. The experiments were found to be well described and executed, which increases the confidence in the approach and its potential impact. I recommend acceptance.
There was quite some variance in opinion on this paper, with some reviewers commenting on problems with clarity and experimental evaluation. The authors rebuttals improved the reviewer opinions slightly. The rebuttal and accompanying revisions are convincing, and the new experimental results are convincing and also very much appreciated. This is one of the first papers taking a comprehensive look at incremental, few shot classification AND regression. Despite some problems with clarity (which were well addressed in rebuttal and revisions), the paper is original and presents novel ideas about incremental few shot learning.  Pros: consideration of both few shot classification and regression, ablation study well executed and convincing.  (remaining) Cons: some minor problems with clarity   please take reviewer comments on board when preparing the camera ready version.
All reviewers voted to accept this paper. The AC recommends acceptance.
the authors demonstrated that vanilla RNN, GRU and LSTM compute at each timestep a hidden state which is the sum of the current input and the weighted sum of the previous hidden states (weights can be either unit or complicated functions), when sigmoid and tanh functions are replaced by their second order taylor series each. they refer to the first term as token level and the second term as sequence level, and claim that the latter can be thought of as summing n gram features in the case of GRU & LSTM due to the complicated weight matrices used for the weighted sum, largely arising from the gating mechanisms.   the reviewers are largely unsure about the significance of the findings in this paper due to a couple of reasons with which i agree. first, it is unclear whether the proposed approximation scheme is enough to capture much of what happens within either GRU or LSTM. if we consider a single step, it s likely fine to ignore the O(x^3) term arising from either sigmoid or tanh, but when unrolled over time, it s unclear whether these error terms will accumulate or cancel each other. without either empirically or theoretically verifying the sanity of this approximation, it s difficult to judge whether the authors  findings are specific to this approximation scheme or do indeed reflect what happens within GRU/LSTM.   second, because the authors have used relative simple benchmarks to demonstrate their points, it is difficult, if not impossible, to tell whether the authors  findings are about the datasets themselves (which are all well known to be easily solvable or solvable very well with n gram classification models and n gram language models) or about GRU/LSTM, which is related to the first weakness shared by the reviewer. the observations that n gram models and simplified GRU/LSTM models work as well as the original GRU/LSTM models on these datasets might simply imply that these datasets don t require any complicated interaction among the tokens beyond counting n grams, which lead to the original GRU/LSTM trained to be simplified (n gram detectors.)   that said, i still believe this direction is important and is filled with many interesting observations to be made. i suggest the authors (1) verify the efficacy of their approximation scheme (probably empirical validation is enough, and (2) demonstrate their point with more sophisticated problems (carefully designed synthetic datasets are perfectly fine.)   
The paper studies the benefit of having multiple servers (with partial coverage) in increase the training speed and latency in Federated Learning.  Of course optimization/learning in the multi server setting comes with a number of challenges which the authors seek to address via novel algorithmic procedures (e.g. FedMes).  I believe the paper is suggesting an important, and potentially impactful, methodology to improve the training speedup/latency of FL. I also acknowledge the additional experiments provided by the reviewers which were quite helpful in addressing some of the concerns. However, as the paper mainly relies on experimental studies to evaluate the performance of the proposed methods, the reviewers (and myself) believe that the paper needs some more investigation in which (i) some of the assumptions (e.g. faster communication between the servers) are either removed or validated; and (ii) more complicated  topologies are considered. 
The authors study generalization in distributed representation learning by describing limits in accuracy and complexity which stem from information theory.   The paper has been controversial, but ultimately the reviewers who provided higher scores presented weaker and fewer arguments. By recruiting an additional reviewer it became clearer that, overall the paper needs a little more work to reach ICLR standards. The main suggestions for improvements have to do with improving clarity in a way that makes the motivation convincing and the practicality more obvious. Boosting the experimental results is a complemental way of increasing convincingness, as argued by reviewers. 
All reviewers agree that this paper does not meet the bar for ICLR. The reviewers provide detailed feedback to the authors on how to improve the writing as well as the overall content of the paper.
The paper received borderline negative scores: 5,6,4.  The authors response to R1 question about the motivations was "...thus can achieve similar classification results with much smaller network sizes. This translates into smaller memory requirements, faster computational speeds and higher expressivity." If this is really the case, then some experimental comparison to compression methods (e.g. Song Han s PhD work at Stanford) is needed to back up this.  R4 raises issues with the experimental evaluation and the AC agrees with them that they are disappointing. In general R4 makes some good suggestions for improving the paper.  The author s rebuttal also makes the general point that the paper should be accepted as it contains ideas, that these are sufficient alone: "We strongly believe that with some fine tuning it could achieve considerably better results, however we also believe that this is not the point in a first submission...". The AC disagrees with this. Ideas are cheap. *Good ideas*, i.e. those that work, as in get good performance on standard benchmarks are valuable however. The reason for having benchmarks is to give some of objective way of seeing if an idea has any merit to it. So while the reviewers and the AC accept that the paper has some interesting ideas, this is not enough for warrant acceptance. 
In this paper, the authors draw connections between probabilistic graphical models (specifically LWF chain graphs) and neural network models. There was general agreement amongst the reviewers that this is an interesting topic that merits further study, and would be of interest to the ICLR audience. At the same time, all of the reviewers have read the author response and there is a consensus that the novelty and significance of this work is limited. The connections between CGs and NNs are somewhat standard and well known, and the significance of the results has not been convincingly demonstrated. 
This paper proposes a new ensemble training method for improving adversarial robustness to multiple attacks (e.g., $\ell_2$, $\ell_1$ and $\ell_\infty$). Specifically, authors adopt the recent Multi Input Multi Output (MIMO) ensemble architecture for computational efficiency. Then, the authors construct the adversarial examples using the outputs of multiple attacks simultaneously. With these examples, standard adversarial training is conducted on MIMO ensemble.  All reviewers are on the negative side. AC agrees with reviewers’ concerns on limited novelty and insufficient empirical evaluation. AC also thinks that the improvement is not that significant compared to the existing method, especially concerning the real world dataset. Overall, AC recommends rejection.
This paper proposes an empirical method to automatically schedule the learning rate in stochastic optimization methods for deep learning, based on line search over a locally fitted model. The idea looks interesting and promising, but the reviewers have serious concerns in the lack of principled support and insufficient empirical evidences. Therefore I recommend rejection of the paper and encourage the author(s) to strengthen the idea and contribution with further theoretical and empirical study.
This application paper applies hyperbolic convolutions in VAE learning to perform unsupervised 3D segmentation.  Addition of these components enables performance improvements in the unsupervised segmentation task. Overall, the paper is borderline and the reviewers mention the limited novelty of the approach, which largely uses components that have been developed before. Even though the paper presents an application of these methods to a relevant and significantly more challenging task than prior work, I recommend rejection from ICLR due to concerns about novelty.
This paper proposes to enhance the robustness of RL and supervised learning algorithms to noise in the observations by dropping input features that are irrelevant for the task. It relies on the information bottleneck framework (well derived in the paper) and learns a parametric compression of the input features that sets them to zero if they are not relevant for the taskn. The method is extensively evaluated on several RL tasks (exploration in VizDoom and DMLab with a noisy “TV” distractor) and supervised tasks (ImageNet or CIFAR 10 classification with noise).  Reviewers have praised the idea, derivation and writing, as well as the extensive experiments on RL and supervised tasks. Critique focused on: * the contrived nature of the TV noise (localised always in the same corner of the image   a standard evaluation according to the authors), * lack of comparison with other feature selection methods, * lack of comparison with Conditional Entropy Bottleneck (done during rebuttal), * more general noise than just specific pixels (clarified by the authors as being the features coming out of a convnet)  Given that the reviewers’ comments were largely addressed by the authors, and given the final scores of the paper, I will recommend acceptance. 
The paper has received 5 reviews with 4 advocating for rejection (marginal or clear cut) and one borderline leaning towards a weak accept. The key concerns voiced by the reviewers are the lack of novelty (*the novelty of the proposed multi derivative architecture is limited*), the lack of comparisons with specific architectures in appropriate setting (rPPGNet without the STVEN module, DeeprPPG, RhythmNet, CVD), and concerns about the use of synthetic data (although authors provide some justifications to that end). It appears that the key to reviewers  scores is that higher order dynamics did not constitute a sufficient novelty.  Given the post rebuttal scores and discussions, AC has no option but to recommend a reject at this point.
This paper presents an extension of the Predictive State Representation (PSRs) to multi agent systems, with a dynamic interaction graph represents each agent’s predictive state based on its “neighborhood” agents. Three types of agent networks are considered: static complete graphs (all agents affect all others experience); static non complete graphs (only some agents affect one another); and dynamic non complete graphs (agents affect one another in a time varying way). A number of theoretical results are presented, including PAC bounds for the approximations in the framework. The paper also contains a number of experiments that clearly show the advantages of the proposed technique over some related methods.  The reviewers unanimously agree that this is a strong paper, with a solid theoretical and empirical analysis.
This clearly written paper describes a simple extension to hard monotonic attention   the addition of a soft attention mechanism that operates over a fixed length window of inputs that ends at the point selected by the hard attention mechanism.  Experiments on speech recognition (WSJ) and on a document summarization task demonstrate that the new attention mechanism improves significantly over the hard monotonic mechanism.  About the only "con" the reviewers noted is that the paper is a minor extension over Raffel et al., 2017, but the authors successfully argue that the strong empirical results render this simplicity a "pro." 
The reviews are of good quality. The responses by the authors are commendable, but reviewers remain of the opinion that the scientific contribution of the paper is limited, no matter how strong the software engineering contribution may be.
This paper proposes to use stacked layers of Gaussian latent variables with a maxent objective function as a regulariser. I agree with the reviewers that there is very little novelty and the experiments are not very convincing.
The paper introduces an approach for learning the dynamics of PDEs. It makes use of bi directional LSTMs trained to regress future values from past observations, up to a given horizon. Experiments are performed on data generated from numerical solvers on two examples, inviscid Burgers and a Navier Stokes system. While the topic is fine, the solution is nothing more than regression with sequence models and only shows that RNNs could learn to predict the data generated by these PDEs. The reviewers also highlight that the comparison with the baselines is not appropriate.
The paper aims to solve the source free domain adaptation, specifically on measurement shift. The proposed method lowers the domain gap via restoring the source feature distribution with a lightweight approximation. The effectiveness and performance are well validated by extensive experiments on various datasets compared with other recent methods, and the ablation analysis supports the claim of the paper well. The paper is well written with clear logic to follow.
This paper explores a post processing method for word vectors to "smooth the spectrum," and show improvements on some downstream tasks.   Reviewers had some questions about the strength of the results, and the results on words of differing frequency. The reviewers also have comments on the clarity of the paper, as well as the exposition of some of the methods.  Also, for future submissions to ICLR and other such conferences, it is more typical to address the authors comments in a direct response rather than to make changes to the document without summarizing and pointing reviewers to these changes. Without direction about what was changed or where to look, there is a lot of burden being placed on the reviewers to find your responses to their comments.
The paper is proposing uncertainty of the NN’s in the training process on analog circuits based chips. As one reviewer emphasized, the paper addresses important and unique research problem to run NN on chips. Unfortunately, a few issues are raised by reviewers including presentation, novelly and experiments. This might be partially be mitigated by 1) writing motivation/intro in most lay person possible way 2) give easy contrast to normal NN (on computers) to emphasize the unique and interesting challenges in this setting. We encourage authors to take a few cycles of edition, and hope this paper to see the light soon. 
This paper proposes CEB, Conditional Entropy Bottleneck, as a way to improves the robustness of a model against adversarial attacks and noisy data. The model is tested empirically using several experiments and various datasets.  We appreciate the authors for submitting the paper to ICLR and providing detailed responses to the reviewers  comments and concerns. After the initial reviews and rebuttal, we had extensive discussions to judge whether the contributions are clear and sufficient for publication. In particular, we discussed the overlap with a previous (arXiv) paper and decided that the overlap should not be considered because it is not published at a conference or journal. Plus the paper makes additional contributions.  However, reviewers in the end did not think the paper showed sufficient explanation and proof of why and how this model works, and whether this approach improves upon other state of the art adversarial defense approaches.  Again, thank you for submitting to ICLR, and I hope to see an improved version in a future publication.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
This paper presents a conditional variational autoencoder (CVAE) approach to solve an instance of stochastic integer program (SIP) using graph convolutional networks. Experiments show that their method achieves high quality solutions with high performance.   It holds merit as an interesting novel application of CVAEs to the ML for combinatorial optimization literature, as well as for the nice empirical results which show a very nice improvement. Two reviewers had a concern that the contribution is a bit narrowly focused toward  MILP focused journal rather than a general purpose ML conference since the core contribution is the novel application. On the other hand, they believe that combinatorial optimization has received growing interest from the ML community in recent years.   All three reviewers vote for borderline accept of this paper. The authors have addressed some of reviewers  concerns, hence some reviewers increased their scores throughout the discussion phase.
This paper develops a mechanism for learning modular state representations in RL that organize recurring patterns into composable schemas. The approach combines modular RNNs as in RIMs (Goyal et al., 2020) with a dynamic feature attention mechanism. There were a variety of concerns in the initial reviews that were addressed by the authors through a set of clarifications and improved empirical analysis, substantially improving the paper. However, there still remain some issues in clarity of presentation and inconsistent empirical results, especially in the form of clear take aways from the empirical analysis and broader insights from the paper, as detailed in the individual reviews. The authors are encouraged to take these aspects into consideration in revising their manuscript.
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
The approach is novel and according to the reviewers  comments addresses a relevant and important problem on EEG data analysis. Differences to related work are discussed. Methods and Experimental results are sound. The authors have provided a comprehensive response to the reviews. 
The paper proposes an approach to hyperparameter tuning based on bilevel optimization, and demonstrates promising empirical results. Reviewer s concerns seem to be addressed well in rebuttals and extended version of the paper.
This papers considers the problem of accuracy disparity in regression for the case of binary sensitive attributes. It provides bounds for accuracy disparity and introduces two methods to enforce this criterion based on representation learning.   The reviews are in agreement that the paper is generally clear and well written, but have different opinions regarding the significance of the contribution and the experimental section. I did read the paper with care myself and overall I do share the concerns raised by Reviewers 3 and 4 that the paper does not place itself accurately wrt to the current literature, both in the discussion and the experimental section. The response to the reviewers about methods that can achieve accuracy disparity for classification is not satisfactory, also considered that two of the analysed datasets are about binary classification tasks. Regarding the results on these datasets, it would be useful to report classification accuracy rather than (in addition to) R^2.  The proposed methods do not seem to show a significant advantage versus the methods considered for comparison.  Minor comments: The proposed methods are inspired by Theorem 3.2. However, is not enforcing accuracy disparity by minimising some distance between conditional distributions what the literature does? I cannot think of other meaningful ways to achieve this criterion. In fact, referring to this theorem, the authors says  However, it is nearly impossible to collect noiseless data with group invariant input distribution. Moreover, there is no guarantee that the upper bound will be lower if we learn the group invariant representation that minimizes dTV(D0(X), D1(X)) alone, since the learned representation could potentially increase the variance. In this regard, we prove a novel upper bound which is free from the above noise term to motivate aligning conditional distributions to mitigate the error disparity . But minimizing dTV(D0(X), D1(X)) might not be desirable if the dependence between the sensitive attribute A and the data is considered legitimate. The point of using conditional distributions is to allow that dependence to be retained.      
This paper proposes a method for learning sentence embeddings such that entailment and contradiction relationships between sentence pairs can be inferred by a simple parameter free operation on the vectors for the two sentences.  Reviewers found the method and the results interesting, but in private discussion, couldn t reach a consensus on what (if any) substantial valuable contributions the paper had proven. The performance of the method isn t compellingly strong in absolute or relative terms, yielding doubts about the value of the method for entailment applications, and the reviewers didn t see a strong enough motivation for the line of work to justify publishing it as a tentative or exploratory effort at ICLR.
The reviews received for this paper raise several critical concerns to which the authors have not provided a response. Thus, in its present form, the paper is not ready for publication.
The reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication. The AC thus proposes "revise and sesubmit".
This paper uses neural amortized inference for clustering processes to automatically tune the number of clusters based on the observed data. The main contribution of the paper is the design of the posterior parametrization based on the DeepSet method.  The reviewers feel that the paper has limited novelty since it mainly follows from existing methodologies. Also, experiments are limited and not all comparisons are made. 
This paper identifies the causal factors behind a major known issue in deep learning for NLP: Fine tuning models on small datasets after self supervised pretraining can be extremely unstable, with models needing dozens of restarts to achieve acceptable performance in some cases. The paper then introduces a simple suggested fix.  Pros:   The motivating problem is important: A large fraction of all computing time used on language understanding tasks involves fine tuning runs under the protocol studied here, and the problem of fine tuning self supervised models should be of broader interest at ICLR.   The proposed fix is simple and well demonstrated. It consists of only an adjustment to the range of values considered in hyperparameter tuning (which is significant, since BERT and related papers *explicitly advise* users to use inappropriate values) and an adjustment to the implementation of the optimizer.  Cons:   The method is demonstrated on a relatively small set of difficult text classification datasets, so the behavior studied here may be different in very different dataset size, task difficulty, or label entropy regimes.  This paper was divisive, so I gave it a fairly close look myself, and I m persuaded by R1 and the other two positive reviewers: This is a classic example of a  strong baselines paper , in that demonstrates that a more careful use of established methods can obviate the need for additional tricks.  R3 raised two major concerns that they presented as potentially fatal, but that I find unpersuasive.   This paper studies stability in model performance, not stability in predictions on individual data points. R3 argues that the latter sense of stability is the more important problem. Stability is an ambiguous term in this context, and both versions of the problem are interesting. However, as the authors pointed out, the definition of stability that is used here is consistent with previous work, and is widely accepted to be a major practical problem in NLP. I don t think this is a weakness of this paper, rather, it s an opportunity for someone else to write another, different paper on a different problem.   R3 claims that the results are described as being more positive than they actually are, and the figure is potentially misleading. Looking at the quantitative results with R3 s points in mind, I still see clear support for both of the paper s main suggestions. R3 opened up some potentially important questions about the handling of outliers in particular, but these questions were raised too late for the authors to be allowed to respond, and I don t see any evidence in the paper that anything improper was done. The marked outliers are clearly much farther from the mean/median in terms of standard deviations than the unmarked outliers. So, I don t see any evidence that these concerns reflect real methodological problems.
The paper touches upon the problem of catastrophic forgetting in continual learning. The idea is to enhance experience reply by explanations of the decision/predictions made. Technically, this "Remembering for the Right Reasons" loss adds an explanation loss to continual learning. This is an interesting idea as also the reviewers agree on. I would like to encourage the authors to have consider a different abbreviation. RRR also stand for "Right for the Right Reasons" loss due to Ross et al.; the authors should use a different abbreviation and also mention the work of Ross et al. (Andrew Slavin Ross, Michael C. Hughes, Finale Doshi Velez: Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations. IJCAI 2017: 2662 2670). Moreover, it might actually be interesting in moving towards interactive learning here as well, because continual learning may also suffer from confounders. Moreover, there is also a connection to HINT (Ramprasaath Ramasamy Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry P. Heck, Dhruv Batra, Devi Parikh: Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded. ICCV 2019: 2591 2600) as it also aims at keeping explanations close to each other. Indeed, they use a ranking loss and do not consider continual learning. Overall, a simple method that is shown empirically to help improving existing replay methods for class incremental learning.
The paper studies the problem of modeling inter object dynamics with occlusions. It provides proof of concept demonstrations on toy 3d scenes that occlusions can be handled by structured representations using object level segmentation masks and depth information. However, the technical novelty is not high and the requirement of such structured information seems impractical real world applications which thus limits the significance of the proposed method.
This submission addresses the problem of detecting malicious PDF files. The proposed solution trains existing CNN architectures on a collected dataset and verifies improved performance over available antivirus software.   There were a number of concerns raised about this work. The main concern the reviewers had with this submission is lack of novelty. The issue is that the paper tackles a standard supervised classification problem which has been extensively explored in the literature and applies an off the shelf classification model. Though the particular application has seen less attention in the ICLR community, the problem setting and solution are well known. Thus, the contribution of the work is not sufficient for acceptance. 
The authors take inspiration from regulatory fit theory and propose a new parameter for policy gradient algorithms in RL that can manage the "regulatory focus" of an agent.  They hypothesize that this can affect performance in a problem specific way, especially when trading off between broad exploration and risk.  The reviewers expressed concerns about the usefulness of the proposed algorithm in practice and a lack of thorough empirical comparisons or theoretical results.  Unfortunately, the authors did not provide a rebuttal, so no further discussion of these issues was possible; thus, I recommend to reject.
The paper proposes an interesting and well motivated improvement of Sharpness Aware Minimization.  Overall the AC and reviewers are satisfied by the author feedback in improving the solidity and rigor of the theoretical results.   The points made by the authors in response to the reviewers initial concerns are essential, especially those regarding interpretation of Corollary 5.2.1, making the proofs rigorous, and fixing the potential for crude convergence bounds. It is therefore critical that the authors incorporate them into their manuscript.
This paper applies deep learning to a problem from OR, namely multistage stochastic optimization (MSSO). The main contribution is a method for learning a neural mapping from MSSO problem instances to value functions, which can be used to warm start the SDDP solver, a state of the art method for solving MSSO. The method is tested on two typical OR problems, inventory control and portfolio management. The reviewers think that the idea is interesting, the empirical results are impressive, and the paper is well written. However, there are reservations on its relevance to the ICLR community.
I thank the authors for their submission and very active participation in the author response period. The paper is well written [R3,R4], tackles a hard problem [R4] in a novel way [R4] with interesting and convincing results [R2]. R3 noted that an empirical comparison to POET would be appropriate. However, in my view the authors addressed these concerns in a satisfactory manner. It seems that R3 has not updated their assessment nor confirmed their current score based on the author response. I am therefore discounting the only review voting for rejection and am siding with R1, R2 and R4. Thus, I recommend acceptance.
This paper starts from the observation that a certain class of rescaled gradient flows   referred to in the paper as RGF and SGF   converge to a solution in finite time (Wibisono et al., 2016; Romero and Benosman, 2020). As a result, it is plausible to ask whether the Euler discretizations of these flows   viewed now as optimization algorithms   enjoy superior convergence properties or not. The authors  main results establish a linear convergence rate under a certain gradient dominance condition, as well as linear convergence to an $\epsilon$ neighborhood of a solution if the algorithms are run with minibatch gradients of size $O(1/\epsilon^\rho)$ for some positive exponent $\rho>0$.  The reviewers raised several concerns regarding the motivation of the authors  work and the comparison of the rates they obtain to other related papers in the literature. The reviewers that raised these concerns were not convinced by the authors  rebuttal and maintained their original assessment during the discussion phase.  From my own reading of the paper, I was perplexed by the fact that the authors did not compare the rates they obtained to existing results in the context of KL optimization, such as the cited paper by Attouch and Bolte and many follow up works in the area. Also, in the stochastic part, while the authors argue that "utilizing batches with size dependent on $1/\epsilon$ is absolutely reasonable and usual, in both theory and practice", it should be noted that a high accuracy requirement (small $\epsilon$) could lead to completely unreasonable batch sizes (effectively exceeding the size of the dataset, especially when $\psi$ is small). Thus, while it is possible to achieve convergence to arbitrarily high accuracy with a sufficiently small step size for a _fixed_ batch size, the rate of this convergence cannot be linear overall   in contrast to the way that the authors frame their result.  In view of the above, I concur that the paper does not clear the bar for ICLR, so I am recommending rejection at this stage (but I would encourage the authors to resubmit a suitably revised version of their paper at the next opportunity).
This paper extends last year s paper on PATE to large scale, real world datasets.  The model works by training multiple "teacher" models   one per dataset, where a dataset might be for example, one user s data   and then distilling those models into a student model. The teachers are all trained on disjoint data. Differential privacy is guaranteed by aggregating the teacher responses with added noise.  The paper shows improved teacher consensus by adding more concentrated noise and allowing the teacher to simply not respond to a student query.  The new results beat the old results convincingly on a variety of measures.  Quality and Clarity: The reviewers and I thought the paper was well written.  Originality: In some sense this work is incremental, extending and improving the existing PATE framework.  However, the extensions and new analysis are non trivial and the results are good.  PROS: 1. Well written though difficult in places for somebody like myself who is not involved in this area. 2. Much improved scalability to real datasets 3. Good theoretical analysis supporting the extensions. 4. Comparison to related work (with a new comparison to UCI medical datasets used in the original paper and better results)  CONS: 1. Perhaps a little dense for the non expert   
This manuscript presents a method to allow RNNs to chain together sequences of behaviors. Reviewers had numerous concerns but the most important is that the problem posed here is solved by a simple method: resetting the state of the RNN before processing a motif.  Overall, reviewers noted a few key topics, although this list is not exhaustive: 1. Experiments are in a very simple but confusing setting. 2. Even though alternatives exist to solving this problem, they are not considered. 3. The networks considered are very simple. 4. The manuscript is difficult to understand. 5. The task admits a trivial solution.  In more detail:  1. The setting of learning to memorize time series and outputting them on command is very simple compared to what most modern work considers. Moreover, there is much confusion in the manuscript about what the setting is precisely.  For example, the setting is described as "independently learn motor motifs in order to build a continuously expandable motif library". But there is no continuously expandable motif library, the motif library is fixed at test time. The authors focus heavily on calling this setting "motor motifs", but these are RNNs that output an arbitrary time series. They are in no sense motor programs and this work is not connected to the extensive literature on motor control in machine learning. More broadly, there is no clear mathematical definition of what the problem being solved is anywhere in the manuscript.  2. It is unusual for manuscripts to not present other baseline models. But more importantly, many other approaches exist to this problem. As one reviewer pointed out, the manuscript essentially sets out to solve a problem that is completely solved in machine learning today. It rejects the solutions that exist for arbitrary reasons, and then adopts its own new solution.  3. The models used are very simple, but this is a consequence of 1, the problem domain being very simple.  4. Reviewers had difficulty understanding the details of the task. In particular, the task description section begins with minutia about the implementation rather than succinct definition of the task.  5. Most critically, reviewers identified that the model could be hard reset and would have the same behavior as the model presented in the manuscript. The proposed solution is essentially hard resetting the state to zero as it stands. There is no reason why a hard reset cannot be followed by a smoothing operation   this seems to be the main objection of the authors.  Overall, the manuscript needs significant improvements. The task considered is too simple by modern ML standards and the fact that it admits a simple solution cannot be overlooked. Demonstrating the idea of the preparatory module on an existing ML task and dataset, while comparing with existing baseline models, carrying out ablations, and producing an extensive quantitative evaluation is what will get the community excited about preparatory modules.
This paper introduces unsupervised meta learning algorithms for RL. Major concerns of the paper include: 1. Lack of clarity. The presentation of the method can be improved. 2. The motivation and justification of applying unsupervised meta learning needs to be strengthened. More discussions and better motivating examples may be useful. 3. Experimental details are not sufficient and comparisons may not be sufficient to support the aim. Overall, this paper cannot be accepted yet. 
This work develops a novel framework for online continual learning, which they authors name Contextual Transformation Networks (CTN). This framework comprises a base network, which learns to map inputs to a shared feature representation, and a controller that efficiently transforms this shared feature vector to task specific features given a task identifier. Both of these components have access to their own memory. The optimization of the both the controller and base network parameters is framed as a bi level optimization framework.  Pros:   important and challenging problem   strong results  Cons:   Currently the writing creates the impression of limited novelty from a technical perspective. I would encourage the authors to more crisply highlight the technical novelty of their method.  
The paper studies the problem of learning the step size of gradient descent for quadratic loss. Interesting theoretical results are presented, which formally support the empirically observed problems of exploding/vanishing gradients, as well as another result showing that if meta learning is done based on the validation performance, optimal performance can be achieved for a simple linear regression task.  On the negative side, there are several issues which preclude publication of the paper in its current stage:  1. The claims in the text seem to be much stronger than what is actually proved.  2. The contributions are not properly connected to the literature (e.g., the relation to Metz et al. 2013 is not properly discussed). 3. Not mentioned in the reviews, but the paper does not explore the connection to similar results coming from online learning/sequential optimization. Recently there has been a surge of papers analyzing meta learning from an online learning perspective; as an example,  Khodak et al. (2019) presents an adaptive step size tuning with guarantees for a much more general problem setting. It could also be interesting to explore if the exploding gradient problem is also related to issues with mirror descent as described in Section 4.1 of Orabona and Pal (2018). 4. The presentation in the main text does not provide enough insight about the results, as too much material is relegated to the appendix. 5. The presentation is often imprecise; it is somewhat questionable (though it is a matter of taste) if the informal theorems are useful (why call them theorems?), but Corollary 1 is not indicated to be informal, yet it is hard to interpret formally. There are other issues such as the statements of Theorems 5 and 6 where conditional expectations are used without explicitly showing the conditions, high probability bounds are stated although the error probability never appears, etc. 6. It is not clear how meta learning helps in Theorem 6 compared to methods adaptively tuning the step size (as a recent work, see, e.g., Joulani et al. 2020 and the references therein).    M. Khodak, M F. Balcan, A. Talwalkar. Adaptive Gradient Based Meta Learning Methods. NeurIPS 2019. F. Orabona, D. Pal. Scale free online learning. Theoretical Computer Science 716, 50 69, 2018. P. Joulani, A. Raj, A. Gyorgy, C. Szepesvari. A simpler approach to accelerated optimization: iterative averaging meets optimism. ICML 2020.  
While the reviewers appreciated the method s ability to replace transformer models and SMILES data augmentation their main concerns were with (a) the experimental section, and (b) the technical innovation over prior work, which updated drafts of the paper did not fully resolve. Specifically for (a) this work performs very similarly to prior work: for reaction outcome prediction the proposed method improves top 1/3/5 for USPTO_STEREO_mixed but is outperformed by prior work for top 1/5/10 for USPTO_460k_mixed; for retrosynthesis the model is outperformed for USPTO_full and only outperforms prior work that does not use templates/atom mapping/augmentation for top 1 on USPTO_50k. The authors argue that their method should be preferred because their method does not require templates, atom mapping, and data augmentation. The reviewers agree that template free and atom mapping free methods are more widely applicable. However, the benefits of being augmentation free is not convincingly stated by the authors who only state that their approach is beneficial by "simplifying data preprocessing and potentially saving training time." The authors should have empirically verified these claim by reporting training time, because it is not obvious that their model which requires pairwise shortest path lengths is actually faster to train. For (b) the reviewers believed that the paper lacked technical novelty given recent work (e.g., NERF). The authors should more clearly distinguish this work from past work (e.g., graphical depictions and finer past work categorization may help with this). Given the similar performance to prior work, the lack of evidence to support training time claims, and the limited technical novelty, I believe this work should be rejected at this time. Once these things are clarified this paper will be improved.
This paper considers the problem of reasoning about uncertain poses of objects in images. The reviewers agree that this is an interesting direction, and that the paper has interesting technical merit. 
The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm.  Yet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation.  While R1 R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments.  The AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to "revise and resubmit" the paper.
This paper proposes a method for collaborative multi agent learning and ad hoc teamwork. The paper includes extensive empirical results across multiple environments (including one of known outstanding high difficulty) and repeatedly performs favourably in comparison to a suitable set of state of the art methods. The proposed method is motivated by theoretical analysis, which was considered interesting but its connection to the method in the initial paper was weak.   Overall, there are remaining concerns which have not been fully addressed in the discussion phase. The authors  responses and discussion with the reviewers should be utilised to improve the material s presentation and to clarify the theory empirical connection in future revisions of the paper.
This paper proposes a new measure for CNN and show its correlation to human visual hardness. The topic of this paper is interesting, and it sparked many interesting discussions among reviews. After reviewing each others’ comments, reviewers decided to recommend reject due to a few severe concerns that are yet to be address. In particular, reviewer 1 and 2 both raised concerns about potentially misleading and perhaps confusing statements around the correlation between HSF and accuracy. A concrete step was suggested by a reviewer   reporting correlation between accuracy and HSF. A few other points were raised around its conflict/agreement with prior work [RRSS19], or self contradictory statements as pointed out by Reviewer 1 and 2 (see reviewer 2’s comment). We hope authors would use this helpful feedback to improve the paper for the future submission.  
The paper is concerned with learning representations for time varying graphs which is an important problem that is relevant to the ICLR community. For this purpose the authors propose a new method to extend skip gram with negative sampling to higher order tensors with the goal to perform an implicit tensor factorization of time varying graphs.The proposed approach shows promising experimental improvements compared to previous methods. Reviewers highlighted also the tasks considered in the paper as well as the theoretical and qualitative analysis as further positive aspects.  However, there exist still concerns regarding the current version of the manuscript. In particular, reviewers raised concerns regarding the novelty of the approach (SGNS, its extension to higher order tensors, as well as the connection to PMI have been studied in the literature). As such the new technical contributions are limited. Reviewers raised also concerns regarding the scalability of the method and its applicability to large graphs. The revised version addresses this concern to some extent by showing experiments on mid sized graphs with 2000/5000 nodes. While this clearly improves the paper, I agree with the majority of the reviewers that the manuscript requires an additional revision to iron out the points raised in this round of reviews. However, the presented results are indeed promising and I d encourage the authors to revise and resubmit their work considering the reviewers  feedback.
The decision for this paper is quite difficult: the methodological ideas are interesting, such as learning the generators directly, but the experimental results are relatively weak. Perhaps learning the generators is too unconstrained. Moreover, the proposed  L conv  builds heavily on prior work such as the  LieConv  model of Finzi et. al, which is not made very transparent in the current narrative. And LieConv does provide better performance than L Conv   this should also be made clear in the text. This was a very difficult call, and after extensive discussion, the decision is intended to be in the long term best interests of the paper. The ideas are interesting and warmly appreciated, the reviewers appreciated aspects of the response, and the project is sufficiently promising that it was felt that their impact would be much greater if the experimental execution were strengthened, such that the project largely terminating at this point would do it a disservice in the long run. There was a general feeling that this is still a  work in a progress  and was somewhat rush written. The authors are strongly encouraged to continue pursuing this work, strengthening the experiments and narrative,  as above.
This paper analyses the dynamics of RNNs, cq GRU and LSTM.    The paper is mostly experimental w.r.t. the difficulty of training RNNs; this is also caused by the fact that the theoretical foundations of the paper seem not to be solid enough.  Experimentation with CIFAR10 is not completely stable.  The review results make the paper balance at the middle.  The merit of the paper for the greater community is doubted, in its current form.
The paper received 5 reviews, one of which had positive feedback. Although there are merits associated with the paper, several concerns raised in the reviews and the discussion period that prevents the paper to be accepted. It appears that experiments on noisy graphs are not properly done and competitive baselines are not used for validations. The quality of the learned graph structure is not adequately analyzed. and the experimental setup was not clearly explained. All these indicate that there is a need for a major revision before the paper can be considered for acceptance.
This paper proposes BOSH attack, a meta algorithm for decision based attack, where a model that can be accessed only via label queries for a given input is attacked by a minimal perturbation to the input that changes the predicted label. BOSH improves over existing local update algorithms by leveraging Bayesian Optimization (BO) and Successive Halving (SH). It has valuable contributions. But various improvements as detailed in the review comments can be made to further strength the manuscript.
This paper considers GNNs for link prediction (predicting which links are likely to appear next). An idea that has been used before is to add virtual nodes to improve the ``under reaching” problem in shallow GNNs; this paper considers this systematically in the context of link prediction. Specifically, one approach developed is to cluster the graph into clusters C(i), I   1, 2, …, k for some k and to add a virtual node u(i) for each index i, which is made adjacent to each node in C(i). This can ease information exchange, particularly in message passing GNNs.   Link prediction is an important problem. However, there seem to be at least three issues with this work: the performance gains obtained are not strong enough, it is not conceptually clear why virtual nodes should help with link prediction, and the analysis is quite a bit about repeating existing analyses on nodes alone. I recommend the authors to address these issues thoroughly in the next version of the paper.
The work tackles the task to convert an artificial neural networks (ANN) to a spiking neural network (SNN). The topic is potentially important for energy efficient hardware implementations of neural networks. There is already quite some literature available on this topic.  Compared to these, the manuscript exhibits a number of strong contributions: It presents a theoretical analysis of the conversion error and consequently arrives at a principled way to reduce the conversion error. The authors test the performance of the conversion on a number of challenging data sets. Their method achieves excellent performances with reduced simulation time / latency (usually, in order to achieve comparable performance to ANNs, one needs to run the SNN for many simulated time steps  this simulation time is reduced by their model).  One reviewer criticized that the article was hard to read, but this opinion was not shared by other reviewers and the authors have improved the readability in a revision.  In summary, I believe that this manuscript presents a very good contribution to the field.
The paper brings the "supermask" idea used in neural architecture search to the application of federated learning, here represented by a single mask of a larger network. The method can be seen as pruning before training, or more precisely pruning instead of training. It is a simplified version of the related works LotteryFL, PruneFL or FedMask, with the difference that here no personalization and no training of the weights is performed, only learning of a global mask. Related work discussion should be improved. While the communication efficiency impact of the method seems minor but positive, the interesting point is that authors here argue that masking will improve robustness to adversarial participants during training.   Unfortunately no theoretical evidence is provided for success of training, in the sense of Byzantine robustness. It is known that robust training can be attacked with small perturbations correlated over time (e.g.  little is enough ), so also over layers, two important aspects which are ignored here   as voting here is only analysed static at a single time point. As pointed out by reviewer JJjz, the considered attack (inverting ranking) is far from being formally proven to be the strongest one, and we would have wished for a more precise discussion of these issues as the target of the paper seems to be mainly robustness.  Concerns on the paper also remained on the level of novelty, as it only uses existing building blocks which are more or less directly applicable from the centralized setting, and on the limited contributions towards formal robustness, and on the limited discussion of related work mentioned by several reviewers, only some of which we were able to address in the discussion phase.  We hope the detailed feedback helps to strengthen the paper in the future.
The authors study separable convolutions in the group convolutional setting, and describe experiments showing them to be more computationally efficient without loss of performance in the setting of some group augmented MNISTs, and show some promising results on un augmented CIFAR10, CIFAR100, and Galaxy10.  The reviewers are mixed; some of the reviewers have concerns about the completeness of the experiments and the novelty of the work, and in particular to what extent the experiments support the specific novelties claimed.  The authors have made some updates to address this in the revision, but my opinion is that the authors should resubmit to the next venue after further experiments and exposition to clarify.
This paper studies potential drawbacks in using softmax over attention in Transformers and evaluates other normalization approaches. Reviewers, while had been positive about the empirical analysis and the insights from the synthetic data experiments, agree that the paper lacks real world experiments/insights. I agree with that and believe the paper falls short in several areas.  1) Drawbacks of softmax: Paper states several generic drawbacks of softmax such as saturation issue leading to vanishing gradients. However the paper does not demonstrate if Transformer models used in practice suffer from this issue under standard training settings. Even the arguments that attention layer focuses on local information is quite vague and not well supported. Overall the analysis is quite weak without much concrete statements and demonstration in real settings.   2) Experiments: The paper presents many synthetic experiments evaluating alternates to softmax varying from layer normalization to pooling. There are no experiments showing if the studied variations actually solve the issues discussed in earlier section. Finally due to the lack of any real world experiments (even small scale ones), it is not clear if the results apply in real world settings.  Overall I think the paper needs significant work in formalizing the drawbacks of using softmax in Transformers and demonstrating that the proposed solutions indeed solve this problem.
This paper presents a method for optimizing parameter matrices of deep learning objectives while enforcing orthonormality constraints.  While advantageous in certain respects, such constraints can be expensive to maintain when using existing methods.  To address this issue, an new algorithm is proposed based on the Cayley Transform and analyzed in terms of convergence.  After the discussion period two reviewers supported acceptance while one still voted for rejection.  Consequently, in recommending acceptance here for a poster, it is worth examining the significance of unresolved concerns.  First, the reject reviewer raised the valid point that the convergence proof relies on the assumption of Lipschitz continuous gradients, and yet the experiments use ReLU activation functions that do not satisfy this criteria.  In my view though, it is sometimes reasonable to derive useful theory under the assumption of Lipschitz continuous derivatives that nonetheless provides insight into the case where these derivatives may not be Lipschitz on a set of measure zero (which would be the case with ReLU activations).  So while ideally it might be nice to extend the theory to remove this assumption, the algorithm seems to work fine with ReLU activations in practice.  And this seems reasonable given the improbability of any iterate exactly hitting the measure zero points where the gradients are discontinuous.  Beyond this issue, some criticisms were mentioned in terms of how and where the timing comparisons were presented.  However, I believe that these issues can be easily remedied in a final revision.
Pros: The paper aims to unify classification and novelty detection which is interesting and challenging.  Cons:   The reviewers find that the work is incremental and contains heuristics. Reviewers find the repurposing of the fake logit in semi supervised GAN discriminator for assigning novelty strange.   The experiments presented are weak and authors do not compare with traditional/stronger approaches for novelty detection such as "learning with abstention" models and density models. GIven the pros and cons, the committe finds the paper to fall short of acceptance in its current form. 
The paper considers FL with periodically shifting distributions, which is a very relevant and timely research question in the area of federated learning, and learning under distributions shifts. The paper proposed an interesting unsupervised way to learn grouping clients into different branches during training, using a federated version of the EM algorithm. Overall the paper contains several solid contributions in some novel combinations, but remained borderline in terms over overall scores. While reviewers were generally positive about the approach, technical soundness and the importance of the question, still some concerns remained.  Concerns included on the level of novelty relative to several recent similar related FL works, and them being included as baselines. Several of these are now discussed in the rebuttal and revisions, but not all in sufficient depth. While the datasets used seem to offer sufficiently hard task from the split between the  day  and  night  distribution, several questions were raised if the treatment of priors is realistic enough. This includes the question of fair hyperparameter tuning with respect to the temporal priors, as well as potential misspecification of the same, towards a more principled treatment of the priors. The authors have answered several of the concerns in the revision, and have added more baseline comparisons, raising the paper narrowly above the acceptance bar in my assessment.  Time wise, the very related paper Marfoq et al 2021 "Federated Multi Task Learning under a Mixture of Distributions" seems to have been available 6 weeks before ICLR deadline. We thank the authors for having included it in discussion and experiments, but the discussion of related contributions needs to be expanded (main difference seems to be supervised vs unsupervised group assignment).  My impression is these points can be addressed in a camera ready version, and I hope the detailed feedback here by all reviewers below will be incorporated.
This paper introduced an adaptive importance sampling strategy to select mini batches to speed up the convergence of network training. The method is well motivated and easy to follow.  The main concerns raised by the reviewers are limited novelty of the proposed simple idea compared to related recent work, and moderate empirical performance.  The authors argue that the particular choice of the adaptive sampling method comes after trying various methods. I believe providing more detailed discussion and comparison with different methods together with the "active bias" paper would help the readers appreciate the insights conveyed in this paper.  The authors provide some additional experiments in the revision. It would make the whole experiment section a lot stronger and convincing if the authors could run more thorough experiments on extra challenging datasets and include all the results int the main text.   Additional experiment to clarify the merit of the proposed method on either faster convergence or lower asymptotic error would also improve the contribution of this paper.
  + sufficiently strong results  + a fast / parallelizable model     Novelty with respect to previous work is not as great (see AnonReviewer1 and AnonReviewer2 s comments)    The same reviewers raised concerns about the discussion of related work (e.g., positioning with respect to work on knowledge distillation). I agree that the very related work of Roy et al should be mentioned, even though it has not been published it has been on arxiv since May.    Ablation studies are only on smaller IWSLT datasets, confirming that the hints from an auto regressive model are beneficial (whereas the main results are on WMT)     I agree with R1 that the important modeling details (e.g., describing how the latent structure is generated) should not be described only in the appendix, esp given non standard modeling choices.  R1 is concerned that a model which does not have any autoregressive components (i.e. not even for the latent state) may have trouble representing multiple modes.  I do find it surprising that the model with non autoregressive latent state works well however I do not find this a sufficient ground for rejection on its own. However, emphasizing this point and discussing the implication in the paper makes a lot of sense, and should have been done.  As of now, it is downplayed. R1 is concerned that such model may be gaming BLEU: as BLEU is less sensitive to long distance dependencies, they may get damaged for the model which does not have any autoregressive components.  Again, given the standards in the field, I do not think it is fair to require human evaluation, but I agree that including it would strengthen the paper and the arguments.   Overall, I do believe that the paper is sufficiently interesting and should get published but I also believe that it needs further revisions / further experiments.   
This papers studies the classical problem of relational learning from a probabilistic perspective. The authors propose four reasonable constraints to encode relational properties, and develop a PGM based variational method for learning relational properties from data. After extensive discussion with the authors, a majority of the reviewer reviewers agree the approach is interesting, if not without some flaws.  The problem studied is interesting, novel, and could lead to new developments in the area of relational learning. It is expected that the experiments have some limitations given the authors have approached the problem from a fresh new angle, which the reviewers have appreciated.  Please pay attention to the suggestions from the reviewers, and in particular, please add a more detailed discussion with statistical relational learning: This material may not be familiar to the broader ML audience, and therefore it is essential to make these comparisons explicit.
This paper explores geometric properties of image perturbations (e.g. frequency content and local consistency) and their impact on the adversarial response of networks.  The reviewers feel that the paper is at times unclear about the meaning of terminology (e.g. “local consistency”) that is not clearly defined.  Also, while the reviewers acknowledge that the paper contains a number of interesting ideas, it is not always clear how the paper’s discussions and contributions differ from existing papers (e.g. Dong 2019, Yin et al., 2019, Wang et al., 2020a, Tsuzuku and Sato 2019) that also discuss the frequency content and smoothness properties of adversarial perturbations.
This paper proposes a model for neural machine translation into morphologically rich languages by modeling word formation through a hierarchical latent variable model mimicking the process of morphological inflection. The model boils down to a VAE like formulation with two latent representation: a continuous one (governed by a Gaussian) which captures lexical semantic aspects, and a discrete one (governed by the Kuma distribution) which captures the morphosyntactic function, shared among different surface forms. Even though the empirical improvements in terms of BLEU scores are fairly small, I find this a very elegant model which may foster interesting future research directions on latent models for NMT.  The reviewers had some concerns with some experimental details and model details that were properly addressed by the authors in their detailed response. In the discussion phase this alleviated the reviewers  concerns, which leads me to recommend acceptance. I urge the authors to follow the reviewer s recommendations to improve the final version of the paper. 
The paper looks at meta learning using random Fourier features for kernel approximations. The idea is to learn adaptive kernels by inferring Fourier bases from related tasks that can be used for the new task. A key insight of the paper is to use an LSTM to share knowledge across tasks.  The paper tackles an interesting problem, and the idea to use a meta learning setting for transfer learning within a kernel setting is quite interesting. It may be worthwhile relating this work to this paper by Titsias et al. (https://arxiv.org/abs/1901.11356), which looks at a slightly different setting (continual learning with Gaussian processes, where information is shared through inducing variables).  Having read the paper, I have some comments/questions: 1. log likelihood should be called log marginal likelihood (wherever the ELBO shows up) 2. The derivation of the ELBO confuses me (section 3.1). First, I don t know whether this ELBO is at training time or at test time. If it was at training time, then I agree with Reviewer #1 in the sense that $p(\omega)$ should not depend on either $x$ or $\mathcal {S}$. If it is at test time, the log likelihood term should not depend on $\mathcal{S}$ (which is the training set), because $\mathcal S$ is taken care of by $p(\omega|\mathcal S)$. However, critically, $p(\omega|\mathcal S)$ should not depend on $x$. I agree with Reviewer #1 that this part is confusing, and the authors  response has not helped me to diffuse this confusion (e.g., priors should not be conditioned on any data). 3. The tasks are indirectly represented by a set of basis functions, which are represented by $\omega^t$ for task $t$. In the paper, these tasks are then inferred using variational inference and an LSTM. It may be worthwhile relating this to the latent variable approach by Saemundsson et al. (http://auai.org/uai2018/proceedings/papers/235.pdf) for meta learning.  4. The expression "meta ELBO" is inappropriate. This is a simple ELBO, nothing meta about it. If we think of the tasks as latent variables (which the paper also states), this ELBO in equation (9) is a vanilla ELBO that is used in variational inference. 5. For the LSTM, does it make a difference how the tasks are ordered? 6. Experiments: Figure 3 clearly needs error bars, and MSEs need to be reported with error bars as well;  6a) Figures 4 and 5 need error bars. 6b) Error bars should also be based on different random initializations of the learning procedure to evaluate the robustness of the methods (use at least 20 random seeds). I don t think any of the results is based on more than one random seed (at least I could not find any statement regarding this). 7. Table 1 and 2: The highlighting in bold is unclear. If it is supposed to highlight the best methods, then the highlighting is dishonest in the sense that methods, which perform similarly, are not highlighted. For example, in Table 1, VERSA or MetaVRF (w/o LSTM) could be highlighted for all tasks because the error bars are so huge (similar in Table 2). 8. One of the things I m missing completely is a discussion about computational demand: How efficiently can we train the model, and how long does it take to make predictions? It would be great to have some discussion about this in the paper and relate this to other approaches.  9. The paper evaluates also the effect of having an LSTM that correlates tasks in the posterior. The analysis shows that there are some marginal gains, but none of the is statistically significant. I would have liked to see much more analysis of the effect/benefit of the LSTM.  Summary: The paper addresses an interesting problem. However, I have reservations regarding some theoretical bits and regarding the quality of the evaluation. Given that this paper also exceeds the 8 pages (default) limit, we are supposed to ask for higher acceptance standards than for an 8 pages paper. Hence, putting everything together, I recommend to reject this paper.
the reviewers were not fully convinced of the setting under which the proposed bipolar activation function was found by the authors to be preferable, and neither am i.
This paper describes an application of reinforcement learning to theorem proving in the connection tableau calculus.  The paper does a reasonable job in the application of RL techniques and the high level issues are important.  However, as the reviewers note, there is little connection to the notion of "analogy" outside of the very general idea that RL methods learn to generalize to novel situations.  I did not find the methods very original as it seems a somewhat mechanical application of RL methods.  That would be fine if the empirical results were convincing or surprising.  However, I found the Robinson arithmetic domains not very interesting as the problems were literally arithmetic, as in 2+5   7, rather than theorems such as the commutativity of addition.  The empirical results were not as convincing in the TPTP domains where MCTS seemed to dominate.  Also there are related papers in the area of deep learning applied to theorem proving that I believe dominate this paper ("learning to reason in large theories" and "an inequality benchmark".
Description: The paper presents a method for encoding a compressed version of an implicit 3D scene, from given images from arbitrary view points. This is achieved via a function, learning with a NeRF model, that maps spatial coordinates to a radiance vector field and is optimized for high compressibility and low reconstruction error. Results shows better compression, higher reconstruction quality and lower bitrates compared to other STOA.  Strengths:   Method for significantly compressing NerF models, which is very useful since such models are often trained for every new scene   Retain reconstruction quality after compression by an order of magnitude   Weaknesses:   The need for decompressing the model before rendering can be done means reduced rendering speed. This also requires longer training times.   Experiments against other scene compression + neural rendering technique will have further strengthened the papers’s claims    The techniques used are well established, and thus there is not as much technical novelty. 
All reviewers have substantial concerns regarding this work including novelty and experimental validation. The authors do not provide a rebuttal for the raised concerns. As such, the area chair agrees with the reviewers and does not recommend it be accepted at this conference.
This paper studies the problem of motion prediction for multiple agents in a scene using transformer based VAE like architecture. The paper received mixed reviews initially which generally tended towards borderline acceptance. All reviews appreciated extensive experiments but had some clarifications and requests for ablations. The authors provided a strong rebuttal that addressed many of the reviewers  concerns. The paper was discussed and all the reviewers updated their reviews in the post rebuttal phase. Reviewers unanimously agree that the paper should be accepted. AC agrees with the reviewers and suggests strong acceptance. The authors are urged to incorporate reviewers  comments in the camera ready.
This work proposes a fully explored masking strategy, segmenting the input text, which maximizes the Hamming distance between any two sampled masks on a fixed text sequence. The hope is to reduce the large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM lead to undesirably large gradient variance, which typically hurts training efficiency with stochastic gradient optimization algorithms.  Pro    A clear and interesting, novel leading theoretical idea. The paper has one good theme that it pursues.   A mostly well written paper   Contains good theoretical discussion   Experiments support the idea  Con    The experiments could be better, especially they don t actually measure a reduction in gradient variance only accuracy   The proofs are at best loose   Alternative methods of reducing variance like using large mini batches are not considered   The results might go away with use of stronger (larger) contextual LMs   There isn t good comparison to other methods of masking like span masking and salient term masking   Some of the things included seem quite haphazard (the ablations don t seem to the point, it s not really clear what this has to do with continual learning)  Overall, this paper feels to be in a premature state. The idea is interesting, but the idea and the paper needs to be developed more, with stronger results. I think it doesn t deserve to be accepted at this time. 
This article proposes a weakly supervised few shot learning method for medical imaging segmentation. While initially, the article presented several problems indicated by the reviewers, e.g., the explanation of the novelty and contributions, the explanation of the method, and the experimental evaluation, the authors made a great effort addressing most of the reviewers  comments and uploaded an updated version of the article. However, still, the evaluation part of the article is a bit weak. But the article contains interesting contributions. Accordingly, I recommend accepting the paper at ICLR2021.
The submission is a detailed and extensive examination of overfitting in vision and language navigation domains. The authors evaluate several methods across multiple environments, using different splits of the environment data into training, validation seen, and validation unseen. The authors also present an approach using semantic features which is shown to have little or no gap between training and validation performance.   The reviewers had mixed reviews and there was substantial discussion about the merits of the paper. However, a significant issue was observed and confirmed with the authors, relating to tuning the semantic features and agent model on the unseen validation data. This is an important flaw, since the other methods were not tuned in this way, and there was no  test  performance given in the paper. For this reason, the recommendation is to reject the paper. The authors are encouraged to fairly compare all models and resubmit their paper at another venue.
All reviewers found that the proposed LM with Brownian motion is interesting and novel. Several reviewers raised (minor) concerns about experiments, but have been generally resolved by the authors.
Strengths: The paper presentation was assessed as being of high quality. Experiments were diverse in terms of datasets and tasks.  Weaknesses: Multiple reviewers commented that the paper does not present substantial novelty compared to previous work.  Contention: One reviewer holding out on giving a stronger rating to the paper due to the issue of novelty.   Consensus: Final scores were two 6s one 3.   This work has merit, but the degree of concern over the level of novelty leads to an aggregate rating that is too low to justify acceptance. Authors are encourage to re submit to another venue. 
This paper tackles the multivariate bandit problem (akin to a factorial experiment) where the player faces a sequence of decisions (that can be viewed as a tree) before obtaining a reward. The authors introduce a framework combining Thompson Sampling with path planning in trees/graphs. More specifically, they consider four path planning strategies, leading to four approaches. The resulting approaches are empirically evaluated on synthetic settings.  Unfortunately, the proposed approaches lack theoretical justification and the current experiments are not strong enough to support the claims made in the paper. Given that most of reviewer s concerns remained valid after rebuttal, I recommend to reject this paper.
This paper presents an intriguing empirical phenomenon in deep learning. They train a variety of architectures for different tasks using different datasets and study the relationship between the learned representations. In particular they collect the representations into a large matrix and take the top left singular vector and measure the cosine of the angle. They show that it is much smaller than one might expect, about 10 degrees or so, and has an approximate monotonicity property as the network is being trained although it does not seem to converge to zero. Moreover this measure also correlates with performance.   The reviewers had divided opinions on this paper. On the one hand, the range of experiments is impressive and truly demonstrates that this is a pervasive phenomenon. On the other hand, it is not so clear what it means. In particular, suppose we have a collection of graphs which have close to the same degree distributions. If we take the top left singular vectors of all the adjacency matrices, they would also have low angles between them. While this is a very different setting and there is no analogy between the experiments in this paper and this toy model, it does raise philosophical questions about whether the phenomenon is meaningful or is a byproduct of something else about the data. This may be a challenging question to answer, but one reviewer brought up a natural next step: One could measure the principal angle between the subspace of the top k left singular vectors across experiments for larger values of k. The authors do bring up the point that the spectrum decays very quickly, so it could be that beyond a certain point the singular vectors behave somewhat randomly. 
In this work the authors consider the automatic selection of time series forecasting model (and hyperparameters) based on historical data. It adopts a conventional feature based meta learning approach. Experimental results show an improved performance over the considered baselines.  The reviewers appreciated the clarifications provided by the authors, but a number of concerns were unresolved. For instance, questions remained regarding the dataset collection, the baselines against which the proposed method was compared to (which were considered too weak) and the large number of missing details in the presentation of the method. Based on this the reviewers concluded that the paper could not be accepted in its current form and would require a major revision.
The paper presents an approach to forecasting over temporal streams of permutation invariant data such as point clouds. The approach is based on an operator (DConv) that is related to continuous convolution operators such as X Conv and others. The reviews are split. After the authors  responses, concerns remain and two ratings remain "3". The AC agrees with the concerns and recommends against accepting the paper.
The manuscript investigates common adversarial attacks on event based data for spiking neural networks. They conclude that also in this setup adversarial attacks can strongly harm SNN performance.  Although the reviewers agree that the paper presents some solid results and is well written, there was also substantial criticism.  The main points were:   It is not very clear how the usual attacks are applied to event based data, and in general experimental setups are unclear.   The methodological contribution of the paper seems limited.   The novelty is limited, in particular Marchisio et al. 2021 investigates a very similar question and goes somewhat further. The author noted that their attacks are not deployed on neuromorphic hardware. A number of other important prior work is not discussed.   The impact of adversarial defences was not considered.   A more detailed comparison of event based attacks to standard ANN attacks would be desired.  After the reviews, the authors have invested substantial efforts to improve the paper. These efforts were appreciated by the reviewers. In particular, the authors ran additional experiments using the defence method TRADES. The results showed that TRADES is effective, but the attack has still a large success.  In summary, the reviewers agree that this is a solid manuscript and an interesting direction, however, they see it finally slightly below acceptance threshold for ICLR.
This paper proposes a personalized federated learning framework based on neural architecture search, in which the local clients perform NAS to search for a better architecture for the private local data. Specifically, the authors extend MiLeNAS, which is an existing NAS algorithm, to be run in the federated learning setting, and use FedAvg for model aggregation. The proposed FedNAS framework is validated against personalized federated learning methods with predefined architectures, such as perFedAvg, Ditto, and local fine tuning, and is shown to largely outperform them on non IID settings with label skew and LDA distribution. FedNAS’s collaborative search for the optimal architecture also yields a better performing global model than FedAvg.  The paper received borderline ratings. Three out of four reviewers are learning negative, while one is leaning negative. The below is the summary of pros and cons of the paper mentioned by the reviewers:  Pros   The idea of using NAS for personalized federated learning seems novel and interesting.    The proposed FedNAS framework is shown to be effective in tackling the data heterogeneity problem, which is a fundamental problem with federated learning.   The authors have released the code for reproducibility.  Cons   The technical contribution of the work seems limited, since the proposed FedNAS straightforwardly combines an existing NAS method (MiLeNAS) with federated averaging, and there is no challenge mentioned for this new problem of federated NAS.    The choice of a specific NAS method (MiLeNAS) is not well justified, and other NAS methods should be also considered.   The motivation is unclear: It is not clear whether the authors aim to perform collaborative automotive design or solve personalized federated learning.   There is no convergence analysis.  While some of the concerns have been addressed away in the authors’ responses during the rebuttal period, the reviewers did not change their ratings, and the final consensus was to reject the paper.   I agree with the authors that combining federated learning with NAS, and applying it for personalized federated learning is a novel idea that intuitively makes sense. However, I agree with the reviewers that the current method is a straightforward combination of an existing NAS method and an existing FL algorithm, the authors should identify new challenges posed by the combination of the two methods, and identify them.   Further, performing NAS on edge devices may be possible, but not the best solution, since it could result in large computational overhead. While the authors mention that MiLeNAS is computational suitable in such settings, there should be a proper investigation of the accuracy efficiency tradeoff, showing how well FedNAS performs against others with the same computational budget (or training time / energy consumption).    Overall, this is a paper that proposes a novel and interesting idea that seems to work, but the paper does not sufficiently examine challenges posed by the new problem. I suggest the authors identify the new challenges and examine the efficiency issue mentioned, and further develop their method, if necessary.
The authors proposes a generative model with a hierarchy of latent variables corresponding to a scene, objects, and object parts.   The submission initially received low scores with 2 rejects and 1 weak reject.  After the rebuttal, the paper was revised and improved, with significant portions of the paper completely rewritten (the description of the model was rewritten and a new experiment comparing the proposed model to SPAIR was added).  While the reviewers acknowledged the improvement in the paper and accordingly adjusted their score upward, the paper is still not sufficiently strong enough to be accepted (it currently has 3 weak rejects).   The reviewer expressed the following concerns: 1. The experiments uses only a toy dataset that does not convincingly demonstrate the generalizability of the method to more realistic/varied scenarios. In particular, the reviewers voiced concern that the dataset is tailored to the proposed method  2. Lack of comparisons with baseline methods such as AIR/SPAIR and other work on hierarchical generative models such as SPIRAL. In the revision, the author added an experiment comparing to SPAIR, so this is partially addressed.  As a whole, the paper is still weak in experimental rigor.  The authors argue that as their main contribution is the design and successful learning of a probabilistic scene graph representation, there is no need for ablation studies or to compare against baselines because their method "can bring better compositionality, interpretability, transferability, and generalization".  This argument is unconvincing as in a scientific endeavor, the validity of such claims needs to be shown via empirical comparisons with prior work and ablation studies.  3. Limited novelty The method is a fairly straightforward extension of SPAIR with another hierarchy layer. This would not be a concern if the experimental aspects of the work was stronger.   The AC agrees with the issues pointed by the reviewers.  In addition, the initial presentation of the paper was very poor.  While the paper has been improved, the changes are substantial (with the description of the method and intro almost entirely rewritten). Regardless, despite the improvements in writing, the paper is still not strong enough to be accepted.  I would recommend the authors improve the evaluation and resubmit. 
This is a borderline case. The paper seems solid although some of the numbers are likely incorrect because in some results tables in the appendix the error taken over all attacks is higher than for the best individual attack (which should never happen).  The main contribution of this paper is to augment a standard adversarial loss (against attacks from different norms) with a “consistency” term (consistency between clean, adversarial and noise augmented samples). The relatively large jump in robustness compared to existing schemes that do adversarial training against multiple norms is a bit surprising. A possible explanation could be that the additional consistency term smoothes the landscape around the clean samples a little bit, which could help to find better adversarial examples. The latter would be very similar to a paper by Pushmeet and colleagues (https://arxiv.org/pdf/1907.02610.pdf) which is not cited, but definitely should. It might also be worthwhile to compare to this paper.   Taken together, this work is interesting but not sufficiently convincing yet to belong to the top papers to be selected for publication at ICLR.  
The paper studies an interesting question of whether neural networks can approximate the target function while keep the output in the constraint set. The constraint set is quite natural for  e.g. multi class classification, where the output has to stay on on the probability manifold. The challenge here is that traditional universal approximation theory only guarantees that $\hat{f}(x) \approx f(x)$, but can not guarantee that $\hat{f}(x)$ lies exactly in the same constraint set as $f(x)$.  The paper made a significant contribution in the theory of deep learning   It is shown that the neural network can indeed approximate any regular functions while keep the output stay in the regular constraint set. This gives a solid backup in terms of the representation power of neural networks in practice, to represent target functions whose outputs are in certain constraint set (e.g. probabilities).
This paper investigates how two means of learning natural language   supervised learning from labeled data and reward maximizing self play   can be combined. The paper empirically investigates this question, showing in two grounded visual language games that supervision followed by self play works better than the reverse.   The reviewers found this paper interesting and well executed, though not especially novel. The last is a reasonable criticism but in this case I think a little beside the point. In any case, since all the reviewers are in agreement I recommend acceptance.
Most of the reviewers agree that this paper presents interesting ideas for an important problem. The paper could be further improved by having a thorough discussion of related works (e.g. Placeto) and construct proxy baselines that reflect these approaches.   The meta reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments.   Thank you for submitting the paper to ICLR.  
The paper studies stability issues of GNN training when data are limited. The key contribution of this work is to use reweighted self training and negative sampling to stabilize GNN. Multiple reviewers raised major concerns on the technical novelty, experimental setup, comparison, and results. No response was provided during discussion. I recommend this submission be rejected.
This paper provides some novel ideas combining neural processes, active learning, and deep sequence models toward accelerating the computationally intensive task of stochastic simulations for epidemic models. The authors incorporate aspects such as spatiotemporal dependence and age structure in the models they consider, and propose an active learning framework to leverage a neural process that can serve as a proxy for direct simulation of the stochastic dynamics. Most of the reviewers agreed that some of these ideas are novel, but the assessments together present a borderline case, and one reviewer remains convinced that the paper needs refinement and a more focused exposition to make clear the contributions and relative efficacy compared to existing state of the art. I tend to agree with some of the concerns raised by that reviewer, who responded favorably to the author response but remained not fully convinced.I also agree that important parts of the paper feel rushed, if I am to interpret "feeling rushed" as a statement on the ideas being spread thin in exposition due to the amount of ground the authors try to cover (rather than a statement on the quality of the presentation). There are many moving parts that combine good intuitions toward accelerating simulation based methods for stochastic epidemic models. Several reviewers mentioned improving the empirical comparisons with recent existing methods, including likelihood based methods, and though I agree with the authors too that it is not possible to exhaustively explore outcomes in stochastic process approaches, the largely empirically supported contributions can be better motivated with a more complete comparison and streamlined exposition. I encourage the authors to continue to revise and refine this manuscript to maximize the potential behind the ideas they propose here.
This paper proposes a new method for lifelong learning of language using language modeling. Their training scheme is designed so as to prevent catastrophic forgetting. The reviewers found the motivation clear and that the proposed method outperforms prior related work. Reviewers raised concerns about the title and the lack of some baselines which the authors have addressed in the rebuttal and their revision.
The authors propose two algorithms and their theoretical analysis for solving bilevel optimization problems where the inner objective is assumed to be strongly convex. The authors have greatly improved the paper to answer reviewer comments and three out of four reviewers have increased their scores. That said, given the large amount of new material added to this paper during the discussion phase, the program committee believes the paper requires a new round of reviews for a confident assessment. We encourage the authors to resubmit their work to a top conference such as ICML.
This paper presents a GNN architecture for policies that solve multi robot task allocation problems. The proposed architecture extends Koul et al (2019) by adding payload constraints and task deadlines. The paper looks at routing problems of medium to large size, e.g. 20 robots and 200  tasks. The reviewers are happy that most of their concerns were addressed by they are still concerned that the experimental validation is focusing too much on the multi TSP or Vehicle Routing Problems, and request more extensive experimental validation on similar optimization problems as in Nunes et al (2017). I tend to agree. The proposed method has a lot of merit and just needs one more iteration of improvements to incorporate further experiments, before it becomes ready for publication.      
The paper empirically studies the behaviour of deep policy gradient algorithms, and reveals several unexpected observations that are not explained by the current theory. All three reviewers are excited about this work and recommend acceptance.
This paper presents a variant of recently developed Kronecker factored approximations to BNN posteriors. It corrects the diagonal entries of the approximate Hessian, and in order to make this scalable, approximates the Kronecker factors as low rank.  The approach seems reasonable, and is a natural thing to try. The novelty is fairly limited, however, and the calculations are mostly routine. In terms of the experiments: it seems like it improved the Frobenius norm of the error, though it s not clear to me that this would be a good measure of practical effectiveness. On the toy regression experiment, it s hard for me to tell the difference from the other variational methods. It looks like it helped a bit in the quantitative comparisons, though the improvement over K FAC doesn t seem significant enough to justify acceptance purely based on the results.  Reviewers felt like there was a potentially useful idea here and didn t spot any serious red flags, but didn t feel like the novelty or the experimental results were enough to justify acceptance. I tend to agree with this assessment. 
The paper in its current form was just not well enough received by the reviewers to warrant an acceptance rating. It seems this work may have promise and the authors are encouraged to continue with this line of work. 
The paper has good contributions to a challenging problem, leveraging a Faster RCNN framework with a novel self supervised learning loss. However reviewer 4 and other chairs (in calibration) considered that the paper does not meet the bar for acceptance. The other reviewers did not champion the paper either, hence i am proposing rejection.   Pros:   R1 and R3 agree that the proposed model improves over related models such as MONET.   The value of the proposed self supervised loss connecting bounding boxes and segmentations is well validated in experiments.  Cons:   R4 gives good suggestions that may be useful to reach a broader readership, namely introducing more of the concepts used in the paper., e.g. "stick breaking, spatial broadcast decoder, multi otsu thresholding" so it becomes more self contained. R4 also suggests improving the writing more generally.   R4 still finds the proposed "method quite complex yet derivative" after the rebuttal.   All reviewers complain about lack of experiments in real data, but the authors did revise their paper and add some coco results in the appendix. These could be part of the main paper in a future version.
 + An intriguing novel regularization method: encouraging larger norms for the feature vector input to the last softmax layer of a classifier.  + Resonably extensive experimental validation shows that it improves test accuracy to some degree.    While a motivation is given, the formal analysis of what is really going on remains very superficial and limited.  Technical note: Simply scaling the softmax layer s input would not change class rankings, so any positive effect of this regularizer on classification performance is due to it changing the learning dynamic in the upper layers as well. The paper could be much stronger if it did provide an analysis regarding how the global learning dynamic is affected in all layers, by the interaction between weight decay and the last layer s feature incay. 
The reviewers appreciated the author replies, the additional experiments (more runs but also more ablations/baselines), and the updated paper. Also R2 is now largely satisfied (but seems to have been too late to post a public reply or to raise the score of the review).  The paper provides important insights in model based RL and its connections to planning, by studying MuZero with systematic ablations. Hence a valuable contribution to the community. All (major) cons have been addressed in the revision.
This article proposes a regularisation scheme to learn classifiers that take into account similarity of labels, and presents a series of experiments. The reviewers found the approach plausible, the paper well written, and the experiments sufficient. At the same time, they expressed concerns, mentioning that the technical contribution is limited (in particular, the Wasserstein distance has been used before in estimation of conditional distributions and in multi label learning), and that it would be important to put more efforts into learning the metric. The author responses clarified a few points and agreed that learning the metric is an interesting problem. There were also concerns about the competitiveness of the approach, which were addressed in part in the authors  responses, albeit not fully convincing all of the reviewers. This article proposes an interesting technique for a relevant type of problems, and demonstrates that it can be competitive with extensive experiments. ``Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year s ICLR.  
This paper examines the extent to which a large language model (LM) can generalize to unseen tasks via "instruction tuning", a process that fine tunes the LM on a large number of tasks with natural language instructions.  At test time, the model is evaluated zero shot on held out tasks.  The empirical results are good, and the 137B FLAN model generally out performs the 175B untuned GPT 3 model.  All reviewers voted to accept with uniformly high scores, despite two commenting on the relative lack of novelty.  The discussion period focused on questions raised by two reviewers regarding the usefulness of fine tuning with instructions vs. multi task fine tuning without instructions.  The authors responded with an ablation study demonstrating that providing instructions at during tuning led to large gains.  Overall the paper s approach and detailed experiments will be useful for other researchers working in this fast moving area in NLP.
This work presents a RNN tailored to generate sketch drawings. The model has novel elements and advances specific to the considered task, and allows for free generation as well as generation with (partial) input. The results are very satisfactory. Importantly, as part of this work a large dataset of sketch drawings is released. The only negative aspect is the insufficient evaluation, as pointed out by R1 who points out the need for baselines and evaluation metrics. R1’s concerns have been acknowledged by the authors but not really addressed in the revision. Still, this is a very interesting contribution.
This paper presents a spatially structured neural memory architecture that supports navigation tasks.  The paper describes a complex neural architecture that integrates visual information, camera parameters, egocentric velocities, and a differentiable 2D map canvas.  This structure is trained end to end with A2C in the VizDoom environment.  The strong inductive priors captured by these geometric transformations is demonstrated to be effective on navigation related tasks in the experiments in this environment.  The reviewers found many strengths and a few weaknesses in this paper.  One strength is that the paper pulls together many related ideas in the mapping literature and combines them in one integrated system.  The reviewers liked the method s ability to leverage semantic reasoning and spatial computation.  They liked the careful updating of the maps and the use of projective geometry.    The reviewers were less convinced of the generality of this method.  The lack of realism in these simulated environments left the reviewers unconvinced that the benefits observed from using projective geometry in this setting will continue to hold in more realistic environments.   The use of fixed geometric transformations with RGBD inputs instead of learned transformations also makes this approach less general than a system that could handle RGB inputs.  Finally, the reviewers noted that the contributions of this paper are not well aligned with the paper s claims.  This paper is not yet ready for publication as the paper s claims and experiments were not sufficiently convincing to the reviewers. 
This paper considers an interesting hypothesis that ReLU networks are biased towards learning learn low frequency Fourier components, showing a spectral bias towards low frequency functions. The paper backs the hypothesis with theoretical results computing and bounding the Fourier coefficients of ReLU networks and experiments on synthetic datasets.  All reviewers find the topic to be interesting and important. However they find the results in the paper to be preliminary and not yet ready for publication.   On theoretical front, the paper characterizes the Fourier coefficients for a given piecewise linear region of a ReLU network. However the bounds on Fourier coefficients of the entire network in Theorem 1 seem weak as they depend on number of pieces (N_f) and max Lipschitz constant over all pieces (L_f), quantities that can easily be exponentially big.  Authors in their response have said that their bound on Fourier coefficients is tight. If so then the paper needs to discuss/prove why quantities N_f and L_f are expected to be small. Such a discussion will help reviewers in appreciating the theoretical contributions more.  On experimental front, the paper does not show spectral bias of networks trained over any real datasets. Reviewers are sympathetic to the challenge of evaluating Fourier coefficients of the network trained on real data sets, but the paper does not outline any potential approach to attack this problem.   I strongly suggest authors to address these reviewer concerns before next submission.  
The paper proposes a new way to train latent variable models. The standard way of training using the ELBO produces biased estimates for many quantities of interest. The authors introduce an unbiased estimate for the log marginal probability and its derivative to address this. The new estimator is based on the importance weighted autoencoder, correcting the remaining bias using russian roulette sampling. The model is empirically shown to give better test set likelihood, and can be used in tasks where unbiased estimates are needed.   All reviewers are positive about the paper. Support for the main claims is provided through empirical and theoretical results. The reviewers had some minor comments, especially about the theory, which the authors have addressed with additional clarification, which was appreciated by the reviewers.   The paper was deemed to be well organized. There were some unclarities about variance issues and bias from gradient clipping, which have been addressed by the authors in additional explanation as well as an additional plot.   The approach is novel and addresses a very relevant problem for the ICLR community: optimizing latent variable models, especially in situations where unbiased estimates are required. The method results in marginally better optimization compared to IWAE with much smaller average number of samples. The method was deemed by the reviewers to open up new possibilities such as entropy minimization. 
The paper proposes a method for inferring which of a set of pretrained neural networks, once fine tuned on a transfer task, will generalize the best. This is accomplished by deriving a quantity based on a mean field approximation of a dynamical system defined on the adjacency matrix of the weights of a neural network, known as the "neural capacitance". The model selection procedure involves attaching a fixed, randomly initialized network onto the outputs of the pretrained network and fine tuning for a small number of iterations, and computing the metric; the fixed network is called the "neural capacitance probe" (NCP).  Reviews, though low confidence, awarded borderline scores, and a central concern was clarity and motivation, in particular the role of the NCP. acZh, the highest confidence and most verbose reviewer, echoed these concerns along with specific criticisms, for example about the heavy reliance on Gao et al (2016) without elaboration. The authors have responded in considerable depth but unfortunately the reviewer has not acknowledged these responses. On the NCP, the authors note that this is an approximation to the ideal metric that they have empirically validated.  Reading the updated draft, I find myself still concurring with reviewer acZh in large degree. The draft has improved with the noted additions, such as Appendix G devoted to an explanation of Gao et al (2016), but the presentation is still quite challenging to follow. I am left with fundamental questions about the soundness of the approximation being made, its wider applicability, and the many arbitrary decisions regarding the architecture of the NCP that appear out of nowhere. How sensitive is the procedure to these choices? Did the authors tune these architectural hyperparameters? Using what data? The table of results does not include units, and for a paper proposing a general purpose metric I d ideally want to see a a robust rationale for hyperparameter selection of method specific hyperparameters as well as a rigorous statistical treatment of the method s performance. Since it involves an approximation, a comparison to the "ideal" or "exact" procedure on a toy problem where the latter is feasible would strengthen the paper considerably. I do appreciate the breadth of architectures and datasets examined, but I believe the central focus of the paper should be explaining the mathematical motivation (perhaps at a higher level and deferring more detail to the appendix), why precisely it makes sense in the context of neural networks (also raised by acZh, with an answer provided that I believe partially addresses this) and justifying the concrete, approximate instantiation of the method involving the NCP and the hyperparameter selection and evaluation protocol that led you to the particular NCP employed.  At a higher level, this is a very mathematically dense paper that relies considerably on concepts outside of what might be considered typical expertise in the ICLR community, reflected in the confidence scores of the reviewers. While I feel that the issues described above already preclude acceptance at this time, I believe it may be difficult to do the proposed method justice in the short conference paper format, and would suggest to the authors to consider a journal submission instead, where a didactic presentation can be given the full attention it deserves without the difficulty created by length constraints.  Finally, I d like to apologize to the authors for the non responsiveness of the Area Chair. The original Area Chair was not able to complete their duty and I have been belatedly assigned this paper to evaluate it, and it is clear that not as much discussion took place as would have been ideal.
This paper proposes new analysis on Variance Collapse of SVGD in High Dimensions. The analysis provides some new insights despite of some limitations.
The reviewers are polarized on this paper and the overall feeling is that it is not quite ready for publication. There is also an interesting interpretability aspect that, while given as a motivation for the approach, is never really explored beyond showing some figures of alignments. One of the main concerns of the method’s effectiveness in practice is the computational cost. There is also concern from one of the reviewers that the formulation could result in creating sparse matching maps where only a few pixels get matched. The authors provide some justification for why this wouldn’t happen, and this should be put in a future draft. Even better would be to show statistics to demonstrate empirically that this doesn’t happen.  There were a number of clarifications that were brought up during the discussion, and the authors should go over this carefully and update the draft to resolve these issues. There is also a typo in the title that should be fixed. 
This paper proposes measures of consistency between back doored and clean models, proposes regularization using those consistency measures, and showcases that such trained models indeed exhibit better consistency. Also, it is demonstrated that the fine tuned model does not deviate too far from the original clean model. The reviewers  comments are all well addressed. Some concerns related to the notion of consistency and how it relates to the detection of backdoors are still left open, but the reviewers seem to be satisfied with the answers. Given the overwhelmingly positive reviews, I propose accept.
The paper studies self training for a one hidden layer architecture, showing that with proper initialization self training will improve over standard supervision. The reviewers appreciated the analysis and thought the results make sense. However, they did comment that the paper does not provide sufficient insight about the effectiveness of self training and this should be discussed in the final version. There were additionally comments about the architecture choice (e.g., fixed output weights), and the authors should also address this.
This paper presents a general self supervised time series representation learning framework. The organization is good, and the architecture is well motivated. However, the paper has limited novelty, and is a straightforward application of ideas in self supervised learning literature.  Experimental results are not entirely convincing. The used dataset is small scale that makes the task simple. A more thorough comparison with recent related work is needed. The presentation is also sometimes hard to follow.
This work studies the question of universal approximation with neural networks under general symmetries. For this purpose, the authors first leverage existing universal approximation results with shallow fully connected networks defined on infinite dimensional input spaces, that are then upgraded to provide Universal Approximation of group equivariant functions using group equivariant  convolutional networks.   Reviewers were all appreciative of the scope of this paper, aimed at unifying different UAT results under the same underlying  master theorem , bringing a much more general perspective on the problem of learning under symmetry. However, reviewers also expressed concerns about the accessibility and readibility of the current manuscript, pointing at the lack of examples and connections with existing models/results. Authors did a commendable job at adding these examples and incorporating reviewers feedback into a much improved revision.   After taking all the feedback into account, this AC has the uncomfortable job of recommending rejection at this time. Ultimately, the reason is that this AC is convinced that this paper can be made even better by doing an extra revision that helps the reader navigate through the levels of generality. As it turns out, this paper was reviewed by three top senior experts at the interface of ML and groups/invariances, who themselves found that the treatment could be made more accessible   thus hinting a difficult read for non experts. In particular, the main result of this work (theorem 9) is based on a rather intuitive idea (that one can leverage UAT for generic neural nets on the generator of an equivariant function), that requires some technical  care  in order to be fleshed out. The essence of the proof can be conveyed in simple terms, after which following through the proof is much easier. Similarly, the paper quickly adopts an abstract (yet precise) formalism in terms of infinite dimensional domains, which again clouds the essential ideas in technical details. While the paper now contains several examples, this AC believes the authors can go to the extra mile of connecting them together, and further discussing the shortcomings of the result   in particular, the remarks on tensor representations and the invariant case are of great importance in practice, and should be discussed more prominently. Finally, while this work is only concerned with universal approximation, an important aspect that is not mentioned here is the quantitative counterpart, ie what are the approximation rates for symmetric functions under the considered models.   
The authors propose a multi scale architecture for generative flows that can learn which dimensions to pass through more flow layers based on a heuristic that judges the contribution to the likelihood. The authors compare the technique to some other flow based approaches. The reviewers asked for more experiments, which the authors delivered. However, the reviewers noted that a comparison to the SOTA for CIFAR in this setting was missing. Several reviewers raised their scores, but none were willing to argue for acceptance. 
The paper considers an extension of randomized smoothing where the smoothing noise may differ for different points. The resulting method shows good performance experimentally. However, the reviewers raised a number of problems which, at the moment, precludes the acceptance of the paper, such as the following:    The paper analyzes the transductive setting, where all the test points are available to fine tune the smoothing parameters of the predictor. It is not clear how this setting corresponds to a real adversarial threat model, and whether the final tuning needs to use the perturbed or unperturbed points. In the first case, the resulting certified radius is different from what is normally used in the literature, while in the latter it is not clear how the method would be useful to mitigate any real adversarial attack.   A related comment is that the paper should explain (and state) properly how the results of Cohen et al.  (2019) are applicable to compute the certified radius, which would also provide a proper explanation why partitioning is used.   The training cost of the procedure seems very high, and this is not discussed.   The clarity of the presentation should be improved.    
The paper initially received negative reviews; the authors did a good job during the response period: two reviewers have updated their scores to 6. The AC has carefully read the reviews, responses, and discussions, and agreed that the authors have also mostly addressed the concerns of reviewer gsUt as well. It is unprofessional for reviewer gsUt to not engage in discussions after multiple requests.  The AC however also agrees with reviewer seqp that the new changes are major, and submissions are supposed to be evaluated in their initial form. Further, neither of the positive reviewers would like to champion the paper.   The final recommendation is to reject the paper. The authors are encouraged to further improve and flesh out the paper based on the reviews for the next venue.
The proposed ConVIRT learns representations of medical data from paired image and text data. While the paper addresses a relevant problem, the reviewers agree that the method has limited novelty. Two reviewers find and that the experiments are not convincing. One reviewer notes that the paper does not compare to the state of the art methods for the tasks. 
This paper investigates learning to represent edit operations for two domains: text and source code. The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). The technical novelty is relatively weak.  Pros: The paper introduces a new dataset for source code edits.    Cons: Reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. As a result, R3 updated their score from 4 to 6.   Verdict: Possible weak accept. None of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real world application of the automatic edits). However, the overall impact and novelty of the paper is relatively weak.
The paper deals with the problem of adjusting the learning rate during gradient descent optimisation. Unfortunately the proposed approach is very similar to methods already presented in the literature and no significant contribution can be recognised. During the rebuttal, the author(s) have acknowledged their ignorance about the relevant literature and provided some further clarifications that did not turn into a revision of the reviewers’ initial assessment of the work.
This article presents novel distillation based methods for neural network training and uncertainty estimation. While the idea is interesting, there is a general agreement amongst reviewers that the paper lacks clarity, adequate discussion of the relevant literature  and comparisons to existing work. Although the revision uploaded by the authors goes in the right direction by adding some experiments and clarifying some of the issues raised by the reviewers, further work is needed to make the submission stronger.
This paper considers a new setting of contextual bandits where the learning agent has the ability to perform interventions on targeted subsets of the population. The problem is motivated from software product experimentation but with more general applicability. The paper provides a method under this setting, with both empirical and theoretical support. Reviewers agree that this is an interesting setting, and the paper contributes new results. The initial concerns on assumptions, correctness and experiments were addressed in the rebuttal. I thus recommend accept. The authors should include the response carefully in the final version.
The paper presents a method for visual robot navigation in simulated environments. The proposed method combines several modules, such as mapper, global policy, planner, local policy for point goal navigation. The overall approach is reasonable and the pipeline can be modularly trained. The experimental results on navigation tasks show strong performance, especially in generalization settings. 
The idea of combining instance level contrastive loss and deep clustering is a promising direction in recent unsupervised/self supervised visual representation learning studies. However, authors did a poor literature review and did not cite and compare with quite a few recent popular work exploring the similar direction. The proposed methodology is not particular novel and the experimental results are also not convincing. Overall, the paper explored a promising research direction, but the paper quality is clearly below acceptance bar. 
Three reviewers are positive, while one is negative. The negative reviewer is well qualified, but the review is not persuasive. Overall, this paper should be published as a wake up call to the research community. Unfortunately, the lesson of this paper is similar to that of several previous papers, in particular  Armstrong, T. G., Moffat, A., Webber, W., & Zobel, J. (2009, November). Improvements that don t add up: ad hoc retrieval results since 1998. In Proceedings of the 18th ACM conference on Information and knowledge management (pp. 601 610).  This submission should be a spotlight, to maximize the chance that future researchers learn its lesson.
The reviewers disagree strongly on this paper. Reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. Reviewer 3 however, was underwhelmed by the results. Reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multi task learning approaches.  After considering all of the discussion so far, I have to agree with reviewer 2 on their assessment. Much of the meta learning literature involves changing the base learner *for a fixed architecture* and seeing how it affects performance. There is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. This is best done by considering carefully controlled and well understood experimental settings. Even still, the performance is quite good relative to popular base learners.  Regarding novelty, I agree it is a simple change to the base learner, using a technique that has been tried before in other settings (linear regression as opposed to classification), however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pre trained CNN features helps to demonstrate the utility of its use in the meta learning settings.  While the novelty can certainly be debated, I want to highlight two reasons why I am opting to accept this paper: 1) simple and effective ideas are often some of the most impactful. 2) sometimes taking ideas from one area (e.g., multi task learning) and demonstrating that they can be effective in other settings (e.g., meta learning) can itself be a valuable contribution. I believe that the meta learning community would benefit from reading this paper. 
All the reviewers and myself have concerns about the potentially incremental nature of this work. While I do understand that the proposed method goes beyond crafting minibatch losses, and instead parametrizes things via a neural network, ultimately it s roughly very similar to simply combining MMD and minibatch discrimination and "learning  the kernel". The theoretical justifications are interesting, but the results are somewhat underwhelming (as an example, DANN s are by no means the state of the art on MNIST >MNIST_M, and this task is rather contrived; the books dataset is not even clearly used by anyone else).  The interesting analysis may make it a good candidate for the workshop track, so I am recommending that.
Authors present a technique to learn embeddings over physiological signals independently using univariate LSTMs tasked to predict future values. Supervised methods are them employed over these embeddings. Univariate approach is taken to improve transferability across institutions, and Shapley values are used to provide interpretable insight. The work is interesting, and authors have made a good attempt at answering reviewers  concerns, but more work remains to be done.  Pros:   R1 & R3: Well written.   R3: Transferrable embeddings are useful in this domain, and not often researched.  Cons:    R3: Method builds embeddings that assume that future task will be relevant to drops in signals. Authors confirm.   R3: Performance improvement is marginal versus baselines. Authors essentially confirm that the small improvement is the accurate number.   R2 & R3: Interpretability evaluation is not sufficient. Medical expert should rate interpretability of results. Authors did not include or revise according to suggestion.  
This submission explores how certain common padding choices can induce spatial biases in convolutional networks. It looks into alternative padding schemes which mitigate these issues and demonstrates significant performance improvements in widely used convnets. Reviewers generally agreed that this is an important point that should be more widely understood in the community, and that the proposed changes are relatively simple to adopt, so this work is likely to be impactful. Most reviewers thought the paper was well written, describing the problem well, and the analysis well executed. Most reviewers acknowledged that most of the weaknesses described in their initial reviews were well addressed by the authors  responses and manuscript updates. Given the strength of the analysis and the impact for many practitioners, I recommend the submission be accepted with a spotlight presentation.
The submission considers a new attack model for adversarial perturbation in a framework where the attacker has neither access to the trained model nor the data used for training the model. The submission suggests a"domain adaptation inspired attack": learn a different model on a similar domain and generate the adversarial perturbations using that model. The authors then also develop a defense for this type of attack and provide some empirical evaluations of the resulting losses on a few NLP benchmark datasets.  The paper refers to the literature on domain adaptation theory to motivate their suggested defense, but this analysis remains on an intuitive (rather than a formally rigorous) level. Furthermore, the empirical evaluation does not compare to a variety of attacks and the defense is only evaluated with respected to the self suggested attack. This is a very minimal bar for a defense to meet.  The reviewers have criticized the submission for the rather minimal extend of empirical evaluation. Given that the submission also doesn t provide a sound theoretical analysis for the  proposed attack and defense, I agree with the reviewers that the submission does not provide sufficient novel insight for publication at ICLR.   In contrast to some of the reviewers, I do find it legitimate (and maybe recommendable even) to focus on one chosen application area such as NLP. I don t see a requirement to also present experiments on image data or re inforcement learning applciations. However, I would recommend that the authors highlight more explicitly what general lessons a reader would learn from their study. This could be done through a more extensive and systematic set of experiments or a through analysis in a well defined theoretical framework.
Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The greatest concern was that the novelty beyond past work has not been sufficiently demonstrated.
The paper proposes a new sparse matrix representation based on Viterbi algorithm with high and fixed index compression ratio regardless of the pruning rate.  The method allows for faster parallel decoding and achieves improved compression of index data storage requirement over existing methods (e.g., magnitude based pruning) while maintaining the pruning rate. The quality of paper seems solid and of interest to a subset of the ICLR audience.
Reviewer ratings varied radically (from a 3 to an 8). However, the reviewer rating the paper as 8 provided extremely little justification for their rating. The reviewers providing lower ratings gave more detailed reviews, and also engaged in  discussion with the authors. Ultimately neither decided to champion the paper, and therefore, I cannot recommend acceptance.
This paper studies the important statistical phenomenon of double descent, a very timely topic, using influence functions, and thereby derives lower bounds for the population loss. The reviewers generally appreciated the conceptual as well as the technical contributions in the work, but argued that the set of assumptions taken by authors can potentially diminish the significance of the analysis. This, as well as additional issues regarding the empirical and the analytical support for the modeling assumptions (and the implied scope of applicability, i.e. lazy\kernel vs. rich regime) have generated considerable discussion between the reviewers and the authors. Along the process, major and minor concerns were addressed to the satisfaction of the reviewers, resulting in a substantial improvement of the overall evaluation. Thus, this AC recommends acceptance.
The paper advocates empowerment for stabilising dynamical systems, the dynamics of which is estimated with Gaussian channels. Baseline comparisons have improved and that makes the experimental section good. While initial versions of the paper were problematic, all reviewer issues have been addressed and acceptance is almost unanimous.
This paper presents Capacity Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity.  The reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing.  Although the ideas presented in the paper are interesting, more work is required for this to be accepted. Therefore at this point, this is unfortunately a rejection.
The reviewer seems to reach a consensus that the paper is not ready for publication at ICLR. One of the major issues seems to be that the paper only analyzes the case of $d 2$. (In the AC s opinion,  $d>2$ might be fundamentally more difficult to analyze than $d 2$).   
The authors propose the use of an ensembling scheme to remove over estimation bias in Q Learning. The idea is simple but well founded on theory and backed by experimental evidence. The authors also extensively clarified distinctions between their idea and similar ideas in the reinforcement learning literature in response to reviewer concerns.
 Description:  The paper presents a generative model, SketchEmbedNet, for class agnostic generation of sketch drawings from images. They leverage sequential data in hand drawn sketches. Results shows this outperforms STOA on few shot classification tasks, and the model can generate sketches from new classes after one shot.  Strengths:   Detailed, technically sound, presentation    Shows that enforcing the decoder to output sequential data leads to a more informative internal representation, and thus generate better quality sketches    Improves over unsupervised STOA methods  Weaknesses:   Experiments are done against methods that do not use the sequential aspect of sketches. Because ground truth in this case contains much more data, comparison is not quite fair.   Will have been useful to see results against a baseline that uses it.   Quality of sketches generated from natural images is low
The reviewers vary in their scores but overall there is agreement that this paper is not ready for acceptance. 
AR1 asks for a clear experimental evaluation showing that capsules and dynamic routing help in the GCN setting. After rebuttal, AR1 seems satisfied that routing in CapsGNN might help generate  more representative graph embeddings from different aspects . AC strongly encourages the authors to improve the discussion on these  different aspects  as currently it feels vague. AR2 is initially concerned about experimental evaluations and whether the attention mechanism works as expected, though, he/she is happy with the revised experiments. AR3 would like to see all biological datasets included in experiments. He/she is also concerned about the lack of ability to preserve fine structures by CapsGNN. The authors leave this aspect of their approach for the future work.  On balance, all reviewers felt this paper is a borderline paper. After going through all questions and responses, AC sees that many requests about aspects of the proposed method have not been clarified by the authors. However, reviewers note that the authors provided more evaluations/visualisations etc. The reviewers expressed hope (numerous times) that this initial attempt to introduce capsules into GCN will result in future developments and  improvements. While AC thinks this is an overoptimistic view, AC will give the authors the benefit of doubt and will advocate a weak accept.  The authors are asked to incorporate all modifications requested by the reviewers. Moreover,  Graph capsule convolutional neural networks  is not a mere ArXiV work. It is an ICML workshop paper. Kindly check all ArXiV references and update with the actual conference venues.
The submission focuses on a set norm normalization layer for neural network models, which stands in contrast to a batch norm.  The majority of the reviewers felt that this submission is not suitable for publication at ICLR in its current form.  These concerns remained after the post rebuttal discussion.  Quoting from the reviewer discussion, the following points remained as significant concerns:   1. lack of novelty. normalization layers have been used previously in other sets architectures. The systematic approach for normalization in the current paper is nice but I am not sure how valuable it is. There exists already extensive literature on normalization layers for graphs (a generalization of sets). 2. lack of motivation for deep networks. The main claim in the paper (see the introduction and figure 1) is that 50 layers should perform better and since it does not seem to be the case in figure 1, it requires studying normalization layers. I am not sure it is a well established claim. What are the assumptions leading to the conclusion that deep architectures should perform better on the task considered in figure 1? I am also concerned that normalization layers have a major effect on improving "not so deep" networks with 3 10 layers and not only the extreme 50 layers case, making the comparison in the paper between only 3 and 50 layers not enough for telling the full story. Thus evaluation on more depths is required.  On the balance, the paper does not meet the threshold for acceptance in this round of peer review.
The paper introduces an interesting application of GNNs, but the reviewers find that the contribution is too limited and the motivation is too weak.
This paper proposed a compositional approach to (conditionally) steer pre trained music transformers to the direction intended by the user.  Overall the scores are mostly negative. The reviewers pointed out some interesting aspects of the paper (e.g., using hard binary constraints as opposed to the soft ones, the contrastive approach). However, one common issue shared by all the reviewers is the clarity of the presentation, which led to many reviewers being confused about various aspects of the paper especially the empirical evaluation. The authors did provide a detailed response to address some of the concerns, but to fully address all the points I anticipate it would require quite substantial change to the paper. A couple reviewers also raised the concerns regarding the limited contribution of the paper. Finally, there appears to be some disagreement between the authors and reviewers regarding how to interpret the listening test results. I hope the authors can take the comments into consideration to further improve this paper for the next submission.
The paper proposes a hierarchical meta imitation learning framework for few shot transfer in the context of long horizon control tasks. Underlying the framework is a hierarchical adaptation of model agnostic meta learning (MAML) that jointly learns the high level policy together with the set of modular low level policies (sub skills), both of which are fine tuned at test time based on a small number of demonstrations. Experimental evaluations on the meta world benchmark as well as a kitchen environment benchmark compare the proposed framework with recent baselines.  As several reviewers note, the problem of jointly learning modular policies together with the high level policy for composing these sub skills is both challenging and interesting to the robotics and learning communities. The manner by which the paper extends existing work in meta learning (MAML) and hierarchical imitation learning is novel and technically sound. The reviewers raised some concerns, notably those regarding (1) the the framework s sensitivity to various hyperparameter settings and its ability to generalize to other domains; (2) the merits of joint optimization over decoupled optimization of the sub skills and high level policy; and (3) the need for experiments/evaluations on different domains. The authors provided a detailed response to each of the reviewers that includes the addition of a different benchmark evaluation (the kitchen environment), new ablation studies, and updates to the text. After a thorough review, however, concerns remain regarding the reproducibility of the results, which call into question some of the key contributions that the paper claims to provide over the existing state of the art. The authors are encouraged to provide a more balanced discussion of the contributions along with evidence to support reproducibility in any future version of the paper.
While the use of RNNs for building session based recommender systems is certainly an important class of applications, the main strength of the paper is to propose and benchmark practical modifications to prior RNN based systems that lead to performance improvements. The reviewers have pointed out that the writing in the paper needs improvement,  modifications are somewhat straightforward and some expected baselines such as comparisons against state of the art matrix factorization based methods is missing. As such the paper could benefit from a revision and resubmission elsewhere.
Main content:  Blind review #3 summarized it well, as follows:  This paper studies knowledge distillation in the context of non autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN >ES/FR/DE, the authors argue that this necessity arises from the overly multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT.   Based on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model.  The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.     Discussion:  Questions were mostly about how robust the results were on other language pairs and random starting points. Authors addressed questions reasonably.  One low review came from a reviewer who admitted not knowing the field, and I agree with the other two reviewers.     Recommendation and justification:  I think papers that offer empirically support for scientific insight (giving an "a ha!" reaction), rather than massive engineering efforts to beat the state of the art, are very worthwhile in scientific conferences. This paper meets that criteria for acceptance.
The paper relies on the analytical tools afforded by on the NTK theory to proposes an adversarial attack that uses the information of the model structure and training data, without the need to access the model under attack. While the reviewers found the problem interesting and well motivated, they feel that the theoretical analysis and the experimental results can be significantly improved. In particular, some of the points that the reviewers did not find convincing during the discussion include: (1) the technical novelty of the work, i.e., applying adversarial attack on NTK at inference time seems a trivial extension of PGD attack; (2) authors  argument that knowing the model is strictly stronger than knowing the original training data; (3) scalability and generalization of the proposed method to settings without training and test set; and (4) comparison to existing sota transfer attacks in the same setting, like no box attack. Addressing the above points will significantly improve the manuscript.
Motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment. Inspired by childhood psychology, they propose a variant of hide and seek game called "Cache" built on top of AI2 THOR, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment. They examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images.  The paper is well written and motivated, and easy to follow. All reviewers agree that the paper will be a great contribution to the ICLR community. I believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively (via static image recognition models), it synthesizes ideas from various disciplines (psychology, embodiment, ML) and provides an excellent framework for future research. For these reasons I m recommending we accept this work as an Oral presentation.
This paper presents an interesting idea: employ GANs in a manner that guarantees the generation of differentially private data.  The reviewers liked the motivation but identified various issues. Also, the authors themselves discovered some problems in their formulation; on behalf of the community, thanks for letting the readers know.  The discovered issues will need to be reviewed in a future submission.
This paper addresses a very interesting topic, and the authors clarified various issues raised by the reviewers. However, given the high competition of ICLR2020, this paper is unfortunately still below the bar. We hope that the detailed comments from the reviewers help you improve the paper for potential future submission.
The submission proposes a setting of two agents, one of them probing the other (the latter being the "demonstrator"). The probing is done in a way that learns to imitate the expert s behavior, with some curiosity driven reward that maximizes the chance that the probing agents has the expert do trajectories that the probing agent hasn t seen before.  All the reviewers found the idea and experiments interesting. The major concern is whether the setup and the environments are too contrived. At least 2 reviewers commented on the fact that the environments/dataset seemed engineered for success of the given method, which is a concern about how this method would generalize to something other than the proposed setup.  I also share the concern with R3 regarding the practicality of the proposed method: it is not obvious to me what problems this would actually be *useful* for, given that the method requires online interaction with an expert agent in order to succeed. The space of such scenarios where we can continuously probe an expert agent many many times for free/cheap is very small and frankly I m not entirely sure why you would need to do imitation learning in that case at all (if the method was shown to work using only a state, rather than requiring a state/action pair from the expert, then maybe it d be more useful).  It s a tough call, but despite the nice results and interesting ideas, I think the method lacks generality and practical utility/significance and thus at this point I cannot recommend acceptance in its current form.
The authors set up a simple combination of an energy based model and a flow based model that corrects the flow based model with an energy based term. The merits of this relative only an energy based model is improved sampling to compute the gradient. The advantage over a only flow based model is that the kinds of transforms that can be used are less limited.
This paper trains a neural network to predict expert strategies (described in natural language) in the game of Angry Birds. While the reviewers agreed this was potentially interesting, there was also a consensus that the scope of the paper was too narrow, that the writing was imprecise, and that the evaluations too few and too qualitative. I agree the paper does not seem thorough enough for ICLR, and recommend rejection.
This paper proposes a new implementation of a previously proposed two stage process for video prediction: first predict future segmentation maps, then map them to video frames. Combined with other advances in video prediction and image generation, this simple idea is shown empirically to work very well, producing video predictions up to many hundreds of frames into the future in real stochastic settings with unprecedented quality. Strong ablation studies over the course of the review process further serve to confirm the value of various design choices involved in the implementation. 
The authors propose a framework for improving the robustness of neural networks to adversarial perturbations via optimal control techniques (Lyapunov Stability and the Pontryagin Maximum Principle, in particular). By considering a continuous time limit of the training process, the authors use the PMP to derive udpate rules for the neural network weights that would result in a robust network. While the approach is interesting, the paper has some serious deficiencies that make it unacceptable for publication in its current form:  1. Quality of empirical evaluation: The authors only report final numbers on CIFAR 10 for a fixed set of adversarial attacks. It has been observed repeatedly in the adversarial robustness literature that adversarial evaluation of neural networks has to be done carefully to guard against possible underestimation of the worst case attack. In particular, the specific details of the adversarial attacks used (number of steps, number of initializations, performance under larger perturbation radii) that are necessary to trust the results are not given (see https://arxiv.org/pdf/1902.06705.pdf for example).  2. Unclear novelty: The authors do not sufficiently explain the novelty in their approach relative to prior work (particular prior work that has used optimal control ideas in this context).  3. Computational cost: The authors do not give sufficient details to judge the computational overhead of their method to judge how much more expensive it is to train with their approach relative to standard or adversarial training.  While one reviewer voted for a weak accept, the other reviewers were in consensus on rejection. The authors did not respond during the rebuttal phase and hence the reviews were unchanged.  In summary, I vote for rejection. However, I think this paper has potentially interesting ideas that should be carefully developed and evaluated in a future revision.
 The paper theoretically investigates two bias correction methods, reweighting and resampling. It considers a very interesting problem and presents illuminating results. The paper could benefit substantially from improving the experiments so that they clearly validate the theoretical results presented.
The authors have provided very detailed responses and added additional experimental results, which have helped address some of the referees  concerns. However, since the modification made to a vanilla GAN algorithm is relatively small, the reviewers are hoping to see the experiments on more appropriate real world datasets (not artificially created imbalanced datasets with relatively few classes), more/stronger baselines, and rigorous theoretical/empirical analysis of the method s sensitivity to the quality of the pre trained classifier. The paper is not ready for publication without these improvements. 
This paper examines the implicit bias of gradient descent of linear group equivalent convolutional neural networks with a single channel and full dimensional kernel when trained on separable data with exponential loss. The main result is that the linear predictor converges in direction to the first order stationary point of the minimum 2/L Schatten matrix norm max margin problem, under some assumptions. This generalizes previous results on linear convolutional neural networks.  I appreciated this paper states the theorems in terms of general group operations; if done correctly and written well, this can be a good reference for future papers. But I think this paper needs a little more work before getting there, as I explain below.  The reviewers were borderline (6,6,6,5) and did not have high confidence. Some stated clarity issues, other criticized the model being used: either that (1) only the case of single channel and full dimensional kernel we examined, or that (2) the full model is not actually invariant. Given previous results (Jagadeesan et al.) I am OK with (1). I find (2) problematic, but not enough to be a reason to reject the paper.   So I took a closer look.  First, I felt that indeed the paper writing could be improved. Specifically, the notation could be better explained (e.g., the h and g functions in eq. 3 are not defined: what are their range and domain?), and more discussions and examples can be added throughout the paper to clarify the significance of the results.  Second, the experimental results in the non Abelian case (figure 4a) and non linear case (figure 5) seemed somewhat weak (not so sparse) after I noticed the y axis does not start from zero, as in Figure 3a.  Most importantly, looking at the proofs, I felt they were rather incremental, as I explain next. The authors claim their main result, Theorem 5.4, does not follow from Yun et al. s paper. But Gunsekar et al. 2018b already had KKT condition results for max margin in parameter space, and even stronger results are in [1] (which the authors should cite and discuss clearly). These already give a stronger version of Theorem A.6. So the main extra contribution here is to extend it to a guarantee on function space (in the space of linear functions beta).   But for L>2, I unless I am missing something, I feel this is straightforward, by using results like in [2], where they relate subdifferentials of unitarily invariant matrix functions to the corresponding vector subdifferentials on the singular values (and in the vector case, the subdifferential is trivial).   The L 2 is not trivial as we need to show the condition in Assumption A.7, which is the most technical part of Gunsekar et al. / Yun et al. papers, and is often non trivial. The authors in this paper, however, did not show this and rather leave to future work in the last paragraph of Appendix A.  I think that this is a concrete opportunity to make the paper better, perhaps following the same methodology in Gunsekar et al. / Yun et al. papers.  Minor comments:  1) The informal Theorem 1 should state we converge to the f.s.p. of eq. 1, not a solution of eq. 1.   2) The main paper is non searchable, which makes it harder to read. 3) Many hype refs in the appendix do not work well (they get me to some random page).  [1] K. Lyu, and J. Li. "Gradient descent maximizes the margin of homogeneous neural networks." 2019. [2] A. S. Lewis  The Convex Analysis of Unitarily Invariant Matrix Functions, 1995
This manuscript expands the range of recent work in reinforcement learning for language games to much larger and more realistic datasets. A timely and relevant contribution and one that is well evaluated. Further work in stabilizing RL approaches for such large scale problems is likely to have other far reaching consequences. Reviewers were unanimous that this is a strong submission after the author discussion period.
The reviewers positively valued the proposed idea of performing permutation selection in permutation decoding via combining node embedding and self attention, which seems to be of high originality. I found that this paper is mostly clearly written, except Section 3.2 as AnonReviewer5 commented. The main concern among the reviewers is regarding applicability of the proposal beyond the BCH codes.   Pros:   The proposal of utilizing self attention for permutation selection in permutation decoding is novel and interesting.   Computational complexity in the decoding phase is only slightly increased compared with random permutation selection, and is far smaller than performing decoding for all permutations. The GPS classifier can be parallelizable to further reduce latency.   The proposal should be applicable beyond the BCH codes to those with decoding based on the Tanner graph, including polar codes.  Cons:   Only the BCH codes were considered, whereas in the authors responses they will add a short analysis on polar codes.   It seems that systematic enumeration of the PG is required, which would limit applicability of the proposal.   There is a room for improvement in presentation:   In Section 3.2, the description of "positional encodings" was unclear to me, in that the ordering of the codeword entries is arbitrary, unlike typical sequence transduction problems to which attention mechanism is being applied.   Dependence of the input vector sequences of the attention head on the permutation $\pi$ is not clearly explained.   Performance of the proposed method might depends on choices of the parity check matrix, which is however not discussed in this paper at all.  Based on the above concerns, the paper is not yet ready for publication in its current form.  Minor points: In page 3, line 14, "that" should be deleted. In references list, "hdpc" should be in capital. "reed muller" should be "Reed Muller".
The paper proposes a lossless image compression consisting of a hierarchical VAE and using a bits back version of ANS. Compared to previous work, the paper (i) improves the compression rate performance by adapting the discretization of latent space required for the entropy coder ANS (ii) increases compression speed by implementing a vectorized version of ANS (iii) shows that a model trained on a low resolution imagenet 32 dataset can generalize its compression capabilities to higher resolution.  The authors addressed properly reviewers  concerns. Main critics which remain are (i) the method is not practical yet (long compression time) (ii) results are not state of the art   but the contribution is nevertheless solid.
This paper presents an approach that combines rule lists with prototype based neural models to learn accurate models that are also interpretable (both due to rules and the prototypes). This combination is quite novel, the reviewers and the AC are unaware of prior work that has combined them, and find it potentially impactful. The experiments on the healthcare application were appreciated, and it is clear that the proposed approach produces accurate models, with much fewer rules than existing rule learning approaches.  The reviewers and AC note the following potential weaknesses: (1) there are substantial presentation issues, including the details of the approach, (2) unclear what the differences are from existing approaches, in particular, the benefits, and (3) The evaluation lacked in several important aspects, including user study on interpretability, and choice of benchmarks.  The authors provided a revision to their paper that addresses some of the presentation issues in notation, and incorporates some of the evaluation considerations as appendices into the paper. However, the reviewer scores are unchanged since most of the presentation and evaluation concerns remain, requiring significant modifications to be addressed.
The reviewers enjoyed reading about an interesting take on lifelong learning, encapsulating an EM methodology for selecting a transfer configuration and then optimizing the parameters. R3 made valid concerns regarding comparison with previous, recent work. R2 also would prefer to see more thorough experiments (ideally in settings where multiple tasks exist, as also commented by R4). During the rebuttal phase the authors made a good effort to run additional experiments which cover the related work aspect better. These experiments and the overall paper were discussed extensively among reviewers after the rebuttal phase.  In the discussions, the reviewers agreed that an interesting idea can be publishable even if it does not achieve SOTA results in all scenarios, as long as it brings new perspectives and shows at least comparable results. However, in the particular case of this paper, there exist remaining concerns regarding the usefulness and applicability of the method. Specifically, the paper could benefit from a more convincing demonstration about how the method can scale (e.g. R3 and R4’s comments), especially since training time and model capacity are important factors to consider for practical continual learning scenarios. Furthermore, it is not clear how the proposed method can be used in combination with other machine learning tools within a continual learning application, for example by leveraging modern deep architectures or by complementing existing adaptive knowledge approaches (as discussed by R3).   Although the opinions of the reviewers are not fully aligned, this borderline paper seemed to lack an enthusiastic endorsement by a reviewer to compensate for the concerns discussed above and the relatively weak experimental results. Therefore I recommend rejection.  
The paper investigates parallelizing MCTS.                                                                                          The authors propose a simple method based on only updating the exploration bonus                                                    in (P) UCT by taking into account the number of currently ongoing / unfinished                                                                simulations.                                                                                                                        The approach is extensively tested on a variety of environments, notably                                                             including ATARI games.                                                                                                                                                                                                                                                  This is a good paper.                                                                                                               The approach is simple, well motivated and effective.                                                                               The experimental results are convincing and the authors made a great effort to                                                      further improve the paper during the rebuttal period.                                                                               I recommend an oral presentation of this work, as MCTS has become a                                                                 core method in RL and planning, and therefore I expect a lot of interest in the                                                     community for this work.                                                                                                                                     
Main summary:  Sngle image super resolution network that can generate high resolution images from the corresponding C JPG images  Discussions reviewer 3: reviewer has a few issues including, claim the method is lossless, want more information about JPG revovering step reviewer 1: (not knowledgable): paper is well written and reviewer gives very few cons reviewer 2: main concerns are wrt novelty and technically sound Recommendation: the 2 more knowledgable reviwers mark this as Reject, I agree.
This work examines how to deal with multiple classes. Unfortunately, as reviewers note, it fails to adequately ground its approach in previous work and show how the architecture relates to the considerable research that has examined the question beforehand.
This paper studies the problem of Pareto fairness without having pre defined protected groups. The reviewers agree that the problem studied here is interesting and relevant. During the initial review period, reviewers identified a major correctness issue. The authors have then substantially changed the algorithm and experiments in the rebuttal period in order to address the issues. Now the convergence result in the paper follows more directly from the prior work of Chen et al. Overall, the technical novelty of the paper appears to be limited. Even though the authors have also strengthened the related work discussion, they should also consider discussing the comparison between their work with that of Lahoti et al., as suggested by one of the reviewers.  
The paper did not strike any reviewer as a critical addition to the literature, including various concerns regarding (1) the use of the general theory of relativity, (2) some components are well known in the past works.
This paper studies the problem of computing a similarity measure between two pieces of code. The main contributions are a configurable alternative (CASS) to abstract syntax trees (ASTs) for representing code and a model for embedding these structures within a Siamese net like architecture. While parts of the ICLR community that make use of ASTs would likely find interest in the options provided by CASS and the associated experiments, the contribution is mostly around feature engineering of AST like structures for one specific application, which is quite niche. The machine learning modeling appears fairly standard. Thus in total, I don’t see enough here to recommend acceptance.
This submission received a diverging set of the final ratings: 6, 3, 6, 5. On the positive side, reviewers appreciated practicality of the approach and supporting empirical results. At the same time, all of them expressed concerns with the presentation (typos, unfinished sentences, inconsistent notations). Additional requests for clarifications and ablation studies have been mainly addressed in the rebuttal. The most skeptical reviewer did not participate in the post rebuttal discussion, thus the final decision took that into account..  The AC has read the paper and verified that the minor technical issues pointed out by the reviewers have been fixed in the updated version (there are still a couple of typos remaining). This submission was further discussed between AC and SAC, as well as in the PC calibration meeting. Both AC and SAC agreed with the comment of Reviewer aAcK who pointed out that generating adversarial samples for mining hard examples has been explored in more general but related contexts before, which limits the novelty of this work to an application of a known idea to a particular domain (3D). At the same time, performance gains on the ModelNet40 dataset are marginal compared to the point cloud based baselines, while the proposed method still uses point clouds for generating adversarial views. In combination with other minor issues pointed out by the reviewers, and given that none of the reviewers was championing the paper, AC and SAC believe that the weaknesses of this paper at the end outweigh its strengths and do not recommend acceptance at this stage.
This is a solid paper providing the first theoretical convergence result for NAS and showing promising empirical results. The authors  proposed GAEA method can be combined with different types of weight sharing algorithms (DARTS, PC DARTS, etc) and is likely to reduce the impact of the architecture discretization step due to finding sparser solutions. I clearly recommend acceptance and would expect this to make a nice spotlight.
This work combines style transfer approaches either in a serial or parallel fashion, and shows that the combination of methods is more powerful than isolated methods. The novelty in this work is extremely limited and not offset by insightful analysis or very thorough experiments, given that most results are qualitative. Authors have not provided a public response. Therefore, we recommend rejection.
Training GANs to generate discrete data is a hard problem. This paper introduces a principled approach to it that uses importance sampling to estimate the gradient of the generator. The quantitative results, though minimal, appear promising and the generated samples look fine. The writing is clear, if unnecessarily heavy on mathematical notation.
The paper proposed an imputation free method to handle missing data by learning an input encoding matrix using RL with the prediction error as reward/penalty signal. Reviewers appreciate the interesting setup where RL is used to deal with missing data, and the method being imputation free. Three out of four reviewers (reviewer he3p, azSY, and 4Cb5) have raised concerns on the complexity of the proposed method, but it seems like all the reviewers see the strength of the work outweigh the weakness.
This paper proposes model poisoning (poisoned parameter updates in a federated setting) in contrast to data poisoning (poisoned training data). It proposes an attack method and compares to baselines that are also proposed in the paper (there are no external baselines). While model poisoning is indeed an interesting direction to consider, I agree with reviewer concerns that the relation to data poisoning is not clearly addressed. In particular, any data poisoning attack could be used as a model poisoning attack (just provide whatever updates would be induced by the poisoned data), so there is no good excuse to not compare to the existing strong data poisoning attacks. One reviewer raised concerns about lack of theoretical guarantees but I do not agree with these concerns (the authors correctly point out in the rebuttal that this is not necessary for an attack focused paper). I do feel there is room to improve the overall clarity/motivation (for instance, equation (1) is presented without any explanation and it is still not clear to me why this is the right formulation).
This paper addresses a continuous time formulation of gradient based meta learning (COMLN) where the adaptation is the solution of a differential equations. In general, outer loop optimization requires backpropagating over trajectories involving gradient updates in the inner loop optimization. It is claimed that one of main advantages of COMLN is able to compute the exact meta gradients in a memory efficient way, regardless of the length of adaptation trajectory. To this end, the forward mode differentiation is used, with exploiting the Jacobian matrix decomposition. All the reviewers agree that the derivation of memory efficient forward mode differentiation is a significant contribution in the few shot learning. The paper is well written and has interesting contributions. Authors did a good job in responding to reviewers’ comments during the discussion period. What is missing in this paper is the discussion of some limitations of the proposed method.  This can be improved in the final version. All reviewers agree to champion this paper. Congratulations on a nice work.
For pairs of pieces of text, the central idea of this paper is to combine the approaches of using bi encoders (where a vector is formed from each text then compared), which are easily trained in an unsupervised manner, with cross encoders (where the two texts are related at the token level), which are normally trained in a supervised manner. The chief contribution of this paper is to train a combined model (as a "trans encoder") by doing co training of both model types in an alternating cyclic self distillation framework. The paper suggests that this allows unsupervised training of a cross encoder. This claim met some pushback from the reviewers, since the method does require good quality aligned text pairs (much like a traditional MT system does), and so the result is a  task specific sentence pair modeling approach rather than a generic unsupervised learning approach.   In the discussion, downsides included the claims of "unsupervised" being overstated, the genuine remaining need for related sentence pairs, the lack of a more theoretical understanding of why this works, and the feeling that the paper is not yet fully mature. Upsides include solid work building from existing models, big performance improvements over SimCSE, novelty in combining previous ideas in a new way for a new problem, and good experiments. To my mind, while the requirement of related sentence pairs does mean the model is task specific and less than fully unsupervised, this is still a common and useful scenario, the performance of the model is strong, and, while the proposed model is built from existing components and ideas, they are combined in an interesting new way to achieve an intriguing and strong new way of training models, and the discussion here (and now in Appendix A.2) of what the authors had to do to get the model to work in terms of choosing different losses, etc., convincingly demonstrated that the authors had thought significantly and deeply about the nature of their proposal and how to get it to work well. Moreover the authors were able to work expeditiously during the reviewing period to address other weaknesses, such as now providing results with other methods than SimCSE (DeCLUTR and Contrastive Tension) and on other language models (RoBERTa).  As such, although this paper is clearly somewhat borderline rather than an unambiguous accept, I find myself quite convinced by the novelty, thoroughness, and intriguing nature of this work, and so my vote is to accept it.
The paper presents a generative approach to learn an image representation along a self supervised scheme.    The reviews state that the paper is premature for publication at ICLR 2020 for the following reasons: * the paper is unfinished (Rev#3); in particular the description of the approach is hardly reproducible (Rev#1); * the evaluation is limited to ImageNet and needs be strenghtened (all reviewers) * the novelty needs be better explained (Rev#1). It might be interesting to discuss the approach w.r.t. "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles", Noroozi and Favaro.  I recommend the authors to rewrite and better structure the paper (claim, state of the art, high level overview of the approach, experimental setting, discussion of the results, discussion about the novelty and limitations of the approach).   
This paper studies a Markov game with a single leader and multiple followers, where the state transitions are independent of the actions of the followers.  The paper studies online and offline RL methods for this subclass of the Markov game, and establishes a sublinear regret bound for the online RL method and sublinear suboptimality of the offline RL methods.  Although the paper appears to contain interesting ideas and contributions, the responses and revisions have not sufficiently addressed some of the major concerns of reviewers.  The reviewers and AC thus agree that the paper is not ready for publication.  For example, the issue of motivation and the myopic follower setting has not been resolved yet.  Also, the discussion on the issues studied in Balcan et al. (2015) is not provided.  Please also note that (Jin et al.,  20b) also proposes an optimistic variant of LSVI, and the exact algorithmic contributions still have some unclarity.
This paper proposes a meta learning method that learns structured features based on constellation modules. Exploiting object parts and their relationships is a promising direction for few shot learning as AnonReviewer3 described. The effectiveness of the proposed method is demonstrated with experiments using standard benchmark, and ablation study.  
The authors propose a learning framework to reframe non stationary MDPs as smaller stationary MDPs, thus hopefully addressing problems with contradictory or continually changing environments. A policy is learned for each sub MDP, and the authors present theoretical guarantees that the reframing does not inhibit agent performance.  The reviewers discussed the paper and the authors  rebuttal. They were mainly concerned that the submission offered no practical implementation or demonstration of feasibility, and secondarily concerned that the paper was unclearly written and motivated. The authors  rebuttal did not resolve these issues.  My recommendation is to reject the submission and encourage the authors to develop an empirical validation of their method before resubmitting.
Unfortunately, I feel the paper is not quite ready for ICLR, even if the reviews seem in general quite positive (though of low confidence).  After reading the reviews and rebuttal, and going over the paper I have to make the following comments:  * The paper do two modification to the backbone architecture, that have an impact on the ability of these systems to continually learn; these changes are adding layer normalization and a mask   * The paper is mostly empirical in nature; while there are some intuitions presented clearly about these ideas, their efficiency is proved empirically, which is completely fine   However:   * The empirical validation seems not sufficient; the main results are permuted MNIST, incremental CIFAR 10, incremental CUB200; the results on permuted MNIST in terms of final accuracy seem surprisingly low (particularly when involving CL solutions, like EWC, ER, HAT .. see table 2; e.g. FA1 < 80% seems very surprising). This seems strange to me and adds a bit of shade on the results    * The proposed methods are simple; There is a strong message behind them, namely that the choice of the backbone (architecture size, normalization layers) has a huge impact on learning. But being a purely empirical result, this really needs to be backed up with analysis and an attempt at understanding of what is going on. E.g. looking at the masks over time .. to they converge to be task specific? Anything that would give a bit of depth to the results. Discussing the Figures (e.g. I m looking at Fig 3 and I was grep ing the text to see a discussion of how one would interpret those results). Why is FashionMNIST used to produced Fig 3, and why is not something like this done for one of the CL benchmark considered. Providing additional typical measures for CL (e.g. showing learning curves).    * Just overall does not seem that the work provides sufficient insight, or analysis.   I do think there is something really interesting in this work, and I do hope the authors will resubmit this work after some modification. And I do agree that there are many aspects of the backbone or architecture that have big impacts on CL and this is an understudied and not well understood topic. So in that sense I think the idea of this work is good. But I just feel it fails short in terms of results, analysis. I feel in the current format, the work will not have the impact it deserves.
The paper proposes to take into accunt the label structure for classification tasks, instead of a flat N way softmax. This also lead to a zero shot setting to consider novel classes. Reviewers point to a lack of reference to prior work and comparisons. Authors have tried to justify their choices, but the overall sentiment is that it lacks novelty with respect to previous approaches. All reviewers recommend to reject, and so do I.
The paper investigates the neural tangent kernel NTK of infinitely wide ensembles of soft trees having a particular  soft decision functions in their internal nodes. A closed form of the NTK is presented as well as a result bounding the changes of the NTK during training. Implications for practical training procedures are briefly discussed and  some experiments are also reported.   The review and discussion phase were difficult with two rather uninformative but positive reviews and a negative  but detailed review. The latter had, however, some serious flaws. For these reasons I carefully read the paper on my own, too. In turned out that the criticized flaws in the presentation mentioned by the negative reviewer are baseless as long as  one already knows what an NTK is. Given the title of the paper and the history of NTKs, I personally think that  such knowledge can and should be assumed.  Overall, I find the paper be actually very well written. The main issue I see is that the results are not overwhelmingly  surprising. Nonetheless, this is a solid contribution, which deserves to be published.
The manuscript proposes a method to adjust a biased model without requiring explicit annotations of biases. The main hypothesis of the manuscript is that there are differences in the direction and magnitude of the loss gradients for underrepresented samples compared to majority patterns in the training data. Based on this hypothesis, the manuscript proposes a rejection sampling method that tries to balance samples in a minibatch. However, a sample with a noisy label can appear to be an underrepresented sample with a correct label which can affect the proposed method. To tackle this, the manuscript also proposes a denoising module that successfully eliminates the effects of noisy labels on the debiasing algorithm proposed. Experiments are performed on various synthetic and real world biased sets.   Positive aspects of the manuscript includes: 1. The results for varying levels of "bias" as well as the success of the proposed "denoising" setup is remarkable for the datasets tested; 2. An interesting hypothesis about the differences between gradient magnitude and direction (as measured by its proximity to an "average" gradient direction for all samples) look different for underrepresented samples as compared to "regular" data sample.  There are also several major concerns, including: 1. Lack of motivation and analysis on the connections between per sample gradients and the majority/minority splits in more complex datasets; 2. The key assumption which motivates the proposed method, namely that minority samples have different gradient distributions than majority ones, deserves a more rigorous validation; 3. The "scalability" of the proposed method. One common theme across these datasets is that they can be "learned" (at least the biased version) with a much smaller amount of data than is present in the training set. Hence, a rejection sampling based method can work even when the minority set diminishes;  4. Assumption about known bias. The proposed method assumes knowledge about which of the factors were biased so that a suitable "bias only" model can be trained by leveraging only the "bias".   Post rebuttal, reviewers stayed with borderline ratings, and they have suggested further improvements: more details about Biased MNIST numbers (to address concerns about known bias), and ablation studies on real datasets (e.g. compare to results without denoising, or denoised using FINE) to fully justify the practical importance of proposed denoising module.
The paper studies the representation learning problem in the linear bandit setting, where each bandit "task" shares a common low dimensional representation. The paper introduces a novel algorithm, it provides theoretical regret guarantees, and it illustrates the effectiveness of the proposed method in a number of experiments.  There is a general agreement among the reviewers about the relevance of the problem and the contribution of the paper. The authors properly addressed concerns about the novelty (e.g., comparison with linear bandit and low rank structure) and about the underlying assumptions. Although some of them do seem relatively strong (and in some cases stronger than the state of the art in bandit, such as the distribution on the contexts), it is indeed non trivial to understand whether such assumptions can be easily relaxed in the representation learning context.   The novelty of the algorithm is more on the specific problem and set of assumptions, but it mostly relies on known principles (e.g., using method of moment for estimating the underlying representation). In this sense, I see this paper more as a useful addition to the fast growing landscape of representation learning methods in online learning, rather than a breakthrough. Also, the structure of the algorithm seems very "theoretical" in nature, since the explore than commit approach is very rarely a good strategy in practice.   Another issue the authors clarified in their revised submission is the actual improvement obtained in the bounds depending on the parameters T, k, d, N. In this respect, I still would like to encourage the authors to further illustrate the regime where the bound is actually better than for the single task approach. For instance, they could consider N fixed to a convenient value and produce a plot with x axis T and y axis the regret bound and report different curves for varying values of k and d. This would further clarify to the reader when representation learning can *provably* improve over plain single task learning.  Overall, given the general support from the reviewers and the revised version of the paper, I consider this contribution is significant enough to propose acceptance. As mentioned above, I believe it will serve as a reference for developing further the literature in this domain.
The proposed method introduces a method for unsupervised image to image mapping, using a new term into the objective function that enforces consistency in similarity between image patches across domains. Reviewers left constructive and detailed comments, which, the authors have made substantial efforts to address.  Reviewers have ranked paper as borderline, and in Area Chair s opinion, most major issued have been addressed:    R3&R2: Novelty compared to DistanceGAN/CRF limited: authors have clarified contributions in reference to DistanceGAN/CRF and demonstrated improved performance relative to several datasets.   R3&R1: Evaluation on additional datasets required: authors added evaluation on 4 more tasks   R3&R1: Details missing: authors added details.   
The authors investigate different tokenization methods for the translation between French and Fon (an African low resource language). Low resource machine translation is a very important topic and it is great to see work on African languages   we need more of this!  Unfortunately, the reviewers unanimously agree that this work might be better suited for a different conference, for example LREC, since the machine learning contributions are small. The AC encourages the authors to consider submitting this work to LREC or a similar conference.
The paper introduces a way of making Ratio Matching (RM) scale better to high dimensional data when training energy based models (EBMs). The main idea is to estimate the sum over the datapoint dimensions in the RM objective with importance sampling (IS), achieving computational savings by using fewer samples than dimensions. A key part of the method is a proposal that uses gradient information w.r.t. discrete variables to efficiently approximate the optimal (minimum variance) proposal, resulting in much better performance compared to uniform sampling. The authors also introduce a biased version of the estimator that samples from the same proposal but drops the importance weights when averaging over the samples, which, somewhat surprisingly, outperforms the unbiased version.  The idea of using Monte Carlo estimation based on importance sampling to speed up Ratio Matching is novel and sound. The use of gradient information to approximate the optimal IS proposal is also novel in this context, though the idea of using gradients this way to reduce the number of EBM energy function evaluations comes from Grathwohl et al. (2021), where it was used to speed up Gibbs sampling.  While the method is well described, the paper is insufficiently rigorous in several places, most importantly in claiming that Eq. 4 corresponds to Ratio Matching, which is not true. Eq. 4 is instead equivalent to the objective for Generalized Score Matching (GSM) given by Eq. 17 in (Lyu, 2009). Crucially, while both GSM and RM recover the true model if the model class is nonparametric/unconstrained, as is stated in (Lyu, 2009) (and thus agree with each other as well as with maximum likelihood estimation) ), they do not yield the same solution for constrained model classes such as neural networks. This means that the method in the paper implements GSM and not RM. Unlike RM, which has been used widely in the literature, GSM is essentially empirically unproven and thus is a less interesting choice. The main difference between the GSM and RM objectives is the presence of the squashing function g(u)   1/(1+u) around the probability ratios in RM to avoid division by zero, when the probability in the denominator is vanishingly small (as is explained above Eq. 12 in (Hyvarinen, 2007)). This means GSM is likely to be prone to stability issues due to using probability ratios directly. This is one possible explanation for the puzzling empirical results in the paper, where the proposed sampling based methods outperform the exact method they are supposed to approximate, with the biased method clearly performing best. The intuition based arguments made in the paper to explain these results are not convincing and need to be improved upon. While, as the authors pointed out in their response, it is possible to apply the strategy in the paper to RM by applying IS to Eq. 3 instead of Eq. 4, that would be essentially a different paper.  One example of puzzling experimental results is Figure 3, which shows that the base method ("Ratio Matching") does not find the correct solution while the proposed approximate methods do. This suggests that there is something wrong either with the method (e.g. with the objective, as mentioned above) or with the experimental setup. In either case, the cause needs to be thoroughly investigated.  Currently the empirical evaluation is primarily MMD based, relying on sampling from the model using MCMC. Ensuring that MCMC chains mix sufficiently well to sample from the true distribution by visiting all of its modes is difficult, and it is important to provide some evidence that this was done. As suggested by a reviewer, the results would be substantially strengthened by reporting the log likelihoods for the models, estimated e.g. using AIS, even if that requires including scaled down versions of some of the experiments.  The title of the paper is misleading and should be changed because the proposed method is specific to EBMs for binary data, even if the intent is to extend it to other types of discrete data in the future.  The clarification and additional results provided by the authors to the reviewers and the AC were appreciated, but unfortunately the outstanding issues with the paper are too major to allow acceptance at this point. The main idea of the paper has substantial promise however, and the authors are encouraged to develop it to its full potential by addressing the points from this meta review as well as the additional ones from the reviewers.  Bibliography correction: Hyvarinen is the solo author of "Estimation of Non Normalized Statistical Models by Score Matching". Peter Dayan was the editor of that paper and not a co author. Please correct your bibliography.
The paper proposes an approach for transfer learning by assigning weights to source samples and learning these jointly with the network parameters. Reviewers had a few concerns about experiments, some of which have been addressed by the authors. The proposed approach is simple which is a positive but it is not evaluated on any of the regular transfer learning benchmarks (eg, the ones used in Kornblith et al., 2018 "Do Better ImageNet Models Transfer Better?"). The tasks used in the paper, such as CIFAR noisy  > CIFAR and SVHN0 4  > MNIST5 9, are artificially constructed, and the paper falls short of demonstrating the effectiveness of the approach on real settings.   The paper is on the borderline with current scores and the lack of regular transfer learning benchmarks in the evaluations makes me lean towards not recommending acceptance. 
The paper presents a stochastic variational inference method for posterior estimation in a Cox process with intensity given by the solution to a diffusion stochastic differential equation. The reviewers highlight the novelty of the approach. Some of the concerns with regards to clarity have been addressed by the authors satisfactorily.   However, an important issue of the approach is that of estimating model parameters, which the authors do not address explicitly by simply referring to that as the task of the modeller. I believe this is an important issue and, although some of the parameters can be estimated along with the neural network parameters, this has not been shown empirically. Along a similar vein, the paper only presents results on a single real dataset (the bike sharing dataset), which questions the applicability of the approach and no other baseline method is presented. At the very least, the authors should have provided an objective evaluation to other doubly stochastic point process models, e.g. based on Gaussian processes, where modern stochastic variational inference algorithms have been presented.  
The main identified issues were the limited contribution and use cases, poor writing and missing baseline comparisons and more needed experiments. These issues were not addressed satisfactorily by the rebuttal and hence, I believe the paper should be revised by the authors and undergo another review process at another conference. I therefore recommend rejection.
This paper proposes an approach for coordinating teams with dynamic composition consisting of an attention mechanism, regularization and communication. The clarity of the paper is currently low seemingly due to the conflated message of the multiple parts of the framework. Improvements to the text via the suggested edits of all reviewers should be a relatively quick fix, but the clearer placement of this piece within the wider literature may require additional experiments to compare against so would be a larger change.   The reviewers did continue to discuss the paper after the end of the open discussion period with the authors and appreciated the additional experiments performed. In the absence of supporting theory, empirical results in a second domain significantly improve the evidence that the method may be more generally applicable. However, the new experiments raised new questions (included in the reviewers later replies) indicating more experiments in the second domain are needed which would require further peer review.  I hope the authors will take the constructive feedback provided here as intended; to improve the paper, submit the work again at a later stage when the second experimental domain is sufficiently explored to support the proposed framework and in doing so maximise its potential for impact. 
This paper presents a technique for compositionally constructing embeddings for nodes in knowledge graphs, hence reducing the memory requirements as well as allowing inductive learning. The reviewers find the direction promising and the approach novel and well motivated. There were some concerns about the experiment results — Reviewer KuBz suggests including more baselines, Reviewer CpaB suggests trying NodePiece on single relation graphs and Reviewer 2qcD notes that NodePiece lags behind the other approaches on some tasks. Most of these concerns seem to have been addressed in the author response and I tend to agree with the authors that single relation graphs are out of the scope of this work. Reviewer X7aq also raised a concern about the claims made regarding (i) uniqueness of the hashes and (ii) sub linearity of the approach. It is good to see that claim (ii) has been removed, but (i) is still present in many places — it would be good to add a discussion about why the hashes are highly likely to be unique in the final version.
The paper proposes to replace dynamic routing in Capsule networks with a trainable layer that produces routing coefficients. The goal is to improve their scalability. This is promising as a research direction but reviewers have raised several concerns about unclear contributions and lack of a thorough evaluation of the approach. There is also a recent relevant work pointed out by Reviewer 1 that should be discussed. Given these concerns, the paper is not suitable for publication in its current form, however I encourage the authors to use reviewers  comments for improving the paper and resubmit in next venues.
This paper studies a problem setup of parameter efficient transfer learning for large scale deep models. The approach consists of learning a diff vector with a sparsity constraint and then pruning the vector using magnitude pruning. A group penalty is also introduced to enhance structured sparsity. The main motivation is that for each new task, we only need to add a few parameters based on a pre trained model without fine tuning it.  The proposed approach possesses technical soundness and shows empirical efficacy for the studied problem setup. During the rebuttal and discussion phases, two of the reviewers raised two major concerns based on which they strongly disagreed with acceptance:   The problem setup is not elaborated sufficiently and falls short of plausibility. An approach targeting at efficiency should either improve inference speed or reduce storage cost. Unfortunately, neither advantage has been well approached.   The technical novelty is somewhat incremental, given the rich previous work on residual adapter, network re parameterization, and network compression (pruning, sparsity etc.).  AC read the paper and agreed that, while the paper has some merit such as a better model for the particular problem setup, the reviewers  concerns are reasonable and need to be addressed in a more convincing way. For example, try to study a practical application in which the proposed approach is essential and useful for efficiency enhancement.
This paper introduces a model free RL algorithm claiming SOTA performance. All but one reviewer agreed on rejection.  #1 The empirical results are based on only 5 seeds (too low) and the plots across 5 domains show no clear evidence of improved performance due to overlapping error bars. The paper s poor empirical practice does not support the main contribution.  #2 The proposed method builds on REDQ, but the authors maintained in the response that their method performed better than REDQ (failing to articulate significant algorithmic novelty) . Even the most positive reviewer (iNq8) did not agree when the authors claimed "our performance improvements are achieved by the innovations we introduced in our algorithm". iNq8 responded "it is unclear whether this performance improvement is really meaningful". The authors never responded to iNq8 s followup questions about overlapping error bars and differences in the behaviours produced by the new method.  Points #1 and #2 combine to form the clear conclusion that this work is not ready in its current from for publication.
The submission is proposed a rejection based on majority review.
Thank you for your submission to ICLR.  As noted, several of the reviewers had fairly low confidence in evaluating this submission.  However, based upon the reviewers and commenters who were familiar with this line of work, as well as my own evaluation of the paper, I believe it is clearly worth publishing at ICLR.  The proposed method pushes the boundary in methods for exact branch and bound based verification of neural networks, using clever tricks from existing relaxations.  And while the method is still likely to be relegated to relatively small networks for the time being, pushing forward the state of the art in exact verification is still a worthy goal suitable for publication at ICLR.  I thus think that the paper is quite clearly above the bar, and should be accepted for publication.
This paper proposes an early stopping strategy based on the disparity of gradients between two batches from the *training set*. Such a criterion would make the held out validation set unnecessary. The idea is motivated by theoretical arguments and benchmarked on experiments, but some issues still need to be worked on.   Regarding theory, the theorems here assume implicitly the independence of the gradients computed on two different batches of training data, but the conditions where gradients on independent examples computed *on trained weights* are independent (or close to being independent) should be discussed. Regarding experiments, the protocol is unusual in that the proposed stopping criterion is compared to a stopping criterion relying on k fold cross validation, instead of the usual stopping criterion on held out validation. What is the motivation for this protocol? Why should we expect a small variability in the optimal number of iterations over the k runs?  In addition, the experiments consider a normalized definition of gradient disparity for which no theory is provided. Although there is an interesting correlation between this normalized gradient disparity and generalization error, this link does not seem strong enough to pick the right number of iterations.   Detail:  Still regarding independence, reporting the empirical standard errors on k fold cross validation is debatable since it is not related to the theoretical standard error (see e.g. [Bengio et al.: No Unbiased Estimator of the Variance of K Fold Cross Validation. J. Mach. Learn. Res. 5: 1089 1105 (2004)]) 
This paper proposes an extension to the Dreamer agent in which planning (either via MCTS or rollouts) is used to select actions, rather than sampling from the policy prior. The results show small improvements over the baseline Dreamer agent.  Pros:   Important study on incorporating decision time planning into Dyna based agents   Evaluation on many control tasks rather than just a few  Cons:   Lack of ablations and detailed analysis   Claims aren t backed up by quantitative results  The reviewers generally felt that the approach taken in the paper lacked novelty. I agree that the approach is somewhat incremental (in fact I think it is also an instance of [1]). While both incremental changes and reimplementations of older methods with newer techniques can indeed be valuable, the current paper falls short in terms of the evaluation. As pointed out by several reviewers, there is no in depth analysis explaining the design choices in which rollouts or MCTS are most likely to help (e.g. search budget, exploration parameters, etc.). As these parameters can play a large role in performance, I think it is important to characterize their effect on the agent otherwise, I do not think there is a clear learning regarding how to translate these results to other domains and tasks. Additionally, and perhaps even more seriously, there are a number of claims made in the paper about the proposed method being more data efficient or higher performance. But, it is not clear visually that these improvements are statistically significant, and no quantitative tests have been run (and if the authors want to make a claim about data efficiency, I d especially encourage them to report a metric like cumulative regret). Finally, while the incomplete runs are not a reason for rejection on their own, they do add to my overall sense that the paper is incomplete in its current form.  Given the above reasons, I do not feel this paper is ready for publication at ICLR. I d encourage the authors to perform more careful ablations of the effect of incorporating search into the agent, and to back up their claims with more rigorous quantitative results.  One small point: the authors wrote in the rebuttal that "we are not aware of any work which investigates look ahead search based planning for continuous control with learned dynamics". Grill et al. [2] uses MCTS with learned dynamics in a modification of MuZero, though only applies it in one continuous control task (Cheetah Run).  1. Silver, D., Sutton, R. S., & Müller, M. (2008). Sample based learning and search with permanent and transient memories. ICML. 2. Grill, J. B., Altché, F., Tang, Y., Hubert, T., Valko, M., Antonoglou, I., & Munos, R. (2020). Monte Carlo tree search as regularized policy optimization. ICML.
The paper introduces a new extrapolation problem for graph representation learning (they refer to it as   counterfactual modeling ). While the problem set up is intriguing and the work likely has merit,  two reviewers (R2 and R4),  found the writing highly problematic and we share their opinion.  Even though some of the concerns they raised, as followed from the rebuttal, were not correct, this confusion, in our view, is largely due to the exposition.  Both these reviewers are experts in geometric deep learning. Their lack of understanding even of relatively central points of the paper, despite clearly investing a large amount of time in reading the paper, indicates that extra work is needed.  The only positive reviewer marked his confidence as very low, provided a rather short review, and did not choose to champion the paper.  While the authors tried to address the reviewers  concerns both in rebuttal and by revising the manuscript, we still feel that much more work is needed before it can be presented at a conference. We understand that this is a challenge to present this work in a conference format; it builds on the diverse background (e.g., in graph representation learning and in causal modeling) and considers a novel setting. However, we still feel that it could have been done much more successfully. In principle, this work may benefit from being presented in a journal paper (e.g., jmlr).
This paper presents an RL agent which progressively synthesis programs according to syntactic constraints, and can learn to solve problems with different DSLs, demonstrating some degree of transfer across program synthesis problems. Reviewers agreed that this was an exciting and important development in program synthesis and meta learning (if that word still has any meaning to it), and were impressed with both the clarity of the paper and its evaluation. There were some concerns about missing baselines and benchmarks, some of which were resolved during the discussion period, although it would still be good to compare to out of the box MCTS.  Overall, everyone agrees this is a strong paper and that it belongs in the conference, so I have no hesitation in recommending it.
This paper addresses a well motivated problem and provides new insight on the theoretical analysis of representational power in quantized networks. The results contribute towards a better understanding of quantized networks in a way that has not been treated in the past.   The most moderate rating (marginally above acceptance threshold) explains that while the paper is technically quite simple, it gives an interesting study and blends well into recent literature on an important topic.   A criticism is that the approach uses modules to approximate the basic operations of non quantized networks. As such it not compatible with quantizing the weights of a given network structure, but rather with choosing the network structure under a given level of quantization. However, reviewers consider that this issue is discussed directly and clearly in the paper.   The reviewers report to be only fairly confident about their assessment, but they all give a positive or very positive evaluation of the paper. 
The paper proposes a Transformer based model called SCformer to perform long sequence time series forecasting by computing efficient segment correlation attention. The reviewers think the method lacks novelty and the experiments need a detailed ablation study.
Thank you for your submission to ICLR.  The reviewers unanimously felt that there were substantial issues with this work, owing to the fact that both the techniques and applications have been considered in a great deal of previous work.  Furthermore, the manuscript itself needs substantial amounts of revision before being suitable for publication.  As there was no response to these points during the rebuttal period, it seems clear that the paper can t be accepted in its current form.
The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF.  Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network.    Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on e.g., assuming away the confounders.  However, I believe the authors address the criticisms of R4 satisfactorily.   Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper.  I do have a few quips myself and some comments that may help the authors to further improve the paper.  1. Re: the design that models each parameter as a spline.  This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks. t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t.   2. If you use a B spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat. As a side note,  the authors should clearly write out how they are choosing the knots to specify the basis functions. Otherwise the paper will not be reproducible.   3. I am not sure how this method would compare to naive (non deep) baselines. Maybe this was considered in a prior work? If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain. Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines.  
This paper has received four positive reviews. The main intellectual contribution of the paper is the introduction of a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning across neurons and even across animals. The reviewers commented on the technical strength of the paper. At the same time, the main contribution remains relatively incremental from a technical standpoint, and while the approach may be of value to future work, the impact of the current study on neuroscience (which is the target here) is quite limited. Nonetheless, there seems to be sufficient enthusiasm from the reviewers to recommend this paper be accepted.
Most of the reviewers pointed out a lack of rigor of this submission, unclear contributions, not too convincing claims and empirical gains. I thank the authors for the effort put in revising the paper and responding to the reviewer concerns. However, the reviewers did not deem them convincing enough.
All three reviewers gave scores of Weak Accept. AC has read the reviews and rebuttal and agrees that the paper makes a solid contribution and should be accepted.
The paper shows the relationship between node embeddings and structural graph representations. By careful definition of what structural node representation means, and what node embedding means, using the permutation group, the authors show in Theorem 2 that node embeddings cannot represent any extra information that is not already in the structural representation. The paper then provide empirical experiments on three tasks, and show in a fourth task an illustration of the theoretical results.  The reviewers of the paper scored the paper highly, but with low confidence. I read the paper myself (unfortunately not with a lot of time), with the aim of increasing the confidence of the resulting decision. The main gap in the paper is between the phrases "structural node representation" and "node embedding", and their theoretical definitions. The analogy of distribution and its samples follows unsurprisingly from the definitions (8 and 12), but the interpretation of those definitions as the corresponding English phrases is not obvious by only looking at the definitions. There also seems to be a sleight of hand going on with the most expressive representations (Definitions 9 and 11), which is used to make the conditional independence statement of Theorem 2. The authors should clarify in the final version whether the existence of such a representation can be shown, or even better a constructive way to get it from data.  Given the significance of the theoretical results, the authors should improve the introduction of the two main concepts by:   relating them to prior work (one way is to move Section 5 towards the front)   explaining in greater detail why Definitions 8 and 12 correspond to the two concepts. For example expanding the part of the proof of Corollary 1 about SVD, to make clear what Definition 12 means.   a corresponding simple example of Definition 8 to relate to a classical method.  The paper provides a nice connection between two disparate concepts. Unfortunately, the connection uses graph invariance and equivariance, which is unfamiliar to many of the ICLR audience. On balance, I believe that the authors can improve the presentation such that a reader can understand the implications of the connection without being an expert in graph isomorphism. As such, I am recommending an accept.  
This paper provides an interesting synthesis of ideas. Although the results could be improved, this is a good paper.
Thank you for submitting you paper to ICLR. Two of the reviewers are concerned that the paper’s contributions are not significant enough —either in terms of the theoretical or experimental contribution   to warrant publication. The authors have improved the experimental aspect to include a more comprehensive comparison, but this has not moved the reviewers.  Summary: The approach is very promising, but more experimental work is still required to demonstrate significance. 
In this paper, the authors propose a model for integrating news representations for stock predictions. While the research direction has good value in real applications, it seems that this particular paper has not done a sufficiently good job in pushing the frontier of this direction. The reviewers have raised quite a few concerns, for example: 1)	Paper writing needs significant improvement (e.g., confusion regarding future news). 2)	Limited technical novelty as compared to previous works 3)	Benchmark datasets are out of date, baselines are a little weak and not well explained, more evaluation measures (Such as Sharpe value) are needed 4)	Marginal improvements over the baselines  The authors have not submitted their rebuttals. Therefore the concerns are still there and we do not think the paper is ready for publication at ICLR. 
The authors discuss an improved distillation scheme for parallel WaveNet using a Gaussian inverse autoregressive flow, which can be computed in closed form, thus simplifying training. The work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably. The AC agrees with the reviewers that the work is a valuable contribution, particularly in the context of end to end neural text to speech systems. 
In this paper, the authors generalized the Fenchel duality formulation of the maximum likelihood for F1 EBM, which leads to a min max optimization formulation. Meanwhile, the optimization reveals a new connection between primal dual MLE with score matching. These contribution is significant and make the paper interesting to the community.   However, there are several issues need to be addressed.     *Experiments*: most of the reviewers concern about the empirical study, which are conducted on synthetic data. The paper will be much stronger if the comparison on real world dataset, e.g., MNIST and CIFAR10, can be conducted.     *Clarification of paper*: I totally understand that due to this is a theoretical oriented paper, it must be notation and derivation heavy. However, the paper will be much easier for reader, if more discussion is added, e.g., the implementation of the proposed algorithms and more explanation about the comparison between primal dual algorithm vs. score matching and the experimental results for broader audiences.     In fact, I personally like the paper very much, which provides a promising solid algorithm for EBM estimation, and the connection to score matching is also novel and different from the current understanding. However, unfortunately, the authors gave up the rebuttal and did not successfully address the concerns from the reviewers. I strongly encourage the authors to revise the draft according to the reviewers  suggestion and resubmit to another venue.
The authors give evidence that is certain cases, the ordering of sample inclusion in a curriculum is not important.  However, the reviewers believe the experiments are inconclusive, both in the sense that as reported, they do not demonstrate the authors  hypothesis, and that they may leave out many relevant factors of variation (such as hyper parameter tuning). 
The paper received generally positive reviews, but the reviewers also had some concerns about the evaluations.  Pros:   An improvement over HashNet, the model ties weights more systematically, and gets better accuracy. Cons:   Tying weights to compress models already tried before.   Tasks are all small and/or audio related.   Unclear how well the results will generalize for 2D convolutions.   HashNet results are preliminary; comparisons with HashNet missing for audio tasks.  Given the expert reviews, I am recommending the paper to the workshop track. 
The paper presents a new framework of synthesizing differential private data using deep generative models. Reviewers liked the significance of the problem. They raised some concerns which was appropriately addressed in the rebuttal.  We hope the authors will take feedback into account and prepare a stronger camera ready version.
I ve gone over this paper carefully and think it s above the bar for ICLR.  The paper proves a relationship between the eigenvalues of the Fisher information matrix and the singular values of the network Jacobian. The main step is bounding the eigenvalues of the full Fisher matrix in terms of the eigenvalues and singular values of individual blocks using Gersgorin disks. The analysis seems correct and (to the best of my knowledge) novel, and relationships between the Jacobian and FIM are interesting insofar as they give different ways of looking at linearized approximations. The Gersgorin disk analysis seems like it may give loose bounds, but the analysis still matches up well with the experiments.  The paper is not quite as strong when it comes to relating the anslysis to optimization. The maximum eigenvalue of the FIM by itself doesn t tell us much about the difficulty of optimization. E.g., if the top FIM eigenvalue is increased, but the distance the weights need to travel is proportionately decreased (as seems plausible when the Jacobian scale is changed), then one could make just as fast progress with a smaller learning rate. So in this light, it s not too surprising that the analysis fails to capture the optimization dynamics once the learning rates are tuned. But despite this limitation, the contribution still seems worthwhile.  The writing can still be improved.  The claim about stability of the linearization explaining the training dynamics appears fairly speculative, and not closely related to the analysis and experiments. I recommend removing it, or at least removing it from the abstract. 
The model presented here may be of use to others in running quantum chemistry simulations, and it may well lead to new advances, but the authors did not sufficiently address the key concerns around the model not being energy conserving and rotation covariant. The approach proposed could be learning such physical rules, and the authors in their general response provide some very preliminary evidence for this, but a much more thorough discussion with a full range of experiments is needed. ICLR is a broad conference where non experts who may have never heard of DFT simulations must parse this work and decide on if/how to follow up. This is a critical missing piece for anyone that wants to do so.  The above is exacerbated by the fact that the work is not well situated against prior work as pointed out by two reviewers. Together these two issues conspire to make understanding this model, the contribution of the work, and what followup is possible, untenable for an ICLR reader. For example, one would not be able to surmise from the manuscript and its brief discussion of rotation covariance that this is likely to result in ForceNet having limited applicability to other DM problems; which one reviewer pointed out and the authors generally agreed with. While the authors respond that perhaps the architecture itself may be useful for other applications, why this would be and what the specific advantage of the current model relative to the state of the art in those fields is unclear. 
Authors propose a new method of semi supervised learning and provide empirical results. Reviewers found the presentation of the method confusing and poorly motivated. Despite the rebuttal, reviewers still did not find clarity on how or why the method works as well as it does.
The paper considers the empirical distribution of layer/channel in CNN ,and proposes to use global null tests with Simes and Fisher statistics to aggregate the p values. This method is competitive while computationally efficient. The underlying theoretical insights are discussed in detail.  The paper received mixed ratings, and the discussions weren t active. So, AC carefully read the paper and inspected all reviews. Reviewer a8KZ comments were factually inaccurate in listing references, and lack substantial feedback on the actual content of the paper. Hence, the review was down weighted.   The other negative reviewer Ni17, as an OoD expert, unfortunately did not offer more feedback to author rebuttals. From what AC comprehends, the authors should have clarified their The theoretical guarantee and compared properly with Liu et. al. 2020 energy score (ES).  Considering the above, AC feels that the study deserves to be published.
This paper proposes a federated learning (FL) scheme that is suitable for clients/devices with heterogeneous resources. The scheme Split Mix trains multiple models of different sizes and adversarial robustness levels, which are tailored to the budgets of the individual device. Empirical results show encouraging results.  It is clear that FL will have to work with clients with diverse resources, a point that is appreciated. Indeed, it is anticipated that widely dispersed inference will have to deal with a highly heterogeneous mix of clients. The study is quite thorough. One aspect that is not convincing in the experiments is the budgets being exponentially distributed: having a strong concentration around a mean (with something like a Gaussian tail), or a power law distribution, would be more suitable.
This paper presents a probabilistic programming language where models are constructed out of building blocks which specify both the distribution and an inference procedure. As a demonstration, they show how a GP LVM can be implemented.  The paper spends a lot of space arguing for the benefits of modularity. Modularity is of course hard to argue with, and the benefits are already understood in the PPL community. But, as the reviewers point out, various other PPLs have already adopted various strategies to enable modular definition of models, and (in cases like Venture) special purpose higher level inference algorithms. This paper contains little discussion of other PPLs and how the specific design decisions relate to theirs, so it s hard to judge whether this paper really covers new ground. Such discussion wasn t added to the revised paper, even though multiple reviewers asked for it. I can t recommend acceptance. 
The paper proposes to overcome the challenge of annotating datasets to train convolutional networks by considering instead an architecture that is composed of stacked support vector machine layers. Each support vector machine is trained on a small patch from the input image. A voting mechanism is used to aggregate the predictions. Results show better performance by the model in the small data regime compared with larger convolutional neural networks trained from scratch.  The reviewers appreciated the relevance of the problem and the originality of the approach. The reviewers also appreciated several parts of the experimental evaluation that were carefully conducted in particular the sensitivity of the analysis with respect to the patch size and the multiple datasets considered for the experimental evaluation. The reviewers also expressed concerns about the adequacy of the evaluation (unfair comparisons), the completeness of the baselines (missing baselines), and the significance of the improvements. In particular, the experimental evaluation was considered too limited given the problem considered.  The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers considered that ‘the experimental evaluation improved a bit , that several concerns were satisfactorily addressed, and yet that the updated results  do not show a significant improvement of the proposed method over existing works, simple baselines or pre trained ResNet . We encourage the paper to pursue their approach further taking into account the reviewers  comments, encouragements, and suggestions. The revision of the paper will generate a stronger submission to a future venue.  Reject.
This paper presents a new method that uses transformers to predict the result of pairwise competitions given each players’ history of past game plays. The reviewers thought this had notable potential benefits for practice. However the reviewers’ also had some significant concerns with the current work in terms of the evaluations used, which were primarily  correlation instead of prediction accuracy or calibration etc. There was also some concern about other aspects of the presentation. We hope that the reviewers’ responses are useful to the authors in revising their work for future submissions as this method has the potential to be very useful for many domains.
The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization.  The experiments are convincing experiments. We encourage the authors to incorporate additional references suggested in the reviews. We recommend acceptance. 
In this paper, the authors propose a simple yet interesting new graph sampling method for graph neural networks.  It addresses the two main problems that GNNs have not previously been extended to deep GNNs: expressivity and computational cost.  Through experiments, the authors show the effectiveness of the proposed algorithm.   Overall, the proposed approach is interesting.  However, the reviewers were still not convinced by the response, and the paper is still below the acceptance threshold.  I encourage the authors to revise the paper based on the reviewer s comments and resubmit it to a future venue. 
The paper presents a generalization bounds for l1 regularized networks.  The reviewers thought the results were clear and sound, but on the other hand rely on rather standard technical tools, and their impact is limited. One question is why this particular regularization is related to practical learning of neural nets where explicit regularization is not used. The authors may want to relate their results to e.g., Theorem 1 in https://arxiv.org/pdf/1412.6614.pdf.  
The paper proposes a new algorithm for solving constrained MDPs called Projection Based Constrained Policy Optimization. Compared to CPO, it projects the solution back to the feasible region after each step, which results in improvements on some of the tasks considered.   The problem addressed is relevant, as many tasks could have important constraints e.g. concerning fairness or safety.   The method is supported through theory and empirical results. It is great to have theoretical bounds on the policy improvement and constraint violation of the algorithm, although they only apply to the intractable version of the algorithm (another approximate algorithm is proposed that is used in practice). The experimental evidence is a bit mixed, with the best of the proposed projections (based on the KL approach) sometimes beating CPO but also sometimes being beaten by it, both on the obtained reward and on constraint satisfaction.   The method only considers a single constraint. I m not sure how trivial it would be to add more than one constraint. The reviewers also mention that the paper does not implement TRPO as in the original paper, as in the original paper the step size in the direction of the natural gradient is refined with a line search if the original step size (calculated using the quadratic expansion of the expected KL) does violate the original constraints. (Line search on the constraint as mentioned by the authors would be a different issue). Futhermore, the quadratic expansion of the KL is symmetric around the current policy in parameter space. This means that starting from a feasible solution the trust region should always overlap with the constraint set when feasibility is maintained, going somewhat agains the argument for PCPO as opposed to CPO brought up by the authors in the discussion with R2. I would also show this symmetry in illustrations such as Fig 1 to aid understanding.    
The authors propose a novel hypersolver framework for solving numerical optimal control problems, learning a low order ODE and a neural network based residual dynamics. They compare their framework with traditional optimal control solvers on a number of control tasks and demonstrate superior performance.  The reviewers are in consensus that the paper makes significant contributions that are validated by the experimental results. The only concern was that the experiments are largely on low dimensional systems, but the reviewers agreed that the results are still worthy of acceptance.
This paper proposes to transfer the image pretrained model to a point cloud model by inflating 2D convolutional filters to 3D convolutional filters and finetuning the inflated image pretrained model, so that 3D point cloud tasks can benefit from 2D image pretraining. Extensive experiments are conducted to validate the effectiveness of the proposed method. Even though the performance gain using the 2D pre training is notable, the novelty of the paper is limited since inflating 2D model to 3D video action recognition has been studied, and theoretical understanding of the proposed model is lacking. During the rebuttal period, the authors addressed most of the reviewers’ concerns by conducting additional experiments. Even though the performance is compelling, all reviewers agree that the novelty for the paper is limited and the discussion on why this method work is not convincing. Meanwhile, one reviewer points out that some claims made by authors are not well supported. Besides, one reviewer points out that the paper might have a broader impact in a computer vision conference but only provide a limited contribution to the ICLR community. After an internal discussion with reviewers. the AC agrees with the reviewers on their judgments and recommends rejecting the paper because of the limited novelty of the paper.
The authors propose an algorithm for generating adversarial examples for ASR systems treating them as black boxes.   Strengths   One of the early works to demonstrate black box attacks on ASR system that recognize phrases instead of isolated words.  Weaknesses   The approach assumes that the logits are available, which may not be realistic for most ASR systems when they are used in practice   typically only the final transcription is available.   Although the technique is applied to continuous speech, algorithmic improvements over prior work of Alzanot et al. is minimal.   Evaluation is weak. For example, cross correlation cannot completely capture the adversarial nature of a generated audio sample.    The authors use a genetic algorithm for generating new set of examples which are pruned and mutated. It’s not clear what guarantees exist that the algorithm will eventually succeed.   The reviewers agree that the presented work puts forth an interesting research direction. But given the deficiencies of the current submission as pointed out by the reviewers, the recommendation is to reject the paper.
This paper proposes to address the high bandwidth cost when transferring data between server and user for machine learning applications. The input data is augment with channel and spatial mask so that the file transfer cost is reduced. While the reviewers agree that this is a well motivated and interesting problem to study, a number of concerns are raised, including loosely specified performance/size trade off, how this work is compared to related work, low novelty relative to a few key missing references. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
This paper proposes a differentiable inductive logic programming method in the vein of recent work on the topic, with efficiency focussed improvements. Thanks the very detailed comments and discussion with the reviewers, my view is that the paper is acceptable to ICLR. I am mindful of the reasons for reluctance from reviewer #3 — while these are not enough to reject the paper, I would strongly, *STRONGLY* advise the authors to consider adding a short section providing comparison to traditional ILP methods and NLM in their camera ready.
The authors study neural networks with binary weights or activations, and the so called "differentiable surrogates" used to train them. They present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability.  The reviewers agree that the main topic of the paper is important (in particular initialization heuristics of neural networks), however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions.  The authors imporved the readability of the manuscript in the rebuttal.  This paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence. Not being familiar with this line of work, I recommend acceptance following the average review score.
This paper received divergent scores (one strong negative and three positives). The positive reviews praise the clear intuition/motivation and strong empirical performance, while the negative review considers the proposed approach ad hoc with limited novelty. I read the paper myself and found myself leaning more towards the negative score. In more details:   I think the paper proposed a cleverly engineered solution to employ two separate RNNs to model two different subsets of the users (active v.s. inactive). To combat overfitting, the authors proposed some tricks: 1) use MF to learn a better initialization and 2) tie some of the parameters together. The ablation study shows that both are quite useful, and not too surprisingly when you have two powerful RNNs and work hard to make sure they don t overfit, they perform better than a single RNN. As I mentioned earlier, this whole approach is quite cleverly engineered and executed. But it s not clear to me if this is something that the ICLR community can benefit from (maybe except a relatively small proportion). I believe this paper can find a bigger audience in venues like KDD whose deadline is coming up.   Furthermore, the authors presented some theoretical analysis to justify their intuition, which to me feels a bit forced. To start, a big assumption is that the optimal user embeddings are very concentrated, which is a rather strong assumption and I hardly believe it will hold in practice (what can even be considered as "concentrated" in high dimensional space?). Following that, the theorem implies that the initial embedding for the inactive users should be the expected optimal embedding, but then later in the paper this point seemingly got completely ignored and instead the authors just proposed to learn a common initialization. Additionally, the theorem suggests that there is an optimal threshold, but later in the paper again this point got completely ignored. I know the theory makes assumptions and builds on simple cases. But my point is that you don t need theorem to show me that two RNNs can perform better than one when trained properly (which, I admit, is non trivial and is the main contribution of the paper). There has been a lot of discussion around the ML community about the trend of making your paper look "mathy" and I don t think this is a good thing.   Minor comment: in MF, the objective wrt each embedding is convex, but the whole optimization is not jointly convex and it is not likely that you can get to the global optimum. It is relatively insensitive to initialization though (comparing with neural nets). 
The paper introduces a method to solve inverse problems: given y, find x such that P(x) y, for a given physical simulator P. A standard approach is to learn a neural net such that the inverse x NN(y;\theta). The authors state that this is problematic because it is difficult to take "higher order" gradient information into consideration when using this standard approach. The method assumes that there is an approximate inverse solver inv(P) and discusses an alternative "Physical Gradient" objective that can incorporate knowledge of an approximate inv(P) and a neural network. The experiments are good though comparing performance on an iteration basis is not always fair since an iteration of the PG method can be much more expensive than standard approaches.  The biggest issue that reviewers had was the clarity of the presentation. The authors have made a reasonable attempt to correct this, but I m inclined to agree with the general reviewer sentiment that the presentation is still not at the required level. I agree that there are many things that are not clear, including the confusing discussion in section 2.1 about how the method takes higher order information into consideration. It only becomes partially transparent later in the experiments what is meant by higher order information.  Overall, I feel this is the basis of a potentially valuable contribution but that the current presentation is quite confusing. As mentioned by others, I would also suggest to find a different name since Physical Gradient is also rather misleading.   The following points were not part of the review process and I do not base the final decision on them, but the authors may want to consider the following:   I believe there is also an error in the basic approach, or at least an approximation is made which is not explained. The error is that the approximate inv(P) depends also on the parameter \theta (since this is used to initialise inv(P)). This dependency is not taken into consideration in the paper. For example, in theorem 1 in appendix A, the calculation of the gradient dM/d\theta is incorrect since the authors assume that inv(P) is independent of \theta, which it is not (since the preconditioner value depends on \theta). If we do take this into account, we would need to know the derivative of inv(P|x) with respect to the preconditioner x. This dependency would alter the gradient, potentially considerably. The gradient in figure 2 for the PG is also incorrect. One may of course simply say that the paper discounts this correction term in order to retain tractability; however, this would need to be stated as an approximation.
The paper proposes transfer learning where the target domain data is evolving along time.  They use both labeled and unlabeled data to learn domain and time invariant features based on a discrepancy measure they introduce.  Their proposed algorithm uses VAE to learn such features.  Reviewers have mixed response, although the author feedback did help.   The main limitation with the paper is that it does not seem to be aware of the very extensive literature on continuous domain adaptation.  The related work only discusses papers on transfer learning, multi source domain adaptation, and continuous learning.  But ignores papers on continuous domain adaptation which are much more related to this paper.  The most recent of these that appeared in ICML 2020 also attempts to learn time invariant features using adversarial methods.  Unfortunately, the reviewers seem to be also unaware of this literature:  1.  Continuously Indexed Domain Adaptation,  Hao Wang, Hao He, Dina Katabi, ICML 2020 2.  Active Adversarial Domain Adaptation  3.  Continuous Domain Adaptation using Optimal Transport 4. Learning to Adapt to Evolving Domains   NeurIPS 2020 5.  Judy Hoffman, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 867–874, 2014. 6 Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Adagraph: Unifying predictive and continuous domain adaptation through graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6568–6577, 2019.  7 Atsutoshi Kumagai and Tomoharu Iwata. Learning future classifiers without additional data. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.  
Dear authors,  As you can see, reviewers agree on the importance of the analysis present in the paper but two reviewers feel like it misses important comparisons.  That said, PPO is a popular algorithm and I also welcome any attempt as improving our understanding of its dynamics. With this paper, information about PPO would be more complete but also more spread out across multiple papers.  At the same time, I am sympathetic to the reviewers  arguments and also feel that the paper would have had a much clearer message had some additional ablation studies been performed, for instance on tabular settings where this is easily done.  The overall assessment is that the paper is not yet ready for publication.
This paper presents a method for inference in state space models with non linear dynamics and linear Gaussian observations. Instead of parameterizing a generative model, the paper proposes to parameterize the conditional distribution of current latent states given previous latent states and observations using locally linear transitions, where the parameters of the linear mappings are given by neural networks. Under fairly standard conditionally independence assumptions, the paper uses known Bayesian filtering/smoothing tricks to derive a recursive estimation algorithm and a parameter estimation method based on a simple maximum likelihood objective.    Overall, the reviewers found the idea to be novel and interesting and I agree.  They also found the relation to the noise2noise objective worth highlighting. Several concerns were raised during the discussion period, which I believe the authors addressed satisfactorily. However, I think the authors should bring the assumed distinction between ‘supervised’, ‘self supervised’ and ‘unsupervised’ upfront, as usually these types of models are trained using the noisy data (to which the authors refer to as unsupervised).   Given the large body of literature on dynamical systems, filters and smoothers, I believe the paper will benefit significantly from more comparisons across a wider range of (and more realistic) datasets.
This paper proposes a meta structural causal model framework, to increase the representation capability of structural equation models. It also considers how to connect data to mechanisms. The paper is conceptually interesting. However, on the technical side, reviewers feel that without supporting proofs or empirical experiments, it is hard to justify the correctness of the proposal and judge its applicability to real world problems.  As authors claimed in their response, "it is our future work of interest to code our proposed framework into a working system and validate it in a proper setting given its early stage status on research in modeling causal cycles." I think some future version of the paper might be a great contribution to the field if a working system were included.
The main contribution of the paper is showing that a model based approach can be competitive with (and even outperform) strong model free methods on the 200M Atari benchmark. This is achieved through a set of improvements over the original Dreamer algorithm.  Reviewers have been polarized over this submission (4,5,8,9). After reading the paper, reviews, rebuttals, and engaging with all reviewers in private conversations, I am recommending acceptance as a poster. I agree with R3 and R4 that « this is impressive work », « results are a convincing demonstration of its utility », « it is an important setup from the perspective of model based RL », « the model is elegant », and « the benchmarking discussion is very useful for the community ».  Although it is true, as R1 puts it, that this work can be seen as « an incremental set of tricks over a prior published approach », these tricks are not obvious and lead to very substantial empirical performance gains. Since the authors described them in details and have also committed to sharing their code, I expect them to be quite valuable to other researchers.  Finally, although I respect R2’s choice to stick to their rating of 4, I believe that their main concern, related to not fully understanding why this work improves on the existing SimPLE algorithm, is indeed justified, but is not enough for rejection. DreamerV2 has a lot of differences compared to SimPLE and it would be very costly to investigate in details the impact of each of them. Hopefully, this work will motivate further research in model based RL that will shed more light on such questions. I would encourage the authors, however, to elaborate a bit more on the differences vs. SimPLE in the « Related work » section (or Appendix, if there is not enough room in the main text).
The work addresses the problem of inferring group structure from unstructured data in multi agent learning settings, proposing a novel approach that has key computational / run time advantages over a prior approach. A key limitation raised by reviewers is the limited quantitative evaluation and comparison to previous approaches, as well as a resulting set of general insights into advantages of the proposed approach compared to prior work (beyond computational benefits). While some of the key limitations were addressed in the rebuttal, the contribution in its current form remains too narrow. The paper is not ready for publication at ICLR at this stage.
The paper proposes a novel method, PI3NN, for estimating prediction intervals (PIs) for quantifying the uncertainty of neural network predictions. The method is based on independently training three neural networks with different loss functions which are then combined via a linear combination where the coefficients for a given confidence level can be found by the root finding algorithm. A specific initialization scheme allows to employ the method to OOD detection.   Reviewers agreed on the importance of the problem of producing reliable confidence estimates.  The proposed method addressed some of the limitations of the existing approaches, and reviewers valued that a theoretical as well as an empirical analysis is provided.   On of the main criticisms was that the theoretical derivation of the method is based on the assumption of the noise being homoscedastic. This however is a common issue with other methods in this area, which are nevertheless all applied (and seem to work) on heteroscedastic data as well and are outperformed by the proposed method. Another main point that was criticized was that the empirical analysis was limited. In turn the authors added another experiments on another dataset and with another network architecture (a LSTM) to their analysis. Moreover, the authors adequately addressed a lot of the concerns and questions of the reviewers in their answers and the revised manuscript.  The final mean scores are exactly borderline (5.5) but with a higher confidence of reviewers voting for acceptance.  Based on the listed points, the paper should be accepted.   I would encourage to  improve the discussion around the dependence on x in Section 3.2, which could still be made clearer, in the final version of the manuscript, and to add the discussion about the limitations of the theoretical analyses (i.e. the applicability  only to the homoscedastic settings) to the conclusion.
This work is interested in using sentence vector representations as a method for both doing extractive summarization and as a way to better understand the structure of vector representations. While the methodological aspects utilize representation learning, the reviewers felt that the main thrust of the work would be better suited for a summarization workshop or even NLP venue, as it did not target DL based contributions. Additionally they felt that the work did not significantly engage with the long literature on the problem of summarization.
"Sleep" is introduced as a way of increasing robustness in neural network training. To sleep, the network is converted into a spiking network and goes through phases of more and less intense activation. The results are quite good when it comes to defending against adversarial examples. Reviewers agree that the method is novel and interesting. Authors responded to the reviewers  questions (one of the reviewers had a quite extensive set of questions) satisfactorily, and improved the paper significantly in the process. I think the paper should be accepted on the grounds of novelty and good results.
This work examines a problem that is of considerable interest to the community and does a good job of presenting the work. The AC recommends acceptance.
This paper presents a mean field analysis of the effect of batch norm on optimization. Assuming the weights and biases are independent Gaussians (an assumption that s led to other interesting analysis), they propagate various statistics through the network, which lets them derive the maximum eigenvalue of the Fisher information matrix. This determines the maximum learning rate at which learning is stable. The finding is that batch norm allows larger learning rates.  In terms of novelty, the paper builds on the analysis of Karakida et al. (2018). The derivations are mostly mechanical, though there s probably still sufficient novelty.  Unfortunately, it s not clear what we learn at the end of the day. The maximum learning rate isn t very meaningful to analyze, since the learning rate is only meaningful relative to the scale of the weights and gradients, and the distance that needs to be moved to reach the optimum. The authors claim that a "higher learning rate leads to faster convergence", but this seems false, and at the very least would need more justification. It s well known that batch norm rescales the norm of the gradients inversely to the norm of the weights; hence, if the weight norm is larger than 1, BN will reduce the gradient norm and hence increase the maximum learning rate. But this isn t a very interesting effect from an optimization perspective. I can t tell from the analysis whether there s a more meaningful sense in which BN speeds up convergence. The condition number might be more relevant from a convergence perspective.  Overall, this paper is a promising start, but needs more work before it s ready for publication at ICLR.  
This work proposes a method for identifying appropriate graphical models through enumeration, pruning of redundant dependencies, and neural network conditionals. While structure learning is an interesting application and there are some promising results, there were a number of concerns around experimental evaluation, computational complexity of the method, clarity of the presentation, and connections to prior work. In particular, R1 s concerns around the large field of structure learning in Bayesian Networks, and unwillingness to use the established terminology (and comparing to methods there) was not sufficiently addressed in the rebuttal.
This paper proposes a federated learning method called FedProf that adaptively selects different subsets of the clients  data in training the global model. There were several concerns brought up in the reviews and discussion. The multivariate Gaussian (with identity covariance) assumption on the neural network representation is limited. The paper also claimed to provide privacy preservation, but there is no formal statement of the actual privacy guarantees. (The fact that it s running federated learning does not guarantee privacy protection.) The presentation could use improvement. The reviewers had issues trying to understand the main theorem. Overall, there is not sufficient support for acceptance.
In this paper, neural networks are taken a step further by increasing their biological likeliness.  In particular, a model of the membranes of biological cells are used computationally to train a neural network.  The results are validated on MNIST.  The paper argumentation is not easy to follow, and all reviewers agree that the text needs to be improved.  ˜The neuroscience sources that the models are based on are possibly outdated.  Finally, the results are too meagre and, in the end, not well compared with competing approaches.  All in all, the merit of this approach is not fully demonstrated, and further work seems to be needed to clarify this.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The problem and approach, steganography via GANs, is interesting.   The results seem promising.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  The original submission was imprecise and difficult to follow and, while the AC acknowledges that the authors made significant improvements, the current version still needs some work before it s clear enough to be acceptable for publication.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Concerns varied by reviewer and there was no main point of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers did not reach a consensus. The final decision is aligned with the less positive reviewers, one of whom was very confident in his/her review. The AC agrees that the paper should be made clearer and more precise. 
 + The paper proposes an interesting empirical measure of ""learnability"" of a trained network: how well the predictive function it represents can be learned by another network. And shows it empirically seems to correlate with better generalization.    The work is purely empirical: it features no theory relating this learnability to generalization    Learnability measure is somewhat ad hoc with moving parts left to be specified (learning network, data splits, ...)    as pointed out by a reviewer, learnability doesn t really provide any answers for now.    the work would be much stronger if it went beyond a mere correlation study, and if learnability considerations allowed to derive a new approach/regularization scheme that was convincingly shown to improve generalization. 
The authors propose a way to produce uncertainty measures in graph neural networks. However, the reviewers find that the methods proposed lack novelty and are incremental additions to prior work.
All three reviewers expressed concerns about the assumptions made for the local stability analysis. The AC thus recommends "revise and resubmit".
Although one reviewer recommended accepting this paper, they were not willing to champion it during the discussion phase and did not seem to truly believe it is currently ready for publication. Thus I am recommending rejecting this submission.
Meta score: 4  The paper concerns the development of a density network for estimating uncertainty in recommender systems.  The submitted paper is not very clear and it is hard to completely understand the proposed method from the way it is presented.  This makes assessing the contribution of the paper  difficult.  Pros:    addresses an  interesting and important problem    possible novel contribution  Cons:    poorly written, hard to understand precisely what is done    difficult to compare with the state of the art, not helped by disorganised literature review    experimentation could be improved  The paper needs more work before being ready for publication.
The topic and ambition of this paper has been judged as important by all reviewers. Yet there is a consensus that the theoretical and experimental contribution is not strong enough to effectively argue for an important novel lead which would justify publication at ICLR. For these rejections, this paper cannot be endorsed for publication at ICLR 2022.
This paper proposes using visual representations learned in a monolingual setting with image annotations into machine translation. Their approach obviates the need to have bilingual sentences aligned with image annotations, a very restricted resource. An attention layer allows the transformer to incorporate a topic image lookup table. Their approach achieves significant improvements over strong baselines. The reviewers and the authors engaged in substantive discussions. This is a strong paper which should be included in ICLR.   
 The paper proposes to train LSTMs to encode car crashes (a temporal sequence of 3D mesh representations).  Decoder LSTMs can then be used to 1) reconstruct the input or 2) predict the future sequence of structural geometry.  The authors propose to use a spectral feature representation based on prior work as input into the encoding LSTM.  The main contribution of the paper (based on the author response) is the introduction of this spectral feature representation to the ML community.  The authors used single 3D truck model to generate 205 simulations, of which 105 was used for training, and 100 for testing.  The authors presented reconstruction errors and TSNE visualization of the LSTM s reconstruction weights.  Discussion Summary: The paper got three weak rejects.  The response provided by the authors failed to convince any of the reviewers to adjust their scores.  The authors did not provide a revision based on the reviewer comments.  Overall, the reviewers found the problem statement to be interesting.  However, they had concerns about the following: 1. It s unclear what is the main technical contribution of the work.  Several of the reviewers pointed out the lack of technical novelty.  From the writing, it s unclear if the proposed spectral feature representation is taken directly from prior work or there was some additional innovation in this submission.  Based on the author response, it seems the proposed feature representation is taken directly from prior work as the authors themselves acknowledge that the submission is taking two known ideas and combining them.  This can be made more explicit in the paper itself.    2. Lack of comparison with existing work and experimental analysis There is no comparison against existing work on predicting 3D structure deformation over time. While the proposed representation is interesting, the is no comparison with other methods or other alternative representations.  Without any comparisons it is difficult to judge how the reconstruction error corresponding to actual reconstruction quality.  How much error is acceptable?  The submissions also fails to elucidate when the proposed representation should be used.  Is it better than alternative representations (use 3D mesh directly? use point clouds? use alternate basis functions?)   3. What is being learned by the model?   R3 pointed out that the authors mention that the model is trained in just half an hour and questioned whether the dynamics function is trivial to learn and that the only two parts of the 3D structure is analyzed.  The authors responded that the "coarse" dynamic is easier to learn than the "fine" scale dynamics.  Is what is learned by the model sufficient?  How well would a model that just modeled the car as a rigid object and predicted the position do?  The lack of comparison against baselines and alternative methods/representations makes it difficult to judge usefulness of the representation/approach that is presented.  4. The paper also has minor typos.  Page 5: "treat the for beams"  > "treat the four beams" Page 7: "marrked"  > "marked"  Overall the paper addresses a interesting problem domain, and introduces a interesting representation to the ML community, but fails to do a proper experimental analysis showing how the representation compares to alternatives.  Since the paper does not claim the novelty of the representation as its contribution, it is essential that it performs a thorough investigation of the task and perform empirical studies comparing the proposed representation/method against baselines and alternatives.
This paper studies the robustness of CapsNets under adversarial attacks. It is found that the votes from primary capsules in CapsNets are manipulated by adversarial examples and that the computationally expensive routing mechanism in CapsNets incurs high computational cost. As such, a new adversarial attack is specially designed by attacking the votes of CapsNets without having to involve the routing mechanism, making the method both effective and efficient.  **Strengths:**   * This is the first work which proposes an attack specifically designed for CapsNets by exploiting their special properties.   * The proposed vote attack is more effective and efficient than the other attacks originally proposed for CNNs rather than CapsNets.   * The paper is generally well written.   * The experimental study is quite comprehensive.   * The code will be made available to facilitate reproducibility.  **Weaknesses:**   * The study is mostly for only one type of CapsNets. It is not clear whether the observations in this paper still hold generally for other types of CapsNets even after some additional experiments have been added.   * The presentation of the paper has room for improvement.  The authors are recommended to proofread the references thoroughly to ensure style consistency such as the consistent use of capitalization, e.g.   * “Star caps”  > “STAR Caps”   * “ieee symposium on security and privacy (sp)”  > “IEEE Symposium on Security and Privacy (SP)”  Despite its weaknesses especially those pointed out by Reviewer 2, this paper would be of interest to other researchers as it is the first paper that studies adversarial attacks on CapsNets. 
This manuscript analyzes the convergence of federated learning wit hstragellers, and provides convergence rates. The proof techniques involve bounding the effects of the non identical distribution due to stragglers and related issues. The manuscript also includes a thorough empirical evaluation. Overall, the reviewers were quite positive about the manuscript, with a few details that should be improved. 
This paper shows empirically that the state of the art language models have a problem of increasing entropy when generating long sequences. The paper then proposes a method to mitigate this problem. As the authors re iterated through their rebuttal, this paper approaches this problem theoretically, rather than through a comprehensive set of empirical comparisons.  After discussions among the reviewers, this paper is not recommended to be accepted. Some skepticism and concerns remain as to whether the paper makes sufficiently clear and proven theoretical contributions.  We all appreciate the approach and potential of this paper and encourage the authors to re submit a revision to a future related venue.
this submission proposes a learning algorithm for resnets based on their interpreration of them as a discrete approximation to a continuous time dynamical system.  all the reviewers have found the submission to be clearly written, well motivated and have proposed an interesting and effective learning algorithm for resnets.
This paper proposes a unified cross lingual pretraining method that works well for both natural language understanding (NLU)—typically done using encoder only architectures like mBERT and XLM—and conditional natural language generation (NLG) tasks like machine translation—typically done using encoder decoder architectures like mBART.  This paper clearly split reviewers, with 2 quite or very positive on it, and 3 thinking or leaning towards thinking that it didn t have enough novelty to merit publication.  Pro   The model produces good SoTA results   The method is easily replicable   It is good for the community for leading systems on benchmark tasks to have published papers describing how they work.  Con    The work is not groundbreaking in technical novelty   The work has to do a better job of communicating its contributions: It s hard to understand how it differs from other methods  On balance, the overall assessment is that the paper is not yet ready in its current form. The hope is that authors find the reviewer comments useful for preparing a future submission:    The paper **has** to do a better job of communicating its contributions. All that most researchers got from the first version was that there was parameter sharing and that helped. The revised version starts to do a better job of explaining the value of having the IS MLM and CS MLM objectives to doing well on NLU and NLG tasks, but much more is needed, as the discussion here shows. Indeed, even the discussion here is often opaque. In describing the key contribution of the paper, in both the revised paper and discussion, the authors fall back on phrases like "elaborately designed" and "exquisite cooperation of parameter sharing and pre training tasks". **What do "elaborately designed" and "exquisite cooperation" mean?!?** I think you can minimally clearly explain the benefits of having an objective like IS MLM for doing better on NLU tasks than the approach taken in mBART. You could argue for the advantages of MLM vs LM generation, which has been shown in other papers, including the original BERT paper and ELECTRA. Concretely, I wonder if you should reverse the contents of section 2 and start with equation (8) and explain why that is a good objective for your system, and better than ones that have been used previously. This discussion should be at a higher level than the current discussion under (8) which tends to be in the weeds. I haven t worked all the details, but I think you could then describe the objectives of section 2.2 before describing the implementation in section 2.1, and the result might be clearer? It would certainly emphasize the importance of these loss functions.   The initial version didn t have important details like the number of languages covered in the main paper; the current version fixes this to the extent of saying you have 50, but still doesn t give the context of how this compares with XLM R and mBART. And several reviewers had questions about the number of parameters of different models. I think you could fix a lot of these concerns by moving Table 8 to the main paper in a future resubmission. It doesn t take up much space and helps a lot in providing these details and easy to find citations for the models compared in other papers. 
This paper presents a MIL method for medical time series data. General consensus among reviewers that work does not meet criteria for being accepted.  Specifically:  Pros:   A variety of meta learning parameters are evaluated for the task at hand.   Minor novelty of the proposed method  Cons:   Minor novelty of the proposed method   Rationale behind architectural design   Thoroughness of experimentation   Suboptimal choice of baseline methods   Lack of broad evaluation across applications for new design   Small dataset size   Significance of improvement 
The paper proposes to improve sequential recommendation by extending SASRec (from prior work) by adding user embedding with SSE regularization.  The authors show that the proposed method outperforms several baselines on five datasets.  The paper received two weak accepts and one reject.  Reviewers expressed concerns about the limited/scattered technical contribution.  Reviewers were also concerned about the quality of the experiment results and need to compare against more baselines.  After examining some related work, the AC agrees with the reviewers that there is also many recent relevant work such as BERT4Rec that should be cited and discussed.  It would make the paper stronger if the authors can demonstrate that adding the user embedding to another method such as BERT4Rec can improve the performance of that model.  Regarding R3 s concerns about the comparison against HGN, the authors indicates there are differences in the length of sequences considered and that some method may work better for shorter sequences while their method works better for longer sequences.  These details seems important to include in the paper.   In the AC s opinion, the paper quality is borderline and the work is of limited interest to the ICLR community.  Such would would be more appreciated in the recommender systems community.  The authors are encouraged to improve the paper with improved discussion of more recent work such as BERT4Rec, add comparisons against these more recent work, incorporate various suggestions from the reviewers, and resubmit to an appropriate venue.
This paper presents a new dataset for fact verification in text from tables. The task is to identify whether a given claim is supported by the information presented in the table. The authors have also presented two baseline models, one based on BERT and based on symbolic reasoning which have an ok performance on the dataset but still very behind the human performance. The paper is well written and the arguments and experiments presented in the paper are sound.  After reviewer comments, the authors have incorporated major changes in the paper. I recommend an Accept for the paper in its current form.
This is an interesting study analyzing learning trajectories and their dependence on hyperparameters, important for better understanding of learning in deep neural networks.  All reviewers agree that the paper has a useful message to the ICLR community, and appreciate changes made by the authors in response to the initial reviews.
This paper studies online MAP inference and learning for nonsymmetric determinantal point processes (NDPPs). The main contribution is an online greedy algorithm. Surprisingly they show that their algorithm outperforms various offline algorithms on real world datasets. That said, the main concern was the novelty with respect to the prior work of Bhaskara et al. who gave an online approximation algorithm for MAP inference in DPPs. To compare the two works: (1) Bhaskara et al. give an algorithm for DPPs, and NDPPs are more complex (2) Bhaskara et al. give provable guarantees on the approximation ratio, but no such guarantees are known for NDPPs (3) And finally, some of the key ingredients in the online algorithm for NDPPs, like the stash, were already in the work of Bhaskara et al. Overall the reviewers felt that this submission would be improved with a clearer discussion of the contributions over prior work.
This paper builds a connection between MixUp and adversarial training. It introduces untied MixUp (UMixUp), which generalizes the methods of MixUp. Then, it also shows that DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios. Though it has some valuable theoretical contributions, I agree with the reviewers that it’s important to include results on adversarial robustness, where both adversarial training and MixUp are playing an important role.
This paper introduces Mahe, a model agnostic hierarchical explanation technique, that constructs a hierarchy of explanations, from local, context dependent ones (like LIME) to global, context free ones. The reviewers found the proposed work to be a quite interesting application of the neural interaction detection (NID) framework, and overall found the results to be quite extensive and promising.  The reviewers and the AC note the following as the primary concerns of the paper: (1) a crucial concern with the proposed work is the clarity of writing in the paper, and (2) the proposed work is quite expensive, computationally, as the exhaustive search is needed over local interactions.  The reviewers appreciated the detailed comments and the revision, and felt the revised the manuscript was much improved by the additional editing, details in the papers, and the additional experiments. However, both reviewer 1 and 3 have strong reservations about the computational complexity of the approach, and the additional experiments did not alleviate it. Further, reviewer 1 is still concerned about the clarity of the work, finding much of the proposed work to be unclear, and recommends further revisions.  Given these considerations, everyone felt that the idea is strong and most of the experiments are quite promising. However, without further editing and some efficiency strategies, it barely misses the bar of acceptance.  
This paper s idea is to augment pre trained word embeddings on a large corpus with embeddings learned on the data of interest. This is shown to yield better results than the pre trained word embeddings alone. This contribution is too limited to justify publication at iclr.
This work presents a "shadow attack" that fools certifiably robust networks by producing imperceptible adversarial examples by search outside of the certified radius. The reviewers are generally positive on the novelty and contribution of the work. 
The paper tackles the problem of detecting anomalies in multiple time series. All the reviewers agreed that the methodology is novel, sound and very interesting. Initially, there were some concerns regarding the experimental evaluation, however, the rebuttal and subsequent discussion cleared up these concerns to some extent and all reviewers are eventually supporting or strongly supporting acceptance.
This paper proposes a message passing neural network to solve PDEs. The paper has sound motivation, clear methodology, and extensive empirical study. However, on the other hand, some reviewers also raised their concerns, especially regarding the lack of clear notations and sufficient discussions on the difference between the proposed method and previous works. Furthermore, there is no ablation study and the generalization to multiple spatial resolution is not clearly explained. The authors did a very good job  during the rebuttal period: many concerns/doubts/questions from the reviewers were successfully addressed and additional experiments have been performed to support the authors  answers. As a result, several reviewers decided to raise their scores, and the overall assessment on the paper turned to be quite positive.
 * Strengths  The paper addresses a timely topic, and reviewers generally agreed that the approach is reasonable and the experiments are convincing. Reviewers raised a number of specific concerns (which could be addressed in a revised version or future work), described below.  * Weaknesses  Some reviewers were concerned the baselines are weak. Several reviewers were concerned that relying on failures observed during training could create issues by narrowing the proposal distribution (Reviewer 3 characterizes this in a particularly precise manner). In addition, there was a general feeling that more steps are needed before the method can be used in practice (but this could be said of most research).  * Recommendation  All reviewers agreed that the paper should be accepted, although there was also consensus that the paper would benefit from stronger baselines and more close attention to issues that could be caused by an overly narrow proposal distribution. The authors should consider addressing or commenting on these issues in the final version.
A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC like problems.  The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton s (2015) meta interpretive learning framework, which is usual for rule learning systems.  Another novel aspect is use of a cache memory for rules.  Pros    the idea of using RL instead of carefully designed discrete search for symbolic learning systems is a very nice novel idea    the experimental results are strong  Cons    the benchmarks are synthetic (although GraphLog does at least include noise)    although the Cropper and Muggleton work is cited the relationship between the search spaces of the two systems is not discussed   this should be corrected in the final version.
This paper presents a large scale annotation of human derived attention maps for ImageNet dataset. This annotation can be used for training more accurate and more interpretable attention models (deep neural networks) for object recognition. All reviewers and AC agree that this work is clearly of interest to ICLR and that extensive empirical evaluations show clear advantages of the proposed approach  in terms of improved classification accuracy. In the initial review, R3 put this paper below the acceptance bar requesting major revision of the manuscript and addressing three important weaknesses: (1) no analysis on interpretability; (2) no details about statistical analysis; (3) design choices of the experiments are not motivated. Pleased to report that based on the author respond, the reviewer was convinced that the most crucial concerns have been addressed in the revision. R3 subsequently increased assigned score to 6. As a result, the paper is not in the borderline bucket anymore. The specific recommendation for the authors is therefore to further revise the paper taking into account a better split of the material in the main paper and its appendix. The additional experiments conducted during rebuttal (on interpretability) would be better to include in the main text, as well as explanation regarding statistical analysis.  
The authors make an experimental study of the relative merits of RNN type approaches and graph neural network approaches to solving node labeling problems on graphs.   They discuss various improvements in gnn constructions, such as residual connections.  This is a borderline paper.  On one hand, the reviewers feel that there is a place for this kind of empirical study, but on the other, there is agreement amongst the reviewers that the paper is not as well written as it could be.  Furthermore, some reviewers are worried about the degree of novelty (of adding residual connections to X).  I will recommend  rejection, but urge the authors to clarify the writing and expand on the empirical  study and resubmit. 
All reviewers (including those with substantial expertise in RL) were solid in their praise for this paper that is also tackling an interesting application that is much less well studied but deserves attention.  
Dear Authors,  Thank you very much for your detailed feedback to the reviewers in the rebuttal phase. The feedback certainly clarified some of the concerns raised by the reviewers and improved their understanding of your work. Indeed, some of the reviewers have increased their scores.  However, overall, we think this paper has rather marginal novelty and there are still several conceptual and technical issues to be further discussed, such as the definition of the grouping concept and the distributional shift assumption.  For these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. I hope that the detailed feedback and additional comments from the reviewers help you improve this work for future publication. 
A method is proposed for removing prior knowledge, presented as a distance matrix, from low dimensional embeddings, to focus them on what is new.  The task of visualizing novely in data is interesting and good solutions would potentially be highly useful.  The proposed method essentially substracts a distance matrix from another. While this is sensible, it is not completely clear in what sense this is _the_ right solution for what the embeddings will be used for.  In final discussions among the reviews, the main remaining concerns were considered severe: comparisons to other methods being limited, and possible problems in one of the experiments. 
This paper presents a neural topic model with the goal of improving topic discovery with a PLSA loss. Reviewers point out major limitations including the following:  1) Empirical comparison is done only with LDA when there are many newer models that perform much better. 2) Related work section is incomplete, especially for the newer models. 3) Writing is unclear in many parts of the paper.  For these reasons, I recommend that the authors make major improvements to the paper before resubmitting to another venue.
The paper proposes a method to produce embeddings of discrete objects, jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others. While the paper is well written, and proposes an interesting solution, the contribution seems rather incremental (as noted by several reviewers), considering the existing literature in the area.  Also, after discussions the usefulness of the method remains a bit unclear   it seems some engineering (related to sparse operations) is still required to validate the viability of the approach. 
The authors propose an approach for continual learning of a sequence of tasks which augments the network with task specific neurons which encode  adversarial subspaces  and prevent interference and forgetting when new tasks are being learnt. The approach is novel and seems to work relatively well on a simple sequence of MNIST or CIFAR10 classes, and has certain advantages, such as not requiring any stored data. However, the reviewers agreed that the presentation of the method is quite confusing and that the paper does not provide adequate intuition, visualisation, or explanation of the claim that they are preventing forgetting through the intersection of adversarial subspaces. Moreover, there was a concern that the baselines were not strong enough to validate the approach.
This paper proposes max margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.  Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state of the art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.  Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.
This paper considers the reinforcement learning in rich observation setting. Concretely, the authors provide a provable sample efficient algorithm for the rich observation factored MDP. As the majority of the reviewers commented, although the techniques used in the proof share some similarities to the existing work, the analysis for the whole algorithm is still challenging. As a theoretical oriented paper, I think this paper should have a position in ICLR.   The major concern of the paper is the necessity of the assumptions (R2). The validation and justification of the Assumptions should be stated clearly in main text, even they are adapted from the prior work. 
Meta Review for Neural Circuit Architectural Priors for Embodied Control  The motivation of this work is to address an important challenge: To understand innate contributions to neural circuits for motor control. This paper proposes both a set of reusable architectural components and design principles, and also interesting principles for producing biologically inspired neural networks for embodied control. This work aims to be at the intersection between neuroscience and machine learning for improving the design of artificial neural networks and improving our understanding of observed biological networks. In their model, various components of biological networks are replicated (such as the balance between excitation and inhibition, sparsity, and oscillation). They show that a resulting model, inspired by C.elegans, can learn to swim more efficiently (when evaluated on the Swimmer RL environment) and requires fewer parameters while achieving similar accuracy as an MLP.  Most reviewers, including myself, recognize (and appreciate) the ambition of this work, and are excited at the goal of looking at problems from the perspectives of both system neuroscience and machine learning. The motivations of this paper are clearly explained, and the paper is well written (also the diagrams are great). I m very excited about this work, and hope to see it succeed, but in the paper s current state (even with the revisions), I don t think it addresses the reviewers  main concerns.  After discussions and examining the paper and the reviews in detail, I feel reviewer GaKc best summarizes the main issues with the work at its current state:    This paper is interesting but does not proposes a significant improvement to the literature as the gap between the promises made in the motivation and actually delivered work is too wide.    From a neuroscience point of view, this work does not provide substantial evidence of the importance of the model at either modeling or simulating biological neural systems.    From a "theoretical" point of view, the model does not provide much advancement to the machine learning community either.  So while the current work (especially in the revised state) I would consider to be an outstanding workshop paper, I cannot recommend it for acceptance at ICLR 2022. An advice I would give to the authors (as someone who publishes to ML conferences, and Sys Neuro/Bio venues) is that for these ML conferences, it might be easier to make the narrative of the work narrower, and well defined. If the method is supposed to demonstrate significant advantages of biologically inspired network architecture over current RL methods, the results should clearly demonstrate convincing experimental results that can persuade the (non neuro) RL community to have interest in the method. If the method does not achieve SOTA results, then try to present the method capable of something really useful that existing RL methods simply fail at (and emphasize that as a core contribution). Conversely, if the narrative is to use a bio inspired network to emulate biological behaviors, the method must have something important to offer for the community of people working on simulating biological neural systems.   I look forward to seeing this work improved and eventually published at a journal or presented at a conference in the future, good luck!
Apologies for only receiving two reviews. R2 gave a WR and R3 gave an A. Given the lack of 3rd review and split nature of the scores, the AC has closely scrutinized the paper/reviews/comments/rebuttal. Thoughts:    Paper is on interesting topic.    AC agrees with R2 s concern about the evaluation not using more complex environments like Mujoco. Without evaluation on a standard benchmark, it is difficult to know objectively if the approach works.     AC agrees with authors that the DISTRAL approach forms a strong baseline.     Nevertheless, the experiments aren t super compelling either.    AC has some concerns about scaling issues w.r.t. model size & #tasks.   The paper is very borderline, but the AC sides with R2 s concerns and unfortunately feels the paper cannot be accepted without a stronger evaluation. With this, it would make a compelling paper. 
The paper receives a unanimous accept over reviewers, though some concerns on novelty exist. So it is suggested to be a probable accept. 
The paper received mixed reviews. On one hand, there is interesting novelty in relation to biological vision systems. On the other hand, there are some serious experimental issues with the machine learning model. While reviewers initially raised concerns about the motivation of the work, the rebuttal addressed those concerns. However, concerns about experiments remained. 
The approach explore the use of Conditional Risk Minimization (CRM) as a post hoc operation to amend a classifier decision by averaging a prior class hierarchy. The authors show that it is beneficial for ranking predictions without sacrifying top 1 accuracy.  The rebuttal period clarified some reviewers  concern on paper presentation and experiments, and all reviewers recommend acceptance after the discussion period.  Although the approach is simple and directly revisits the use of CRM for deep models, the AC considers that the contribution is meaningful, and that the proposed method provides predictions with good ranking and calibration properties. The paper also sheds light into interesting issues in state of the art methods integrating class hierarchies during training. The AC therefore recommends paper acceptance. 
This paper brings interesting ideas (decentralized setting, auto distillation) but it does not meet the very high requirements that a publication at ICLR requires.  Three main reasons for that:  1/ Motivation & justification: Ultimately the paper is advocating for a pure decentralized approach "which encodes each entity from and only from the embeddings of its neighbors" with the main motivation being to represent better on unseen entities at training. This is quite radical and leads to a complex model and training procedure for a benefit and justification that are not very clear. Are there that many unseen entities in general? What would periodically retrain the whole model do? The computational cost associated to DecentRL should be discussed with regards to that. Some implementation details in appendix A.2 seems rather critical and are not motivated.  2/ Missing comparisons and references: as noted by several reviewers, it would be helpful to have comparisons of other methods that are dealing with missing entities. Some much simpler heuristics could be tried for instance (retraining the model, averaging neighbors, etc.). A discussion with DeepWalk, that is really an adaptation of CBOW for KG should also be added.  3/ Clarity could be improved. Thanks to reviewers  comments, the clarity has increased but could still be worked on as noted by several reviewers. For instance, the analogy  with CBOW right in the intro is confusing: in the 2nd paragraph, CBOW is used as a common manner for methods that are limited, but in the 3rd paragraph, CBOW is also used as an intuition for DecentRL. Some content from supplementary material like the description in A.1 would add a lot of clarity if added earlier.  We encourage the authors to use the many comments from the reviewers to improve further the paper. 
The reviewers rightly point out that presented analysis is limiting and that the experimental results are not extensive enough. Moreover, several existing work that use raw waveforms have interesting analysis of what the network is trying to learn. Given these comments, the AC recommends that the paper be rejected. 
The paper introduces the concept of overfitting in meta learning and proposes some solutions to address this problem. Overall, this is a good paper. It would be good if the authors could relate this work to meta learning approaches, which are based on hierarchical (Bayesian) modeling for learning a task embedding.  [1] Hausman et al. (ICLR 2018): Learning an Embedding Space for Transferable Robot Skills  https://openreview.net/pdf?id rk07ZXZRb [2] Saemundsson et al. (UAI 2018): Meta Reinforcement Learning with Latent Variable Gaussian Processes http://auai.org/uai2018/proceedings/papers/235.pdf 
I think there is a lot to commend in this paper: the general approach for training f_phi in this way is creative and interesting, the discussion of the amortization gap is thought provoking, and the general idea is not something that I have seen in the literature before. That said, the reviewers raise a number of important concerns about the approach, chief of which is that the paper s explanation for why the method works is questionable. The proposed method can be viewed as simply a more expressive policy architecture. In fact, I suspect strongly that the modest increase in performance is likely explained by this alone. The discussion with Reviewer 2 in particular makes this issue very clear. I don t think the authors offered a very compelling response to this. Therefore, I think there are just too many question marks about this approach to accept the paper for publication at this time.  I do however think that this line of research is very promising, and I would encourage the authors to continue this work and flesh out the evaluation to be more rigorous and complete, understand whether the increase in performance comes down simply to increased expressivity or if the discussion of amortization is closer to reality, and also address other concerns raised by R2 and the other reviewers.
This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set.  This substantially improves upon existing work.  The proposed method is well supported both with theory and experiments.  All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion.  The determinantal point process might not be one of the most popular topics in the ICLR community today but certainly is relevant.
The paper argues that existing evaluation metrics for GGMs are insufficient and perform an extensive empirical study questioning their ability to measure the diversity and fidelity of the generated graphs. To solve these limitations, they propose a new evaluation metric that computes the Maximum Mean Discrepancy (MMD) between graph representations of the sampled and real graphs, as extracted from an untrained GGM model.   All the reviewers agreed that the research problem is interesting and the overall idea behind the proposed metric is sound and novel. While there were some concerns regarding some details/comparisons/conclusions of the experimental evaluation, the rebuttal managed to cleared up these concerns and all the reviewers eventually supported acceptance.
This paper considers meta learning based on MAML.  The authors use Neural Tangent Kernels (NTKs) to develop two meta learning algorithms that avoid the inner loop adaptation, which makes MAML computationally intensive.  Experimental results demonstrate favorable empirical performance over existing methods.   The paper is generally well written and readable.  The proposed methods are well motivated and based on solid theoretical ground.  The emprirical performance shows advantages in efficiency and quality.   This work is worth acceptence in ICLR 2021. 
This paper proposes an anonymization method for federated learning based on the Indian buffet process. The reviewers found the idea interesting, but raised the following main concerns (please see the reviews for more details): * Motivation and terminology needs clarification * Better comparison with secure aggregation methods * Missing privacy guarantees Overall the reviewers of this paper are borderline. I hope the authors will take the reviewers  feedback into account when revising the paper. 
This work presents a method for debiasing graph embeddings. The main concerns for the work were originally identified by Reviewer 3, who pointed out that the method is only capable of linear debiasing. Authors responded by updating the manuscript in several places to mention this limitation as well as adding Table 3 to the Appendix showing that SVM s with non linear kernels are still able to identify bias in the embeddings. Reviewers agreed that this addition improved the manuscript, however some reviewers still had concerns about the revised manuscript. This AC has several recommendations for improving the paper. First additional revision is needed to better address the limitations of linear debiasing, for example Table 1 still reads "MONET is successful in removing all metadata information from the topology embeddings – the links in the graph are no longer an effective predictor of political party".  Statements like this are a bit misleading, as the embeddings will still be biased with respect to a non linear classifiers (as evident by Table 3). Additionally, updating Table 1 and related experiments to measure embedding bias with respect to non linear classifiers would help clarify the limitations for perspective readers. Second, the paper should be updated to address remaining concerns that the linear debiasing assumption limits the applicability of the method. One could either discuss or demonstrate additional applications of the method that work even with the linear assumption, extend MONET so it can improve model bias with respect to non linear classifiers, or show that MONET still outperforms baselines when the non linear assumption is violated.
This paper proposes a defense technique against query based attacks based on randomization applied to a DNN s output layer. It further shows that for certain types of randomization, they can bound the probability of introducing errors by carefully setting distributional parameters. It has some valuable contributions; however, the rebuttal does not fully address the concerns.
This paper proposes a suite of benchmark visual model based RL tasks to evaluate causal discovery approaches under systematically varying causal graphs. Despite some disagreement on this point among reviewers, I would come down on the side of saying that a better executed version of this paper would have been a good fit at ICLR. However, its current drawbacks make this a borderline reject. The most important of these drawbacks is: it is unclear to what extent results on these simple environments translate to more realistic complex ones.  Reviewers have also pointed to omitted relevant work that could be discussed in future versions, such as PHYRE. Another relevant benchmark in this vein: https://arxiv.org/abs/1907.09620  
The paper introduces a neat idea that an SGD update can be written as a solution of the linear least squares problem with a given backpropagated output; this is generalized to a larger batch size, giving a sort of "block" gradient type update. Some notes that the columns of $O_t$ have to be scaled are made, but not clear why. The paper then goes into the experiments, and then gets back to the fast approximation of DGB. It really looks like bad organization of the paper, which was noted.  The reviewers agree that the actual computational improvements are marginal, and all recommend rejection. As a recommendation, I would suggest to restructure the paper for a more coherent view, and also the improvements in Top 1 are not very stimulating. The general view is interesting, but it is not clear what insight it brings.
This paper presents a new multi task training and evaluation set up called the Natural Language Decathlon, and evaluates models on it. While this AC is sympathetic to any work which introduces new datasets and evaluation tasks, the reviewers agreed amongst themselves that the paper is not quite ready for publication. The main concern is that multi task learning should show benefits of transferring representations or other model components between tasks, demonstrating better generalisation and less task specific overfitting, but that the results in the paper do not properly show this effect. A more thorough study of which tasks "interact constructively" and what model changes can properly exploit this needs to be done. With this further work, the AC has no doubt that this dataset and task suite, and associated models, will be very valuable to the NLP community.  I should note that there were some issues during the review period which lead to AC confidential communication between AC and authors, and AC and reviewers, to be leaked to the reviewers. It was due to an OpenReview bug, and no party is at fault. Through private discussion with the interested parties, we were able to resolve this matter, and through careful examination of the discussion, I am satisfied that the reviews and final recommendations of the reviewers were properly argued for and presented in good faith.
The reviewers agree that the idea of utilizing covariance information in the few shot setting is interesting. There are concerns with the novelty of the paper, as well as the correctness in terms of ensuring the covariance matrix is PSD in all cases. There are some concerns with the experimental evaluation as well. In this area, Omniglot is a good sanity check, but other baseline datasets like miniImagenet are necessary to determine if this approach is truly useful.
This paper has some interesting ideas that have been implemented in a rather ad hoc way; the presentation focuses perhaps too much on engineering aspects.
The reviewers raised a number of major concerns including lack of explanations, lack of baseline comparisons, and lack of discussion on pros and cons of  the main contribution of this work    the presented Temporal Gaussian Mixture (TGM) layer. The authors’ rebuttal addressed some of the reviewers’ comments but failed to address all concerns (especially when it comes to the success of TGMs; it remains unclear whether this could be attributed solely to the way TGMs are applied rather than to their fundamental methodological advantage). Having said that, I cannot suggest this paper for presentation at ICLR.
This paper provides a mean field theory analysis of batch normalization. First there is a negative result as to the necessity of gradient explosion when using batch normalization in a fully connected network. They then provide further insights as to what can be done about this, along with experiments to confirm their theoretical predictions.  The reviewers (and random commenters) found this paper very interesting. The reviewers were unanimous in their vote to accept.
This paper presents an approach combining multi agent with hierarchical RL in a custom made simulated humanoid robotics setting.  Although it is an interesting premise and has a compelling motivation (multi agent, real world interaction, humanoid robotics), the reviewers had some trouble pinpointing what the significant contributions are. Partly this is due to lack of clarity in the presentation, such as with overlong sections (eg 5.2), unclear descriptions, mistakes in the text, etc. Reviewers also remarked that this paper might be trying to do too much, without performing the necessary experiments/comparisons and analyses needed to interpret the contributions of each component.   This work is definitely promising and has the potential to make a nice contribution, given some additional care (experiments, analyses) and rewriting/polishing. As it is, it’s probably a bit premature for publication at ICLR. 
This paper analyzes the latent concepts learned in BERT. In contrast to previous work which tries to map embeddings to predefined linguistic concepts this paper sets out to discover what is inherently learned by BERT. This is however easier said that done, since  there is no easy way to inspect the embeddings and draw conclusions on what is being learned. The authors adopt a methodology which could be used to inspect the inner workings of other pretrained models. They employ hierarchical clustering to discover latent concepts and then inspect these clusters by manually labeling them. The reviewers raised various issues regarding the number of clusters, and the amount of effort required which de facto renders the approach not very portable. The authors addressed the comments and flagged several difficulties with undertaking such an analysis. I will vote for the paper to be presented as an oral for two reasons a) it is difficult to analyze pretrained models, and although I am not convinced what the authors propose is feasible, it will at least get the discussion going, b) the manually annotated dataset is useful and will go towards allowing us to perform comparisons between models c) the annotation tool will be useful to others if the authors are considering releasing it.
This paper tackles the problem of autonomous skill discovery by recursively chaining skills backwards from the goal in a deep learning setting, taking the initial conditions of one skill to be the goal of the previous one. The approach is evaluated on several domains and compared against other state of the art algorithms.  This is clearly a novel and interesting paper. Two minor outstanding issues are that the domains are all related to navigation, and it would be interesting to see the approach on other domains, and that the method involves a fair bit of engineering in piecing different methods together. Regardless, this paper should be accepted.
Evaluating this paper is somewhat awkward because it has already been through multiple reviewing cycles, and in the meantime, the trick has already become widely adopted and inspired interesting follow up work. Much of the paper is devoted to reviewing this follow up work. I think it s clearly time for this to be made part of the published literature, so I recommend acceptance. (And all reviewers are in agreement that the paper ought to be accepted.)  The paper proposes, in the context of Adam, to apply literal weight decay in place of L2 regularization. An impressively thorough set of experiments are given to demonstrate the improved generalization performance, as well as a decoupling of the hyperparameters.   Previous versions of the paper suffered from a lack of theoretical justification for the proposed method. Ordinarily, in such cases, one would worry that the improved results could be due to some sort of experimental confound. But AdamW has been validated by so many other groups on a range of domains that the improvement is well established. And other researchers have offered possible explanations for the improvement. 
This paper proposes to quantify the uncertainty of neural network models with Beta, Dirichlet and Dirichlet Multinomial likelihood. This paper is clearly written with a sound main idea. However, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output. All the reviewers therefore considered this paper to be of limited novelty. Reviewer 2 also had a concern about the mixed experimental results of the proposed method.  Reviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters. It is a common consideration in a Bayesian approach and I encourage the authors to discussed different sources of uncertainty in future revisions.
The paper is hard to follow at times. The heuristic reward has little justification   not clear how this would extend to other domains. Lack of empirical comparisons (see e.g. Hester et al., Deep Q Learning from Demonstrations, 2017). 
This submission proposes a trainable quantum tensor network (QTN) for quantum embedding generation on a variational quantum circuit (VQC), which is followed by a significant empirical study on the QTN VQC performance on the MNIST test dataset. However, the discussion during the review period raises some serious concerns about both the theoretical and empirical contributions of the submission.   On the theory side, we acknowledge the authors propose the use of tensor train network (TTN) as the dimension reduction layer of input features, design the tensor product encoding (TPE), and characterize the representation power of QTN VQC.  However, a general feeling among all reviewers is that these contributions are kind of observational, rather than deep theoretical insights, based on existing results. For example, TTN is a well established tool in dimension reduction, and the representation power of QTN VQC can be derived as simple corollaries of the existing universal approximation theorem for a feed forward neural network.  Moreover, the authors emphasize a lot that the use of TTN allows a genuine end to end quantum implementation of QTN VQC because TTN can be relatively easier to implement as quantum circuits. However, the benefit of this end to end quantum implementation is rather unclear, especially for NISQ applications. In particular, because NISQ machines have limited quantum resources, should not a classical quantum hybrid implementation be preferred in the interest of saving quantum resources? As genuine quantum implementation becomes the major motivation for using TTN, there is no other theoretical justification for selecting TTN.   We appreciate the authors’ efforts in carrying out the empirical study and the active interaction during the discussion period. Unfortunately,  most of the reviewers are not convinced by the existing experimental results. This is partially because the authors seek to support a very strong claim that QTN VQC would outperform the state of the art classical solutions, whereas the limitation of current quantum devices prevents any empirical study of QTN VQC at the scale comparable to the classical solutions. Moreover, as pointed out by one of the reviewers, there is hardly any existing evidence that quantum neural networks would be useful at all for commonly used datasets in NLP and vision. Given that, it is very unlikely that a conclusion could be reached by an empirical study on small scale instances. Nevertheless, since the authors aim to compare QTN with other quantum embedding methods,  other kinds of experiments are possible without directly comparing with the state of the art classical solutions.   We believe that the submission would benefit a lot from addressing the concerns for both the theory and empirical parts and hope the authors would pursue it.
The paper proposed a novel approach that leverages the discrepancies between the (global) series association and the (local) prior association for detecting anomalies in time series. The authors provided detailed empirical support to motivate the above detection criterion, and introduced a two branch attention architecture for modeling the discrepancies and establishing an anomaly score.  All reviewers acknowledge the technical novelty of this work (including the key insight of modeling anomalousness with Transformer’s self attention and concrete training mechanism via a minimax optimization process) as well as the comprehensiveness of the empirical study.   Meanwhile, there were some concerns in the positioning of the work, in particular in the clarity in connection to related work, and some reviews concern the clarity of the presentation (e.g. missing some details in experimental results), and the clarity of the exposition of the training process. The authors provided effective feedback during the discussion phase, which helped clarify many of the above concerns. All reviewers agree that the revision makes a solid paper and unanimously recommend acceptance of this work.   The authors are strongly encouraged to take into account the feedback from the discussion phase to further improve the clarity concerning the technical details as well as the reproducibility of the results.
The paper studies inductive biases in DRL, by comparing with different reward shaping, and curriculums. The authors performed comparative experiments where they replace domain specific heuristics by such adaptive components.  The paper includes very little (new) scientific contributions, and, as such, is not suitable for publication at ICLR.
Simple idea (which is a positive) to regularize RNNs, broad applicability, well written paper. Initially, there were concerns about  comparisons, but he authors have provided additional experiments that have made the paper stronger. 
Reviewers all found the work well motivated in addressing uncertainty, a topic that has not seen much focus in meta learning and few shot learning. They describe the challenges well: small sample sizes and OOD shift. They then propose a solution they find works well empirically to overcome these challenges based on a set encoder and an energy function respectively.  The proposal is largely one of engineering components that have been found to work well in the literature. I m sympathetic to this style of research (particularly in today s neural network research), although the reviewers raise a primary concern about whether the choices leading to the proposal are justified. In particular, two Reviewers argue that there are no clear ablations compared to alternative simpler approaches, and so the approach of selecting a Set Transformer is rather arbitrary. My perspective is that theory provides one sufficient but not necessary angle to do this, and I do find the authors  replies to the two reviewers convincing. In particular, they add a baseline to estimate covariances suggested by Reviewer Zz5v and they describe how the current baselines do in fact use the shrinkage suggestion by Reviewer QrCN.  I recommend the authors use the reviewers  feedback to enhance their submission s clarity and overall quality.
The paper proposes a method for offline meta RL, where we meta train on pre collected offline data for several RL tasks and adapt to a new task with a small amount of data. The paper assumes that there is no interaction with the environment either during meta train or meta test.  In this setting, motivated by the ide of leveraging offline experience from multiple tasks to enable fast adaptation to new tasks, the paper introduces MACAW, which combines the consistent MAML and the popular offline AWR, improving upon them by adding capacity through parameterization and adding an extra objective in the policy update. As a result, the MACAW proposed for the offline meta RL has the desirable property of being consistent, i.e., converging to a good policy if enough time and data for the meta test task are given, regardless of meta training.    Pros:  + Most of the experiments are well executed, using good baselines. Extensive ablations on the various modifications to MAML+AWR confirmed the utility of the approach for the fully offline meta RL problem. + MACAW is a simple algorithm with theoretical guarantees; the modifications to the policy functions are backed by theory.    Cons:   The reviewers have concerns on the formulation of offline meta RL. One major contribution of the paper is to introduce offline meta RL. However the paper largely borrows the meta RL formulation from the online setting where task MDP. The reviewers think that directly borrowing from regular meta RL as the formulation of offline meta RL might be misleading. The reviewers suggest including behavior policy as part of the task definition for offline meta RL formulation.    Several reviewers raised concerns that the fully offline setting might be unrealistic. Although the author did add a motivation, the reviewers would be interested in seeing MACAW being adapted online at test time on in distribution tasks.     Unfortunately, the authors accidentally revealed their names in one of the modified versions.  
The manuscript develops a novel method for uncertainty prediction that can be used in the context of active or reinforcement learning problems. They consider experiments such as an OOD Detection task wherein a ResNet is trained on CIFAR10 and predictions must subsequently be made for in  versus out of distribution (SVHN) data.  The work develops an approach based on directly estimating epistemic (as opposed to a aleatoric) uncertainty by learning to predict generalization error and then subtracting estimated aleatoric uncertainty. Reviewers found the essential approach to be novel and creative. However, there were several issues raised by reviewers that are not well addressed by responses by the authors. For example, Zaec worries about the dependence of the approach on an oracle for estimating aleatoric uncertainty. Multiple reviewers were concerned that this would make the approach unsuitable for many situations and thus limit the applicability of the ideas.  Multiple reviewers also found the manuscript to be difficult to understand. I agree with the sentiment. While there may indeed be an interesting and important idea here, the text and explication of the algorithm and approach are complicated and leave the reader unsure about the contribution. I would recommend that the authors invest time and effort in simplifying and streamlining the narrative and presenting the technical innovation so that it is easier to judge. In it s current form, the manuscript is premature for publication.
The authors explore modeling the relationship between domain slot pairs in multi domain dialogue state tracking via use of special tokens in pre trained contextualized word embeddings (i.e., one special token for each domain slot pair or special tokens for the domain and the slot that are merged). Beyond this, the basic architecture is very similar to the TRADE architecture (and papers that build on this general slot gate + slot value classifier) for the fixed vocabulary setting. Experiments are conducted on the MultiWOZ 2.1/2.2 datasets, demonstrating impressive improvements over recent results.    Pros   + They demonstrate that domain slot interdependencies can be modeled through special tokens for use with pre trained embeddings. + The top line empirical results are impressive.    Cons     Lack of a deep dive on the empirical analysis to show precisely why/where the proposed method is working better than existing work.   The methodological advance is minimal beyond using better pre trained embeddings.   Only one dataset when others exist and this is largely an empirical paper.   The writing is rushed and reads like a  late breaking  paper.  Evaluating along the specified dimensions: * Quality: The quality of the work was the primary concern of the reviewers. Specifically, this reads like a  late breaking  paper where the table of results is impressive, but there isn t significant examination of the empirical results showing why/when it works relative to competing methods. Focusing just on Tables 2 & 3, much of the improvement is ostensibly really due to the more powerful embeddings. Contextualizing this wrt {SimpleTOD, TRADE, DSTQA, Picklist}, this appears a minor methodological innovation centered around the input embeddings. The empirical results are impressive, but may very well be a result of the more powerful pre trained embeddings   additional empirical analysis and discussion might be able to convince the reader otherwise, but is lacking here. * Clarity: This is a very simple idea, so it should be easily understood by most familiar with the research area. That being said, the paper seems very rushed in general. * Originality: This applies ideas used in many NLP applications to the dialogue state tracking problem. As previously stated, the architecture is similar to several existing DST formulations   where the core idea is to model slot value interdependencies through the contextualized embeddings using special token. While not a trivial idea, it also is something that many could/would have put together. Until it is abundantly clear that this isn t really a study of how to apply larger pre trained embeddings to DST problems, it isn t clear that this is a significant dialogue systems advance beyond the strong performance. * Significance: As stated, this isn t a significant methodological advance. However, the empirical results appear very impressive   although the reviewers expressed some concerns regarding the evaluation. Since this is largely empirical, one of the reviewers pointed out that additional relevant datasets now exist, which would significantly strengthen the case.  In summary, the empirical results appear impressive, ostensibly setting the SoTA. However, there were several concerns regarding the novelty of the approach, if it is actually working better due to the reasons stated, sufficient analysis of the empirical results, amongst other things. Thus, despite the impressive results, the consensus evaluation was that this work is not ready for publication in its current form (even if the top line results should be disseminated). 
The paper analyzes MDPs with execution delays. Interesting theoretical results and experiments are provided, which show the benefits of the proposed algorithms. However, some issues are highlighted in the reviews, such as the lack of theoretical analysis of the proposed delayed Q learning method, and the simplicity of the experiments. The latter is at least partially addressed by the authors in the rebuttal, and the new experiments should be incorporated in the final paper.  
This paper presents "stein bridge", a joint training framework that connects an explicit (unnormalized) density estimator and an implicit sample generator via Stein discrepancy. The idea and methodology are valid and of interest. But the raised concerns were not properly addressed. 
This paper constructs a variant of deep CNNs which is provably invertible, by replacing spatial pooling with multiple shifted spatial downsampling, and capitalizing on residual layers to define a simple, invertible representation. The authors show that the resulting representation is equally effective at large scale object classification, opening up a number of interesting questions.  Reviewers agreed this is an strong contribution, despite some comments about the significance of the result; ie, why is invertibility a "surprising" property for learnability, in the sense that F(x)   {x,  phi(x)}, where phi is a standard CNN satisfies both properties: invertible and linear measurements of F producing good classification. All in all, this will be a great contribution to the conference. 
This paper introduces C learning, an approach to integrate temporal abstractions to value based methods. Specifically, it uses accessibility functions that estimate horizon aware value functions for goal reaching RL problems. Such an approach allows trading off reliability and speed. After careful consideration I’m recommending the acceptance of this paper. The main weaknesses raised by the reviewers were addressed during the rebuttal, including the improvement of presentation and the introduction of new experiments and baselines. There were not many actionable criticisms left after the discussion and the reviewers acknowledged that the paper has improved since its first version.  For the final version of the manuscript, I recommend the authors to further take R2’s comments/suggestions into consideration. Further incorporating the discussion about TDMs in the main text will improve clarity, better position the paper, and increase its likelihood of having impact. 
The paper investigates why adversarial training can sometimes degrade model performance on clean input examples.   The reviewers agreed that the paper provides valuable insights into how adversarial training affects the distribution of activations. On the other hand, the reviewers raised concerns about the experimental setup as well as the clarity of the writing and felt that the presentation could be improved.   Overall, I think this paper explores a very interesting direction and such papers are valuable to the community. It s a borderline paper currently but I think it could turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue.    
The paper presents a novel problem formulation, that of generating 3D object shapes based on their functionality. They use a dataset of 3d shapes annotated with functionalities to learn a voxel generative network that conditions on the desired functionality to generate a voxel occupancy grid. However, the fact that the results are not very convincing  resulting 3D shapes are very coarse  raises questions regarding the usefulness of the proposed problem formulation.  Thus, the problem formulation novelty alone is not enough for acceptance. Combined with a motivating application to demonstrate the usefulness of the problem formulation and results, would make this paper a much stronger submission. Furthermore, the authors have greatly improved the writing of the manuscript during the discussion phase.
This paper proposes X Mixup, a model that considers the source languages and target languages together for cross lingual transfer. The designed model takes a pair of sentences (or the translated sentences) in a source language and a target language as the input and computes the cross attention between them.    The empirical results are convincing. Reviewers think this paper is well written and the idea is interesting.
The paper proposes a decision theoretic framework for meta learning. The ideas and analysis are interesting and well motivated, and the experiments are thorough. The primary concerns of the reviewers have been addressed in new revisions of the paper. The reviewers all agree that the paper should be accepted. Hence, I recommend acceptance.
The authors extend the framework of randomized smoothing to handle non Gaussian smoothing distribution and use this to show that they can construct smoothed models that perform well against l2 and linf adversarial attacks. They show that the resulting framework can obtain state of the art certified robustness results improving upon prior work.  While the paper contains several interesting ideas, the reviewers were concerned about several technical flaws and omissions from the paper:  1) A theorem on strong duality was incorrect in the initial version of the paper, though this was fixed in the rebuttal. However, the reasoning of the authors on the "fundamental trade off" is specific to the particular framework they consider, and is not really a fundamental trade off.  2) The justification for the new family of distributions constructed by the author is not very clear and the experiments only show marginal improvements over prior work. Thus, the significance of this contribution is not clear.  Some of the issues were clarified during the rebuttal, but the reviewers remained unconvinced about the above points.  Thus, the paper cannot be accepted in its current form. 
Main content:  This paper presents negation handling approaches for Amharic sentiment classification.     Discussion:  All reviewers agree the paper is poorly written, uses outdated approaches, and requires better organization and formatting.     Recommendation and justification:  This paper after more work might be better submitted in an NLP workshop on low resource languages, rather than ICLR which is more focused on new machine learning methods.
This paper presents an encoder decoder based architecture to generate summaries. The real contribution of the paper is to use  a recoder matrix which takes the output from an existing encoder decoder network and tries to generate the reference summary again. The output here is basically the softmax layer produced by the first encoder decoder network which then goes through a feed forward layer before being fed as embeddings into the recoder. So, since there is no discretization, the whole model can be trained jointly. (the original loss of the first encoder decoder model is used as well anyway).  I agree with the reviewers here, that this whole model can in fact be viewed as a large encoder decoder model, its not really clear where the improvements come from. Can you just increase the number of parameters of the original encoder decoder model and see if it performs as good as the encoder decoder + recoder? The paper also does not achieve SOTA on the task as there are other RL based papers which have been shown to perform better, so the choice of the recorder model is also not empirically justified. I recommend rejection of the paper in its current form.
This paper addresses features that are missing at random in deep supervised learning, especially regression and classification. A deep latent generative model is trained in conjunction with a discriminative model, so that the distribution of the covariates is properly modeled and allows efficient variational inference for imputation.  Superior performance is achieved in low capacity domain or when strong inductive bias is present in the discriminative model.  The paper is well written, with solid empirical support.  The approach is also generic.  Although there is some concern on novelty, overall this paper appears a solid contribution and is a good addition to the proceedings.
This paper introduces a new convolution like operation, called a Harmonic Convolution (weighted combination of dilated convolutions with different dilation factors/anchors), which operates on the STFT of an audio signal. Experiments are carried on audio denoising tasks and sound separation and seems convincing, but could have been more convincing: (i) with different types of noises for the denoising task (ii) comparison with more methods for sound separation. Apart those two concerns, the authors seem to have addressed most of reviewers  complaints. 
This paper explores why adversarial examples do not transfer well in adversarial examples on automatic speech recognition systems. The authors propose a number of potential causes that are then quickly evaluated in turn.  This could be an excellent paper, but in its current form, it is borderline. The main problem with the paper is that it proposes a number of causes for the limited transferability, and then evaluates each of them with one quick experiment and just a paragraph of text. In particular, none of the results actually convince me that the claim is definitely correct, and many of the experimental setups are confusing or would have other explanations other than the one variable that is aiming to be controlled for.  That said, even with these weaknesses, this paper raises interesting and new questions with an approach I have not seen previosuly. So while I don t believe the paper has done much to actually demystify transferability, it does take steps towards performing scientific experiments to understand why it is so hard. And these experiments, while not perfect, can serve as the basis for future work to extend and understand which factors are most important.
Summary: Authors present an approach to improve the robustness of vision transformers by mapping standard tokens into discrete tokens that are invariant to small perturbations. Method is applied to a variety of backbone architectures and evaluated on a range of out of distribution forms of ImageNet test set. Significant performance gains are measured across many of these tasks.  Pros:   Novel, simple, effective approach   General approach applicable across model variants, complimentary to other methods to improve robustness.   Comprehensive study, evaluated on many ImageNet robustness benchmarks   Well written overall  Cons:   Biggest issue: 3 reviewers point out concerns about validity of claims that ViT architecture is more reliant on local patterns and less on global context. This seems mostly a semantic issue around conjectures about why the method works – it does not invalidate the value of the new approach or its solid results.  Authors have responded to reviewer concerns by changing wording in paper to relax the claims, specifying “shape information” rather than “global information”.  They have also added experiments to measure shape bias, as defined in prior art, to backup these claims.    Paper missing baselines of data augmentation strategies. Authors have responded by including such comparative experiments.    Paper is missing ablation studies on changing the type of codebook. Authors have responded by including multiple variations of codebooks, and varying the codebook size.  This paper was a close call based on the reviews. However, in AC opinion, the critiques have been adequately addressed by the authors. This is confirmed by adding an extra expert reviewer to the pool, who agreed with some earlier critiques, and was satisfied with the changes and additional experiments presented by the authors. AC recommendation is to accept.
This paper is an empirical studies of methods to stabilize offline (ie, batch) RL methods where the dataset is available up front and not collected during learning. This can be an important setting in e.g. safety critical or production systems, where learned policies should not be applied on the real system until their performance and safety is verified. Since policies leave the area where training data is present, in such settings poor performance or divergence might result, unless divergence from the reference policy is regularized. This paper studies various methods to perform such regularization.   The reviewers are all very happy about the thoroughness of the empirical work. The work only studies existing methods (and combination thereof), so the novelty is limited by design. The paper was also considered well written and easy to follow. The results were very similar between the considered regularizers, which somehow limits the usefulness of the paper as practical guideline (although at least now we know that perhaps we do not need to spend a lot of time choosing the best between these). Bigger differences were observed between "value penalties" versus "policy regularization". This seems to correspond to theoretical observations by Neu et al (https://arxiv.org/abs/1705.07798, 2017), which is not cited in the manuscript. Although unpublished, I think that work is highly relevant for the current manuscript, and I d strongly recommend the authors to consider its content. Some minor comments about the paper are given below.  On the balance, the strong point of the paper is the empirical thoroughness and clarity, whereas novelty, significance, and theoretical analysis are weaker points. Due to the high selectivity of ICLR, I unfortunately have to recommend rejection for this manuscript.  I have some minor comments about the contents of the paper:   The manuscript contains the line:  "Under this definition, such a behavior policy πb is always well defined even if the dataset was collected by multiple, distinct behavior policies". Wouldn t simply defining the behavior as a mixture of the underlying behavior policies (when known) work equally well?   The paper mentions several earlier works that regularize policies update using the KL from a reference policy (or to a reference policy). The paper of Peters is cited in this context, although there the constraint is actually on the KL divergence between state action distributions, resulting in a different type of regularization.
Summary: Authors present an approach for transformer based object detection that “fully pretrains” the encoder structure of the transformer, and drops the pretrained convolutional backbone used in other works.   Pros:   Eliminates need of extra visual backbone   Fewer parameters than other works   Achieves competitive performance, especially controlling for model size  Cons:   Multiple reviewers raised concerns about authors only evaluating their approach with pretraining from ImageNet 1K, which is not considered large scale. Authors replied with new experimental data including pretraining from ImageNet 22k, which improved results.    Multiple reviewers raised concerns about the need for more ablation experiments, which the authors addressed.  Reviewer scores lean toward accept. Those that lean toward reject raised issues that the authors appear to have addressed sufficiently. Not all reviewers have replied to authors, though understood at least some reviewers on this paper are on end of year time off.  Overall recommendation: accept.
The paper introduces a purely spike based method for training spiking neural networks with recurrence, by extending the recently published "implicit differentiation on the equilibration state (IDE)" technique. As a purely spike based method for both the forward pass and the gradient computation, the proposed technique potentially represents an important advance.  Based on the original submission, the reviewers had difficulties understanding the paper s contributions and verify its claims. I commend the authors for engaging with the reviewers by answering their questions and updating the paper to better explain the contributions. However, even after considerable back and forth, the most positive reviewer still expressed major concerns [[1](https://openreview.net/forum?id VQyHD2R3Aq&noteId Ubrdawfds5)], and the other reviewers appeared unmoved, based on their scores.  The reviewers  principal concerns were a little hard to distill. It is possible that their initial difficulties with understanding and validating the paper s contributions made it hard to fully appreciate the paper. One reviewer is unable to verify that the algorithm is purely spike based, and that the energy costs are appropriately calculated. They are also unsure if the method will scale to more complex settings [[1](https://openreview.net/forum?id VQyHD2R3Aq&noteId Ubrdawfds5)]. A second reviewer was also initially unable to verify the same claims, was unsure if the theoretical improvements were sufficiently significant, and whether the method could apply to non IF models of spiking neural networks [[2](https://openreview.net/forum?id VQyHD2R3Aq&noteId cc2tMpSst0t)]. The authors addressed this in their response by performing additional experiments with LIF neurons, but it wasn t clear if the main concerns were sufficiently addressed, since the reviewer did not change their score.  Based on the largely negative appraisal by all reviewers, I recommend that be paper be rejected. However, I strongly encourage the authors to revise and resubmit their paper to a future conference, focusing on making sure that their central claims can be more easily understood and verified.
The authors propose a new method for neural architecture search, except it s not exactly that because model training is separated from architecture, which is the main point of the paper. Once this network is trained, sub networks can be distilled from it and used for specific tasks.  The paper as submitted missed certain details, but after this was pointed out by reviewers the details were satisfactorily described by the authors.   The idea of the paper is original and interesting. The paper is correct and, after the revisions by authors, complete. In my view, this is sufficient for acceptance.
The paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition.  The revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating.  I think the authors have adequately addressed the reviewer concerns.  The final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies.
General consensus among reviewers that paper does not meet criteria for publication.  Pro:   Improvement over the original IDP proposal.   Some promising preliminary results.  Con:   Insufficient comparison to other methods of network compression,   Insufficient comparison to other datasets (such as ImageNet)   Insufficient evaluation on variety of other models   Writing could be more clear
The paper proposed a novel way to compress arbitrary networks by learning epitiomes and corresponding transformations of them to reconstruct the original weight tensors. The idea is very interesting and the paper presented good experimental validations of the proposed method on state of the art models and showed good MAdd reduction. The authors also put a lot of efforts addressing the concerns of all the reviewers by improving the presentation of the paper, which although can still be further improved, and adding more explanations and validations on the proposed method. Although there s still concerns on whether the reduction of MAdd really transforms to computation reduction, all the reviewers agreed the paper is interesting and useful and further development of such work would be useful too. 
All reviewers appreciate the suggested EM approach to goal conditioned long horizon reinforcement learning, and the technical contributions of the paper. While there is a mix in ratings, even the most critical reviewers feels that the paper has clear merits and is acceptable, and there are two solid acceptance recommendations. Overall, the papers significantly meets the standards of an ICLR paper acceptance.
This paper proposes an algorithmic approach to estimating upper and lower bounds of the rate distortion (R D) function of a data source on the basis of samples drawn from it. The proposed upper bound is based on the variational objective employed in the Blahut Arimoto algorithm, whereas the proposed lower bound is based on the dual characterization of the R D function. In both bounds neural networks trained with samples are utilized. Experimental results on four sources (Gaussian, banana shaped, GAN generated images, and natural images) are provided.  The four review scores were initially two positives and two negatives. Some reviewers evaluated positively the argument on the lower bound of the R D function. On the other hand, one reviewer showed his/her concern about lack of the argument on statistical confidence of the obtained bounds. In response, the authors have addressed it in Section A.6 in the Supplementary Materials (SM) of the revised manuscript with the experiment using GAN generated images (high dimensional data with low intrinsic dimension), and the results are summarized in Tables 1 4 and Figure 10 in SM. The authors have in their revision also provided a specification for the range of the sources to which the proposal would be applicable. Description of the experiments on images, which was missing in the initial version as pointed out by some reviewers, has been added in the revised manuscript. Still, as some reviewers mentioned, the main weakness is that the proposal failed to demonstrate its usefulness to estimate the lower bound in settings where the data dimension is truly high, as in the experiment with natural images, where statistical confidence analysis was not conducted either. Three reviewers have revised their respective scores upward after the author response.  Despite some weaknesses I think that this paper provides a novel and interesting algorithmic approach to estimating the rate distortion function. I would therefore like to recommend acceptance of this paper, and would like to encourage the authors to perform confidence analysis also for the experiments on natural images.
This paper proposes to learn a visual tracking network for an object detection loss as well as the ordinary tracking objective for enhancing the reliability of the tracking network.  The reviewers were unanimous in their opinion that the paper should not be accepted to ICLR in its current form.  A main concern is that the proposed method shows improvement over a relatively weak base system.  Although the author response proposed to include additional analysis, but the reviewers felt that without the additional analysis already included it was not possible to change the overall review score.
Thank you for submitting you paper to ICLR. This paper was enhanced noticeably in the rebuttal period and two of the reviewers improved their score as a result. There is a good range of experimental work on a number of different tasks. The addition of the comparison with Liu & Feng, 2016 to the appendix was sensible. Please make sure that the general conclusions drawn from this are explained in the main text and also the differences to Tran et al., 2017 (i.e. that the original model can also be implicit in this case).
The authors show that models trained to satisfy adversarial robustness properties do not possess robustness to naturally occuring distribution shifts. The majority of the reviewers agree that this is not a surprising result especially for the choice of natural distribution shifts chosen by the authors (for instance it would be better if the authors compare to natural distribution shifts that look similar to the adversarial corruptions). Moreover, this is a survey study and no novel algorithms are presented, so the paper cannot be accepted on that merit either.
I have read the paper and the reviews carefully. Despite the numerical scores, I think this paper is above the bar for ICLR, and recommend acceptance.  This paper addresses the now well known problem that generative models often assign higher likelihoods to out of distribution examples, rendering likelihoods useless for OOD detection. They diagnose this as resulting from differences in compressibility of the input, and propose to compensate for this by comparing the log likelihood to the description length from a strong image compressor. They show this performs well against a variety of OOD detection methods.  The idea is a natural one, and certainly should have been one of the first things tried in addressing this phenomenon. I m a little surprised it hasn t been done before, but none of the reviewers or I are aware of a prior reference, so AFAIK it s novel. One reviewer believes the contribution is small; while it s simple, I think the field will benefit from a careful implementation and testing of this approach.  Multiple reviewers raise the concern of whether generative models  bias towards low complexity inputs is just a matter of needing better generative models. I don t think so: even arbitrarily good generative models will still be limited by the inherent compressibility of an input (e.g. as measured by Kolmogorov complexity).  I m also not concerned about the lack of an explicit threshold; if one has proposed a good score function, there are many ways one could choose a threshold, depending on the task. 
This paper was reviewed by 5 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The authors model point processes with equivariant normalizing flows. Reviewers agreed that the paper is well written and addresses a problem of interest to the ICLR community, some reviewers considered the contribution to be incremental.  Perhaps the biggest contribution is a closed form expression for the trace that needs to be computed as part of the normalizing flow, which is valuable but not particularly emphasized. The authors combine this trace formulation with an equivariant normalizing flow to model the conditional density of point locations given cardinality. (As an aside, it was unclear to me if and how those conditional distributions share parameters; in some contexts, the conditional density could look very different depending on the number of points in the set.) Overall, the paper is interesting but needs a little more to lift it over the bar.
The paper is develops a self training framework for graph convolutional networks where we have partially labeled graphs with a limited amount of labeled nodes. The reviewers found the paper interesting. One reviewer notes the ability to better exploit available information and raised questions of computational costs. Another reviewer felt the difference from previous work was limited, but that the good results speak for themselves. The final reviewer raised concerns on novelty and limited improvement in results. The authors provided detailed responses to these queries, providing additional results.  The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.
This paper presents a series of negative results regarding the convergence of deterministic, "reasonable" algorithms in min max games. The defining characteristic of such algorithms is that (a) the algorithm s fixed points are critical points of the game; and (b) they avoid strict maxima from almost any initialization. The authors then construct a range of simple $2$ dimensional "market games" in which every reasonable algorithm fails to converge, from almost any initialization.  The paper received three positive recommendations and one negative, with all reviewers indicating high confidence. After my own reading of the paper, I concur with the majority view that the paper s message is an interesting one for the community and will likely attract interest in ICLR.  In more detail, I view the authors  result as a cautionary tale, not unlike the NeurIPS 2019 spotlight paper of Vlatakis Gkaragkounis et al, and a concurrent arxiv preprint by Hsieh et al. (2020). In contrast to the type of cycling/recurrence phenomena that are well documented in bilinear games (and which can be resolved through the use of extra gradient methods), the non convergence phenomena described by the authors of this paper appear to be considerably more resilient, as they apply to all "reasonable" algorithms. Determining whether GANs (or other practical applications of min max optimization) can exhibit such phenomena is an important open question, and one which needs to be informed by a deeper understanding of the theory. I find this paper successful in this regard and I am happy to recommend acceptance.
This paper develops a method for sample selection that exploits the memorization effect. While the paper has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR in terms of presentation of the results and experimental validation. The paper will benefit from a revision and resubmission to another venue.
The authors propose a method for distilling a student network from a teacher network and while additionally constraining the intermediate representations from the student to match those of the teacher, where the student has the same width, but less depth than the teacher. The main novelty of the work is to use the intermediate representation from the teacher as an input to the student network, and the experimental comparison of the approach against previous work.    The reviewers noted that the method is simple to implement, and the paper is clearly written and easy to follow. The reviewers raised some concerns, most notably that the authors were using validation accuracy to measure performance, and were thus potentially overfitting to the test data, and regarding the novelty of the work. Some of the criticisms were subsequently amended in the revised version where results were reported on a test set (the conclusions are as before).  Overall, the scores for this paper were close to the threshold for acceptance, and while it was a tough decision, the AC ultimately felt that the overall novelty of the work was slightly below the acceptance bar.
This paper aims to improve performance on edge devices by utilizing a large capacity network in the cloud. To this end, the authors suggest using the routing network that decides whether to use the base model (on the edge device) or the global model (on the cloud). They also propose an overall training scheme for learning not only model parameters, but also network architectures. After the discussion period, 3 reviewers are on the negative side, and 1 reviewer is positive. AC thinks that the authors’ response was not enough to convince the negative reviewers. In particular, AC agrees with the negative comments of reviewers on limited novelty, unclear motivation for the proposed method, and unclear presentations. Overall, AC recommends rejection.
This paper proposes a self exciting temporal point process model with a non stationary triggering kernel to model complex dependencies in temporal and spatio temporal event data. The kernel is represented by its finite rank decomposition and a set of neural basis functions (feature functions). The proposed model has superior performance in comparison to other state of the arts methods. All the reviewers recognized that the model is interesting and advances the state of the art in a meaningful way. While they were some concerns regarding the experimental evaluation, particularly in terms of real data, and the presentation, the rebuttal/revision by the authors cleared up these concerns.
This paper relates the problem of influence maximization and adversarial attacks on GCNs.  The paper, and its formulation and assumptions stirred up quite a discussion among the reviewers and the authors. I do appreciate the thorough rebuttal that the authors provided, and the reviewers did take it into account (and revised their scores).  However, all in all, I am afraid that there are just a few too many concerns with this paper.  If the authors take the reviews to heart, they should be able to improve the manuscript and submit a stronger and improved version to the next conference. 
The paper analyzes the gradient flow dynamics of deep equilibrium models with linear activations and establishes linear convergence for quadratic loss and logistic loss; several exciting results and connections, solid contribution, accept! 
This paper describes a new method for creating word embeddings that can operate on corpora from more than one language.  The algorithm is simple, but rivals more complex approaches.    The reviewers were happy with this paper.  They were also impressed that the authors ran the requested multi lingual BERT experiments, even though they did not show positive results. One reviewer did think that non contextual word embeddings were of less interest to the NLP community, but thought your arguments for the computational efficiency were convincing.
Although reviews were initially a little polarized, they trend toward accepting the paper after rebuttal and discussion.  The most negative review raised issues of datasets, baselines, and experiments, and various details that they find confusing. These concerns were not shared by the other reviewers for the most part. Following a detailed rebuttal the most negative reviewer ended up siding with the more positive reviewers.
The paper shows empirical evidence that the the optimal action value function Q* often has a low rank structure. It uses ideas from the matrix estimation/completion literature to provide a modification of value iteration that benefits from such a low rank structure. The reviewers are all positive about this paper. They find the idea novel and the writing clear. There have been some questions about the relation of this concept of rank to other definitions and usage of rank in the RL literature. The authors’ rebuttal seem to be satisfactory to the reviewers. Given these, I recommend acceptance of this paper.
This paper proposes a reinforcement learning algorithm for continuous action domains that combines a short horizon model based objective and a long horizon value estimate.  The stated benefits to this approach are the ability to modify the model based objective without extensive retraining.  This model based objective can also capture custom constraints and implements a linear dynamics model that is used in conventional control theory.  The proposed method was evaluated on two domains (the mountain car domain and a custom crane domain) and compared to a continuous action space method (DDPG).    This discussion of this paper highlighted both strengths and weaknesses.  The reviewers said the presentation was clear.  The reviewers also appreciated the relevance of the problem.  The primary weakness was the evaluation of the method. One repeated concern from the reviewers was having only one standard domain for evaluation (mountain car).  Another concern was the absence of other model based algorithms, which was addressed by the author response.     This paper is not yet ready to be published, despite its possible benefits,  due to the lack of evidence for this method on more continuous action problems.   
This paper proposes an approach to estimating uncertainty in deep neural network models that avoids the need to make multiple forward passes through a network or through multiple individual models in a posterior ensemble. In terms of strengths, this is an important and timely topic that is of significant interest. The paper is clearly written for the most part. In terms of weaknesses, the significance of the work is low. As the reviewers note, there are multiple questions around the experimental evaluation that remain unresolved following the author feedback and discussion. In particular, the authors do not compare to baseline MCMC methods like HMC/SGHMC that can yield gold standard estimates of posterior predictive uncertainty. While not feasible for large scale models, MCMC methods provides crucial sanity checks for uncertainty estimation on small scale (e.g., MNIST scale) models. Posterior distillation methods like Bayesian Dark Knowledge are also not considered in the evaluation and should be compared to where the distillation computation is feasible. There are also foundational technical correctness issues with respect to uncertainty quantification due to the fact that the paper is approximating the measure of uncertainty produced by MC Dropout, which itself only approximates the true Bayesian posterior predictive distribution under additional assumptions. This makes empirical comparissons to MCMC methods all the more important. Following the discussion, the reviewers agree that the paper is not yet ready for publication.
The paper s stated contributions are:  (1) a new perspective on learning with label noise, which reduces the problem to a similarity learning (Ie, pairwise classification) task  (2) a technique leveraging the above to learn from noisy similarity labels, and a theoretical analysis of the same  (3) empirical demonstration that the proposed technique surpasses baselines on real world benchmarks  Reviewers agreed that (1) is an interesting new perspective that is worthy of study. In the initial set of reviews, there were concerns about (2) and (3); for example, there were questions on whether the theoretical analysis studies the "right" quantity (pointwise vs pairwise loss), and a number of questions on the experimental setup and results (Eg, the computational complexity of the technique). Following a lengthy discussion, the authors clarified some of these points, and updated the paper accordingly.  At the conclusion of the discussion, three reviewers continued to express concerns on the following points:    *Theoretical justification*. Following Theorem 3, the authors assert that their results "theoretically justifies why the proposed method works well". The analysis indeed provides some interesting properties of the reduction, such as the fact that it preserves learnability (Appendix F), and that the "total noise" is reduced (Theorem 2). However, a complete theoretical justification would involve guaranteeing that the quantity of interest (Ie, the clean pointwise classification risk) is guaranteed to be small under the proposed technique. Such a guarantee is lacking.      This is not to suggest that such a guarantee is easy   as the authors note, this might involve a bound that relates pointwise and pairwise classification in multi class settings, and such bounds have only recently been shown for binary problems   or necessary for their method being practical useful (per discussion following Theorem 3). Nonetheless, without such a bound, there are limits to what the current theory justifies about the technique s performance in terms of the final metric of interest.    *Comparison to SOTA*. Reviewers noted that the gains of the proposed technique are often modest, with the exception of CIFAR 100 with high noise. Further, the best performing results are significantly worse than those reported in two recent works, namely, Iterative CV and DivideMix. The authors responded to the former in the discussion, and suggested that they might be able to combine results with the latter. While plausible, given that the latter sees significant gains (Eg, >40% on CIFAR 100), concrete demonstration of this point is advisable: it is not immediately apparent to what extent the gains of the proposed technique seen on "simple" methods (Eg, Forward) would translate more "complex" ones (Eg, DivideMix).     In the response, the authors also mentioned that (at least the initial batch of) the experiments are intended to be a proof of concept. This would be perfectly acceptable for a work with a strong theoretical justification. However, per above, this point is not definitive.    *Creation of Clothing1M*. The authors construct a variant of Clothing1M which merges the classes 3 and 5. Given that prior work compares methods on the original data, and that this potentially reflects noise one may encounter in some settings, it is advisable to at least report results on the original, unmodified version.    *Issues with clarity*. There are some grammatical issues (Eg, "is exact the"), typos (Eg, "over 3 trails"), notational inconsistencies (Eg, use of C for # of classes in Sec 2, but then c in Sec 3.1), and imprecision in explanation (Eg, Sec 3.2 could be clearer what precise relationships are used from [Hsu et al. 2019]).     These are minor but ought to be fixed with a careful proof read.  Cumulatively, these points suggest that the work would be served by further revision and review. The authors are encouraged to incorporate the reviewers  detailed comments.
The paper proposes an adversarial inductive transfer learning method that handles distribution changes in both input and output spaces.  While the studied problem is interesting, reviewers have major concerns about the incremental modeling contribution, the lack of comparative study to existing methods and ablation study to disentangling different modules. Overall, the current study is less convincing from either theoretical analysis or experimental results.  Hence I recommend rejection.
The paper proposes using unlabelled speech data for TTS by decoupling parts of the model. However, all reveiwers agree that the technique is already known and the experimental results are not strong enough to make advantage of training on more data. A reject.
The paper addresses vision based and proprioception based policies for learning quadrupedal locomotion, using simulation and real robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real world results. Given that there are also real robot evaluations, and an interesting sim to real transfer, the paper appears to be an important acceptance to ICLR.
This paper examines how different distributions of the layer wise number of CNN filters, as partitioned into a set of fixed templates, impacts the performance of various baseline deep architectures.  Testing is conducting from the viewpoint of balancing accuracy with various resource metrics such as number of parameters, memory footprint, etc.  In the end, reviewer scores were partitioned as two accepts and two rejects.  However, the actual comments indicate that both nominal accept reviewers expressed borderline opinions regarding this work (e.g., one preferred a score of 4 or 5 if available, while the other explicitly stated that the paper was borderline acceptance worthy).  Consequently in aggregate there was no strong support for acceptance and non dismissable sentiment towards rejection.  For example, consistent with reviewer comments, a primary concern with this paper is that the novelty and technical contribution is rather limited, and hence, to warrant acceptance the empirical component should be especially compelling.  However, all the experiments are limited to cifar10/cifar100 data, with the exception of a couple extra tests on tiny ImageNet added after the rebuttal.  But these latter experiments are not so convincing since the base architecture has the best accuracy on VGG, and only on a single MobileNet test do we actually see clear cut improvement.  Moreover, these new results appear to be based on just a single trial per data set (this important detail is unclear), and judging from Figure 2 of the revision, MobileNet results on cifar data can have very high variance blurring the distinction between methods.  It is therefore hard to draw firm conclusions at this point, and these two additional tiny ImageNet tests notwithstanding, we don t really know how to differentiate phenomena that are intrinsic to cifar data from other potentially relevant factors.  Overall then, my view is that far more testing with different data types is warranted to strengthen the conclusions of this paper and compensate for the modest technical contribution.  Note also that training with all of these different filter templates is likely no less computationally expensive than some state of the art pruning or related compression methods, and therefore it would be worth comparing head to head with such approaches.  This is especially true given that in many scenarios, test time computational resources are more critical than marginal differences in training time, etc.
Dear authors,  Your proposition of adding a noise scaling with the diagonal of the gradient covariance to the updates as a middle ground between the identity and the full covariance is interesting and tackles the timely question of the links between optimization and generalization.  However, the reviewers had concerns about the experiments that did not reveal to which extent each trick had an influence. I would like to add that, even though the term Fisher is used for both the true Fisher and tne empirical one, these two matrices encore very different kind of information. In particular, the latter is only defined when there is a dataset. Hence, your case study  (section 3.2) which uses the true Fisher does not apply to the empirical Fisher.  I encourage the authors to pursue in this direction but to update the experimental section in order to highlight the impact of each technique used.
The paper presents a method for automatically generating levels of varying complexity for training the agent.  The results are well summarized in the paper abstract, "significantly improved sample efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments." The work is clearly presented, and the experiments are thorough.   R1, R2, and R3 voted to accept the paper with 7, 6, and 7 scores. R4 voted to reject the paper with a score of 5. The reviewers mostly agree (except for R4) that significant performance gains have been achieved. R4 is unsatisfied as he/she believes that performance gains are small and exploit the simulator (e.g., using resets).   The paper s main pro is well summarized by R4 s comment, "The method of the paper is simple and can be incorporated into many existing RL algorithms."  The main drawback of the paper is that many curriculum learning techniques have been proposed in the past. E.g., Matiisen et al. (https://arxiv.org/pdf/1707.00183.pdf). In fact the authors discuss this work in the related work section, but dub it multi agent RL work. This is not true. The method of Matiisen et al. is very similar to the proposed approach but uses a different criterion for learning progress. Comparison to this work is warranted, without which the paper should not be accepted. In the post rebuttal discussion, R2 and R3 agree that this comparison is necessary. Therefore, I recommend that this paper be rejected for now and resubmitted to a future venue after incorporating a comparison with Matiisen et al.    
. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The proposed method is novel and effective   The paper is clear and the experiments and literature review are sufficient (especially after revision).   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  The original weaknesses (mainly clarity and missing details) were adequately addressed in the revisions.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted.
The reviewers were mostly concerned about the practical impact/implications of the proposed methods. There was a long discussion across multiple threads of the benefits of the approach proposed in CNNs vs larger language models, dissecting the benefits in terms of training time (as opposed to memory or FLOPs, which may have a non linear impact on running time). Overall, the authors did a good job of putting their contribution into context and addressing the reviewer concerns.
The paper extracts feature interactions in recommender systems and studies the effect of these interactions on the recommendations. While the focus is on recommender systems the authors claim that the ideas can be generalised to other domains also.   All reviewers found the empirical results and analysis thereof to be very interesting and useful. This paper saw a healthy discussion between the authors and reviewers and all reviewers agreed that this paper makes a useful contribution. I recommend that the authors address all the concerns of the reviewers in the final version of the paper. 
The authors present an approach to multi task learning. Reviews are mixed. The main worries seem to be computational feasibility and lack of comparison with existing work. Clearly, one advantage to Cross stitch networks over the proposed approach is that their approach learns sharing parameters in an end to end fashion and scales more efficiently to more tasks. Note: The authors mention SluiceNets in their discussion, but I think it would be appropriate to directly compare against this architecture   or DARTS [https://arxiv.org/abs/1806.09055], maybe   since the offline RSA computations only seem worth it if better than *anything* you can do end to end. I would encourage the authors to map out this space and situate their proposed method properly in the landscape of existing work. I also think it would be interesting to think of their approach as an ensemble learning approach and look at work in this space on using correlations between representations to learn what and how to combine. Finally, some work has suggested that benefits from MTL are a result of easier optimization, e.g., [3]; if that is true, will you not potentially miss out on good task combinations with your approach?  Other related work:  [0] https://www.aclweb.org/anthology/C18 1175/ [1] https://www.aclweb.org/anthology/P19 1299/ [2] https://www.aclweb.org/anthology/N19 1355.pdf   a somewhat similar two stage approach [3] https://www.aclweb.org/anthology/E17 2026/
This paper proposes a novel architecture for learning Hamiltonian dynamics from data. The model outperforms the existing state of the art Hamiltonian Neural Networks on challenging physical datasets. It also goes further by proposing a way to deal with observation noise and a way to model stiff dynamical systems, like bouncing balls. The paper is well written, the model works well and the experimental evaluation is solid. All reviewers agree that this is an excellent contribution to the field, hence I am happy to recommend acceptance as an oral.
All three reviewers recommended rejection and there was no rebuttal.
The paper introduces a variant of the variational autoencoder (VAE) for probabilistic non negative matrix factorization. The main idea is to use a Weibull distribution in the latent space. There is agreement among the reviewers that the paper is technically sound and well written, but that it lacks in motivation and demonstration of utility of the proposed method. All the reviewers think the approach is not particularly novel and somewhat incremental. The main issue is that the empirical evaluation of the algorithm is also quite limited. Specifically, it should have been compared with Bayesian NMF. Many papers have addressed Bayesian NMF with variational inference (Cemgil; Fevotte & Dikmen; Hoffman, Blei & Cook) like in VAE. Experimentally, Bayesian NMF and the proposed PAE NMF could easily be quantitatively compared on matrix completion tasks. Overall, there was consensus among the reviewers that the paper is not ready for publication. 
This paper is a bad fit for ICLR and the authors may consider submitting to more theoretical venues. This paper studies algebraic geometry (an area unfamiliar to most ICLR readers) of program synthesis, with the "hope that algebraic geometry can assist in developing the next generation of synthesis machines." Unfortunately, this paper does not get far enough down that path, and its implications cannot realistically be appreciated by an ICLR audience. The reviewers indicate that their low confidence is due to their lack of understanding of algebraic geometry and not due to their lack of understanding of program synthesis. The featured implication of the paper is that synthesized programs are singularities of analytic functions, which is not very meaningful to the ICLR audience. Even if external reviewers verified the correctness of the math, the ICLR audience would still not understand the implications. 
The paper proposes a regularization method that introduces an information bottleneck between parameters and predictions.  The reviewers agree that the paper proposes some interesting ideas, but those idea need to be clarified. The paper lacks in clarity. The reviewers also doubt whether the paper is expected to have significant impact in the field.
None of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation.  The response of the authors towards this criticism is also not sufficient.  The final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison.  We think that addressing these immediate concerns would improve the quality of this paper.
The authors present a different perspective on the mode collapse and mode mixture problems in GAN based on some recent theoretical results.   This is an interesting work. However, two reviewers have raised some concerns about the results and hence given a low rating of the paper. After reading the reviews and the rebuttal carefully I feel that the authors have addressed all the concerns of the reviewers. In particular, at least for one reviewer I felt that there was a slight misunderstanding on the reviewer s part which was clarified in the rebuttal. The concerns of R1 about a simpler baseline have also been addressed by the authors with the help of additional experiments. I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted.   Having said that, I strongly recommend that in the final version, the authors should be a bit more clear in motivating the problem. In particular, please make it clear that you are only dealing with the generator and do not have an adversarial component in the training. Also, as suggested by R3 add more intuitive descriptions to make the paper accessible to a wider audience.  
The reviews were largely split in the beginning. Although all reviewers find that the idea of diversity and affinity measures for data augmentation is intriguing and potentially useful, they also raised many concerns such as computationally expensive nature of the diversity metric, lack of clear methodology of how to utilize those metrics to design augmentation strategies in practice, and weak organization and presentation of some experiments which are seemingly less related to the main point of the paper. During the discussion phase authors made significant efforts to improve the paper, and some of the concerns are favorably addressed. As a result, two reviewers raised their initial scores, yet we still think the paper is on the borderline.   Overall, this paper presents an interesting and unique idea that potentially stimulate the community, while it also has some key weaknesses and has much room for improvements. Considering both pros and cons, we decided to accept the paper.
While the reviewers considered the basic idea of adding supervision intermediate to differentiable programming style architectures to be interesting and worthy of effort, they were unsure if 1: the proposed abstractions for discussing ntm and nram are well motivated/more generally applicable 2: the methods used in this work to give intermediate supervision are more generally applicable  
This paper proposed a new family of losses for GANs and showed that this family is quite general and encompasses a number of existing losses as well as some new loss functions. The paper compared experimentally the existing losses and the new proposed losses. But the benefit of this family is not clear theoretically, and this work did not also provide the very helpful insights for the practical application of GANs. 
Reviewers and myself agree that the contribution is clear, significant, and has enough originality. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Solid technical contribution, specially the use of continuous noise levels.   Clever application of diffusion/score matching models to a new domain and task, with conditioning.   Good empirical results, both objective and subjective.   Listening samples provided.  Cons:   Lack of formal comparison with flow based vocoders.   Potentially limited novelty.   No official code available.  Note: Readers may also be interested in concurrent work https://openreview.net/forum?id a xFK8Ymz5J ("DiffWave: A Versatile Diffusion Model for Audio Synthesis").
This paper proposes a hierarchical flow based generative model to learn disentangled features at different levels of abstractions.  The key technical contribution is a combination of renormalization group and flow based models. The reviewers do find the idea interesting. However, the merit of the work with respect to StyleGAN and StyleFlow has not been well established. AR3 made the following comment:  “Specially, compared with the style based generator[1,2], …, I don’t find superiorities of the proposed method.” The authors responded to the comment briefly (but not convincingly) in their rebuttal. There is no mention of it in the revised paper. A proper account of the issue would require major revision to the paper.
This paper proposes an unsupervised learning method for GANs, called SLOGAN, which allows conditional generation of samples, by utilizing clustering structures of training data in a latent space. The main significance of the proposal over existing unconditional conditional GANs is that it is capable of dealing with training data with imbalance in the latent space. The proposal consists of the use of implicit reparameterization based on the generalized Stein lemma, which makes learning of the mixing coefficient parameters possible, as well as introduction of the U2C loss.  The initial review score distribution is such that two of them are just above the acceptance threshold, and two others are just below it. Upon reading the review comments and the author responses, as well as the paper itself, I think that the evaluations of the reviewers are more or less coherent with each other:  1. The proposed method is moderately, if not significantly, novel: The differences from DeLiGAN are the use of implicit reparameterization based on the generalized Stein lemma, learning of the mixing coefficient parameters, and introduction of the U2C loss. 2. The experimental results, while demonstrating effectiveness of the proposed method to some extent, were not convincing enough.  As for the item 2, the authors have provided results of additional experiments in their responses, as suggested by the reviewers, and two reviewers have revised their scores upward accordingly.  Yet another point I would like to mention is that in some numerical results summarized in Tables 1 and 2, as well as in several other places, one can notice somewhat large errors, so that one might be able to question the statistical significance of the claimed best performing methods, shown in bold. (If my guess would be correct, the authors regarded the *best in the mean* as the best, ignoring the standard error, and did not perform any statistical testing to confirm the significance.) I would therefore appreciate additional assessment of significance of the numerical results via proper statistical testing.  Because of the above, I would like to recommend acceptance of this paper.
All reviewers agree that the presented ADA Nets approach is very interesting and sufficiently novel, addressing the degradation problem in face clustering. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I disagree with one reviewer’s comment – that the focus of the paper is too narrow – because clustering techniques are of great interest to the ICLR community. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster.
This paper proposes to introduce perturbation biases as a counter measure against adversarial perturbations. The perturbation biases are additional bias terms that are trained by a variant of gradient ascent. Serious issues were raised in the comments. No rebuttal was provided.
This paper addresses the problem of estimating a “birds eyed view” overhead semantic layout estimate of a scene given an input pair of stereo images of the scene. The authors present an end to end trainable deep network that fuses features derived from the stereo images and projects these features into an overhead coordinate frame which is passed through a U Net style model to generate the final top view semantic segmentation map. The model is trained in a fully supervised manner. Experiments are performed on the CARLA and KITTI datasets.   While R2 was positive, they still had some concerns after reading the rebuttal and the other reviews. Specifically, they were not convinced about the value of the IPM module. This concern was also shared by R4, especially in light of the relationship to Roddick et al. BMVC 2019. R1 had concerns about the experiments, specifically the quantitative comparisons to MonoLayout. The authors addressed these comments, but it is still not clear if the differences can be attributed to the number of classes, how they are weighted, or the training split used? R3 had questions about the utility of BEV predictions in general. However, as stated by R2, there is a lot of value in approaching the problem in this way.   In conclusion, while there were some positive comments from the reviewers, there were also several significant concerns. With no reviewer willing to champion the paper, there is not enough support to justify accepting the paper in its current form.  
Three of the reviewers are significantly concerned about this submission while R3 was positive during review. During discussion, R3 also agreed that there are concerns not only on experimental designs and results but also the proposed model. Thus a reject is recommended.
This paper proposes a new method for measuring pairwise similarity between data points. The method is based on the idea that similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in a Random Projection tree.   Reviewers found important limitations in this work, pertaining to clarity of mathematical statements and novelty. Unfortunately, the authors did not provide a rebuttal, so these concerns remain. Moreover, the program committee was made aware of the striking similarities between this submission and the preprint https://arxiv.org/abs/1908.10506 from Yan et al., which by itself would be grounds for rejection due to concerns of potential plagiarism.  As a result, the AC recommends rejection at this time. 
I tend to agree with reviewers. This is a bit more of an applied type of work and does not lead to new insights in learning representations.  Lack of technical novelty Dataset too small
Learn to complete an equation by filling the blank with a missing function or numeral, and also to evaluate an expression.  Along the way learn to determine if an identity holds (e.g. sin^2(x) + cos^2(x)   1).  They use a TreeNN with a separate node for each expression in the grammar.  PROS: 1. They ve put together a new dataset of equational expressions for learning to complete an equation by filling in the blank of a missing function (or value) and function evaluation.   They ve done this in a nice way with a generator and will release it.  2. They ve got two interesting ideas here and they seem to work.  First, they train the network to jointly learn to manipulate symbols and to evaluate them.  This helps ground the symbolic manipulations in the validity of their evaluations.  They do this by using a common tree net for both processes with both a symbol node and a number node.  They train on identities (sin^2(x) + cos^2(x)   1) and also on ground expressions (+(1,2)   3).  The second idea is to help the system learn the interpretation map for the numerals like the "2" in "cos^2(x) with the actual number 2.  They do this by including equations which relate decimals with their base 10 expansion.  For example 2.5   2*10^0 + 5*10^ 1.  The "2.5" is (I think) treated as a number and handled by the number node in the network.  The RHS leaves are treated as symbols and handled by the symbol node of the network. This lets them learn to represent decimals using just the 10 digits in their grammar and ties the interpretation of the symbols to what is required for a correct evaluation (in terms of their model this means "aligning" the node for symbol with the node for number).  3. Results are good over what seem to us reasonable baselines  CONS:  1. The architecture isn t new and the idea of representing expression trees in a hierarchical network isn t new either.  2. The writing, to me, is a bit unclear in places and I think they still have some work to do follow the reviewers  advice in this area.  I really wrestled with this one, and I appreciate the arguments that say it s not novel enough but I feel that there is something interesting in here and if the authors do a clean up before final submission it will be ok.
Three of the reviewers are very positive about this work, and R3 is slightly concerned about the datasets, writing, and notations etc. The authors responded to these concerns in detail and have agreed to take care of these comments. Thus an accept is recommended based on the understanding that the authors will fulfil their commitments.
This paper presents a novel framing of what s at stake when selecting/segmenting text for use in language model pretraining. Four reviewers with experience working with these models agreed that the conceptual and theoretical work here is insightful and worth sharing. The empirical work is fairly small scale and does not yet support broad conclusions, but reviewers did not see such conclusions as necessary for the paper to be valuable.
This work investigates neural network pruning through the lens of its influence over specific exemplars (which are found to often be lower quality or mislabelled images) and how removing them greatly helps metrics. The insight from the paper is interesting, as recognized by reviewers. However, experiments do not suggest that the findings shown in the paper would generalize to more pruning methods. Nor do the authors give directions for tackling the "hard exemplar" problem. Authors  response did provide justifications and clarifications, however the core of the concern remains. Therefore, we recommend rejection.
A hierarchical method is presented for developing humanoid motion control, using low level control fragments, egocentric visual input, recurrent high level control. It is likely the first demonstration of 3D humanoids learning to do memory enabled tasks using only proprioceptive and head based ego centric vision. The use of control fragments as opposed to mocapclip based skills allows for finer grained repurposing of pieces of motion, while still allowing for mocap based learning  Weaknesses: It is largely a mashup up of previously known results (R2).  Caveat: this can be said for all research at some sufficient level of abstraction. The motions are jerky when transitions happen between control fragments (R2,R3). There are some concerns as to whether the method compares against other methods; the authors note that they are either not directly comparable, i.e., solving a different problem, or are implicitly contained in some of the comparisons that are performed in the paper.  Overall, the reviewers and AC are in broad agreement regarding the strengths and weaknesses of the paper.  The AC believes that the work will be of broad interest. Demonstrating memory enabled, vision driven, mocap imitating skills is a broad step forward. The paper also provides a further datapoint as  to which combinations of method work well, and some of the specific features required to make them work.  The paper could acknowledge motion quality artifacts, as noted by the reviewers and  in the online discussion.  Suggest to include  [Peng et al 2017] as some of the most relevant related HRL humanoid control work, as per the reviews & discussion.  
This paper describes a new and experimentally useful way to propose masked spans for MLM pretraining, by masking spans of text that co occur more often than would be expected given their components   ie that are statistically likely to be non compositional phrases.  The authors should make some attempt to connect their PMI heuristic with prior methods for statistical phrase finding and term recognition, eg https://www.aaai.org/Papers/IJCAI/2007/IJCAI07 439.pdf or https://link.springer.com/chapter/10.1007/978 3 540 85287 2_24 in the final paper.
This paper introduces an object perception and control method for RL, derived from a control as inference formulation within a POMDP.  The paper provides a theoretical derivation and experiments where the proposed joint inference approach outperforms baselines.  The discussion focussed on understanding the paper s contribution relative to prior work. The reviewers highlighted the similarities with earlier systems (R1, R2, R4), the unclear benefits of joint inference over independently trained modules in the experiments (R3), and the lack of clarity of the presentation (R1, R2, R3).  The authors responded to some of these criticisms, bolstering the paper with additional experiments to show the benefits of joint inference and increasing the discussion of related work.  The reviewers examined the revisions and rebuttal and found the paper still did not resolve all their original concerns.  Two limitations mentioned in the final phase of the discussion were the use of a single environment to evaluate the general framework, and continuing doubts on the contribution of joint inference mechanism to the measured performance.  Four knowledgeable reviewers indicate reject as their concerns were not adequately resolved.  The paper is therefore rejected.
This paper investigates the role of representation learning when the distribution over the feature space has a long tail. The main motivation is to determine how much of the overall learning, in this case, is bottlenecked specifically by representation learning. The main findings are that vanilla learning gives brittle long tailed representations, harming overall performance. The paper suggests a form of data augmentation to remedy this. Reviewers acknowledge that this investigation is worthwhile. However, many concerns were raised as to whether experiments support the drawn conclusions. A more principled approach to the data augmentation methodology is also needed. The authors address some of these, providing further experiments, but these were not enough to sway reviewers. Since results are fundamentally empirical in nature, this shortcoming indicates that the paper is not ready to share with the community just yet. Stronger experiments with clearer evidence are needed to fully support the thesis of the work.
This paper proposes a new quantum machine learning framework which is evaluated on the MNIST dataset. While the paper was relatively well written, reviewers noted that most of the ideas are already well established and used in quantum machine learning community. Thus it was not clear what novelty is provided relative to related work.
This paper studies the effect of training image classifier with masked images to exclude distraction regions in the image and avoid formation of spurious correlation between them and predicted labels. The paper proposes actdiff regularizer and demonstrates that it prevents such overfitting phenomenon on synthetic data.  However, there was no success on real data. This is important as it shows that the improvement reported in some saliency map based approaches in the literature may be due to other regularization effects such as cutout.  This was a unique submission in my batch, as it embraces its negative results. Among our internal discussions, all reviewers that and we all believe that negative results are important and should be encouraged. However, in order for the negative results to be sufficiently insightful for the entire community, they need to be examined under well organized experiments. This is the aspect that the reviewers think the paper needs to improve on.  In particular, R2 believes the paper could consider a larger set of possible regularizations as well as a broader range of  applications. The insights in such setting may then lead to solid insights on why the current approaches are not very helpful, and in which direction the follow up researches should focus on.
All 3 reviewers consider the paper insufficiently good, including a post rebuttal updated score. All reviewers + anonymous comment find that the paper isn t well enough situated with the appropriate literature. Two reviewers cite poor presentation   spelling /grammar errors making hte paper hard to read. Authors have revised the paper and promise further revisions for final version. 
The reviewers are unanimous that the submission does not clear the bar for ICLR.
The reviewers agree that the EM perspective of Federated Learning is novel and interesting. However, a common criticism is that the connection made is rather shallow and not sufficiently developed. There look to be quite interesting potentials of the proposed framework and the specific FedSparse method, but I agree with the reviewers that both aspects need further development before they are in publishable form.
The paper describes a genetic algorithm for molecular optimization under constraints. The aim is to generate molecules with better properties while close to an initial lead molecule. The proposed approach is a two stage one. The first stage aims to satisfy constraints and searches for feasible molecules that are similar to the lead. The second stage optimizes the molecular property. The method is evaluated on logP optimization task, with minor improvement over previous work.  The reviewers point out the following strengths and weaknesses:  Strengths:    Molecular optimization under structural constraints is an important research direction.   Comprehensive related work section.  Weaknesses:    Lack of novelty because it is a standard application of genetic algorithm.   The results show that the proposed method did not outperform existing baselines.   The main claim of the paper (benefit of two stage procedure) is not supported by ablation study.   The authors only conduct experiments on improving LogP, which is a benchmark that is too easy and not challenging.   The objective function and cross over operation are the same or very similar to previous work.   The experimental evaluation is limited, and the overall setting is not very relevant to real world tasks.  Overall, all reviewers vote for rejection. It is clear that the paper needs more work before it can be published.
This manuscript proposes a technique for co manifold learning that exploits smoothness jointly over the rows and columns of the data. This is an important topic worth further study in the community.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the clarity of the presentation. Further improvement of the clarity   particularly clarification of the learning goals, combined with additional convincing experiments would significantly strengthen this submission.
This paper proposes to regularize neural network in function space rather than in parameter space, a proposal which makes sense and is also different than the natural gradient approach.  After discussion and considering the rebuttal, all reviewers argue for acceptance. The AC does agree that this direction of research is an important one for deep learning, and while the paper could benefit from revision and tightening the story (and stronger experiments); these do not preclude publishing in its current state.  Side comment: the visualization of neural networks in function space was done profusely when the effect of unsupervised pre training on neural networks was investigated (among others). See e.g. Figure 7 in Erhan et al. AISTATS 2009 "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre Training". This literature should be cited (and it seems that tSNE might be a more appropriate visualization techniques for non linear functions than MDS).
The pursued here goal to explore what a broader and more nuanced notion of "imperceptible" perturbation is quite intriguing and could be a basis of really impactful investigations. However, as pointed out in the reviews and comments, the current treatment of this topic suffers from significant presentation and framing shortcomings.   In particular, this work would benefit from stating clearly what is the main topic of study (and what is not), as current framing tends to confuse the readers. It would be also useful to discuss (and acknowledge) that having the split into background and foreground being driven by a (most likely non robust) model makes the resulting notion of perturbation rather tricky to consistently analyze/certify. Also, the analysis of the properties of the model, although done fairly competently, is missing some important aspects.  Still, once these points are properly addressed, this would constitute a valuable contribution. 
All reviewers agree that this paper is interesting, but needs improvement in order to be suitable for a highly competitive venue such as ICLR. Reviewer 3 is especially incisive and detailed, but other reviewers make similar points.
The paper addresses the problem of fair representation learning. The authors propose to use Rényi correlation as a measure of (in)dependence between the predictor and the sensitive attribute and developed a general training framework to impose fairness with theoretical properties. The empirical evaluations have been performed using standard benchmarks for fairness methods and the SOTA baselines   all this supports the main claims of this work s contributions.  All the reviewers and AC agree that this work has made a valuable contribution and recommend acceptance. Congratulations to the authors!  
The paper propose to analyze bitcoin addresses using graph embeddings. The reviewers found that the paper was too incomplete for publication. Important information such as a description of datasets and metrics was omitted.
There are many recent methods for the formal verification of neural networks. However, most of these methods do not soundly model the floating point representation of real numbers. This paper shows that this unsoundness can be exploited to construct adversarial examples for supposedly verified networks. The takeaway is that future approaches to neural network verification should take into account floating point semantics.  This was a borderline paper. On the other hand, to anyone well versed in formal methods, it is not surprising that unsound verification leaves the door open for exploits. Also, there is prior work (Singh et al., NeurIPS 2018) on verification of neural networks that explicitly aims for soundness w.r.t. floating point arithmetic. On the other hand, it is true that many adversarial learning researchers do not appreciate the value of this kind of soundness. In the end, the decision came down to the significance of the result. Here I have to side with Reviewer 1: the impact of this problem is limited in the first place, and also, the issue of floating point soundness has come up in prior work on neural network verification. For these reasons, the paper cannot be accepted this time around.
The paper presents Dopamine, an open source implementation of plenty of DRL methods. It presents a case study of DQN and experiments on Atari. The paper is clear and easy to follow.  While I believe Dopamine is a very welcomed contribution to the DRL software landscape, it seems there is not enough scientific content in this paper to warrant publication at ICLR. Regarding specifically the ELF and RLlib papers, I think that the ELF paper had a novelty component, and presented RL baselines to a new environment (miniRTS), while the RLlib paper had a stronger "systems research" contribution. This says nothing about the future impact of Dopamine, ELF, and RLlib – the respective software.
The paper studies offline meta reinforcement learning. Overall the scope of this contribution seems limited. Reviewers have raised concerns about the significance of the presented results given the assumptions, and that the experimental environments are not extensive and do not fully support the claimed advances. 
In this paper, the authors established interesting theoretical results regarding the behavior Graph Neural Tangent Kernel (GNTK). They also provide sufficient evidence (some of which during rebuttal) that their approach is valid. We have had many discussions and I suggest that the authors apply reviewers  comments to the final version of their paper.
This paper proposes several improvements for the MAML algorithm that improve its stability and performance. Strengths: The improvements are useful for future researchers building upon the MAML algorithm. The results demonstrate a significant improvement over MAML. The authors revised the paper to address concerns about overstatements  Weaknesses: The paper does not present a major conceptual advance. It would also be very helpful to present a more careful ablation study of the six individual techniques. Overall, the significance of the results outweights the weaknesses. However, the authors are strongly encouraged to perform and include a more detailed ablation study in the final paper. I recommend accept.
The paper proposes a simple method for improving the sample efficiency of GAIL, essentially a way of turning inverse reinforcement learning into classification. As reviewers noted, the method is based on a simple idea with potentially broad applicability.  Concerns were raised about the multiple components of the system and what each contributed, and missing pointers to the literature. A baseline wherein GAIL is initialized with behaviour cloning, although only suggested but not tried in previous works. The authors did, however, attempt this setting and found it to hurt, not help, performance. I find this surprising and would urge the authors to validate that this isn t merely an uninteresting artifact of the setup, however I commend the authors for trying it and don t believe that a surprising result in this regard is a barrier to publication.  As several reviewers did not provide feedback on revisions addressing their concerns, this Area Chair was left to determine to a large degree whether or not reviewer concerns were in fact addressed.  I thank AnonReviewer4 for revisiting their review towards the end of the period, and concur with them that many of the concerns raised by reviewers have indeed been adequately dealt with.  
The paper presents a novel gradient estimator for optimizing VAEs with discrete latents, that is based on using a Direct Loss Minimization approach (as initially developed for structured prediction) on top of the Gumble max trick. This is an interesting and original alternative to the use of REINFORCE or Gumble Softmax. The approach is mathematically well detailed, but exposition could be easier to follow if it used a more standard notation. After clarifications by the authors, reviewers agreed that the main theorerm is correct. The proposed method is shown empirically to converge faster than Gumbel softmax, REBAR, and RELAX baselines in number of epochs. However, as questioned by one reviewer, the proposed method appears to require many more forward passes (evaluations) of the decoder for each example. Authors replied by highlighting that an argmax can be more computationally efficient than softmax (in cases when the discrete latent space is structured), and also clarified in the paper their use of an essential computational approximation they make for discrete product spaces. These are important aspects that affect computational complexity. But they do not address the question raised about using significantly more decoder evaluations for each example. A fair comparison for sampling based gradient estimation methods should rest on actual number of decoder evaluations and on resulting timing. The paper currently does not sufficiently discuss the computational complexity of the proposed estimator against alternatives, nor take this essential aspect into account in the empirical comparisons it reports.  We encourage the authors to refocus the paper and fully develop and showcase a use case where the approach could yield a clear a computational advantage, like the structured encoder setting they mentioned in the rebuttal. 
The authors consider distilling posterior expectations for Bayesian neural networks. While reviewers found the material interesting, and the responses thoughtful, there were questions about the practical utility of the work. Evaluations of classification favour NLL (and typically do not show accuracy), and regression (which was considered in the original Bayesian Dark Knowledge paper) is not considered. In general, it is difficult to assess and interpret how the approach is working, and in what application regime it would be a gold standard, e.g., with respect to downstream tasks. The authors are encouraged to continue with this work, taking reviewer comments into account in a final version.
Thank you very much for the detailed feedback to the reviewers, which helped us better understand your paper. Thanks also for revising the manuscript significantly; many parts were indeed revised.  However, due to the major revision, we find more points to be further discussed, which requires another round of reviews/rebuttals. For this reason, we decided not to accept this paper. We hope that the reviewers  comments are useful for improving the paper for potential future publication. 
Strengths: The paper tackles a novel, well motivated problem related to options & HRL. The problem is that of learning transition policies, and the paper proposes a novel and simple solution to that problem, using learned proximity predictors and transition policies that can leverage those. Solid evaluations are done on simulated locomotion and manipulation tasks. The paper is well written.  Weaknesses: Limitations were not originally discussed in any depth.  There is related work related to sub goal generation in HRL. AC: The physics of the 2D walker simulations looks to be unrealistic; the character seems to move in a low gravity environment, and can lean forwards at extreme angles without falling. It would be good to see this explained.  There is a consensus among reviewers and AC that the paper would make an excellent ICLR contribution. AC: I suggest a poster presentation; it could also be considered for oral presentation based on the very positive reception by reviewers.
Although borderline, all reviews are somewhat below the acceptance threshold. The main issue appears to be that the reviewers find the main claims unsupported by the experiments. Other complaints center around the presentation, which could be improved in a revision.
The paper addresses open set DA, where samples from novel classes in the target domain get clustered  into new (unlabeled) classes. A key novelty in the learning setup is that it is assumed that one  has access to a knowledge graph over classes (both source and target). That KG is used for grouping  target samples into novel classes.   Reviewers were concerned that the method is not explained with sufficient  details and the experiments lacked comparisons with openset DA baselines.  No rebuttal was submitted.   The paper cannot be accepted to ICLR.
The paper proposes a novel approach to neural net construction using dynamical systems approach,  such as higher order Runge Kutta method; this approach also allows a dynamical systems interpretation of DenseNets and CliqueNets. While all reviewers agree that this is an intersting a novel approach, along the lines of recent developments in the field on dynamical systems approaches to deep nets, they also suggest to further improve the writing/clarity of the paper and also strengthen  the empirical results (currently, the method only provided advantage on CIFAR 10, while being somewhat suboptimal on other datasets, and more evidence for empirical advantages of the proposed approach would be great). Overall, this is a very interesting and promising work, and with a few more empirical demonstrations of the method s superiority as well as more polished wiriting the paper would make a nice contribution to ML community.
Despite a lively discussion and author explanation and revision, this paper remains below the bar for publication at ICLR. The technical exposition and goals remain poorly explained. The technical contribution is not sufficient. And the utility of the empirical results remain in question. The strong consensus among the reviewers who submitted reviews in a timely manner is that the paper is not suitable for publication.  The 5th and final review, was submitted too late, well beyond the end of the discussion period, and hence was not considered in this decision.
The proposed LAN provides a visualization of the selectivity of networks to its inputs. It takes a trained network as golden target and estimates an LAN to predict masks that can be applied on inputs to generate the same outputs. But the significance of the proposed method is unclear, "what is the potential usage of the model?". Empirical justification of that would make it stronger.  
The paper is well motivated by neuroscience that our brains use information from outside the receptive field of convolutive processes through top down mechanisms. However, reviewers feel that the results are not near the state of the art and the paper needs further experiments and need to scale to larger datasets. 
This paper considers the problem of on device training for federated learning. This is an important problem since, in real world settings, the clients have limited compute and memory, and local training needs to be efficient. The paper shows that the standard sparsity based speed up techniques that consider top K weights/activations during forward and/or backward pass do not work well in the federated setting and proposes several solutions to mitigate this issue. The proposed solutions are demonstrated to work well on several datasets.  In their initial assessment, given that this is largely an empirical insights driven paper, the reviewers mainly expressed concerns about the experimental evaluation (e.g., only one dataset CIFAR10 and one architecture ResNet18) and lack of more baselines (e.g., Federated Dropout). The authors responded in detail to the reviews and also conducted additional experiments and the reviewers and authors engaged in discussion. As the discussion converged, the reviewers agreed that the revised manuscript addresses their key concerns and their assessment, on an average, are now learning largely towards a borderline accept.  I also read the reviews, the discussion, and read the paper. I think the paper is a good initial attempt at providing a general approach to enable on device federated learning when the clients are lightweight devices (e.g. edge devices). Even though the study is somewhat preliminary, the current manuscript, after the revision during the discussion phase, is significantly improved version of the original submission and does address the key concerns from the reviewers. Overall, I would rate the paper for a borderline acceptance.
The reviews are of adequate quality. The responses by the authors are commendable, but ICLR is selective and reviewers continue to believe that more experiments and more rigorous analysis are needed.
This paper presents an interesting method for code generation using a graph based generative approach.  Empirical evaluation shows that the method outperforms relevant baselines (PHOG).  There is consensus among reviewers that the methods are novel and is worth acceptance to ICLR.
This paper proposes a new algorithmic approach to reduced variance in off policy, policy gradient updates.  Multiple reviewers were concerned with both the soundness of the proposed approach, and the cost of using rollouts. In particular, the interaction between the target policy and the behavior policy, and how they are swapped was unclear, where the algorithms in the paper do not match the code provided.  The results show apparent reduction in variance across runs compared with TD3: clear improvements in two domains, minor improvements , and/or an increase in variance in others. In some domains there was decrease in mean performance. The reviewers wanted comparisons with other baseline methods (even in terms of variance across runs).  It is difficult to evaluate the results in this paper, as the performance is averaged over only 5 runs, and runs which result in "failure" are discarded from analysis. The authors explain this was done in the original TD3 code, and one can sympathise in following common practices in the literature. However, the consensus of the reviewers and the AC was that this choice was not well defended, obscures a key difficulty of the learning problem, and makes algorithms look considerably stronger then they actually are. This is particularly confounding in a paper about improving the robustness of learning algorithms. This is not acceptable empirical practice and we strongly encourage the authors to discontinue this.  The reviewers gave nice suggestions including changing the pitch of the paper, and including results in noisy tasks. To reduce the burden of doing more scientific experiments, we suggest the authors start with small or even designed problems to carefully study robustness of learning and the potential improvements due to their algorithm. After this is done in a statistically significant way, it would be natural to move to more demonstration style scaled up results.
The authors propose a novel algorithm for batch RL with offline data. The method is simple and outperforms a recently proposed algorithm, BCQ, on Mujoco benchmark tasks.  The main points that have not been addressed after the author rebuttal are: * Lack of rigor and incorrectness of theoretical statements. Furthermore, there is little analysis of the method beyond the performance results. * Non standard assumptions/choices in the algorithm without justification (e.g., concatenating episodes). * Numerous sloppy statements / assumptions that are not justified. * No comparison to BEAR, making it challenging to evaluate their state of the art claims. The reviewers also point out several limitations of the proposed method. Adding a brief discussion of these limitations would strengthen the paper.  The method is interesting and simple, so I believe that the paper has the potential to be a strong submission if the authors incorporate the reviewers suggestions in a future submission. However, at this time, the paper falls below the acceptance bar.
While the reviewers agreed that the problem of learning robust policies is an important one, there were a number of major concerns raised about the paper, and as a result I would recommend that the paper not be accepted at this time. The important points are: (1) limited novelty in light of prior work in this area (see R2 and R3); (2) a number of missing comparisons (see R2). There is also a bit of confusion in the reviews, which I think stems from a somewhat unclear statement in the paper of the problem formulation. While there is nothing wrong with assuming access to a parameterized simulator and studying robustness under parametric variation, this is of course a much stronger assumption than some prior work on robust reinforcement learning. Clarity on this point is crucial, and there are a large number of prior methods that can likely do well in this setting (e.g., based on system ID, etc.).
The paper proposes a new variant of capsule networks, where iterative routing is replaced by an attention based procedure inspired by Induced Set Attention from Set Transformers. The method is competitive on several classification benchmarks and improves generalization to unseen views on SmallNORB.  The reviewers note that the method is presented well (R2, R3, R4), is more scalable than other capsules variants (R3, R4), and the results are good (R1, R2, R3, R4). However, the reviewers also point out missing relevant baselines (R2, R3, R4), limited amount of generalization experiments (R3), and issues with the positioning of the method and the details of the formulation (R1). In particular, R1 did a very thorough job at reading the paper and discussing with the authors.  The issue with missing baselines has been satisfactorily addressed in the updated version of the paper.  Considering all this feedback and after reading the paper myself, I would summarize the pros and cons of the paper as follows.  Pros: 1. Good presentation 2. The method is more scalable than prior capsule based models 3. Competitive results on several small  to mid scale classification datasets 4. Good results on viewpoint generalization on SmallNORB   Cons: 1. Classification results on all datasets are worse than non capsules models (SE ResNet, AA ResNet). I could not find a discussion of this fact either in the paper, or in the authors’ responses. Given this fact, superior generalization (or some other nice properties) would be a potential advantage of the proposed model. Which leads to the next point. 2. Generalization results on SmallNORB are encouraging, but it is just a single dataset. If these results are key to showing the benefit of the method (as argued in the previous point), it is crucial to demonstrate this generalization in more settings, e.g. at least on MultiMNIST and AffNIST, as suggested by R3. 3. Scalability of the method is only studied in limited detail (I do appreciate Figure 2). The best indication in the direction of scalability is that the model can be trained on ImageNet (which is great), but it performs worse than the ResNet 50 used as a backbone and it is not explained why (even after one of the reviewers asked about it) and how expensive computationally the model is. 4. I share the concerns of R1 regarding the use of the term “MoG”. It is a mathematical term, so one would expect mathematical precision when using it.   4a. It is unclear how the mixing probabilities \phi are learned (IIUC they get no gradient, as described by R1) and if they are in some way actually learned, it is unclear how it is guaranteed that they sum to one.   4b. MoG usually comes with the standard procedure of fitting it to data (EM), which IIUC the authors are not following here. This should be clearly explained.  5. A relatively more minor concern: again, as pointed out by R1, the use of “self ” in “self attention” does not seem accurate. Self attention assumes inputs to the attention procedure attend to themselves in some sense. As one consequence, the output sequence has the same length as the input sequence. ISAB from Set Transformer can be seen as a factorized version of self attention where first inducing points attend to the inputs and then the inputs attend to the inducing points, so the output of the whole block is still the same length as the input. But in the proposed model this second step of going back to the inputs is absent and the length of the output sequence is generally different from the length of the input sequence.  Note: I partially share the doubts R1 raised on the positioning of the method as “capsules” as opposed to “attention”, but I believe it is not the authors’ fault that the definition of what capsules are is historically vague and that this term has been used in many different ways in the past. I would strongly recommend to discuss this point in the updated version of the paper and I hope the capsules community manages to get more clarity on what exactly capsules are. But I do not count this point as a weakness here.  Based on all this evidence, I recommend rejection at this point. The paper has its merit, but it has unfortunate gaps both on the experimental and the presentation sides, as listed above. Some of these have been mentioned during the discussion phase, but the authors have not quite addressed them. There is no mechanism to ensure these are fixed in the final version, so resubmission to a different venue is the only option. 
The paper proposes an approach for learning class level and individual level (token level) representations based on Wasserstein distances between data subsets.  The idea is appealing and seems to have applicability to multiple tasks.  The reviewers voiced significant concerns with the unclear writing of the paper and with the limited experiments.  The authors have improved the paper, but to my mind it still needs a good amount of work on both of these aspects.  The choice of wording in many places is imprecise.  The tasks are non standard ones so they don t have existing published numbers to compare against; in such a situation I would expect to see more baselines, such as alternative class/instance representations that would show the benefit specifically of the Wasserstein distance based approach.  I cannot tell from the paper in its current form whether or when I would want to use the proposed approach.  In short, despite a very interesting initial idea, I believe the paper is too preliminary for publication.
The paper presents a methodology for modeling, and learning, trust in a multi agent reinforcement learning system. The reviewers considered this to be an interesting and important question to answer. Nevertheless, they maintained concerns on multiple fronts. The paper could benefit from being more focused. Authors are strongly encouraged to further scale down the claims in the introduction, and ensure that claims made there and later in the paper are matched with experiments that quantify, and validate, the notions introduced. Model choices made, as well assumptions introduced should be clearly motivated/mapped to reality, in light of their strength. Extending experiments to broader example settings, as outlined in the reviews, would also strengthen the work.
This paper proposes the Skill Action (SA) architecture, based on the insight that semi MDPs in the option framework can be posed as an equivalent MDP. The paper presents interesting theoretical results and very promising empirical results. We thank the reviewers for their revisions, which provided more insights into the method. Of particular interest was the discussion on the "dominant skill problem". We still feel that the paper would benefit from additional experiments, as discussed in detail in all of the reviews. I believe with this inclusion this will be an impactful paper.
This paper studies the unlabeled entity problem in NER. Specifically, performance degradation in training of NER models due to unlabeled entities. It analyzes the reason through evaluation on synthetic datasets and finds that it is due to the fact that all the unlabeled entities are treated as negative examples. To cope with the problem, it proposes a negative sampling method which considers the use of only a small subset of unlabeled entities. Experimental results show that the proposed method achieves better performances than the baselines on real world datasets and achieves competitive performances compared with the state of the art methods on well annotated datasets.  Pros •	The paper is clearly written. •	The proposed method appears to be technically sound. •	Experimental results support the main claims. •	The findings in the paper are useful for the field.  Cons •	Novelty of the work might not be enough.  The authors have addressed some clarity and reference issues pointed out by the reviewers in the rebuttal.  Discussions have been made among the reviewers. 
This work proposes a federated version of the classical $\chi^2$ correlation test. The key new step is the use of stable projection to reduce computational overheads associated with the use of secure multi party protocols. Overall while the contribution is of interest the novelty is rather limited. I also consider the work to be somewhat outside of scope for ICLR. It would be more suitable for a security or statistics focused venue. Therefore I do not recommend acceptance.
This paper describes a clever new class of piecewise linear RNNs that contains a long time scale memory subsystem. The reviewers found the paper interesting and valuable, and I agree. The four submitted reviews were unanimous in their vote to accept. The theoretical insights and empirical results are impactful and would be suitable for spotlight presentation.
The idea to adapt the noise variance in the certification of a base classifier sounds natural and interesting, but unfortunately fundamentally flawed, as correctly pointed out by Reviewer viFi (also acknowledged in the authors  response): the author s main algorithm does not lead to any theoretical certification while the empirical fix (based on memory), however successful in one s experiment, does not rule out the possibility of failure when future test samples flood in. Incidentally, I believe this fallacy may have also answered Reviewer Xsdx s question (why this has not been done before). I agree with Reviewer viFi that the writing of this work is a bit deceptive and will require significant change. In particular, one cannot wave hands at claims on certification: you need to formally prove the memory based empirical fix will provably certify a region for what classifier and under what assumption. Therefore, the current draft cannot be accepted. Please consider rethinking about the idea and rewriting the paper according to the reviewers  comments.
This paper proposes a solution to learn Granger temporal causal network for multivariate time series by adding attention named prototypical Granger causal attention in LSTM.   The work aims to address an important problem. The proposed solution seems effective empirically. However, two major issues have not been fully addressed in the current version: (1) the connection between Granger causality and the attention mechanism is not fully justified; (2) the complex design overkills the whole concept of Granger causality (since its popularity is due to the simplicity).   The paper would be a strong publication in the future if the two issues can be addressed in a satisfactory way. 
This paper proposes a GA based method for optimizing the loss function a model is trained on to produce better models (in terms of final performance). The general consensus from the reviewers is that the paper, while interesting, dedicates too much of its content to analyzing one such discovered loss (the Baikal loss), and that the experimental setting (MNIST and Cifar10) is too basic to be conclusive. It seems this paper can be so significantly improved with some further and larger scale experiments that it would be wrong to prematurely recommend acceptance. My recommendation is that the authors consider the reviewer feedback, run the suggested further experiments, and are hopefully in the position to submit a significantly stronger version of this paper to a future conference.
The submission proposes a variant of a Transformer architecture that does not use positional embeddings to model local structural patterns but instead adds a recurrent layer before each attention layer to maintain local context. The approach is empirically verified on a number of domains.  The reviewers had concerns with the paper, most notably that the architectural modification is not sufficiently novel or significant to warrant publication, that appropriate ablations and baselines were not done to convincingly show the benefit of the approach, that the speed tradeoff was not adequately discussed, and that the results were not compared to actual SOTA results.  For these reasons, the recommendation is to reject the paper.
The authors present a centralized neural controller for multi agent reinforcement learning.   The reviewers are are not convinced that there is sufficient novelty, considering the authors setup as essentially a special case of other recent works, with added adjustments to the neural networks that are standard in the literature.  I personally am more bullish about this paper than the reviewers, as I think engineering an architecture to perform well in interesting scenarios is worth reporting.  However, the reviewers are mostly in agreement, and their reviews were neither sloppy nor factually incorrect.  So I will recommend rejection, following their judgement.  Nevertheless, I encourage the authors to continue strengthening the results and the presentation and resubmit.  
Reviewers are in agreement that this work is a useful, clear, documentary piece of work that shows the utility of CLIP on a number of popular V+L tasks.  There is a somewhat persistent concern that simply demonstrating that a stronger visual encoder leads to improvements downstream is not an insightful result on which the community can build.
This work adds the positional encoding (akin to those in transformers, but adapted) to GNNs. In their reviews, reviewers raised a number of concerns about this work, in particular, lack of novelty, lack of ablations to demonstrate the claims of the paper, lack of comparison to previous work (e.g., position aware GNNS, Graphormer and GraphiT which would appear very related to this work), lack of motivation (e.g., the introduced positional loss do not actually improve performance), and whether the experimental results were really significant. During the rebuttal, the authors replied to the reviews, to address. the concerns that they could. Of the reviewers, unfortunately only one reviewer elected to respond to the authors. It is disappointing that the four other reviewers did not respond and overall the reviewers did not discuss this paper further.  The authors chose to highlight privately to the AC that two reviewers who scored the paper unfavourably did not respond. The authors then argued this should be taken into account in the score (presumably to make acceptance more likely) however, two favourable reviewers also did not respond (not highlighted by the authors). I understand this kind of private request to the AC to dismiss unfavourable reviews (especially if they do not respond) is becoming common I find it unhelpful I can see who and who has not responded.  Nonetheless, looking at the responses to the original concerns of the reviewers highlighted above, I believe the authors have adequately addressed the concerns of the reviewers. Therefore i recommend acceptance but only as a poster.
The authors propose a new mini batch selection method for training deep NNs. Rather than random sampling, selection is based on a sliding window of past model predictions for each sample and uncertainty about those samples. Results are presented on MNIST and CIFAR.  The reviewers agreed that this is an interesting idea which was clearly presented, but had concerns about the strength of the experimental results, which showed only a modest benefit on relatively simple datasets. In the rebuttal period, the authors added an ablation study and additional results on Tiny ImageNet. However, the results on the new dataset seem very marginal, and R1 did not feel that all of their concerns were addressed. I’m inclined to agree that more work is required to prove the generalizability of this approach before it’s suitable for acceptance. 
The reviewers unanimously appreciated the clarity of the work as well as the framing of the proposed method. Congratulations.
The results in the paper are interesting, and the modifications improve the paper further. Reviewers found teh paper interesting and potentailly applicable to many models.
The paper extends MuZero to stochastic (but observable) MDPs. To represent stochastic dynamics, it splits transitions into two parts: a deterministic transition to an afterstate (incorporating all observations and actions up to the current time), followed by a stochastic outcome (accounting for new randomness that follows the last action). The transition to an afterstate is similar in spirit to ordinary MuZero s dynamics model; the stochastic outcome is learned by a VQ VAE. At planning time, MuZero retains the MCTS lookahead from ordinary MuZero. Stochastic MuZero achieves impressive results: e.g., it maintains the original MuZero s strong performance on the deterministic game of Go, while improving on MuZero significantly (and achieving superhuman performance) on the stochastic game of backgammon.  This is a strong paper overall: it presents a convincing and successful extension of the already influential MuZero work, along with large scale computational experiments confirming the utility of the approach. There are nonetheless a few weaknesses: first, compared to the original AlphaZero and MuZero work, it is perhaps less surprising that the given approach is successful, since it is more closely related to prior work. Second, due to the large scale computational infrastructure needed, it is only possible to run some of the experiments once. This is not in itself a problem, but care needs to be taken in interpreting the results of such single run experiments: e.g., any figures that show results of single run experiments should have a clear warning label, and any statements such as "stochastic MuZero performs better than original MuZero" should be tempered with a caveat about how reliable these conclusions are likely to be. Section 5.4 (which runs shorter experiments using three random seeds each) makes a start at evaluating reliability, but (a) the headline results in previous sections do not contain any caveats or pointers to 5.4, and (b) 5.4 should explicitly acknowledge that it cannot hope to detect even quite common failure cases with so few seeds.
This paper proposes a contrastive learning framework that leverages hard negative samples for self supervised training. The proposed framework is theoretically analyzed and its efficacy is examined on several datasets/problems. A group of expert reviewers reviewed the paper and provided positive ratings for this paper. I agree with the reviewers and I recommend accepting this submission.   One of the main discussion points among the reviewers was to what degree Pr1 is "approximately" satisfied in the proposed framework. There are several approximations in this paper that are not fully analyzed.  Some of these approximations could be examined assuming that labeled data is available during training. For example, $p_x^+$ is approximated using a set of semantics preserving transformations. In practice, the distribution induced by augmenting $x$ is very different than the distribution that samples from the instances in the class of $x$. The effect of this approximation could be easily examined by sampling from true class labels. Additionally, it would be very helpful to visualize how $q$ samples from the negative instances and how much it follows Pr1.  I would like to ask the authors to add a small limitations section to the final camera ready version that lists all the assumptions and approximations made in this paper. Please provide a high level analysis on how such assumptions could be validated or such approximations could be measured if labeled data or additional information was provided. This discussion is extremely important for future practitioners to understand the basic assumptions that may not hold in reality and it will enable them to improve upon this work.  
All three reviewers recommend borderline rejection based on limited novelty, missing comparisons with other methods, and runtime inefficiency. The authors’ response helped clarify other questions but did not eliminate the main concerns about the paper. The AC agrees with the reviewers that, in its current form, the paper does not pass the acceptance bar of ICLR. The reviews have detailed comments and suggestions that should help the authors to improve the work for another conference.
This paper proposes a change in the attention mechanism of Transformers yielding the so called "Tensor Product Transformer" (TP Transformer). The main idea is to capture filler role relationships by incorporating a Hadamard product of each value vector representation (after attention) with a relation vector, for every attention head at every layer. The resulting model achieves SOTA on the Mathematics Dataset. Attention maps are shown in the analysis to give insights into how TP Transformer is capable of solving the Mathematics Dataset s challenging problems.   While the modified attention mechanism is interesting and the analysis is insightful (and improved with the addition of an experiment in NMT after the rebuttal), the reviewers expressed some concerns in the discussion stage:  1. The comparison to baseline is not fair (not to mention the 8.24% claim in conclusion). The proposed approach adds 5 million parameters to a normal transformer (table 1, 5M is a lot!), but in terms of interpolation, it only improves 3% (extrapolation improves 0.5%) at 700k steps. The rebuttal claimed that it is fair as long as the hidden size is comparable, but I don t think that s a fair argument. I suspect that increasing the feedforward hidden size (d_ff) of a normal transformer to match parameters (and add #training steps to match #train steps) might change the conclusion. 2. The new experiment on WMT further convinces me that the theoretical motivation does not hold in practice. Even with the added few million more parameters, it only improved BLEU by 0.05 (we usually consider >0.5 as significant or non random). This might be because the feedforward and non linearity can disambiguate as well.   I also found the name TP Transformer a bit misleading, since what is proposed and tested here is the Hadamard product (i.e. only the diagonal part of the tensor product).   I recommend resubmitting an improved version of this paper with  stronger empirical evidence of outperformance of regular Transformers with comparable number of parameters.
This paper builds on the recent theoretical work by Khemakhem et al. (2019) to propose a novel flow based method for performing non linear ICA. The paper is well written, includes theoretical justifications for the proposed approach and convincing experimental results. Many of the initial minor concerns raised by the reviewers were addressed during the discussion stage, and all of the reviewers agree that this paper is an important contribution to the field and hence should be accepted. Hence, I am happy to recommend the acceptance of this paper as an oral. 
The authors propose the OPT in Pareto algorithm that considers multi objective optimization, and includes an extra "non informative" reference metric for choosing between different Pareto optimal solutions.  The reviewers generally agreed that the work was compelling. However, one reviewer (6MZF) brought up the fact that the proposal is extremely similar to one proposed by a different arXiv paper, and convincingly argued that the authors of this paper were aware of the other before submission.  This is a difficult situation. On the one hand, for the purposes of establishing priority, an arXiv paper "doesn t count". On the other hand, I believe that authors are obligated to appropriately credit all relevant work of which they are aware, in *any* form: this includes journals, conference proceedings, preprints, emails, personal conversations, stackoverflow posts, tweets, etc. In this case, it seems that the authors did not adhere to this second condition, and while they have updated their manuscript, two reviewers said that they were unsatisfied by the changes on this point.  I want to emphasize that this isn t a question of priority: the first to publish "wins", and nobody has published this work, yet. However, other researchers working on the same problem, and proposing similar solutions, *must* be appropriately credited, even by the eventual winners (if they are aware of them).
This paper presents a method for unsupervised domain adaptation, focusing on the object detection problem. Under this framework, the paper proposes modules of domain adaptive instance normalization, global style alignment and local content alignment. The proposed method is evaluated on multiple datasets.  Several reviewers have pointed out that the paper lacks discussion and comparison to related methods. The paper has some merits but, the lack of a proper presentation, and the fact that there are not enough experimental results to support all claims in the paper, result in a submission that does not meet the bar of ICLR publication. Hence, the current paper is recommended to be not published at ICLR.
Most reviewers are positive about this work, though they believe it is somewhat incremental, and its theoretical contributions are minor. None of the reviewers are very excited about this work. Overall, the PC believes this is a borderline paper.  Minor note: During the discussions, the paper by Xiao et al., "Characterizing Attacks on Deep Reinforcement Learning" (2019) was brought up. The authors claimed that they did not compare with that paper because the best attack there (obs fgsm wb) had already been studied. In a later stage of discussions, one of the reviewers stated that the method obs nn wb in that paper performed better in some domains. Even though this is not a major issue, it is advisable to the authors to make sure that this is indeed the case, and if it is, provide proper comparison with that paper.  We encourage the authors to consider the reviewers  comments to improve the paper and resubmit to a future venue.
The paper proposes a method to combine the decision of an ensemble of RL agents. It uses an uncertainty measure based on the TD error, and suggests a weighted average or weighted voting mechanism to combine their policy or value functions to come up with a joint decision. The reviewers raised several concerns, including whether the method works in the stochastic setting, whether it favours deterministic parts of the state space, its sensitivity to bias, and unfair comparison to a single agent setting. There is also a relevant PhD dissertation (Elliot, 2017), which the authors surprisingly refused to discuss and cite because apparently it was not published at any conference. A PhD dissertation is a citable reference, if it is relevant. If it is, a good scholarship requires proper citation.  Overall, even though the proposed method might potentially be useful, it requires further investigations. Two out of three reviewers are not positive about the paper in its current form. Therefore, I cannot recommend acceptance at this stage.  Elliott, Daniel L., The Wisdom of the crowd : reliable deep reinforcement learning through ensembles of Q functions, PhD Dissertation, Colorado State University, 2017
The paper proposes a new meta learning algorithm which promises greater robustness to adversarial examples. I will be brief, as the fault with the paper is quite clear: the experimental results are not sufficient. The attack used (FGSM) is particularly dated and weak, and the comparison to existing defences is insufficient. Additionally, prior work (Adversarially Robust Few Shot Learning: A Meta Learning Approach) obtains better results, and is not compared against. The reviewers provided further criticism regarding the motivation for and explanation of the method, but the empirical aspects of the paper are where it primarily falls short of the publishable standard for ICLR.  I recommend rejection, and invite the authors to consider demonstrating robustness to a wider range of attacks (including non gradient based), and a more thorough comparison to defence methods, before resubmitting to another conference.
This paper presents an approach for distilling a larger teacher model into a set of students that can run in parallel at lower cost. The main strengths are that the approach appears conceptually sound and reasonably well executed. The main weaknesses are that the differences relative to previous work is fairly slim, and the experimental results are overly idealized. While the main benefit of the approach is improvement in latency, the experiments evaluate in terms of FLOPS. There was some back and forth between the authors and reviewers about these points. Authors added some additional results and seem to acknowledge the limitations, e.g., saying “Our time and hardware constraints did not allow us to perform experiments in a realistic deployment.” However, for a paper primarily concerned with reducing latency, reviewers were unconvinced that this evaluation was sufficient.
This paper develops a hybrid search space consisting of both multiplication based and multiplication free operators. It also presents a weight sharing mechanism for searching in the introduced search space.  Pros: * A hybrid search space is developed. * Strong empirical results are reported for both CV and NLP tasks. * The paper is well written and is easy to follow.  Cons:   * Incremental technical novelty. * Missing baselines and competing methods. * Missing information on the search cost. * Lack of insights into the discovered architectures  The rebuttal has provided most missing information and comparisons, and it has provided additional insights into the searched architectures. However, the reviewers still rate this paper at borderline primarily due to the limited technical novelties. Unfortunately, given these concerns, this submission does not meet the bar for acceptance at ICLR.
The authors propose a technique for compressing neural networks by examining the correlations between filter responses, by removing filters which are highly correlated. This differentiates the authors’ work from many other works which compress the weights independent of the task/domain.  Strengths: Clearly written paper PFA KL does not require additional hyperparameter tuning (apart from those implicit in choosing \psi) Experiments demonstrate that the number of filters determined by the algorithm scale with complexity of the task  Weaknesses: Results on large scale tasks such as Imagenet (subsequently added by the authors during the rebuttal period) Compression after the fact may not be as good as training with a modified loss function that does compression jointly Insufficient comparisons on ResNet architectures which make comparisons against previous works harder  Overall, the reviewers were in agreement that this work (particularly, the revised version) was close to the acceptance threshold. In the ACs view, the authors addressed many of the concerns raised by the reviewers in the revisions. However, after much deliberation, the AC decided that the weaknesses 2, and 3 above were significant, and that these should be addressed in a subsequent submission.
The majority of the reviewers believe that this paper is not ready for publication. Among their concerns is that the paper has limited novelty, especially in relation to existing work that use the KL constraint. Some of the reviewers also believe that the arguments are sometimes hand wavy and not rigorous. For example, in the discussion period after Nov. 24th, it is mentioned that the argument by the authors that "KL constraint >decrease the approximation error >increase performance" is not precise enough. I encourage the authors to take these comments into account and improve their paper.
The paper analyzes convolutional kernels and their sample complexity as compared to different architectures, and in particular the effect of pooling. The analysis proceeds by characterizing the RKHS in this setting (for a distribution on the cube) and using results by Mei and others to obtain separation between different architectures.  The reviewers appreciated the fact that this is an example worked out in detail, resulting in a clear message about sample complexity gaps between architectures. However, there was also concerns that some of the conclusions do appear in previous works, so that there is no surprising insight here.  In future versions, the authors are encouraged to more clearly explain the novel aspects of the paper (as well as where the main technical novelties and tools are).
All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR and the authors did not respond during the rebuttal phase.
The paper presents an interesting approach for defining conditional diffusion models. The core idea of this work is based on a new analysis of how class centers evolve in the forward diffusion process. On this positive side, this work builds on top of this analysis and introduces conditional diffusion processes that are guided towards class centers. This paper shows marginal improvements in small image datasets (MNIST and CIFAR 10) and auxiliary applications such as image inpainting and attribute based image synthesis (demonstrated through only qualitative experiments). On the negative side, the proposed approach has the fundamentally limiting assumption that a class can be represented by a cluster center in the RGB space. Unfortunately, this assumption does not hold for practical datasets such as ImageNet where samples in each class have high diversity, and the class centers in the pixel space are not very distinct for different categories. The reviewers have rated this paper slightly above the borderline. They have acknowledged the novelty of the proposed guided diffusion. But they have criticized the submission for the lack of experiments on more common and challenging benchmarks. They also have criticized this work for not providing quantitative results on the auxiliary tasks. I agree with the reviewers that these experiments would shed light on whether the class center idea would hold for more challenging scenarios.  In the rebuttal, the authors provided additional quantitative results for the text to image generation task. However, these results show that the proposed method is outperformed by prior works. Most other auxiliary tasks including image inpainting and attribute to face generation are still demonstrated through qualitative experiments without detailed quantitative results.   In summary, given the limitation of the proposed approach, this submission currently lacks an in depth analysis of the proposed work on challenging benchmarks and a detailed quantitative comparison to relevant baselines for the auxiliary tasks. Because of these concerns, we believe that the paper in its current form is not ready for publication at ICLR at this point.
Pros   Lays out bounds for multi domain adaptation based on earlier work on a single source target domain pair.   Shows gains over choosing the best source domain for a target domain, or naively combining domains.  Cons   The reviewers agree that the extensions are relatively straightforward extensions to single source target pair.   Hard max doesn’t consider the partial contribution of multiple source domains, and considers the worst case scenario.   Soft max addresses some of these issues; the authors provide reasonable justification for the algorithm but it’s not clear that the specific choice of \alphas leads to the tightest bound.  The reviewers noted that the authors significantly improved the paper during the revision process. The AC feels that the presented techniques would be of interest to the community and would help lead discussions towards theoretically optimal ways to do domain adaptation given multiple domains. The authors are therefore encouraged to submit to the workshop track. 
This work studied an important issue, i.e., adversarial transferability, in adversarial examples. It provides a novel perspective that samples in  the low density region of the ground truth distribution where models are not well trained have stronger transferability across different models. Based on that, it proposed a metric called Alignment between its Adversarial attack and the Intrinsic attack (AAI) to indicate transferability. Inspired by the connection between AAI and transferability, this work further proposed to replace the regular ReLU activation with some smooth activation functions, to enhance the transferability.   Most reviewers appreciate that the observation is interesting, and the theoretical analysis and the proposed method are intuitive. The reviewers posed some important comments on experiments, and the relationship between the proposed method and the proposed metric. The authors provided satisfied responses to most of these concerns. Although there is one remaining concern that AAI may be not the best metric to choose the structural hyper parameters, the reviewer still thought it is a good theoretical starting point to further analyze the adversarial transferability.   After reading the submission, reviewers  comments and the discussions between reviewers and authors, I believe that this work has provided a valuable perspective, a reasonable theoretical analysis and an effective solution for adversarial transferability. It could inspire further studies on adversarial transferability.
This paper proposes adding noise to the parameters of a deep network when taking actions in deep reinforcement learning to encourage exploration.  The method is simple but the authors demonstrate its effectiveness through thorough empirical analysis across a variety of reinforcement learning tasks (i.e. DQN, DDPG, and TRPO).  Overall the paper is clear, well written and the reviewers enjoyed it.  However, a common trend among the reviews was that the authors overstated their claims and contributions.  The reviewers called out some statements in particular (e.g. the discussion of ES and RL) which the authors appear to have addressed when comparing their revisions (thank you).  Overall, a clear, well written paper conveying a simple but effective idea for exploration that often works across a variety of RL tasks.  The authors also released open source code along with their paper for reproducibility (as evidenced by the reproducibility study below), which is appreciated.  Pros:   Clear and well written   Thorough experiments across deep RL domains   A simple strategy for exploration that is effective empirically  Cons:   Not a panacea for exploration (although nothing really is)   Claims are somewhat overstated   Lacks a strong justification for the method other than that it is empirically effective and intuitive
This paper presents a learned inference architecture which generalizes HMC. It defines a parameterized family of MCMC transition operators which share the volume preserving structure of HMC updates, which allows the acceptance ratio to be computed efficiently. Experiments show that the learned operators are able to mix significantly faster on some simple toy examples, and evidence is presented that it can improve posterior inference for a deep latent variable model. This paper has not quite demonstrated usefulness of the method, but it is still a good proof of concept for adaptive extensions of HMC.  
This paper studies the problem of estimating the trajectory of a linear dynamical system when the covariances for the process and observation noise are unknown. The standard solution is to estimate these covariances from data, and this paper instead suggests an optimization procedure. They show promising experimental results. However there are two shortcomings: In terms of theoretical guarantees, they can only show convergence to a local optimum. Moreover they assume they have access to the ground truth hidden states. Although this is an assumption that has appeared in earlier works, it seems to limit the applicability.
This paper addresses an important and relevant problem in reinforcement learning: learning from off policy data, taking into account the offsets in the visitation distribution of states. This has the promise of lowering variance even with long horizon roll outs. Existing methods have required access to the behavior policy (or have required data from the stationary distribution). The novel proposed approach instead uses an alternative method, based on the fixed point of the "backward flow" operator, to calculate the importance ratios required for policy evaluation in discrete and continuous environments.   In the initial version of the submission, several concerns were expressed regarding both the quality of the paper and clarity. The authors have updated the paper to address these concerns to the satisfaction of the reviewers, who are now unanimously in favor of acceptance. 
The paper examines an idea that knowledge and rewards are stationary and reusable across tasks. An interesting paper that combines number of related topics (meta RL, HRL, time scale in RL, and attention), improving the speed of training.  The authors have addressed the reviewer comments, strengthening the paper. The reviewers  agree, and I concur, that the paper contributes a novel model,  valuable to the ICLR community. It is well thought out, presented, and evaluated. 
Reviewers could not reach consensus here and important concerns from one reviewer on empirical results could not be convincingly addressed. The authors have provided a comprehensive response to the reviews, yet failed to convince them. 
This paper focuses on studying the impact of initialization and activation functions on the Neural Tangent Kernel (NTK) type analysis. The authors claim to make a connection between NTK and edge of chaos analysis. The reviewers had some concern about (1) impact of smooth activations "any NTK based training method for DNNs should use a Smooth Activation Function from the class S and the network should be initialized on the EOC" (2) proofs of residual networks (3) and why mixing NTK with EOC is interesting. Some of these concerns were addressed in the response. I do share the reviewer concerns about (2). The authors need to give a clear proof. I think this combination of NTK and EOC could be interesting but needs to be better motivated. As a result I do not recommend publication.
The reviewers all agree that this paper proposes a very interesting approach of finding useful information encoded inside a generative model. They show how foreground/background semantics learnt in a generative model are useful for tasks like segmentation. This is a general approach that can be applied to other models in the future. It is an accept.
This paper proposes a knowledge distillation strategy to enable the use of a large server side model in federated learning while satisfying the computation constraints of resource limited clients. The problem is relevant and well motivated, and the paper presents compelling experimental results to support the proposed strategy. However, reviewers had the following major comments suggestions/: 1) The theoretical analysis section needs improvement in terms of the technical depth and rigor 2) Better explanation of how the proposed strategy compares with previous works/baselines 3) Considering the privacy and scalability properties of the proposed strategy.  The paper generated lots of constructive post rebuttal discussions between the authors and the reviewers, and I believe the authors received several ideas to improve the work and appreciated the reviews. One of the reviewers increased their score. However, based on the current scores, I still recommend rejection. I do think the paper has promise, and with improvements, the revised version will make an excellent contribution.
 pros:   good, clear writing   interesting analysis   very important research area   nice results on multi task omniglot  cons:   somewhat limited experimental evaluation  The reviewers I think all agree that the work is interesting and the paper well written. I think there is still a need for more thorough experiments (which it sounds like the authors are undertaking).  I recommend acceptance.  
The authors introduce the idea of using Wasserstein distances over latent "behavioral spaces" to measure the similarity between two polices, for use in RL algorithms.  Depending on the choice of behavioral embedding, this method produces different regularizers for policy optimization, in some cases recovering known algorithms such as TRPO.  This approach generalizes ideas of similarity used in many common algorithms like TRPO, making these ideas widely applicable to many policy optimization approaches.  The reviewers all agree that the core idea is interesting and would likely be useful to the community.  However, a primary concern that was not sufficiently resolved during the rebuttal period was the experimental evaluation   both the ability of the experiments to be replicated, as well as whether they provide sufficient insight into how/why the algorithm performs.  Thus, I recommend rejection of this paper at this time.
This paper proposes a new criterion for neural architecture search that does not require the expensive step of training the model. The reviewers found the proposed approach of relying on gradient statistics promising. However, the reviewers found that the clarity of the paper needs to be improved and that the empirical evidence is too limited to support some of the claims.
The paper investigates the relationship between data augmentations used during training and their effect on the accuracy when evaluated on unseen corruptions at test time. The paper proposes a metric called minimal sample distance (MSD) to measure the similarity between augmentations during training time and corruptions at test time.  The reviewers agree that the paper aims to solve an important problem and the paper has some interesting findings. However, the current version has a few shortcomings:   Some of the claims about “overfitting” are confusing, especially for data augmentations that use ops similar to those in ImageNet C. This is already known and which is why some papers uses a subset of operations (e.g. AugMix uses a subset of AutoAugment operations).   The main take home message and novelty is unclear: The initial version titled (“Is Robustness Robust?“) seemed to argue that we may be overfitting to Imagenet C, but the rebuttal and the updated version revised some of the claims (see response to R3 and R4).  In light of the revision, I’m not sure how the main take home messages differ from existing papers such as Yin et al. 2019 or “Many faces of robustness”.    One of the main differences is quantification of the distribution similarity, however, as pointed out by R2, this analysis does not explain when stylized corruptions would help, so the current version of the paper feels a bit incomplete to me.  I recommend the authors to revise the draft based on reviewer feedback and resubmit the paper to another venue. 
The paper proposes a framework for continual/lifelong learning that has potential to overcome the problems of catastrophic forgetting and data privacy.  R1, R2 and AC agree that the proposed method is not suitable for lifelong learning in its current state as it linearly increases memory and computational cost over time (for storing features of all points in the past and increasing model capacity with new tasks) without account for budget constraints.  The authors responded in their rebuttal that the data is not stored in the original form, but using feature representation (which is important for privacy issues). The main concern, however, was about the fact that one has to store information about all previous data points which is not feasible in lifelong learning. In the revision the authors have tried to address some of the R1’s and R2’s suggestion about taking into account the budget constraints. However more in depth analysis is required to assess feasibility and advantage of the proposed approach.  The authors motivate some of the key elements in their model as to protect privacy. However no actual study was conducted to show that this has been achieved.  The comments from R3 were too brief and did not have a substantial impact on the decision.  In conclusion, AC suggests that the authors prepare a major revision addressing suitability of the proposed approach for continual learning under budget constraints and for privacy preservation and resubmit for another round of reviews.   
The paper argues that GNNs can be understood as a graph signal denoising. While this interpretation is not surprising and not novel, the unified view does seem insightful according to some reviewers. Yet, it is not clear how much insight can be drawn from the presented theory, as no significantly better architecture or experimental results are presented.   Additional criticism was raised wrt unclear relation between GAT and the graph signal denoising, the fact that analysis focused on one layer and does not explain the relations between layers and how nonlinear activation functions affect these theoretical findings, and that the objective of GNN cannot be viewed as a simple combination of graph denoising problems. Several reviewers complained that the paper is hard to follow.   In light of the above, despite the significant efforts of the authors to address these issues in the rebuttal, we believe the paper is below the bar and recommend Rejection. 
The paper uses dynamical systems theory to evaluate feed forward neural networks.  The theory is used to compute the optimal depth of resnets.  An interesting approach, and a good initiative.  At the same time, the approach seems not to be thought through well enough, and the work needs another level of maturation before publication.  The application that is realised is too immature, and the corresponding contributions are not significant in their current form.  All reviewers agree on rejection of the paper.
The authors provide an interesting improvement on privacy attacks in federated learning, demonstrating the ability to extract individual points even over large batches. While there were some concerns about the technical difficulty of the approach, reviewers were broadly in support of the work. As I tend to agree, this is an interesting strengthening beyond what it appears we were able to do before. This is yet another piece of evidence against the canard in FL that only sharing gradient updates provides privacy guarantees.